{"cells":[{"metadata":{"_cell_guid":"34f49aad-8c6b-42eb-a8ba-f25ff89860dc","_uuid":"73015d7c4bb41b695d044e0900bc339868000e03"},"cell_type":"markdown","source":"# Data Exploration"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import log_loss\n\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\npd.set_option('display.max_colwidth', 200)\n# Any results you write to the current directory are saved as output.","execution_count":37,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"13bf8b8469037f83054bfc086fb3c0a66ec3efe6"},"cell_type":"code","source":"class Lodash:    \n    def flow(self, *args):\n        def fns(payload):\n            result = payload\n            for fn in args:\n                result = fn(result)\n            return result\n        return fns\ndash = Lodash()","execution_count":6,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true},"cell_type":"markdown","source":"## Training Dataset structure\n*Id*: the id of for the data\n\n*pairqid1*: The id for the first question\n\n*aid2*: The id for the second question.\n\n*is_duplicate*: Is considered that the two questions are the same"},{"metadata":{"_cell_guid":"38d5343e-c175-4e18-906f-a6e40466d228","_uuid":"a85fd8785dbceaf303cfa907d50cefdd44ed8a33","trusted":true},"cell_type":"code","source":"train  =  pd.read_csv('../input/train.csv')\ntrain.head(10)","execution_count":3,"outputs":[]},{"metadata":{"_uuid":"7a6e61b7586cf37f1d31b91aebadc396e1497a08"},"cell_type":"markdown","source":"## Checking the data\n\nBasic functions to count for the whole dataframe."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"26e5b1ea6acf99068e50c09bb6236fc18b320e98"},"cell_type":"code","source":"def getNumberOfRows(df):\n    return len(df)\n\ndef getDuplicates(df):\n    return df[df['is_duplicate'] == 1]\n\ndef getDuplicatesPercentage(df):\n    duplicateRows = dash.flow(getDuplicates, getNumberOfRows)(df)\n    totalRows = getNumberOfRows(df)\n    return round(duplicateRows*100/totalRows,2)\n\ndef getQuestionsIds(df):\n    return df['qid1'].tolist() + df['qid2'].tolist()\n\ndef getTotalNumberOfQuestions(df):\n    return dash.flow(getQuestionsIds, len)(df)\n\ndef getUniqueQuestionsIds(df):\n    return dash.flow(getQuestionsIds, set)(df)\n\ndef getTotalNumberOfUniqueQUestions(df):\n    return dash.flow(getUniqueQuestionsIds, len)(df)\n\ndef getRepeatedQuestionsIds(df):\n    serie = dash.flow(getQuestionsIds, pd.Series)(df)\n    counts = serie.value_counts()\n    repeated = counts[counts > 1]\n    return list(set(repeated.index.tolist()))\n\ndef getNumberOfRepeatedQuestionsIds(df):\n    return dash.flow(getRepeatedQuestionsIds, len)(df)\n\ndef getPercentageOfRepeatedQuestions(df):\n    return round(getNumberOfRepeatedQuestionsIds(df)/getTotalNumberOfUniqueQUestions(df),2)*100","execution_count":4,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d08bed2af1068337913e2cb0836bca1bf45d7102"},"cell_type":"code","source":"def printReport(df):\n    print(\"Total Number of Questions Pairs: {:,}\".format(getNumberOfRows(df)))\n    print(\"Total Number of Questions: {:,}\".format(getTotalNumberOfQuestions(df)))\n    print(\"Total Number of Unique Questions: {:,}\". format(getTotalNumberOfUniqueQUestions(df)))\n    print(\"Total Number of Repeated Questions: {:,}\".format(getNumberOfRepeatedQuestionsIds(df)))\n    print(\"Percentage of Repeated Questions: {:,}%\".format(getPercentageOfRepeatedQuestions(df)))\n    print('\\n')\n\nprintReport(train)","execution_count":8,"outputs":[]},{"metadata":{"_cell_guid":"ae58aa30-4b12-4fc6-b96e-5b7357286893","_uuid":"1d7c5b3a871fd2e9b4ef4320dd2232f187efbb38"},"cell_type":"markdown","source":"## Number of duplicated pairs\nThe percentage of datapoints that are considered duplicated is 36.9%"},{"metadata":{"_cell_guid":"755415af-ebd7-4f2f-8b2f-b01fabecd771","_uuid":"6178dd858a9d005ebc915d2b4e7f2be402e40a92","trusted":true},"cell_type":"code","source":"numberOfQuestions = train.size\nnumberOfDuplicatedPairs = train[train['is_duplicate'] == 1].size\npercentageDuplicatedPairs = numberOfDuplicatedPairs*100/numberOfQuestions\n# print('Total number of question pairs for training: {}'.format(len(df_train)))\n# print('Duplicate pairs: {}%'.format(round(df_train['is_duplicate'].mean()*100, 2)))\n# qids = pd.Series(df_train['qid1'].tolist() + df_train['qid2'].tolist())\n# print('Total number of questions in the training data: {}'.format(len(\n#     np.unique(qids))))\n# print('Number of questions that appear multiple times: {}'.format(np.sum(qids.value_counts() > 1)))\nprint('The size of the training set is: {}'.format(numberOfQuestions))\nprint('Total number of duplicated pairs: {}'.format(numberOfDuplicatedPairs))\nprint('Percentage Duplicated pairs: {}'.format(percentageDuplicatedPairs))","execution_count":9,"outputs":[]},{"metadata":{"_cell_guid":"33275ec6-54ae-4f17-83d3-598f859ef7df","_uuid":"f45cbeb1cb04a625ae12b3db5ed55720d45c7967","trusted":true},"cell_type":"code","source":"count_duplicate = train['is_duplicate'].value_counts()\nplt.bar([0, 1], count_duplicate.values)\nplt.ylabel('Number of pairs')\nplt.xlabel('Duplicate', fontsize=12)\nplt.title('Balance of classes in training set')\nplt.show()\n\n","execution_count":25,"outputs":[]},{"metadata":{"_cell_guid":"3eec752a-3f7c-4ee4-b23f-942199467706","_uuid":"ef7591b7e332057c273429993eabc562a8a089ab"},"cell_type":"markdown","source":"### Quantiles size for Training Data, number of words\n\nGiven that to feed the model the size of the input has to be standard, it's required to check the size of the questions.\n"},{"metadata":{"_cell_guid":"7923ab9b-0462-4042-b984-a593000733ba","_uuid":"08946ac6ff5b12a4510b8f98da5a828376a344be","trusted":true},"cell_type":"code","source":"questions_train = pd.concat([train['question1'], train['question2']])\nq_train_len = questions_train.apply(lambda x: len(str(x).split(' ')))\nquantiles_train = q_train_len.quantile([0.25, 0.5, 0.75, 0.99])\nprint(\"Quantiles Train: \")\nprint(quantiles_train)","execution_count":27,"outputs":[]},{"metadata":{"_cell_guid":"f7c02e66-ff7c-477d-a6d5-27ae8d59e677","_uuid":"2e50572f8f398a27dfde959a2590122b02da7c9f"},"cell_type":"markdown","source":"We can see that the mean of words per question is 10 and that the 99 percent of the words are covered by a length of 31 words, so using 32 words as input lenght for questions is good enought."},{"metadata":{"_cell_guid":"e99b6248-c24d-4a12-9624-8e0dfb7fdb52","_uuid":"07ec920db92cc6b8308902a2a8baa303c7b1ca68","trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10, 5))\nplt.hist(q_train_len, bins=100, range=[0,300])\nplt.title('Length Training set')\nplt.xlabel('Number of characters per question')\nplt.ylabel('Number of questions')\nplt.yscale('log', nonposy='clip')","execution_count":28,"outputs":[]},{"metadata":{"_cell_guid":"f93e54e8-c7a6-4910-8303-7b66ae520b44","_uuid":"a031c63e2f591e11229290eaed136d2164dec800","collapsed":true},"cell_type":"markdown","source":"## Testing Data Set"},{"metadata":{"_cell_guid":"04bfc318-775f-4447-b954-f9a119447b25","_uuid":"24b7f4ae5f088d78fba5aea7dc454636a2177c68","trusted":true},"cell_type":"code","source":"test  =  pd.read_csv('../input/test.csv')\ntest.head(10)","execution_count":39,"outputs":[]},{"metadata":{"_cell_guid":"b65c98cb-ebee-4479-9a67-71cf1b5fca6d","_uuid":"6691df314ec20f7b3d96a991b6152506f0349a90"},"cell_type":"markdown","source":"### Quantiles for words\n\nFor the test dataset 32 is still a good upper boundary for the number of words"},{"metadata":{"_cell_guid":"2645759f-8336-49b4-a5eb-e324cee6df0d","_uuid":"bcb03c211231877321d5e9684960a2808a397a14","trusted":true},"cell_type":"code","source":"questions_test = pd.concat([test['question1'], test['question2']])\nq_test_len = questions_test.apply(lambda x: len(str(x).split(' ')))\nquantiles_test = q_test_len.quantile([0.25, 0.5, 0.75, 0.99])\nprint(\"Quantiles test: \")\nprint(quantiles_test)","execution_count":30,"outputs":[]},{"metadata":{"_cell_guid":"837a12b0-dbc9-4586-ad38-f9bce8f4c2fe","_uuid":"488aa4e73daac8a206328daaf96ad60ca7bff8fa","trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10, 5))\nplt.hist(q_test_len, bins=100, range=[0,300])\nplt.title('Length Training set')\nplt.xlabel('Number of characters per question')\nplt.ylabel('Number of questions')\nplt.yscale('log', nonposy='clip')","execution_count":31,"outputs":[]},{"metadata":{"_uuid":"8ab1b2a8920f012260006a45d5029a34d6e560dc"},"cell_type":"markdown","source":"# BenchMarks"},{"metadata":{"_uuid":"636ff309d90414e58ba8e65f5a349e8e7dcc0d72"},"cell_type":"markdown","source":"### Statistics for pairs with repeated questions.\n\nHere we show that for the next mutually exclusive sets, the percentage of duplicated pairs might vary\n\n- No question is repeated: 22.4%\n- One question is repeated: 15.71%\n- The two questions are repeated: 71.6%\n\nThis is a bias given in the way the data is collected, but still we can use this percentages in our testing set, if the distribution is similar, to get a good score."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"aef1c3143d016155a3959e59f7e9e6c927fbada1"},"cell_type":"code","source":"\ndef getRowsWithNoRepeatedQuestion(df):\n    nonRepeatedQeuestions = set(getUniqueQuestionsIds(df)) - set(getRepeatedQuestionsIds(df))\n    return df[ (df['qid1'].isin(nonRepeatedQeuestions) & df['qid2'].isin(nonRepeatedQeuestions)) ]\n\ndef getRowsWithAtLeastOneRepeatedQuestion(df):\n    repeatedQuestionsIds = getRepeatedQuestionsIds(df)\n    nonRepeatedQeuestions = set(getUniqueQuestionsIds(df)) - set(getRepeatedQuestionsIds(df))\n    onlyQ1IsRepeated = df['qid1'].isin(repeatedQuestionsIds) & df['qid2'].isin(nonRepeatedQeuestions)\n    onlyQ2IsRepeated = df['qid2'].isin(repeatedQuestionsIds) & df['qid1'].isin(nonRepeatedQeuestions)\n\n    return df[ onlyQ1IsRepeated | onlyQ2IsRepeated]\n\ndef getRowsWithTwoRepeatedQuestions(df):\n    repeatedQuestionsIds = getRepeatedQuestionsIds(df)\n    return df[ (df['qid1'].isin(repeatedQuestionsIds) & df['qid2'].isin(repeatedQuestionsIds)) ]\n\n    ","execution_count":33,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"18540776ef5c5d2772e3fde60185fb2299cc6e83"},"cell_type":"code","source":"def printDuplicatePairsReport(df):\n    print(\"Total Number of Questions Pairs: {:,}\".format(getNumberOfRows(df)))\n    print(\"Percentage of pairs Marked as duplicate: {:,}%\\n\".format(getDuplicatesPercentage(df)))\n\ndef printStatisticsForPairsWithRepeatedQuestions(df):\n    print(\"## Statistics for rows with no repeated questions ##\")\n    dash.flow(getRowsWithNoRepeatedQuestion, printDuplicatePairsReport)(df)\n    print(\"## Statistics for rows with one repeated question ##\")\n    dash.flow(getRowsWithAtLeastOneRepeatedQuestion, printDuplicatePairsReport)(df)\n    print(\"## Statistics for rows with two repeated questions ##\")\n    dash.flow(getRowsWithTwoRepeatedQuestions, printDuplicatePairsReport)(df)\n\nprintStatisticsForPairsWithRepeatedQuestions(train)","execution_count":34,"outputs":[]},{"metadata":{"_uuid":"1c489ecfd42536c12acd5626c861e9c9a797945f"},"cell_type":"markdown","source":"## base score\n\nA good base score is to use These precentages for each group in the test case in order to get a base score.\nto do this we need to:\n - Read the test file.\n - Create ids for each question.\n - generate a df similar to the train ds.\n - Assign the score according to the group the row belongs (no repeated question, one repeated questions, two repeated questions)\n - create the score to upload it."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"ad88ce80d550f9699e69f6d2185f1e66c57b67f5"},"cell_type":"code","source":"#Read the file\n#test = pd.read_csv('../input/test.csv', nrows= 10000)\ntest = pd.read_csv('../input/train.csv', nrows = 100)\n\nduplicate_score = test['is_duplicate'].mean()","execution_count":35,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f225ac610725f67d6047edd76ea9d4ec33ed3dac"},"cell_type":"code","source":"def classifyByRepetition(row, repeatedQuestions):\n    q1IsRepeated = row['question1'] in repeatedQuestions\n    q2IsRepeated = row['question2'] in repeatedQuestions\n    \n    if not q1IsRepeated and not q2IsRepeated:\n        return 0.224\n    elif q1IsRepeated and q2IsRepeated:\n        return 0.0716\n    else:\n        return 0.1571\n\ndef getRepeatedQuestions(df):\n    return set(df['question1'].append(df['question2'], ignore_index = True)\n        .value_counts()\n        .where(lambda x: x>1)\n        .dropna()\n        .index\n        .get_values())\n    \ndef addIsDuplicatedColum(df):\n    getIsDuplicateValue = lambda row: classifyByRepetition(row, getRepeatedQuestions(df))\n    df['prediction_is_duplicate'] =  df.apply(getIsDuplicateValue, axis=1 )\n    return df\n\ndef calculate_and_print_benchmark_based_on_duplicate_percentage(df):\n    print('Binary Cross-Entropy Score based on duplicates:', log_loss(df['is_duplicate'], np.zeros_like(df['is_duplicate']) + duplicate_score)) \n\ndef calculate_and_print_benchmark_based_on_repetition(df):\n    print('Binary Cross-Entropy Score based on questions repetition:', log_loss(df['is_duplicate'], df['prediction_is_duplicate']))\n    \ndef mainAddIsDuplicatedColumnAndTransformToCsv(df):\n    df = addIsDuplicatedColum(df)\n    calculate_and_print_benchmark_based_on_duplicate_percentage(df)\n    calculate_and_print_benchmark_based_on_repetition(df)\n    #df.to_csv('base_submission.csv', index = False, columns=['test_id', 'is_duplicate'])\n\nmainAddIsDuplicatedColumnAndTransformToCsv(test)\n","execution_count":38,"outputs":[]},{"metadata":{"_cell_guid":"76f93eb4-5d8b-4def-85db-e9af366b80cf","_uuid":"2a7e5610b631347b12b3b45fc049177db5d587a3"},"cell_type":"markdown","source":"### Counting Open and yes/no questions\n\nI would like to know how many questions classified as yes/no questions are in the dataset.\nThis might help to see if yes/no questions are easier to tackle than open questions."},{"metadata":{"_cell_guid":"7cb3df5c-d178-48d6-b17c-b933e9421daf","_uuid":"de46a5d09e26cb92318c257dee16f486ec433a6a","trusted":true,"collapsed":true},"cell_type":"code","source":"\n\ndef isOpenQuestion(q):\n    yesNoQuestionsInitializers = ['is', 'are', 'should', 'do', 'does', 'can']\n    openQuestionInitializer = ['what', 'how', 'why', 'who', 'when', 'where', 'which', \"what's\", \"how's\", \"why's\", \"who's\", \"when's\", \"where's\"]\n    isOpen = any( str(q).lower().startswith(i) for i in openQuestionInitializer)\n    isYesNo = any( str(q).lower().startswith(i) for i in yesNoQuestionsInitializers)\n    if isOpen:\n        return 0\n    elif isYesNo:\n        return 1\n    else:\n        return 2\n\ntrain['Q1TypeOfQuestion'] = train['question1'].apply(isOpenQuestion)\ntrain['Q2TypeOfQuestion'] = train['question2'].apply(isOpenQuestion)","execution_count":16,"outputs":[]},{"metadata":{"_cell_guid":"e2dab02e-f447-491a-8889-b436a599007e","_uuid":"37b56cb08cce023d9610aa13918d18bc4909b323","trusted":true},"cell_type":"code","source":"numberOfYesNoQuestions = train[train['Q1TypeOfQuestion'] == 1].shape[0] + train[train['Q2TypeOfQuestion'] == 1].shape[0]\nprint(\"Number of yes/no questions: \", numberOfYesNoQuestions)","execution_count":19,"outputs":[]},{"metadata":{"_cell_guid":"984d3af8-eda3-4cfd-b22a-11476114243e","_uuid":"7f27098d0e1e72a5f8b2686cc8b087bcaf7d7b8d","trusted":true},"cell_type":"code","source":"numberOfOpenQuestions = train[train['Q1TypeOfQuestion'] == 0].shape[0] + train[train['Q2TypeOfQuestion'] == 0].shape[0]\nnumberOfOpenQuestions\nprint(\"Number of Open questions: \", numberOfOpenQuestions)","execution_count":20,"outputs":[]},{"metadata":{"_cell_guid":"be025cdd-94f0-4884-89a4-2a2f1401ce60","_uuid":"63ae7f11d26d019313fe5f530bd60a7cedd32f71","trusted":true},"cell_type":"code","source":"numberOfNonClassifiedQuestions = train[train['Q1TypeOfQuestion'] == 2].shape[0] + train[train['Q2TypeOfQuestion'] == 2].shape[0]\nnumberOfNonClassifiedQuestions\nprint(\"Number of no classified questions: \", numberOfNonClassifiedQuestions)","execution_count":21,"outputs":[]},{"metadata":{"_cell_guid":"00f3e89a-d209-4b3a-b561-d1fbb916a408","_uuid":"36ac76a6f610d1cedfd33dbf26c0d81ac2a0f4d2"},"cell_type":"markdown","source":"From this numbers we can conclude that the number of open questions in the dataset bigger that the number of yes no questions, even if all the classified questions where yes/no questions, the number of open questions is way bigger"},{"metadata":{"_cell_guid":"e79e3537-88d9-4bfd-a711-fbc3dabf3693","_uuid":"ca75beefcca06bbd033d9b6c0b3c0cc862b1ced7"},"cell_type":"markdown","source":"### Creation of a correlation matrix based on the number of duplicates given the kind of question"},{"metadata":{"_cell_guid":"b4fc1f53-2ac6-44a5-aaf6-b5c4ed4cdc04","_uuid":"b4cd5e029b905d366218fc0f3fc67e80ee8ffb22","trusted":true},"cell_type":"code","source":"corrMat = [[0,0,0],[0,0,0],[0,0,0]]\nfor i in range(0,3):\n    for j in range(0,3):\n        corrMat[i][j] = train[(train['Q1TypeOfQuestion'] == i) & (train['Q2TypeOfQuestion'] == j)].is_duplicate.mean()\ncorrMat = np.array(corrMat)\nsns.heatmap(corrMat, vmax=0.5, square=True, annot=True)","execution_count":22,"outputs":[]},{"metadata":{"_cell_guid":"92e65b1a-a704-4954-98b6-880e2effe573","_uuid":"7fa800cdda3867f687e0ec50bf238caaf258fbed"},"cell_type":"markdown","source":"We can see from this correlation map that if the questions are the same type of questions the correlation is higher than in other cases."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}