{"nbformat_minor": 2, "nbformat": 4, "metadata": {"language_info": {"pygments_lexer": "ipython3", "nbconvert_exporter": "python", "name": "python", "codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "version": "3.6.1"}, "kernelspec": {"language": "python", "name": "python3", "display_name": "Python 3"}}, "cells": [{"metadata": {"_cell_guid": "d1e11764-8d67-4a6e-841c-4d7dffaf9ae1", "_execution_state": "idle", "_uuid": "abb808962ca9f6701560fac69ff96860a95f7524", "collapsed": false}, "source": "This kernel uses MXNet and XGBoost to achieve a 0.395 score.  The results of the MXNet prediction are used as a feature in the final XGBoost classifier.  See if you can do better !!  Big thanks to the other kernel submitters (@beluga,  for the feature engineering tips.  Switch MXNet context to train on GPU. ", "cell_type": "markdown"}, {"outputs": [], "metadata": {"trusted": false, "_cell_guid": "c724cdaf-ef6f-4f93-8561-62e7ae0530a4", "_uuid": "142a8d16792c96eab686b2b9c8472d60cca4cb89", "collapsed": true}, "execution_count": null, "cell_type": "code", "source": "import pandas as pd\nimport numpy as np\nimport mxnet as mx\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom __future__ import print_function\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nimport xgboost as xgb\nimport scipy as sc\n\nN = 100000 # number of sample rows in plots\nnp.random.seed(1337)"}, {"outputs": [], "metadata": {"trusted": false, "_uuid": "2545ba529a8825318bcc925bdda85aafd0d118d6", "_cell_guid": "024d3f82-5169-436c-a854-658474eb6496"}, "execution_count": null, "cell_type": "code", "source": "test = pd.read_csv('test.csv', parse_dates=['pickup_datetime'])\ntrain = pd.read_csv('train.csv', parse_dates=['pickup_datetime', 'dropoff_datetime'])\n\nprint('Test shape : {}'.format(test.shape))\nprint('Train shape : {}'.format(train.shape))\n"}, {"outputs": [], "metadata": {"trusted": false, "_cell_guid": "93aa43ea-002d-41c7-87a8-9ec2ec34196b", "_uuid": "2116112c631e36dffd9d6bb80c62c10d4c748359", "collapsed": true}, "execution_count": null, "cell_type": "code", "source": "def haversine_array(lat1, lng1, lat2, lng2):\n    lat1, lng1, lat2, lng2 = map(np.radians, (lat1, lng1, lat2, lng2))\n    AVG_EARTH_RADIUS = 6371  # in km\n    # calculate haversine\n    lat = lat2 - lat1\n    lng = lng2 - lng1\n    d = np.sin(lat * 0.5) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(lng * 0.5) ** 2\n    h = 2 * AVG_EARTH_RADIUS * np.arcsin(np.sqrt(d))\n    return h\n\ndef dummy_manhattan_distance(lat1, lng1, lat2, lng2):\n    a = haversine_array(lat1, lng1, lat1, lng2)\n    b = haversine_array(lat1, lng1, lat2, lng1)\n    return a + b\n\ndef bearing_array(lat1, lng1, lat2, lng2):\n    AVG_EARTH_RADIUS = 6371  # in km\n    lng_delta_rad = np.radians(lng2 - lng1)\n    lat1, lng1, lat2, lng2 = map(np.radians, (lat1, lng1, lat2, lng2))\n    y = np.sin(lng_delta_rad) * np.cos(lat2)\n    x = np.cos(lat1) * np.sin(lat2) - np.sin(lat1) * np.cos(lat2) * np.cos(lng_delta_rad)\n    return np.degrees(np.arctan2(y, x))"}, {"outputs": [], "metadata": {"trusted": false, "_cell_guid": "f1a58605-034c-4227-8c86-1ed71c308ab0", "_uuid": "85a2c5e4ed44e317e7f48c63061692b021b22da7", "collapsed": true}, "execution_count": null, "cell_type": "code", "source": "## Fix up store and fwd flag\ntrain['store_and_fwd_flag'] = 1 * (train.store_and_fwd_flag.values == 'Y')\ntest['store_and_fwd_flag'] = 1 * (test.store_and_fwd_flag.values == 'Y')\n\n## PCA \nfull = pd.concat([train, test])\ncoords = np.vstack((full[['pickup_latitude', 'pickup_longitude']], \n                   full[['dropoff_latitude', 'dropoff_longitude']]))\n\npca = PCA().fit(coords)\ntrain['pickup_pca0'] = pca.transform(train[['pickup_latitude', 'pickup_longitude']])[:, 0]\ntrain['pickup_pca1'] = pca.transform(train[['pickup_latitude', 'pickup_longitude']])[:, 1]\ntrain['dropoff_pca0'] = pca.transform(train[['dropoff_latitude', 'dropoff_longitude']])[:, 0]\ntrain['dropoff_pca1'] = pca.transform(train[['dropoff_latitude', 'dropoff_longitude']])[:, 1]\ntest['pickup_pca0'] = pca.transform(test[['pickup_latitude', 'pickup_longitude']])[:, 0]\ntest['pickup_pca1'] = pca.transform(test[['pickup_latitude', 'pickup_longitude']])[:, 1]\ntest['dropoff_pca0'] = pca.transform(test[['dropoff_latitude', 'dropoff_longitude']])[:, 0]\ntest['dropoff_pca1'] = pca.transform(test[['dropoff_latitude', 'dropoff_longitude']])[:, 1]"}, {"outputs": [], "metadata": {"trusted": false, "_cell_guid": "8ab7b02b-c4ed-42ca-a795-3669fcc05cdc", "_uuid": "2607fd40f6de44dbfea423993a51c6efcb961115", "collapsed": true}, "execution_count": null, "cell_type": "code", "source": "## Distance feature calculations\n\ntrain.loc[:, 'distance_haversine'] = haversine_array(train['pickup_latitude'].values, train['pickup_longitude'].values, train['dropoff_latitude'].values, train['dropoff_longitude'].values)\ntrain.loc[:, 'distance_dummy_manhattan'] = dummy_manhattan_distance(train['pickup_latitude'].values, train['pickup_longitude'].values, train['dropoff_latitude'].values, train['dropoff_longitude'].values)\ntrain.loc[:, 'direction'] = bearing_array(train['pickup_latitude'].values, train['pickup_longitude'].values, train['dropoff_latitude'].values, train['dropoff_longitude'].values)\ntrain.loc[:, 'pca_manhattan'] = np.abs(train['dropoff_pca1'] - train['pickup_pca1']) + np.abs(train['dropoff_pca0'] - train['pickup_pca0'])\n\ntest.loc[:, 'distance_haversine'] = haversine_array(test['pickup_latitude'].values, test['pickup_longitude'].values, test['dropoff_latitude'].values, test['dropoff_longitude'].values)\ntest.loc[:, 'distance_dummy_manhattan'] = dummy_manhattan_distance(test['pickup_latitude'].values, test['pickup_longitude'].values, test['dropoff_latitude'].values, test['dropoff_longitude'].values)\ntest.loc[:, 'direction'] = bearing_array(test['pickup_latitude'].values, test['pickup_longitude'].values, test['dropoff_latitude'].values, test['dropoff_longitude'].values)\ntest.loc[:, 'pca_manhattan'] = np.abs(test['dropoff_pca1'] - test['pickup_pca1']) + np.abs(test['dropoff_pca0'] - test['pickup_pca0'])\n\ntrain.loc[:, 'center_latitude'] = (train['pickup_latitude'].values + train['dropoff_latitude'].values) / 2\ntrain.loc[:, 'center_longitude'] = (train['pickup_longitude'].values + train['dropoff_longitude'].values) / 2\ntest.loc[:, 'center_latitude'] = (test['pickup_latitude'].values + test['dropoff_latitude'].values) / 2\ntest.loc[:, 'center_longitude'] = (test['pickup_longitude'].values + test['dropoff_longitude'].values) / 2\n\n\n\n\n\n"}, {"outputs": [], "metadata": {"trusted": false, "_cell_guid": "8669fa3f-42d4-4968-9456-2dae1aec07ef", "_uuid": "021b4195ee297176112083a8776cf828035753a1", "collapsed": true}, "execution_count": null, "cell_type": "code", "source": "## Temporal features\n\ntrain.loc[:, 'pickup_weekday'] = train['pickup_datetime'].dt.weekday\ntrain.loc[:, 'pickup_hour_weekofyear'] = train['pickup_datetime'].dt.weekofyear\ntrain.loc[:, 'pickup_hour'] = train['pickup_datetime'].dt.hour\ntrain.loc[:, 'pickup_minute'] = train['pickup_datetime'].dt.minute\ntrain.loc[:, 'pickup_dt'] = (train['pickup_datetime'] - train['pickup_datetime'].min()).dt.total_seconds()\ntrain.loc[:, 'pickup_week_hour'] = train['pickup_weekday'] * 24 + train['pickup_hour']\n\ntrain.loc[:, 'pickup_date'] = train['pickup_datetime'].dt.date\ntest.loc[:, 'pickup_date'] = test['pickup_datetime'].dt.date\n\ntest.loc[:, 'pickup_weekday'] = test['pickup_datetime'].dt.weekday\ntest.loc[:, 'pickup_hour_weekofyear'] = test['pickup_datetime'].dt.weekofyear\ntest.loc[:, 'pickup_hour'] = test['pickup_datetime'].dt.hour\ntest.loc[:, 'pickup_minute'] = test['pickup_datetime'].dt.minute\ntest.loc[:, 'pickup_dt'] = (test['pickup_datetime'] - train['pickup_datetime'].min()).dt.total_seconds()\ntest.loc[:, 'pickup_week_hour'] = test['pickup_weekday'] * 24 + test['pickup_hour']"}, {"outputs": [], "metadata": {"trusted": false, "_cell_guid": "e9221596-4000-4d90-a7e6-2533dc207624", "_uuid": "34eed2988adc0cbc7758b6cd6dfa8733b6f060fb", "collapsed": true}, "execution_count": null, "cell_type": "code", "source": "## Speed Features\n\ntrain.loc[:, 'avg_speed_h'] = 1000 * train['distance_haversine'] / train['trip_duration']\n# train.loc[:, 'average_speed_v'] = 1000 * train['distance_vincenty'] / train[trip_duration]\n# train.loc[:, 'average_speed_gc'] = 1000 * train['distance_great_circle'] / train[trip_duration]\ntrain.loc[:, 'avg_speed_m'] = 1000 * train['distance_dummy_manhattan'] / train['trip_duration']"}, {"outputs": [], "metadata": {"trusted": false, "_uuid": "1cd09b380b309681e35d7c028a73ce81205bfd76", "_cell_guid": "5b656750-dfe8-4a75-9266-60bf30fb5fcf"}, "execution_count": null, "cell_type": "code", "source": "## Binning\n\ntrain.loc[:, 'pickup_lat_bin'] = np.round(train['pickup_latitude'], 3)\ntrain.loc[:, 'pickup_long_bin'] = np.round(train['pickup_longitude'], 3)\n# Average speed for regions\ngby_cols = ['pickup_lat_bin', 'pickup_long_bin']\ncoord_speed = train.groupby(gby_cols).mean()[['avg_speed_h']].reset_index()\ncoord_count = train.groupby(gby_cols).count()[['id']].reset_index()\ncoord_stats = pd.merge(coord_speed, coord_count, on=gby_cols)\ncoord_stats = coord_stats[coord_stats['id'] > 100]\nfig, ax = plt.subplots(ncols=1, nrows=1)\nax.scatter(train.pickup_longitude.values[:N], train.pickup_latitude.values[:N], color='black', s=0.02, alpha=0.05)\nax.scatter(coord_stats.pickup_long_bin.values, coord_stats.pickup_lat_bin.values, c=coord_stats.avg_speed_h.values,\n           cmap='RdYlGn', s=10, alpha=0.3, vmin=0, vmax=8)\nax.set_xlim(-74.03, -73.77)\nax.set_ylim(40.63, 40.85)\nax.set_xlabel('Longitude')\nax.set_ylabel('Latitude')\nfig.savefig('coords.png', figsize=(16, 10), dpi=300)\nplt.title('Average speed')\nplt.show()\n\ntrain.loc[:, 'pickup_lat_bin'] = np.round(train['pickup_latitude'], 2)\ntrain.loc[:, 'pickup_long_bin'] = np.round(train['pickup_longitude'], 2)\ntrain.loc[:, 'center_lat_bin'] = np.round(train['center_latitude'], 2)\ntrain.loc[:, 'center_long_bin'] = np.round(train['center_longitude'], 2)\ntrain.loc[:, 'pickup_dt_bin'] = (train['pickup_dt'] // (3 * 3600))\ntest.loc[:, 'pickup_lat_bin'] = np.round(test['pickup_latitude'], 2)\ntest.loc[:, 'pickup_long_bin'] = np.round(test['pickup_longitude'], 2)\ntest.loc[:, 'center_lat_bin'] = np.round(test['center_latitude'], 2)\ntest.loc[:, 'center_long_bin'] = np.round(test['center_longitude'], 2)\ntest.loc[:, 'pickup_dt_bin'] = (test['pickup_dt'] // (3 * 3600))"}, {"outputs": [], "metadata": {"trusted": false, "_uuid": "13cd36bd1a3c1515ad535b7cb4bcc9eded463f30", "_cell_guid": "5fdd3912-c053-4258-bae9-2ca6c7a3c1ba"}, "execution_count": null, "cell_type": "code", "source": "train['log_trip_duration'] = np.log(train['trip_duration'].values + 1)\n\nfor gby_col in ['pickup_hour', 'pickup_date', 'pickup_weekday', 'pickup_dt_bin',\n               'pickup_week_hour']:\n    gby = train.groupby(gby_col).mean()[['avg_speed_h', 'avg_speed_m', 'log_trip_duration']]\n    gby.columns = ['%s_gby_%s' % (col, gby_col) for col in gby.columns]\n    train = pd.merge(train, gby, how='left', left_on=gby_col, right_index=True)\n    test = pd.merge(test, gby, how='left', left_on=gby_col, right_index=True)\n\nfor gby_cols in [['center_lat_bin', 'center_long_bin'],\n                 ['pickup_hour', 'center_lat_bin', 'center_long_bin']] :\n    coord_speed = train.groupby(gby_cols).mean()[['avg_speed_h']].reset_index()\n    coord_count = train.groupby(gby_cols).count()[['id']].reset_index()\n    coord_stats = pd.merge(coord_speed, coord_count, on=gby_cols)\n    coord_stats = coord_stats[coord_stats['id'] > 100]\n    coord_stats.columns = gby_cols + ['avg_speed_h_%s' % '_'.join(gby_cols), 'cnt_%s' %  '_'.join(gby_cols)]\n    train = pd.merge(train, coord_stats, how='left', on=gby_cols)\n    test = pd.merge(test, coord_stats, how='left', on=gby_cols)"}, {"outputs": [], "metadata": {"trusted": false, "_uuid": "970e6dcd5652ad35c23081e8117fabff7a3e3bad", "_cell_guid": "1b698adc-8ff2-4776-95d0-b8293990e33b"}, "execution_count": null, "cell_type": "code", "source": "feature_names = list(train.columns)\ndo_not_use_for_training = ['id', 'log_trip_duration', 'pickup_datetime', 'dropoff_datetime', 'trip_duration', 'check_trip_duration',\n                           'pickup_date', 'avg_speed_h', 'avg_speed_m', 'pickup_lat_bin', 'pickup_long_bin',\n                           'center_lat_bin', 'center_long_bin', 'pickup_dt_bin']\nfeature_names = [f for f in train.columns if f not in do_not_use_for_training]\nprint(feature_names)\nprint('We have %i features.' % len(feature_names))\ntrain[feature_names].count()"}, {"outputs": [], "metadata": {"trusted": false, "_uuid": "7cdf7893fc3cc13a8fcc53612e6b460a7f6c8118", "_cell_guid": "7b37c082-83c2-4e34-bf9b-ecc24e9f2c1e"}, "execution_count": null, "cell_type": "code", "source": "y = np.log(train['trip_duration'].values + 1)\n\ntrain[feature_names] = train[feature_names].fillna(0)\ntest[feature_names] = test[feature_names].fillna(0)\nscaler = StandardScaler()\nscaled_X = scaler.fit_transform(train[feature_names].values)\nXtr, Xv, ytr, yv = train_test_split(scaled_X, y, test_size=0.2, random_state=1982)"}, {"outputs": [], "metadata": {"trusted": false, "_uuid": "e1455ab58aecfa6aca3b9250bceb8c256aca601b", "_cell_guid": "b411cbdf-0ce5-4bc2-a241-724940ad7b37"}, "execution_count": null, "cell_type": "code", "source": "batch_size=2000\n\ntrain_iter = mx.io.NDArrayIter(Xtr, ytr, batch_size, shuffle=True, label_name='lin_reg_label')\nval_iter = mx.io.NDArrayIter(Xv, yv, batch_size, shuffle=False)"}, {"outputs": [], "metadata": {"trusted": false, "_uuid": "5b43a949716a8974726cd080799a0da2ff266dae", "_cell_guid": "d92e9193-b6be-40a7-8a40-81d63f006377"}, "execution_count": null, "cell_type": "code", "source": "print(\"Building model...\")\nnet = mx.sym.Variable('data')\nY = mx.symbol.Variable('lin_reg_label')\nnet = mx.sym.FullyConnected(net, name='fc1', num_hidden=1024)\nnet = mx.sym.Activation(net, name='relu1', act_type=\"relu\")\nnet = mx.sym.Dropout(net, p=0.1)\nnet = mx.sym.FullyConnected(net, name='fc2', num_hidden=256)\nnet = mx.sym.Activation(net, name='relu2', act_type=\"relu\")\nnet = mx.sym.Dropout(net, p=0.1)\nnet = mx.sym.FullyConnected(net, name='fc3', num_hidden=1024)\nnet = mx.sym.Activation(net, name='relu3', act_type=\"relu\")\nnet = mx.sym.FullyConnected(net, name='fc4', num_hidden=1)\nnet = mx.sym.LinearRegressionOutput(net, label=Y, name='lro')\n\nmx.viz.plot_network(net)"}, {"outputs": [], "metadata": {"trusted": false, "_uuid": "7776da64a3359b5f4131f5cb726a4b43e881d3db", "_cell_guid": "4a90016f-9bc1-4f61-818e-8357a7d615bd"}, "execution_count": null, "cell_type": "code", "source": "print(\"Training model\")\nnum_epoch = 400\nlearning_rate = 0.25\nmomentum = 0.002\n\nimport logging\nlogging.getLogger().setLevel(logging.DEBUG)\n\nmod = mx.mod.Module(symbol=net, label_names=['lin_reg_label'], context=mx.cpu())\nmod.fit(\n    train_iter,\n    eval_data=val_iter,\n    optimizer='nag',\n    optimizer_params={'learning_rate':learning_rate, 'momentum':momentum},\n    eval_metric='rmse',\n    batch_end_callback = mx.callback.Speedometer(batch_size, 500), \n    num_epoch=num_epoch\n)\n\nmod.score(val_iter, ['rmse', 'acc'])"}, {"outputs": [], "metadata": {"trusted": false, "_uuid": "d9dc6f2e0c6602c5766787090dedcd82465113f1", "_cell_guid": "9e6ba5b1-1018-4711-892d-13d021350e92"}, "execution_count": null, "cell_type": "code", "source": "## Add in NN-predicted feature and retrain using XGB\nscaled_nn_train = scaler.fit_transform(train[feature_names].values)\nscaled_nn_test = scaler.fit_transform(test[feature_names].values)\n\nnn_train_iter = mx.io.NDArrayIter(scaled_nn_train, None, batch_size)\nnn_test_iter = mx.io.NDArrayIter(scaled_nn_test, None, batch_size)\n\ntrain['nn_preds'] = np.exp(mod.predict(nn_train_iter).asnumpy())\ntest['nn_preds'] = np.exp(mod.predict(nn_test_iter).asnumpy())\n\nfeature_names.append('nn_preds')\n"}, {"outputs": [], "metadata": {"trusted": false, "_uuid": "1a02f96981759f06c96e419e89a7164c9b0c0fcc", "_cell_guid": "91c5fca3-6d82-4b8c-b078-94553354a8f1"}, "execution_count": null, "cell_type": "code", "source": "Xtr, Xv, ytr, yv = train_test_split(train[feature_names].values, y, test_size=0.2, random_state=1987)\ndtrain = xgb.DMatrix(Xtr, label=ytr)\ndvalid = xgb.DMatrix(Xv, label=yv)\ndtest = xgb.DMatrix(test[feature_names].values)\nwatchlist = [(dtrain, 'train'), (dvalid, 'valid')]\n\n# Try different parameters\nxgb_pars = {'min_child_weight': 150, 'learning_rate': 0.1, 'eta': 0.3, 'colsample_bytree': 0.7, 'max_depth': 14,\n            'subsample': 0.8, 'lambda': 1., 'nthread': 4, 'booster' : 'gbtree', 'silent': 1,\n            'eval_metric': 'rmse', 'objective': 'reg:linear'}"}, {"outputs": [], "metadata": {"trusted": false, "_uuid": "8f65c0dedbd850b993159f891c4d558705de5455", "_cell_guid": "cc074110-1db6-452a-9be4-f5d9e15ed0b0"}, "execution_count": null, "cell_type": "code", "source": "# You could try to train with more epoch\nmodel = xgb.train(xgb_pars, dtrain, 100, watchlist, early_stopping_rounds=50,\n                  maximize=False, verbose_eval=20)\nprint('Modeling RMSLE %.5f' % model.best_score)\nxgb_preds = np.exp(model.predict(dtest))\n\n"}, {"outputs": [], "metadata": {"trusted": false, "_uuid": "3070ecbadb33b06df3dd865412329d7cd4d60cda", "_cell_guid": "94cc42dc-ea9b-422b-b8c8-820e08f80e95"}, "execution_count": null, "cell_type": "code", "source": "# Create file from predictions.\ntest['trip_duration'] = (test['nn_preds'] + xgb_preds) / 2\ntest[['id', 'trip_duration']].to_csv('submission.csv', index=False)"}]}