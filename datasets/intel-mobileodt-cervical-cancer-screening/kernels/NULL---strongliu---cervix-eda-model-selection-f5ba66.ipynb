{"cells":[{"metadata":{"_cell_guid":"419ad209-85bf-c4ef-d3d7-d8fa2b6be0ea","_uuid":"8f4b8ad462f8db4ad38fd371736ccbb505ddaf43"},"cell_type":"markdown","source":"# Cervix EDA\n\nIn this competition we have a multi-class classification problem with **three** classes. We are asked, given an image, to identify the cervix type.\n\nFrom the data description:\n\n*In this competition, you will develop algorithms to correctly classify cervix types based on cervical images. These different types of cervix in our data set are all considered normal (not cancerous), but since the transformation zones aren't always visible, some of the patients require further testing while some don't. This decision is very important for the healthcare provider and critical for the patient. Identifying the transformation zones is not an easy task for the healthcare providers, therefore, an algorithm-aided decision will significantly improve the quality and efficiency of cervical cancer screening for these patients.*\n\nThe submission format is asking for a probability for each of the three different cervix types.\n\nIn this notebook we will be looking at:\n\n* basic dataset stats like number of samples per class, image sizes\n* different embeddings of RGB image space\n* pairwise distances and a clustermap of images in RGB space\n* (linear) model selection with basic multi class evaluation metrics.\n\n**If you like this kernel, please give an upvote, thanks! :)**","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"6f6e8717-2620-3fbc-dff4-7093bde04be9","_uuid":"296af0894492bac5dbebf7dd721c2112f2c7c03b","trusted":true,"collapsed":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom skimage.io import imread, imshow\nimport cv2\n\n%matplotlib inline\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input/train\"]).decode(\"utf8\"))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"a640fa62-669c-a08c-aa2b-20c4c713bcdd","_uuid":"545ed1fc9ee148ac170934b532daf41ce524df45"},"cell_type":"markdown","source":"We are given training images for each of cervix types. Lets first count them for each class.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"df96bdda-f3bf-84f9-0abd-ee6161018846","_uuid":"87d01b22e59e58d0d6b5313048c5cf3875863c24","trusted":true,"collapsed":true},"cell_type":"code","source":"from glob import glob\nbasepath = '../input/train/'\n\nall_cervix_images = []\n\nfor path in sorted(glob(basepath + \"*\")):\n    cervix_type = path.split(\"/\")[-1]\n    cervix_images = sorted(glob(basepath + cervix_type + \"/*\"))\n    all_cervix_images = all_cervix_images + cervix_images\n\nall_cervix_images = pd.DataFrame({'imagepath': all_cervix_images})\nall_cervix_images['filetype'] = all_cervix_images.apply(lambda row: row.imagepath.split(\".\")[-1], axis=1)\nall_cervix_images['type'] = all_cervix_images.apply(lambda row: row.imagepath.split(\"/\")[-2], axis=1)\nall_cervix_images.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"172accdb-dc4d-17da-fa39-0740c32a54f9","_uuid":"202e8cea940b0ce56834e3fe2e6ba29ea35046cb"},"cell_type":"markdown","source":"## Image types\n\nNow that we have the data in a handy dataframe we can do a few aggregations on the data. Let us first see how many images there are for each cervix type and which file types they have.\n\nAll files are in JPG format and Type 2 is the most common one with a little bit more than 50% in the training data in total, Type 1 on the other hand has a little bit less than 20% in the training data.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"1c42f54f-45fb-7a2e-6c89-ba7c7a6a48a3","_uuid":"dc27cd706099fab3ad029920394fa2a6109fbb6b","trusted":true,"collapsed":true},"cell_type":"code","source":"print('We have a total of {} images in the whole dataset'.format(all_cervix_images.shape[0]))\ntype_aggregation = all_cervix_images.groupby(['type', 'filetype']).agg('count')\ntype_aggregation_p = type_aggregation.apply(lambda row: 1.0*row['imagepath']/all_cervix_images.shape[0], axis=1)\n\nfig, axes = plt.subplots(nrows=2, ncols=1, figsize=(10, 8))\n\ntype_aggregation.plot.barh(ax=axes[0])\naxes[0].set_xlabel(\"image count\")\ntype_aggregation_p.plot.barh(ax=axes[1])\naxes[1].set_xlabel(\"training size fraction\")","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"4b3581d5-c9a3-49e4-427c-08ef8ede1053","_uuid":"e63641ff0c6cabc3824d7baa552bc781e29f8247"},"cell_type":"markdown","source":"Now, lets read the files for each type to get an idea about how the images look like.\n\nThe images seem to vary alot in they formats, the first two samples have only a circular area with the actual image, the last sample has the image in a rectangle.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"0b500f12-8d54-4dab-a9d1-8bc2fb2de755","_uuid":"375bcd52514af89b7b97cfa9393d7ec36682a61c","trusted":true,"collapsed":true},"cell_type":"code","source":"fig = plt.figure(figsize=(12,8))\n\ni = 1\nfor t in all_cervix_images['type'].unique():\n    ax = fig.add_subplot(1,3,i)\n    i+=1\n    f = all_cervix_images[all_cervix_images['type'] == t]['imagepath'].values[0]\n    plt.imshow(plt.imread(f))\n    plt.title('sample for cervix {}'.format(t))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"4db0231d-6d7c-5780-400d-d27cbea25434","_uuid":"9c14cfe1b2a4bc14a06911e7b3df16d5e3d844d2"},"cell_type":"markdown","source":"## Image dimensions\n\nNow, in order to get an idea of how many different shapes of images by class there are, lets have a look at. To reduce runtime, take only a subsample per class.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"24dca5d4-1055-00b3-d5e1-a9eb8de6cb64","_uuid":"0420824e922e6e5382659a3634da455e3a2dc089","trusted":true,"collapsed":true},"cell_type":"code","source":"from collections import defaultdict\n\nimages = defaultdict(list)\n\nfor t in all_cervix_images['type'].unique():\n    sample_counter = 0\n    for _, row in all_cervix_images[all_cervix_images['type'] == t].iterrows():\n        #print('reading image {}'.format(row.imagepath))\n        try:\n            img = imread(row.imagepath)\n            sample_counter +=1\n            images[t].append(img)\n        except:\n            print('image read failed for {}'.format(row.imagepath))\n        if sample_counter > 35:\n            break","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c3aa56f1-e78e-55e4-a961-52a189c18403","_uuid":"35075c0309c2301df995a7d21fc73ff80b7d68c6","trusted":true,"collapsed":true},"cell_type":"code","source":"dfs = []\nfor t in all_cervix_images['type'].unique():\n    t_ = pd.DataFrame(\n        {\n            'nrows': list(map(lambda i: i.shape[0], images[t])),\n            'ncols': list(map(lambda i: i.shape[1], images[t])),\n            'nchans': list(map(lambda i: i.shape[2], images[t])),\n            'type': t\n        }\n    )\n    dfs.append(t_)\n\nshapes_df = pd.concat(dfs, axis=0)\nshapes_df_grouped = shapes_df.groupby(by=['nchans', 'ncols', 'nrows', 'type']).size().reset_index().sort_values(['type', 0], ascending=False)\nshapes_df_grouped","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"dabf9549-9b4c-11b8-6182-ec71de2aab5d","_uuid":"d30b564509f15ac26820f9c45564d1664e95e84a"},"cell_type":"markdown","source":"All of the images in our sample have three channels, we can ignore this information for now. Now lets build a barplot to get an idea of the distribution of image dimensions by cervix type.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"336a5362-4eee-090d-5ac5-3a84e6970b00","_uuid":"0a30e28d002f0f43d9600c3701a6d42ba4d7efdf","trusted":true,"collapsed":true},"cell_type":"code","source":"shapes_df_grouped['size_with_type'] = shapes_df_grouped.apply(lambda row: '{}-{}-{}'.format(row.ncols, row.nrows, row.type), axis=1)\nshapes_df_grouped = shapes_df_grouped.set_index(shapes_df_grouped['size_with_type'].values)\nshapes_df_grouped['count'] = shapes_df_grouped[[0]]\n\nplt.figure(figsize=(10,8))\n#shapes_df_grouped['count'].plot.barh(figsize=(10,8))\nsns.barplot(x=\"count\", y=\"size_with_type\", data=shapes_df_grouped)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"1008461b-6a10-64a2-1716-4ba7d2d2dce4","_uuid":"ec61d6ac6326cea44714ab7120523df770e4a4ea"},"cell_type":"markdown","source":"# TSNE embedding\n\nWe will now take all of the sample images, rescale them & convert them to grayscale. This will result in a matrix, where each row are all flattened pixel for the grayscale images.\n\nThe original images have, as we have seen earlier, quite a high resolution, so scaling them down to **100 x 100** is resulting in a great loss of information, so the embedding to two dimensions is likely not going to have a good structure where we can separate visually by cervical cancer types. Also, we are giving only very few images per class that TSNE can work with to find a good, distance preserving, embedding.\n\nI've added the option to choose to convert them to grayscale before passing them to TSNE. Per default each image will now keep its RGB information and therefore will be transformed to a 100*100*3 = 30.000 dimensional vector.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"f83a4a94-b331-a1a5-def9-68678758c30b","_uuid":"5224d0f1376e420e96f826a010ae99c3f4ac4eae","trusted":true,"collapsed":true},"cell_type":"code","source":"def transform_image(img, rescaled_dim, to_gray=False):\n    resized = cv2.resize(img, (rescaled_dim, rescaled_dim), cv2.INTER_LINEAR)\n\n    if to_gray:\n        resized = cv2.cvtColor(resized, cv2.COLOR_RGB2GRAY).astype('float')\n    else:\n        resized = resized.astype('float')\n\n    normalized = cv2.normalize(resized, None, 0.0, 1.0, cv2.NORM_MINMAX)\n    timg = normalized.reshape(1, np.prod(normalized.shape))\n\n    return timg/np.linalg.norm(timg)\n\nrescaled_dim = 100\n\nall_images = []\nall_image_types = []\n\nfor t in all_cervix_images['type'].unique():\n    all_images = all_images + images[t]\n    all_image_types = all_image_types + len(images[t])*[t]\n\n# - normalize each uint8 image to the value interval [0, 1] as float image\n# - rgb to gray\n# - downsample image to rescaled_dim X rescaled_dim\n# - L2 norm of each sample = 1\ngray_all_images_as_vecs = [transform_image(img, rescaled_dim) for img in all_images]\n\ngray_imgs_mat = np.array(gray_all_images_as_vecs).squeeze()\nall_image_types = np.array(all_image_types)\ngray_imgs_mat.shape, all_image_types.shape","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ec6a4789-f799-8ce7-7854-0a1fe0a88baf","_uuid":"8054d6b74e26cf3713ef1ef92dadb914dee672a7"},"cell_type":"markdown","source":"### 3D t-SNE with cervix indicators\n\nNow let's project the 100x100x3 images to three dimensions to check for low dimensional patterns.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"e7fd77ee-8449-2ca7-2691-1906ee10286b","_uuid":"0cc6d0789608a5aa14ce26a3a240276ceeb05d27","trusted":true,"collapsed":true},"cell_type":"code","source":"from sklearn.manifold import TSNE\ntsne = TSNE(\n    n_components=3,\n    init='random', # pca\n    random_state=101,\n    method='barnes_hut',\n    n_iter=500,\n    verbose=2\n).fit_transform(gray_imgs_mat)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"508824d4-7475-5722-bb85-73c1bd79c1ee","_uuid":"6b345dd86be11c4e58bdd9ad43729ab781d18f37","trusted":true,"collapsed":true},"cell_type":"code","source":"from sklearn import preprocessing\n\ntrace1 = go.Scatter3d(\n    x=tsne[:,0],\n    y=tsne[:,1],\n    z=tsne[:,2],\n    mode='markers',\n    marker=dict(\n        sizemode='diameter',\n        color = preprocessing.LabelEncoder().fit_transform(all_image_types),\n        colorscale = 'Portland',\n        colorbar = dict(title = 'cervix types'),\n        line=dict(color='rgb(255, 255, 255)'),\n        opacity=0.9\n    )\n)\n\ndata=[trace1]\nlayout=dict(height=800, width=800, title='3D embedding of images')\nfig=dict(data=data, layout=layout)\npy.iplot(fig, filename='3DBubble')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"5f8e4fcc-7686-ed91-bcfb-78a7384cf911","_uuid":"2644b408d6a874d5ddb564be65f0ead12dff0619"},"cell_type":"markdown","source":"We can clearly see that there is a big heterogeneous cluster and a few quite distant outliers.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"c9a14afc-2aab-8519-b4d2-53c9eb36337c","_uuid":"40b6e89d222fbb134b9c6a7a15d13336be7c523e","trusted":true,"collapsed":true},"cell_type":"code","source":"for t in all_cervix_images['type'].unique():\n    tsne_t = tsne[np.where(all_image_types == t), :][0]\n    plt.scatter(tsne_t[:, 0], tsne_t[:, 1])\nplt.legend(all_cervix_images['type'].unique())","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"dba5ac7f-3868-04c6-79f6-3236bfea95a1","_uuid":"86a3d65c7fa134b5eb83216e69bf6a9064cf170f"},"cell_type":"markdown","source":"# Image clustering with TSNE embedding\n\nLet us now use the compressed images and the distance-preserving transformation of TSNE to get an understanding of what image clusters there are.\n\nFor that we will use matplotlib to replace the previously red, green and blue points with the actual images that were input to the TSNE transformation.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"72cd02f8-1bb5-77d6-3029-6b78111abe6c","_uuid":"aa95f34b9de63cdb0f8557f631e1e4c2d1e8ae49","trusted":true,"collapsed":true},"cell_type":"code","source":"from matplotlib.offsetbox import OffsetImage, AnnotationBbox\ndef imscatter(x, y, images, ax=None, zoom=0.01):\n    ax = plt.gca()\n    images = [OffsetImage(image, zoom=zoom) for image in images]\n    artists = []\n    for x0, y0, im0 in zip(x, y, images):\n        ab = AnnotationBbox(im0, (x0, y0), xycoords='data', frameon=False)\n        artists.append(ax.add_artist(ab))\n    ax.update_datalim(np.column_stack([x, y]))\n    ax.autoscale()\n    #return artists\n\nnimgs = 60\nplt.figure(figsize=(10,8))\nimscatter(tsne[0:nimgs,0], tsne[0:nimgs,1], all_images[0:nimgs])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"660e028d-fba5-e767-850e-6eaa2c463f84","_uuid":"153aebedb8170cdba71a0510aa54c3050cf41777"},"cell_type":"markdown","source":"# Clustering of pairwise image distances\n\nTo get a different view of how images relate to each other from a purely numerical point of view, lets now look at pairwise distances. For that we'll use scipy's pdist method.\n\nThe yellow somewhat clustered area tells us there are a few images that have relatively high distance to all other images in the sample of our training images we read.\nOn the left and top of the clustermap we find one of three colors for each row and column, this color indicates the type of cervix.\n\n* Type 1: Red\n* Type 2: Green\n* Type 3: Blue","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"fe9f7daa-880d-b944-9705-7bec5a550e32","_uuid":"f1c3462ef2e9a0e555257a99070b8d1c7e3fb13b","trusted":true,"collapsed":true},"cell_type":"code","source":"pal = sns.color_palette(\"hls\", 3)\nsns.palplot(pal)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f54fc443-44d5-69a9-0d2a-1d506ebd467f","_uuid":"9ff11f000bd7750d0f7926d32b1aa1216dac62cb","trusted":true,"collapsed":true},"cell_type":"code","source":"from scipy.spatial.distance import pdist, squareform\n\nsq_dists = squareform(pdist(gray_imgs_mat))\n\nall_image_types = list(all_image_types)\n\nd = {\n    'Type_1': pal[0],\n    'Type_2': pal[1],\n    'Type_3': pal[2]\n}\n\n# translate each sample to its color\ncolors = list(map(lambda t: d[t], all_image_types))\n\nsns.clustermap(\n    sq_dists,\n    figsize=(12,12),\n    row_colors=colors, col_colors=colors,\n    cmap=plt.get_cmap('viridis')\n)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3c09b509-3249-d7e0-2c07-f9df3c81cb72","_uuid":"93dea308c77a20c26fd274525eab58422cfb16e0"},"cell_type":"markdown","source":"Here is the unclustered distance matrix.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"1becfcaa-0503-2ee6-dc38-8d29ac8e463a","_uuid":"5e3f7c0708900d5811e8997889efdfb1a9d5f68d","trusted":true,"collapsed":true},"cell_type":"code","source":"\nmask = np.zeros_like(sq_dists, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\nplt.figure(figsize=(12,12))\nsns.heatmap(sq_dists, cmap=plt.get_cmap('viridis'), square=True, mask=mask)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"763f409d-3e8d-2045-8014-cbd2d4e48001","_uuid":"2b14b83ad4e93d95272fe578334319f84a51b633"},"cell_type":"markdown","source":"----------\n\n# Image neighbourhood\n\nGiven the pool of cervix images, and the similarity matrix above, we will now plot the extrem cases of the similarity matrix.\n\nTo do that we find the image that has:\n\n* maximal distance on average compared to the all other images\n* minimal distance on average compared to the all other images\n\nand compare both candidates to the mean image from out set of images.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"32ac6a8f-4636-1f6a-b762-1d4992704a1d","_uuid":"a6b222d6171d88e4961451a8e98fb03011e02287","trusted":true,"collapsed":true},"cell_type":"code","source":"# upper triangle of matrix set to np.nan\nsq_dists[np.triu_indices_from(mask)] = np.nan\nsq_dists[0, 0] = np.nan\n\nfig = plt.figure(figsize=(12,8))\n# maximally dissimilar image\nax = fig.add_subplot(1,3,1)\nmaximally_dissimilar_image_idx = np.nanargmax(np.nanmean(sq_dists, axis=1))\nplt.imshow(all_images[maximally_dissimilar_image_idx])\nplt.title('maximally dissimilar')\n\n# maximally similar image\nax = fig.add_subplot(1,3,2)\nmaximally_similar_image_idx = np.nanargmin(np.nanmean(sq_dists, axis=1))\nplt.imshow(all_images[maximally_similar_image_idx])\nplt.title('maximally similar')\n\n# now compute the mean image\nax = fig.add_subplot(1,3,3)\nmean_img = gray_imgs_mat.mean(axis=0).reshape(rescaled_dim, rescaled_dim, 3)\nplt.imshow(cv2.normalize(mean_img, None, 0.0, 1.0, cv2.NORM_MINMAX))\nplt.title('mean image')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"940d21df-393e-99e2-8a17-0b314167fc75","_uuid":"663af3f1264af0b76778d0ac34752eb80e556bad"},"cell_type":"markdown","source":"The image on the left has quite a lot of blue in it which I would not expect to be present in the majority of samples. I think it is a good representation of maximally dissimilar.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"af9d24c3-b3e0-6fc5-eca6-0eb5f672c793","_uuid":"15c2cd61ae08594b42405179a2b398c06e07da8c"},"cell_type":"markdown","source":"----------","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"a7b22b2d-86d2-1567-ba25-57d734402371","_uuid":"c88caeb4b1fc92d21e4b6a50e4b86a6375f08881"},"cell_type":"markdown","source":"# Model Selection\n\nNow that we've established a basic idea about the data, let's do the most straightforward approach, where we take the resized color images and labels and train a, most likely quite heavily regularized, linear model like logistic regression on it.\n\nIt is quite important to understand that we only have read a few training instances, 108, and have thousands of dimensions. To be able to cope with that we'll most likely end up using L1 regularization.\n\nFor the multi-class problem we are faced with here, we'll use standard approach of OVR (one vs rest), meaning we will train three models where each of them is designed to distinguish class 1, 2 and 3 from the others respectively.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"50191441-b485-9dad-d015-9936815e67af","_uuid":"93f56975ab3a7d3ae80e73997983c4892de4e448","collapsed":true,"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import Normalizer\ny = LabelEncoder().fit_transform(all_image_types).reshape(-1)\nX = gray_imgs_mat # no need for normalizing, we already did this earlier Normalizer().fit_transform(gray_imgs_mat)\nX.shape, y.shape","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"68d8cd18-9a36-c3bf-a6ba-7c5e2a0e51f3","_uuid":"98713a507b372f4fac4de39f3bd68d8f315daf8b","collapsed":true,"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV, train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n\nX_train.shape, X_test.shape, y_train.shape, y_test.shape","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"bec40df9-ba5a-7ccb-3da5-1d367d5317aa","_uuid":"a748ed186c9e3d51c18934e508e04df6923cb651","collapsed":true,"trusted":true},"cell_type":"code","source":"y_train, y_test","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c76652c1-6200-a650-06a7-cbcd6c32b212","_uuid":"e0c94a8373e75eaa5fba811a0f29478cc6f0753b","collapsed":true,"trusted":true},"cell_type":"code","source":"clf = LogisticRegression()\ngrid = {\n    'C': [1e-9, 1e-6, 1e-3, 1e0],\n    'penalty': ['l1', 'l2']\n}\ncv = GridSearchCV(clf, grid, scoring='neg_log_loss', n_jobs=-1, verbose=1)\ncv.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"7f97b140-3611-7e2e-f410-7edccb87d377","_uuid":"d1f3045c7924342ac03bccbc1e7913b834c8baf7","collapsed":true,"trusted":true},"cell_type":"code","source":"for i in range(1, len(cv.cv_results_['params'])+1):\n    rank = cv.cv_results_['rank_test_score'][i-1]\n    s = cv.cv_results_['mean_test_score'][i-1]\n    sd = cv.cv_results_['std_test_score'][i-1]\n    params = cv.cv_results_['params'][i-1]\n    print(\"{0}. Mean validation neg log loss: {1:.6f} (std: {2:.6f}) - {3}\".format(\n        rank,\n        s,\n        sd,\n        params\n    ))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"d10b2a63-f1d8-40d9-2a4e-919056e59adc","_uuid":"6ee87c0a886be7f45e7af883ecd0eab3ca9bb668","collapsed":true,"trusted":true},"cell_type":"code","source":"y_test_hat_p = cv.predict_proba(X_test)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"43805be4-5ad9-6500-ede7-543485701285","_uuid":"3f67d7190152a8c7abe87e64148767ac848d3152","collapsed":true,"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,8))\nsns.distplot(y_test_hat_p[:,0], color='red')\nsns.distplot(y_test_hat_p[:,1], color='blue')\nsns.distplot(y_test_hat_p[:,2], color='green')\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c5e66954-b3f0-6d4f-1d1e-7abbe3e9a623","_uuid":"e2d08b21caad2616c5e418c681992194aa748998","collapsed":true,"trusted":true},"cell_type":"code","source":"dfy = pd.DataFrame({'0': y_test_hat_p[:,0], '1': y_test_hat_p[:,1], '2': y_test_hat_p[:,2]})\nsns.pairplot(dfy)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"38e033ce-6e48-f7e6-ee67-f497ff9fd48c","_uuid":"028df95b58d390128275ec135886eeae853e2236"},"cell_type":"markdown","source":"## Confusion matrix\n\nThe confusion matrix is a standard analysis tool in binary and multi-class classification where in each entry **C_{i,j}** corresponds to the number of samples that have true class label **i** and are predicted as **j**.\n\nWe can see that our models have a great problem with confusing class zero with class two.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"747cc004-7745-d1ac-685f-0ada954baba8","_uuid":"0e83cdc87ad6bd24f8e5a225cddf11b91aff3979","collapsed":true,"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\n\ny_test_hat = cv.predict(X_test)\n\ndata = [\n    go.Heatmap(\n        z=confusion_matrix(y_test, y_test_hat),\n        x=[0, 1, 2],\n        y=[0, 1, 2],\n        colorscale='Viridis',\n        text = True ,\n        opacity = 1.0\n    )\n]\n\nlayout = go.Layout(\n    title='Test Confusion matrix',\n    xaxis = dict(ticks='', nticks=36),\n    yaxis = dict(ticks='' ),\n    width = 900, height = 700,\n    \n)\n\n\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename='labelled-heatmap')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"d63c4c82-3e23-726d-930b-a1838019586c","_uuid":"63fc4cdab2d28a840814efd15d91e7f63bd899f9"},"cell_type":"markdown","source":"## Confusion matrix on training set\n\nLooking at the train confusion matrix, we can see that we almost only predict class 1.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"594b463c-c745-d249-768f-c1f47e7799c8","_uuid":"6487b0133586892696f83ad701abdde95f1f47d5","collapsed":true,"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\n\ny_train_hat = cv.predict(X_train)\n\ndata = [\n    go.Heatmap(\n        z=confusion_matrix(y_train, y_train_hat),\n        x=[0, 1, 2],\n        y=[0, 1, 2],\n        colorscale='Viridis',\n        text = True ,\n        opacity = 1.0\n    )\n]\n\nlayout = go.Layout(\n    title='Training Confusion matrix',\n    xaxis = dict(ticks='', nticks=36),\n    yaxis = dict(ticks='' ),\n    width = 900, height = 700,\n    \n)\n\n\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename='labelled-heatmap')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"75c5beb1-3a6f-9bc4-8090-9c9cf9e3a8f1","_uuid":"a6573085759d8484182199a1274c21a19f7c6420","collapsed":true,"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":1}