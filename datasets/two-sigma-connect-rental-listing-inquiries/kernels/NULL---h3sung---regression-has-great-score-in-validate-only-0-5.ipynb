{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "d29d6549-e6f5-b40d-8c89-4bab6ef051da"
      },
      "source": [
        "I merely use some basic features here. If using some better (but complex) features, I can achieve scores below 0.5 in validate set. However, only ~0.67 score can be obtained on public scoreboard with all these trials."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b70206cd-ac8c-bb0c-b6ca-1a53c952ac7a"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import time\n",
        "import operator\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import defaultdict\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.svm import LinearSVC, SVC, SVR\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import KFold, cross_val_score\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
        "from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor\n",
        "from xgboost import XGBClassifier, XGBRegressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "9d8df851-0fbd-231a-99c4-c14be21a621e"
      },
      "outputs": [],
      "source": [
        "print(\"\\nReading data...\")\n",
        "tn_data = pd.read_json('../input/train.json').set_index('listing_id')\n",
        "tt_data = pd.read_json('../input/test.json').set_index('listing_id')\n",
        "print(\"done\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c5b77491-2e5f-a98a-81a9-5853b70b77bc"
      },
      "outputs": [],
      "source": [
        "# drop duplicate data\n",
        "tn_data['features'] = tn_data.loc[:,'features'].apply(lambda x: ' '.join(x))\n",
        "tn_data['photos'] = tn_data.loc[:,'photos'].apply(lambda x: ' '.join(x))\n",
        "tt_data['features'] = tt_data.loc[:,'features'].apply(lambda x: ' '.join(x))\n",
        "tt_data['photos'] = tt_data.loc[:,'photos'].apply(lambda x: ' '.join(x))\n",
        "tn_data = tn_data.drop_duplicates()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d82cb535-1b52-aec5-5011-0a152f705401"
      },
      "outputs": [],
      "source": [
        "# extract needed features to tn_sdata\n",
        "tn_sdata = tn_data[['bathrooms', 'bedrooms', 'latitude', 'longitude', 'price']]\n",
        "tt_sdata = tt_data[['bathrooms', 'bedrooms', 'latitude', 'longitude', 'price']]\n",
        "\n",
        "# define colnames\n",
        "raw_colnames = ['bathrooms', 'bedrooms', 'latitude', 'longitude', 'price']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d19ba82c-0137-6a7e-ddf7-449db99c33de"
      },
      "outputs": [],
      "source": [
        "# deal with number of images\n",
        "tn_sdata.loc[:,'n_images'] = tn_data.loc[:,'photos'].apply(lambda x: len(x.split()))\n",
        "tt_sdata.loc[:,'n_images'] = tt_data.loc[:,'photos'].apply(lambda x: len(x.split()))\n",
        "\n",
        "# deal with number of features\n",
        "tn_sdata.loc[:,'n_features'] = tn_data.loc[:,'features'].apply(lambda x: len(x.split()))\n",
        "tt_sdata.loc[:,'n_features'] = tt_data.loc[:,'features'].apply(lambda x: len(x.split()))\n",
        "\n",
        "# deal with number of words in descriptions\n",
        "tn_sdata.loc[:,'n_words'] = tn_data.loc[:,'description'].apply(lambda x: len(x.split()))\n",
        "tt_sdata.loc[:,'n_words'] = tt_data.loc[:,'description'].apply(lambda x: len(x.split()))\n",
        "\n",
        "# define colnames\n",
        "len_colnames = ['n_images', 'n_features', 'n_words']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "a7894c81-d143-912e-d1f8-73a48165f629"
      },
      "outputs": [],
      "source": [
        "# deal with building ID\n",
        "tn_sdata.loc[:,'building_id'] = np.array(tn_data.loc[:,'building_id'] != '0', dtype=int).reshape(-1,1)\n",
        "tt_sdata.loc[:,'building_id'] = np.array(tt_data.loc[:,'building_id'] != '0', dtype=int).reshape(-1,1)\n",
        "id_colnames = ['building_id']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "dad2935c-29b8-fb69-6cf3-ee11a7514528"
      },
      "outputs": [],
      "source": [
        "colnames = raw_colnames + len_colnames + id_colnames\n",
        "print(colnames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "335c5853-87fc-d01a-83db-fad58ec45420"
      },
      "outputs": [],
      "source": [
        "# deal with interest_level\n",
        "tn_cy = tn_data[['interest_level']]\n",
        "tn_ry = np.zeros((tn_sdata.shape[0],), dtype=int)\n",
        "tn_ry[np.array(tn_cy['interest_level'] == 'medium')] = 1\n",
        "tn_ry[np.array(tn_cy['interest_level'] == 'high')] = 1\n",
        "\n",
        "# merge label into tn_sdata\n",
        "pd.options.mode.chained_assignment = None  # default='warn'\n",
        "tn_sdata['interest_level'] = pd.Series(tn_ry, index=tn_sdata.index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "316b0e77-2fa6-025a-8fbb-da02bdb181c1"
      },
      "outputs": [],
      "source": [
        "# split dataset into subtrain and validate sets\n",
        "stn_sdata, vld_sdata = train_test_split(tn_sdata, test_size = 0.2, random_state=64)\n",
        "\n",
        "# partition x and y\n",
        "stn_x = stn_sdata[colnames]\n",
        "stn_y = np.array(stn_sdata['interest_level'], dtype=int)\n",
        "vld_x = vld_sdata[colnames]\n",
        "vld_y = np.array(vld_sdata['interest_level'], dtype=int)\n",
        "tt_x = tt_sdata[colnames]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "cda7fce9-5c05-99bc-3e2a-60a5b44f7b4c"
      },
      "outputs": [],
      "source": [
        "clf = XGBRegressor(n_estimators=100,\n",
        "                        learning_rate=0.1,\n",
        "                        max_depth=3,\n",
        "                        min_child_weight=1,\n",
        "                        colsample_bytree=.9,\n",
        "                        colsample_bylevel=.5,\n",
        "                        gamma=0.0005,\n",
        "                        scale_pos_weight=1,\n",
        "                        base_score=.5,\n",
        "                        reg_lambda=1,\n",
        "                        reg_alpha=1,\n",
        "                        missing=0,\n",
        "                        seed=514)\n",
        "\n",
        "k=10\n",
        "kf = KFold(n_splits=k)\n",
        "for i, idx in zip(range(k), kf.split(tn_sdata)):\n",
        "    # split data\n",
        "    stn_idx, vld_idx = idx\n",
        "    stn_x = tn_sdata[colnames].iloc[stn_idx]\n",
        "    vld_x = tn_sdata[colnames].iloc[vld_idx]\n",
        "    stn_y = np.array(tn_sdata['interest_level'].iloc[stn_idx], dtype=int)\n",
        "    vld_y = np.array(tn_sdata['interest_level'].iloc[vld_idx], dtype=int)\n",
        "    \n",
        "    # train model\n",
        "    clf.fit(stn_x, stn_y)\n",
        "\n",
        "    # predict on subtrain and validate sets\n",
        "    ratio = 0.9\n",
        "    stn_p = clf.predict(stn_x)\n",
        "    stn_p = np.array(list(map(lambda x: min(max(x,0),1), stn_p)))\n",
        "    stn_p = np.array(list(map(lambda x: [1-x, ratio*x, (1-ratio)*x], stn_p)))\n",
        "    vld_p = clf.predict(vld_x)\n",
        "    vld_p = np.array(list(map(lambda x: min(max(x,0),1), vld_p)))\n",
        "    vld_p = np.array(list(map(lambda x: [1-x, ratio*x, (1-ratio)*x], vld_p)))\n",
        "    \n",
        "    # prevent -inf caused by 0 prediction\n",
        "    threshold = 0.0001\n",
        "    stn_p = stn_p + threshold * np.ones(stn_p.shape)\n",
        "    stn_p = stn_p / np.sum(stn_p, axis=1).reshape(-1,1)\n",
        "    vld_p = vld_p + threshold * np.ones(vld_p.shape)\n",
        "    vld_p = vld_p / np.sum(vld_p, axis=1).reshape(-1,1)\n",
        "    \n",
        "    print('\\nFold: %d'%i)\n",
        "    print('Subtrain logloss: %f'%log_loss(stn_y, stn_p, labels=[0,1,2]))\n",
        "    print('Validate logloss: %f'%log_loss(vld_y, vld_p, labels=[0,1,2]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "55f661b9-d0d5-63e5-f1c3-60e4fce03c5f"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}