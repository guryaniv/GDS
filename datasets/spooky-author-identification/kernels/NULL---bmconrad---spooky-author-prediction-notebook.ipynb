{"metadata": {"kernelspec": {"display_name": "Python 3", "name": "python3", "language": "python"}, "language_info": {"file_extension": ".py", "name": "python", "codemirror_mode": {"version": 3, "name": "ipython"}, "version": "3.6.3", "mimetype": "text/x-python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3"}}, "nbformat": 4, "nbformat_minor": 1, "cells": [{"metadata": {"_uuid": "2ec1c3d467a209df0a2b45f322f6699758db14ac", "_cell_guid": "ecb08636-c4b4-406c-bdcd-78dfdf598444"}, "source": ["## Predicting Spooky Authors: Part 1\n", "### Author: Blake Conrad"], "cell_type": "markdown"}, {"execution_count": null, "metadata": {"_uuid": "7cab997a81343819a159eb5ec98efb09ee6e2511", "_cell_guid": "da4dd525-0ce9-4f01-91dc-5306d600aef3", "collapsed": true}, "source": ["import numpy as np # linear algebra\n", "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n", "\n", "df_train = pd.read_csv(\"../input/train.csv\")\n", "df_test = pd.read_csv(\"../input/test.csv\")"], "outputs": [], "cell_type": "code"}, {"metadata": {"_uuid": "e3c8f0f8726d97d46b5083ca03ceeddf3de74095", "_cell_guid": "2556eef7-0b7f-421d-8365-a34f307616ce"}, "source": ["## Bag of words\n", "Text preprocessing, tokenizing and filtering of stopwords are included in a high level component that is able to build a dictionary of features and transform documents to feature vectors:\n", "\n", "Occurrence count is a good start but there is an issue: longer documents will have higher average count values than shorter documents, even though they might talk about the same topics.\n", "To avoid these potential discrepancies it suffices to divide the number of occurrences of each word in a document by the total number of words in the document: these new features are called tf for Term Frequencies.\n", "Another refinement on top of tf is to downscale weights for words that occur in many documents in the corpus and are therefore less informative than those that occur only in a smaller portion of the corpus.\n", "This downscaling is called tf\u2013idf for \u201cTerm Frequency times Inverse Document Frequency\u201d."], "cell_type": "markdown"}, {"execution_count": null, "metadata": {"_uuid": "fb840512cca511eddd1f5fa01d6125046899c542", "_cell_guid": "9c187752-2432-45d1-af25-33ff08e603c5", "collapsed": true}, "source": ["from sklearn.feature_extraction.text import CountVectorizer\n", "from sklearn.feature_extraction.text import TfidfTransformer\n", "\n", "# Feed: \n", "#  1. A list of sentences\n", "#  2. A pandas dataframe that represents a list of sentences\n", "# E.g., [\"This is the first sentence, yes.\",\n", "#        \"Now youre getting the idea, aren't you?\",\n", "#        ...]\n", "def get_bag_of_words(X):\n", "    \n", "    count_vect = CountVectorizer()\n", "    X_counts = count_vect.fit_transform(X)\n", "    #X_counts.shape\n", "\n", "    tf_transformer = TfidfTransformer(use_idf=False).fit(X_counts)\n", "    X_tf = tf_transformer.transform(X_counts)\n", "    #X_tf.shape\n", "\n", "    tfidf_transformer = TfidfTransformer()\n", "    X_tfidf = tfidf_transformer.fit_transform(X_counts)\n", "    #X_tfidf.shape\n", "    #X_tfidf.data\n", "    \n", "    print(\"Bag of words created!\")\n", "    return X_tfidf"], "outputs": [], "cell_type": "code"}, {"metadata": {"_uuid": "e4f3549083e048476079917b3919899f2f3645e5", "_cell_guid": "fb3ded62-1ee6-48c8-b77f-f73e6ee9e9a3"}, "source": ["## Split training sets\n", "It is important to remember when training and testing that upon the split in the data sets prior to building our term frequencies and term frequency inverse document frequencies, the vocabulary is subject to change with the data fed into it. To avoid any dimensionality mismatching, we just build the universal vocabulary with our training and testing set, but only train the model on the observations in the data related to the training data points. Likewise when testing, we only use the testing points. This is loosely managed by the `pd.concat([df1,df2,...,dfn])` function which is the same as a `rbind(df1,df2)` in R, By putting X_train in front of the bag of words for the training term frequency inverse document frequencies, we know to only seek from `0:NumberOfTrainingRows == 0:len(X_train)`, similarly for the testing; since we put it first in the `pd.concat`; `0:len(X_test)`. We use this logic again later when actually building a predictor for the model and cross validation, so it is good to understand the little trick now. "], "cell_type": "markdown"}, {"execution_count": null, "metadata": {"_uuid": "6fc2d72dfce482b3913ba0a84865b0de54a8af92", "_cell_guid": "93fd46a1-d1ca-4c6b-a5d4-7be2131239e3"}, "source": ["from sklearn.model_selection import train_test_split\n", "X_train, X_test, y_train, y_test = train_test_split(df_train[\"text\"],\n", "                                                    df_train['author'],\n", "                                                    test_size=0.33, random_state=42)\n", "\n", "X_train_tfidf = get_bag_of_words(pd.concat([X_train, X_test]))\n", "X_test_tfidf = get_bag_of_words(pd.concat([X_test, X_train]))"], "outputs": [], "cell_type": "code"}, {"metadata": {"_uuid": "cc08345b979dadfd966593ac77bb6148abf300a5", "_cell_guid": "b2cca014-e0d3-4618-9097-e7040af90fd2"}, "source": ["## Train a Naive Bayes Model to predict Authors\n", "\n", "MultinomialNB implements the naive Bayes algorithm for multinomially distributed data, and is one of the two classic naive Bayes variants used in text classification (where the data are typically represented as word vector counts, although tf-idf vectors are also known to work well in practice). The distribution is parametrized by vectors \\theta_y = (\\theta_{y1},\\ldots,\\theta_{yn}) for each class y, where n is the number of features (in text classification, the size of the vocabulary) and \\theta_{yi} is the probability P(x_i \\mid y) of feature i appearing in a sample belonging to class y."], "cell_type": "markdown"}, {"execution_count": null, "metadata": {"_uuid": "0846f0b35290299c13f1acbb6817df41cd53d8fd", "_cell_guid": "cd1adb3b-34de-46ba-845f-4d9983c3a449"}, "source": ["from sklearn.naive_bayes import MultinomialNB\n", "\n", "# Fit on our term frequency inverse document frequency\n", "clf = MultinomialNB().fit(X_train_tfidf[:len(X_train)], y_train)\n", "\n", "# Build the test data set\n", "y_pred = clf.predict(X_test_tfidf[:len(X_test)])\n", "print(\"Top 5 Predictions on X_test: \", y_pred[:5])\n", "\n"], "outputs": [], "cell_type": "code"}, {"metadata": {"_uuid": "b2f7fecd65d670faab880f6dec813e5d6eea91ec", "_cell_guid": "ce6dee72-4b8a-4750-b1f4-89f237905fde"}, "source": ["## Performance\n", "Now we are interesting in seeing how well we actually predicted. I import some precious libraries, then build a little helper function to get a quick glance at how well we did in some standard areas of performance analysis. I typically like to use `accuracy` as a benchmark, however `log_loss` or `entropy` and others are used depending on the type of problem being dealt with.\n", "\n", "## Report\n", "\n", "Not too bad, we got a 79% Accuracy  on our first shot. Lets take a look at building a `Pipeline`, `Parameter Grid Optimization`, and `Cross Validation` to see what our best model looks like. Additionally, we can start to look at other models (I.e., New pipelines) for `SVM`, `Random Forest`,  `Adaptive Boosting`, and `Ensemble`."], "cell_type": "markdown"}, {"execution_count": null, "metadata": {"_uuid": "0cb9541c9351dc283fcff22bd4f738a2defc6583", "_cell_guid": "3341ebdc-d8be-44e2-af85-c0d10f04f340"}, "source": ["#import entropy/log loss as a metric\n", "from sklearn.metrics import precision_score, \\\n", "    recall_score, confusion_matrix, classification_report, \\\n", "    accuracy_score, f1_score\n", "from sklearn.metrics import log_loss\n", "from sklearn.metrics import classification_report\n", "\n", "def generate_results(y_true, y_pred):\n", "    \n", "    print ('Accuracy:\\n', accuracy_score(y_test, y_pred))\n", "    print ('F1 score:\\n', f1_score(y_test, y_pred, average='macro'))\n", "    print ('Recall:\\n', recall_score(y_test, y_pred, average='macro'))\n", "    print ('Precision:\\n', precision_score(y_test, y_pred, average='macro'))\n", "    print ('clasification report:\\n', classification_report(y_test,y_pred))\n", "    print ('confussion matrix:\\n',confusion_matrix(y_test, y_pred))\n", "    #print ('log loss:\\n',log_loss(y_test, y_pred))\n", "    #print entropy/log_loss as a metric\n", "\n", "generate_results(y_test, y_pred)\n", "\n", "print(classification_report(y_test, y_pred, target_names=clf.classes_))"], "outputs": [], "cell_type": "code"}, {"metadata": {"_uuid": "1e04afe420fb98edd3b5a55a138f6a268ee9caac", "_cell_guid": "1eae3624-f1d1-408c-a59b-ce2ef3d435b4"}, "source": ["## Actually do a Kaggle Prediction\n", "1. `X_train_tfidf = get_bag_of_words(pd.concat([df_train[\"text\"], df_test['text']]))`Again, we build vocabulary on the training and testing data"], "cell_type": "markdown"}, {"execution_count": null, "metadata": {"_uuid": "b4a78acc4ff3a1dc5fe6605125187ca67955f0a0", "_cell_guid": "a43d2739-4821-48c7-bd6d-77ae3df2f392", "collapsed": true}, "source": ["X_train_tfidf = get_bag_of_words(pd.concat([df_train[\"text\"], df_test['text']]))\n", "y_train = df_train[\"author\"]\n", "X_test_tfidf = get_bag_of_words(pd.concat([df_test[\"text\"], df_train[\"text\"]]))\n", "y_pred = []\n", "\n", "clf = MultinomialNB().fit(X_train_tfidf[:len(df_train)], y_train)\n", "y_pred = clf.predict_proba(X_test_tfidf[:len(df_test)])\n", "results = pd.DataFrame({'id':df_test[\"id\"]})\n", "results[clf.classes_] = pd.DataFrame(y_pred)\n", "\n", "# For the results, I need a table like the following\n", "#\n", "# id | P(author1) | P(author2) | P(author3)\n", "#\n", "results.head()\n"], "outputs": [], "cell_type": "code"}, {"metadata": {"_uuid": "f23f06ab184a92a91dafa6dcb266d480226883e8", "_cell_guid": "bb90c5e9-7640-40b0-b26c-86844b9bc1fc"}, "source": ["## Consider K-Fold Cross Validation"], "cell_type": "markdown"}, {"execution_count": null, "metadata": {"_uuid": "4bcc050e9d17aec59bca1180e84c235030c19293", "_cell_guid": "1fc3f71c-9b2d-43f5-90a4-2045b638e90c", "collapsed": true}, "source": ["from sklearn.model_selection import KFold\n", "\n", "# Create 10 folds to test our data on\n", "kf = KFold(n_splits=10)\n", "scores=[]\n", "for train_index, test_index in kf.split(df_train):\n", "    \n", "    # Foldi Train/Test Data\n", "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n", "    \n", "    X_traini, X_testi = df_train.loc[train_index,\"text\"], df_train.loc[test_index,\"text\"]\n", "    y_traini, y_testi = df_train.loc[train_index,\"author\"], df_train.loc[test_index,\"author\"]\n", "    \n", "    # Foldi Train/Test Bag of words\n", "    X_train_tfidfi = get_bag_of_words(pd.concat([X_traini, X_testi]))\n", "    X_test_tfidfi = get_bag_of_words(pd.concat([X_testi, X_traini]))\n", "    \n", "    # Foldi Model\n", "    clfi = MultinomialNB().fit(X_train_tfidfi[:len(X_traini)], y_traini)\n", "\n", "    # Test Foldi Model on Foldi held out data\n", "    y_predi = clfi.predict(X_test_tfidfi[:len(X_testi)])\n", "    \n", "    # Append results, iterate\n", "    scores.append(accuracy_score(y_testi, y_predi))\n", "print(\"Accuracy Scores After 10-Fold Cross Validation:\")\n", "print(scores)\n", "print(\"Average Accuracyy After 10 Folds:\")\n", "print(np.mean(scores))"], "outputs": [], "cell_type": "code"}, {"execution_count": null, "metadata": {"_uuid": "80e2e07ea23841dfd140b2eed4106c311d7bbd9f", "_cell_guid": "6a41e252-8472-4348-afd6-b95a36ccccb6", "collapsed": true}, "source": [], "outputs": [], "cell_type": "code"}, {"metadata": {"_uuid": "35e6a71c5ef3aafecb782070819813a734da7ccc", "_cell_guid": "17cfbd6f-a438-4227-8913-6c7d3f6f7cb8"}, "source": ["## SVM\n", "So lets take a look at SVM. Some important things to note:\n", "<ul>\n", "<li>The Hyper Parameter `C=1.0` in our model is at default. This is fine, but the area around the hyperplane that we accept/reject from might be more predictive if we allow some wiggle room. This allows for a natural and prescribed amount of error in the training phase to mispredict, which may be helpful for the <em>real world situations</em> we will encounter (I.e., avoids overfitting).</li>\n", "<li>The Parameter `kernel=\u2019rbf\u2019 in our model is at default. This could be more predictive if we mapped into higher dimensions with different kernel functions. Also, `degree=3` is the initial, but this will tell the degree of the polynomial we wish to map to.</li>\n", "\n", "</ul\n", "<p> It turns out SVM predicts very poorly in this case! So it has been removed :) </p>"], "cell_type": "markdown"}, {"metadata": {"_uuid": "88a58fa66d94beb60cd9e9673f033e1bd8276f2a", "_cell_guid": "2441c660-4803-4371-bdcf-0909670c3e62"}, "source": ["## Parameter Tuning\n", "We will use grid search for this, which will make great use for the SVM model we just build (and our Naive Bayes model too!). So for starters, lets do a grid search on the best `alpha` value for the Naive Bayes model, then do a grid search on the best `C` value for the SVM.\n", "\n", "Obviously, such an exhaustive search can be expensive. If we have multiple CPU cores at our disposal, we can tell the grid searcher to try these eight parameter combinations in parallel with the n_jobs parameter. If we give this parameter a value of -1, grid search will detect how many cores are installed and uses them all:"], "cell_type": "markdown"}, {"metadata": {"_uuid": "0bd83f814a4d8da3c1cd96b861632ad8099b72ba", "_cell_guid": "22fdf7a8-b2bc-40ef-b6c6-55fc170e4f2d"}, "source": ["## Pipeline objects\n", "Each of the pipeline objects acts as an sklearn estimator (I.e., pipelineObject.fit(X,y)) and the coolest part about them is that you can throw them into a GridSearch CV object and it will optimize the best parameter for each layer of the pipe. Pretty cool eh?"], "cell_type": "markdown"}, {"execution_count": null, "metadata": {"_uuid": "b8334b2ab1456bf28c10de2845b3687e7975ec9d", "_cell_guid": "9fa629ee-59c6-4724-be50-126096c3e36f"}, "source": ["from sklearn.model_selection import GridSearchCV\n", "from sklearn.pipeline import Pipeline\n", "\n", "nb_parameters = {#'vect__ngram_range': [(1, 1), (1, 2),(1,3)],\n", "                 #'tfidf__use_idf': (True, False),\n", "                 'alpha': (1,0,0.01, 0.001, 0.0001)}\n", "\n", "svm_parameters = {#'vect__ngram_range': [(1, 1), (1, 2),(1,3)],\n", "                 #'tfidf__use_idf': (True, False),\n", "                 'C': (1, 10, 100, 1000),\n", "                 'kernel': ('linear', 'rbf'),\n", "                 'gamma': (0.0001, 0.001, 0.01, 0.1)}\n", "xgb_parameters = {#'vect__ngram_range': [(1, 1), (1, 2),(1,3)],\n", "                 #'tfidf__use_idf': (True, False),\n", "                 'n_estimators':[1000,1500,2000],\n", "                 'max_depth':[3],\n", "                 'subsample':[0.5],\n", "                 'learning_rate':[0.01, 0.02, 0.03, 0.04, 0.05],\n", "                 'min_samples_leaf': [1],\n", "                 'random_state': [3]}\n", "\n", "\n", "from sklearn import ensemble\n", "from sklearn.svm import SVC\n", "\n", "def getXGBPipe():\n", "    clf_xgb_pipe = Pipeline([('vect', CountVectorizer()), \n", "                              ('tfidf', TfidfTransformer()),\n", "                              ('clf', ensemble.GradientBoostingClassifier())])\n", "def getNaiveBayesPipe():\n", "    clf_nb_pipe = Pipeline([('vect', CountVectorizer()), \n", "                              ('tfidf', TfidfTransformer()),\n", "                              ('clf', MultinomialNB())])\n", "    return clf_nb_pipe\n", "\n", "\n", "def getSVMPipe():\n", "    clf_svm_pipe = Pipeline([('vect', CountVectorizer()), \n", "                              ('tfidf', TfidfTransformer()),\n", "                              ('clf', SVC())])\n", "    return clf_svm_pipe\n", "\n", "\n"], "outputs": [], "cell_type": "code"}, {"metadata": {"_uuid": "dd2dcc5b3fc66c1bddf71007ead65e5fa3a14c87", "_cell_guid": "b7b36162-c6c9-4c63-b459-b0c1e8f499c3"}, "source": ["## Grid Search for Naive Bayes Parameters\n", "\n", "## Predict on the optimal estimator"], "cell_type": "markdown"}, {"execution_count": null, "metadata": {"_uuid": "aa5d78fbe9d2e285105bff8a3d140f5acd3c13e1", "_cell_guid": "3b9dc9b1-ae2d-4f00-bcea-474d59d5a8f1"}, "source": ["gs_clf_nb = GridSearchCV(MultinomialNB(), nb_parameters, n_jobs=-1, cv=10)\n", "gs_clf_nb_fit = gs_clf_nb.fit(X_train_tfidf[:len(X_train)], y_train)\n", "best_nb_clf_fit = gs_clf_nb_fit.best_estimator_\n", "print(\"The best alpha: \", best_nb_clf_fit.alpha)\n", "y_pred = best_nb_clf_fit.predict_proba(X_test_tfidf[:len(X_test)])\n", "generate_results(y_pred, y_test)\n"], "outputs": [], "cell_type": "code"}, {"metadata": {"_uuid": "392d521eb93b33c865e06f0dc0ea3abdaa54f6a1", "_cell_guid": "d9f30c2a-e940-4ab0-9713-5fb70734309c"}, "source": ["## Submittion for the best NB"], "cell_type": "markdown"}, {"execution_count": null, "metadata": {"_uuid": "44601775c9216365d894ad809064c482e9dd65a0", "_cell_guid": "78acef98-d41d-4579-a40b-e85e16ebb22a", "collapsed": true}, "source": ["X_train_tfidf = get_bag_of_words(pd.concat([df_train[\"text\"], df_test['text']]))\n", "y_train = df_train[\"author\"]\n", "X_test_tfidf = get_bag_of_words(pd.concat([df_test[\"text\"], df_train[\"text\"]]))\n", "y_pred = []\n", "\n", "clf = best_nb_clf_fit.fit(X_train_tfidf[:len(df_train)], y_train)\n", "y_pred = clf.predict_proba(X_test_tfidf[:len(df_test)])\n", "results = pd.DataFrame({'id':df_test[\"id\"]})\n", "results[clf.classes_] = pd.DataFrame(y_pred)\n", "\n", "# For the results, I need a table like the following\n", "#\n", "# id | P(author1) | P(author2) | P(author3)\n", "#\n", "results.to_csv(\"11102017_2_bestNB.csv\")"], "outputs": [], "cell_type": "code"}, {"metadata": {"_uuid": "cf48b00a0becf0024423758c742592b7215d759f", "_cell_guid": "93123552-f35c-4ade-ae55-aa58c0285a2b"}, "source": ["## What if we could do more with our NB, like support it with other features?\n", "Consider the following: Each author as a unique person, will have a unique language, vocabulary, and choose their words uniquely.\n", "If the postulate is true, then the most unfrequent words could be predictive in revealing who is who by just their words.\n", "Does K-means reveal any secrets about 3 unique groups? Lets try!\n", "Does another 50 columns with booleans for least frequent words (and booleans to represent if we saw it or not) help identify these spooky authors? Lets try!"], "cell_type": "markdown"}, {"metadata": {"_uuid": "248b396686fb964d68331ed2d78ea14fcb3457a2", "_cell_guid": "5c445520-aef2-474f-b4f1-f5e4f031e9e6"}, "source": ["## Kmeans | k=3\n", "\n", "Create the kmeans for"], "cell_type": "markdown"}, {"execution_count": null, "metadata": {"_uuid": "7e5b4f111d1db46c1adfac2c32e8879aa15ba07e", "_cell_guid": "288f2631-c3c4-4c1c-9890-788dc1907b96"}, "source": ["from sklearn.cluster import KMeans\n", "#log_loss(y_pred, y_true)\n", "\n", "kmeans_column_train = KMeans(n_clusters=3, random_state=0).fit(X_train_tfidf[:len(X_train)]).labels_\n", "kmeans_column_test = KMeans(n_clusters=3, random_state=0).fit(X_test_tfidf[:len(X_test)]).labels_\n", "\n", "# Cast into a usable form to append as a column\n", "kmeans_column_train = np.matrix(kmeans_column_train).T\n", "kmeans_column_test = np.matrix(kmeans_column_test).T\n", "\n", "print(\"Kmeans columns created!\")"], "outputs": [], "cell_type": "code"}, {"metadata": {"_uuid": "dfa16ed29db6de1d5ec345b09625defdb82336d7", "_cell_guid": "6bae9289-64ca-4aa7-8e1c-19cc802563b4"}, "source": ["## Feature Engineering | K-means with K=3\n", "\n", "Lets append the kmeans results columns to our actually data matrices `X_train_tfidf` and `X_test_tfidf` then calculate some metrics and see if it is responsive with our best estimator above (Naive Bayes).\n", "\n", "So it looks pretty promising, lets submit it again and see what we get."], "cell_type": "markdown"}, {"execution_count": null, "metadata": {"_uuid": "32cd3be6a70904384c1c15a2705c3fe45b5e71c1", "_cell_guid": "d52557ee-91d4-4159-ab81-d105d7e31d07", "collapsed": true}, "source": ["# Append the columns\n", "X_train_raw = np.matrix(X_train_tfidf[:len(X_train)].toarray())\n", "X_test_raw = np.matrix(X_test_tfidf[:len(X_test)].toarray())\n", "\n", "X_train_new = np.concatenate((X_train_raw, kmeans_column_train), axis=1)\n", "X_test_new = np.concatenate((X_test_raw, kmeans_column_test), axis=1)\n", "\n", "clf = best_nb_clf_fit.fit(X_train_new, y_train)\n", "y_pred = clf.predict_proba(X_test_new)\n", "\n", "generate_results(y_pred, y_test)"], "outputs": [], "cell_type": "code"}, {"metadata": {}, "source": ["## Feature Engineering | PCA + Kmeans + Naive Bayes"], "cell_type": "markdown"}, {"execution_count": null, "metadata": {}, "source": ["\n", "# 1. PCA on X_train_tfidf and X_test_tfidf\n", "from sklearn.decomposition import PCA\n", "pca = PCA().fit(np.matrix(X_train_tfidf[:len(X_train)].toarray()))\n", "pca\n", "\n", "# 2. Visualize to pick the top K components that maximize variance\n", "\n", "# 3. Kmeans on our top K columns\n", "\n", "# 4. with PCA + Kmeans run NB on it\n", "\n", "# 5. Predict on NB and report out how well we did\n", "\n"], "outputs": [], "cell_type": "code"}, {"metadata": {"_uuid": "54380df89170531d28f12a81055dab1a9ac7786e", "_cell_guid": "1b51e0c3-c600-4d19-bc8e-9b57f4a0b4e2"}, "source": ["## Submit NB with Kmeans Support Column\n", "\n", "This did not actually improve the score! The clusters may not be finding the 3 authors as I suspected to begin with ... "], "cell_type": "markdown"}, {"execution_count": null, "metadata": {"_uuid": "e2927cad4d4983b4ab3f54a418532885a7b5e834", "_cell_guid": "5d137142-a8d8-42ec-bedc-513dbed8d858", "collapsed": true}, "source": ["from sklearn.cluster import KMeans\n", "\n", "X_train_tfidf = get_bag_of_words(pd.concat([df_train[\"text\"], df_test['text']]))\n", "y_train = df_train[\"author\"]\n", "X_test_tfidf = get_bag_of_words(pd.concat([df_test[\"text\"], df_train[\"text\"]]))\n", "y_pred = []\n", "print(\"Data matrix created!\")\n", "\n", "kmeans_column_train = KMeans(n_clusters=3, random_state=0).fit(X_train_tfidf[:len(df_train[\"text\"])]).labels_\n", "kmeans_column_test = KMeans(n_clusters=3, random_state=0).fit(X_test_tfidf[:len(df_test[\"text\"])]).labels_\n", "kmeans_column_train = np.matrix(kmeans_column_train).T\n", "kmeans_column_test = np.matrix(kmeans_column_test).T\n", "\n", "print(\"Kmeans columns created!\")\n", "\n", "\n", "X_train_raw = np.matrix(X_train_tfidf[:len(df_train[\"text\"])].toarray())\n", "X_train_new = np.concatenate((X_train_raw, kmeans_column_train), axis=1)\n", "X_test_raw = np.matrix(X_test_tfidf[:len(X_test)].toarray())\n", "X_test_new = np.concatenate((X_test_raw, kmeans_column_test), axis=1)\n", "print(\"Column appended!\")\n", "\n", "clf = best_nb_clf_fit.fit(X_train_new, y_train)\n", "print(\"Estimator built!\")\n", "y_pred = clf.predict_proba(X_test_new)\n", "print(\"Predictions casted!\")\n", "\n", "\n", "results = pd.DataFrame({'id':df_test[\"id\"]})\n", "results[clf.classes_] = pd.DataFrame(y_pred)\n", "\n", "# For the results, I need a table like the following\n", "#\n", "# id | P(author1) | P(author2) | P(author3)\n", "#\n", "results.to_csv(\"11102017_2_kmeans.csv\")\n", "print(\"Writing out!\")"], "outputs": [], "cell_type": "code"}, {"metadata": {"_uuid": "d91f9573969563071e3ff45d658032ac77a5ea65", "_cell_guid": "c2137e37-a2a6-4daa-bae2-4d0ae863636f"}, "source": ["## Repeat for XGB\n", "\n", "This takes quite a long time to complete .. so I will hold off for now during my experimentation stages :) "], "cell_type": "markdown"}, {"execution_count": null, "metadata": {"_uuid": "d0e644d26e21da659ff6815908b044aac1138013", "_cell_guid": "b943cd90-b703-47a1-8d45-120a977db5d2", "collapsed": true}, "source": ["gs_clf_xgb = GridSearchCV(ensemble.GradientBoostingClassifier(), xgb_parameters, n_jobs=-1, cv=3)\n", "gs_clf_xgb_fit = gs_clf_xgb.fit(np.matrix(X_train_tfidf[:len(X_train)], y_train)\n", "best_xgb_clf_fit = gs_clf_xgb_fit.best_estimator_\n", "print(\"The best xgb: \", best_nb_clf_fit)\n", "y_pred = best_xgb_clf_fit.predict_proba(X_test_tfidf[:len(X_test)])\n", "generate_results(y_pred, y_test)"], "outputs": [], "cell_type": "code"}, {"metadata": {"_uuid": "99fab9ec488b23bc6a0927876dc24d97a3eaaf08", "_cell_guid": "3c51b2e8-e4da-42ae-8933-9eb4f85f79da"}, "source": ["## Run a baseline XGB model\n", "\n", "This run takes a while, but do it next time!!"], "cell_type": "markdown"}, {"execution_count": null, "metadata": {"_uuid": "b2a99472a43e1ee72682a814643839aca5650194", "_cell_guid": "40a95288-795d-4b1f-b7c9-d9a2e402345e", "collapsed": true}, "source": ["gs_clf_xgb = ensemble.GradientBoostingClassifier().fit(np.matrix(X_train_tfidf[:len(X_train)].toarray()),\n", "                                                       y_train)\n", "print(\"Classifier fit!\")\n", "y_pred = best_xgb_clf_fit.predict_proba(np.matrix(X_test_tfidf[:len(X_test)].toarray()))\n", "print(\"Classifier predictions casted!!\")\n", "generate_results(y_pred, y_test)"], "outputs": [], "cell_type": "code"}, {"metadata": {"_uuid": "8d282a80332d81b9814b9ce220fb85e5e4e6d0c6", "_cell_guid": "26ae9731-6c12-4cd7-b6a6-3de7c6de8b45"}, "source": ["## Feature Engineering | Naive Bayes + XGB\n", "\n", "Well so far we have a pretty promising Naive Bayes model prediciting quite well. The Kmeans didn't help us out any, but maybe XGB will. My next consideration is to see the following:\n", "\n", "<ul> \n", "<li> Consider how well XGB does independently. </li>\n", "<li> Consider an average between the XGB result set and the NB result set, test to see how predictive it is.</li>\n", "\n", "</ul>"], "cell_type": "markdown"}, {"metadata": {"_uuid": "5b3c4dbc1389aff2f3f30a8a8036db3a13d2ad3a", "_cell_guid": "afe5814e-ec50-49bb-ab90-6318cd2adc68"}, "source": ["## Feature Engineering | Most Unfrequent Words\n", "\n", "Considering an assumption I made above that individuals think and rationalize uniquely (I.e., behavioral economics), their individual syntax should be unique as well. So the likelihood of two individuals choosing the exact same phrase is unlikely. So the question I want to pose is as follows: Are words that the vocabulary sees less able to identify who said them? Some things to consider for this postulate:\n", "\n", "<ul> \n", "<li> What is the count of unique words that fits an appropriate threshold? </li>\n", "<li> Should they be considered as boolean values individually appended to their columns (I.e., did this uncommon word appear in this sentence (for M new words, M new columns)?</li>\n", "\n", "</ul>"], "cell_type": "markdown"}, {"metadata": {"_uuid": "5c4cc941f05ef91acbc6edcee9525143045b9c28", "_cell_guid": "8c8e0313-f09a-49ad-bf5b-4218bc6447df"}, "source": ["## Try a XGB Model to see performance gain\n", "## Try other relavent models to see performance gain\n", "## Do an ensemble combinations to see performance gain\n", "## Feature engineering to see performance gain\n", "\n", "## To be continued .."], "cell_type": "markdown"}, {"metadata": {"_uuid": "32affba2211f4fc146bb9060a0d8301a2fc678b5", "_cell_guid": "c310628d-0838-4105-8b29-0b6485ca0079"}, "source": [], "cell_type": "markdown"}, {"execution_count": null, "metadata": {"_uuid": "a7463c6a68c6ec972ea756c98a46bcd43aa1c82f", "_cell_guid": "7468d57d-750d-4793-8414-7c9cb3e33b5c", "collapsed": true}, "source": [], "outputs": [], "cell_type": "code"}, {"execution_count": null, "metadata": {"_uuid": "86ad0717c1794d2e05434907dcd04b8b6b40c3c5", "_cell_guid": "a7096c5d-5980-4ee5-b684-e10e57a0d6b8", "collapsed": true}, "source": [], "outputs": [], "cell_type": "code"}]}