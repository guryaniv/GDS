{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \n\nimport itertools\nimport os\nimport re\n\nfrom collections import Counter\nfrom dask import delayed, compute\nfrom dask.diagnostics import ProgressBar\nfrom fuzzywuzzy import fuzz, process\nfrom IPython.core.display import display\nfrom itertools import cycle, islice\nfrom sklearn.datasets import fetch_20newsgroups\n\nProgressBar().register()\n\nchunk_size = 300\npd.options.display.max_columns = chunk_size\npd.options.display.max_rows = chunk_size","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c9d76da86e32966ab6f12469cbceff72b6adc0f1"},"cell_type":"markdown","source":"This kernel presents a full solution for cipher #4. Thank you very much to Phil and all kagglers who have made this competition so much fun.\n\n# 1. Loading the Data\n## *1.1* Loading, Preprocessing and Chunking the Plaintexts"},{"metadata":{"trusted":true,"_uuid":"2c9246a3010aa7b3a267c8a655b39d98f7306fe0"},"cell_type":"code","source":"train_p = fetch_20newsgroups(subset='train')\ntest_p = fetch_20newsgroups(subset='test')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cc3e375d7dec76cc41387fbf0a7de0c77af26bae"},"cell_type":"code","source":"df_p = pd.concat([pd.DataFrame(data = np.c_[train_p['data'], train_p['target']],\n                                   columns= ['text','target']),\n                      pd.DataFrame(data = np.c_[test_p['data'], test_p['target']],\n                                   columns= ['text','target'])],\n                     axis=0).reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"355a1093b6a1ff2d5aa0790d2b32496af30310e4"},"cell_type":"code","source":"df_p['target'] = df_p['target'].astype(np.int8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"087b354a917bc03d0e6422ded725c4289b032c0f"},"cell_type":"code","source":"df_p['text'] = df_p['text'].map(lambda x: x.replace('\\r\\n','\\n').replace('\\r','\\n').replace('\\n','\\n '))\ndf_p.loc[df_p['text'].str.endswith('\\n '),'text'] = df_p.loc[df_p['text'].str.endswith('\\n '),'text'].map(lambda x: x[:-1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8f1268cd9c752ce205fb767c9f32e4daf2094f1e"},"cell_type":"code","source":"p_text_chunk_list = []\np_text_index_list = []\n\nfor p_index, p_row in df_p.iterrows():\n    p_text = p_row['text']\n    p_text_len = len(p_text)\n    if p_text_len > chunk_size:\n        for j in range(p_text_len // chunk_size):\n            p_text_chunk_list.append(p_text[chunk_size*j:chunk_size*(j+1)])\n            p_text_index_list.append(p_index)\n        if p_text_len%chunk_size > 0:\n            p_text_chunk_list.append(p_text[chunk_size*(p_text_len // chunk_size):(chunk_size*(p_text_len // chunk_size)+p_text_len%chunk_size)])\n            p_text_index_list.append(p_index)\n    else:\n        p_text_chunk_list.append(p_text)\n        p_text_index_list.append(p_index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0f838961d96a5e76629d75a3d3322ad11118b8d2"},"cell_type":"code","source":"df_p_chunked = pd.DataFrame({'text' : p_text_chunk_list, 'p_index' : p_text_index_list})\ndf_p_chunked = pd.merge(df_p_chunked, df_p.reset_index().rename(columns={'index' : 'p_index'})[['p_index','target']],on='p_index',how='left')\n\ndf_p_chunked_list = []\nfor i in np.sort(df_p_chunked['target'].unique()):\n    df_p_chunked_list.append(df_p_chunked[df_p_chunked['target'] == i])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"59e0e9b94d0a0962b37c47d647e3af94c5b5af56"},"cell_type":"markdown","source":"## *1.2* Loading the Competition Train and Test Sets"},{"metadata":{"trusted":true,"_uuid":"71ce129bfcd2838d5668ba4df37ef2875aad5de4"},"cell_type":"code","source":"competition_path = '../input/20-newsgroups-ciphertext-challenge/'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d2ab6bb688063ffa2fef60965e86f5b77a539794"},"cell_type":"code","source":"train = pd.read_csv(competition_path + 'train.csv').rename(columns={'ciphertext' : 'text'})\ntest = pd.read_csv(competition_path + 'test.csv').rename(columns={'ciphertext' : 'text'})","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"685808fd539215b9db45647157790a48ddca0ce8"},"cell_type":"markdown","source":"## *1.3* Loading the Cipher #3 Encryption and Decryption Functions"},{"metadata":{"trusted":true,"_uuid":"cf1ff5b1077b5908ecdc78d6ebcbbf11636490c3"},"cell_type":"code","source":"cipher_path = '../input/cipher-1-cipher-2-full-solutions/'\n\ncipher2_map = pd.read_csv(cipher_path + '/cipher2_map.csv')\n\ncipher2_map = pd.concat([cipher2_map,pd.DataFrame(data=[['D','\\x10']],columns=['cipher','plain'])],axis=0,ignore_index=True)\n#Cheating a bit with this update, the cipher character D has to be added to the cipher_2 map because a cipher #4 text, it would have appeared as missing in our decryption matching at the end of this kernel\n\ntranslation_2_pt = str.maketrans(''.join(cipher2_map['plain']),''.join(cipher2_map['cipher'])) # cipher #2 encryption\ntranslation_2_ct = str.maketrans(''.join(cipher2_map['cipher']), ''.join(cipher2_map['plain'])) # cipher #2 decryption\n\ndef shift_char(c,shift):\n    if c.islower():\n        return(chr((ord(c) - ord('a') + shift) % 26 + ord('a')))\n    else:\n        return(chr((ord(c) - ord('A') + shift) % 26 + ord('A')))\n\ndef replace_alpha(l,l_alpha_new):\n    res = []\n    i_alpha = 0\n    for i in range(len(l)):\n        if l[i].isalpha():\n            res.append(l_alpha_new[i_alpha])\n            i_alpha += 1\n        else:\n            res.append(l[i])\n    return(res)\n\ndef fractional_vigenere(s,key):\n    l = list(s)\n    l_alpha = [x for x in l if x.isalpha()]\n    l_alpha_shifted = [shift_char(c,-shift) for c, shift in zip(l_alpha,list(islice(cycle(key), len(l_alpha))))]\n    return(''.join(replace_alpha(l,l_alpha_shifted)))\n\nkey_ord_3 = [7, 4, 11, 4, 13, -1, 5, 14, 20, 2, 7, 4, -1, 6, 0, 8, 13, 4, 18] # cipher #3 decryption key\nkey_ord_3_n = [-x for x in key_ord_3] # cipher #3 encryption key","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"78e61d5c5f5377a088e65cb0fd56999696fe16f5"},"cell_type":"markdown","source":"# *2* Decrypting Cipher #4\n## *2.1* Looking at Character Frequencies"},{"metadata":{"trusted":true,"_uuid":"c8aece69370fb0fdafce55bacbe69153a204bad1"},"cell_type":"code","source":"p_counts = pd.Series(Counter(''.join(df_p['text']))).rename(\"counts\").to_frame().sort_values(\"counts\", ascending = False)\np_counts = 1000000 * p_counts / p_counts.sum()\np_counts = p_counts.reset_index().rename(columns = {\"index\":\"p_char\"})\n\nc_counts = []\nfor i in range(1,5):\n    counts = pd.Series(Counter(''.join(pd.concat([train[train['difficulty'] == i][['text']],test[test['difficulty'] == i][['text']]],axis=0)['text']))).rename('counts').to_frame().sort_values('counts', ascending = False)\n    counts = 1000000 * counts / counts.sum()\n    counts = counts.reset_index().rename(columns = {'index':'c_{}_char'.format(i)})\n    c_counts.append(counts)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"703260f3a39a11e38898f0f97bf18d06b4fc5f73"},"cell_type":"code","source":"pd.concat([p_counts] + c_counts, axis = 1).head(20)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"022d7f038e1e88b0cc1a4182bd538c65236f56df"},"cell_type":"markdown","source":"Since character frequencies are almost equal for cipher #3 and cipher #4, it seems plausible that cipher #4 is merely a transposition. \nTo identify this transposition, we shall generate a crib by matching plaintexts and ciphertexts by target and length and character counts."},{"metadata":{"_uuid":"14f2f3652361ea955d88f9c97eb62e526e0feed8"},"cell_type":"markdown","source":"## *2.2* Generating and Using a Crib (matched plaintexts and ciphertexts)"},{"metadata":{"trusted":true,"_uuid":"2b1ddd7454eea68d0f0f5db1c173fc162102ef2b"},"cell_type":"code","source":"df_p_chunked['text_len'] = df_p_chunked['text'].map(len) \ndf_p_chunked['pt_text'] = df_p_chunked['text'].map(lambda x: fractional_vigenere(x.translate(translation_2_pt),key_ord_3_n))\ndf_p_chunked['pt_counter'] = df_p_chunked['pt_text'].map(lambda x: Counter(x))\ndf_p_chunked = df_p_chunked.reset_index().rename(columns={'index' : 'p_chunk_index'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"67eace5ce844afbe0995f960ff65935ac2b323cc"},"cell_type":"code","source":"difficulty_level = 4\n\ntrain = train[train['difficulty'] == difficulty_level]\ntrain['text_len'] = train['text'].map(len) \n\ntest = test[test['difficulty'] == difficulty_level]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"72e686f293ce270dbd09d162b19a9f8e4e15d4a0"},"cell_type":"code","source":"res = []\nfor i in train.index[:]:\n    c_text = train.loc[i]['text']\n    c_target = train.loc[i]['target']\n    c_len = train.loc[i]['text_len']\n    c_counter = Counter(c_text)\n    p_chunk_indexes = list(df_p_chunked[(df_p_chunked['text_len'] == c_len) & (df_p_chunked['target'] == c_target) & (df_p_chunked['pt_counter'] == c_counter)]['p_chunk_index'].values)\n    if len(p_chunk_indexes) == 1:\n        res.append((i,p_chunk_indexes[0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fd1c44b640bc82a9c6239a57b7f100065a7eb6c5"},"cell_type":"code","source":"df_crib2 = pd.DataFrame(res,columns=['train_index','p_chunk_index'])\ndf_crib2['c_text'] = df_crib2['train_index'].map(lambda x: train.loc[x,'text'])\ndf_crib2['pt_text'] = df_crib2['p_chunk_index'].map(lambda x: df_p_chunked.loc[x,'pt_text'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ef516624197b4bdc3a0fe500e0e54ffc53d43044"},"cell_type":"code","source":"df_crib2['text_len'] = df_crib2['c_text'].map(len)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6c57af8f671310b4b4d96b168c8435ca98fdf43e"},"cell_type":"markdown","source":"Now we look at each ciphertext&plaintext pair and infer the transposition using characters in the message that have a count equal to 1."},{"metadata":{"trusted":true,"_uuid":"68ec0550c21baed2ebf8643857a4b45ce70a27a7"},"cell_type":"code","source":"def trans_mapper(x):\n    pt_text = x['pt_text']\n    c_text = x['c_text']\n    \n    pt_series = pd.Series(Counter(pt_text)).rename('pt_counts').to_frame().sort_values('pt_counts', ascending = False)\n    c_series = pd.Series(Counter(c_text)).rename('c_counts').to_frame().sort_values('c_counts', ascending = False)\n    ptc_series = pd.merge(pt_series, c_series, left_index=True,right_index=True,how='outer')\n\n    if len(ptc_series[ptc_series['pt_counts'] != ptc_series['c_counts']]) > 0:\n        return(np.nan)\n\n    trans_map = ptc_series[ptc_series['pt_counts'] == 1]\n    trans_map = trans_map.reset_index().rename(columns={'index' : 'char'})\n    trans_map['p_char_index'] = trans_map['char'].map(lambda x: pt_text.find(x))\n    trans_map['c_char_index'] = trans_map['char'].map(lambda x: c_text.find(x))\n\n    return(dict(zip(trans_map['p_char_index'].values,trans_map['c_char_index'].values)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7ff59f61d8c90e8e06f412440249edfa9ede64b7"},"cell_type":"code","source":"df_crib2['trans_map'] = df_crib2.apply(lambda x: trans_mapper(x),axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7857976abcf17f4e7aa8a24909a847e15ae3b45c"},"cell_type":"markdown","source":"If we focus on the cipher &  plaintexts of length 300 (the most frequent ones):"},{"metadata":{"trusted":true,"_uuid":"38fb3ddc0c7dfe1773629692ce9ea1e6ad7e234e"},"cell_type":"code","source":"trans_len = 300\ntrans_dict = {}\nfor i in range(trans_len):\n    temp = set()\n    for j in df_crib2[df_crib2['text_len'] == trans_len].index:\n        t_map = df_crib2.loc[j,'trans_map']\n        if i in t_map:\n            temp = temp.union([t_map[i]])\n    trans_dict[i] = temp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d6f8461bc9eb9641b68bfe576b62dbb64feac9ba"},"cell_type":"code","source":"trans_s = pd.Series(trans_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"042a2a4e2c84f75eec2626636245de21008476d9"},"cell_type":"code","source":"print(trans_s.map(len).min())\nprint(trans_s.map(len).max())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7cd498124824d7971f3e65c898c9397445daec1c"},"cell_type":"code","source":"trans_s = trans_s.map(lambda x: list(x)[0] if len(list(x))>0 else np.nan)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8d90a2d051fe1619ffd54bd510ba9f5211827525"},"cell_type":"code","source":"def rowcol(i,n_cols):\n    row = i // n_cols\n    col = i % n_cols\n    return((row,col))\n\ndef draw_square(n,n_cols):\n    n_rows = rowcol(n,n_cols)[0]\n    df = pd.DataFrame(data=[[-1]*n_cols]*n_rows,index=range(n_rows),columns=range(n_cols))\n    for i in range(n):\n        df.loc[rowcol(i,n_cols)] = i\n    return(df)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"aa515c70653cfca37038cbd2efe226edc82c3933"},"cell_type":"markdown","source":"Now if we look closer at this transposition, we can see a pattern:\n* The plaintext is written in rows of 24 columns (since the plaintext is of length 300, there is at most 12 such rows)\n* Then it is read \n    * by column 0 from top to bottom\n    * then by alternating reading from top to bottom in columns j and 24-j for j between 1 and 11\n    * and finally by column 12 from top to bottom     "},{"metadata":{"trusted":true,"_uuid":"6eee310153bccf33a2eb24fcea68a6b457d7c9fc"},"cell_type":"code","source":"trans_s.rename('c_char_index').to_frame().reset_index().rename(columns={'index':'p_char_index'}).sort_values(by='c_char_index').T.style.format(\"{:.0f}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6d7d1946265231837f5725b23b6fbba9b69ba9a8"},"cell_type":"code","source":"draw_square(300,24).style.format(\"{:.0f}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9d4aee70e3717ecb76e13f6918abbf6878dc9abb"},"cell_type":"code","source":"def read_col(col,square):\n    n_rows = len(square)\n    res = []\n    for i in range(n_rows):\n        x_col = square.loc[i,col]\n        if ~np.isnan(x_col):\n            res = res + [x_col]\n    return(res)\n\ndef alternate_cols(col_1,col_2,square):\n    n_rows = len(square)\n    res = []\n    for i in range(n_rows):\n        x_col1 = square.loc[i,col_1]\n        x_col2 = square.loc[i,col_2]\n        if ~np.isnan(x_col1):\n            res = res + [x_col1]\n        if ~np.isnan(x_col2):\n            res = res + [x_col2]\n    return(res)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d99dab868217191e1befcecae3bff8931cbce275"},"cell_type":"code","source":"encipher_trans_dict = {}\ndecipher_trans_dict = {}\nfor i in range(1,301):\n    df = draw_square(i,24)\n    res = []\n    res = res + read_col(0,df)\n    for j in range(1,12):\n        res = res + alternate_cols(j,24-j,df)\n    res = res + read_col(12,df)\n    encipher_trans_dict[i] = res\n    decipher_trans_dict[i] = np.argsort(res)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"48769867847a6a29666c337bc25751288c85e4af"},"cell_type":"code","source":"def encipher4(p_text):\n    p_len = len(p_text)\n    res = encipher_trans_dict[p_len]\n    return(''.join([p_text[int(res[i])] for i in range(p_len)]))\n\ndef decipher4(c_text):\n    c_len = len(c_text)\n    res = decipher_trans_dict[c_len]\n    return(''.join([c_text[int(res[i])] for i in range(c_len)]))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"85c5b4484b89aba418c4447a3ae58ecf1a0951aa"},"cell_type":"markdown","source":"# 3. Matching Ciphertexts and Plaintexts\n## *3.1.* For the Train Set"},{"metadata":{"trusted":true,"_uuid":"b0a6976c4895d68ed590a25638d8a049106ed2ac"},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0376b049169caf2cd2aea96d9539d5b96e158bd0"},"cell_type":"code","source":"train['ct_text'] = train['text'].map(lambda x: fractional_vigenere(decipher4(x),key_ord_3).translate(translation_2_ct))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"15b3c7aa7c28f9d4313077b7fe8851fc0e8217b6"},"cell_type":"code","source":"target_list = np.sort(df_p_chunked['target'].unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e000218acfe68cf3c8bb735b9b79f9394ac26682"},"cell_type":"code","source":"p_indexes_dict = {}\nfor i in target_list[:]:\n    df = df_p_chunked_list[i]\n    for j in train[train['target'] == i].index[:]:\n        ct_text = train.loc[j,'ct_text']\n        new_p_indexes = set(df[df['text'] == ct_text]['p_index'])\n        if len(new_p_indexes) > 0:\n            p_indexes_dict[j] = p_indexes_dict.get(j,set()).union(new_p_indexes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c22f10c1c7e449a0590174214909c8f89c17f93f"},"cell_type":"code","source":"train_p_indexes = pd.DataFrame(pd.Series(data=list(p_indexes_dict.values()), index = p_indexes_dict.keys(),dtype=object)).rename(columns={0:'p_indexes'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"633ef61c574dfea6cdf2504bf30ac012b0a3deb2"},"cell_type":"code","source":"print(train.shape[0])\nprint(train_p_indexes.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"3bebf1a1eb76e85274e5fb5b0af00ee255170470"},"cell_type":"code","source":"train = train.join(train_p_indexes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c9e7be1f388cad3474a03120dbf45d7c2f7dc99c"},"cell_type":"code","source":"train.to_pickle('train_4.pkl')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"de88e6d2241a19dfe697ec9cb28b25944a4a5e94"},"cell_type":"markdown","source":"## *3.2* For the Test Set"},{"metadata":{"trusted":true,"_uuid":"b2615714e66b4dd9e611878eb7b02faa4a936e1b"},"cell_type":"code","source":"test['ct_text'] = test['text'].map(lambda x: fractional_vigenere(decipher4(x),key_ord_3).translate(translation_2_ct))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a2b02390337b82e81ef3094a760bde1e25e13c82"},"cell_type":"code","source":"p_indexes_dict = {}\nfor i in target_list[:]:\n    df = df_p_chunked_list[i]\n    for j in test.index[:]:\n        t_text = test.loc[j,'ct_text']\n        new_p_indexes = set(df[df['text'] == ct_text]['p_index'])\n        if len(new_p_indexes) > 0:\n            p_indexes_dict[j] = p_indexes_dict.get(j,set()).union(new_p_indexes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"02cea8869a22d40df0e321ab2e1e61c9a65bfb0f"},"cell_type":"code","source":"test_p_indexes = pd.DataFrame(pd.Series(data=list(p_indexes_dict.values()), index = p_indexes_dict.keys(),dtype=object)).rename(columns={0:'p_indexes'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"02e1ec7848ba8c26879388b243391548915b185f"},"cell_type":"code","source":"print(test.shape[0])\nprint(test_p_indexes.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1f38dc8d0a787e3ab2447574233f5980462aec05"},"cell_type":"code","source":"test = test.join(test_p_indexes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"904cabf31dafe0778f379b18e53faa6b69836181"},"cell_type":"code","source":"test.to_pickle('test_4.pkl')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}