{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "a275b738-d038-b900-7cb2-7cc23b09b56a"
      },
      "source": [
        "## I read other Kaggle scripts claiming 99% plus accuracy. \n",
        "##To be clear, accuracy is not the correct metric in this case. ##\n",
        "***Reasons:***\n",
        "\n",
        " 1. There are very less fraud transactions. Any model will learn on the non-fraud transactions and predict with more that 90% accuracy.\n",
        "\n",
        "\n",
        "----------\n",
        "\n",
        "\n",
        "2 In banking its more important to not to miss out a fraud transaction. So the focus needs to be on this metric. So the accuracy would be (true frauds detected)/(total frauds predicted by model).\n",
        "\n",
        "Lets see now, if Naive Bayes is really naive and how it does against fraud. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "082e4461-4658-d496-bf2a-d06f5b181112"
      },
      "outputs": [],
      "source": [
        "import numpy as np # linear algebra\n",
        "from math import log\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "from subprocess import check_output\n",
        "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n",
        "import seaborn as sns\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d3c060fa-0c57-ac90-f5db-ae5e33365705"
      },
      "outputs": [],
      "source": [
        "credit_data=pd.read_csv('../input/creditcard.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "a9ebc308-bf48-cc33-1db7-69885ddd57bc"
      },
      "outputs": [],
      "source": [
        "credit_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "cacf825a-c2de-2b03-50af-0b113ff69303"
      },
      "outputs": [],
      "source": [
        "credit_data['Time_in_hours']=credit_data['Time']/3600\n",
        "credit_data['Log_Amount']=np.log(credit_data['Amount']+1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "23973012-31b9-62fe-dba8-aef3af3543f0"
      },
      "source": [
        "Looking into the time variable and looking what were the peak hours of transactions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "218870de-0833-2562-862d-2949fac4c630"
      },
      "outputs": [],
      "source": [
        "sns.plt.hist(data=credit_data,x='Time_in_hours')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "a1c4a715-9f6f-3bff-8e31-eb8ecba8dde8"
      },
      "source": [
        "***The trend seems fair. There are lower number of transactions during the start of the days and transactions increase later on which seems reasonable. Do fraud transactions have a pattern as such at which time there is increase in fraudulent attempts?***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "41d88d19-fdaa-a3dc-1c3c-82736896071d"
      },
      "outputs": [],
      "source": [
        "sns.FacetGrid(data=credit_data[credit_data['Class']==1],col='Class').map(sns.plt.hist,'Time_in_hours')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "2ec1f76f-3b06-8799-6406-ea42aa203d56"
      },
      "source": [
        "There is no reasonable inference that can be made about the pattern of fraudulent transactions regarding time. We'll see whether to keep this variable or drop it. Lets see if Amount variable has some insights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ce30a70a-c71d-d550-d79f-d9d11dceb34c"
      },
      "outputs": [],
      "source": [
        "plt.figure(1)\n",
        "plt.subplot(211)\n",
        "sns.plt.hist(data=credit_data[credit_data['Class']==0],x='Amount',label='Normal Transactions')\n",
        "plt.legend(loc='best')\n",
        "plt.subplot(212)\n",
        "sns.plt.hist(data=credit_data[credit_data['Class']==1],x='Amount',label='Fraud Transactions')\n",
        "plt.legend(loc='best')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "8c5890c3-6b34-7777-7871-48dce08aaf76"
      },
      "outputs": [],
      "source": [
        "#Function to find in the correlation matrix the values which are of significant use for us.\n",
        "def is_high(x):\n",
        "    for i in range(len(x)):\n",
        "        if (x.iloc[i]<0.5 and x.iloc[i]>-0.5):\n",
        "            x.iloc[i]=0\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "961e85bf-186d-f459-dcfc-97bdffcc5448"
      },
      "outputs": [],
      "source": [
        "a=credit_data.corr(method='spearman')\n",
        "a=a.apply(is_high)\n",
        "#To display the whole correlation matrix\n",
        "pd.options.display.max_columns = 50\n",
        "a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "fbf46ea8-f1ba-de8e-e6d4-df4669350ac0"
      },
      "outputs": [],
      "source": [
        "#To split the data into test and train dataset.\n",
        "def split_data(dataset,ratio):\n",
        "    sample=np.random.rand(len(dataset))<ratio\n",
        "    return(dataset[sample],dataset[~sample])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "d7069d75-e029-7b13-3580-13ba415efd53"
      },
      "source": [
        "Getting the list of all the columns in the credits dataset. So to begin I will initially add all the variables to the model and remove one by one variable and see removing which ones increases the accuracy the most. Also I read in other Kaggle scripts claiming 99% plus accuracy. \n",
        "To be clear, accuracy is not the correct metric in this case. Reasons:\n",
        "\n",
        " 1. There are very less fraud transactions. Any model will learn on the non-fraud transactions and predict with more that 90% accuracy.\n",
        "2. In banking its more important to not to miss out a fraud transaction. So the focus needs to be on this metric. So the accuracy would be (true frauds detected)/(total frauds predicted by model).\n",
        "\n",
        "Lets see now, if Naive Bayes is really naive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "02c6ec42-c0be-0604-f3be-3fef71ef1324"
      },
      "outputs": [],
      "source": [
        "col=list(credit_data.columns.values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "96462c68-ae15-159e-b147-721e21d2d6ec"
      },
      "outputs": [],
      "source": [
        "#Function to classify based on Naive Bayes. The algorithm runs 10 times and gives the mean of \n",
        "#predicted accuracy for each time.And it also tell which variable I removed from the total variable\n",
        "#list so that I come to know which ones have to be removed.\n",
        "def NB_Classify(ratio,drop_var):\n",
        "    print('You dropped:',drop_var)\n",
        "    #print (train.groupby('Class').count()['V1'])\n",
        "    #print (test.groupby('Class').count()['V1'])\n",
        "    pred_acc=[]\n",
        "    for i in range(10):\n",
        "        train,test=split_data(credit_data,ratio)\n",
        "        clf=GaussianNB()\n",
        "        clf.fit(train.drop(drop_var,axis=1),train['Class'])\n",
        "        pred=clf.predict(test.drop(drop_var,axis=1))\n",
        "        #print(pd.crosstab(test['Class'],pred))\n",
        "        #print('You dropped:',drop_var)\n",
        "        #print(accuracy_score(test['Class'],pred))\n",
        "        pred_acc.append([pd.crosstab(test['Class'],pred).iloc[1,1]/(pd.crosstab(test['Class'],pred).iloc[1,0]+pd.crosstab(test['Class'],pred).iloc[1,1])])\n",
        "    #' and got an accuracy of: ',np.mean(pred_acc)) \n",
        "    print(np.mean(pred_acc))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ddd78207-978f-20ac-abb7-83bcf8dabc4b"
      },
      "outputs": [],
      "source": [
        "for var in col:\n",
        "    NB_Classify(0.6,['Class','Log_Amount',var])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "679dcb86-5f96-9e21-4c02-b52ea99f91be"
      },
      "source": [
        "**As it can be seen above time is not a good predictor in this case and this clearly supports our inference from the histograms that time had nothing to do with the fraud transactions. So on removing time from the list of predictors**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "cbbd4e8f-c689-fc32-7790-515b6cf5a65f"
      },
      "outputs": [],
      "source": [
        "NB_Classify(0.6,['Class','Time','Log_Amount'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "64d6ee84-422b-8f1b-28b0-b24d89da4b8a"
      },
      "source": [
        "## So Naive Bayes classifier is able to detect more than 80% of the fraud transactions. This is good, given the simplicity of the algorithm and its pretty basic and not hardware hungry. ##"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "00237ef4-2756-1f95-e442-409e04a7206e"
      },
      "source": [
        "***Next step would be to increase the accuracy of Naive Bayes but I think it did pretty fine. So will move on to some other classifier such as SVM and see how it performs compared to Naive Bayes.***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "5f1374b8-4cb4-affb-bcc4-44884d076d67"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}