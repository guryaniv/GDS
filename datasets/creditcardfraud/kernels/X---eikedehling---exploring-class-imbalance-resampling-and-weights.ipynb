{"cells": [{"metadata": {"collapsed": false, "_execution_state": "idle", "_cell_guid": "a75702fd-08e7-461b-9db6-4ff72cff966a", "_uuid": "ae261fbb7906f7ba842dd9b45f993243469b9fb6"}, "execution_count": null, "cell_type": "markdown", "outputs": [], "source": "In this notebook i'm going to explore class imbalance, how that affects a classification algorithm. Then i show two strategies for dealing with the imbalance and how to tune them.\n\nMainly this was quite interesting for myself to learn how this works :-)\n\nWe start by loading and scaling data, and creating training and test set"}, {"metadata": {"trusted": false, "_execution_state": "idle", "_cell_guid": "450e9a24-65a8-4783-aad9-f2fd5adc17ff", "_uuid": "f88ec2e776500add32df34a1d61f5dc06b526e46"}, "execution_count": null, "cell_type": "code", "outputs": [], "source": "import numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.model_selection import train_test_split\n\ndata = pd.read_csv('../input/creditcard.csv')\n\n# Separata data into X/y\ny = data['Class'].values\nX = data.drop(['Class', 'Time'], axis=1).values\n\nnum_neg = (y==0).sum()\nnum_pos = (y==1).sum()\n\n# Scaling..\nscaler = RobustScaler()\nX = scaler.fit_transform(X)\n\n# Split into train/test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"}, {"execution_count": null, "source": "First, let's summarize the data a bit. Most important is the class distribution as we'll see further down.\n\nThere is very few fraud cases (class=1) vs very many non-fraud cases (class=0)", "metadata": {"collapsed": false, "_execution_state": "idle", "_cell_guid": "c3df7748-9157-46e1-94ad-8e32c359d617", "_uuid": "caf1d1d67b93253088f7c671905c566a76c1dfe4"}, "outputs": [], "cell_type": "markdown"}, {"execution_count": null, "source": "import seaborn as sns\n\nprint(data.groupby('Class').size())\n\nsns.countplot(x=\"Class\", data=data)", "metadata": {"trusted": false, "collapsed": false, "_execution_state": "idle", "_cell_guid": "bc80e534-b7d5-4a95-9c00-c109aedb80e0", "_uuid": "aff6384f51c4f07c576d69e47ccf2091896e7a15"}, "outputs": [], "cell_type": "code"}, {"metadata": {"collapsed": false, "_execution_state": "idle", "_cell_guid": "d17d0805-17ab-4827-b096-c49cf0922351", "_uuid": "50b601d991aaad5b706992431962e9243920f7a4"}, "execution_count": null, "cell_type": "markdown", "outputs": [], "source": "First attempt at prediction credit card fraud : Use a plain logistic regression.\n\nThis fails pretty bad, only part of the fraud is detected and there is many false fraud reports.\n\nClearly the simple logistic regression is not good enough..."}, {"metadata": {"trusted": false, "collapsed": false, "_execution_state": "idle", "_cell_guid": "142fabae-80bb-4428-b53d-5400524de1e2", "_uuid": "38b5786b2a89204aa05b7193c02ac68ade5bc654"}, "execution_count": null, "cell_type": "code", "outputs": [], "source": "from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom mlxtend.plotting import plot_decision_regions, plot_confusion_matrix\nfrom matplotlib import pyplot as plt\n\nlr = LogisticRegression()\n\n# Fit..\nlr.fit(X_train, y_train)\n\n# Predict..\ny_pred = lr.predict(X_test)\n\n# Evaluate the model\nprint(classification_report(y_test, y_pred))\nplot_confusion_matrix(confusion_matrix(y_test, y_pred))"}, {"metadata": {"collapsed": false, "_execution_state": "idle", "_cell_guid": "b34b57c5-022d-48f3-85ef-820a98d7f59a", "_uuid": "445b8d6f44c9ae05b6599513c936bfbb31079923"}, "execution_count": null, "cell_type": "markdown", "outputs": [], "source": "One possibility is to tell the logistic regression there is class-imbalance and to put weights on errors proportional to the class imbalance. Documentation suggesets that should help..\n\nHowever this ends up tipping the scale in the other wrong direction: Almost all fraud is detected, but there is way to many false negatives..."}, {"metadata": {"trusted": false, "collapsed": false, "_execution_state": "idle", "_cell_guid": "63334698-7441-4e25-8cd7-7b845d3d87c3", "_uuid": "16d13c1aa224b5ac3948558cfd765a11818afa60"}, "execution_count": null, "cell_type": "code", "outputs": [], "source": "lr = LogisticRegression(class_weight='balanced')\n\n# Fit..\nlr.fit(X_train, y_train)\n\n# Predict..\ny_pred = lr.predict(X_test)\n\n# Evaluate the model\nprint(classification_report(y_test, y_pred))\nplot_confusion_matrix(confusion_matrix(y_test, y_pred))"}, {"metadata": {"collapsed": false, "_execution_state": "idle", "_cell_guid": "c1a4c899-0565-404d-a7a5-fabc7be1094b", "_uuid": "d8f0a2bbcbdfd648aa6e3c033fb42a7b1cc65549"}, "execution_count": null, "cell_type": "markdown", "outputs": [], "source": "We can also tune the class weights manually to find a better trade-off between false positives, false negatives and detected fraud cases. The F1 score is a metric that attempts to take that tradeoff.\n\nBelow we explore the effect of weighting on F1 score to figure out the optimum."}, {"metadata": {"trusted": false, "collapsed": false, "_execution_state": "idle", "_cell_guid": "15dcfc83-d465-4471-9358-5bc355bb4528", "_uuid": "1bf827518f3842621397f12e1497e456226343da"}, "execution_count": null, "cell_type": "code", "outputs": [], "source": "from sklearn.model_selection import GridSearchCV\n\nweights = np.linspace(0.05, 0.95, 20)\n\ngsc = GridSearchCV(\n    estimator=LogisticRegression(),\n    param_grid={\n        'class_weight': [{0: x, 1: 1.0-x} for x in weights]\n    },\n    scoring='f1',\n    cv=3\n)\ngrid_result = gsc.fit(X, y)\n\nprint(\"Best parameters : %s\" % grid_result.best_params_)\n\n# Plot the weights vs f1 score\ndataz = pd.DataFrame({ 'score': grid_result.cv_results_['mean_test_score'],\n                       'weight': weights })\ndataz.plot(x='weight')"}, {"metadata": {"collapsed": false, "_execution_state": "idle", "_cell_guid": "2ee38c2a-caca-45d8-b3eb-3ce29dddcedf", "_uuid": "23b2a6d544605cfe3f1ca8378286d5942b7e16d1"}, "execution_count": null, "cell_type": "markdown", "outputs": [], "source": "Now we create a logistic regression with the optimum parameters we discovered above and plot results again. This version reseults in a more balanced tradeoff between false positives, false negatives and finding fraud cases."}, {"metadata": {"trusted": false, "collapsed": false, "_execution_state": "idle", "_cell_guid": "bd3fa686-c0d7-4ddd-98fa-bc3cee0e70b8", "_uuid": "7eb2348e13c49b9c40bea2e25f9fe9346d5001d7"}, "execution_count": null, "cell_type": "code", "outputs": [], "source": "lr = LogisticRegression(**grid_result.best_params_)\n\n# Fit..\nlr.fit(X_train, y_train)\n\n# Predict..\ny_pred = lr.predict(X_test)\n\n# Evaluate the model\nprint(classification_report(y_test, y_pred))\nplot_confusion_matrix(confusion_matrix(y_test, y_pred))"}, {"metadata": {"collapsed": false, "_execution_state": "idle", "_cell_guid": "e8d852de-0540-4ade-acf3-59d56145ba49", "_uuid": "9caaca8e8a52eef03dc6b7d10cca110cc5bca4fa"}, "execution_count": null, "cell_type": "markdown", "outputs": [], "source": "Another approach is to re-sample the data to balance the positive/negatives classes. This should result in similar as the weighting. Let's see what this does:"}, {"metadata": {"trusted": false, "collapsed": false, "_execution_state": "idle", "_cell_guid": "889eb5a7-5422-4e5f-989b-7975750feae9", "_uuid": "529978c2e1351ddcb8165dff3c7dc71904fb1f68"}, "execution_count": null, "cell_type": "code", "outputs": [], "source": "from imblearn.over_sampling import SMOTE\nfrom imblearn.pipeline import make_pipeline\n\npipe = make_pipeline(\n    SMOTE(),\n    LogisticRegression()\n)\n\n# Fit..\npipe.fit(X_train, y_train)\n\n# Predict..\ny_pred = pipe.predict(X_test)\n\n# Evaluate the model\nprint(classification_report(y_test, y_pred))\nplot_confusion_matrix(confusion_matrix(y_test, y_pred))"}, {"metadata": {"collapsed": false, "_execution_state": "idle", "_cell_guid": "aab3dcbd-acb5-4175-8aa0-73277fca63eb", "_uuid": "7552fe8fc9f1413d2144211a5bf32b1def728601"}, "execution_count": null, "cell_type": "markdown", "outputs": [], "source": "So the SMOTE approach suffers a similar issue as the auto-balancing of weights by sklearn LogisticRegression. It results in many false fraud reports.\n\nWe can tune SMOTE re-sampling and achieve a similar effect..."}, {"metadata": {"trusted": false, "collapsed": false, "_execution_state": "idle", "_cell_guid": "39c55ddb-29a5-414c-b3ce-7f730f03df59", "_uuid": "e1fa354c46fa7e2718b13cbcf5c62ded40558e5f"}, "execution_count": null, "cell_type": "code", "outputs": [], "source": "import warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n\npipe = make_pipeline(\n    SMOTE(),\n    LogisticRegression()\n)\n\nweights = np.linspace(0.005, 0.05, 10)\n\ngsc = GridSearchCV(\n    estimator=pipe,\n    param_grid={\n        #'smote__ratio': [{0: int(num_neg), 1: int(num_neg * w) } for w in weights]\n        'smote__ratio': weights\n    },\n    scoring='f1',\n    cv=3\n)\ngrid_result = gsc.fit(X, y)\n\nprint(\"Best parameters : %s\" % grid_result.best_params_)\n\n# Plot the weights vs f1 score\ndataz = pd.DataFrame({ 'score': grid_result.cv_results_['mean_test_score'],\n                       'weight': weights })\ndataz.plot(x='weight')"}, {"metadata": {"collapsed": false, "_execution_state": "idle", "_cell_guid": "5d4a19b3-840d-4d74-89ea-495d82589211", "_uuid": "b333154898456941144e8f5ef7c5dc5d500c8f31"}, "execution_count": null, "cell_type": "markdown", "outputs": [], "source": "So now let's train a model with the best discovered params and see how it does!"}, {"metadata": {"trusted": false, "collapsed": false, "_execution_state": "idle", "_cell_guid": "d269275c-b2de-408f-bdd6-98857d9207c0", "_uuid": "fc874673a046d109d91033c4b05a4f67523565a9"}, "execution_count": null, "cell_type": "code", "outputs": [], "source": "pipe = make_pipeline(\n    SMOTE(ratio=0.015),\n    LogisticRegression()\n)\n\n# Fit..\npipe.fit(X_train, y_train)\n\n# Predict..\ny_pred = pipe.predict(X_test)\n\n# Evaluate the model\nprint(classification_report(y_test, y_pred))\nplot_confusion_matrix(confusion_matrix(y_test, y_pred))"}, {"metadata": {"collapsed": false, "_execution_state": "idle", "_cell_guid": "00c40be1-ec5f-405b-9fb4-cf81da66322f", "_uuid": "7ecb9f22da7ecbdcfab8cc72aeab0d316be90c1a"}, "execution_count": null, "cell_type": "markdown", "outputs": [], "source": "So to conclude this investigation. You can detect a decent amount of fraud cases using logistic regression classifier. The thing is, you need to deal with the class imbalance (many non-fraud vs few fraud). I showed two successfull strategies for dealing with that and how to tune this."}], "metadata": {"kernelspec": {"language": "python", "display_name": "Python 3", "name": "python3"}, "language_info": {"name": "python", "file_extension": ".py", "nbconvert_exporter": "python", "version": "3.6.1", "pygments_lexer": "ipython3", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}}}, "nbformat_minor": 0, "nbformat": 4}