{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport scipy\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3d6b8e377fa3f1a51206a5885deb981ab2bbd1f2"},"cell_type":"markdown","source":"# Data Preview"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"collapsed":true},"cell_type":"code","source":"train = pd.read_csv(r\"../input/train.csv\")\ntest = pd.read_csv(r\"../input/test.csv\")\npd.set_option('display.width', 700)\ntrain.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3d818c7a2a186c2829e2063c0129074f3ed85d21","collapsed":true},"cell_type":"code","source":"test.head(5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b3ab8f7db45223cc14ee7e2dcfee52fa2b6b1ec6"},"cell_type":"markdown","source":"# Primary Exploration of the Data¶\n"},{"metadata":{"trusted":true,"_uuid":"9e99d4675450a88d1220d92fdfaa933c7edf2597","collapsed":true},"cell_type":"code","source":"print(train.dtypes)\ntrain.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"ca4a6b7960d6799db13fefe145447e364c121341"},"cell_type":"markdown","source":"By looking at above table, there's no eye catching anomaly in the data."},{"metadata":{"trusted":true,"_uuid":"43a992ac3f13e97569e0820c09c67fef7c86f702","collapsed":true},"cell_type":"code","source":"print(\"The train data shape before any operation on the data: {} \".format(train.shape))\nprint(\"The test data shape before any operation on the data: {} \".format(test.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0a9d999c289026fd9f6567d311846bb860c3993c","collapsed":true},"cell_type":"code","source":"# Drop Id col as it is not helpful in the classification\n\ntrain.drop(\"Id\", axis=1, inplace=True)\ntest.drop(\"Id\", axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ba1ea9a1e975b7e82c9f31402a210d21fe301847"},"cell_type":"markdown","source":"# Check for Nans/NA/Missing Data"},{"metadata":{"trusted":true,"_uuid":"9b4ab95d6440b1215ea368b5f935a69f748fbc9b","collapsed":true},"cell_type":"code","source":"# Find out the frequency of nulls in the columns\n\n# For training Data\ncount_nans = len(train) - train.count()\ncount_nans = count_nans.to_frame()\ncount_nans.columns=[\"train_nan_count\"]\ncount_nans[\"%_train_nans\"]=(count_nans[\"train_nan_count\"]/train.shape[0]) * 100\n\n# For test data\ncount_nans[\"test_nan_count\"] = len(test) - test.count()\ncount_nans[\"%_test_nans\"]=(count_nans[\"test_nan_count\"]/test.shape[0]) * 100\n\ncount_nans.sort_values(\"train_nan_count\", ascending=False, inplace=True)\ncount_nans.query('train_nan_count > 0 or test_nan_count > 0')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fec08210b92d5b89885aa7c136b5b85310e024fe"},"cell_type":"markdown","source":"There's no NaNs/NA in any column"},{"metadata":{"trusted":true,"_uuid":"743a5715d6fa01e3d2d06b8a90d2cbbfa6cc203b","collapsed":true},"cell_type":"code","source":"# Only for Training data\ncorr_matrix = train.corr().abs()\ncorr_matrix.dropna(axis=1, how=\"all\", inplace=True) # remove the columns which is all Nan\ncorr_matrix.dropna(axis=0, how=\"all\", inplace=True) # remove the rows which is all Nan\nplt.subplots(figsize=(15,10))\nsns.heatmap(corr_matrix, cmap=\"jet\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f0dad106091d6a0eabfd5c0c4e8f479a8120b40d"},"cell_type":"markdown","source":"By looking at above heatmap following observations can be made:\n\n**The shadows at the 9 AM and at 3 PM** : It makes sense because the shadows can be pretty similar at 9 AM and 3 PM at the summaer soltice. Only in the opposite directions. So one of the column can be dropped\n\n**Wilderness Area 4 and Elevation**: This correlation doen't makes sense in the real world. It doesn't explain any relationship. So we'll keep those columns as it is."},{"metadata":{"trusted":true,"_uuid":"a045f6e8884b1054eac72cd5b28c920cfe015b4b","collapsed":true},"cell_type":"code","source":"train.drop(\"Hillshade_3pm\", inplace=True, axis=1)\ntest.drop(\"Hillshade_3pm\", inplace=True, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"85539ee619c380a62f0a3031fdca403355ef31ef","collapsed":true},"cell_type":"code","source":"# take out training target data for further computation\ny_train = train[\"Cover_Type\"]\ntrain.drop([\"Cover_Type\"], inplace=True, axis=1)\ntrain.columns","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c40ed4afa055e337728ab7690c84cd534a1a5cd9"},"cell_type":"markdown","source":"# Find the skewness"},{"metadata":{"trusted":true,"_uuid":"5f314baf5815496753923ab969cdf0dfa864032e","collapsed":true},"cell_type":"code","source":"# Find skewness in the y_train\ny_train.hist()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"47a466c412330403ccf7a67217dc9ee2917d639b"},"cell_type":"markdown","source":"Based on above plot y is uniform"},{"metadata":{"trusted":true,"_uuid":"0d627f7b375263a8fd9c90975ce97b7d1bfdc506","collapsed":true},"cell_type":"code","source":"# Checking Non-Binary columns only\nskew_check_cols = [\"Elevation\", \"Aspect\", \"Slope\", \"Vertical_Distance_To_Hydrology\", \"Horizontal_Distance_To_Hydrology\", \n                   \"Horizontal_Distance_To_Roadways\",\"Hillshade_9am\",\"Hillshade_Noon\",\"Horizontal_Distance_To_Fire_Points\" ]\ntrain[skew_check_cols].hist(figsize=(25, 25))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bac93a34eba7d7ccbae5f9ea8cb6465ab9ce99ac","scrolled":true,"collapsed":true},"cell_type":"code","source":"# skewness = train[skew_check_cols].skew().abs().sort_values(ascending=False)\n# print(skewness)\n# skewed_cols = skewness[skewness>0.5].index.tolist()\n# print(skewed_cols)\n# train[skewed_cols] = train[skewed_cols].apply(np.log1p)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3eb188f78e3c6c1ef2b16cd424c1d23966d40e78","scrolled":true,"collapsed":true},"cell_type":"code","source":"train[skew_check_cols].skew().abs().sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5d07db95e71c8ba15fc31c9f3313162184cf8ae1"},"cell_type":"markdown","source":"## Feature Engineering"},{"metadata":{"trusted":true,"_uuid":"d2c29023b0e19e54aee001400ff38d00db017eb6","collapsed":true},"cell_type":"code","source":"# ####################### Train data #############################################\n# train['HF1'] = train['Horizontal_Distance_To_Hydrology']+train['Horizontal_Distance_To_Fire_Points']\n# train['HF2'] = abs(train['Horizontal_Distance_To_Hydrology']-train['Horizontal_Distance_To_Fire_Points'])\n# train['HR1'] = abs(train['Horizontal_Distance_To_Hydrology']+train['Horizontal_Distance_To_Roadways'])\n# train['HR2'] = abs(train['Horizontal_Distance_To_Hydrology']-train['Horizontal_Distance_To_Roadways'])\n# train['FR1'] = abs(train['Horizontal_Distance_To_Fire_Points']+train['Horizontal_Distance_To_Roadways'])\n# train['FR2'] = abs(train['Horizontal_Distance_To_Fire_Points']-train['Horizontal_Distance_To_Roadways'])\n# train['ele_vert'] = train.Elevation-train.Vertical_Distance_To_Hydrology\n\n# train['slope_hyd'] = (train['Horizontal_Distance_To_Hydrology']**2+train['Vertical_Distance_To_Hydrology']**2)**0.5\n# train.slope_hyd=train.slope_hyd.map(lambda x: 0 if np.isinf(x) else x) # remove infinite value if any\n\n# #Mean distance to Amenities \n# train['Mean_Amenities']=(train.Horizontal_Distance_To_Fire_Points + train.Horizontal_Distance_To_Hydrology + train.Horizontal_Distance_To_Roadways) / 3 \n# #Mean Distance to Fire and Water \n# train['Mean_Fire_Hyd']=(train.Horizontal_Distance_To_Fire_Points + train.Horizontal_Distance_To_Hydrology) / 2 \n\n# ####################### Test data #############################################\n# test['HF1'] = test['Horizontal_Distance_To_Hydrology']+test['Horizontal_Distance_To_Fire_Points']\n# test['HF2'] = abs(test['Horizontal_Distance_To_Hydrology']-test['Horizontal_Distance_To_Fire_Points'])\n# test['HR1'] = abs(test['Horizontal_Distance_To_Hydrology']+test['Horizontal_Distance_To_Roadways'])\n# test['HR2'] = abs(test['Horizontal_Distance_To_Hydrology']-test['Horizontal_Distance_To_Roadways'])\n# test['FR1'] = abs(test['Horizontal_Distance_To_Fire_Points']+test['Horizontal_Distance_To_Roadways'])\n# test['FR2'] = abs(test['Horizontal_Distance_To_Fire_Points']-test['Horizontal_Distance_To_Roadways'])\n# test['ele_vert'] = test.Elevation-test.Vertical_Distance_To_Hydrology\n\n# test['slope_hyd'] = (test['Horizontal_Distance_To_Hydrology']**2+test['Vertical_Distance_To_Hydrology']**2)**0.5\n# test.slope_hyd=test.slope_hyd.map(lambda x: 0 if np.isinf(x) else x) # remove infinite value if any\n\n# #Mean distance to Amenities \n# test['Mean_Amenities']=(test.Horizontal_Distance_To_Fire_Points + test.Horizontal_Distance_To_Hydrology + test.Horizontal_Distance_To_Roadways) / 3 \n# #Mean Distance to Fire and Water \n# test['Mean_Fire_Hyd']=(test.Horizontal_Distance_To_Fire_Points + test.Horizontal_Distance_To_Hydrology) / 2\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d5087b7ce6f50f1c34f84565b0899f52d0b99340"},"cell_type":"markdown","source":"# Normalize The Data"},{"metadata":{"trusted":true,"_uuid":"d0eab7f413fe13e3e644728f6d580f14a06eef20","collapsed":true},"cell_type":"code","source":"# from sklearn.preprocessing import StandardScaler\n# from sklearn.preprocessing import RobustScaler\n\n# train = RobustScaler(with_centering=True, with_scaling=True).fit_transform(train, y_train)\n# test = RobustScaler(with_centering=True, with_scaling=True).fit_transform(test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9db9fddcfbb2361b71577f7f94562203e1ea1934"},"cell_type":"markdown","source":"# Modeling\n\nWe'll try following models:\n\nLogistic Regression <br/>\nSVM  <br/>\nNeural Net  <br/>\nExtraTreesClassifier\n"},{"metadata":{"trusted":true,"_uuid":"2229e7fe328efc070637dc03571c0e728a6df07a","collapsed":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import roc_curve\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import cross_val_score\n\n\ndef kfold_cv(train, model, y_true):\n    accuracy = []\n    kfold = StratifiedKFold(n_splits=10, random_state=None)\n    for tr, te in kfold.split(train, y_true):\n        model.fit(train.iloc[tr], y_true.iloc[tr])\n        predictions = model.predict(train.iloc[te])\n        accuracy.append(accuracy_score(predictions, y_true.iloc[te]))\n    \n    print(\"KFold Accuracy:{}\".format(np.mean(accuracy)))\n\ndef calculate_metrics(y_true, y_pred):\n    #acc_score = accuracy_score(y_true, y_pred)\n    print(\"accuracy score:{}\".format(accuracy_score(y_true, y_pred)))\n    print(\"f1 score:{}\".format(f1_score(y_true, y_pred, average=\"micro\")))\n    # ROC Curve","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"59001fc64c133f94f3d2a2792e0df2351b340a9e","collapsed":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\n\n# TODO: Try multiscoring\ndef _grid_search(model, param_grid, train, y_train):\n    clf = GridSearchCV(model, param_grid, cv=10, scoring='accuracy', refit=True)\n    clf.fit(train, y_train)\n    print(\"Best parameters set found on development set:\")\n    print(clf.best_params_)\n    print(\"Grid scores on development set:\")\n    means = clf.cv_results_['mean_test_score']\n    stds = clf.cv_results_['std_test_score']\n    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n        print(\"%0.3f (+/-%0.03f) for %r\"% (mean, std * 2, params))\n    print(clf.scorer_)\n    print(clf.cv_results_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bf6e3140ed7c6d3f955ebd8a8db43dc6936fff18","collapsed":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nprint(\"Logistic_Regression\")\nlr = LogisticRegression()\nlr.fit(train, y_train)\ny_train_pred = lr.predict(train)\ny_test_pred = lr.predict(test)\n\nkfold_cv(train, lr, y_train)\ncalculate_metrics(y_train, y_train_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"20702487e7c32888899bb2430b0b73765321d1ca","collapsed":true},"cell_type":"code","source":"# from sklearn.ensemble import GradientBoostingClassifier\n# print(\"Gradient Boost\")\n# cl = GradientBoostingClassifier(n_estimators=200, learning_rate=0.1)\n# cl.fit(train, y_train)\n# y_train_pred = cl.predict(train)\n# y_test_pred = cl.predict(test)\n\n# # print(\"classifier score {}\".format(cl.score(train, y_train)))\n\n# kfold_cv(train, cl, y_train)\n# calculate_metrics(y_train, y_train_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"58f3a646fa8690c96efac2d83e6631fd66b31767"},"cell_type":"code","source":"# import sys\n# orig_stdout=sys.stdout\n# sys.stdout=open('output.txt', 'w')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7eca5a2e896cbecf525291e97f116a8a5bb3b47d","collapsed":true},"cell_type":"code","source":"import lightgbm as lgb\n\n# d_train = lgb.Dataset(train, label=y_train)\n# params = {}\n# params['learning_rate'] = 0.003\n# params['boosting_type'] = 'gbdt'\n# params['objective'] = 'binary'\n# params['metric'] = 'binary_logloss'\n# params['sub_feature'] = 0.5\n# params['num_leaves'] = 10\n# params['min_data'] = 50\n# params['max_depth'] = 10\n\n# print(\"lgb_classifier\")\n# param_grid = {'n_estimators':[500, 600, 800], 'learning_rate':[0.1], 'max_depth':[40, 60], 'num_leaves':[127, 255], 'boosting_type':['gbdt']}\n# cl = lgb.LGBMClassifier()\n# _grid_search(cl, param_grid, train, y_train)\n\ncl = lgb.LGBMClassifier(boosting_type='gbdt', learning_rate=0.1, max_depth=40, n_estimators=500, num_leaves=63, objective=\"multiclass\")\ncl.fit(train, y_train)\ny_train_pred = cl.predict(train)\ny_test_pred = cl.predict(test)\n\nkfold_cv(train, cl, y_train)\ncalculate_metrics(y_train, y_train_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eee4274fac15d9b7f4982de5143da086d84fa415","collapsed":true},"cell_type":"code","source":"from sklearn.metrics import classification_report\nprint(classification_report(y_train, y_train_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"42df05c965d270a237f2b182426bce298d5273e2","collapsed":true},"cell_type":"code","source":"from xgboost import XGBClassifier\n# print(\"xgb_classifier\")\n# param_grid = {'n_estimators':[70, 300, 500], 'learning_rate':[0.001, 0.01, 0.1], 'max_depth':[2, 10, 20]}\n# cl = XGBClassifier(n_jobs=10)\n# _grid_search(cl, param_grid, train, y_train)\n\n# cl = XGBClassifier(n_jobs=20, n_estimators=1000, learning_rate=0.1, max_depth=100)\n# cl.fit(train, y_train)\n# y_train_pred = cl.predict(train)\n# y_test_pred = cl.predict(test)\n\n# # print(\"classifier score {}\".format(cl.score(train, y_train)))\n\n# kfold_cv(train, cl, y_train)\n# calculate_metrics(y_train, y_train_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fd35b4fa95f9ed37df05e296aae4c27f1f9f27a6","collapsed":true},"cell_type":"code","source":"# from sklearn.ensemble import ExtraTreesClassifier\n\n# cl = ExtraTreesClassifier(n_estimators=400)#, oob_score=True, bootstrap=True)\n# cl.fit(train, y_train)\n# y_train_pred = cl.predict(train)\n# y_test_pred = cl.predict(test)\n\n# #print(\"oob score {}\".format(cl.oob_score_))\n\n# kfold_cv(train, cl, y_train)\n# calculate_metrics(y_train, y_train_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3942cdc8d513a0b15f3e2e2c400eca08aaf9f2b4","collapsed":true},"cell_type":"code","source":"idx = pd.read_csv(\"../input/test.csv\").Id\nmy_submission = pd.DataFrame({'Id': idx, 'Cover_Type': y_test_pred})\n# you could use any filename. We choose submission here\nmy_submission.to_csv('submission.csv', index=False)\nmy_submission.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}