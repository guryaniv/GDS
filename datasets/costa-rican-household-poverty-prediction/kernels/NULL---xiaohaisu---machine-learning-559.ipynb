{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a415dae828226051fb167d18e5f1c18cdfeba20a"},"cell_type":"code","source":"# Get the data.\ntrain = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\n\nprint(train.info())\nprint(test.info())\nprint(train.head())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c9a1e54737e26d10ae9737e2bc70d1a62a3545d4"},"cell_type":"markdown","source":"## Exploratory Data Analysis"},{"metadata":{"trusted":true,"_uuid":"307a0a457c80000b559670f6ed242919c871439c"},"cell_type":"code","source":"# EDA\n# As we can see we have three types of data and most of the integer data are booleans. So here we only \n# concentrate on exploring float data and objects data.\n\n# Float data.\nfrom collections import OrderedDict\nplt.figure(figsize = (30, 26))\n\ncolor_use = {1: 'r', 2: 'g', 3: 'c', 4: 'y'}\ncolors = OrderedDict(color_use)\npoverty_level = {1: 'extreme poverty', 2: 'moderate poverty', 3: 'vulnerable households', \n                            4: 'nonvulnerable households'}\npl_marker = OrderedDict(poverty_level)\n\nfloat_data = train.select_dtypes('float')\nfor index, columns in enumerate(float_data):\n    axis = plt.subplot(4, 2, index + 1)\n    for pl, cl in colors.items():\n        sb.kdeplot(train.loc[train['Target'] == pl, columns], ax = axis, color = cl, \n                   label = pl_marker[pl])\n        plt.xlabel('values of ' f'{columns}')\n        plt.ylabel('proportion')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"abac3579e4ddaf78324eea3212d3607bf1bf6829"},"cell_type":"code","source":"# From these float distributions we may find some features that can influence the poverty level \n# significantly and do further analysis.\n\n# Object data.\nobject_data = train.select_dtypes('object')\nprint(object_data.info())\nprint(object_data.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"154cf51a706366dd3c5909462ecb49989011bc60"},"cell_type":"code","source":"# We can ignore the \"Id\" and \"idhogar\" because they are identifying features based on the problem. For the\n# other three features we can replace \"yes\" to \"1\" and replace \"no\" to \"0\" according to the documentation \n# on the kaggle.\nreplace_yn = {'yes': 1, 'no': 0}\n\ntrain['dependency'] = train['dependency'].replace(replace_yn).astype(np.float)\ntrain['edjefe'] = train['edjefe'].replace(replace_yn).astype(np.float)\ntrain['edjefa'] = train['edjefa'].replace(replace_yn).astype(np.float)\n\ntest['dependency'] = test['dependency'].replace(replace_yn).astype(np.float)\ntest['edjefe'] = test['edjefe'].replace(replace_yn).astype(np.float)\ntest['edjefa'] = test['edjefa'].replace(replace_yn).astype(np.float)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"628cbd17d5fcce26d96a80cfbe2368422db1660f","scrolled":true},"cell_type":"code","source":"train['dependency'].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e9bbacfa6e32d1a65525077e371c7a3eba67c7d3"},"cell_type":"code","source":"# Make plots if needed.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4498aa4e160d09722879216f248982cf780d8c0f"},"cell_type":"code","source":"# Since some of individuals have different poverty level in the same household, we only need to consider \n# the poverty level of the head of the household according to the ducumentation on kaggle. Here we use the \n# poverty level of the head of the household as the true target variable.\ntrain_num_househould = train.groupby('idhogar')['Target'].nunique()\nprint('Numbers of households: ', len(train_num_househould))\ntrain_unique = train_num_househould[train_num_househould == 1]\nprint('Numbers of households whose poverty levels of family members are the same: ', len(train_unique))\ntrain_not_unique = train_num_househould[train_num_househould != 1]\nprint('Numbers of households whose poverty levels of family members are not the same', len(train_not_unique))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"aef627ccca918039524c580ee1e5ce0ece9279f9"},"cell_type":"code","source":"# One example of these ununique targets within each household.\nprint(train[train['idhogar'] == train_not_unique.index[0]][['idhogar', 'parentesco1', 'Target']])\n# Locate the head of household.\nhead = train[(train['idhogar'] == train_not_unique.index[0]) & (train['parentesco1'] == 1)]\nprint(head)\n# Select the target variables that need to be changed.\nnot_unique_target = train.loc[train['idhogar'] == train_not_unique.index[0], 'Target']\nprint(not_unique_target)\n# Change the target variables.\nnot_unique_target = int(head['Target'])\nprint(not_unique_target)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5c774224692749ee8326326017540f4148a8997c"},"cell_type":"code","source":"# Set the true label for members of households whose labels are not the same.\n# Here we write a for loop to replace all the ununique target variables.\nfor unique_hhid in train_not_unique.index:\n    hh_head = train[(train['idhogar'] == unique_hhid) & (train['parentesco1'] == 1)]\n    train.loc[train['idhogar'] == unique_hhid, 'Target'] = hh_head['Target']\n\n# Let's check if it works.\ntrain_num_househould = train.groupby('idhogar')['Target'].nunique()\nprint('Numbers of households: ', len(train_num_househould))\ntrain_unique = train_num_househould[train_num_househould == 1]\nprint('Numbers of households whose poverty levels of family members are the same: ', len(train_unique))\ntrain_not_unique = train_num_househould[train_num_househould != 1]\nprint('Numbers of households whose poverty levels of family members are not the same', len(train_not_unique))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a649e22716132e2805e87aef589c10325d918b62"},"cell_type":"code","source":"# Now let's deal with the missing values.\n# Let's check the missiong values for both train and test data.\ntrainms = pd.DataFrame(train.isnull().sum())\ntrainms['counts'] = trainms\ntrainms['ratio'] = trainms['counts']/len(train)\nprint(trainms.sort_values(by = 'ratio', ascending = False).head(10))\n\n\ntestms = pd.DataFrame(test.isnull().sum())\ntestms['counts'] = testms\ntestms['ratio'] = testms['counts']/len(test)\nprint(testms.sort_values(by = 'ratio', ascending = False).head(10))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"68fd8217c5a552a582b97956a2c76ccbffa46d73"},"cell_type":"code","source":"# Now we know what features have the most missing values, then we can analyze them based on their own \n# meanings and decide whether we should delete the features or replace the missing values.\nhead_train = train.loc[train['parentesco1'] == 1]\nhead_test = test.loc[test['parentesco1'] == 1]\n\n# rez_esc: years behind in school.\n# Actually I don't really know the mean of \"years behind in school\", but what I do know is that it may be \n# related to the age feature. So let's check what's the relationship between this feature and age.\nprint(train.loc[train['rez_esc'].isnull()]['age'].describe())\nprint(train.loc[train['rez_esc'].notnull()]['age'].describe())\nprint(test.loc[test['rez_esc'].isnull()]['age'].describe())\nprint(test.loc[test['rez_esc'].notnull()]['age'].describe())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"40900933c2695be15552eea5d6caf1a4baa56f1f"},"cell_type":"code","source":"# Now we find that all the defined \"rez_esc\" values are between the age 7 and 17. Those who are younger \n# than 7 or older than 17 have missing values, which means we should set \"rez_esc\" values of these people \n# to 0 instead of null. And if there are some other situations, we can leave the values to be imputed and \n# add a boolean flag.\ntrain.loc[(train['age'] > 17) & (train['rez_esc'].isnull()), 'rez_esc'] = 0\ntrain.loc[(train['age'] < 7) & (train['rez_esc'].isnull()), 'rez_esc'] = 0\n\ntest.loc[(test['age'] > 17) & (test['rez_esc'].isnull()), 'rez_esc'] = 0\ntest.loc[(test['age'] < 7) & (test['rez_esc'].isnull()), 'rez_esc'] = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"04a8addea291d586772dc0393d4d27b120cb932b"},"cell_type":"code","source":"# v18q1: number of tablets household owns.\n# This could be compared with the feature \"v18q: owns a tablet\". Maybe the missing values in \"v18q1\" \n# indicates that the households don't even own a tablet, which means the value is 0 in \"v18q\".\n# First let's check the values of \"v18q1\".\nprint('Numbers of null values of \"v18q1\" in train: ', head_train['v18q1'].isnull().sum())\nprint('Numbers of \"0\" of \"v18q\" in train: ', head_train[head_train['v18q'] == 0]['v18q'].count())\nprint('Numbers of null values of \"v18q1\" in test: ', head_test['v18q1'].isnull().sum())\nprint('Numbers of \"0\" of \"v18q\" in test: ', head_test[head_test['v18q'] == 0]['v18q'].count())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f702c1f736515425a1a0d14adbc6c9cf1e9d3622"},"cell_type":"code","source":"# Here we can see that every household has missing value in \"v18q1\" doesn't have any tablet. Then we can \n# replace the missing values to 0.\ntrain['v18q1'] = train['v18q1'].fillna(0)\ntest['v18q1'] = test['v18q1'].fillna(0)\n\ntrain['v18q1'].value_counts().plot.bar(color = 'red')\nplt.xlabel('values of \"v18q1\"')\nplt.ylabel('counts of individuals')\nplt.title('\"v18q1\" in train')\nplt.show()\n\ntrain['v18q'].value_counts().plot.bar(color = 'red')\nplt.xlabel('values of \"v18q\"')\nplt.ylabel('counts of individuals')\nplt.title('\"v18q\" in train')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f566d83d983deb4b805a6145971aad2cb2adaecf"},"cell_type":"code","source":"test['v18q1'].value_counts().plot.bar(color = 'blue')\nplt.xlabel('values of \"v18q1\"')\nplt.ylabel('counts of individuals')\nplt.title('\"v18q1\" in test')\nplt.show()\n\ntest['v18q'].value_counts().plot.bar(color = 'blue')\nplt.xlabel('values of \"v18q\"')\nplt.ylabel('counts of individuals')\nplt.title('\"v18q\" in test')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0d9743aef7899911ac23925096ad547f1db66ec0"},"cell_type":"code","source":"# v2a1: monthly rent payment.\n# According to the explanations of the documentation on the kaggle, this feature may be related to the \n# \"tipovivi\", which represents the status of the house. For example, if people already own this house, then \n# they don't need to pay the rent anymore.\nstatus = ['tipovivi1', 'tipovivi2', 'tipovivi3', 'tipovivi4', 'tipovivi5']\n\ntrain.loc[train['v2a1'].isnull(), status].sum().plot.bar(color = 'green')\nplt.xticks([0, 1, 2, 3, 4], ['own and fully paid house', 'own and paying in installments', \n                            'rented', 'precarious', 'other'], rotation = 40)\nplt.title('status of the house for missing values of \"v2a1\" in train')\nplt.show()\n\ntest.loc[test['v2a1'].isnull(), status].sum().plot.bar(color = 'cyan')\nplt.xticks([0, 1, 2, 3, 4], ['own and fully paid house', 'own and paying in installments', \n                            'rented', 'precarious', 'other'], rotation = 40)\nplt.title('status of the house for missing values of \"v2a1\" in test')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"36abd5943b23aeb400fa8d9df44b5b5090352e29"},"cell_type":"code","source":"# We can see that most of the households who don't have monthly rent payment generally own their house. But \n# we don't know the reason of missing values for the other situations.\ntrain.loc[(train['tipovivi1'] == 1), 'v2a1'] = 0\ntest.loc[(test['tipovivi1'] == 1), 'v2a1'] = 0\n\ntrain['v2a1'].isnull().value_counts()\ntest['v2a1'].isnull().value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"471cdbf44114c05098006060c75c245e2cea8a3f"},"cell_type":"code","source":"# Since the rest of the missing values belong to \"rez_esc\" and \"v2a1\", which only contain integer values. \n# So we simply replace the remaining missing values to the median of the column values.\ntrain.fillna(train.median(), inplace = True)\ntest.fillna(test.median(), inplace = True)\n\nprint(train.isnull().sum().sort_values(ascending = False).head())\nprint(test.isnull().sum().sort_values(ascending = False).head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9ba3858d8d21f739a0b686e7526e49c99ef07b74"},"cell_type":"markdown","source":"## Feature Engineering"},{"metadata":{"trusted":true,"_uuid":"82598977b1388fc990ddbb1d125ff184cd62f34b"},"cell_type":"markdown","source":"#### There are several different categories of variables:\n#### 1. Individual Variables: these are characteristics of each individual rather than the household\n#### Boolean: Yes or No (0 or 1)\n#### Ordered Discrete: Integers with an ordering\n#### 2. Household variablesBoolean: Yes or No\n#### Ordered Discrete: Integers with an ordering\n#### Continuous numeric\n#### 3. Squared Variables: derived from squaring variables in the data\n#### 4. Id variables: identifies the data and should not be used as features"},{"metadata":{"trusted":true,"_uuid":"3a35d0380be51923fdee3692c9d9518aafa854a7"},"cell_type":"code","source":"# ID_variables.\n# We will keep these in the data since we need them for identification.\nidv = ['Id', 'idhogar', 'Target']\n\n# Individual variables.\nib = ['v18q', 'dis', 'male', 'female', 'estadocivil1', 'estadocivil2', 'estadocivil3', 'estadocivil4', \n      'estadocivil5', 'estadocivil6', 'estadocivil7', 'parentesco1', 'parentesco2',  'parentesco3', \n      'parentesco4', 'parentesco5', 'parentesco6', 'parentesco7', 'parentesco8',  'parentesco9', \n      'parentesco10', 'parentesco11', 'parentesco12', 'instlevel1', 'instlevel2', 'instlevel3', \n      'instlevel4', 'instlevel5', 'instlevel6', 'instlevel7', 'instlevel8', 'instlevel9', 'mobilephone']\n\nio = ['rez_esc', 'escolari', 'age']\n\n# Househould variables.\nhb = ['hacdor', 'hacapo', 'v14a', 'refrig', 'paredblolad', 'paredzocalo', 'paredpreb','pisocemento', \n      'pareddes', 'paredmad','paredzinc', 'paredfibras', 'paredother', 'pisomoscer', 'pisoother', \n      'pisonatur', 'pisonotiene', 'pisomadera','techozinc', 'techoentrepiso', 'techocane', 'techootro', \n      'cielorazo', 'abastaguadentro', 'abastaguafuera', 'abastaguano', 'public', 'planpri', 'noelec', \n      'coopele', 'sanitario1', 'sanitario2', 'sanitario3', 'sanitario5',   'sanitario6', 'energcocinar1', \n      'energcocinar2', 'energcocinar3', 'energcocinar4', 'elimbasu1', 'elimbasu2', 'elimbasu3', 'elimbasu4', \n      'elimbasu5', 'elimbasu6', 'epared1', 'epared2', 'epared3', 'etecho1', 'etecho2', 'etecho3', \n      'eviv1', 'eviv2', 'eviv3', 'tipovivi1', 'tipovivi2', 'tipovivi3', 'tipovivi4', 'tipovivi5', \n      'computer', 'television', 'lugar1', 'lugar2', 'lugar3', 'lugar4', 'lugar5', 'lugar6', 'area1', \n      'area2']\n\nho = [ 'rooms', 'r4h1', 'r4h2', 'r4h3', 'r4m1','r4m2','r4m3', 'r4t1',  'r4t2', 'r4t3', 'v18q1', \n      'tamhog','tamviv','hhsize','hogar_nin', 'hogar_adul','hogar_mayor','hogar_total',  'bedrooms', \n      'qmobilephone']\n\nhcn = ['v2a1', 'dependency', 'edjefe', 'edjefa', 'meaneduc', 'overcrowding']\n\n# Squared Variables.\nsv = ['SQBescolari', 'SQBage', 'SQBhogar_total', 'SQBedjefe', 'SQBhogar_nin', 'SQBovercrowding', \n      'SQBdependency', 'SQBmeaned', 'agesq']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"08b27cefd23cb4040c96565ea01e197bb0eca5ab"},"cell_type":"code","source":"# Sometimes variables are squared or transformed as part of feature engineering because it can help linear\n# relationships that are non-linear. However, here we are going to use models that are more complex, these \n# squared features may be redundant since they are highly correlated with the non-squared features. This \n# means these squared will hurt the models badly by adding irrelevant information.\n# We can take a look at the relationshop between the squared features and non-squared features.\nsb.lmplot('edjefe', 'SQBedjefe', data = train)\nplt.title('\"edjefe\" vs \"SQBedjefe\"')\nplt.show()\n\nsb.lmplot('dependency', 'SQBdependency', data = train)\nplt.title('\"dependency\" vs \"SQBdependency\"')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7af9a687a9a55cf2192475f0ef7894b9028f05d6"},"cell_type":"code","source":"# These features are highly correlated, we don't need to keep them both in the data. So I decide to delete \n# all of the squared variables.\ntrain = train.drop(columns = sv)\ntest = test.drop(columns = sv)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"029e29e6cf7ade688250c07538908e7362b3ff12"},"cell_type":"code","source":"# Household variables.\nhhtrain = train.loc[train['parentesco1'] == 1, :]\nhhtrain = hhtrain[idv + hb + ho + hcn]\n\nhhtest = test.loc[test['parentesco1'] == 1, :]\nhhtest = hhtest[['Id', 'idhogar'] + hb + ho + hcn]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ec89516000ea587c0a3b86f2af2a09b74ba4ec10"},"cell_type":"code","source":"# Then we need to check the correlations between these features. If there are some features highly \n# correlated, we have to delete one of the pair so that it will not cause data redundant.\ncor_hh = hhtrain.corr()\nuptr = np.triu(np.ones(cor_hh.shape), k = 1).astype(np.bool)\nup = cor_hh.where(uptr)\n\na = [column for column in up.columns if any(abs(up[column]) > 0.9)]\nprint(a)\n\ncor_hh.loc[cor_hh['tamviv'] > 0.9, cor_hh['tamviv'] > 0.9]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"e53230a021b0dee7f044c513bb71340b6dbdb857"},"cell_type":"code","source":"sb.heatmap(cor_hh.loc[cor_hh['tamviv'].abs() > 0.9, cor_hh['tamviv'].abs() > 0.9], annot = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b18f47ee938b2946dd7dc1583048e3b7219f20ce"},"cell_type":"code","source":"# Based on the hearmap, we drop some of the features.\nhhtrain = hhtrain.drop(columns = ['tamhog', 'r4t3', 'hogar_total', 'hhsize'])\nhhtest = hhtest.drop(columns = ['tamhog', 'r4t3', 'hogar_total', 'hhsize'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ec09eef7dcc96b69b7c3da7160e1ac392603ad28"},"cell_type":"code","source":"# Individual vriables.\nitrain = train[idv + ib + io]\n\nitest = test[['Id', 'idhogar'] + ib + io]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"99c2dc8442735f766492f97d8dbf4588e19ceb9b"},"cell_type":"code","source":"# Identify redundant features.\ncor_i = itrain.corr()\nuptr = np.triu(np.ones(cor_i.shape), k = 1).astype(np.bool)\nup = cor_i.where(uptr)\n\nb = [column for column in up.columns if any(abs(up[column]) > 0.9)]\nprint(b)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3938b078c916b091b06e20a92165b77a475eeac3"},"cell_type":"code","source":"# This is related to the \"male\" feature, so we can move one of them. Here we remove the \"male\".\nitrain = itrain.drop(columns = 'male')\nitest = itest.drop(columns = 'male')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"784fe3ddb1c1ee91aa605d6174f3723a749e1e35"},"cell_type":"code","source":"# Finally, we just need to aggregate the individual and household data, and we can run the model.\n# Since the boolean aggregations can be the same, but this will create many redundant columns that need \n# to be deleted. We will do the aggregations and then go back to drop the redundant columns.\naggf = lambda x: x.max() - x.min()\naggf.__name__ = 'aggregation'\n\niagg_train = itrain.drop(columns = 'Target').groupby('idhogar').agg(['min', 'max', aggf])\niagg_test = itest.groupby('idhogar').agg(['min', 'max', aggf])\niagg_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3bcf647488088ace81030bdd6ebdb0f49a2dd110"},"cell_type":"code","source":"# Rename the columns so that we can know their exact meaning.\nrntrain = []\nfor realname in iagg_train.columns.levels[0]:\n    for aggval in iagg_train.columns.levels[1]:\n        rntrain.append(f'{realname}-{aggval}')\niagg_train.columns = rntrain\n\nrntest = []\nfor realname in iagg_test.columns.levels[0]:\n    for aggval in iagg_test.columns.levels[1]:\n        rntest.append(f'{realname}-{aggval}')\niagg_test.columns = rntest\niagg_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"47ad4dafc8c1f192ca686cea53bc2ce5f0989a3b"},"cell_type":"code","source":"# Check the correlations so that we can delete the redundant data.\ncor_aggtrain = iagg_train.corr()\n\nuptr_agg = cor_aggtrain.where(np.triu(np.ones(cor_aggtrain.shape), k = 1).astype(np.bool))\ndeletrain = [column for column in uptr_agg.columns if any(abs(uptr_agg[column])> 0.95)]\nprint(len(deletrain))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"52d69f61c6aaa27a59d3b9e67d71df74d4f6f0bf"},"cell_type":"code","source":"cor_aggtest = iagg_test.corr()\n\nuptr_agg = cor_aggtest.where(np.triu(np.ones(cor_aggtest.shape), k = 1).astype(np.bool))\ndeletest = [column for column in uptr_agg.columns if any(abs(uptr_agg[column])> 0.95)]\nprint(len(deletest))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"711082c6c9dedfed0ab5be8bcbb36ca5c5099bc2"},"cell_type":"code","source":"# Merge the data.\niagg_train = iagg_train.drop(columns = deletrain)\ncleaned_train = hhtrain.merge(iagg_train, on = 'idhogar', how = 'left')\ncleaned_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7fa11dc1b3952ff851f02be6c03944c38892cd26"},"cell_type":"code","source":"iagg_test = iagg_test.drop(columns = deletest)\ncleaned_test = hhtest.merge(iagg_test, on = 'idhogar', how = 'left')\ncleaned_test.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2f292939d00e3626402aaab2e7b60d76014b446b"},"cell_type":"markdown","source":"## Model"},{"metadata":{"trusted":true,"_uuid":"40eeed5ff499561b220478f7e888dad2f234ecff"},"cell_type":"code","source":"# First we use a simple method to establish a baseline accuracy based on kfold cross validation.\nimport sklearn\nfrom sklearn.model_selection import train_test_split, cross_val_score, KFold\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import classification_report, accuracy_score, f1_score, make_scorer\nfrom sklearn.preprocessing import Imputer, MinMaxScaler\nfrom sklearn.pipeline import Pipeline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d1248446b1399c579fc7a398c889a816840ba262"},"cell_type":"code","source":"# Define the target labels.\ny = np.array(cleaned_train['Target'])\n\n# Since we are going to use different models to compare their performances, scaling the features would be \n# very necessary. Some distance metric-based models such as kNN and SVM may not be applied to the data if \n# the data is not scaled.\n# Extract the training data.\ntrain_set = cleaned_train.drop(columns = ['Id', 'idhogar', 'Target'])\ntest_set = cleaned_test.drop(columns = ['Id', 'idhogar'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"961c30e69a0aa4998c86de0ca37efa6636525f6d"},"cell_type":"code","source":"# Scale the data.\npipeline = Pipeline([('scaler', MinMaxScaler())])\n\ntrain_set = pipeline.fit_transform(train_set)\ntest_set = pipeline.fit_transform(test_set)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"14b2b12df46b3f40009d42d1cac110f2770b4422"},"cell_type":"code","source":"# kNN.\nknn_classifier = KNeighborsClassifier(n_neighbors = 5)\nacc_sco = make_scorer(accuracy_score)\nf1_sco = make_scorer(f1_score, average = 'macro')\n\nacc_knn = cross_val_score(knn_classifier, train_set, y, cv = 10, scoring = acc_sco)\nf1_knn = cross_val_score(knn_classifier, train_set, y, cv = 10, scoring = f1_sco)\n\nprint(np.mean(acc_knn))\nprint(np.mean(f1_knn))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b01057a9f84188090b9a6eecf4256716fd653f17"},"cell_type":"code","source":"# The accuracy increases with the increasing of n_neighbors.\naccknn = []\nf1knn = []\nfor i in range(4, 11):\n    knn_classifier = KNeighborsClassifier(n_neighbors = i)\n    acc_knn = cross_val_score(knn_classifier, train_set, y, cv = 10, scoring = acc_sco)\n    f1_knn = cross_val_score(knn_classifier, train_set, y, cv = 10, scoring = f1_sco)\n    a = np.mean(acc_knn)\n    b = np.mean(f1_knn)\n    accknn.append(a)\n    f1knn.append(b)\n    \nprint(accknn)\nprint(f1knn)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"131bf5fde1f5cd840caf3614bf12994909be55b1"},"cell_type":"code","source":"# After we get the baseline accuracy, we will use some of the advanced models to do further machine \n# learning.\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import LinearSVC, SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.linear_model import LogisticRegression","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"44d6ffa3f96e07c33ce8c4e49a9e2ae55c4d920f"},"cell_type":"code","source":"# Random Forest.\nrf_classifier = RandomForestClassifier()\nacc_rf = cross_val_score(rf_classifier, train_set, y, cv = 10, scoring = acc_sco)\nf1_rf = cross_val_score(rf_classifier, train_set, y, cv = 10, scoring = f1_sco)\n\nprint(np.mean(acc_rf))\nprint(np.mean(f1_rf))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e9b8bd4d3540fa8c77e2ee23cd06f016471680b0"},"cell_type":"code","source":"accrf = []\nf1rf = []\nfor i in range(1, 6):\n    rf_classifier = RandomForestClassifier(max_depth = i)\n    acc_rf = cross_val_score(rf_classifier, train_set, y, cv = 10, scoring = acc_sco)\n    f1_rf = cross_val_score(rf_classifier, train_set, y, cv = 10, scoring = f1_sco)\n    c = np.mean(acc_rf)\n    d = np.mean(f1_rf)\n    accrf.append(c)\n    f1rf.append(d)\n    \nprint(accrf)\nprint(f1rf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7ca85d0e69395a35f3a1d13b10f5be5e4eab00fa"},"cell_type":"code","source":"# SVM.\nsvm_classifier = SVC(kernel = 'rbf')\nacc_svm = cross_val_score(svm_classifier, train_set, y, cv = 10, scoring = acc_sco)\nf1_svm = cross_val_score(svm_classifier, train_set, y, cv = 10, scoring = f1_sco)\n\nprint(np.mean(acc_svm))\nprint(np.mean(f1_svm))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f76cd1c1f1a4a2744f842f26361ca13832f8dbbc"},"cell_type":"code","source":"svm_classifier = SVC(kernel = 'linear')\nacc_svm = cross_val_score(svm_classifier, train_set, y, cv = 10, scoring = acc_sco)\nf1_svm = cross_val_score(svm_classifier, train_set, y, cv = 10, scoring = f1_sco)\n\nprint(np.mean(acc_svm))\nprint(np.mean(f1_svm))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"283629ff356d4696012a352e1b157f6cef74e3a1"},"cell_type":"code","source":"svm_classifier = SVC(kernel = 'poly')\nacc_svm = cross_val_score(svm_classifier, train_set, y, cv = 10, scoring = acc_sco)\nf1_svm = cross_val_score(svm_classifier, train_set, y, cv = 10, scoring = f1_sco)\n\nprint(np.mean(acc_svm))\nprint(np.mean(f1_svm))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ef9087d27f8a78c8475e07247e7b178d734dc3eb"},"cell_type":"code","source":"# Naive Bayes.\nnb_classifier = GaussianNB()\nacc_nb = cross_val_score(nb_classifier, train_set, y, cv = 10, scoring = acc_sco)\nf1_nb = cross_val_score(nb_classifier, train_set, y, cv = 10, scoring = f1_sco)\n\nprint(np.mean(acc_nb))\nprint(np.mean(f1_nb))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5e53018d53102becc61e857c73560cec3f1abd99"},"cell_type":"code","source":"# LDA.\nlda_classifier = LinearDiscriminantAnalysis(n_components = 1)\nacc_lda = cross_val_score(lda_classifier, train_set, y, cv = 10, scoring = acc_sco)\nf1_lda = cross_val_score(lda_classifier, train_set, y, cv = 10, scoring = f1_sco)\n\nprint(np.mean(acc_lda))\nprint(np.mean(f1_lda))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ca32c557e2f26c14532f0d0e4bb61ce49cebca55"},"cell_type":"code","source":"lda_classifier = LinearDiscriminantAnalysis(n_components = 2)\nacc_lda2 = cross_val_score(lda_classifier, train_set, y, cv = 10, scoring = acc_sco)\nf1_lda2 = cross_val_score(lda_classifier, train_set, y, cv = 10, scoring = f1_sco)\n\nprint(np.mean(acc_lda2))\nprint(np.mean(f1_lda2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"70b857a6f6ad64f0ae689bda1fd8c8300b76db0b"},"cell_type":"code","source":"# Logistic Regression.\nlr_classifier = LogisticRegression()\nacc_lr = cross_val_score(lr_classifier, train_set, y, cv = 10, scoring = acc_sco)\nf1_lr = cross_val_score(lr_classifier, train_set, y, cv = 10, scoring = f1_sco)\n\nprint(np.mean(acc_lr))\nprint(np.mean(f1_lr))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"aa18c046efa913de8a28ecb1e26d7ee0b7590a72"},"cell_type":"code","source":"all_model = pd.DataFrame(columns = ['model', 'avg_acc', 'avg_f1'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e7d5b6c8a5b03b525b2574d29560797740e0c32d"},"cell_type":"code","source":"all_model = all_model.append(pd.DataFrame({'model': 'kNN', 'avg_acc': np.mean(acc_knn), \n                                          'avg_f1': np.mean(f1_knn)}, index = [0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fc631d7ff1acf17cd1a31d5d6d23628ab6bbc8f9"},"cell_type":"code","source":"all_model = all_model.append(pd.DataFrame({'model': 'Random Forest', 'avg_acc': np.mean(acc_rf), \n                                          'avg_f1': np.mean(f1_rf)}, index = [0]))\nall_model = all_model.append(pd.DataFrame({'model': 'SVM', 'avg_acc': np.mean(acc_svm), \n                                          'avg_f1': np.mean(f1_svm)}, index = [0]))\nall_model = all_model.append(pd.DataFrame({'model': 'Naive Bayes', 'avg_acc': np.mean(acc_nb), \n                                          'avg_f1': np.mean(f1_nb)}, index = [0]))\nall_model = all_model.append(pd.DataFrame({'model': 'LDA', 'avg_acc': np.mean(acc_lda), \n                                          'avg_f1': np.mean(f1_lda)}, index = [0]))\nall_model = all_model.append(pd.DataFrame({'model': 'Logistic Regression', 'avg_acc': np.mean(acc_lr), \n                                          'avg_f1': np.mean(f1_lr)}, index = [0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a9c37752333a40cd7dca547f516439e34358b41e"},"cell_type":"code","source":"print(all_model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2c65cbe05fed7d5e7719b737f479bad9f01a80d5"},"cell_type":"code","source":"all_model.set_index('model', inplace = True)\nall_model['avg_acc'].plot.bar(color = 'orange', figsize = (8, 6), yerr = list(all_model['avg_acc']))\nplt.title('Model Average Accuracy Comparision')\nplt.ylabel('Average Accuracy with F1 Score')\nall_model.reset_index(inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2bbf77bcfef5481e448fa402baaefa1fe72ca5a6"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c73ae6e92eabec260df3f3a69c717742a7f2806f"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c2cb18ab1e349203e83c1e552b46bda68e417cdc"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}