{"cells":[{"metadata":{"_uuid":"3087a96d690f55d82f6c301334ca5c296fb4508b"},"cell_type":"markdown","source":"# Home Credit Solution \n## This Kernal is made for beginners learning purpose.\n### Feel free to fork and use it. \n### You will learn how to handle large dataset without being confused.\n![](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSkr6pXKnRSPy5tYRwUBMr0nZwjAzDVAzVFAigd6bauuwvHoQTf)\n"},{"metadata":{"_uuid":"8d588c1f25daf2de9d84b9b86cb20d98df5ac302"},"cell_type":"markdown","source":"# 1. Introduction\nIn this notebook, we will take an initial look at the Home Credit default risk machine learning competition currently hosted on Kaggle. The objective of this competition is to use historical loan application data to predict whether or not an applicant will be able to repay a loan."},{"metadata":{"_uuid":"bf71e3000fcfe57176d1cbbe24097fae518f6f24"},"cell_type":"markdown","source":"# 2. Importing requires packages"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"# numpy and pandas for data manipulation\nimport numpy as np\nimport pandas as pd \n\n# sklearn preprocessing for dealing with categorical variables\nfrom sklearn.preprocessing import LabelEncoder\n\n# File system manangement\nimport os\nimport gc\n# Suppress warnings \nimport warnings\nwarnings.filterwarnings('ignore')\n\n# matplotlib and seaborn for plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"96bfa9ad20a17c549082a231a25f2ad7910d637e"},"cell_type":"markdown","source":"# 3. Retrieving the Data"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"collapsed":true},"cell_type":"code","source":"# List files available\nprint(os.listdir(\"../input/\"))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"92dc8d3311c854878d6544e256c1873e0c008087"},"cell_type":"markdown","source":"# 4. Exploration of Application Train/Test Data."},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"5d09edbeaea23b434aba4040877a95cd227058ce","collapsed":true},"cell_type":"code","source":"# Training data\napp_train = pd.read_csv('../input/application_train.csv')\napp_test = pd.read_csv('../input/application_test.csv')\nprint('Training data shape: {}, Testing data shape: {}'.format( app_train.shape,app_test.shape))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7866b36782828432e72d1b46a7175470aa080061"},"cell_type":"markdown","source":"## 4.1 Merge both train and test dataset"},{"metadata":{"trusted":true,"_uuid":"d07be4ad6b75346b7931125aab517aa39aa5742e","collapsed":true},"cell_type":"code","source":"df = app_train\ndf = df.append(app_test)\ndf.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"43fde147c2a6fc364707c252a404b71ea1e98355"},"cell_type":"markdown","source":"We can see there are some anamolies in  `DAYS_EMPLOYED`.\n\n`DAYS_EMPLOYED` at max seems to have very large positive value.\n"},{"metadata":{"_uuid":"9d1a6e70189b3446627ba3f2d4386a79642625ed"},"cell_type":"markdown","source":"## 4.2 Removing Anamolies / Outlier\nIn statistics, an outlier is an observation point that is distant from other observations. An outlier may be due to variability in the measurement or it may indicate experimental error; the latter are sometimes excluded from the data set. An outlier can cause serious problems in statistical analyses.\n\n\n"},{"metadata":{"trusted":true,"_uuid":"3a76ddc0858aec603ca2e819806ecb6f540d02c5","collapsed":true},"cell_type":"code","source":"app_train['DAYS_EMPLOYED'].plot.hist(title = 'Days Employment Histogram');\nplt.xlabel('Days Employment');","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"acd16b22ea8afc01a10763ca2edabea53e9abdf5"},"cell_type":"markdown","source":"365243 days is somewhat around 1000 years, which is impossible.\n\nSo, its best for us to replace it by NaN."},{"metadata":{"trusted":true,"_uuid":"9388c3b659a85e9b4fbfa41570162fb1e6568cfd","collapsed":true},"cell_type":"code","source":"# NaN values for DAYS_EMPLOYED: 365.243 -> nan\ndf['DAYS_EMPLOYED'].replace(365243, np.nan, inplace= True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8068216fe68d03c2728d5a28f05e9c3ba15fed74"},"cell_type":"markdown","source":"## 4.3 Categorical Variables "},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"93b650b0b5ad44230ba4f6ca20de27b5acc42846","collapsed":true},"cell_type":"code","source":"# Number of unique classes in each object column\napp_train.select_dtypes('object').apply(pd.Series.nunique, axis = 0)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3e0667c221e50c8d98eafbfc079fa0aca706b64b"},"cell_type":"markdown","source":"`CODE_GENDER` has unknown value 'XNA'. Its better to remove these 4 rows."},{"metadata":{"_kg_hide-output":false,"_kg_hide-input":true,"trusted":true,"_uuid":"7d45d560cf48816e56c7c79f35fb3ff4821f3365","collapsed":true},"cell_type":"code","source":"print (df['CODE_GENDER'].value_counts())\n\n# Remove the rows with XNA value in CODE_GENDER\nprint ('\\nSize Before {}'.format(df.shape))\ndf = df[df['CODE_GENDER'] != 'XNA']\nprint ('Size After {}'.format(df.shape))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"98b9bcdd91be82ca4e71bdeb0d5d0c5ac9a00ead"},"cell_type":"markdown","source":"## 4.4 Label Encoder and One Hot Encoding\n\n**Label Encoder **: As you might know by now, we can’t have text in our data if we’re going to run any kind of model on it. So before we can run a model, we need to make this data ready for the model.\n\nAnd to convert this kind of categorical text data into model-understandable numerical data, we use the Label Encoder class.\n\nSuppose, we have a feature State which has 3 category i.e India , France, China . So, Label Encoder will categorize them as 0, 1, 2.\n\n**One Hot Encoding** : One hot encoding is a process by which categorical variables are converted into a form that could be provided to ML algorithms to do a better job in prediction. \n\nIf we applied OHE on say Gender column which has category male,female. OHE will create two new column Gender_male, Gender_female and store the value 0 and 1 according to the main category value."},{"metadata":{"trusted":true,"_uuid":"4353d5c61b5f7477e4cba6e5ccea6377d05a7ba9","scrolled":true,"collapsed":true},"cell_type":"code","source":"# Create a label encoder object\nle = LabelEncoder()\nle_count = 0\n# Iterate through the columns\nfor col in df:\n    if df[col].dtype == 'object':\n        # If 2 or fewer unique categories\n        if len(list(df[col].unique())) <= 2:\n            le.fit(df[col])\n            df[col] = le.transform(df[col])\n            le_count += 1\nprint ('{} variable are label encoded'.format(le_count))\n\n\n# One Hot Encoding \ncategorical_columns = [col for col in df.columns if df[col].dtype == 'object']\ndf = pd.get_dummies(df, columns= categorical_columns, dummy_na= True)\nprint ('The shape of dataset after One hot encoding: {}'.format(df.shape))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4e88bb99f17265c9f742e022e39f89abc2708163"},"cell_type":"markdown","source":"## 4.5 Add some feature variables\nSince i don't consider myself as a credit expert. I have used this features from [this](https://www.kaggle.com/jsaguiar/updated-0-792-lb-lightgbm-with-simple-features/code#L261) script by Aguiar.\n\n- DAYS_EMPLOYED_PERC: the percentage of the days employed relative to the client's age.\n- INCOME_CREDIT_PERC: the percentage of the credit amount relative to a client's income.\n- INCOME_PER_PERSON : the percentage of income per person.\n- ANNUITY_INCOME_PERC: the percentage of the loan annuity relative to a client's income.\n- PAYMENT_RATE : the percentage of rate of payment annually."},{"metadata":{"trusted":true,"_uuid":"9ace9e56fc96b1c8522e83b40907a6e4017795f3","collapsed":true},"cell_type":"code","source":"df['DAYS_EMPLOYED_PERC'] = df['DAYS_EMPLOYED'] / df['DAYS_BIRTH']\ndf['INCOME_CREDIT_PERC'] = df['AMT_INCOME_TOTAL'] / df['AMT_CREDIT']\ndf['INCOME_PER_PERSON'] = df['AMT_INCOME_TOTAL'] / df['CNT_FAM_MEMBERS']\ndf['ANNUITY_INCOME_PERC'] = df['AMT_ANNUITY'] / df['AMT_INCOME_TOTAL']\ndf['PAYMENT_RATE'] = df['AMT_ANNUITY'] / df['AMT_CREDIT']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bbae91d713e58ad1f4d1aec26c5d1c2a576b9a49"},"cell_type":"markdown","source":"## Target Column Distribution \nThe target is what we are asked to predict: either a 0 for the loan was repaid on time, or a 1 indicating the client had payment difficulties. We can first examine the number of loans falling into each category."},{"metadata":{"trusted":true,"_uuid":"b95194fd3173620aacccaf2dca25c1c2483dc301","collapsed":true},"cell_type":"code","source":"app_train['TARGET'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":false,"_kg_hide-input":true,"trusted":true,"scrolled":true,"_uuid":"6df3c4a5ec08184ce4ad2b679e9269569cd0fe12","collapsed":true},"cell_type":"code","source":"app_train['TARGET'].astype(int).plot.hist();","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"402e1d4b650b47438acb3955ee9aa24602b66cfb"},"cell_type":"markdown","source":"There are far more loans that were repaid on time than loans that were not repaid. Once we get into more sophisticated machine learning models, we can weight the classes by their representation in the data to reflect this imbalance."},{"metadata":{"_uuid":"0f4c0c2eafde587d52c9d11e790e56f644f33ec6"},"cell_type":"markdown","source":"## Garbage Collection\nPython’s memory allocation and deallocation method is automatic. The user does not have to preallocate or deallocate memory similar to using dynamic memory allocation in languages such as C or C++.\n\nHere we are using **Manual Garbage Collection**\n\nInvoking the garbage collector manually during the execution of a program can be a good idea on how to handle memory being consumed by reference cycles.\nThe garbage collection can be invoked manually in the following way:"},{"metadata":{"trusted":true,"_uuid":"5801e936e0d04a613b5a012d69f0a3fdaeb1de7d","collapsed":true},"cell_type":"code","source":"del app_test, app_train\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"010b0443df92a322a4552871a8dba6f975c49931"},"cell_type":"markdown","source":"If you wish not to use GC then better watch your RAM. It is likely to over-exceed and Kernel error may come up."},{"metadata":{"_uuid":"c2884ca0da33fed7f3efed609aa1d503fed46468"},"cell_type":"markdown","source":"## Handling Categorical Features"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"44fad6607f547d42b42e330405ea9ae7cace49ed"},"cell_type":"code","source":"# function to obtain Categorical Features\ndef _get_categorical_features(df):\n    feats = [col for col in list(df.columns) if df[col].dtype == 'object']\n    return feats\n\n# function to factorize categorical features\ndef _factorize_categoricals(df, cats):\n    for col in cats:\n        df[col], _ = pd.factorize(df[col])\n    return df \n\n# function to create dummy variables of categorical features\ndef _get_dummies(df, cats):\n    for col in cats:\n        df = pd.concat([df, pd.get_dummies(df[col], prefix=col)], axis=1)\n    return df ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"c2565e144ab38bf44fa574e233671f78295e4a17"},"cell_type":"code","source":"# # factorize the categorical features from train and test data\n# df_cats = _get_categorical_features(df)\n# df = _factorize_categoricals(df, df_cats)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0611d7b5506df130bd520468997ea88ad54b578a","collapsed":true},"cell_type":"markdown","source":"# 5. Exploration of Bureau and Bureau_data"},{"metadata":{"trusted":true,"_uuid":"1e00071483a9a0f08d1bfd2e738b1a77898d7ce2","collapsed":true},"cell_type":"code","source":"bureau = pd.read_csv('../input/bureau.csv', nrows = None)\nbb = pd.read_csv('../input/bureau_balance.csv', nrows = None)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cbc5e03abc2fadf4ed578278b70c585195a6405e"},"cell_type":"markdown","source":"## 5.1 One Hot Encoding"},{"metadata":{"trusted":true,"_uuid":"eeb10caba8b02429f349a9b27a5a1951280ef0bc","collapsed":true},"cell_type":"code","source":"bereau_cats = _get_categorical_features(bureau)\nbb_cats = _get_categorical_features(bb)\n\nbureau = _get_dummies(bureau,bereau_cats)\nbb = _get_dummies(bb,bb_cats)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"011378235f3d2f9c877ac810b25550f09bc00ef8"},"cell_type":"markdown","source":"## 5.2 Feature Engineering - Bureau Data"},{"metadata":{"trusted":true,"_uuid":"fa9d9b0ed7d82e2d697983043de1445f9d24eb0f","collapsed":true},"cell_type":"code","source":"# Average Values for all bureau features \nbureau_avg = bureau.groupby('SK_ID_CURR').mean()\nbureau_avg['buro_count'] = bureau[['SK_ID_BUREAU','SK_ID_CURR']].groupby('SK_ID_CURR').count()['SK_ID_BUREAU']\nbureau_avg.columns = ['b_' + f_ for f_ in bureau_avg.columns]\ndf = df.merge(right=bureau_avg.reset_index(), how='left', on='SK_ID_CURR')\n#df.head()\ndel bb, bureau_avg\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5402dc0e5b7acf9a177b94249ee186b0205b4e90"},"cell_type":"markdown","source":"# 6. Exploration of Previous Application"},{"metadata":{"trusted":true,"_uuid":"1291f7782b48ceffe05f29b895e65e979f7ca67b","collapsed":true},"cell_type":"code","source":"prev = pd.read_csv('../input/previous_application.csv', nrows = None)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cc3e1b660e7512b48b364d1e8b9e152ed4528a80"},"cell_type":"markdown","source":"## 6.1 Handling Outliers"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"5c67c7c7de830aeb42e45c2275f09ce51d252be6"},"cell_type":"code","source":"prev['DAYS_FIRST_DRAWING'].replace(365243, np.nan, inplace= True)\nprev['DAYS_FIRST_DUE'].replace(365243, np.nan, inplace= True)\nprev['DAYS_LAST_DUE_1ST_VERSION'].replace(365243, np.nan, inplace= True)\nprev['DAYS_LAST_DUE'].replace(365243, np.nan, inplace= True)\nprev['DAYS_TERMINATION'].replace(365243, np.nan, inplace= True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c8b307f4c5aae26dfcf5550c7c2d34f72befcc6e"},"cell_type":"markdown","source":"## 6.2 One Hot Encoding"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"765cd8fd11dc5f63c9235031439531b234c9a1c8"},"cell_type":"code","source":"prev_app_cats = _get_categorical_features(prev)\nprev = _get_dummies(prev, prev_app_cats)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"618ca2846f5feb87574863c80ec443fc721f4179"},"cell_type":"markdown","source":"## 6.3 Feature Engineering - Previous Application"},{"metadata":{"trusted":true,"_uuid":"9e36a67a4a872bb1e6fe1588a862e5321f84b596","collapsed":true},"cell_type":"code","source":"## count the number of previous applications for a given ID\nprev_apps_count = prev[['SK_ID_CURR', 'SK_ID_PREV']].groupby('SK_ID_CURR').count()\nprev['SK_ID_PREV'] = prev['SK_ID_CURR'].map(prev_apps_count['SK_ID_PREV'])\n\n## Average values for all other features in previous applications\nprev_apps_avg = prev.groupby('SK_ID_CURR').mean()\nprev_apps_avg.columns = ['p_' + col for col in prev_apps_avg.columns]\ndf = df.merge(right=prev_apps_avg.reset_index(), how='left', on='SK_ID_CURR')\n\n## Garbage Collection\ndel prev, prev_apps_avg\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4382aa7739dc6c27137bc8b13794741cb20cd537"},"cell_type":"markdown","source":"# 7. Exploration of POS Cash Balance"},{"metadata":{"trusted":true,"_uuid":"50c46aec0b3f95b28956c4397d2e94b83eb1e84c","collapsed":true},"cell_type":"code","source":"pos = pd.read_csv('../input/POS_CASH_balance.csv', nrows = None)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d220d6e10f1716a6ce1bb2bf0ea618e34b6272e0"},"cell_type":"markdown","source":"## 7.1 One Hot Encoding"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"8765ad462f46a345e3dde9efc33d9cb97e2da6fc"},"cell_type":"code","source":"pos_cats = _get_categorical_features(pos)\npos = _get_dummies(pos, pos_cats)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a886fd19e34aa4e2bed9eabce2aa4ffa055c5ba7"},"cell_type":"markdown","source":"## 7.2 Feature Engineering - POS Cash Balance"},{"metadata":{"trusted":true,"_uuid":"69f06ba34abe1b9df55847eeaf8b27192c6799e1","collapsed":true},"cell_type":"code","source":"### count the number of pos cash for a given ID\npos_count = pos[['SK_ID_CURR', 'SK_ID_PREV']].groupby('SK_ID_CURR').count()\npos['SK_ID_PREV'] = pos['SK_ID_CURR'].map(pos_count['SK_ID_PREV'])\n\n## Average Values for all other variables in pos cash\npos_avg = pos.groupby('SK_ID_CURR').mean()\ndf = df.merge(right=pos_avg.reset_index(), how='left', on='SK_ID_CURR')\n\n\ndel pos, pos_avg\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f8e38e768741a7ed0ecae45b0256a226bd430dea"},"cell_type":"markdown","source":"# 8. Exploration of Installment Payments"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"462518633720db6e6ed44970c3be8c65a1e458a1"},"cell_type":"code","source":"ins = pd.read_csv('../input/installments_payments.csv', nrows = None)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c091f246e3e64673c5e8d533dce9a9ca13523086"},"cell_type":"markdown","source":"## 8.1 One Hot Encoding"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"3b57322728a6a17e6c099b56db6fcd7556b32fda"},"cell_type":"code","source":"ins_cats = _get_categorical_features(ins)\nins = _get_dummies(ins, ins_cats)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0bd58e38ff0acbb2510ae029c19d8a1fcd49b82f"},"cell_type":"markdown","source":"## 8.2 Adding some new features"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"312febe9c255cfdf4aa60731ba9f85a67eb68640"},"cell_type":"code","source":"# Percentage and difference paid in each installment (amount paid and installment value)\nins['PAYMENT_PERC'] = ins['AMT_PAYMENT'] / ins['AMT_INSTALMENT']\nins['PAYMENT_DIFF'] = ins['AMT_INSTALMENT'] - ins['AMT_PAYMENT']\n\n# Days past due and days before due (no negative values)\nins['DPD'] = ins['DAYS_ENTRY_PAYMENT'] - ins['DAYS_INSTALMENT']\nins['DBD'] = ins['DAYS_INSTALMENT'] - ins['DAYS_ENTRY_PAYMENT']\nins['DPD'] = ins['DPD'].apply(lambda x: x if x > 0 else 0)\nins['DBD'] = ins['DBD'].apply(lambda x: x if x > 0 else 0)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"98e28fab652cd47c8ad9ef14f4ac889e078c747d"},"cell_type":"markdown","source":"## 8.3 Feature Engineering - Installment Payments"},{"metadata":{"trusted":true,"_uuid":"bafc150100853613b8e19eb4a84c16bd3fb33727","collapsed":true},"cell_type":"code","source":"## count the number of previous installments\ncnt_inst = ins[['SK_ID_CURR', 'SK_ID_PREV']].groupby('SK_ID_CURR').count()\nins['SK_ID_PREV'] = ins['SK_ID_CURR'].map(cnt_inst['SK_ID_PREV'])\n\n## Average values for all other variables in installments payments\navg_inst = ins.groupby('SK_ID_CURR').mean()\navg_inst.columns = ['i_' + f_ for f_ in avg_inst.columns]\ndf = df.merge(right=avg_inst.reset_index(), how='left', on='SK_ID_CURR')\n\ndel ins, avg_inst\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9249c4627c0dd74da455bab7683087966e49641d"},"cell_type":"markdown","source":"# 9. Exploration of Credit Card"},{"metadata":{"trusted":true,"_uuid":"23dc2a5acbe8d5fd767158248c97ec3a88d1a9de","collapsed":true},"cell_type":"code","source":"cc = pd.read_csv('../input/credit_card_balance.csv', nrows = None)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ba4def079c1aac85c54ca57ed9adc82241fa1fda"},"cell_type":"markdown","source":"## 9.1 One Hot Encoding"},{"metadata":{"trusted":true,"_uuid":"56389d553951ca7ea8d491f85d1f7d9eefac9cff","collapsed":true},"cell_type":"code","source":"ccbal_cats = _get_categorical_features(cc)\ncredit_card_balance = _get_dummies(cc, ccbal_cats)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7ece39f68f44cfcf967da377b425491f6324290b"},"cell_type":"markdown","source":"## 9.2 Feature Engineering - Credit Card"},{"metadata":{"trusted":true,"_uuid":"bb64f15c35af67d291a3e37cd76933f5ee5d8f17","collapsed":true},"cell_type":"code","source":"### count the number of previous applications for a given ID\nnb_prevs = credit_card_balance[['SK_ID_CURR', 'SK_ID_PREV']].groupby('SK_ID_CURR').count()\ncredit_card_balance['SK_ID_PREV'] = credit_card_balance['SK_ID_CURR'].map(nb_prevs['SK_ID_PREV'])\n\n### average of all other columns \navg_cc_bal = credit_card_balance.groupby('SK_ID_CURR').mean()\navg_cc_bal.columns = ['cc_bal_' + f_ for f_ in avg_cc_bal.columns]\ndf = df.merge(right=avg_cc_bal.reset_index(), how='left', on='SK_ID_CURR')\n\ndel cc, avg_cc_bal\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ca0e83f7af3872c844713165309c98c537183486"},"cell_type":"markdown","source":"# 10. LightGBM\n\n- It is a gradient boosting framework that uses **tree based learning algorithm**.\n- It grows tree vertically while other algorithm grows trees horizontally meaning that Light GBM grows tree leaf-wise while other algorithm grows level-wise.\n-  It is prefixed as ‘Light’ because of its** high speed**.\n- It can handle the **large size of data** and takes **lower memory to run.**\n- This is popular because it focuses on **accuracy of results.**\n-  LGBM also supports **GPU learning.**\n\n**Why not to use Light GBM?**\n\n- It is **not advisable** to use LGBM on **small datasets**.\n- Light GBM is sensitive to overfitting and can easily **overfit small data**.\n\nThe only complicated thing is **parameter tuning.** Light GBM covers more than 100 parameters but don’t worry, you don’t need to learn all.\n\nLets learn about some of the parameters we used in our model :\n\n- **n_estimators** : number of boosting iterations.\n- **objective** : This is the most important parameter and specifies the application of your model, whether it is a regression problem or classification problem. LightGBM will by default consider model as a regression model.\n\n    - regression: for regression\n    - binary: for binary classification\n    - multiclass: for multiclass classification problem\n   \n- **learning_rate** : This determines the impact of each tree on the final outcome. GBM works by starting with an initial estimate which is updated using the output of each tree. The learning parameter controls the magnitude of this change in the estimates. Typical values: 0.1, 0.001, 0.003…\n- **reg_alpha** : L1 regularization\n-  **reg_lambda** : L2 regularization\n"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"78a26e928b0d4d97b539829d3026ba15ffd185ef"},"cell_type":"code","source":"from lightgbm import LGBMClassifier\nfrom sklearn.metrics import roc_auc_score, roc_curve\nfrom sklearn.model_selection import KFold, StratifiedKFold","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ebaba9834da56d250b2519c3af9a807f602c9f25"},"cell_type":"markdown","source":"## 10.1 Prepare Final Train and Test data"},{"metadata":{"trusted":true,"_uuid":"e831191319f489018f28a40ac116f30bdf764775","collapsed":true},"cell_type":"code","source":"# Divide in training/validation and test data\ntrain_df = df[df['TARGET'].notnull()]\ntest_df = df[df['TARGET'].isnull()]\nprint(\"Starting LightGBM. Train shape: {}, test shape: {}\".format(train_df.shape, test_df.shape))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3d090c8dba1cdb9783eb020a7625d299cd40fc8c"},"cell_type":"markdown","source":"## 10.2 Cross Validation Model\n**K-Fold Cross Validation**: To reduce variability, in most machine learning methods multiple rounds of cross-validation are performed using different partitions, and the validation are averaged at the end. This method is known as k-fold cross validation.\n\nIncrease the `n_splits` to make better prediction. But it may increase the time of processing."},{"metadata":{"trusted":true,"_uuid":"72d9d6fb37fbd90c0f116a315aba6eefc4c3ea7b","collapsed":true},"cell_type":"code","source":"folds = KFold(n_splits=2, shuffle=True, random_state=1001)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8333f124dcde94d2fe874909075fd1a251a5595b","collapsed":true},"cell_type":"code","source":"# Create arrays and dataframes to store results\noof_preds = np.zeros(train_df.shape[0])\nsub_preds = np.zeros(test_df.shape[0])\nfeature_importance_df = pd.DataFrame()\nfeats = [f for f in train_df.columns if f not in ['TARGET','SK_ID_CURR','SK_ID_BUREAU','SK_ID_PREV','index']]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8ce1f53eded9d9a461396351b52d55900944264d"},"cell_type":"markdown","source":"## 10.3 Fitting the model and Predicting"},{"metadata":{"trusted":true,"_uuid":"4f76c5f8f54bee6c0cb04f6220edbf750034a8c9","scrolled":false,"collapsed":true},"cell_type":"code","source":"# Iterate through each fold\nfor train_indices, valid_indices in folds.split(train_df[feats],train_df['TARGET']):\n        \n    # Training data for the fold\n    train_features, train_labels = train_df[feats].iloc[train_indices], train_df['TARGET'].iloc[train_indices]\n    # Validation data for the fold\n    valid_features, valid_labels = train_df[feats].iloc[valid_indices], train_df['TARGET'].iloc[valid_indices]\n        \n    # Create the model\n    model = LGBMClassifier(n_estimators=5000, objective = 'binary', \n                                   class_weight = 'balanced', learning_rate = 0.05, \n                                   reg_alpha = 0.1, reg_lambda = 0.1, \n                                   subsample = 0.8, n_jobs = -1, random_state = 50)\n    \n        \n    # Train the model\n    model.fit(train_features, train_labels, eval_metric = 'auc',\n                eval_set = [(valid_features, valid_labels), (train_features, train_labels)],\n                eval_names = ['valid', 'train'],\n                early_stopping_rounds = 100, verbose = 200)\n    \n    oof_preds[valid_indices] = model.predict_proba(valid_features, num_iteration=model.best_iteration_)[:, 1]\n    sub_preds += model.predict_proba(test_df[feats], num_iteration=model.best_iteration_)[:, 1] / folds.n_splits\n\n    fold_importance_df = pd.DataFrame()\n    fold_importance_df[\"feature\"] = feats\n    fold_importance_df[\"importance\"] = model.feature_importances_\n    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n    del model, train_features, train_labels, valid_features, valid_labels\n    gc.collect()\n\nprint('Full AUC score %.6f' % roc_auc_score(train_df['TARGET'], oof_preds))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"384c0148e87ca10f93a3b5bc56aac1ba27c3b8ac"},"cell_type":"markdown","source":"## 10.5 Submission"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"4cffc8ba362bb8486ab9d957ea5958031c8cba6c"},"cell_type":"code","source":"test_df['TARGET'] = sub_preds\ntest_df[['SK_ID_CURR', 'TARGET']].to_csv('submission.csv', index= False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c6090253fa93d3bf2b28fd5f8357943a67a4d15b"},"cell_type":"markdown","source":"**Thanks for taking time to go through the kernel. **\n\n**Show your appreciation by upvoting or commenting about your feedbacks. **"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}