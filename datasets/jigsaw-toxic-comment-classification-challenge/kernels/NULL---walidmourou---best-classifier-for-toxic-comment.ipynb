{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport nltk\nfrom nltk.stem.wordnet import WordNetLemmatizer \nimport string\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import LinearSVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix, roc_curve, auc\nimport os\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"# Toxic Comment Classification Challenge\n**Identify and classify toxic online comments**\n\nThe data is a collection of comment text that has been classified throw six classes.\nThe competition consist on predicting negative online behaviours, like toxic comments (i.e. comments that are rude, disrespectful or otherwise likely to make someone leave a discussion). So the goal is to create a classification model that can perform the highest accuracy."},{"metadata":{"_uuid":"493a310e665fd5d4900d2363a763262cf34df86a"},"cell_type":"markdown","source":"## Analyse the data"},{"metadata":{"trusted":true,"_uuid":"688665f1929714110ef23e311d0883e159255a6d"},"cell_type":"code","source":"# Load the data\ndata = pd.read_csv('../input/train.csv')\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"950f44bc39a317436f2eeb198b29068c62720bab"},"cell_type":"code","source":"print(\"There is {} messages.\".format(len(data)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8670880fdbc49f63d6868278661535101333dd60"},"cell_type":"markdown","source":"**Comment classes**\n\nLet's look at the different classes and how many comment by class. That is clear we have an imbalanced data throw calsses. When we encounter such problems, we are bound to have difficulties solving them with standard algorithms. Conventional algorithms are often biased towards the majority class, not taking the data distribution into consideration. In the worst case, minority classes are treated as outliers and ignored. For some cases, such as fraud detection or cancer prediction, we would need to carefully configure our model or artificially balance the dataset, for example by undersampling or oversampling each class.\n\nHowever, in our case of learning imbalanced data, the majority classes might be of our great interest. It is desirable to have a classifier that gives high prediction accuracy over the majority class, while maintaining reasonable accuracy for the minority classes. Therefore, we will leave it as it is."},{"metadata":{"trusted":true,"_uuid":"b100937ebad5ff9965b3b4d8c01c8ca182a8d98e"},"cell_type":"code","source":"classes = ['toxic','severe_toxic','obscene','threat','insult','identity_hate']\noccurence = []\nprint(\"\\n{:^15} | {:^15} | {:^5}\".format(\"Class\", \"Occurrence\", \"%\"))\nprint(\"*\"*42)\nfor clas in classes:\n    print(\"{:15} | {:>15} | {:^5.2f}\".format(clas, \n                                             data[clas].value_counts()[1], \n                                             data[clas].value_counts()[1]*100/len(data)\n                                            )\n         )\n    occurence.append(data[clas].value_counts()[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c382f89d5cd4a4fe9ebb7aaf42650618e9c4421c"},"cell_type":"code","source":"plt.figure(figsize=(10, 6))\nplt.bar(classes, occurence)\nplt.title(\"Number of comments per category\")\nplt.ylabel('# of Occurrences', fontsize=12)\nplt.xlabel('category', fontsize=12)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"48e316861baef2d52dab22fd3bfb59c6c2a891e6"},"cell_type":"markdown","source":"**Remarks**\n\n9.58% of the messages are considered toxic and 1% are considered severe toxic. \n\nBut a message can belong to more than one class so let's take a look:"},{"metadata":{"trusted":true,"_uuid":"73025a6e2d003ad2780791d6b8c537f4b79a89b8"},"cell_type":"code","source":"data['all'] = data[classes].sum(axis=1)\ndata['any'] = data['all'].apply(lambda x:1 if x>0 else 0)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"43638d59624d338a0ae221789e0cff4af726111d"},"cell_type":"code","source":"in_classes = data['all'].value_counts()\nprint(\"\\n{:^10} | {:^10} | {:^6}\".format(\"# Classes\", \"# Comment\", \"%\"))\nprint(\"*\"*33)\nfor idx in range(7):\n    print(\"{:10} | {:>10} | {:>6.2f}\".format(idx, \n                                             in_classes[idx], \n                                             in_classes[idx]*100/len(data)\n                                            )\n         )\nprint(\"*\"*33)\nprint(\"{:^10} | {:>10} | {:>6}\".format(\"\", len(data), \"100.00\"))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0025537e8afaaf4f5c5910ffdbdee37fa3adb9d7"},"cell_type":"markdown","source":"**Remarks**\n\n3.99% of messages belong only to one class and 2.18% belong to two classes. \n\nThere is 31 messages that belong to all classes."},{"metadata":{"trusted":true,"_uuid":"706c87b124ab056aa655386fbfab248fd223b1a9"},"cell_type":"code","source":"df = pd.DataFrame(in_classes.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d3aa63ddda1fb5ab0eb9ea16714d0e548d77fd1e"},"cell_type":"code","source":"ax = df.plot.bar(stacked=True, figsize=(10, 6), legend=False)\nax.set_ylabel('# of Occurrences', fontsize=12)\nax.set_xlabel('# of classes', fontsize=12)\nax.set_title(\"# of messages per # of classes associated\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0f72386c6ef72eb8eb2b7b1f9a66d275d159cbbf"},"cell_type":"markdown","source":"**Examples of toxic message**"},{"metadata":{"trusted":true,"_uuid":"5d5695d00f6e5a34346103d3a7aed69ca6aafaa7"},"cell_type":"code","source":"# toxic\ndata[data['toxic']==1].iloc[1,1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bbb059ed3c071ef63b890280fd200bea6f60b14a"},"cell_type":"code","source":"# severe_toxic\ndata[data['severe_toxic']==1].iloc[2,1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"aab4bb60ec6966e39a73171ed4f9803cd52e1f12"},"cell_type":"code","source":"# obscene\ndata[data['obscene']==1].iloc[3,1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a6df9118c0355c895b94890a5dbee3c416ea0dac"},"cell_type":"code","source":"# threat\ndata[data['threat']==1].iloc[4,1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ebe65414fd77a8f25b84493de1068cc0904d1950"},"cell_type":"code","source":"# insult\ndata[data['insult']==1].iloc[5,1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eedd4cf9dad8b0dd160eadd3b008d51e11c8c883"},"cell_type":"code","source":"# identity_hate\ndata[data['identity_hate']==1].iloc[6,1]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"23376ede71cd86dfcb19a9ad6c487d68a33e24c5"},"cell_type":"markdown","source":"**Comment text behavior**\n\nLet's look at the length of the comment text"},{"metadata":{"trusted":true,"_uuid":"43a30f8722649edf71f4209afb9d05d1ddd2851c"},"cell_type":"code","source":"lens = data['comment_text'].str.len()\nlens.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"604e6595bca98dffbeab7fa7db6db8c886bcc5ce"},"cell_type":"code","source":"# Statistics:\nprint('Minimum : ', lens.min())\nprint('Maximum : ', lens.max())\nprint('Median : ', lens.median())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"66e602969b4c8031029c783e27f539ebf6e5398b"},"cell_type":"code","source":"# horizontal boxplot\nplt.figure(figsize=(15,4))\nplt.boxplot(lens, 0, 'gD', 0, showmeans=True)\n# The length of comment text is varying a lot. There is a lot of outlier.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"592c3d5d204141ac639b14732a115bc666e513e5"},"cell_type":"markdown","source":"## Natural Language Processing (NLP)"},{"metadata":{"trusted":true,"_uuid":"93e4b19ff17add96a1ed080acbf43ee5ef9c586a"},"cell_type":"code","source":"# Split data using stratifying variable \"all\" to take into account the imbalanced data throw calsses\ndatatrain, datatest = train_test_split(data, test_size=0.2, stratify=data[\"all\"], random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"67caccd38e78ae5d2942aa057e626a754d66190b"},"cell_type":"markdown","source":"### Text Preprocessing\n#### Cleaning data (Noise Removal)\nAny piece of text which is not relevant to the context of the data and the end-output can be specified as the noise.\n\nFor example – language stopwords (commonly used words of a language – is, am, the, of, in etc), URLs or links, social media entities (mentions, hashtags), punctuations and industry specific words. This step deals with removal of all types of noisy entities present in the text.\n\nA general approach for noise removal is to prepare a dictionary of noisy entities, and iterate the text object by tokens (or by words), eliminating those tokens which are present in the noise dictionary."},{"metadata":{"trusted":true,"_uuid":"b4a0deca5289c192c2fdd8f9d39cb4a3dd2c923b"},"cell_type":"code","source":"# Here we create a list of noisy entities\nuseless_words = nltk.corpus.stopwords.words(\"english\") + list(string.punctuation) + [\"\\'m\"] + [\"\\'s\"] + [\"\\'\\'\"] + [\"``\"] + [\"n\\'t\"] + [\"ca\"]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d9611412fe4aef06f7c81f5ec4900ba114963033"},"cell_type":"markdown","source":"#### Lexicon Normalization\n* Stemming:  Stemming is a rudimentary rule-based process of stripping the suffixes (“ing”, “ly”, “es”, “s” etc) from a word.\n* Lemmatization: Lemmatization, on the other hand, is an organized & step by step procedure of obtaining the root form of the word, it makes use of vocabulary (dictionary importance of words) and morphological analysis (word structure and grammar relations)."},{"metadata":{"trusted":true,"_uuid":"3529a1669dc64521677891e9ca5b4f28bfd47f23"},"cell_type":"code","source":"lem = WordNetLemmatizer()\ndef clean_data(txt):\n    txt = nltk.word_tokenize(txt.lower())\n    txt = [word for word in txt if not word in useless_words]\n    txt = [lem.lemmatize(w, \"v\") for w in txt]\n    return ' '.join(word for word in txt)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"72691bc1d4ace55409bb7f1fbfbb885aa551035f"},"cell_type":"code","source":"# datatest['comment_text'] = datatest['comment_text'].apply(lambda x:clean_data(x))\n# datatrain['comment_text'] = datatrain['comment_text'].apply(lambda x:clean_data(x))\n\ndatatest['comment_text'] = datatest['comment_text'].apply(lambda x:clean_data(x))\ndatatrain['comment_text'] = datatrain['comment_text'].apply(lambda x:clean_data(x))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3563c405d61f1a558c77592b7a70dc098d47e1a5"},"cell_type":"markdown","source":"### Text to Features (Feature Engineering on text data)\nTo analyse a preprocessed data, it needs to be converted into features. Depending upon the usage, text features can be constructed using assorted techniques – Syntactical Parsing, Entities / N-grams / word-based features, Statistical features, and word embeddings.\n#### Term Frequency – Inverse Document Frequency (TF – IDF)\nTF-IDF is a weighted model commonly used for information retrieval problems. It aims to convert the text documents into vector models on the basis of occurrence of words in the documents without taking considering the exact ordering. For Example – let say there is a dataset of N text documents, In any document “D”, TF and IDF will be defined as –\n\nTerm Frequency (TF) – TF for a term “t” is defined as the count of a term “t” in a document “D”\n\nInverse Document Frequency (IDF) – IDF for a term is defined as logarithm of ratio of total documents available in the corpus and number of documents containing the term T."},{"metadata":{"trusted":true,"_uuid":"90b5cfbd72eafd6b5789de0a61dcd7626b9a482b"},"cell_type":"code","source":"def ROC_curve_plot(datatest, prediction, classes, figure_title):\n    # Compute ROC curve and ROC area for each class\n    nbr_classes = len(classes)\n    fpr = dict()\n    tpr = dict()\n    roc_auc = dict()\n\n    y = np.zeros(nbr_classes*len(datatest))\n    y_hat = np.zeros(nbr_classes*len(datatest))\n\n    for idx,clas in enumerate(classes):\n        print('... Processing {}'.format(clas))\n        print('Cofusion Matrix:\\n', confusion_matrix(datatest[clas], prediction[:,idx]))\n        fpr[clas], tpr[clas], _ = roc_curve(datatest[clas], prediction[:,idx])\n        roc_auc[clas] = auc(fpr[clas], tpr[clas])\n\n        y[idx*len(datatest):(idx+1)*len(datatest)] = datatest[clas].values\n        y_hat[idx*len(datatest):(idx+1)*len(datatest)] = prediction[:,idx]\n        \n    # Compute average ROC curve and ROC area\n    fpr[\"all\"], tpr[\"all\"], _ = roc_curve(y, y_hat)\n    roc_auc[\"all\"] = auc(fpr[\"all\"], tpr[\"all\"])\n    \n    plt.figure(figsize=(10,10))\n    for i in [\"all\"] + classes:\n        plt.plot(fpr[i], tpr[i], label='{0} (area = {1:0.2f})'.format(i, roc_auc[i]))\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.0])\n    plt.xlabel('False Positive Rate', fontsize=12)\n    plt.ylabel('True Positive Rate' , fontsize=12)\n    plt.title(figure_title,           fontsize=12)\n    plt.legend(loc=\"lower right\",     fontsize=12)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"faebfa8eae37f60f65bc88d07789a10ce1aa5950"},"cell_type":"markdown","source":"### Naive Bayes"},{"metadata":{"trusted":true,"_uuid":"289b57f8f0f5bf6e4d6465ffcb13e0cfda81bb68"},"cell_type":"code","source":"NB_pipeline = Pipeline([\n                        ('tfidf', TfidfVectorizer()),\n                        ('clf', OneVsRestClassifier(MultinomialNB(fit_prior=True, class_prior=None))),\n                       ])\n\nNB_pipeline.fit(datatrain['comment_text'], datatrain[classes])\nprediction = NB_pipeline.predict(datatest['comment_text'])\n\nROC_curve_plot(datatest, prediction, classes, 'ROC curve : Naive Bayes Classifier')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f227c7087f8474f031c147adccab467bab809805"},"cell_type":"markdown","source":"### LinearSVC"},{"metadata":{"trusted":true,"_uuid":"e0fe7c5437656d35816089f76f9c4e652de3a02a"},"cell_type":"code","source":"SVC_pipeline = Pipeline([\n                         ('tfidf', TfidfVectorizer()),\n                         ('clf', OneVsRestClassifier(LinearSVC(), n_jobs=1)),\n                        ])\n\nSVC_pipeline.fit(datatrain['comment_text'], datatrain[classes])\nprediction = SVC_pipeline.predict(datatest['comment_text'])\n\nROC_curve_plot(datatest, prediction, classes, 'ROC curve : Linear SVC Classifier')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"832664a293c4637f49eac984c149cbdc3740499c"},"cell_type":"markdown","source":"### Logistic Regression"},{"metadata":{"trusted":true,"_uuid":"dc9056be08f32e6275aa88a647d7b27d43dc1200"},"cell_type":"code","source":"LogReg_pipeline = Pipeline([\n                            ('tfidf', TfidfVectorizer()),\n                            ('clf', OneVsRestClassifier(LogisticRegression(solver='sag'), n_jobs=1)),\n                           ])\n\nLogReg_pipeline.fit(datatrain['comment_text'], datatrain[classes])\nprediction = LogReg_pipeline.predict(datatest['comment_text'])\n\nROC_curve_plot(datatest, prediction, classes, 'ROC curve : Logistic Regression Classifier')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"49c31d418a2ee3526d084dad4a6d08537a38ed91"},"cell_type":"markdown","source":"### Random Forest"},{"metadata":{"trusted":true,"_uuid":"ce02bdcd1575e0c4ee8cd4b10fa0a7a147945426"},"cell_type":"code","source":"# from sklearn.ensemble import RandomForestClassifier\n# RandomForest_pipeline = Pipeline([\n#                             ('tfidf', TfidfVectorizer()),\n#                             ('clf', OneVsRestClassifier(RandomForestClassifier(), n_jobs=1)),\n#                            ])\n\n# RandomForest_pipeline.fit(datatrain['comment_text'], datatrain[classes])\n# prediction = RandomForest_pipeline.predict(datatest['comment_text'])\n\n# ROC_curve_plot(datatest, prediction, classes, 'ROC curve : Random Forest Classifier')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"62ad3a81a8b42d61b0886577099cca65df26613d"},"cell_type":"markdown","source":"### XGBoost"},{"metadata":{"trusted":true,"_uuid":"8e026fa7c35d6ec19b5b32ae8ef3f1792e2f40f3"},"cell_type":"code","source":"from xgboost import XGBClassifier\nXGBoost_pipeline = Pipeline([\n                            ('tfidf', TfidfVectorizer()),\n                            ('clf', OneVsRestClassifier(XGBClassifier(), n_jobs=1)),\n                           ])\n\nXGBoost_pipeline.fit(datatrain['comment_text'], datatrain[classes])\nprediction = XGBoost_pipeline.predict(datatest['comment_text'])\n\nROC_curve_plot(datatest, prediction, classes, 'ROC curve : XGBoost Classifier')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a3d831553991f6ca485ea4cab77417083a18c031"},"cell_type":"code","source":"### Decision tree classifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"42113057c99d653d50d176e5b9c33df0902e27de"},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\nDecisionTree_pipeline = Pipeline([\n                            ('tfidf', TfidfVectorizer()),\n                            ('clf', OneVsRestClassifier(DecisionTreeClassifier())),\n                           ])\n\nDecisionTree_pipeline.fit(datatrain['comment_text'], datatrain[classes])\nprediction = DecisionTree_pipeline.predict(datatest['comment_text'])\n\nROC_curve_plot(datatest, prediction, classes, 'ROC curve : Decision Tree Classifier')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a15a2eff20d7109a8eece87227c6ee8929b82193"},"cell_type":"code","source":"### Multi-layer Perceptron","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"87ff97ce36ad01e3a7ba72863809a522f815d033"},"cell_type":"code","source":"# from sklearn.neural_network import MLPClassifier\n# MLPClassifier_pipeline = Pipeline([\n#                             ('tfidf', TfidfVectorizer()),\n#                             ('clf', OneVsRestClassifier(MLPClassifier())),\n#                            ])\n\n# MLPClassifier_pipeline.fit(datatrain['comment_text'], datatrain[classes])\n# prediction = MLPClassifier_pipeline.predict(datatest['comment_text'])\n\n# ROC_curve_plot(datatest, prediction, classes, 'ROC curve : Multi-layer Perceptron Classifier')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d45ba070e686229b1624ec828f9ffb41b833a8c8"},"cell_type":"markdown","source":"                                        Area Under the ROC Curve Table"},{"metadata":{"_uuid":"f127d137fe796cd8cc9965bf3340028a4b043f14"},"cell_type":"markdown","source":"| Classifier | ALL   |Toxic  | Severe Toxic   |Obscene  | Threat   |Insult  | Identity Hate   |\n|------|------|------|------|------|------|------|------|\n|   Naive Bayes | 0.55|   0.58  | 0.50|   0.55  | 0.50|   0.52  | 0.50|\n|   LinearSVC  | 0.81|   0.84  | 0.64|   0.86  | 0.57|   0.78  | 0.62|\n|   Logistic Regression  | 0.77|   0.80  | 0.62|   0.82  | 0.53|   0.75  | 0.58|\n|   Random Forest  | 0.72|   0.74  | 0.53|   0.77  | 0.51|   0.70  | 0.53|\n|   XGBoost  | 0.71|   0.71  | 0.55|   0.78  | 0.55|   0.70  | 0.57|\n|   Decision tree classifier  | 0.82|   0.83  | 0.61|   0.88  | 0.61|   0.79  | 0.67|\n|   Multi-layer Perceptron  | 0.81|   0.84  | 0.64|   0.86  | 0.61|   0.78  | 0.66|"},{"metadata":{"trusted":true,"_uuid":"b138b0e9a2885b54cf0ac980c45dc1314a2bc7e3"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}