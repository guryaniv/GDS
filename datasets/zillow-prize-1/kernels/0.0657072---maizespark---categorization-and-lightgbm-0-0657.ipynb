{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":false,"collapsed":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"code","source":"# data analysis and wrangling\nimport random as rnd\n\n# visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# machine learning\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neural_network import MLPClassifier","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"9e51790d-17ae-42ad-8912-dd61db561baa","scrolled":true,"_uuid":"1a12d32d60ee9f3b9735acbd367998ccd18648eb","trusted":false,"collapsed":true},"cell_type":"code","source":"df2016 = pd.read_csv('../input/train_2016_v2.csv')\ndf2016['merger']=df2016.parcelid*10000+2016\ndf2017 = pd.read_csv('../input/train_2017.csv')\ndf2017['merger']=df2016.parcelid*10000+2017\ndf=pd.concat([df2016,df2017])\ndel df2016\ndel df2017\nprop2016 = pd.read_csv('../input/properties_2016.csv')\nprop2016['merger']=prop2016.parcelid*10000+2016\nprop2017 = pd.read_csv('../input/properties_2017.csv')\nprop2017['merger']=prop2016.parcelid*10000+2017\nprop=pd.concat([prop2016,prop2017])\ndel prop2016\ndel prop2017","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"60cf055a-4ade-4890-8876-9f27bc2f7901","scrolled":false,"_uuid":"8776a0a73b4afb2c7b4428caefb1aa35253fd5f2","trusted":false,"collapsed":true},"cell_type":"code","source":"sample = pd.read_csv('../input/sample_submission.csv')\nsample=sample.rename(index=str,columns={'ParcelId':'parcelid'})\ndates=sample.columns\ntest=sample[['parcelid','201611']]\ntest['year']=2016\ntest2=test[['parcelid','year']]\ntest2.year=2017\ndel test['201611']\ntest=pd.concat([test,test2])\ndel test2\ntest['merger']=test.parcelid*10000+test.year\ntest.year=test.year-2016","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"47ed1c80-f669-4658-8dc7-a17e06d38e8a","scrolled":true,"_uuid":"8843579ed359cb7c5cf32000bf35cf3a64916f01","trusted":false,"collapsed":true},"cell_type":"code","source":"print(df.info())\nprint(prop.info())\ndf = df.merge(prop, how='left', on='merger')\ntest=test.merge(prop, how='left', on='merger')\ndel prop\ndf['parcelid']=df.merger/10000\ntest['parcelid']=test.merger/10000\ndel df['merger']\ndel test['merger']\ndel df['parcelid_x']\ndel df['parcelid_y']\ndel test['parcelid_x']\ndel test['parcelid_y']","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"6439e232-657f-4649-a66d-0db334d67fbc","collapsed":true,"_uuid":"d88ea3a0b4939435c52ef2993ca4f316535d6fbb","trusted":false},"cell_type":"code","source":"for c, dtype in zip(df.columns, df.dtypes):\n    if dtype == np.float64:\n        df[c] = df[c].astype(np.float32) \n    elif dtype == np.int64:\n        df[c] = df[c].astype(np.int32)\nfor c, dtype in zip(test.columns, test.dtypes):\n    if dtype == np.float64:\n        test[c] = test[c].astype(np.float32) \n    elif dtype == np.int64:\n        test[c] = test[c].astype(np.int32) ","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"2ee5ec1e-6977-4675-876c-8e16f705b0bc","scrolled":true,"_uuid":"fe5ea0d860904a05258e60c559d4601d366f6cc3","trusted":false,"collapsed":true},"cell_type":"code","source":"df.describe().transpose()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f00f8225-18bd-4645-809a-c2be7b9385c9","scrolled":true,"_uuid":"bab59e127fd91b95af3b44a51da4a6d9c9c360bd","trusted":false,"collapsed":true},"cell_type":"code","source":"df.head().transpose()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f305482c-516a-47f9-8791-ff5093b09918","scrolled":true,"_uuid":"c1bb64b9f74797ba8f497c581f0ab8ba6e9939c3","trusted":false,"collapsed":true},"cell_type":"code","source":"#air conditioning type 3 and 5 have only one sample, so we may need to absorb them to type 1. Correlation is very low, but it simply means that \n#increasing type id doesn't mean greater logerror. They are still believed to be related. There is a significant difference of error whether\n#this data is NaN or not. Therefore we will keep NaN as separate category. Also, because typeid 5 and typeid 13 have similar error with\n#significant size, I would choose to merge 13 to 5. In this way, this feature will be proportional (exponential maybe) to logerror.\ndf.airconditioningtypeid=df.airconditioningtypeid.fillna(-1)\ndf.airconditioningtypeid[df.airconditioningtypeid==3.0]=1.0\ndf.airconditioningtypeid[df.airconditioningtypeid==9.0]=1.0\ndf.airconditioningtypeid[df.airconditioningtypeid==13.0]=5.0\n\ntest.airconditioningtypeid=test.airconditioningtypeid.fillna(-1)\ntest.airconditioningtypeid[test.airconditioningtypeid==3.0]=1.0\ntest.airconditioningtypeid[test.airconditioningtypeid==9.0]=1.0\ntest.airconditioningtypeid[test.airconditioningtypeid==13.0]=5.0\n\nprint(df.logerror[df.airconditioningtypeid==-1].mean(),df.logerror[df.airconditioningtypeid!=-1].mean())\ndf[['airconditioningtypeid','logerror']].groupby(['airconditioningtypeid'], as_index=False)['logerror'].agg(['mean','count'])\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c4a5863d-4e0e-4d47-8489-5026b68a0e57","scrolled":true,"_uuid":"e4ab2b36d0853edddc70744f55838bacc0776239","trusted":false,"collapsed":true},"cell_type":"code","source":"#architecture type 3, 10, and 21 has insignificant samples. I keep NaN as separate category because being NaN leads to smaller error.\n#There are less than 300 data points for this feature, and vast majority has value 7. The rest types have very small number of data points.\n#Therefore, rather than using typeid, I would rather use if the house has architecture type info or not.\ndf.architecturalstyletypeid=df.architecturalstyletypeid.fillna(-1)\ndf.architecturalstyletypeid[df.architecturalstyletypeid!=-1]=1\ntest.architecturalstyletypeid=test.architecturalstyletypeid.fillna(-1)\ntest.architecturalstyletypeid[test.architecturalstyletypeid!=-1]=1\nprint(df.logerror[df.architecturalstyletypeid==-1].mean(),df.logerror[df.architecturalstyletypeid!=-1].mean())\n\ndf[['architecturalstyletypeid','logerror']].groupby(['architecturalstyletypeid'], as_index=False)['logerror'].agg(['mean','count'])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"fb3a1503-edfc-4d65-9383-755d1c9d3c47","scrolled":true,"_uuid":"39efce317c8f7bfeb581b1062a6a2977e22a6d9d","trusted":false,"collapsed":true},"cell_type":"code","source":"#This data is meaningful even though data is only available for 43 houses. When this data exists, error gets reduced significantly. Also, this\n#data has high positive correlation with the error when it's there. As the error for null is about one sixth of the error for nonnull, I am\n#comfortable with giving -1 to null.\nprint(df.logerror[df.basementsqft.isnull()].mean(),df.logerror[df.basementsqft.notnull()].mean())\nprint(df.logerror[df.basementsqft.isnull()].count(),df.logerror[df.basementsqft.notnull()].count())\nprint(np.corrcoef(df.basementsqft[df.basementsqft.notnull()],df.logerror[df.basementsqft.notnull()]))\ndf.basementsqft=df.basementsqft.fillna(-1)\ntest.basementsqft=test.basementsqft.fillna(-1)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"81bf4717-83db-42bd-af1b-0ca6703ccf24","scrolled":true,"_uuid":"a4e32e69bed9930c5f4a9b5640782ca55513de86","trusted":false,"collapsed":true},"cell_type":"code","source":"#This is nonull feasure. As you can imagine the number of bathrooms is proportional to the size of house. Therefore, number of bathroom is\n#supposed to be proportional to the error. There are some bathroom counts which have very few samples. I would say the data is not significant\n#when there are less than 50 samples. Therefore, I chose to make 7.5 to 8 and 8.5 and 8.5+ to 9.\nprint(df.logerror[df.bathroomcnt.isnull()].count(),df.logerror[df.bathroomcnt.notnull()].count())\nprint(np.corrcoef(df.bathroomcnt[df.bathroomcnt.notnull()],df.logerror[df.bathroomcnt.notnull()]))\nprint(df[['bathroomcnt','logerror']].groupby(['bathroomcnt'], as_index=False)['logerror'].agg(['mean','count']))\ndf.bathroomcnt[df.bathroomcnt==7.5]=8\ndf.bathroomcnt[df.bathroomcnt>=8.5]=9\ntest.bathroomcnt[test.bathroomcnt==7.5]=8\ntest.bathroomcnt[test.bathroomcnt>=8.5]=9\ndf[['bathroomcnt','logerror']].groupby(['bathroomcnt'], as_index=False)['logerror'].agg(['mean','count'])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"27c02b9f-5b27-4c11-9c2d-ecf0e83ac006","scrolled":true,"_uuid":"3e97df9edb16ef61512377693247f10efa566442","trusted":false,"collapsed":true},"cell_type":"code","source":"#Bedroom works in a similar way with bathroom. Therefore, I approached bedroom in a similar way.\nprint(df.logerror[df.bedroomcnt.isnull()].count(),df.logerror[df.bedroomcnt.notnull()].count())\nprint(np.corrcoef(df.bedroomcnt[df.bedroomcnt.notnull()],df.logerror[df.bedroomcnt.notnull()]))\nprint(df[['bedroomcnt','logerror']].groupby(['bedroomcnt'], as_index=False)['logerror'].agg(['mean','count']))\ndf.bedroomcnt[df.bedroomcnt>9]=10\ntest.bedroomcnt[test.bedroomcnt>9]=10\ndf[['bedroomcnt','logerror']].groupby(['bedroomcnt'], as_index=False)['logerror'].agg(['mean','count'])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"9023d419-4a5b-4603-a512-9c13aac306b7","scrolled":true,"_uuid":"1312ab892bd6c4e45e14bd9728d90985951f4305","trusted":false,"collapsed":true},"cell_type":"code","source":"#There are significant amount of nonull data. Additionally, error for null data is almost as same as notnull data. Type 6,8,11 are insignificant\n#Therefore, this will be treated as nan.\nprint(df.logerror[df.buildingqualitytypeid.isnull()].mean(),df.logerror[df.buildingqualitytypeid.notnull()].mean())\nprint(df.logerror[df.buildingqualitytypeid.isnull()].count(),df.logerror[df.buildingqualitytypeid.notnull()].count())\nprint(np.corrcoef(df.buildingqualitytypeid[df.buildingqualitytypeid.notnull()],df.logerror[df.buildingqualitytypeid.notnull()]))\ndf.buildingqualitytypeid=df.buildingqualitytypeid.fillna(-1)\ndf.buildingqualitytypeid[df.buildingqualitytypeid==6.0]=-1\ndf.buildingqualitytypeid[df.buildingqualitytypeid==8.0]=-1\ndf.buildingqualitytypeid[df.buildingqualitytypeid==11.0]=-1\ntest.buildingqualitytypeid=test.buildingqualitytypeid.fillna(-1)\ntest.buildingqualitytypeid[test.buildingqualitytypeid==6.0]=-1\ntest.buildingqualitytypeid[test.buildingqualitytypeid==8.0]=-1\ntest.buildingqualitytypeid[test.buildingqualitytypeid==11.0]=-1\ndf[['buildingqualitytypeid','logerror']].groupby(['buildingqualitytypeid'], as_index=False)['logerror'].agg(['mean','count'])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"208e4439-8b26-4651-ac08-96981a34f01a","scrolled":true,"_uuid":"bfb8658d0b2b727b8f795d6bc1cad1910cebeb23","trusted":false,"collapsed":true},"cell_type":"code","source":"#This data is hard to be determined. Only 16 data points, and they are only one type. I may not use this data at all.\nprint(df.logerror[df.buildingclasstypeid.isnull()].mean(),df.logerror[df.buildingclasstypeid.notnull()].mean())\nprint(df.logerror[df.buildingclasstypeid.isnull()].count(),df.logerror[df.buildingclasstypeid.notnull()].count())\nprint(np.corrcoef(df.buildingclasstypeid[df.buildingclasstypeid.notnull()],df.logerror[df.buildingclasstypeid.notnull()]))\nprint(df[['buildingclasstypeid','logerror']].groupby(['buildingclasstypeid'], as_index=False)['logerror'].agg(['mean','count']))\ndel df['buildingclasstypeid']\ndel test['buildingclasstypeid']","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f3786192-0bb5-4559-a885-ddd29a44a23d","scrolled":true,"_uuid":"ab487f5caa868d5043f96d69688229c29806f9cb","trusted":false,"collapsed":true},"cell_type":"code","source":"#calculatebathnbr is duplicate data as bathroomcnt. This data won't be used.\nprint(df.logerror[df.calculatedbathnbr.isnull()].count(),df.logerror[df.calculatedbathnbr.notnull()].count())\nprint(np.corrcoef(df.calculatedbathnbr[df.calculatedbathnbr.notnull()],df.logerror[df.calculatedbathnbr.notnull()]))\nprint(df[['calculatedbathnbr','logerror']].groupby(['calculatedbathnbr'], as_index=False)['logerror'].agg(['mean','count']))\ndel df['calculatedbathnbr']\ndel test['calculatedbathnbr']","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"5595cb24-a5c2-4335-9ec3-586e24672d6e","scrolled":false,"_uuid":"d637417c0f641f435ef2a2a616a17f691138258a","trusted":false,"collapsed":true},"cell_type":"code","source":"#decktypeid is in a similar situation with buildingclasstypeid.\nprint(df.logerror[df.decktypeid.isnull()].count(),df.logerror[df.decktypeid.notnull()].count())\nprint(np.corrcoef(df.decktypeid[df.decktypeid.notnull()],df.logerror[df.decktypeid.notnull()]))\nprint(df[['decktypeid','logerror']].groupby(['decktypeid'], as_index=False)['logerror'].agg(['mean','count']))\ndel df['decktypeid']\ndel test['decktypeid']","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e51e85c1-c6b8-4e33-b55c-c70fceb28dbf","scrolled":false,"_uuid":"b8300760fb6fc0d57b0010c81bd4f883aa3b08a9","trusted":false,"collapsed":true},"cell_type":"code","source":"#This data is a little different from other bathroom data. Around 12k data points available, but majority are 1. Additionally, error from null\n#is slightly lower than notnull. I am going to convert 3 and 4 to 2.\nprint(df.logerror[df.threequarterbathnbr.isnull()].mean(),df.logerror[df.threequarterbathnbr.notnull()].mean())\nprint(df.logerror[df.threequarterbathnbr.isnull()].count(),df.logerror[df.threequarterbathnbr.notnull()].count())\nprint(np.corrcoef(df.threequarterbathnbr[df.threequarterbathnbr.notnull()],df.logerror[df.threequarterbathnbr.notnull()]))\nprint(df[['threequarterbathnbr','logerror']].groupby(['threequarterbathnbr'], as_index=False)['logerror'].agg(['mean','count']))\ndf.threequarterbathnbr=df.threequarterbathnbr.fillna(-1)\ndf.threequarterbathnbr[df.threequarterbathnbr==3.0]=2.0\ndf.threequarterbathnbr[df.threequarterbathnbr==4.0]=2.0\ntest.threequarterbathnbr=test.threequarterbathnbr.fillna(-1)\ntest.threequarterbathnbr[test.threequarterbathnbr==3.0]=2.0\ntest.threequarterbathnbr[test.threequarterbathnbr==4.0]=2.0\ndf[['threequarterbathnbr','logerror']].groupby(['threequarterbathnbr'], as_index=False)['logerror'].agg(['mean','count'])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"db3b7fb7-f56b-49ce-8232-46ed2911dddd","scrolled":true,"_uuid":"a1161a4e2ce342d683b8697a41c69edb09bfc33d","trusted":false,"collapsed":true},"cell_type":"code","source":"#1st floor size of course is proportional to error. As there are so many unique values for this measure, I chose to use intervals for this\n#feature. Additionally, I changed group 6,7,8,9 to 5.\nprint(df.logerror[df.finishedfloor1squarefeet.isnull()].mean(),df.logerror[df.finishedfloor1squarefeet.notnull()].mean())\nprint(df.logerror[df.finishedfloor1squarefeet.isnull()].count(),df.logerror[df.finishedfloor1squarefeet.notnull()].count())\nprint(np.corrcoef(df.finishedfloor1squarefeet[df.finishedfloor1squarefeet.notnull()],df.logerror[df.finishedfloor1squarefeet.notnull()]))\ndf['first_floor']=pd.cut(df.finishedfloor1squarefeet, 10).cat.codes\ntest['first_floor']=pd.cut(df.finishedfloor1squarefeet, 10).cat.codes\nprint(df[['first_floor','logerror']].groupby(['first_floor'], as_index=False)['logerror'].agg(['mean','count']))\ndf.first_floor[df.first_floor>5]=5\ntest.first_floor[test.first_floor>5]=5\ndf[['first_floor','logerror']].groupby(['first_floor'], as_index=False)['logerror'].agg(['mean','count'])\ndel df['finishedfloor1squarefeet']\ndel test['finishedfloor1squarefeet']","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"52bcba90-fd42-47a2-abe3-59a039cc67ad","scrolled":false,"_uuid":"fa90de2309fadb6ae151804ba4c42675e3b78b33","trusted":false,"collapsed":true},"cell_type":"code","source":"#this is the size of the whole house. It looks like we can apply the samething we did for the first floor size.\nprint(df.logerror[df.calculatedfinishedsquarefeet.isnull()].mean(),df.logerror[df.calculatedfinishedsquarefeet.notnull()].mean())\nprint(df.logerror[df.calculatedfinishedsquarefeet.isnull()].count(),df.logerror[df.calculatedfinishedsquarefeet.notnull()].count())\nprint(np.corrcoef(df.calculatedfinishedsquarefeet[df.calculatedfinishedsquarefeet.notnull()],df.logerror[df.calculatedfinishedsquarefeet.notnull()]))\ndf['finished_sqr']=pd.cut(df.calculatedfinishedsquarefeet, 10).cat.codes\ntest['finished_sqr']=pd.cut(df.calculatedfinishedsquarefeet, 10).cat.codes\nprint(df[['finished_sqr','logerror']].groupby(['finished_sqr'], as_index=False)['logerror'].agg(['mean','count']))\ndf.finished_sqr[df.finished_sqr>4]=4\ntest.finished_sqr[test.finished_sqr>4]=4\ndf[['finished_sqr','logerror']].groupby(['finished_sqr'], as_index=False)['logerror'].agg(['mean','count'])\ndel df['calculatedfinishedsquarefeet']\ndel test['calculatedfinishedsquarefeet']","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"747711c6-a2ca-4732-a06d-1bde3dfb12da","scrolled":true,"_uuid":"ecf9ee51923383c5c488df2d8684639b6748c319","trusted":false,"collapsed":true},"cell_type":"code","source":"#when the total area is known, error reduces a lot. Similar approach with other size features.\nprint(df.logerror[df.finishedsquarefeet6.isnull()].mean(),df.logerror[df.finishedsquarefeet6.notnull()].mean())\nprint(df.logerror[df.finishedsquarefeet6.isnull()].count(),df.logerror[df.finishedsquarefeet6.notnull()].count())\nprint(np.corrcoef(df.finishedsquarefeet6[df.finishedsquarefeet6.notnull()],df.logerror[df.finishedsquarefeet6.notnull()]))\ndf['finished_sqr6']=pd.cut(df.finishedsquarefeet6, 10).cat.codes\ntest['finished_sqr6']=pd.cut(df.finishedsquarefeet6, 10).cat.codes\nprint(df[['finished_sqr6','logerror']].groupby(['finished_sqr6'], as_index=False)['logerror'].agg(['mean','count']))\ndf.finished_sqr6[df.finished_sqr6>6]=6\ntest.finished_sqr6[test.finished_sqr6>6]=6\ndf[['finished_sqr6','logerror']].groupby(['finished_sqr6'], as_index=False)['logerror'].agg(['mean','count'])\ndel df['finishedsquarefeet6']\ndel test['finishedsquarefeet6']","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b49724b0-5a78-449c-8cc8-9bdb9b9ede39","scrolled":true,"_uuid":"82e29633c7b19619cebda61b8375a1340642b211","trusted":false,"collapsed":true},"cell_type":"code","source":"#when the total area is known, error reduces a lot. Similar approach with other size features.\nprint(df.logerror[df.finishedsquarefeet12.isnull()].mean(),df.logerror[df.finishedsquarefeet12.notnull()].mean())\nprint(df.logerror[df.finishedsquarefeet12.isnull()].count(),df.logerror[df.finishedsquarefeet12.notnull()].count())\nprint(np.corrcoef(df.finishedsquarefeet12[df.finishedsquarefeet12.notnull()],df.logerror[df.finishedsquarefeet12.notnull()]))\ndf['finished_sqr12']=pd.cut(df.finishedsquarefeet12, 10).cat.codes\ntest['finished_sqr12']=pd.cut(df.finishedsquarefeet12, 10).cat.codes\nprint(df[['finished_sqr12','logerror']].groupby(['finished_sqr12'], as_index=False)['logerror'].agg(['mean','count']))\ndf.finished_sqr12[df.finished_sqr12>5]=5\ntest.finished_sqr12[test.finished_sqr12>5]=5\ndf[['finished_sqr12','logerror']].groupby(['finished_sqr12'], as_index=False)['logerror'].agg(['mean','count'])\ndel df['finishedsquarefeet12']\ndel test['finishedsquarefeet12']","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"0543586e-873f-4397-a7be-4af3743db7ed","scrolled":true,"_uuid":"53d4566b33207fa01239c49615c433d03e557000","trusted":false,"collapsed":true},"cell_type":"code","source":"#Only 33 notnull values, and there are 17 data points for 1440ft2. Therefore, I categorized by less than 1440, equal to 1440, larger than 1440,\n#and null.\nprint(df.logerror[df.finishedsquarefeet13.isnull()].mean(),df.logerror[df.finishedsquarefeet13.notnull()].mean())\nprint(df.logerror[df.finishedsquarefeet13.isnull()].count(),df.logerror[df.finishedsquarefeet13.notnull()].count())\nprint(np.corrcoef(df.finishedsquarefeet13[df.finishedsquarefeet13.notnull()],df.logerror[df.finishedsquarefeet13.notnull()]))\ndf.finishedsquarefeet13[df.finishedsquarefeet13<1440.0]=0\ndf.finishedsquarefeet13[df.finishedsquarefeet13==1440.0]=1\ndf.finishedsquarefeet13[df.finishedsquarefeet13>1440.0]=2\ndf.finishedsquarefeet13=df.finishedsquarefeet13.fillna(-1)\ntest.finishedsquarefeet13[test.finishedsquarefeet13<1440.0]=0\ntest.finishedsquarefeet13[test.finishedsquarefeet13==1440.0]=1\ntest.finishedsquarefeet13[test.finishedsquarefeet13>1440.0]=2\ntest.finishedsquarefeet13=test.finishedsquarefeet13.fillna(-1)\ndf[['finishedsquarefeet13','logerror']].groupby(['finishedsquarefeet13'], as_index=False)['logerror'].agg(['mean','count'])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"180b8741-4abd-4593-8145-be126575efd4","scrolled":true,"_uuid":"2d78c8a981248131c15ac03f45840463e64f2c07","trusted":false,"collapsed":true},"cell_type":"code","source":"#when the total area is known, error reduces a lot. Similar approach with other size features.\nprint(df.logerror[df.finishedsquarefeet15.isnull()].mean(),df.logerror[df.finishedsquarefeet15.notnull()].mean())\nprint(df.logerror[df.finishedsquarefeet15.isnull()].count(),df.logerror[df.finishedsquarefeet15.notnull()].count())\nprint(np.corrcoef(df.finishedsquarefeet15[df.finishedsquarefeet15.notnull()],df.logerror[df.finishedsquarefeet15.notnull()]))\nprint(np.min(df.finishedsquarefeet15))\nprint(np.max(df.finishedsquarefeet15))\ndf.finishedsquarefeet15[df.finishedsquarefeet15==np.max(df.finishedsquarefeet15)]=10000.0\ndf['finished_sqr15']=pd.cut(df.finishedsquarefeet15, 10).cat.codes\ntest.finishedsquarefeet15[test.finishedsquarefeet15==np.max(test.finishedsquarefeet15)]=10000.0\ntest['finished_sqr15']=pd.cut(df.finishedsquarefeet15, 10).cat.codes\nprint(df[['finished_sqr15','logerror']].groupby(['finished_sqr15'], as_index=False)['logerror'].agg(['mean','count']))\ndf.finished_sqr15[df.finished_sqr15>5]=5\ntest.finished_sqr15[test.finished_sqr15>5]=5\ndf[['finished_sqr15','logerror']].groupby(['finished_sqr15'], as_index=False)['logerror'].agg(['mean','count'])\ndel df['finishedsquarefeet15']\ndel test['finishedsquarefeet15']","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"9d2ede27-60be-4c4c-8270-c0a6823339e2","scrolled":false,"_uuid":"8d25fd634b9db48661b08e16dbb9ec00ace7bb23","trusted":false,"collapsed":true},"cell_type":"code","source":"#when the total area is known, error reduces a lot. Similar approach with other size features.\nprint(df.logerror[df.finishedsquarefeet50.isnull()].mean(),df.logerror[df.finishedsquarefeet50.notnull()].mean())\nprint(df.logerror[df.finishedsquarefeet50.isnull()].count(),df.logerror[df.finishedsquarefeet50.notnull()].count())\nprint(np.corrcoef(df.finishedsquarefeet50[df.finishedsquarefeet50.notnull()],df.logerror[df.finishedsquarefeet50.notnull()]))\nprint(np.min(df.finishedsquarefeet50))\nprint(np.max(df.finishedsquarefeet50))\ndf['finished_sqr50']=pd.cut(df.finishedsquarefeet50, 10).cat.codes\ntest['finished_sqr50']=pd.cut(df.finishedsquarefeet50, 10).cat.codes\nprint(df[['finished_sqr50','logerror']].groupby(['finished_sqr50'], as_index=False)['logerror'].agg(['mean','count']))\ndf.finished_sqr50[df.finished_sqr50>5]=5\ntest.finished_sqr50[test.finished_sqr50>5]=5\ndf[['finished_sqr50','logerror']].groupby(['finished_sqr50'], as_index=False)['logerror'].agg(['mean','count'])\ndel df['finishedsquarefeet50']\ndel test['finishedsquarefeet50']","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"a72069e7-0ebc-4130-981d-ace065999f61","scrolled":false,"_uuid":"7a12f551663531548026879cffffa7b04aa054ae","trusted":false,"collapsed":true},"cell_type":"code","source":"#There's no null for fips. We do not know specific meaning of each code, so we just categorize it.\nprint(df.logerror[df.fips.isnull()].mean(),df.logerror[df.fips.notnull()].mean())\nprint(df.logerror[df.fips.isnull()].count(),df.logerror[df.fips.notnull()].count())\nprint(np.corrcoef(df.fips[df.fips.notnull()],df.logerror[df.fips.notnull()]))\ndf[['fips','logerror']].groupby(['fips'], as_index=False)['logerror'].agg(['mean','count'])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f9e916bb-2612-4c64-8b83-808380438e70","scrolled":false,"_uuid":"b286a4eda2aecdb2568789521e1efa382c55f1a7","trusted":false,"collapsed":true},"cell_type":"code","source":"#number of fireplace is proportional to error. This measure is another indirect measure on size.\nprint(df.logerror[df.fireplacecnt.isnull()].mean(),df.logerror[df.fireplacecnt.notnull()].mean())\nprint(df.logerror[df.fireplacecnt.isnull()].count(),df.logerror[df.fireplacecnt.notnull()].count())\nprint(np.corrcoef(df.fireplacecnt[df.fireplacecnt.notnull()],df.logerror[df.fireplacecnt.notnull()]))\nprint(df[['fireplacecnt','logerror']].groupby(['fireplacecnt'], as_index=False)['logerror'].agg(['mean','count']))\ndf.fireplacecnt[df.fireplacecnt>3]=3\ntest.fireplacecnt[test.fireplacecnt>3]=3\ndf[['fireplacecnt','logerror']].groupby(['fireplacecnt'], as_index=False)['logerror'].agg(['mean','count'])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ea2836f9-1f6f-417a-ac84-9cb186545f0b","_uuid":"035adc1b4ff37bebf78063ed32983664291661a0","trusted":false,"collapsed":true},"cell_type":"code","source":"#This data is not giving good enough information. It only gives true when fire place is there. It means house without fireplace and no info are\n#treated same. Another strange thing is that fireplacecnt gave info for more than 10k datapoints. I would rather choose not to use this data.\nprint(df.logerror[df.fireplaceflag.isnull()].mean(),df.logerror[df.fireplaceflag.notnull()].mean())\nprint(df.logerror[df.fireplaceflag.isnull()].count(),df.logerror[df.fireplaceflag.notnull()].count())\nprint(df[['fireplaceflag','logerror']].groupby(['fireplaceflag'], as_index=False)['logerror'].agg(['mean','count']))\ndel df['fireplaceflag']\ndel test['fireplaceflag']","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c19a8733-f4ee-48de-ad44-8279c259d2ea","scrolled":false,"_uuid":"29561d0bccadfb532c6f9008777b2481d0ffccee","trusted":false,"collapsed":true},"cell_type":"code","source":"#this is a good data with only few null data. Similar approach to other bathroom or room measure. Merged 8+ full baths into 8 baths.\nprint(df.logerror[df.fullbathcnt.isnull()].mean(),df.logerror[df.fullbathcnt.notnull()].mean())\nprint(df.logerror[df.fullbathcnt.isnull()].count(),df.logerror[df.fullbathcnt.notnull()].count())\nprint(np.corrcoef(df.fullbathcnt[df.fullbathcnt.notnull()],df.logerror[df.fullbathcnt.notnull()]))\nprint(df[['fullbathcnt','logerror']].groupby(['fullbathcnt'], as_index=False)['logerror'].agg(['mean','count']))\ndf.fullbathcnt[df.fullbathcnt>8]=8\ntest.fullbathcnt[test.fullbathcnt>8]=8\ndf[['fullbathcnt','logerror']].groupby(['fullbathcnt'], as_index=False)['logerror'].agg(['mean','count'])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f4ab5d67-3620-46be-9e21-f81dd47d69dc","_uuid":"c1d0e3ffbd42a536cd4d9779c3e20d266024d937","trusted":false,"collapsed":true},"cell_type":"code","source":"#about 1/3 datapoints are notnull. Merged 6+ garages to 6 garages.\nprint(df.logerror[df.garagecarcnt.isnull()].mean(),df.logerror[df.garagecarcnt.notnull()].mean())\nprint(df.logerror[df.garagecarcnt.isnull()].count(),df.logerror[df.garagecarcnt.notnull()].count())\nprint(np.corrcoef(df.garagecarcnt[df.garagecarcnt.notnull()],df.logerror[df.garagecarcnt.notnull()]))\nprint(df[['garagecarcnt','logerror']].groupby(['garagecarcnt'], as_index=False)['logerror'].agg(['mean','count']))\ndf.garagecarcnt[df.garagecarcnt>6]=6\ntest.garagecarcnt[test.garagecarcnt>6]=6\ndf[['garagecarcnt','logerror']].groupby(['garagecarcnt'], as_index=False)['logerror'].agg(['mean','count'])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"304babfb-8646-4a60-964b-d99291b8a31c","scrolled":false,"_uuid":"9dc1db427be3f68719305f65efb0a767568b0937","trusted":false,"collapsed":true},"cell_type":"code","source":"#when the total area is known, error reduces a lot. Similar approach with other size features.\nprint(df.logerror[df.garagetotalsqft.isnull()].mean(),df.logerror[df.garagetotalsqft.notnull()].mean())\nprint(df.logerror[df.garagetotalsqft.isnull()].count(),df.logerror[df.garagetotalsqft.notnull()].count())\nprint(np.corrcoef(df.garagetotalsqft[df.garagetotalsqft.notnull()],df.logerror[df.garagetotalsqft.notnull()]))\ndf.garagetotalsqft[df.garagetotalsqft==np.max(df.garagetotalsqft)]=4500\ndf['garage_size']=pd.cut(df.garagetotalsqft, 10).cat.codes\ntest.garagetotalsqft[test.garagetotalsqft==np.max(test.garagetotalsqft)]=4500\ntest['garage_size']=pd.cut(df.garagetotalsqft, 10).cat.codes\nprint(df[['garage_size','logerror']].groupby(['garage_size'], as_index=False)['logerror'].agg(['mean','count']))\ndf.garage_size[df.garage_size>5]=5\ntest.garage_size[test.garage_size>5]=5\nprint(df[['garage_size','logerror']].groupby(['garage_size'], as_index=False)['logerror'].agg(['mean','count']))\ndel df['garagetotalsqft']\ndel test['garagetotalsqft']","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"fca33055-6d44-4418-b00c-5e404cdd75cf","_uuid":"74d6b50d7c7f7aad195a8281cb8a9f41f962f0d3","trusted":false,"collapsed":true},"cell_type":"code","source":"#Even though there are only few houses with hottub or spa, error is meaningfully low when it is true.\nprint(df.logerror[df.hashottuborspa.isnull()].mean(),df.logerror[df.hashottuborspa.notnull()].mean())\nprint(df.logerror[df.hashottuborspa.isnull()].count(),df.logerror[df.hashottuborspa.notnull()].count())\nprint(df[['hashottuborspa','logerror']].groupby(['hashottuborspa'], as_index=False)['logerror'].agg(['mean','count']))\ndf.hashottuborspa=df.hashottuborspa*1\ntest.hashottuborspa=test.hashottuborspa*1\ndf.hashottuborspa=df.hashottuborspa.fillna(-1)\ntest.hashottuborspa=test.hashottuborspa.fillna(-1)\ndf[['hashottuborspa','logerror']].groupby(['hashottuborspa'], as_index=False)['logerror'].agg(['mean','count'])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"97901574-f91a-4927-9e22-df1b4677b176","scrolled":true,"_uuid":"2cbfe0c451a0c44755c244ee77c52e5cc717bb7e","trusted":false,"collapsed":true},"cell_type":"code","source":"#heating system has decent amount of data. I chose to merge minority types into one\nprint(df.logerror[df.heatingorsystemtypeid.isnull()].mean(),df.logerror[df.heatingorsystemtypeid.notnull()].mean())\nprint(df.logerror[df.heatingorsystemtypeid.isnull()].count(),df.logerror[df.heatingorsystemtypeid.notnull()].count())\nprint(df[['heatingorsystemtypeid','logerror']].groupby(['heatingorsystemtypeid'], as_index=False)['logerror'].agg(['mean','count']))\ndf.heatingorsystemtypeid[df.heatingorsystemtypeid==11]=10\ndf.heatingorsystemtypeid[df.heatingorsystemtypeid==12]=10\ndf.heatingorsystemtypeid[df.heatingorsystemtypeid==14]=10\ntest.heatingorsystemtypeid[test.heatingorsystemtypeid==11]=10\ntest.heatingorsystemtypeid[test.heatingorsystemtypeid==12]=10\ntest.heatingorsystemtypeid[test.heatingorsystemtypeid==14]=10\ndf[['heatingorsystemtypeid','logerror']].groupby(['heatingorsystemtypeid'], as_index=False)['logerror'].agg(['mean','count'])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"74fdf99e-b63b-493d-977c-552228e9aae6","scrolled":true,"_uuid":"cd662d88e251438189464f2149b1e0fd4038ac3d","trusted":false,"collapsed":true},"cell_type":"code","source":"#latitude may not be proportional to the result, but it may mean something significant.\nprint(df.logerror[df.latitude.isnull()].mean(),df.logerror[df.latitude.notnull()].mean())\nprint(df.logerror[df.latitude.isnull()].count(),df.logerror[df.latitude.notnull()].count())\nprint(np.corrcoef(df.latitude[df.latitude.notnull()],df.logerror[df.latitude.notnull()]))\n#df.latitude[df.latitude==np.max(df.latitude)]=4500\ndf.latitude=pd.cut(df.latitude, 10).cat.codes\ntest.latitude=pd.cut(df.latitude, 10).cat.codes\nprint(df[['latitude','logerror']].groupby(['latitude'], as_index=False)['logerror'].agg(['mean','count']))\n#df.latitude[df.latitude>5]=5\n#print(df[['latitude','logerror']].groupby(['latitude'], as_index=False)['logerror'].agg(['mean','count']))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"6ec30fcb-f05c-4828-bace-dbc28ce6c9e1","_uuid":"fc72086de90045abecb08461a093ae65c010f9c3","trusted":false,"collapsed":true},"cell_type":"code","source":"#longitude is also straightforward\nprint(df.logerror[df.longitude.isnull()].mean(),df.logerror[df.longitude.notnull()].mean())\nprint(df.logerror[df.longitude.isnull()].count(),df.logerror[df.longitude.notnull()].count())\nprint(np.corrcoef(df.logerror[df.longitude.notnull()],df.longitude[df.longitude.notnull()]))\n#df.latitude[df.latitude==np.max(df.latitude)]=4500\ndf.longitude=pd.cut(df.longitude, 10).cat.codes\ntest.longitude=pd.cut(df.longitude, 10).cat.codes\nprint(df[['longitude','logerror']].groupby(['longitude'], as_index=False)['logerror'].agg(['mean','count']))\n#df.latitude[df.latitude>5]=5\n#print(df[['latitude','logerror']].groupby(['latitude'], as_index=False)['logerror'].agg(['mean','count']))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"2a86960d-e86f-4f87-8b97-33d96100fd30","scrolled":true,"_uuid":"119553cc6934d6003358205c3c9cf517a6b8329d","trusted":false,"collapsed":true},"cell_type":"code","source":"#It looks like lot size doesn't have good relationship with the error. I may choose not to use this feature.\nprint(df.logerror[df.lotsizesquarefeet.isnull()].mean(),df.logerror[df.lotsizesquarefeet.notnull()].mean())\nprint(df.logerror[df.lotsizesquarefeet.isnull()].count(),df.logerror[df.lotsizesquarefeet.notnull()].count())\nprint(np.corrcoef(df.logerror[df.lotsizesquarefeet.notnull()],df.longitude[df.lotsizesquarefeet.notnull()]))\nprint(df[['lotsizesquarefeet','logerror']].groupby(['lotsizesquarefeet'], as_index=False)['logerror'].agg(['mean','count']))\ndel df['lotsizesquarefeet']\ndel test['lotsizesquarefeet']","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"330deb60-20b2-4440-9550-077be6b3682e","scrolled":true,"_uuid":"61e40f999cfbeb5dbf910371d37a505c42a423d2","trusted":false,"collapsed":true},"cell_type":"code","source":"#of course number of stories is a significant factor. What's strange is that correlation is negative while it looks like positive based on pivot.\nprint(df.logerror[df.numberofstories.isnull()].mean(),df.logerror[df.numberofstories.notnull()].mean())\nprint(df.logerror[df.numberofstories.isnull()].count(),df.logerror[df.numberofstories.notnull()].count())\nprint(np.corrcoef(df.logerror[df.numberofstories.notnull()],df.longitude[df.numberofstories.notnull()]))\nprint(df[['numberofstories','logerror']].groupby(['numberofstories'], as_index=False)['logerror'].agg(['mean','count']))\ndf.numberofstories=df.numberofstories.fillna(-1)\ntest.numberofstories=test.numberofstories.fillna(-1)\ndf.numberofstories[df.numberofstories>3]=3\ntest.numberofstories[test.numberofstories>3]=3\ndf[['numberofstories','logerror']].groupby(['numberofstories'], as_index=False)['logerror'].agg(['mean','count'])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f878677a-06d4-4588-8f9e-d8b35e7e12d1","scrolled":true,"_uuid":"67d60b2291f787e9b64435ee54ca32e5f24c0f65","trusted":false,"collapsed":true},"cell_type":"code","source":"#No house has multiple pools. This measure is more about true or false.\nprint(df.logerror[df.poolcnt.isnull()].mean(),df.logerror[df.poolcnt.notnull()].mean())\nprint(df.logerror[df.poolcnt.isnull()].count(),df.logerror[df.poolcnt.notnull()].count())\nprint(np.corrcoef(df.poolcnt[df.poolcnt.notnull()],df.logerror[df.poolcnt.notnull()]))\nprint(df[['poolcnt','logerror']].groupby(['poolcnt'], as_index=False)['logerror'].agg(['mean','count']))\ndf.poolcnt=pd.Categorical(df.poolcnt)\ndf.poolcnt=df.poolcnt.cat.codes\ntest.poolcnt=pd.Categorical(test.poolcnt)\ntest.poolcnt=test.poolcnt.cat.codes\ndf[['poolcnt','logerror']].groupby(['poolcnt'], as_index=False)['logerror'].agg(['mean','count'])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3c817434-15ac-4a0a-b8eb-ca6f2c7296dc","scrolled":false,"_uuid":"cc7de894c19f67790c8d1c1810245f62b10a2435","trusted":false,"collapsed":true},"cell_type":"code","source":"#different pool size gives different errors. \nprint(df.logerror[df.poolsizesum.isnull()].mean(),df.logerror[df.poolsizesum.notnull()].mean())\nprint(df.logerror[df.poolsizesum.isnull()].count(),df.logerror[df.poolsizesum.notnull()].count())\nprint(np.corrcoef(df.logerror[df.poolsizesum.notnull()],df.poolsizesum[df.poolsizesum.notnull()]))\ndf.poolsizesum=pd.cut(df.poolsizesum, 10).cat.codes\ntest.poolsizesum=pd.cut(df.poolsizesum, 10).cat.codes\nprint(df[['poolsizesum','logerror']].groupby(['poolsizesum'], as_index=False)['logerror'].agg(['mean','count']))\ndf.poolsizesum[df.poolsizesum>4]=4\ndf.poolsizesum[df.poolsizesum==0]=1\ntest.poolsizesum[test.poolsizesum>4]=4\ntest.poolsizesum[test.poolsizesum==0]=1\nprint(df[['poolsizesum','logerror']].groupby(['poolsizesum'], as_index=False)['logerror'].agg(['mean','count']))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"9bf78a8e-42ea-4b45-bb58-310a8076627c","_uuid":"576886bfb3814735704dbc2c972577f46d38ab4d","trusted":false,"collapsed":true},"cell_type":"code","source":"#I do not know what's really different between this feature and hottuborspa feature, but result is different, so I can't really remove this.\nprint(df.logerror[df.pooltypeid10.isnull()].mean(),df.logerror[df.pooltypeid10.notnull()].mean())\nprint(df.logerror[df.pooltypeid10.isnull()].count(),df.logerror[df.pooltypeid10.notnull()].count())\nprint(np.corrcoef(df.pooltypeid10[df.pooltypeid10.notnull()],df.logerror[df.pooltypeid10.notnull()]))\nprint(df[['pooltypeid10','logerror']].groupby(['pooltypeid10'], as_index=False)['logerror'].agg(['mean','count']))\ndf.pooltypeid10=df.pooltypeid10.fillna(-1)\ntest.pooltypeid10=test.pooltypeid10.fillna(-1)\ndf[['pooltypeid10','logerror']].groupby(['pooltypeid10'], as_index=False)['logerror'].agg(['mean','count'])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"bba75afc-49d3-4f18-905a-038a3dfcef32","_uuid":"d07b13e44bfac8c6d04c62231d0110f4bfe99394","trusted":false,"collapsed":true},"cell_type":"code","source":"#this measure is true or null\nprint(df.logerror[df.pooltypeid2.isnull()].mean(),df.logerror[df.pooltypeid2.notnull()].mean())\nprint(df.logerror[df.pooltypeid2.isnull()].count(),df.logerror[df.pooltypeid2.notnull()].count())\nprint(np.corrcoef(df.pooltypeid2[df.pooltypeid2.notnull()],df.logerror[df.pooltypeid2.notnull()]))\nprint(df[['pooltypeid2','logerror']].groupby(['pooltypeid2'], as_index=False)['logerror'].agg(['mean','count']))\ndf.pooltypeid2=df.pooltypeid2.fillna(-1)\ntest.pooltypeid2=test.pooltypeid2.fillna(-1)\ndf[['pooltypeid2','logerror']].groupby(['pooltypeid2'], as_index=False)['logerror'].agg(['mean','count'])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c74fe758-6699-442d-a6bb-6a0c8ae70a9a","scrolled":true,"_uuid":"0b582dd8d5e11fa822bcd4ea44e7812f1d0f7c81","trusted":false,"collapsed":true},"cell_type":"code","source":"#it's true or null\nprint(df.logerror[df.pooltypeid7.isnull()].mean(),df.logerror[df.pooltypeid7.notnull()].mean())\nprint(df.logerror[df.pooltypeid7.isnull()].count(),df.logerror[df.pooltypeid7.notnull()].count())\nprint(np.corrcoef(df.pooltypeid7[df.pooltypeid7.notnull()],df.logerror[df.pooltypeid7.notnull()]))\nprint(df[['pooltypeid7','logerror']].groupby(['pooltypeid7'], as_index=False)['logerror'].agg(['mean','count']))\ndf.pooltypeid7=df.pooltypeid7.fillna(-1)\ntest.pooltypeid7=test.pooltypeid7.fillna(-1)\ndf[['pooltypeid7','logerror']].groupby(['pooltypeid7'], as_index=False)['logerror'].agg(['mean','count'])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f99cadbf-a0f3-4642-9799-ab59c0afae4a","scrolled":true,"_uuid":"0fa6c15de529d97829cbd138e51b6cb0015b8301","trusted":false,"collapsed":true},"cell_type":"code","source":"#This code has so many different types. I only considered the ones with more than 20 houses of the same code. Otherwise, I treated\n#it as equivalent to null.\nprint(df.logerror[df.propertycountylandusecode.isnull()].mean(),df.logerror[df.propertycountylandusecode.notnull()].mean())\nprint(df.logerror[df.propertycountylandusecode.isnull()].count(),df.logerror[df.propertycountylandusecode.notnull()].count())\n#print(np.corrcoef(df.propertycountylandusecode[df.propertycountylandusecode.notnull()],df.logerror[df.propertycountylandusecode.notnull()]))\nprint(df[['propertycountylandusecode','logerror']].groupby(['propertycountylandusecode'], as_index=False)['logerror'].agg(['mean','count']))\npivot=df[['propertycountylandusecode','logerror']].groupby(['propertycountylandusecode'], as_index=True)['logerror'].agg(['mean','count'])\npivot['codes']=np.arange(len(pivot))\ncode_dict=pivot.codes.to_dict()\nsigs=pivot.index[pivot['count']>20].tolist()\ndf.propertycountylandusecode=df.propertycountylandusecode.apply(lambda x: x if x in sigs else -1)\ntest.propertycountylandusecode=test.propertycountylandusecode.apply(lambda x: x if x in sigs else -1)\ndf['propertycountylandusecode']=df.propertycountylandusecode.map(code_dict)\ntest['propertycountylandusecode']=test.propertycountylandusecode.map(code_dict)\ndf[['propertycountylandusecode','logerror']].groupby(['propertycountylandusecode'], as_index=True)['logerror'].agg(['mean','count'])\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"be12881c-dde2-44a8-a877-536782c679c2","scrolled":true,"_uuid":"060610e97088cab54a5e78fcaeaf258210b8fc55","trusted":false,"collapsed":true},"cell_type":"code","source":"#This feature has so many different types. I only considered the ones with more than 50 houses of the same code. Otherwise, I \n#treated it as equivalent to null.\nprint(df.logerror[df.propertylandusetypeid.isnull()].mean(),df.logerror[df.propertylandusetypeid.notnull()].mean())\nprint(df.logerror[df.propertylandusetypeid.isnull()].count(),df.logerror[df.propertylandusetypeid.notnull()].count())\n#print(np.corrcoef(df.propertycountylandusecode[df.propertycountylandusecode.notnull()],df.logerror[df.propertycountylandusecode.notnull()]))\nprint(df[['propertylandusetypeid','logerror']].groupby(['propertylandusetypeid'], as_index=False)['logerror'].agg(['mean','count']))\npivot=df[['propertylandusetypeid','logerror']].groupby(['propertylandusetypeid'], as_index=True)['logerror'].agg(['mean','count'])\npivot['codes']=np.arange(len(pivot))\ncode_dict=pivot.codes.to_dict()\nsigs=pivot.index[pivot['count']>50].tolist()\ndf.propertylandusetypeid=df.propertylandusetypeid.apply(lambda x: x if x in sigs else -1)\ntest.propertylandusetypeid=test.propertylandusetypeid.apply(lambda x: x if x in sigs else -1)\ndf['propertylandusetypeid']=df.propertylandusetypeid.map(code_dict)\ntest['propertylandusetypeid']=test.propertylandusetypeid.map(code_dict)\ndf[['propertylandusetypeid','logerror']].groupby(['propertylandusetypeid'], as_index=True)['logerror'].agg(['mean','count'])\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"84e59eb0-edfe-4996-982a-538df83384ff","scrolled":true,"_uuid":"6f4e308e091a37af5e7559b88e23660b731c1da9","trusted":false,"collapsed":true},"cell_type":"code","source":"#propertyzoningdesc has some meaningful relationship with error. There are a few of them with significant difference from total\n#average with decent sample size.\nprint(df.logerror[df.propertyzoningdesc.isnull()].mean(),df.logerror[df.propertyzoningdesc.notnull()].mean())\nprint(df.logerror[df.propertyzoningdesc.isnull()].count(),df.logerror[df.propertyzoningdesc.notnull()].count())\nprint(df[['propertyzoningdesc','logerror']].groupby(['propertyzoningdesc'], as_index=False)['logerror'].agg(['mean','count']))\npivot=df[['propertyzoningdesc','logerror']].groupby(['propertyzoningdesc'], as_index=True)['logerror'].agg(['mean','count'])\npivot['codes']=np.arange(len(pivot))\ncode_dict=pivot.codes.to_dict()\nsigs=pivot.index[pivot['count']>10].tolist()\ndf.propertyzoningdesc=df.propertyzoningdesc.apply(lambda x: x if x in sigs else -1)\ntest.propertyzoningdesc=test.propertyzoningdesc.apply(lambda x: x if x in sigs else -1)\ndf.propertyzoningdesc=df.propertyzoningdesc.map(code_dict)\ntest.propertyzoningdesc=test.propertyzoningdesc.map(code_dict)\ndf[['propertyzoningdesc','logerror']].groupby(['propertyzoningdesc'], as_index=True)['logerror'].agg(['mean','count'])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c0d1ed3c-c7c5-4959-88cc-f51811d502e4","scrolled":true,"_uuid":"ab569f6fd335561ddc7996fc9a9787a2eef35676","trusted":false,"collapsed":true},"cell_type":"code","source":"#censustractandblock doesn't look like it's correlated with the error.\nprint(df.logerror[df.censustractandblock.isnull()].mean(),df.logerror[df.censustractandblock.notnull()].mean())\nprint(df.logerror[df.censustractandblock.isnull()].count(),df.logerror[df.censustractandblock.notnull()].count())\nprint(np.corrcoef(df.censustractandblock[df.censustractandblock.notnull()],df.logerror[df.censustractandblock.notnull()]))\nprint(df[['censustractandblock','logerror']].groupby(['censustractandblock'], as_index=False)['logerror'].agg(['mean','count']))\nprint(df[['rawcensustractandblock','logerror']].groupby(['rawcensustractandblock'], as_index=False)['logerror'].agg(['mean','count']))\ndel df['censustractandblock']\ndel df['rawcensustractandblock']\ndel test['censustractandblock']\ndel test['rawcensustractandblock']","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e286faff-57c4-46e9-8477-e3558ae4e1f1","scrolled":false,"_uuid":"49ba4b444ba8a1f2d255918bedf46760e34bb294","trusted":false,"collapsed":true},"cell_type":"code","source":"#county\nprint(df.logerror[df.regionidcounty.isnull()].mean(),df.logerror[df.regionidcounty.notnull()].mean())\nprint(df.logerror[df.regionidcounty.isnull()].count(),df.logerror[df.regionidcounty.notnull()].count())\n#print(np.corrcoef(df.propertycountylandusecode[df.propertycountylandusecode.notnull()],df.logerror[df.propertycountylandusecode.notnull()]))\nprint(df[['regionidcounty','logerror']].groupby(['regionidcounty'], as_index=False)['logerror'].agg(['mean','count']))\ndf[['regionidcounty','logerror']].groupby(['regionidcounty'], as_index=True)['logerror'].agg(['mean','count'])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"9162ae97-d979-437b-a9ce-fe4f8bde7f36","scrolled":true,"_uuid":"c03de83df0495c0d64b813d27584405ed21a089f","trusted":false,"collapsed":true},"cell_type":"code","source":"#city. using only city with more than 50 transactions\nprint(df.logerror[df.regionidcity.isnull()].mean(),df.logerror[df.regionidcity.notnull()].mean())\nprint(df.logerror[df.regionidcity.isnull()].count(),df.logerror[df.regionidcity.notnull()].count())\n#print(np.corrcoef(df.propertycountylandusecode[df.propertycountylandusecode.notnull()],df.logerror[df.propertycountylandusecode.notnull()]))\nprint(df[['regionidcity','logerror']].groupby(['regionidcity'], as_index=False)['logerror'].agg(['mean','count']))\npivot=df[['regionidcity','logerror']].groupby(['regionidcity'], as_index=True)['logerror'].agg(['mean','count'])\npivot['codes']=np.arange(len(pivot))\ncode_dict=pivot.codes.to_dict()\nsigs=pivot.index[pivot['count']>50].tolist()\ndf.regionidcity=df.regionidcity.apply(lambda x: x if x in sigs else -1)\ntest.regionidcity=test.regionidcity.apply(lambda x: x if x in sigs else -1)\ndf['regionidcity']=df.regionidcity.map(code_dict)\ntest['regionidcity']=test.regionidcity.map(code_dict)\ndf[['regionidcity','logerror']].groupby(['regionidcity'], as_index=True)['logerror'].agg(['mean','count'])\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"7a6b7b1a-f4ce-4021-96dd-c7ca5e6a2b4d","scrolled":true,"_uuid":"f01b270b355083809303d23973178402046bcf06","trusted":false,"collapsed":true},"cell_type":"code","source":"#zipcode\nprint(df.logerror[df.regionidzip.isnull()].mean(),df.logerror[df.regionidzip.notnull()].mean())\nprint(df.logerror[df.regionidzip.isnull()].count(),df.logerror[df.regionidzip.notnull()].count())\nprint(np.corrcoef(df.regionidzip[df.regionidzip.notnull()],df.logerror[df.regionidzip.notnull()]))\nprint(df[['regionidzip','logerror']].groupby(['regionidzip'], as_index=False)['logerror'].agg(['mean','count']))\npivot=df[['regionidzip','logerror']].groupby(['regionidzip'], as_index=True)['logerror'].agg(['mean','count'])\npivot['codes']=np.arange(len(pivot))\ncode_dict=pivot.codes.to_dict()\nsigs=pivot.index[pivot['count']>50].tolist()\ndf.regionidzip=df.regionidzip.apply(lambda x: x if x in sigs else -1)\ntest.regionidzip=test.regionidzip.apply(lambda x: x if x in sigs else -1)\ndf['regionidzip']=df.regionidzip.map(code_dict)\ntest['regionidzip']=test.regionidzip.map(code_dict)\ndf[['regionidzip','logerror']].groupby(['regionidzip'], as_index=True)['logerror'].agg(['mean','count'])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"6da6fb3c-0e95-41e1-9e35-1840b19593b6","scrolled":true,"_uuid":"fb0f9652635f1064d3455384cada8ceabe139cd2","trusted":false,"collapsed":true},"cell_type":"code","source":"#neighborhood\nprint(df.logerror[df.regionidneighborhood.isnull()].mean(),df.logerror[df.regionidneighborhood.notnull()].mean())\nprint(df.logerror[df.regionidneighborhood.isnull()].count(),df.logerror[df.regionidneighborhood.notnull()].count())\n#print(np.corrcoef(df.regionidneighborhood[df.regionidneighborhood.notnull()],df.logerror[df.regionidneighborhood.notnull()]))\nprint(df[['regionidneighborhood','logerror']].groupby(['regionidneighborhood'], as_index=False)['logerror'].agg(['mean','count']))\npivot=df[['regionidneighborhood','logerror']].groupby(['regionidneighborhood'], as_index=True)['logerror'].agg(['mean','count'])\npivot['codes']=np.arange(len(pivot))\ncode_dict=pivot.codes.to_dict()\nsigs=pivot.index[pivot['count']>50].tolist()\ndf.regionidneighborhood=df.regionidneighborhood.apply(lambda x: x if x in sigs else -1)\ntest.regionidneighborhood=test.regionidneighborhood.apply(lambda x: x if x in sigs else -1)\ndf['regionidneighborhood']=df.regionidneighborhood.map(code_dict)\ntest['regionidneighborhood']=test.regionidneighborhood.map(code_dict)\ndf[['regionidneighborhood','logerror']].groupby(['regionidneighborhood'], as_index=True)['logerror'].agg(['mean','count'])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"15074be5-6a91-4f26-b98f-eb9555689d1e","scrolled":true,"_uuid":"2053c266a394629e6cba3a6fce3ef3d13b3c41bd","trusted":false,"collapsed":true},"cell_type":"code","source":"#Primary residence room count\nprint(df.logerror[df.roomcnt.isnull()].mean(),df.logerror[df.roomcnt.notnull()].mean())\nprint(df.logerror[df.roomcnt.isnull()].count(),df.logerror[df.roomcnt.notnull()].count())\nprint(np.corrcoef(df.logerror[df.roomcnt.notnull()],df.roomcnt[df.roomcnt.notnull()]))\nprint(df[['roomcnt','logerror']].groupby(['roomcnt'], as_index=False)['logerror'].agg(['mean','count']))\ndf.roomcnt[df.roomcnt<3]=0\ndf.roomcnt[df.roomcnt>11]=12\ntest.roomcnt[test.roomcnt<3]=0\ntest.roomcnt[test.roomcnt>11]=12\ndf[['roomcnt','logerror']].groupby(['roomcnt'], as_index=False)['logerror'].agg(['mean','count'])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"04d829ab-f072-42ff-b1e2-855ee067af39","scrolled":true,"_uuid":"f4a8e65ebe969d26f2d5e292717442c1468cdf3e","trusted":false,"collapsed":true},"cell_type":"code","source":"#story type. Only one kind, and only 43 houses.\nprint(df.logerror[df.storytypeid.isnull()].mean(),df.logerror[df.storytypeid.notnull()].mean())\nprint(df.logerror[df.storytypeid.isnull()].count(),df.logerror[df.storytypeid.notnull()].count())\n#print(np.corrcoef(df.propertycountylandusecode[df.propertycountylandusecode.notnull()],df.logerror[df.propertycountylandusecode.notnull()]))\nprint(df[['storytypeid','logerror']].groupby(['storytypeid'], as_index=False)['logerror'].agg(['mean','count']))\ndf[['storytypeid','logerror']].groupby(['storytypeid'], as_index=True)['logerror'].agg(['mean','count'])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"fe0ae423-acc6-459b-ac74-f552fb068397","scrolled":true,"_uuid":"df486aba3e5bf35d59a9d9209c26b86a545ae224","trusted":false,"collapsed":true},"cell_type":"code","source":"#construction type. Chose to only care if it exists because there were three types but only one of them has majority.\nprint(df.logerror[df.typeconstructiontypeid.isnull()].mean(),df.logerror[df.typeconstructiontypeid.notnull()].mean())\nprint(df.logerror[df.typeconstructiontypeid.isnull()].count(),df.logerror[df.typeconstructiontypeid.notnull()].count())\n#print(np.corrcoef(df.propertycountylandusecode[df.propertycountylandusecode.notnull()],df.logerror[df.propertycountylandusecode.notnull()]))\nprint(df[['typeconstructiontypeid','logerror']].groupby(['typeconstructiontypeid'], as_index=False)['logerror'].agg(['mean','count']))\n#df.typeconstructiontypeid[df.typeconstructiontypeid==0]=1\n#df.typeconstructiontypeid[df.typeconstructiontypeid==2]=1\ndf[['typeconstructiontypeid','logerror']].groupby(['typeconstructiontypeid'], as_index=True)['logerror'].agg(['mean','count'])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b34ccf30-5dd4-4ac6-8e34-e7b3c7f3f719","scrolled":true,"_uuid":"e3c38b7e45173d6f4e3139e918ad73762085f9e7","trusted":false,"collapsed":true},"cell_type":"code","source":"#unit count\nprint(df.logerror[df.unitcnt.isnull()].mean(),df.logerror[df.unitcnt.notnull()].mean())\nprint(df.logerror[df.unitcnt.isnull()].count(),df.logerror[df.unitcnt.notnull()].count())\nprint(np.corrcoef(df.logerror[df.unitcnt.notnull()],df.unitcnt[df.unitcnt.notnull()]))\nprint(df[['unitcnt','logerror']].groupby(['unitcnt'], as_index=False)['logerror'].agg(['mean','count']))\n#df.unitcnt[df.unitcnt<3]=0\ndf.unitcnt[df.unitcnt>4]=5\ntest.unitcnt[test.unitcnt>4]=5\ndf[['unitcnt','logerror']].groupby(['unitcnt'], as_index=False)['logerror'].agg(['mean','count'])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c0a47dc2-8155-470b-914e-df29b6626647","_uuid":"37cc6c77c17ae9b77c70fc59455e0a76597005eb","trusted":false,"collapsed":true},"cell_type":"code","source":"#Patio in yard\nprint(df.logerror[df.yardbuildingsqft17.isnull()].mean(),df.logerror[df.yardbuildingsqft17.notnull()].mean())\nprint(df.logerror[df.yardbuildingsqft17.isnull()].count(),df.logerror[df.yardbuildingsqft17.notnull()].count())\nprint(np.corrcoef(df.logerror[df.yardbuildingsqft17.notnull()],df.yardbuildingsqft17[df.yardbuildingsqft17.notnull()]))\ndf.yardbuildingsqft17=pd.cut(df.yardbuildingsqft17, 10).cat.codes\ntest.yardbuildingsqft17=pd.cut(df.yardbuildingsqft17, 10).cat.codes\nprint(df[['yardbuildingsqft17','logerror']].groupby(['yardbuildingsqft17'], as_index=False)['logerror'].agg(['mean','count']))\ndf.yardbuildingsqft17[df.yardbuildingsqft17>3]=3\ntest.yardbuildingsqft17[test.yardbuildingsqft17>3]=3\n#df.yardbuildingsqft17[df.yardbuildingsqft17==0]=1\ndf[['yardbuildingsqft17','logerror']].groupby(['yardbuildingsqft17'], as_index=False)['logerror'].agg(['mean','count'])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"659f2941-81a7-4cdd-b1c8-7d4095eab2be","scrolled":true,"_uuid":"011de82bae87d4de3ca79c9097f2843fb6398bc2","trusted":false,"collapsed":true},"cell_type":"code","source":"#Storage size\nprint(df.logerror[df.yardbuildingsqft26.isnull()].mean(),df.logerror[df.yardbuildingsqft26.notnull()].mean())\nprint(df.logerror[df.yardbuildingsqft26.isnull()].count(),df.logerror[df.yardbuildingsqft26.notnull()].count())\nprint(np.corrcoef(df.logerror[df.yardbuildingsqft26.notnull()],df.yardbuildingsqft26[df.yardbuildingsqft26.notnull()]))\ndf.yardbuildingsqft26=pd.cut(df.yardbuildingsqft26, 10).cat.codes\ntest.yardbuildingsqft26=pd.cut(df.yardbuildingsqft26, 10).cat.codes\nprint(df[['yardbuildingsqft26','logerror']].groupby(['yardbuildingsqft26'], as_index=False)['logerror'].agg(['mean','count']))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"4707bb5a-feee-4bc8-916a-08a444ee40b6","_uuid":"c33361998393c5e1680c245525581deb9e8962fe","trusted":false,"collapsed":true},"cell_type":"code","source":"#built year\nprint(df.logerror[df.yearbuilt.isnull()].mean(),df.logerror[df.yearbuilt.notnull()].mean())\nprint(df.logerror[df.yearbuilt.isnull()].count(),df.logerror[df.yearbuilt.notnull()].count())\nprint(np.corrcoef(df.logerror[df.yearbuilt.notnull()],df.yearbuilt[df.yearbuilt.notnull()]))\ndf.yearbuilt=pd.cut(df.yearbuilt, 10).cat.codes\ntest.yearbuilt=pd.cut(df.yearbuilt, 10).cat.codes\nprint(df[['yearbuilt','logerror']].groupby(['yearbuilt'], as_index=False)['logerror'].agg(['mean','count']))\n#df.yearbuilt[df.yearbuilt>4]=4\n#df.poolsizesum[df.poolsizesum==0]=1\n#print(df[['yearbuilt','logerror']].groupby(['yearbuilt'], as_index=False)['logerror'].agg(['mean','count']))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"55609b74-6f17-4c2c-9f29-5c32f3da01d2","scrolled":true,"_uuid":"7dc985a160c7af4a03740eccdcd36ec9d7699d37","trusted":false,"collapsed":true},"cell_type":"code","source":"#Assessment value. As even intervals doesn't work well, I chose to categorize manually.\nprint(df.logerror[df.taxvaluedollarcnt.isnull()].mean(),df.logerror[df.taxvaluedollarcnt.notnull()].mean())\nprint(df.logerror[df.taxvaluedollarcnt.isnull()].count(),df.logerror[df.taxvaluedollarcnt.notnull()].count())\nprint(np.corrcoef(df.logerror[df.taxvaluedollarcnt.notnull()],df.taxvaluedollarcnt[df.taxvaluedollarcnt.notnull()]))\nbins=np.multiply([1,2,3,4,5,6,8,10,12.5,15,17.5,20,25,30,40,50,100],100000)\ndf.taxvaluedollarcnt=pd.cut(df.taxvaluedollarcnt,bins=bins).cat.codes\ntest.taxvaluedollarcnt=pd.cut(test.taxvaluedollarcnt,bins=bins).cat.codes\n#df.taxvaluedollarcnt=pd.cut(df.taxvaluedollarcnt, 50)#.cat.codes\nprint(df[['taxvaluedollarcnt','logerror']].groupby(['taxvaluedollarcnt'], as_index=False)['logerror'].agg(['mean','count']))\n#df.yearbuilt[df.yearbuilt>4]=4\n#df.poolsizesum[df.poolsizesum==0]=1\n#print(df[['yearbuilt','logerror']].groupby(['yearbuilt'], as_index=False)['logerror'].agg(['mean','count']))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e27b7325-dbb5-4d4a-b8f8-12e6dd138cdd","_uuid":"91de5eade441807e93f7072018b5c116dc776a52","trusted":false,"collapsed":true},"cell_type":"code","source":"#structure tax value\nprint(df.logerror[df.structuretaxvaluedollarcnt.isnull()].mean(),df.logerror[df.structuretaxvaluedollarcnt.notnull()].mean())\nprint(df.logerror[df.structuretaxvaluedollarcnt.isnull()].count(),df.logerror[df.structuretaxvaluedollarcnt.notnull()].count())\nprint(np.corrcoef(df.logerror[df.structuretaxvaluedollarcnt.notnull()],df.structuretaxvaluedollarcnt[df.structuretaxvaluedollarcnt.notnull()]))\nbins=np.multiply([1,2,3,4,5,6,8,10,12.5,15,17.5,20,25,30,40,50,100],100000/2)\ndf.structuretaxvaluedollarcnt=pd.cut(df.structuretaxvaluedollarcnt,bins=bins).cat.codes\ntest.structuretaxvaluedollarcnt=pd.cut(test.structuretaxvaluedollarcnt,bins=bins).cat.codes\nprint(df[['structuretaxvaluedollarcnt','logerror']].groupby(['structuretaxvaluedollarcnt'], as_index=False)['logerror'].agg(['mean','count']))\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"8ffed306-52c0-448e-a803-82e84f249526","_uuid":"6ab652239fe560053f9ae74f8340a83657231493","trusted":false,"collapsed":true},"cell_type":"code","source":"#land tax value\nprint(df.logerror[df.landtaxvaluedollarcnt.isnull()].mean(),df.logerror[df.landtaxvaluedollarcnt.notnull()].mean())\nprint(df.logerror[df.landtaxvaluedollarcnt.isnull()].count(),df.logerror[df.landtaxvaluedollarcnt.notnull()].count())\nprint(np.corrcoef(df.logerror[df.landtaxvaluedollarcnt.notnull()],df.landtaxvaluedollarcnt[df.landtaxvaluedollarcnt.notnull()]))\nbins=np.multiply([1,2,3,4,5,6,8,10,12.5,15,17.5,20,25,30,40,50,100],100000/2)\ndf.landtaxvaluedollarcnt=pd.cut(df.landtaxvaluedollarcnt,bins=bins).cat.codes\ntest.landtaxvaluedollarcnt=pd.cut(test.landtaxvaluedollarcnt,bins=bins).cat.codes\nprint(df[['landtaxvaluedollarcnt','logerror']].groupby(['landtaxvaluedollarcnt'], as_index=False)['logerror'].agg(['mean','count']))\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"59477902-4303-49ef-b43a-3214ceb8be1e","scrolled":true,"_uuid":"5cb86c709df79cd857f8402b8d0590d7bacd3179","trusted":false,"collapsed":true},"cell_type":"code","source":"#tax amount\nprint(df.logerror[df.taxamount.isnull()].mean(),df.logerror[df.taxamount.notnull()].mean())\nprint(df.logerror[df.taxamount.isnull()].count(),df.logerror[df.taxamount.notnull()].count())\nprint(np.corrcoef(df.logerror[df.taxamount.notnull()],df.taxamount[df.taxamount.notnull()]))\nbins=np.multiply([1,2,3,4,5,6,8,10,12.5,15,17.5,20,25,30,40,50,100],100000/40)\ndf.taxamount=pd.cut(df.taxamount,bins=bins).cat.codes\ntest.taxamount=pd.cut(test.taxamount,bins=bins).cat.codes\nprint(df[['taxamount','logerror']].groupby(['taxamount'], as_index=False)['logerror'].agg(['mean','count']))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"91a2acd8-3d8c-4a8b-a254-b04532fc2ba7","_uuid":"51102edf05dcc575afa98637d567fa19d10686d7","trusted":false,"collapsed":true},"cell_type":"code","source":"#Assessment year is just one year before transaction year. This variable is unnecessary.\nprint(df.logerror[df.assessmentyear.isnull()].mean(),df.logerror[df.assessmentyear.notnull()].mean())\nprint(df.logerror[df.assessmentyear.isnull()].count(),df.logerror[df.assessmentyear.notnull()].count())\nprint(np.corrcoef(df.logerror[df.assessmentyear.notnull()],df.assessmentyear[df.assessmentyear.notnull()]))\n#df.assessmentyear=pd.cut(df.assessmentyear, 10).cat.codes\nprint(df[['assessmentyear','logerror']].groupby(['assessmentyear'], as_index=False)['logerror'].agg(['mean','count']))\ndel df['assessmentyear']\ndel test['assessmentyear']","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"40def8b3-f93d-4596-8668-d61b3ce7b02a","scrolled":true,"_uuid":"2f6104a48ad13e6025a4a2f68aa57dfb5b0be87d","trusted":false,"collapsed":true},"cell_type":"code","source":"#tax delinquency\nprint(df.logerror[df.taxdelinquencyflag.isnull()].mean(),df.logerror[df.taxdelinquencyflag.notnull()].mean())\nprint(df.logerror[df.taxdelinquencyflag.isnull()].count(),df.logerror[df.taxdelinquencyflag.notnull()].count())\n#print(np.corrcoef(df.propertycountylandusecode[df.propertycountylandusecode.notnull()],df.logerror[df.propertycountylandusecode.notnull()]))\nprint(df[['taxdelinquencyflag','logerror']].groupby(['taxdelinquencyflag'], as_index=False)['logerror'].agg(['mean','count']))\ndf.taxdelinquencyflag=pd.Categorical(df.taxdelinquencyflag)\ndf.taxdelinquencyflag=df.taxdelinquencyflag.cat.codes\ntest.taxdelinquencyflag=pd.Categorical(test.taxdelinquencyflag)\ntest.taxdelinquencyflag=test.taxdelinquencyflag.cat.codes\ndf[['taxdelinquencyflag','logerror']].groupby(['taxdelinquencyflag'], as_index=True)['logerror'].agg(['mean','count'])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"99e274b8-707e-4318-8db5-0e6df0306bdb","scrolled":false,"_uuid":"16a29f53421eaf287613681150d8fa1bdb2a9f5d","trusted":false,"collapsed":true},"cell_type":"code","source":"#tax delinquency year\nprint(df.logerror[df.taxdelinquencyyear.isnull()].mean(),df.logerror[df.taxdelinquencyyear.notnull()].mean())\nprint(df.logerror[df.taxdelinquencyyear.isnull()].count(),df.logerror[df.taxdelinquencyyear.notnull()].count())\n#print(np.corrcoef(df.propertycountylandusecode[df.propertycountylandusecode.notnull()],df.logerror[df.propertycountylandusecode.notnull()]))\nprint(df[['taxdelinquencyyear','logerror']].groupby(['taxdelinquencyyear'], as_index=False)['logerror'].agg(['mean','count']))\ndf[['taxdelinquencyyear','logerror']].groupby(['taxdelinquencyyear'], as_index=True)['logerror'].agg(['mean','count'])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"12a32798-a7c3-4d05-8ddb-cbcd3add3e63","_uuid":"a0c987798636045dce2230f1b6f7b3d24d8dd37e","trusted":false,"collapsed":true},"cell_type":"code","source":"#In real estate market, seasonality does matter. I saw manay people just choose not to use transactiondate as variable, but it should be. Also,\n#we do give different results for different months.\ndf.transactiondate=df.transactiondate.str.replace('-','').str[:6]\ndf['month']=df.transactiondate.str[4:6]\ndf['year']=df.transactiondate.str[:4]\nprint(df[['year','month','logerror']].groupby(['year','month'], as_index=False)['logerror'].agg(['mean','count']))\ndf.year=pd.Categorical(df.year)\ndf.year=df.year.cat.codes\ndf.month=pd.Categorical(df.month)\ndf.month=df.month.cat.codes\ndel df['transactiondate']","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"d913cfb7-a41c-4259-be19-1cbc596d1c55","_uuid":"48798a1da6d8c0198059bfc177d63619284ce7d5","trusted":false,"collapsed":true},"cell_type":"code","source":"df[df.isnull()]=-1\ntest[test.isnull()]=-1\nfor c, dtype in zip(df.columns, df.dtypes):\n    if dtype == np.float64:\n        df[c] = df[c].astype(np.int32) \n    elif dtype == np.float32:\n        df[c] = df[c].astype(np.int32)\nfor c, dtype in zip(test.columns, test.dtypes):\n    if dtype == np.float64:\n        test[c] = test[c].astype(np.int32) \n    elif dtype == np.float32:\n        test[c] = test[c].astype(np.int32)\nprint(df.info())\nprint(test.info())","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"a6c77023-b243-43e4-832b-86c65bdfb5f2","scrolled":false,"_uuid":"90d86b6cff67d3c4508f40b1ec69155d7a69c89e","trusted":false,"collapsed":true},"cell_type":"code","source":"import xgboost as xgb\nfrom sklearn.preprocessing import LabelEncoder\nimport lightgbm as lgb\nimport gc\nfrom sklearn.linear_model import LinearRegression\nimport random\n\ndf=df.reindex(sorted(df.columns), axis=1)\nlabel=df.logerror\ndf=df.drop(['parcelid','logerror'],axis=1)\nd_train = lgb.Dataset(df, label=label)\n\n# Parameters\nXGB_WEIGHT = 0.6000\nBASELINE_WEIGHT = 0.0000\nOLS_WEIGHT = 0.0600\n\nXGB1_WEIGHT = 0.8000  # Weight of first in combination of two XGB models\n\nBASELINE_PRED = 0.0115   # Baseline based on mean of training data, per Oleg\n\n##### RUN LIGHTGBM\n\nparams = {}\nparams['max_bin'] = 10\nparams['learning_rate'] = 0.0021 # shrinkage_rate\nparams['boosting_type'] = 'gbdt'\nparams['objective'] = 'regression'\nparams['metric'] = 'l1'          # or 'mae'\nparams['sub_feature'] = 0.345    # feature_fraction (small values => use very different submodels)\nparams['bagging_fraction'] = 0.85 # sub_row\nparams['bagging_freq'] = 40\nparams['num_leaves'] = 512        # num_leaf\nparams['min_data'] = 500         # min_data_in_leaf\nparams['min_hessian'] = 0.05     # min_sum_hessian_in_leaf\nparams['verbose'] = 0\nparams['feature_fraction_seed'] = 2\nparams['bagging_seed'] = 3\n\nnp.random.seed(0)\nrandom.seed(0)\n\nprint(\"\\nFitting LightGBM model ...\")\nclf = lgb.train(params, d_train, 430)\n\ndel d_train; gc.collect()\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"bde1d02a-a677-4fef-b423-bbdd4a860fb0","collapsed":true,"scrolled":true,"_uuid":"23c3ca8816a3fcae931389184dad9428b99af592","trusted":false},"cell_type":"code","source":"test['month']=0\ntest=test.reindex(sorted(test.columns), axis=1)\nfor d in dates:\n    if d!='parcelid':\n        test.month=int(d[4:6])-1\n        sample[d]=clf.predict(test[test.year==(int(d[:4])-2016)].drop(['parcelid'],axis=1))\nsample.to_csv('lightGBM.csv',index=False)","execution_count":null,"outputs":[]}],"metadata":{"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}