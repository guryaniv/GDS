{"cells":[{"metadata":{"_uuid":"3038ceb1939f22cf3d1ac0c9487d4c5ffce198b2","_cell_guid":"74a72eed-35bf-4558-8380-40b6787d241a"},"cell_type":"markdown","source":"Thanks to @ AhmetArdem \n\nhttps://github.com/aerdem4/mercari-price-suggestion\nhttps://www.kaggle.com/c/mercari-price-suggestion-challenge/discussion/49968\n\n1. Fitting RNN tokenization & Sklearn Vectorizations was done only on training data (Took pawel and Konstantin's suggestion)\n2. Prediction of RNN and WordBatch done in Parallel \n3. Wordbatch findings \n    The various feature concantenations produce a significant improvement \n    For ex:  In the below code, Adding X_desc3, X_cat_brand, X_name, X_name2  to the regular fields available in MPC training \n                 data improves score from 0.43X to 0.41X (Without RNN ensemble) \n4. Increased batch_size for successive epochs \n5. Absolutely no word processing was applied ( Except for basic NaN handling )\n    Tried some regex replacements based on ELI5 observations but did not give a huge boost so removed them\n6. Running 2 epochs in RNN should push score into 0.40X ( This could have been possible with extra time provided in Stage 2)\n7. Replaced relu with a PReLU\n"},{"metadata":{"_uuid":"8c873ba17f7c0000c685199705584e91044960ee","_cell_guid":"61051e5e-092c-4611-bdca-fd028d930592","trusted":false,"collapsed":true},"cell_type":"code","source":"import numpy as np \nimport os\nimport gc\nimport time\nstart_time = time.time()\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scipy\n\nos.environ['MKL_NUM_THREADS'] = '4'\nos.environ['OMP_NUM_THREADS'] = '4'\n\n# Sklearn model definition \n\nfrom sklearn.linear_model import Ridge, LogisticRegression\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.pipeline import FeatureUnion\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.preprocessing import LabelBinarizer\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler\nfrom scipy.sparse import csr_matrix, hstack\nfrom sklearn.model_selection import train_test_split\n\n# Keras model definition \n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Input, Dropout, Dense, BatchNormalization, Activation, concatenate, GRU, Embedding, Flatten\nfrom keras.models import Model\nfrom keras.callbacks import ModelCheckpoint, Callback, EarlyStopping#, TensorBoard\nfrom keras import backend as K\nfrom keras import optimizers\nfrom keras import initializers\n\n# Tensor Flow \n\nimport tensorflow as tf\n\nstart_time = time.time()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cf81ee70c1aee2881726d1d1749d3d5bf47e0382","_cell_guid":"c84c8062-3165-4335-9563-1b4c2e932456","collapsed":true,"trusted":false},"cell_type":"code","source":"# RNN Preprocessing, Transformation methods \n#------------------RNN -------------------\n\ndef preprocess_RNN(dataset):\n    \n    print(\"Filling Missing Values\")\n    dataset['category_name'].fillna(value='missing', inplace=True)\n    dataset['brand_name'].fillna(value='missing', inplace=True)\n    dataset['item_description'].fillna(value='missing', inplace=True)\n    \n    print(\"Casting data types to type Category\")\n    dataset['category_name'] = dataset['category_name'].astype('category')\n    dataset['brand_name'] = dataset['brand_name'].astype('category')\n    dataset['item_condition_id'] = dataset['item_condition_id'].astype('category')\n    \n    print(\"RNN PreProcessing completed\")\n    \n    return dataset \n\ndef fit_RNN_text(dataset):\n\n    print(\"Fit Name and Item description fields for Tokenization\")\n    raw_text = np.hstack([dataset.item_description.str.lower(), dataset.name.str.lower()])\n    word_token = Tokenizer()\n    word_token.fit_on_texts(raw_text)\n    print(\"RNN Data fit completed\")\n    \n    return word_token\n\ndef fit_RNN_label(dataset):\n\n    print(\"Fit Categorical variables on full Merged Test and Train data\")\n    \n    le_name = LabelEncoder()\n    le_name.fit(dataset.category_name)\n    le_brand = LabelEncoder()\n    le_brand.fit(dataset.brand_name)\n    \n    print(\"Completed Label fitting\")\n    return le_name, le_brand\n\ndef transform_RNN(dataset, le_name, le_brand, word_token):\n    print(\"Use Defined Label encoders to Encode brand and category_name\")\n    dataset['category'] = le_name.transform(dataset.category_name)\n    dataset['brand'] = le_brand.transform(dataset.brand_name)\n    print(\"Convert Text to sequences\")\n    dataset[\"seq_item_description\"] = word_token.texts_to_sequences(dataset.item_description.str.lower())\n    dataset[\"seq_name\"] = word_token.texts_to_sequences(dataset.name.str.lower())\n    print(\"Sequence Conversion Completed\")\n    \n    return dataset \n\ndef get_keras_data(dataset):\n    X = {\n        'name': pad_sequences(dataset.seq_name, maxlen=MAX_NAME_SEQ)\n        ,'item_desc': pad_sequences(dataset.seq_item_description\n                                    , maxlen=MAX_ITEM_DESC_SEQ)\n        ,'brand': np.array(dataset.brand)\n        ,'category': np.array(dataset.category)\n        ,'item_condition': np.array(dataset.item_condition_id)\n        ,'num_vars': np.array(dataset[[\"shipping\"]])\n    }\n    \n    print(\"Data ready for Vectorization\")\n    \n    return X","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ff5670dff82f007a5e55194ac792d98d33fe3071","_cell_guid":"5269299c-9cf2-46cc-a7d1-05edf3f21cd9","collapsed":true,"trusted":false},"cell_type":"code","source":"def RNN_model():\n\n    #Inputs\n    name = Input(shape=[X_train[\"name\"].shape[1]], name=\"name\")\n    item_desc = Input(shape=[X_train[\"item_desc\"].shape[1]], name=\"item_desc\")\n    brand = Input(shape=[1], name=\"brand\")\n    category = Input(shape=[1], name=\"category\")\n    item_condition = Input(shape=[1], name=\"item_condition\")\n    num_vars = Input(shape=[X_train[\"num_vars\"].shape[1]], name=\"num_vars\")\n    \n    #Embeddings layers\n    emb_size = 60\n    \n    emb_name = Embedding(MAX_TEXT, emb_size//3)(name)\n    emb_item_desc = Embedding(MAX_TEXT, emb_size)(item_desc)\n    emb_brand = Embedding(MAX_BRAND, 10)(brand)\n    emb_category = Embedding(MAX_CATEGORY, 10)(category)\n    emb_item_condition = Embedding(MAX_CONDITION, 5)(item_condition)\n    \n    rnn_layer1 = GRU(25) (emb_item_desc)\n    rnn_layer2 = GRU(12) (emb_name)\n    \n    #main layer\n    main_l = concatenate([\n         Flatten() (emb_brand)\n        , Flatten() (emb_category)\n        , Flatten() (emb_item_condition)\n        , rnn_layer1\n        , rnn_layer2\n        , num_vars\n    ])\n    main_l = Dropout(0.1)(Dense(512,activation='relu') (main_l))\n    main_l = Dropout(0.1)(Dense(64,activation='relu') (main_l))\n    \n    #output\n    output = Dense(1,activation=\"linear\") (main_l)\n    \n    #model\n    model = Model([brand, category, item_condition, item_desc, name, num_vars], output)\n    optimizer = optimizers.Adam()\n    model.compile(loss=\"mse\", \n                  optimizer=optimizer)\n    return model\n\ndef rmsle(y, y_pred):\n    import math\n    assert len(y) == len(y_pred)\n    to_sum = [(math.log(y_pred[i] + 1) - math.log(y[i] + 1)) ** 2.0 \\\n              for i, pred in enumerate(y_pred)]\n    return (sum(to_sum) * (1.0/len(y))) ** 0.5\n\ndef eval_model(model):\n    val_preds = model.predict(X_valid)\n    val_preds = np.expm1(val_preds)\n    y_pred = val_preds[:, 0]\n    \n    y_true = np.array(valid_prices)\n    \n    yt = pd.DataFrame(y_true)\n    yp = pd.DataFrame(y_pred)\n    \n    print(yt.isnull().any())\n    print(yp.isnull().any())\n    \n    v_rmsle = rmsle(y_true, y_pred)\n    print(\" RMSLE error on dev test: \"+str(v_rmsle))\n    return v_rmsle\n\nexp_decay = lambda init, fin, steps: (init/fin)**(1/(steps-1)) - 1","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c39ec1c006d1df0abd181421d4a26afa10bc3d87","_cell_guid":"dbeed561-c01e-40fa-8f50-70aae94c4f55","trusted":false,"collapsed":true},"cell_type":"code","source":"# Loading data \nt1 = time.time()\n\ntrain = pd.read_table('../input/mercari-price-suggestion-challenge/train.tsv', sep='\\t')\ntrain = train[train['price'] !=0 ]\nntrain = train.shape[0]\ntrain['target'] = np.log1p(train['price'])\ntest = pd.read_table('../input/mercari-price-suggestion-challenge/test_stg2.tsv', sep='\\t')\nntest = test.shape[0]\nmerge = pd.concat([train, test], 0, ignore_index = True)\n\ndel test, train\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e599e59dcbb2112998f8de68bf1d7297ccafe8e3","_cell_guid":"d1f19ba7-c2e3-40a8-b6f1-bb5809a944f5","trusted":false,"collapsed":true},"cell_type":"code","source":"# Training RNN \n# Pre Process RNN Data - Missing data, Tokenization \n\nt1 = time.time()\nmerge = preprocess_RNN(merge)\nt2 = time.time()\nprint(\"Time taken for RNN preprocess \"+str(t2-t1))\nprint(merge.shape)\n\n# Fitting Categorical columns on test and train data \nt1 = time.time()\nle_name, le_brand = fit_RNN_label(merge)\nt2 = time.time()\nprint(\"Time taken to Fit on RNN \"+str(t2-t1))\n\n# Fitting Text data on Train data only \nt1 = time.time()\ntrain = merge[:ntrain]\nword_token = fit_RNN_text(train)\n\n# Transforming training data to be readied for RNN TRAINING \ntrain = transform_RNN(train, le_name, le_brand, word_token)\nt2 = time.time()\nprint(\"Time taken to Fit and Transform on RNN \"+str(t2-t1))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ad73954fdcc5d078b2da61b77c4a65a3d6e773e9","_cell_guid":"d0bdff57-2b48-400a-9025-0c6015ecdaee","collapsed":true,"trusted":false},"cell_type":"code","source":"print(train.shape, merge.shape)\ndel merge\ngc.collect","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ed6d253ed88a6a598f604872e0be6aec5af6b2f6","_cell_guid":"05a7b4ad-886b-4ef6-ba0d-6199293c7e15","collapsed":true,"trusted":false},"cell_type":"code","source":"#EMBEDDINGS MAX VALUE\n#Max_text is USED TO CALCULATE VOCABULARY LENGTH for EMBEDDING \n#MAX_Seq values are USED TO CALCULATE PADDING LENGTHS \n# For padding \nMAX_NAME_SEQ = 20 \nMAX_ITEM_DESC_SEQ = 60 \nMAX_CATEGORY_NAME_SEQ = 20 \nMAX_CATEGORY = np.max(train.category.max())+1\nMAX_BRAND = np.max(train.brand.max())+1\nMAX_CONDITION = 6\n# For Vocab length --> THE VALUE OF MAX_TEXT is THE VALUE OF THE VOCABULARY LENGTH \nMAX_TEXT = np.max([np.max(train.seq_name.max()) , np.max(train.seq_item_description.max())])+2\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"32d559af6d1c092bc8c3e9701b075976ad0a4852","_cell_guid":"93fcde06-e29b-4258-b65c-652e38125517","trusted":false,"collapsed":true},"cell_type":"code","source":"# Split into test and train data \n\ndtrain, dvalid = train_test_split(train, random_state=233, train_size=0.99)\n\ndtrain['target'] =np.log1p(dtrain['price'])\ntarget  = np.array(dtrain.target)\ndvalid['target'] =np.log1p(dvalid['price'])\nvalid_prices = np.array(dvalid.target)\nprint(dvalid.shape, dtrain.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ad729523103e716651fec52fa8a9f1b41698d309","_cell_guid":"81d55ccc-3ce8-46db-b849-6fb1110152da","trusted":false,"collapsed":true},"cell_type":"code","source":"# Sequence padding \ndel train\nX_train = get_keras_data(dtrain)\nX_valid = get_keras_data(dvalid)\ndel dtrain, dvalid\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e17c1f8c467b5cf6fe7c6ad12aa451b8659a67f4","_cell_guid":"32c2256e-89d7-4447-b94c-e443f66d39e9","collapsed":true,"trusted":false},"cell_type":"code","source":"#FITTING THE MODEL\nepochs = 1\nBATCH_SIZE = 512 * 3\nsteps = int(len(X_train['name'])/BATCH_SIZE) * epochs\nlr_init, lr_fin = 0.009, 0.0045\nlr_decay = exp_decay(lr_init, lr_fin, steps)\nmodelRNN = RNN_model()\nK.set_value(modelRNN.optimizer.lr, lr_init)\nK.set_value(modelRNN.optimizer.decay, lr_decay)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"265d9dda17efa97bd0ee9848e6ba28247fc9c3e5","_cell_guid":"21f286e2-1805-43b8-b96d-a466f813b935","trusted":false,"collapsed":true},"cell_type":"code","source":"# Fitting RNN \n\nos.environ['OMP_NUM_THREADS'] = '4'\nos.environ['MKL_NUM_THREADS'] = '4'\nfor i in range(2):\n    history = modelRNN.fit(X_train, target\n                    , epochs=epochs\n                    , batch_size=2**(11+i)\n                    , validation_data = (X_valid, valid_prices)\n                    , verbose=1\n                    )\n    # Evaluate RMSLE \n    v_rmsle = eval_model(modelRNN)\n    print('[{}] Finished predicting valid set...'.format(time.time() - start_time))\n    \nprint(\"Finished Fitting the model\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c1d8bc047da7c8cef50b033f302bfa775513163e","_cell_guid":"4284d217-b635-45b8-abac-ef6238de4b0e","collapsed":true,"trusted":false},"cell_type":"code","source":"del X_train, X_valid\ndel target, valid_prices\ndel epochs, lr_init, lr_fin, exp_decay, lr_decay\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0ccd3e3063613d112a85a588e9f8a291082fc3bd","_cell_guid":"45b3672f-85c9-4c65-80e3-3dba44e2b06f","collapsed":true,"trusted":false},"cell_type":"code","source":"t1 = time.time()\ntrain = pd.read_csv('../input/mercari-price-suggestion-challenge/train.tsv', sep='\\t')\ntrain = train[train['price'] != 0]\ntrain['target'] = np.log1p(train['price'])\nn_train = train.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"db54702eab67bca0233fdadea97d597fdfbaec39","_cell_guid":"faaf26e5-93ce-42d3-aa8d-28d557b2aaf0","collapsed":true,"trusted":false},"cell_type":"code","source":"def split_cat(text):\n    try: \n        s1, s2, s3 = text.split(\"/\")\n        return s1, s2, s3\n    \n    except: return (\"No Label\", \"No Label\", \"No Label\")\n\ndef wordbatch_preprocess(df):\n    df[\"category_name\"] = df[\"category_name\"].fillna(value=\"missing\").astype(str)\n    df[\"name\"] = df[\"name\"].fillna(value=\"missing\").astype(str)\n    df[\"brand_name\"] = df[\"brand_name\"].fillna(value=\"missing\").astype(str)\n    df[\"item_description\"] = df[\"item_description\"].fillna(value=\"missing\").astype(str)\n    df[\"item_condition_id\"] = df[\"item_condition_id\"].astype(int)\n    df[\"shipping\"] = df[\"shipping\"].astype(int)\n    print(\"Got so far\")\n    \n    print(df.dtypes)\n    df['subcat_0'], df['subcat_1'], df['subcat_2'] = zip(*df['category_name'].apply(lambda x: split_cat(x)))\n    print(df.dtypes)\n    \n    return df\n\n# Pre Processing for word batch \n\ntrain = wordbatch_preprocess(train)\n\nprint(\"Pre Process for Word batch is done\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e7fce8168be6811f9caf53404687a9750cc7094e","_cell_guid":"5c977c62-fdc7-4a71-828a-9b4336556321","collapsed":true,"trusted":false},"cell_type":"code","source":"import numpy as np\nfrom scipy.sparse import hstack\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport wordbatch\nfrom wordbatch.extractors import WordBag\nfrom wordbatch.models import FM_FTRL\n\nclass WordBatchModel(object):\n    def __init__(self):\n        self.wb_desc = None\n        self.desc_indices = None\n        self.cv_name, self.cv_name2 = None, None\n        self.cv_cat0, self.cv_cat1, self.cv_cat2 = None, None, None\n        self.cv_brand = None\n        self.cv_condition = None\n        self.cv_cat_brand = None\n        self.desc3 = None\n        self.model = None\n\n    def train_wbm(self, df):\n\n        self.wb_desc = wordbatch.WordBatch(None,\n                                           extractor=(WordBag, {\"hash_ngrams\": 2, \"hash_ngrams_weights\": [1.0, 1.0],\n                                                                \"hash_size\": 2 ** 28, \"norm\": \"l2\", \"tf\": 1.0,\n                                                                \"idf\": None}), procs=8)\n        self.wb_desc.dictionary_freeze = True\n        X_desc = self.wb_desc.fit_transform(df['item_description'])\n        self.desc_indices = np.array(np.clip(X_desc.getnnz(axis=0) - 1, 0, 1), dtype=bool)\n        X_desc = X_desc[:, self.desc_indices]\n        \n        self.cv_cat0 = CountVectorizer(min_df=2)\n        X_category0 = self.cv_cat0.fit_transform(df['subcat_0'])\n        \n        self.cv_cat1 = CountVectorizer(min_df=2)\n        X_category1 = self.cv_cat1.fit_transform(df['subcat_1'])\n        \n        self.cv_cat2 = CountVectorizer(min_df=2)\n        X_category2 = self.cv_cat2.fit_transform(df['subcat_2'])\n\n        self.cv_brand = CountVectorizer(min_df=2, token_pattern=\".+\")\n        X_brand = self.cv_brand.fit_transform(df['brand_name'])\n\n        # Variations \n\n        self.cv_name = CountVectorizer(min_df=2, ngram_range=(1, 1),binary=True, token_pattern=\"\\w+\")\n        X_name = 2 * self.cv_name.fit_transform(df['name'])\n        \n        self.cv_name2 = CountVectorizer(min_df=2, ngram_range=(2, 2), binary=True, token_pattern=\"\\w+\")\n        X_name2 = 0.5 * self.cv_name2.fit_transform(df['name'])\n                                                      \n        df[\"cat_brand\"] = [a + \" \" + b for a, b in zip(df[\"category_name\"], df[\"brand_name\"])]\n        self.cv_cat_brand = CountVectorizer(min_df=10, token_pattern=\".+\")\n        X_cat_brand = self.cv_cat_brand.fit_transform(df[\"cat_brand\"])\n        \n        self.cv_condition = CountVectorizer(token_pattern=\".+\")\n        X_condition = self.cv_condition.fit_transform((df['item_condition_id'] + 10 * df[\"shipping\"]).apply(str))\n            \n        self.desc3 = CountVectorizer(ngram_range=(3, 3), max_features=1000, binary=True, token_pattern=\"\\w+\")\n        X_desc3 = self.desc3.fit_transform(df[\"item_description\"])\n\n        X = hstack((X_condition,\n                    X_desc, X_brand,\n                    X_category0, X_category1, X_category2,\n                    X_name, X_name2,\n                    X_cat_brand, \n                    X_desc3\n                    )).tocsr()\n\n        print(\"X Reconstructed\")\n        y = df['target'].values\n        #y = y.reshape(y.shape[0],1)\n        print(\"Y created\")\n\n        self.model = FM_FTRL(alpha=0.01, beta=0.01, L1=0.00001, L2=0.1, D=X.shape[1], alpha_fm=0.02, L2_fm=0.0,\n                             init_fm=0.01, D_fm=200, e_noise=0.0001, iters=10, inv_link=\"identity\", threads=4)\n        print(\"Model Defined\")\n        self.model.fit(X, y)\n\n    def predict(self, df):\n        X_desc = self.wb_desc.transform(df[\"item_description\"])\n        X_desc = X_desc[:, self.desc_indices]\n        \n        X_brand = self.cv_brand.transform(df['brand_name'])\n\n        X_name = 2 * self.cv_name.transform(df[\"name\"])\n        X_name2 = 0.5 * self.cv_name2.transform(df[\"name\"])\n\n        X_category0 = self.cv_cat0.transform(df['subcat_0'])\n        X_category1 = self.cv_cat1.transform(df['subcat_1'])\n        X_category2 = self.cv_cat2.transform(df['subcat_2'])\n        \n        X_condition = self.cv_condition.transform((df['item_condition_id'] + 10 * df[\"shipping\"]).apply(str))\n        \n        df[\"cat_brand\"] = [a + \" \" + b for a, b in zip(df[\"category_name\"], df[\"brand_name\"])]\n        X_cat_brand = self.cv_cat_brand.transform(df[\"cat_brand\"])\n        \n        X_desc3 = self.desc3.transform(df[\"item_description\"])\n\n        X = hstack((X_condition,\n                    X_desc, X_brand,\n                    X_category0, X_category1, X_category2,\n                    X_name, X_name2,\n                    X_cat_brand, \n                    X_desc3\n                   )).tocsr()\n\n        return self.model.predict(X)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0ab140b7c5afc9f36e57bcb5ae926b60577a6ed0","_cell_guid":"560ea823-2d90-4b90-8298-40e1367885ee","collapsed":true,"trusted":false},"cell_type":"code","source":"# Training WordBatch \nos.environ['MKL_NUM_THREADS'] = '4'\nos.environ['OMP_NUM_THREADS'] = '4'\nprint(train.shape)\nt1 = time.time()\nwbm = WordBatchModel()\nwbm.train_wbm(train)\nt2 = time.time()\nprint(\"Time taken for Word Batch is \"+str(t2-t1))\ndel train \ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b74f70876a1053499af5d8e85930c13b35c11c3a","_cell_guid":"df1cfd98-dafb-4603-832e-513f30befb47","collapsed":true,"trusted":false},"cell_type":"code","source":"import time\nt1 = time.time()\ndef load_test():\n    for df in pd.read_csv('../input/mercari-price-suggestion-challenge/test_stg2.tsv', sep='\\t', chunksize= 1000000):\n        yield df\n\ntest_ids = np.array([], dtype=np.int32)\npreds= np.array([], dtype=np.float32)\n\ni = 0 \n    \nfor df in load_test():\n    \n    i +=1\n    print(\" Chunk number is \"+str(i))\n    df1 = df2 = df\n    testRNN = preprocess_RNN(df1)\n    print(df1.dtypes)\n    print(testRNN.dtypes)\n    testRNN = transform_RNN(testRNN, le_name, le_brand, word_token)\n    print(testRNN.dtypes)\n    X_testRNN = get_keras_data(testRNN)\n    predsRNN1 = modelRNN.predict(X_testRNN, batch_size = BATCH_SIZE, verbose = 1)\n    test_id = df['test_id']\n    del testRNN\n    del df['seq_item_description'], df['seq_name'], df['brand'], df['category']\n    gc.collect()\n    print(\"RNN Prediction is done\")\n    \n    print(df.isnull().any())\n    print(df.dtypes)\n    test_WB = wordbatch_preprocess(df2)\n    print(df.dtypes)\n    print(\"Word batch preprocess done\")\n    predsWB1 = wbm.predict(test_WB)\n    print(\"Word Batch Prediction done\")\n\n    predsRNN = np.expm1(predsRNN1)\n    predsWB = np.expm1(predsWB1)\n    \n    predsRNN = predsRNN.reshape(predsRNN.shape[0],1)\n    predsWB = predsWB.reshape(predsWB.shape[0],1)\n\n    predsRNN = np.clip(predsRNN, 0, predsRNN.max())\n    predsWB = np.clip(predsWB, 0, predsWB.max())\n    \n    preds= np.append(preds, ((predsRNN + predsWB)*0.5))\n    test_ids = np.append(test_ids, test_id)\n    \nprint(\"All chunks done\")\nt2 = time.time()\nprint(\"Total time for Parallel Batch Prediction is \"+str(t2-t1))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c27a3d388d4c0c538ea323068368fa0499fd7d4f","_cell_guid":"fa9dedd0-c25d-4965-8b27-a24a075b7490","collapsed":true,"trusted":false},"cell_type":"code","source":"print(preds.shape)\nprint(preds)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7601ad41bcd3dccc6bcb49a74d962b9d500c6625","_cell_guid":"1dcb44e2-e17a-43cf-8191-6c6b79ce04ac","collapsed":true,"trusted":false},"cell_type":"code","source":"import pandas as pd \n\nsubmission = pd.DataFrame( columns = ['test_id', 'price'])\nsubmission['test_id'] = test_ids\nsubmission['price'] = preds\n\nprint(\"Check Submission NOW!!!!!!!!@\")\nsubmission.to_csv(\"BatchPrediction_MemoryCoreOptimized.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"591adecb59835251ff4a9ac6372150216168a55f","_cell_guid":"cf2f2e1d-8a52-4ed3-bab0-f84c1ba0ff02","collapsed":true,"trusted":false},"cell_type":"code","source":"submission","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5575b52bc1fd314c2b876fb08ee7c8adbf8bd910","_cell_guid":"a458819a-285b-443e-9337-b63e3c8345be","collapsed":true,"trusted":false},"cell_type":"code","source":"t2 = time.time()\nprint(\"Total time taken is \"+str(t2-start_time))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"590c05434e4e27294e469deb7c5be400429f2a5c","_cell_guid":"94166b2a-861f-4819-a7e9-f42e322a11c2","collapsed":true,"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}