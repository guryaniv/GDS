{"cells":[{"metadata":{"_uuid":"7acf6bf0ea6ec407b3f60cbd74227409bab01dc0"},"cell_type":"markdown","source":"## **Import Statments**"},{"metadata":{"_cell_guid":"01c9209f-c439-4c69-90fd-205c4a4dcd92","_uuid":"e69de2804383f8ecf5393b06b6f0eb0f28baebbb","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn import preprocessing\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.model_selection import train_test_split, KFold\nfrom nltk.corpus import stopwords\nfrom nltk.stem.snowball import SnowballStemmer\nimport matplotlib\nfrom matplotlib import pyplot as plt\n# import seaborn as sns\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cebee1e5f5866f197d290457b6eb1c6df1ea50f6"},"cell_type":"markdown","source":"# **Loading and inspecting data**"},{"metadata":{"_uuid":"52fb58be63fa4e505ff5f0bdc39fe5f8f5343ebe"},"cell_type":"markdown","source":"Reading the data"},{"metadata":{"_cell_guid":"b6f79fa1-2835-41f7-bb7b-4e064f1577fb","_uuid":"bf9032ed06694055aa69c0348ce3e83980dac41d","trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"../input/train.csv\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"82c4e9bac98a38d2cda837e9bd800bb0cc2d917c"},"cell_type":"markdown","source":"Displaying the head of the data"},{"metadata":{"_cell_guid":"b68d74a0-0d6d-4d72-859f-cfeab9d9ee59","_uuid":"7c9accf9d18b1c4c5aa5910ed81cb427db3ce393","trusted":true},"cell_type":"code","source":"data.head(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"583784c0480b10f0988bf9cca647ce49f1875e6c"},"cell_type":"markdown","source":"Shape of the data"},{"metadata":{"_cell_guid":"72d28e14-9b17-4fc8-8767-ff2f2e42a9af","_uuid":"01ba1c4d1b23a52f5e58590284e51e572da27121","trusted":true},"cell_type":"code","source":"data.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e552e4d4d13508aae7e0b7320c7d783b5773874a"},"cell_type":"markdown","source":"**Bar chart of class proportion**"},{"metadata":{"_cell_guid":"775efcb2-ddb8-46a0-8413-f0dff397d18a","_uuid":"837dea12767caf37978e73b8e37fd014f7529a62","trusted":true},"cell_type":"code","source":"# extracting the number of examples of each class\nEAP_len = data[data['author'] == 'EAP'].shape[0]\nHPL_len = data[data['author'] == 'HPL'].shape[0]\nMWS_len = data[data['author'] == 'MWS'].shape[0]\nprint(EAP_len,HPL_len, MWS_len, \"    total = \", EAP_len+HPL_len+MWS_len)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"659bfbb5-fdd2-4211-bcc7-cef086a053a4","_uuid":"707efe4c88881c03081442dffcbb1a02a664802c","trusted":true},"cell_type":"code","source":"# bar plot of the 3 classes\nplt.bar(10,EAP_len,3, label=\"EAP\")\nplt.bar(15,HPL_len,3, label=\"HPL\")\nplt.bar(20,MWS_len,3, label=\"MWS\")\nplt.legend()\nplt.ylabel('Number of examples')\nplt.title('Propoertion of examples')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"34aa05cf2321b1e60d92701b398aadc8ea000b0e"},"cell_type":"markdown","source":"# **Feature Engineering**"},{"metadata":{"_uuid":"9f5a8f3df2f314072fdbbdace296ce645b5c7602"},"cell_type":"markdown","source":"![](http://)## **Removing punctions**"},{"metadata":{"_uuid":"27c4d955fda7f5374c9c5f8a4c0a78e75599885c"},"cell_type":"markdown","source":"**Funtion to remove punctuation**"},{"metadata":{"_cell_guid":"19f1782b-f6f9-420c-a747-289c45e62687","_uuid":"c1ae99d031c1248cedd74d84699d3c1cc6bd5078","trusted":true},"cell_type":"code","source":"def remove_punctuation(text):\n    '''a function for removing punctuation'''\n    import string\n    # replacing the punctuations with no space, \n    # which in effect deletes the punctuation marks \n    translator = str.maketrans('', '', string.punctuation)\n    # return the text stripped of punctuation marks\n    return text.translate(translator)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"939d34384ba7be3e602390c572b4e1f64f6f56eb"},"cell_type":"markdown","source":"**Apply the function to each examples**"},{"metadata":{"_cell_guid":"d2fafe6b-9bf1-4e90-bd25-bb003c29e180","_uuid":"426c5f09a587d9cd04f2159b7111907a0e3a25a1","trusted":true},"cell_type":"code","source":"data['text'] = data['text'].apply(remove_punctuation)\ndata.head(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7090eede4d881b930c543ea51708a65522112eba"},"cell_type":"markdown","source":"## **Removing stopwords**"},{"metadata":{"_uuid":"39c25e79f9aeabdfe4d8400d844def14cd1f2855"},"cell_type":"markdown","source":"**Extract the stop words**"},{"metadata":{"_cell_guid":"5561eee4-cc1f-4a12-b959-6233ca9f3999","_uuid":"e495c38becd43825387056e3c7e836ab64c8e96f","trusted":true},"cell_type":"code","source":"# extracting the stopwords from nltk library\nsw = stopwords.words('english')\n# displaying the stopwords\nnp.array(sw)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3f2f8acb-8929-4dc2-a79e-eb394e28ee78","_uuid":"2e9d00e3a25a4a478fd6fb58a2983c11ea91839e","trusted":true},"cell_type":"code","source":"print(\"Number of stopwords: \", len(sw))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"aeef3fb8459b3ae2c1fb29c1dddd0c9c26d25669"},"cell_type":"markdown","source":"**Function to remove stopwords**"},{"metadata":{"_cell_guid":"ecf70a80-334e-42bc-a238-cd9ae39e324e","_uuid":"4b1d9da7087c810c07fb77e0aa9081871b97af22","trusted":true},"cell_type":"code","source":"def stopwords(text):\n    '''a function for removing the stopword'''\n    # removing the stop words and lowercasing the selected words\n    text = [word.lower() for word in text.split() if word.lower() not in sw]\n    # joining the list of words with space separator\n    return \" \".join(text)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ecc918f568599c29301ee258d7beecdd293f5897"},"cell_type":"markdown","source":"**Apply the function to each examples**"},{"metadata":{"_cell_guid":"692cc993-932e-4c67-8b0b-20926e495e3a","scrolled":true,"_uuid":"58301bc228c8ba1403cd5dcf9696b5bb2e5c9198","trusted":true},"cell_type":"code","source":"data['text'] = data['text'].apply(stopwords)\ndata.head(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"29f7bfbb6d4755cec8d6d9c53997410f4a125e3a"},"cell_type":"markdown","source":"## **Top words before stemming**"},{"metadata":{"_uuid":"673ecdc6fca51324ea817b9f465b4bdcf62de234"},"cell_type":"markdown","source":"**Collect vocabulary count**\n\nWe will not use word counts as feature for NLP since tf-idf is a better metric"},{"metadata":{"_cell_guid":"e398cb01-a8c4-4ea8-afb1-428898ad7dab","_uuid":"7ecc0e3ccf65dfc9a01271e5b84a9885b79d6754","trusted":true},"cell_type":"code","source":"# create a count vectorizer object\ncount_vectorizer = CountVectorizer()\n# fit the count vectorizer using the text data\ncount_vectorizer.fit(data['text'])\n# collect the vocabulary items used in the vectorizer\ndictionary = count_vectorizer.vocabulary_.items()  ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2dda4f2a8cfb07bca739a0ce9e8535434338b1a6"},"cell_type":"markdown","source":"Store the vocab and counts in a pandas dataframe"},{"metadata":{"_cell_guid":"85f9f01f-edd4-43a2-8375-a3801b7975f8","collapsed":true,"_uuid":"559abc3827e7154cb6c9cbe662620eaca9184eed","trusted":false},"cell_type":"code","source":"# lists to store the vocab and counts\nvocab = []\ncount = []\n# iterate through each vocab and count append the value to designated lists\nfor key, value in dictionary:\n    vocab.append(key)\n    count.append(value)\n# store the count in panadas dataframe with vocab as index\nvocab_bef_stem = pd.Series(count, index=vocab)\n# sort the dataframe\nvocab_bef_stem = vocab_bef_stem.sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e25326389fc7de019fea6a4d9528f361576195ef"},"cell_type":"markdown","source":"**Bar plot of top words before stemming**"},{"metadata":{"_cell_guid":"3c6f2850-ad05-4bef-b024-198d0267dd7d","scrolled":false,"_uuid":"ed02e10fa9b1db3ce8d11269f973a7b60915285c","trusted":false},"cell_type":"code","source":"top_vacab = vocab_bef_stem.head(20)\ntop_vacab.plot(kind = 'barh', figsize=(5,10), xlim= (25230, 25260))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9814b4ccee8ee9d44f0d1b70389f7561aa245d66"},"cell_type":"markdown","source":"##  **Stemming operations**"},{"metadata":{"_uuid":"45d8b172e99a9dde285de7df44f646355dcbd38d"},"cell_type":"markdown","source":"Stemming operation bundles together words of same root. E.g. stem operation bundles \"response\" and \"respond\" into a common \"respon\" "},{"metadata":{"_uuid":"ae8311bf9c0448985c1ccdb42a88da95cc6d05e9"},"cell_type":"markdown","source":"**A funtion to carry out stemming operation**"},{"metadata":{"_cell_guid":"da669b28-e6da-4f1c-b219-0161b5c2fb8b","collapsed":true,"_uuid":"5967cc037e541f621ad817e262a83e23510a116f","trusted":false},"cell_type":"code","source":"# create an object of stemming function\nstemmer = SnowballStemmer(\"english\")\n\ndef stemming(text):    \n    '''a function which stems each word in the given text'''\n    text = [stemmer.stem(word) for word in text.split()]\n    return \" \".join(text) ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"333d1bdfee2b084e4efa4ebd46902015ae6be28c"},"cell_type":"markdown","source":"**Apply the function to each examples**"},{"metadata":{"_cell_guid":"9dfaf9e2-c77d-4f68-8b7b-aa26cda11707","_uuid":"60d64a3db3e6dd388b27eac313735e722fdea6a0","trusted":false},"cell_type":"code","source":"data['text'] = data['text'].apply(stemming)\ndata.head(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b20be7faedebcc25a1daac2734ffad7a0235c3d2"},"cell_type":"markdown","source":"## **Top words after stemming operation**"},{"metadata":{"_uuid":"c403430ae4b526e932328b72b68d7b02f8065277"},"cell_type":"markdown","source":"**Collect vocabulary count**"},{"metadata":{"_cell_guid":"e7cc96d8-20a4-4f8d-97d1-de69bd7981a3","collapsed":true,"_uuid":"9135efbc2f7d23479d16480b4f0960a52d20cc00","trusted":false},"cell_type":"code","source":"# create the object of tfid vectorizer\ntfid_vectorizer = TfidfVectorizer(\"english\")\n# fit the vectorizer using the text data\ntfid_vectorizer.fit(data['text'])\n# collect the vocabulary items used in the vectorizer\ndictionary = tfid_vectorizer.vocabulary_.items()  ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c7d6bf78ab85334b803f7cbb09f8d07292da1a88"},"cell_type":"markdown","source":"**Bar plot of top words after stemming**"},{"metadata":{"_cell_guid":"a8422280-9502-4551-8d67-038acd330c42","_uuid":"caa0a0876ba968cfa73b296f67adaf31de0d8e73","trusted":false},"cell_type":"code","source":"# lists to store the vocab and counts\nvocab = []\ncount = []\n# iterate through each vocab and count append the value to designated lists\nfor key, value in dictionary:\n    vocab.append(key)\n    count.append(value)\n# store the count in panadas dataframe with vocab as index\nvocab_after_stem = pd.Series(count, index=vocab)\n# sort the dataframe\nvocab_after_stem = vocab_after_stem.sort_values(ascending=False)\n# plot of the top vocab\ntop_vacab = vocab_after_stem.head(20)\ntop_vacab.plot(kind = 'barh', figsize=(5,10), xlim= (15120, 15145))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8a99ed91e66d75bc2f20e9e5bb1af07212b6ab75"},"cell_type":"markdown","source":"## **Histogram of text length of each writer**"},{"metadata":{"_uuid":"45653408f00d65b37b68f95216bcd6690f0a7b7f"},"cell_type":"markdown","source":"A function to return the length of text"},{"metadata":{"collapsed":true,"_uuid":"013726305048a78a5c4bd70fffaa2fce3104f0b8","trusted":false},"cell_type":"code","source":"def length(text):    \n    '''a function which returns the length of text'''\n    return len(text)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"61b3576325d2ddc41eb14e69c91941b932f2dcc1"},"cell_type":"markdown","source":"Apply the function to each example"},{"metadata":{"_uuid":"faeefd6f1b2f80eea64d650c909288514b52a5fd","trusted":false},"cell_type":"code","source":"data['length'] = data['text'].apply(length)\ndata.head(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b6b94d40236d9a8419bacceb02554a6b117b12ec"},"cell_type":"markdown","source":"**Extracting data of each class**"},{"metadata":{"collapsed":true,"_uuid":"454272e105d4cadd5730d500308c45e912bd3de2","trusted":false},"cell_type":"code","source":"EAP_data = data[data['author'] == 'EAP']\nHPL_data = data[data['author'] == 'HPL']\nMWS_data = data[data['author'] == 'MWS']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e3e5d7f11b97ead24e873adda355cda8cbd30c20"},"cell_type":"markdown","source":"**Histogram of text lenght of  each writer**\n\nAs we can see the distributions coincides so it better to leave out text length as a feature for predictive modelling"},{"metadata":{"_cell_guid":"934609a3-18a4-496c-84ab-3c19ba888944","_uuid":"a34e00e8b70fea1e2068ccaec69f087dafa15597","trusted":false},"cell_type":"code","source":"matplotlib.rcParams['figure.figsize'] = (12.0, 6.0)\nbins = 500\nplt.hist(EAP_data['length'], alpha = 0.6, bins=bins, label='EAP')\nplt.hist(HPL_data['length'], alpha = 0.8, bins=bins, label='HPL')\nplt.hist(MWS_data['length'], alpha = 0.4, bins=bins, label='MWS')\nplt.xlabel('length')\nplt.ylabel('numbers')\nplt.legend(loc='upper right')\nplt.xlim(0,300)\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"633b9c762704b33cf58bda3bc8893370b7e6ce9f"},"cell_type":"markdown","source":"# **Top words of each writer and their count**"},{"metadata":{"_uuid":"19461c453d2a3ff35c9cd62d4f944ae442f5dba4"},"cell_type":"markdown","source":"## **Edgar Allan Poe**"},{"metadata":{"_cell_guid":"7ae948fc-5aa4-4793-bebc-cb3b7c36cc68","_uuid":"d2c6ce9b179276fb10c67a7e417d7f39810e7a8d","trusted":false},"cell_type":"code","source":"# create the object of tfid vectorizer\nEAP_tfid_vectorizer = TfidfVectorizer(\"english\")\n# fit the vectorizer using the text data\nEAP_tfid_vectorizer.fit(EAP_data['text'])\n# collect the vocabulary items used in the vectorizer\nEAP_dictionary = EAP_tfid_vectorizer.vocabulary_.items()\n\n# lists to store the vocab and counts\nvocab = []\ncount = []\n# iterate through each vocab and count append the value to designated lists\nfor key, value in EAP_dictionary:\n    vocab.append(key)\n    count.append(value)\n# store the count in panadas dataframe with vocab as index\nEAP_vocab = pd.Series(count, index=vocab)\n# sort the dataframe\nEAP_vocab = EAP_vocab.sort_values(ascending=False)\n# plot of the top vocab\ntop_vacab = EAP_vocab.head(20)\ntop_vacab.plot(kind = 'barh', figsize=(5,10), xlim= (9700, 9740))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3451a8e86abbfc84d61ed26d461970ecc396f373"},"cell_type":"markdown","source":"## **Mary Shelley**"},{"metadata":{"_cell_guid":"a7f09e96-0f8d-4e0f-8e3c-e58bb2e4a877","_uuid":"b4d1517da8ce8dd3af761f14d331ef1ef7e41f85","trusted":false},"cell_type":"code","source":"# create the object of tfid vectorizer\nHPL_tfid_vectorizer = TfidfVectorizer(\"english\")\n# fit the vectorizer using the text data\nHPL_tfid_vectorizer.fit(HPL_data['text'])\n# collect the vocabulary items used in the vectorizer\nHPL_dictionary = HPL_tfid_vectorizer.vocabulary_.items()\n# lists to store the vocab and counts\nvocab = []\ncount = []\n# iterate through each vocab and count append the value to designated lists\nfor key, value in HPL_dictionary:\n    vocab.append(key)\n    count.append(value)\n# store the count in panadas dataframe with vocab as index    \nHPL_vocab = pd.Series(count, index=vocab)\n# sort the dataframe\nHPL_vocab = HPL_vocab.sort_values(ascending=False)\n# plot of the top vocab\ntop_vacab = HPL_vocab.head(20)\ntop_vacab.plot(kind = 'barh', figsize=(5,10), xlim= (9300, 9330))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9e83c3bfdef56ec39cd1d2ee22f172544df2f554"},"cell_type":"markdown","source":"## **HP Lovecraft**"},{"metadata":{"_cell_guid":"5f8aa150-fb29-43c5-96e9-295ec9976108","scrolled":false,"_uuid":"fff75d0664db171ab4f5462e7146677aa1113448","trusted":false},"cell_type":"code","source":"# create the object of tfid vectorizer\nMWS_tfid_vectorizer = TfidfVectorizer(\"english\")\n# fit the vectorizer using the text data\nMWS_tfid_vectorizer.fit(MWS_data['text'])\n# collect the vocabulary items used in the vectorizer\nMWS_dictionary = MWS_tfid_vectorizer.vocabulary_.items()\n# lists to store the vocab and counts\nvocab = []\ncount = []\n# iterate through each vocab and count append the value to designated list\nfor key, value in MWS_dictionary:\n    vocab.append(key)\n    count.append(value)\n# store the count in panadas dataframe and vocab as index    \nMWS_vocab = pd.Series(count, index=vocab)\n# sort the dataframe\nMWS_vocab = MWS_vocab.sort_values(ascending=False)\n# plot of the top vocab\ntop_vacab = MWS_vocab.head(20)\ntop_vacab.plot(kind = 'barh', figsize=(5,10), xlim= (7010, 7040))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3233ec9c78559037610cbeb6ab5b5df745ec80f8"},"cell_type":"markdown","source":"**As we can see the top words of each writer are cleary distinct and are in huge numbers. Word Count or TF-IDF of the can provide a good feature**"},{"metadata":{"_uuid":"46b198492d0f00883df7a323265d749521b2a340"},"cell_type":"markdown","source":"# **TF-IDF Extraction**"},{"metadata":{"_uuid":"5402ebd329b2704ab3d6122033968e66c73c81c5"},"cell_type":"markdown","source":"tf-idf weight is product of two terms: the first term is the normalized Term Frequency (TF), aka. the number of times a word appears in a document, divided by the total number of words in that document; the second term is the Inverse Document Frequency (IDF), computed as the logarithm of the number of the documents in the corpus divided by the number of documents where the specific term appears.\n\nTF(t) = (Number of times term t appears in a document) / (Total number of terms in the document).\n\nIDF(t) = log_e(Total number of documents / Number of documents with term t in it)."},{"metadata":{"_cell_guid":"b134a10d-30ff-403c-82cc-2110eb268349","collapsed":true,"_uuid":"8d5b967414890a09d788184e99420892637d13b8","trusted":false},"cell_type":"code","source":"# extract the tfid representation matrix of the text data\ntfid_matrix = tfid_vectorizer.transform(data['text'])\n# collect the tfid matrix in numpy array\narray = tfid_matrix.todense()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bb6912d0cc5a02db1f2ba769fd912ce4e31bad4d","trusted":false},"cell_type":"code","source":"# store the tf-idf array into pandas dataframe\ndf = pd.DataFrame(array)\ndf.head(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"183649f44098456d2b852eecb7875c2909ee2476"},"cell_type":"markdown","source":"# **Training Model**"},{"metadata":{"_uuid":"ba4a60172ecee54a8fe7922b896fcb5d159a569a"},"cell_type":"markdown","source":"We are going to train Naive Bayes Classifier. Naive Bayes Classifier is a good choice given we have a medium sized dataset, NB classifier scales well and also NB classifier has been historically used in NLP tasks. We will train Multinomial and Bernoulli NB classifier, since they almost always outperfrom Gaussian NB classifier in NLP tasks"},{"metadata":{"_uuid":"ca9b288109dd09138bbddacaeae592e2da9dcc04"},"cell_type":"markdown","source":"Adding the output to the dataframe"},{"metadata":{"_cell_guid":"34d1ef75-a99f-40e7-8c14-3e858eca498f","_uuid":"952960b4e73a3ca32d70b2ac83988534e1a101e5","trusted":false},"cell_type":"code","source":"df['output'] = data['author']\ndf['id'] = data['id']\ndf.head(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3d5d61fe51a3b2ce7b5c3ad75fe03159f1318efe"},"cell_type":"markdown","source":"Features and output of the models"},{"metadata":{"_cell_guid":"6bfb45f6-ecc1-4b24-a24a-3a821a69d5d4","collapsed":true,"_uuid":"99eae22c3d85cf743eebbc6d8e245579570436d0","trusted":false},"cell_type":"code","source":"features = df.columns.tolist()\noutput = 'output'\n# removing the output and the id from features\nfeatures.remove(output)\nfeatures.remove('id')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ba07b66f6e32cabbfbd004fe890227b462041474"},"cell_type":"markdown","source":"**Import neccassary sklearn modules**"},{"metadata":{"_cell_guid":"2553c2ee-0db9-49c0-b3a7-18ce78d70878","collapsed":true,"_uuid":"7d42005f434806b6565eec6b57841c1890336e22","trusted":false},"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score, log_loss\nfrom sklearn.model_selection import GridSearchCV","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"829631ba0779a90e9cdb4d431721ef05e345b209"},"cell_type":"markdown","source":"##  **Tuning Multinomial Naive Bayes Classifier**"},{"metadata":{"_uuid":"b921e046f04792e0bb916c85253263b7135f7c10"},"cell_type":"markdown","source":"List of alpha parameter we are going to try"},{"metadata":{"_cell_guid":"488ad806-0a3b-4795-9730-d95b88c1a227","_uuid":"1ee5208dcd4f3d6fba2aede322d2f562668b0ece","trusted":false},"cell_type":"code","source":"alpha_list1 = np.linspace(0.006, 0.1, 20)\nalpha_list1 = np.around(alpha_list1, decimals=4)\nalpha_list1","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d0b090c8ccbd3086ba36a09b99fc65da89a4e7b1"},"cell_type":"markdown","source":"GridSearchCV allows us tune parameters of a model through k-fold cross validataion using parameter grid in one go"},{"metadata":{"_uuid":"ccd9d21330b1196347457d63e076f216a735b8f4"},"cell_type":"markdown","source":"**Gridsearch**"},{"metadata":{"_cell_guid":"e729a93d-4494-4b60-a7d8-341295213a77","collapsed":true,"_uuid":"949d7d0b08716bd0810daee1a7149da32dbb7472","trusted":false},"cell_type":"code","source":"# parameter grid\nparameter_grid = [{\"alpha\":alpha_list1}]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3b100f21-ca87-4719-9787-4ee72f3710a2","_uuid":"8ef2b8fd6e2e6ff1251f4cb4dcbe649a0eea8292","trusted":false},"cell_type":"code","source":"# classifier object\nclassifier1 = MultinomialNB()\n# gridsearch object using 4 fold cross validation and neg_log_loss as scoring paramter\ngridsearch1 = GridSearchCV(classifier1,parameter_grid, scoring = 'neg_log_loss', cv = 4)\n# fit the gridsearch\ngridsearch1.fit(df[features], df[output])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"217600cc34d154a7cfb5d7582ed28cc6d60bb45d"},"cell_type":"markdown","source":"Collect results in pandas dataframe"},{"metadata":{"_cell_guid":"abc35582-e198-4a8b-b88c-ee89b1280728","collapsed":true,"_uuid":"9f3745b3a268741db7d377c04744ce959f128c5d","trusted":false},"cell_type":"code","source":"results1 = pd.DataFrame()\n# collect alpha list\nresults1['alpha'] = gridsearch1.cv_results_['param_alpha'].data\n# collect test scores\nresults1['neglogloss'] = gridsearch1.cv_results_['mean_test_score'].data","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5f6414ec9116dd15df464a4d61612abc2429056a"},"cell_type":"markdown","source":"**Plot of logloss vs alpha**"},{"metadata":{"_cell_guid":"c0e77516-5171-4cc5-a0dc-886e2996bc85","_uuid":"c5372a147d30a393c8b719f4876802f1bff26925","trusted":false},"cell_type":"code","source":"matplotlib.rcParams['figure.figsize'] = (12.0, 6.0)\nplt.plot(results1['alpha'], -results1['neglogloss'])\nplt.xlabel('alpha')\nplt.ylabel('logloss')\nplt.grid()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"26e88445-4f22-42a7-8b5b-49ba2921ac69","_uuid":"264b848a45aefcb16a9ce657d72450848a44590f","trusted":false},"cell_type":"code","source":"print(\"Best parameter: \",gridsearch1.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"83341408-5fab-436d-9585-527a72a54845","_uuid":"f84af251aa6c0373c49d5190af9fa0c4e445d678","trusted":false},"cell_type":"code","source":"print(\"Best score: \",gridsearch1.best_score_) ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9d3e9027f4d824c86396e4fe55cf8b7433254678"},"cell_type":"markdown","source":"##  **Tuning Multinomial Naive Bayes Classifier**"},{"metadata":{"_uuid":"e47b455b6a72f7e5b054ebea14fb2d4e2889f700"},"cell_type":"markdown","source":"List of alpha parameter we are going to try"},{"metadata":{"_cell_guid":"529aba58-ac0e-4c7c-ab5a-3f66eb124b18","_uuid":"dccf4bbe71a557881e699d247fc5e76387161366","trusted":false},"cell_type":"code","source":"alpha_list2 = np.linspace(0.006, 0.1, 20)\nalpha_list2 = np.around(alpha_list2, decimals=4)\nalpha_list2","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dbfd677cf894e7330b98c75a248f7473a01d0d29"},"cell_type":"markdown","source":"Parameter grid"},{"metadata":{"_cell_guid":"ed43f361-7531-418d-af69-66e807832d62","collapsed":true,"_uuid":"71a171b9ffd633b960073f485ce2f8d2ba2cee50","trusted":false},"cell_type":"code","source":"parameter_grid = [{\"alpha\":alpha_list2}]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0487f76ecf6b04dafa9a7c869b39bb410c735cf2"},"cell_type":"markdown","source":"**Gridsearch**"},{"metadata":{"_cell_guid":"d2dc6842-6d25-471c-be42-05c34794338d","_uuid":"da8f4c067e231fa4798b29bd88a9ad1a2e4d5490","trusted":false},"cell_type":"code","source":"# classifier object\nclassifier2 = MultinomialNB()\n# gridsearch object using 4 fold cross validation and neg_log_loss as scoring paramter\ngridsearch2 = GridSearchCV(classifier2,parameter_grid, scoring = 'neg_log_loss', cv = 4)\n# fit the gridsearch\ngridsearch2.fit(df[features], df[output])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b2e155b3df491f9773b919421f1780980d380b22"},"cell_type":"markdown","source":"Collect results in pandas dataframe"},{"metadata":{"_cell_guid":"5431f17a-f1b6-4c74-9bfe-19e5b63b9951","collapsed":true,"_uuid":"8ad1f72bf5c38287d1087e92c7f39ba062756c36","trusted":false},"cell_type":"code","source":"results2 = pd.DataFrame()\n# collect alpha list\nresults2['alpha'] = gridsearch2.cv_results_['param_alpha'].data\n# collect test scores\nresults2['neglogloss'] = gridsearch2.cv_results_['mean_test_score'].data","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c0126d07b5a362a6fc5adc41f99f6937c3fc4941"},"cell_type":"markdown","source":"**Plot of logloss vs alpha**"},{"metadata":{"_cell_guid":"5b6fc67c-8d44-4da7-9cdf-79803fed57dd","_uuid":"fe953689dde08dd6da718827ffa872e392c7c0be","trusted":false},"cell_type":"code","source":"matplotlib.rcParams['figure.figsize'] = (12.0, 6.0)\nplt.plot(results2['alpha'], -results2['neglogloss'])\nplt.xlabel('alpha')\nplt.ylabel('logloss')\nplt.grid()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f0d11d37-c5e1-4ae3-9156-4345fd586f21","_uuid":"f713d859794149482dea12361ac31cf0e6cf59c1","trusted":false},"cell_type":"code","source":"print(\"Best parameter: \",gridsearch2.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"470471d3-be58-4997-b6b6-5d827b7bc80f","_uuid":"8002c07ddd455a3193566646244904eaba9e73c3","trusted":false},"cell_type":"code","source":"print(\"Best score: \",gridsearch2.best_score_)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python","file_extension":".py","nbconvert_exporter":"python","version":"3.6.3","pygments_lexer":"ipython3"}},"nbformat":4,"nbformat_minor":1}