{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nfrom bs4 import BeautifulSoup\nimport re\nimport random\nfrom sklearn.model_selection import train_test_split\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning, module='bs4')\nlemmatizer = WordNetLemmatizer()\n\n#set random seed for the session\nrandom.seed(1)\nnp.random.seed(1)\n\n#load train data\ntrain=pd.read_csv(\"../input/train.tsv\",sep='\\t')\nprint(train.shape)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0ae954991eb0e6ba9e8e545fd55c589c286917ba"},"cell_type":"code","source":"#load test data\ntest=pd.read_csv(\"../input/test.tsv\",sep='\\t')\nprint(test.shape)\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"49c1720be3ab5ab7b9dbecf1ae303062b364dbe8"},"cell_type":"code","source":"#method to clean the reviews, tokenize, remove stop words and lemmatize them.\ndef clean_sentences(df):\n    reviews = []\n\n    for sent in tqdm(df['Phrase']):\n        \n        #remove html content\n        review_text = BeautifulSoup(sent).get_text()\n        \n        #remove non-alphabetic characters\n        review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n    \n        #tokenize the sentences\n        words = word_tokenize(review_text.lower())\n    \n        #stop words removal\n        omit_words = set(stopwords.words('english'))\n        words = [x for x in words if x not in omit_words]\n        \n        #lemmatize each word to its lemma\n        lemma_words = [lemmatizer.lemmatize(i) for i in words]\n    \n        reviews.append(lemma_words)\n\n    return(reviews)\n\n#cleaned reviews for both train and test set retrieved\ntrain_sentences = clean_sentences(train)\ntest_sentences = clean_sentences(test)\nprint(len(train_sentences))\nprint(len(test_sentences))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"#collect the dependent values\ntarget=train.Sentiment.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8e4a72a52b8266a92d120a70fe9b7188ebc538b1"},"cell_type":"code","source":"# Set values for various parameters of word2vec\nnum_features = 250  # Word vector dimensionality. Determines the no of words each word in the vocabulary will\n#be associated with. Must be tuned.\nmin_word_count = 50   # Minimum word count. Wods occuring below the threshold will be ignored\nnum_workers = 1       # Number of threads to run in parallel\ncontext = 15          # Context window size to be considered for each word                                             \ndownsampling = 1e-3   # Downsample setting for frequent words. To prevent more frequent words from dominating.\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8a902be667b9257e3df5935e02fd90d080ed2331"},"cell_type":"code","source":"from gensim.models import word2vec\n\n#Model Word2Vec\nmodel = word2vec.Word2Vec(train_sentences, workers=num_workers, \\\n            size=num_features, min_count = min_word_count, \\\n            window = context, sample = downsampling)\n\n# If the model is not going to be trained further, init_sims can be called \n# init_sims will make the model much more memory-efficient.\nmodel.init_sims(replace=True)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"_kg_hide-output":false,"trusted":true,"_uuid":"5c72422dc81fadb64d71a0bcbe63aa06bb8869d4"},"cell_type":"code","source":"#the word2vec builds associations for each word with other words in the vocabulary of the problem.\n#the model can be used to test differnt word associations. similarities, etc. to know how good the model\n#or the vocabulary is. In the example here, 4 words are given and the model chosses the dissimilar word amonsgt them.\n#here it chooses length. Good Bad & Nice are adjectives to describe a movie, 'Length' is something.\n#Hence the model has built this association.\nprint(model.wv.doesnt_match(\"length good bad nice\".split()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c8920fb20feb2938df92bbc045b4f04176d1cf59"},"cell_type":"code","source":"# test the same condition with totally differnt word. The output is correctly 'red'.\nprint(model.wv.doesnt_match(\"fun good bad red\".split()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d8b0abdcf8cbe479edf091dfaf804de96b9e648a"},"cell_type":"code","source":"#print the three most common words\nprint(model.wv.index2word[0], model.wv.index2word[1], model.wv.index2word[2])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4683c09c02884b39b8208dfe1c1ce85c81f89335"},"cell_type":"code","source":"#print the three least common words\ntotal_words = len(model.wv.vocab)\nprint(model.wv.index2word[total_words - 1], model.wv.index2word[total_words - 2], model.wv.index2word[total_words - 3])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"26b0b56855128d590f932695419d20fd29bc05b5"},"cell_type":"code","source":"#print the most similar words associated with 'actor'. The word 'role' and 'lead' appears at the top.\nprint(model.wv.most_similar(\"actor\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6455143ad8c9d04220ead0db36c602c4ff4075cc"},"cell_type":"code","source":"def createFeatureVector(words, model, num_features):\n    #initialize a 1D array with length as num of features of word2vec model chosen by us. \n    #Here it is 250.\n    featVector = np.zeros((num_features,),dtype=\"float32\")\n    \n    nWords = 0\n    \n    # Index2word is a list that contains the names of the words in \n    # the model's vocabulary. Convert it to a set, as set is faster\n    index2word_set = set(model.wv.index2word)\n    \n    # Loop over each word and add it to the feature vector to get the total sum of feature vectors of the\n    #entire review\n    for word in words:\n        if word in index2word_set: \n            nWords = nWords + 1.\n            featVector = np.add(featVector,model[word])\n            \n    # Divide the result by the number of words to get the average of the feature vectors of \n    #all words in the review\n    if(nWords != 0):\n        featVector = np.divide(featVector,nWords)\n    return featVector\n\n#calculates the average of the feature vectors for each review using the word2vec values assigned for \n#each word\ndef avgFeatureVectors(sentences, model, num_features):\n    overallFeatureVectors = []\n    for sentence in tqdm(sentences):\n        overallFeatureVectors.append(createFeatureVector(sentence, model, num_features)) \n    return overallFeatureVectors\n\ntrain_vect = avgFeatureVectors( train_sentences, model, num_features )\ntest_vect = avgFeatureVectors( test_sentences, model, num_features )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"860b3ebb1714acc7f02e2c581b493f76b4b83d7a"},"cell_type":"code","source":"#split into train and validation sets.\nX_train,X_val,y_train,y_val=train_test_split(train_vect,target,test_size=0.2,stratify=target)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6a10ce482cf7762779733852555f01fd35d48315"},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\n#Model RF\nmodel_rf = RandomForestClassifier(random_state=1, n_estimators=500, verbose=1, n_jobs=-1, oob_score=True)\nmodel_rf.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"79aef9e65235fefd20cbcb1483d95bdec24dfa74"},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\n\n#the score for the trained model. Here it is around 62%. It can be further increased, by fine-tuning\n#word2vec model as well as rf model. The no of estimators for example can be increased to boost the accuracy.\n#The score can definitely be improved further by fine tuning and possibly grid search.\nresult = model_rf.predict( X_val )\nscore = accuracy_score(y_val,result)\nprint(score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"699191fa46ba588a634560d505c6e92fc8c59f6e"},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\n\n#visualizing the confusion matrix of the results. As expected the category of neutral ('2') is the best\n# performing as more no of records in the dataset belong to this class. Other classes are little\n#under-represented and hence not learnt well by the model. Over-sampling, Under-sampling or combination \n#of both can mitigate this problem to a degree. Also SMOTE can be tried to create synthetic samples for\n#under-represented classes to balance the equation little bit. Class weights is also another option\n# that can be used to tell the model to pay more attention to certain classes while training.\n\nlabels = [0,1,2,3,4]\nconf = confusion_matrix(y_val,result,labels)\nfig = plt.figure()\nax = fig.add_subplot(111)\ncax = ax.matshow(conf)\nplt.title('Confusion matrix of the classifier')\nfig.colorbar(cax)\nax.set_xticklabels([''] + labels)\nax.set_yticklabels([''] + labels)\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.show()\nprint(conf)\n\n#here 0 - negative, 1- somewhat negative, 2- neutral, 3- somewhat positive, 4- positive","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"601f53c35da7abce62c21e90a0c965f21fcba3be"},"cell_type":"code","source":"#predict test classes and submit\ny_pred=model_rf.predict(test_vect)\n\nsub_file = pd.read_csv('../input/sampleSubmission.csv',sep=',')\nsub_file.Sentiment=y_pred\nsub_file.to_csv('Submission.csv',index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}