{"cells":[{"metadata":{"_uuid":"282de4e4a4be38327e0ebeddfb51571345ca2611"},"cell_type":"markdown","source":"<h1>Baseline model with Decision Trees</h1>\n\nThis is a baseline model to classify the cover_type dataset using decision trees . In many cases machine learning algorithms don't perform well without feature engineering which is the process of filling NaNs and missing values , creating new features and etc... . I will be performing some exploratory data analysis to perform feature engineering before implementing the suitable model."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os\nprint(os.listdir(\"../input\"))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d5c9260341f59916e7b13e946a55deb10002e89a"},"cell_type":"markdown","source":"Now we should load the train and test data into two seperate dataframes"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"../input/train.csv\")\ntest = pd.read_csv(\"../input/test.csv\")\n\n# The following two lines determines the number of visible columns and \n#the number of visible rows for dataframes and that doesn't affect the code\npd.set_option('display.max_columns', 500)\npd.set_option('display.max_rows', 500)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9779f47110b06f8b4d074dc92a676a4bc011f9ed"},"cell_type":"markdown","source":"<h1>**Data Exploration and Analysis**</h1>"},{"metadata":{"_uuid":"f94ac150f8a77111372bdfca111a8d77eb31751c"},"cell_type":"markdown","source":"Now we should go further to explore our data to be able to know which features to use and if we can synthesize new features. Now i will show the first 5 rows. "},{"metadata":{"trusted":true,"_uuid":"16fa1c06f2910a63aa6369ba75eede063b5c3d99"},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1bb2dc24ddd365f71718ab70a0e24c9514ffc0f3"},"cell_type":"markdown","source":"Let's now see how many data points we have for training."},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"a9f35db9e241fbaffb6adfdf0b019a007d57f9bb"},"cell_type":"code","source":"print(\"The number of traning examples(data points) = %i \" % train.shape[0])\nprint(\"The number of features we have = %i \" % train.shape[1])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1faada8ab307447da4bf8c75797d5c0b6c0a18b0"},"cell_type":"markdown","source":"Let's check if any of the columns contains NaNs or Nulls so that we can fill those values if they are insignificant or drop them. We may drop a whole column if most of its values are NaNs or fill its value according to its relation with other columns in the dataframe. Nones can also be 0 in some datasets and that is why i am going to use the describe of the train to see if the range of numbers is not reasonable or not. if you are dropping rows with NaNs and you notice that you need to drop a large portion of your dataset then you should think about filling the NaN values or drop a column that has most of its values missing."},{"metadata":{"trusted":true,"_uuid":"333ab6d0ba59277d4bc5b46d6af9752286d2edf5"},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7b4d2f7edb2d280e34bb2c6f479554f4653d6a4c"},"cell_type":"code","source":"train.drop(['Id'], axis = 1, inplace = True)\ntrain.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"69edb39bdca020f68c4a9b068dadd4862c126284"},"cell_type":"markdown","source":"It seems we don't have any NaN or Null value among the dataset we are trying to classify. Let's now discover the correlation matrix for this dataset and see if we can combine features or drop some according to its correlation with the output labels."},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"370d9012a10f74a9587b60e18c0b338ff0c9301e"},"cell_type":"code","source":"import seaborn as sns\n\n\nimport matplotlib.pyplot as plt\n\n\ncorr = train.corr()\nf, ax = plt.subplots(figsize=(25, 25))\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\nsns.heatmap(corr, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cc04710ccdaa02440eb2872217be9d3cf9189144","scrolled":true},"cell_type":"code","source":"corr","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"669f24bc28e6eda1feaa714b51daabfe598b0aff"},"cell_type":"markdown","source":"Let's now explore some relations between features that we can add later to make the algorithm perform better."},{"metadata":{"trusted":true,"_uuid":"30502b1a4f4759c3e09680ae26eade57073aa6b5"},"cell_type":"code","source":"import matplotlib.pyplot as plt\nclasses = np.array(list(train.Cover_Type.values))\n\ndef plotRelation(first_feature, sec_feature):\n    \n    plt.scatter(first_feature, sec_feature, c = classes, s=10)\n    plt.xlabel(first_feature.name)\n    plt.ylabel(sec_feature.name)\n    \nf = plt.figure(figsize=(25,20))\nf.add_subplot(331)\nplotRelation(train.Horizontal_Distance_To_Hydrology, train.Horizontal_Distance_To_Fire_Points)\nf.add_subplot(332)\nplotRelation(train.Horizontal_Distance_To_Hydrology, train.Horizontal_Distance_To_Roadways)\nf.add_subplot(333)\nplotRelation(train.Elevation, train.Vertical_Distance_To_Hydrology)\nf.add_subplot(334)\nplotRelation(train.Hillshade_9am, train.Hillshade_3pm)\nf.add_subplot(335)\nplotRelation(train.Horizontal_Distance_To_Fire_Points, train.Horizontal_Distance_To_Hydrology)\nf.add_subplot(336)\nplotRelation(train.Horizontal_Distance_To_Hydrology, train.Vertical_Distance_To_Hydrology)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ec47eafa0174498d963616ce2a9d4502377151c2"},"cell_type":"markdown","source":"As you can see there are some important relations that the model can infere from these new features according to the plots and also the correlation matrix and the heatmap. I will now add these features to the training data and the test data. I have read many resources as this [study](https://rstudio-pubs-static.s3.amazonaws.com/160297_f7bcb8d140b74bd19b758eb328344908.html), this grat [course](https://www.coursera.org/learn/competitive-data-science) and from that great [kernel](https://www.kaggle.com/codename007/forest-cover-type-eda-baseline-model).\n\nAlso it seems that the vertical distance contain some negative number and it gave me better performance when taken the absolute for the column. It is really important to notice that Tree based models only fits vertical and horizontal lines so it is very important to engineer some oblique or tilted features like slope and etc... ."},{"metadata":{"_uuid":"278a7807c8638cc6f6774d5666977377c768cba4"},"cell_type":"markdown","source":"Now we should seperate the training set from the labels and name them x and y then we will split them into training and test sets to be able to see how well it would do on unseen data which will give anestimate on how well it will do when testing on Kaggle test data. I will use the convention of using 80% of the data as training set and 20% for the test set."},{"metadata":{"trusted":true,"_uuid":"f9ee27e349f745f85c32132cb9dbc047eff9e0ba"},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"8284a1b7b2b3ab724e256ca2797c7d210da431e6"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx = train.drop(['Cover_Type'], axis = 1)\ny = train['Cover_Type']\nprint( y.head() )\n\nx_train, x_test, y_train, y_test = train_test_split( x.values, y.values, test_size=0.25, random_state=42 )","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b6e180b0511b577a844cd93cc135e9d11d945a4c"},"cell_type":"markdown","source":"It is important to know if the number of points in the classes are balanced. If the data is skewed then we will not be able to use accuracy as a performance metric since it will be misleading but if it is skewed we may use F-beta score or precision and recall.  Precision or recall or F1 score. the choice depends on the problem itself. Where high recall means low number of false negatives , High precision means low number of false positives and     F1 score is a trade off between them. You can refere to this article for more about precision and recall http://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html"},{"metadata":{"trusted":true,"_uuid":"b851644f8c0ac6df69a35daade502864c1a33120"},"cell_type":"code","source":"unique, count= np.unique(y_train, return_counts=True)\nprint(\"The number of occurances of each class in the dataset = %s \" % dict (zip(unique, count) ), \"\\n\" )","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9e1c61793c3f011867abd6b0251c76bd3d26cda6"},"cell_type":"markdown","source":"It seems the data points in each class are almost balanced so it will be okay to use accuracy as a metric to measure how well the ML model performs"},{"metadata":{"_uuid":"17f1aee2236f13ec6c51358a18558d8084e725df"},"cell_type":"markdown","source":"Since we have only 15120 training examples then I have tried **SVMs** but it didn't give me great performance so i tried **Ensemble learning using Extra trees** instead and it gave me much better results than the SVM algorithm . If you don't know which estimator or algorithm to use you can check the Scikit Learn Cheat sheet below.\n![](http://scikit-learn.org/stable/_static/ml_map.png)"},{"metadata":{"_uuid":"c6d641d997b1dfc8d27e383d8fb3b4e0d35534ad"},"cell_type":"markdown","source":"When using ExtraTrees or even any machine learning algorithm it is very important to remember to perform feature scaling to make the model converge faster. Also if you plan to use SVM classifier it would perform better with compression techniques like [PCA](https://www.coursera.org/lecture/machine-learning/principal-component-analysis-algorithm-ZYIPa) ."},{"metadata":{"trusted":true,"_uuid":"31ea6f953b6037edce4e2362d67178da4bb217fe"},"cell_type":"code","source":"from sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn import decomposition\n\nscaler = StandardScaler()\nscaler.fit(x_train)\nx_train = scaler.transform(x_train)\nx_test = scaler.transform(x_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9d38421ba7af29bb5533d9a05f23f98e0356037f"},"cell_type":"markdown","source":"Now it is time to fit the decision classifier algorithm and for that we will use Scikit learn DecisionTreeClassifier"},{"metadata":{"trusted":true,"_uuid":"4de9c79f0e1fbb67b537481643263696060f2691"},"cell_type":"code","source":"train.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"76b0e7c0ac88f1cf8bdeee53324556f34d175f1c"},"cell_type":"code","source":"###### from sklearn.svm import SVC\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import classification_report\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n#uncomment the commented code and uncomment the commented to perform gridsearchCV\nfrom xgboost import XGBClassifier\n\nclf = DecisionTreeClassifier(random_state=0)\n\nclf.fit(x_train, y_train)\nprint('Accuracy of classifier on training set: {:.2f}'.format(clf.score(x_train, y_train) * 100))\nprint('Accuracy of classifier on test set: {:.2f}'.format(clf.score(x_test, y_test) * 100))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"158e22b0436e06245743926705b896ee4c88ded5"},"cell_type":"markdown","source":"The last thing to do now is to predict Kaggle test set to get the results and submit the result csv file."},{"metadata":{"trusted":true,"_uuid":"3463f1c44c3745492e8d0f278ac99c5d04bb6690","scrolled":true},"cell_type":"code","source":"test.head()\n\nid = test['Id']\ntest.drop(['Id'] , inplace = True , axis = 1)\n\ntest = scaler.transform(test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"029a8a08c3776df62dd35ba04f7f05da93f68f8e"},"cell_type":"markdown","source":"And now let's see the predictions using the predict function in sklearn"},{"metadata":{"trusted":true,"_uuid":"cebbeb049f98d1719addc1d7a7777d887cc41106"},"cell_type":"code","source":"#Uncomment the commented code and comment the other line to run the grid search predict\n\n# predictions = grid.best_estimator_.predict(test)\npredictions = clf.predict(test)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5fb3f0f195217cd6b9c8c77452fc044edbddaf6f"},"cell_type":"markdown","source":"Finally we should output the predictions in the format they want in the competition."},{"metadata":{"trusted":true,"_uuid":"afe93e3cd020bc43d3df80e5be4b621781117fda"},"cell_type":"code","source":"out = pd.DataFrame()\nout['Id'] = id\nout['Cover_Type'] = predictions\nout.to_csv('my_submission.csv', index=False)\nout.head(5)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}