{"cells":[{"metadata":{"_uuid":"dd3bb740d0ae00a90f56413f2852af2996eed765"},"cell_type":"markdown","source":"This notebook serves as a guideline to loading in large datasets with Pandas. For a detailed explanation, please refer to this excellent link, https://www.dataquest.io/blog/pandas-big-data/. I follow its content closely and adapt my code by writing functions and classes for future use. Let's begin by importing some useful modules"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # plotting\n%matplotlib inline\nimport seaborn as sns # advanced visualization\nimport warnings # control warnings\nwarnings.filterwarnings('ignore')\nimport pprint # prettyprint module\nimport time # measure how much time to run some code\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dd3f1dc80898f351a381abf6a8bae717aa95791f"},"cell_type":"markdown","source":"# Helper functions"},{"metadata":{"trusted":true,"_uuid":"19bd7c968042a926db4f1aa24ac46153805fb4e2"},"cell_type":"code","source":"def mem_usage(pds_obj):\n    '''\n    MEM_USAGE calculates how much memory used by loading in an object\n    \n    Inputs:\n    - pds_obj: pandas dataframe or series\n    \n    Outputs:\n    - a string showing how much memory already occupied by pds_obj\n    '''\n    if isinstance(pds_obj, pd.DataFrame):\n        usage = pds_obj.memory_usage(deep=True).sum() # sum all memory usages in each column if a dataframe\n    else: # a Series\n        usage = pds_obj.memory_usage(deep=True)\n    # Convert usage from bytes to megabytes\n    usage = usage / 1024 ** 2 # traditional conversion, not mathematical conversion\n    return \"{:.2f} MB\".format(usage)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1b1fc2ef2a4b236d7f00934a4d338442f0ac1866"},"cell_type":"markdown","source":"# Example with a Toy Dataset\nThe main idea for memory efficiency is to specify the data types we want pandas to parse when reading in the csv files via the argument `dtype` of the `pd.read_csv` method. To this end, we'll play with a toy dataset, figure out the best data types for memory reduction, save the specifications in a dictionary, and finally pass this dictionary to the argument `dtype` while reading the main csv file."},{"metadata":{"trusted":true,"_uuid":"aba32185c10704a02d2aa550391b3fd0f376c4d8"},"cell_type":"code","source":"# Use the first 1m rows as toy data\nstart_time = time.time() # start timer\ntoy_data = pd.read_csv('../input/train.csv', nrows=1e6) # read in the first 1m rows\nend_time = time.time() # end timer\nprint('Run time: %.2f seconds' %(end_time - start_time)) # total run time","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"207e10824b301b0cfb55f986162dcff78bc26fcc"},"cell_type":"markdown","source":"Some readers may wonder why we just read in 1m rows but not some other numbers. This question will be answered at the end of the notebook."},{"metadata":{"trusted":true,"_uuid":"3529ba6abd73247afe935f022ba1514176c8ffcf"},"cell_type":"code","source":"# Memory usage for just one chunk\nprint('Memory usage for just one chunk:', mem_usage(toy_data))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"36860a287e341bb2bf4b024adada22cc9d3179bd"},"cell_type":"markdown","source":"Thus, the total memory usage for our training data is roughly `2238.98 MB * 9 = 20150.82 MB ~ 20 GB`. We definitely need a way to load the data in more efficiently. This can be done by downcasting `float` and `int` and converting `object` to `category`. Again, for a more detailed explanation, please refer to the link at the beginning of this notebook.\n\nIndeed, most of the time `object` takes up much more memory comparing to the other two and thus, converting it to other type for memory efficient is essential. The following demonstrates this point,"},{"metadata":{"trusted":true,"_uuid":"7e85ad802674827b4ed9d90152134b6d871b779d"},"cell_type":"code","source":"# Total memory usage by each data type in toy_data\ndata_types = ['int', 'float', 'object']\nfor dtype in data_types:\n    selected_data = toy_data.select_dtypes(include=[dtype])\n    print(dtype, 'takes up', mem_usage(selected_data), 'in memory space')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"828536dd6f786217567e7cfbb6f305d45f1ffeba"},"cell_type":"markdown","source":"Downcasting data types is the key to memory reduction in pandas. Let's start with `int`."},{"metadata":{"_uuid":"a1424a5a5900de31c343dd97cfcb86e9ba762b0a"},"cell_type":"markdown","source":"# Downcasting \"int\" data types\n`int` can be downcasted to `unsigned` so save some memory. Also, we won't have to worry about negative entries as Pandas will reserve the original data type if it fails to convert."},{"metadata":{"trusted":true,"_uuid":"ce2ce453bfffab7dc981315585b328753cb71aa5"},"cell_type":"code","source":"# Downcast int to unsigned\ntoy_data_int = toy_data.select_dtypes(include=['int'])\nconverted_int = toy_data_int.apply(pd.to_numeric, downcast='unsigned')\n\n# Effect of downcasting on int\ncompared_int = pd.concat([toy_data_int.dtypes, converted_int.dtypes], axis=1)\ncompared_int.columns = ['before', 'after']\ncompared_int.apply(pd.Series.value_counts)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f302cd5b3a3fcf488d3a0e17bf1ec9977518e5c1"},"cell_type":"markdown","source":"Observe that all `int64` data, where each datum takes up 64 bytes in the memory space, have been converted to `uint8` and `uint16` data, where each datum now takes up only 8 or 16 bytes in the memory space, respectively. The following result shows that we have made some progress regarding memory efficiency, though not significantly since we don't have that many `int64` columns (!!!),"},{"metadata":{"trusted":true,"_uuid":"e1d722216d477f9a91495b41b864e6d1edb2fc8a"},"cell_type":"code","source":"# Compare memory reduction between toy_data_int and converted_int\n# Note that we only consider `int` data type. So although the result\n# may indicate a significance reduction, it is insignificant overall\n# since most of our memory space is taken up by (1) `object`  and\n# (2) `float` data types\nprint('toy_data_int takes up', mem_usage(toy_data_int))\nprint('converted_int takes up', mem_usage(converted_int))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"786b8975f646609e5a7fea4e92de6015fa1498c4"},"cell_type":"code","source":"# Finally, replace `int` columns in toy_data with a more efficient `int` columns from toy_data_int\noptimized_toy_data = toy_data.copy()\noptimized_toy_data[converted_int.columns] = converted_int","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e6100f0ef240b7b05f1db60854ec4509862fef9d"},"cell_type":"markdown","source":"# Downcasting \"float\" data types\nWhen loading in data, Pandas will treat continuous variables as `float64` data type (please help me double check if this is correct!!!) Pandas can attempt to downcast them to as small as `np.float32` type, if it's appropriate for it to do so."},{"metadata":{"trusted":true,"_uuid":"f6245f2501f6f64a0cdba2bcb78af3dcafda9581"},"cell_type":"code","source":"# Downcast float64 to just float\ntoy_data_float = toy_data.select_dtypes(include=['float'])\nconverted_float = toy_data_float.apply(pd.to_numeric, downcast='float')\n\n# Effect of downcasting to just float\ncompared_float = pd.concat([toy_data_float.dtypes, converted_float.dtypes], axis=1)\ncompared_float.columns = ['before', 'after']\ncompared_float.apply(pd.Series.value_counts)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ced239482992bc44859a47fe0b565263c5bdf672"},"cell_type":"code","source":"# Compare memory reduction between toy_data_float and converted_float.\n# Similar note as in downcasting `int64` to 'unsigned'\nprint('toy_data_float takes up', mem_usage(toy_data_float))\nprint('converted_float takes up', mem_usage(converted_float))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fc8879ec0ff7a8cb4cc023822e6cf8f28646e133"},"cell_type":"code","source":"# Replace `float64` columns of toy_data_float by new data types\noptimized_toy_data[converted_float.columns] = converted_float","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"95969cac59bc3729ac6e5fc2f6b6a28ebdb6ad7e"},"cell_type":"markdown","source":"Thus, so far we have achieved roughly `(1 - 1992.94 / 2238.98) * 100 ~ 11%` of memory reduction,"},{"metadata":{"trusted":true,"_uuid":"a4f355b854f0fcf09bbd0ad2c891ca1a55d873fe"},"cell_type":"code","source":"# Total memory reduction so far\nprint(\"toy_data takes up\", mem_usage(toy_data), 'in memory space')\nprint(\"optimized_toy_data takes up\", mem_usage(optimized_toy_data), 'in memory space')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fb0a750723798b6aacdb78d0e5db9d73f8eef6a7"},"cell_type":"markdown","source":"# Downcasting \"object\" data types\nFinally, we can work on converting `object` data type into `category`. Due to some restrictions with `category` type (see link at the beginning of this notebook), we will only convert an `object` column into `category` if there are at most 50% of unique values within that column. The downcasting process will be slighly different comparing to that of `int` and `float` since we will need to use the astype method."},{"metadata":{"trusted":true,"_uuid":"ed200224bae77f8337a85894c09465f66828e933"},"cell_type":"code","source":"# Isolate `object` data type\ntoy_data_obj = toy_data.select_dtypes(include=['object'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0da5eb9483aa595be99ab4e95b7ce313287662a1"},"cell_type":"code","source":"# Downcast `object` to `category` if column has at most 50% of unique values\nconverted_obj = pd.DataFrame()\n\nfor col in toy_data_obj.columns:\n    num_unique_values = len(toy_data_obj[col].unique())\n    total_num_values = len(toy_data_obj[col])\n    # Only convert to `category` type if column has at most 50% of unique values\n    if num_unique_values / total_num_values < 0.5:\n        converted_obj.loc[:, col] = toy_data_obj[col].astype('category')\n    else:\n        converted_obj.loc[:, col] = toy_data_obj[col]\n        \n# Effect of downcasting `object` to `category`\ncompared_obj = pd.concat([toy_data_obj.dtypes, converted_obj.dtypes], axis=1)\ncompared_obj.columns = ['before', 'after']\ncompared_obj.apply(pd.Series.value_counts)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ad80424b3c6f582002e2a388c6e4efa23e38096d"},"cell_type":"code","source":"# Compare memory reduction between toy_data_obj and converted_obj.\nprint('toy_data_obj takes up', mem_usage(toy_data_obj))\nprint('converted_obj takes up', mem_usage(converted_obj))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"21d2b5697d6b0d08ae22179fa3e2d5f030e15f6c"},"cell_type":"markdown","source":"Whoops! We have achieved an amzing reduction results. Let's look at the first few roes of `converted_obj`. Note that the entries in these columns appear exactly the same as before conversion, but internally, they have been optimized via encoding."},{"metadata":{"trusted":true,"_uuid":"14b649d05d49ad8749de55c77b08f901b9024e9d"},"cell_type":"code","source":"# Columns appear to be exactly the same before conversion\nconverted_obj.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a87554a543120c4f4359b32868ddb3c9feb2324d"},"cell_type":"code","source":"# Internal structure has been optimized for memory efficiency\nconverted_obj.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"17dfb25ce25fb509eded6a69f1e531b159f8031c"},"cell_type":"markdown","source":"Let's finally replace some of the `object` data types in `optimized_toy_data` by `category` type and observe the significant reduction in memory usage."},{"metadata":{"trusted":true,"_uuid":"17615b660064dbf2bd687b5c1239bee3b11bde23"},"cell_type":"code","source":"optimized_toy_data[converted_obj.columns] = converted_obj","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d2228cd8a8ff3b59baae990047dd7a6ba1b45aa2"},"cell_type":"markdown","source":"By optimizing the columns, we have achived an impressive `(1 - 274.64 / 2238.98) * 100 ~ 88%` memory reduction!"},{"metadata":{"trusted":true,"_uuid":"72fc29987015a7937fb2d64e99ee84587f8fda8f"},"cell_type":"code","source":"# Final total memory reduction\nprint(\"toy_data takes up\", mem_usage(toy_data), 'in memory space')\nprint(\"optimized_toy_data takes up\", mem_usage(optimized_toy_data), 'in memory space')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"813a7e06e6fe63b49234d3e9449a0713faf59bab"},"cell_type":"markdown","source":"# Reading the Entire Dataset\nWe will load in the entire `train.csv` by specifying the specific data type for each column."},{"metadata":{"trusted":true,"_uuid":"b76152ea9727c03c86d992cc0b41b52c8d2babd7"},"cell_type":"code","source":"# Get data types of columns\ndtypes = optimized_toy_data.dtypes\n\n# Zip columns' names and their types accordingly\ncol_name = dtypes.index\ncol_type = [item.name for item in dtypes.values]\nparsing_types = dict(zip(col_name, col_type))\n\n# Print out nicely first 10 items with prettyprint\npreview = {key: value for key, value in list(parsing_types.items())[:10]}\npp = pprint.PrettyPrinter(indent=4)\npp.pprint(preview)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e9a2857024fb9f885c5d32565b1b47c26236ca17"},"cell_type":"code","source":"# Load in the entire dataset\nstart_time = time.time()\ndf = pd.read_csv('../input/train.csv', dtype=parsing_types)\nend_time = time.time()\nprint('Run time: %.2f minutes' %((end_time - start_time) / 60)) # total run time\nprint('Optimized df takes up', mem_usage(df), 'in memory space')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1908d3e47b0ce6d4096dddffec954ba12b135ed3"},"cell_type":"markdown","source":"# Putting Pieces Together\nFollowing are the functions that form the pipeline for reading any large datasets in Pandas. `mem_usage`, `downcast_int`, `downcast_float`, `downcast_obj`, and `optimal_dtypes` are helper functions. `load_dataset` is the main code to optimize your datasets. You may use it to read in`train_csv` to train your models and read in`test_csv` to test your models."},{"metadata":{"trusted":true,"_uuid":"33817f777559f3b7b3c4a623b58b5b9c720c56cb"},"cell_type":"code","source":"def mem_usage(pds_obj):\n    '''\n    MEM_USAGE calculates how much memory used by loading in an object\n    \n    Inputs:\n    - pds_obj: pandas dataframe or series\n    \n    Outputs:\n    - a string showing how much memory already occupied by pds_obj\n    '''\n    if isinstance(pds_obj, pd.DataFrame):\n        usage = pds_obj.memory_usage(deep=True).sum() # sum all memory usages in each column if a dataframe\n    else: # a Series\n        usage = pds_obj.memory_usage(deep=True)\n    # Convert usage from bytes to megabytes\n    usage = usage / 1024 ** 2 # traditional conversion, not mathematical conversion\n    return \"{:.2f} MB\".format(usage)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f13878b6093abbd5f9b6a7ef57e33fcf53e105a7"},"cell_type":"code","source":"def downcast_int(toydf):\n    '''\n    DOWNCAST_INT downcasts variables of int type to unsigned int type, whenever possible, for memory reduction.\n    \n    Inputs:\n    - toydf: a dataframe\n    \n    Outputs:\n    - converted_int: a new dataframe with some columns, possibly all, of unsigned int type\n    '''\n    # Isolate columns of `int` type\n    toydf_int = toydf.select_dtypes(include=['int'])\n    # Downcast each column to `unsigned` with apply function\n    converted_int = toydf_int.apply(pd.to_numeric, downcast='unsigned')\n    \n    return converted_int","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"40edcc6d4ca275c44df0fb02b55895b842aadcce"},"cell_type":"code","source":"def downcast_float(toydf):\n    '''\n    DOWNCAST_FLOAT downcasts variables of float64 type to float type, whenever possible, for memory reduction.\n    The smallest possible type is np.float32.\n    \n    Inputs:\n    - toydf: a dataframe\n    \n    Outputs:\n    - converted_float: a new dataframe with some columns, possibly all, of float type\n    '''\n    # Isolate columns of `float` type\n    toydf_float = toydf.select_dtypes(include=['float'])\n    # Downcast each column to `float` with apply function\n    converted_float = toydf_float.apply(pd.to_numeric, downcast='float')\n    \n    return converted_float","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f0b4020afb784b9682d94c577e0389079a026912"},"cell_type":"code","source":"def downcast_obj(toydf):\n    '''\n    DOWNCAST_OBJ downcasts variables of object type to category type, whenevere possible, for memory reduction.\n    \n    Inputs:\n    - toydf: a dataframe\n    \n    Outputs:\n    - converted_obj: a new dataframe with some columns, possibly all, of category type.\n      Due to Pandas' technical reason, only columns with at most 50% of unique values will be converted to category type. \n      See link `http://pandas.pydata.org/pandas-docs/stable/categorical.html#gotchas` for more info.\n    '''\n    # Isolate columns of `object` type\n    toydf_obj = toydf.select_dtypes(include=['object'])\n    # Initialize a dataframe that will hold converted object\n    converted_obj = pd.DataFrame()\n    # Downcast `object` type to `category` type\n    for col in toydf_obj.columns:\n        num_unique_values = len(toydf_obj[col].unique())\n        total_num_values = len(toydf_obj[col])\n        if num_unique_values / total_num_values < 0.5: # only downcast columns with at most 50% of unique values\n            converted_obj.loc[:, col] = toydf_obj[col].astype('category')\n        else:\n            converted_obj.loc[:, col] = toydf_obj[col]\n    \n    return converted_obj","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b58d4b5e61f664005adb00eca90805d6a6010d4b"},"cell_type":"code","source":"def optimal_dtypes(toydf):\n    '''\n    OPTIMAL_DTYPES finds the optimal data type of each column for memory reduction\n    \n    Inputs:\n    - toydf: a dataframe\n    \n    Outputs:\n    - dict_of_optimalDataTypes: dictionary of (key, value) = (column name, optimal data type) \n      and is used to parse data when loading in a dataset\n    '''\n    # Create a copy of toydf so not mess up with original data\n    optimized_toydf = toydf.copy()\n    \n    # Optimize columns of `int` type\n    converted_int = downcast_int(optimized_toydf)\n    optimized_toydf[converted_int.columns] = converted_int\n    \n    # Optimize columns of `float` type\n    converted_float = downcast_float(optimized_toydf)\n    optimized_toydf[converted_float.columns] = converted_float\n    \n    # Optimize columns of `object` type\n    converted_obj = downcast_obj(optimized_toydf)\n    optimized_toydf[converted_obj.columns] = converted_obj\n    \n    # Create a dictionary where (key, value) = (column name, optimal data type)\n    series_of_optimalDataTypes = optimized_toydf.dtypes # a pandas series\n    col_names = series_of_optimalDataTypes.index\n    col_types = [item.name for item in series_of_optimalDataTypes.values]\n    dict_of_optimalDataTypes = dict(zip(col_names, col_types)) # dictionary to parse data when loading in a dataset\n    \n    # Returns\n    return dict_of_optimalDataTypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"59b6a9430eced9ca9d707240575dc379652816fa"},"cell_type":"code","source":"def load_dataset(df_name):\n    '''\n    LOAD_DATASET reads in the entire dataset whose columns have been optimized for memory reduction.\n    This function also prints out total loading run time and memory usage of the dataset.\n    \n    Inputs:\n    - df_name (str): name of file for reading\n    \n    Outputs:\n    - df: dataframe with optimized columns\n    '''\n    # Start timer\n    start_time = time.time()\n    \n    # Find optimal data types to parse the entire dataset\n    path = '../input/' + df_name # path to the dataset\n    toy_data = pd.read_csv(path, nrows=1e6) # get a sample dataset from original data to analyze\n    parsing_types = optimal_dtypes(toy_data) # optimal data types\n    \n    # Load in the entire dataset with optimal data types\n    df = pd.read_csv(path, dtype=parsing_types)\n    \n    # End timer\n    end_time = time.time()\n    \n    # Returns\n    print('Run time: %.2f minutes' %((end_time - start_time) / 60)) # total run time\n    print('This df takes up', mem_usage(df), 'in memory space') # memory usage\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9747e2a8c8afda4d6d4d761c10befc72a0f0bf98"},"cell_type":"code","source":"# Let's test our main function, load_dataset\nprint(\"Training data\")\ndf_train = load_dataset('train.csv')\nprint(\"========================================\")\nprint(\"Test data\")\ndf_test = load_dataset('test.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"34bd0e5231e782f2aecdd7e9461045b7a80126b9"},"cell_type":"markdown","source":"# Final Thoughts\n1. As promised above, I will answer why I chose to read in 1m rows but not some other numbers. I used 1m of rows as toy data because I felt it was the right choice since `train_csv` has roughly 9m of observations. I could have used 500 rows as toy data, but then the conversion from `object` type to `category` type would have been trickier since we couldn't have a rough estimate of unique values in each column for the entire dataset. In contrast, I didn't want to use, say 2m rows, as my toy data, simply because it could take me too much time. Recall that we are just at the stage of reading in data while no analysis has been done yet!\n1. My kernel was inspired by [Theo](https://www.kaggle.com/theoviel/load-the-totality-of-the-data)'s :) I was just obsessed with a systematic way to work with **any** large dataset with pandas and that was how I googled to find the link at the beginning of my kernel. Thank you Theo very much for the inspiration!\n1. If your dataset has `datetime` columns, you can isolate and treat them separately (though in a similar manner).\n1. It would be nice to organize the above functions into a class for more convenience. I'm really sucked at this :( ... so I hope someone can help me with this task.\n1. I love Pandas (whether you are talking about a programming library or the animals), but I think Dask will be a better tool for very large datasets. I've heard some of its downsides comparing to Pandas, e.g., the lack of certain methods for data analysis, but Dask is coming better."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}