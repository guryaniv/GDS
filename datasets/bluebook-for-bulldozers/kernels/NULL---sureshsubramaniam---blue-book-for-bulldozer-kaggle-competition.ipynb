{"cells":[{"metadata":{"_uuid":"4767828d94f4ed707ef35fd234d78e72e6747aa2"},"cell_type":"markdown","source":"### Blue Book for Bulldozer - Kaggle Competition"},{"metadata":{"_uuid":"32a8705a7f9601dcfa0d93b15d08aa1d0d18401d"},"cell_type":"markdown","source":"This notebook is mostly created using the steps explained in the excellent Machine Learning course by Jeremy Howard and developed by Jeremy Howard and Rachel Thomas.  The courses are available at http://www.fast.ai/\n\nThe idea behind this notebook is to take the reader step by step of how to use RandomForest in any competition.  I have tried to clarify some aspects for a beginner and give reasons for some decision taken.\n\nThe high levels steps are as follows\n1. Create the best model possible using only the training set (Train.csv)\n    - Pre-process the training dataset and change all categories to codes, impute missing values and add some variables.\n    - Split the dataset into training and validation sets (validation set being nearly the same size as the Kaggle provided   validation set. \n    - Separate the dependent variable.\n    - Create the base model using all variables.\n    - From the base model, find out the most important features and remove all unimportant features from the dataset.\n    - Run the model again using only important features.\n    - Detect and remove redundant features\n    - Remove features which have a temporal sequence to make the model more general\n2. Train the model on the whole training set\n    - Run randomforest with a large number of entimators and finetuned paramters on the whole Kaggle provided training dataset.\n    - This is the final model.\n3. Apply the model on validation set (Valid.csv) and predict the SalePrice\n    - Combine the Kaggle provided training and validation sets and pre-process the data.\n    - Separate the datasets into training and validation and fit the above created model using the training data.\n    - Predict the dependent variable using this fitted model.\n4. Calculate the RMSLE using the actual SalePrice in the training set and the predicted SalePrice.\n\nNotes:\n1. I could not import the 'fastai' package into a Windows 10 environment and hence have included the 'fastai' functions I used in the notebook.\n2. To run the notebook, the path to the dataset needs to be provided.\n3. Further optimization of the model is possible by using the Machine_Appendix.csv which contains a more accurate year of manufacture and some more attributes.\n4. Jeremy Howard also suggested using one-hot encoding of some variables.  This has not been included here.\n5. The course by Jeremy stops at finding the RMSE score using a validation set derived from the training set.  I have used the actual validation set provided by Kaggle to calculate the final RMSE.  This is what Kaggle would do if you submit your preductions."},{"metadata":{"_uuid":"81a76e75629a4d952700d67b54f0b9f29d989c05"},"cell_type":"markdown","source":"#### Environment Setup"},{"metadata":{"_uuid":"a503292d0a054307fe491736c0356cf804527597"},"cell_type":"markdown","source":"Import necessary packages"},{"metadata":{"trusted":true,"_uuid":"3cede1fddc41375104974dd9271a4a0f9de3d6fb"},"cell_type":"code","source":"import pandas as pd\nimport re\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\nfrom IPython.display import display\nimport numpy as np\nimport math\nfrom sklearn import metrics\nfrom pandas.api.types import is_string_dtype, is_numeric_dtype\nimport matplotlib.pyplot as plt \nfrom sklearn.ensemble import forest\nimport scipy\nfrom scipy.cluster import hierarchy as hc","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"45dc0ea499ed3e27a4961850fd0d609c8fcce682"},"cell_type":"markdown","source":"#### Compile necessary fastai functions"},{"metadata":{"trusted":true,"_uuid":"81200f667eff7d192e27ecaa299d432ac29170ab"},"cell_type":"code","source":"def rmse(x,y): \n    return math.sqrt(((x-y)**2).mean())\n\ndef print_score(m):\n    res = [rmse(m.predict(X_train), y_train), rmse(m.predict(X_valid), y_valid),\n                m.score(X_train, y_train), m.score(X_valid, y_valid)]\n    if hasattr(m, 'oob_score_'): res.append(m.oob_score_)\n    print(res)\n\ndef split_vals(a,n): \n    return a[:n].copy(), a[n:].copy()\n\ndef get_oob(df):\n    m = RandomForestRegressor(n_estimators=40, max_features=0.6, n_jobs=-1, oob_score=True)\n    x, _ = split_vals(df, n_trn)\n    m.fit(x, y_train)\n    return m.oob_score_\n\ndef add_datepart(df, fldname, drop=True, time=False):\n    fld = df[fldname]\n    fld_dtype = fld.dtype\n    if isinstance(fld_dtype, pd.core.dtypes.dtypes.DatetimeTZDtype):\n        fld_dtype = np.datetime64\n\n    if not np.issubdtype(fld_dtype, np.datetime64):\n        df[fldname] = fld = pd.to_datetime(fld, infer_datetime_format=True)\n    targ_pre = re.sub('[Dd]ate$', '', fldname)\n    attr = ['Year', 'Month', 'Week', 'Day', 'Dayofweek', 'Dayofyear',\n            'Is_month_end', 'Is_month_start', 'Is_quarter_end', 'Is_quarter_start', 'Is_year_end', 'Is_year_start']\n    if time: attr = attr + ['Hour', 'Minute', 'Second']\n    for n in attr: df[targ_pre + n] = getattr(fld.dt, n.lower())\n    df[targ_pre + 'Elapsed'] = fld.astype(np.int64) // 10 ** 9\n    if drop: df.drop(fldname, axis=1, inplace=True)\n        \ndef train_cats(df):\n    for n,c in df.items():\n        if is_string_dtype(c): df[n] = c.astype('category').cat.as_ordered()\n\ndef fix_missing(df, col, name, na_dict):\n    if is_numeric_dtype(col):\n        if pd.isnull(col).sum() or (name in na_dict):\n            df[name+'_na'] = pd.isnull(col)\n            filler = na_dict[name] if name in na_dict else col.median()\n            df[name] = col.fillna(filler)\n            na_dict[name] = filler\n    return na_dict\n\ndef proc_df(df, y_fld=None, skip_flds=None, ignore_flds=None, do_scale=False, na_dict=None,\n            preproc_fn=None, max_n_cat=None, subset=None, mapper=None):\n    if not ignore_flds: ignore_flds=[]\n    if not skip_flds: skip_flds=[]\n    if subset: df = get_sample(df,subset)\n    else: df = df.copy()\n    ignored_flds = df.loc[:, ignore_flds]\n    df.drop(ignore_flds, axis=1, inplace=True)\n    if preproc_fn: preproc_fn(df)\n    if y_fld is None: y = None\n    else:\n        if not is_numeric_dtype(df[y_fld]): df[y_fld] = df[y_fld].cat.codes\n        y = df[y_fld].values\n        skip_flds += [y_fld]\n    df.drop(skip_flds, axis=1, inplace=True)\n\n    if na_dict is None: na_dict = {}\n    else: na_dict = na_dict.copy()\n    na_dict_initial = na_dict.copy()\n    for n,c in df.items(): na_dict = fix_missing(df, c, n, na_dict)\n    if len(na_dict_initial.keys()) > 0:\n        df.drop([a + '_na' for a in list(set(na_dict.keys()) - set(na_dict_initial.keys()))], axis=1, inplace=True)\n    if do_scale: mapper = scale_vars(df, mapper)\n    for n,c in df.items(): numericalize(df, c, n, max_n_cat)\n    df = pd.get_dummies(df, dummy_na=True)\n    df = pd.concat([ignored_flds, df], axis=1)\n    res = [df, y, na_dict]\n    if do_scale: res = res + [mapper]\n    return res\n\ndef numericalize(df, col, name, max_n_cat):\n    if not is_numeric_dtype(col) and ( max_n_cat is None or col.nunique()>max_n_cat):\n        df[name] = col.cat.codes+1","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e09f291cc7e7aeac9aef2789a33f6d2e338c574c"},"cell_type":"markdown","source":"#### Dataset import and pre-processing"},{"metadata":{"trusted":true,"_uuid":"84632a93318667d619fc077e661af5d0577e08dd"},"cell_type":"code","source":"df_raw = pd.read_csv('../input/bulldozer-training-dataset/Train.csv', low_memory=False, parse_dates=['saledate'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a1e2e870444b3b72951f0f4e419f331350d17cc6"},"cell_type":"code","source":"df_raw.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"ddad498ca70fbf3b9bccfe862a1d71f5f17abe45"},"cell_type":"code","source":"#Change SalePrice to log because the evaluation is for RMSLE\ndf_raw.SalePrice = np.log(df_raw.SalePrice)\n#Change dates to date parts\nadd_datepart(df_raw, 'saledate')\n#Add a column for age of bulldozer\ndf_raw['age'] = df_raw['saleYear'] - df_raw['YearMade'] ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"42d7ec5deeedb1817e0d648bf4fe1ce50a6a326c"},"cell_type":"code","source":"#Change string variables to category type\ntrain_cats(df_raw)\n#Specify order for variable UsageBand and change to codes\ndf_raw.UsageBand.cat.set_categories(['High', 'Medium', 'Low'], ordered=True, inplace=True)\ndf_raw.UsageBand = df_raw.UsageBand.cat.codes\n#Change categories to code and missing values to 0, replace missing numeric values with median, \n#add column to indicate replaced missing values and separate the dependent variable as a separate df\ndf, y, nas = proc_df(df_raw, 'SalePrice')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"25ea32478e47731ecd0f4c87357f3f0d3ada1e6e"},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"c88fcff20b02ae181223bb41137d32fdb04e9a1f"},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0759b28bede2fad6cd4a6b5f5f22edf2954604d4"},"cell_type":"markdown","source":"#### Run the base model"},{"metadata":{"trusted":false,"_uuid":"a4817c0d5f4fe2ca7140ff1af59ae060a4102aa7"},"cell_type":"code","source":"#Split the dataset into training and validation sets. Use 12,000 as the validation set\n\nn_valid = 12000  # same as Kaggle's test set size\nn_trn = len(df)-n_valid\nraw_train, raw_valid = split_vals(df_raw, n_trn) #for using unprocessed data if needed.\nX_train, X_valid = split_vals(df, n_trn)\ny_train, y_valid = split_vals(y, n_trn)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"2c0e377ca03380318242d87a45a3ecccac633146"},"cell_type":"code","source":"X_train.shape, X_valid.shape, y_train.shape, y_valid.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"917e8e952a9f4f83ace868beecd16236086dbce9"},"cell_type":"code","source":"#Run base model\nm = RandomForestRegressor(n_jobs=-1)\nm.fit(X_train, y_train)\nprint_score(m);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b5a9d02bd34fecfa0cdb066d3664fcd8867f2d02"},"cell_type":"markdown","source":"This model is pretty good and we are already in the top 25% of the leaderboard!"},{"metadata":{"_uuid":"aa3b14ad4b7456ce6d27c9fd31c046a87838f415"},"cell_type":"markdown","source":"#### Feature Engineering"},{"metadata":{"_uuid":"1a2bbce280d38e56f11a1d588fe28f7cc8dc5f7c"},"cell_type":"markdown","source":"Various methods are used to remove unimportant and redundant features.  This not only simplifies the model but also improves the scores."},{"metadata":{"_uuid":"87dcf44bfa383da111e8c7c8381ea2838fab96c1"},"cell_type":"markdown","source":"#### Feature importance"},{"metadata":{"trusted":false,"_uuid":"973de0e1a54c3bd088ca71d4678115baee1f6860"},"cell_type":"code","source":"#Use the feature importance to find the most important ones\nfeature_importance = pd.DataFrame({'Feature' : X_train.columns, 'Importance' : m.feature_importances_})\nfeature_importance.sort_values('Importance', ascending=False, inplace=True)\nfeature_importance.head(30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"4dedd78026ac8048a36c1a795139174437f91005"},"cell_type":"code","source":"feature_importance.plot('Feature', 'Importance')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"c3e48589a866fd299684bc0473fda441214dd1bd"},"cell_type":"code","source":"# Run the model for various cut off values for the importance to find the best set of importance features\nfor i in [0.001, 0.002, 0.003, 0.004, 0.005, 0.006, 0.007, 0.008, 0.009, 0.010, 0.011, 0.012]:\n    important_features = feature_importance[feature_importance['Importance'] > i]\n    df_important = df[important_features['Feature']]\n    X_train, X_valid = split_vals(df_important, n_trn)\n    y_train, y_valid = split_vals(y, n_trn)\n\n    m = RandomForestRegressor(n_estimators=40, max_features=0.5, n_jobs=-1, oob_score=True)\n    m.fit(X_train, y_train)\n    print_score(m)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"2ca39657963c5cc72ca9a62a66d448ae10f1f9e8"},"cell_type":"code","source":"#The best cut off point seems to be 0.0.006 when the RMSE score is 0.22312856564640468.\nimportant_features = feature_importance[feature_importance['Importance'] > 0.006]\ndf_important = df[important_features['Feature']]\nX_train, X_valid = split_vals(df_important, n_trn)\ny_train, y_valid = split_vals(y, n_trn)\n\nm = RandomForestRegressor(n_estimators=40, max_features=0.5, n_jobs=-1, oob_score=True)\nm.fit(X_train, y_train)\nprint_score(m)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"2dfb8100bc74ca9a6122ba0ab5b34cec0a88f7bf"},"cell_type":"code","source":"#Detect and remove redundant features\n#Draw dendogram of feature clusters\ncorr = np.round(scipy.stats.spearmanr(df_important).correlation, 4)\ncorr_condensed = hc.distance.squareform(1-corr)\nz = hc.linkage(corr_condensed, method='average')\nfig = plt.figure(figsize=(16,10))\ndendrogram = hc.dendrogram(z, labels=df_important.columns, orientation='left', leaf_font_size=16)\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"6d4b93640e63ca76643634d8bb45aa63c76e3839"},"cell_type":"code","source":"#These feature pairs are in the same cluster'\ncluster_pairs = ['saleDayofyear', 'state', 'Drive_System', 'fiSecondaryDesc', 'MachineID', 'ModelID', 'saleElapsed', 'YearMade', 'Enclosure', 'Coupler_System', 'fiModelDescriptor', 'ProductSize','fiBaseModel', 'fiModelDesc']\n#Base OOB score\nget_oob(df_important)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"192cdda38f1d1035eb3e0409268db0d0e00a0eb6"},"cell_type":"code","source":"#Get the OOB score after dropping each of the variables in the cluster pairs\nfor c in cluster_pairs:\n    print(c, get_oob(df_important.drop(c, axis=1)))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"8fe6fda1dfcfd4b479c27b3de72830aaedc83e04"},"cell_type":"code","source":"#For each pair select the attribute which impacts the score less (score is higher) and remove it and calculate OOB\nto_drop = ['state', 'Drive_System', 'MachineID', 'Coupler_System', 'fiModelDescriptor','fiModelDesc']\nget_oob(df_important.drop(to_drop, axis=1))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"b3b65e0e61abd977ee2ed0d300aeb3555fe69823"},"cell_type":"code","source":"#OOB score has decreased slightly after removing attributes but model has become simpler.\n#Run the random forest on the dataset after dropping the columns\ndf_keep = df_important.drop(to_drop, axis=1)\nX_train, X_valid = split_vals(df_keep, n_trn)\nm = RandomForestRegressor(n_estimators=40, max_features=0.5, n_jobs=-1, oob_score=True)\nm.fit(X_train, y_train)\nprint_score(m)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"8ffb62852f6bd6d2539cbdb9eb360b4c0114b4f5"},"cell_type":"code","source":"df_keep.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"2d78dee98712a22efda0d9272d691c58f96a31a6"},"cell_type":"code","source":"#Remove time related features to generalize the model more\n#Label the validation and training set and calculate the OOB score\ndf_ext = df_keep.copy()\ndf_ext['is_valid'] = 1\ndf_ext.is_valid[:n_trn] = 0\nx, y, nas = proc_df(df_ext, 'is_valid')\n\nm = RandomForestClassifier(n_estimators=40, max_features=0.5, n_jobs=-1, oob_score=True)\nm.fit(x, y);\nm.oob_score_\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"1d61765fecc7dd9592f91591039c90a38e9eb281"},"cell_type":"code","source":"#Very high OOB score\n#Find the important features, i.e. the features which help rf predict the validation and training sets\nfeature_importance_ext = pd.DataFrame({'Feature' : x.columns, 'Importance' : m.feature_importances_})\nfeature_importance_ext.sort_values('Importance', ascending=False, inplace=True)\nfeature_importance_ext.head(30)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"c03aa43b1cb2aef5eecdf7c0797e88274860b27f"},"cell_type":"code","source":"#Drop the top 1 and see if the RMSe improves\nto_drop = ['SalesID']\ndf_keep = df_important.drop(to_drop, axis=1)\nX_train, X_valid = split_vals(df_keep, n_trn)\nm = RandomForestRegressor(n_estimators=40, max_features=0.5, n_jobs=-1, oob_score=True)\nm.fit(X_train, y_train)\nprint_score(m)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7314b2829e5dbc3040b1ddf408c7137305d4291d"},"cell_type":"markdown","source":"There is a slight improvement."},{"metadata":{"trusted":false,"_uuid":"630e66060f6136b960b33c663e89314311566ad0"},"cell_type":"code","source":"#Run the final model\nm = RandomForestRegressor(n_estimators=160, max_features=0.5, n_jobs=-1, oob_score=True)\nm.fit(X_train, y_train)\nprint_score(m)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"808c4ab67dd13e19ba959732175910e5f99876bf"},"cell_type":"markdown","source":"The final model looks pretty good and the RMSE decreased from to 0.21570860916579637, mainly due to feature selection and fine tuning parameters.\n\nWhat we have essentially done in the previous steps is to fine tune the hyper parameters and select the subset of features which gives the best score and generalizes the model the best. So the best model is RandomForestRegressor(n_estimators=160, max_features=0.5, n_jobs=-1, oob_score=True) and the features to use are df_keep.columns"},{"metadata":{"_uuid":"c0be6aa57c4796071d0d8fb31a23760ae0ef4206"},"cell_type":"markdown","source":"#### Run model on actual validaiton set"},{"metadata":{"_uuid":"8d8e003e44a6201e47ffc97a6299426a2ac29b05"},"cell_type":"markdown","source":"Now lets train the model on the full training dataset and check the score on the validation set provided by Kaggle.\n\nTo get the same set of category codes and uniformly imputing missing values, we are joining the training and validation sets and pre-processing them together. After preprocessing we will separate them again"},{"metadata":{"trusted":false,"_uuid":"b4384067a750778dc2fb75a31635bf27ab2aaa42"},"cell_type":"code","source":"#Import data\ndf_raw = pd.read_csv('../input/bulldozer-training-dataset/Train.csv', low_memory=False, parse_dates=['saledate'])\ndf_validation = pd.read_csv('../input/bluebook-for-bulldozers/Valid.csv', low_memory=False, parse_dates=['saledate'])\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5634d7da0432ffea743368a120e854d7f28efe4d"},"cell_type":"markdown","source":"Just to be sure, check the column names and columns in the Training and validation sets.  "},{"metadata":{"trusted":false,"_uuid":"7ba8bceb15c657223c31a877a315b202801899b5"},"cell_type":"code","source":"print('training shape',df_raw.shape)\nprint('validation shape', df_validation.shape)\nprint('difference between training and validaiton', set(df_raw.columns) - set(df_validation.columns))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"6a1d8ab42719f5b906aaa961acf057fdd561ec38"},"cell_type":"code","source":"#Separate out the SalePrice as y and change it to log and drop it from the training set\ny = np.log(df_raw['SalePrice'])\ndf_raw = df_raw.drop('SalePrice', axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"6debed898b0b2b1f74e53bfae1cbd35306237ada"},"cell_type":"code","source":"#Append the validation set to the training set\ndf_train_valid = df_raw.append(df_validation)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"8a66c485f90e95a56769a6a5237358f651272f62"},"cell_type":"code","source":"df_train_valid.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"1ac911a476a75835508512043af44cd3aa1b38b7"},"cell_type":"code","source":"#Change dates to date parts\nadd_datepart(df_train_valid, 'saledate')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"ca96cf5a77d64d4f5ea66a8d980a90d83eddd491"},"cell_type":"code","source":"#Add a column for age of bulldozer\ndf_train_valid['age'] = df_train_valid['saleYear'] - df_train_valid['YearMade'] \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"ee8ad5623f1c7003145439a48d3f64672a2a4649"},"cell_type":"code","source":"#Change string variables to category type\ntrain_cats(df_train_valid)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"ba915addffc90a7a44a5e62aca2c728db24198ec"},"cell_type":"code","source":"#Specify order for variable UsageBand and change to codes\ndf_train_valid.UsageBand.cat.set_categories(['High', 'Medium', 'Low'], ordered=True, inplace=True)\ndf_train_valid.UsageBand = df_train_valid.UsageBand.cat.codes\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"89273c9c7d782bc36b945db13d831df2adb7b333"},"cell_type":"code","source":"#Change other categories into codes and replace NaNs with 0.\ncat_cols = list(df_train_valid.select_dtypes(include=['category']).columns)  #Above UsageType is changed to Int \nfor col in cat_cols:\n    s = df_train_valid[col] \n    df_train_valid[col] = s.cat.codes+1","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"141ff72ef6abbfc6972ebb6a995f151bf107de08"},"cell_type":"code","source":"#Replace the NaNs for the numerical column with mean\ndf_train_valid['auctioneerID'].fillna(df_train_valid['auctioneerID'].median(), inplace=True)\ndf_train_valid['MachineHoursCurrentMeter'].fillna(df_train_valid['MachineHoursCurrentMeter'].median(), inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"b8a83fe4aa65c79855d1d0b76e83ef5cab70ffae"},"cell_type":"code","source":"#Check if df has NaNs\ndf_train_valid.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"10e1cda856d471f2940abc9fed22ebf9230cfc43"},"cell_type":"code","source":"df_train_valid.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"7299f3b020c75e5d6c5141e7af0927f53255d96f"},"cell_type":"code","source":"df_train_valid.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7cacc4c3beb6d8b2f8cc25ab60b4c37791ecc735"},"cell_type":"markdown","source":"The pre-processed dataset is ready.  Now need to choose only columns which were in our final model and run the model."},{"metadata":{"trusted":false,"_uuid":"b352794ff69dd7f5c8700e386cd8bbb3c1184329"},"cell_type":"code","source":"# These were the columns in the final model\ndf_keep.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"1c95c7e31bba38771c3da61d371db51e33b78efd"},"cell_type":"code","source":"#Choose only columns which were used in the final model\ndf_train_valid = df_train_valid[df_keep.columns]\n\n#Separate the training and validation sets\ndf_valid = df_train_valid.tail(11573)\ndf_train = df_train_valid.head(401125)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"1b6e74dcb43358cd10f9ed46a45682adbfa523d7"},"cell_type":"code","source":"print(df_valid.shape)\nprint(df_train.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"afb312228626d66322fb019856db8fe17d2ac289"},"cell_type":"code","source":"#Train the model on training set and dependent variable using out final model\nm = RandomForestRegressor(n_estimators=160, max_features=0.5, n_jobs=-1, oob_score=True)\nm.fit(df_train, y) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"39866918cd2e4924a400fa281498f54cbb2602d7"},"cell_type":"code","source":"#Import the validation solution\nsolution = pd.read_csv('../input/bluebook-for-bulldozers/ValidSolution.csv', low_memory=False)\ny_actual = np.log(solution.SalePrice)     ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"f89ac160412e8862cf33c5bbdae6c80351650a53"},"cell_type":"code","source":"#Calculate the RMSE using the prediction from the validation set and the actual provided by Kaggle in the file 'ValidSolutions.csv'\nrmse(m.predict(df_valid), y_actual)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9cd94156bdd980afa285f7edc1e3b478a9482000"},"cell_type":"markdown","source":"That's it! Further fine tuning can be done by selecting a differnt combinations of features and perhaps replacing YearMade with the corrected data.  I leave to the reader to do these and better the score above.  \nHope this notebook helped!"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}