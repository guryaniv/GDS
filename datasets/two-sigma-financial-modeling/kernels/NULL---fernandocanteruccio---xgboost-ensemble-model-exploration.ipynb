{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "cfbd9d83-42e8-4a3d-6fc2-30f2976c12f7"
      },
      "source": [
        "This notebook is an assistant visualization script for production and exploration of some ensemble models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "136634b6-4607-f321-2757-306b58af6504"
      },
      "outputs": [],
      "source": [
        "#XGBoost prediction script\n",
        "#importing modules\n",
        "\n",
        "import kagglegym\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import xgboost as xgb\n",
        "from time import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "b8223572-b98a-ed3a-5ffe-db1e6298cc5e"
      },
      "source": [
        "Setting up the environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "408dc43c-fea3-1b80-42ed-1f98ea17aa33"
      },
      "outputs": [],
      "source": [
        "#Making environment ################################################################################################################################################################\n",
        "\n",
        "# The \"environment\" is our interface for code competitions\n",
        "env = kagglegym.make()\n",
        "\n",
        "# We get our initial observation by calling \"reset\"\n",
        "observation = env.reset()\n",
        "\n",
        "train = observation.train\n",
        "# Note that the first observation we get has a \"train\" dataframe\n",
        "print(\"Train has {} rows\".format(len(observation.train)))\n",
        "\n",
        "# The \"target\" dataframe is a template for what we need to predict:\n",
        "print(\"Target column names: {}\".format(\", \".join(['\"{}\"'.format(col) for col in list(observation.target.columns)])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "6a2aebf8-5733-221a-3cab-e79de65b7751"
      },
      "source": [
        "Some preprocessing of the data. Clipping the y signal helps, as shown in the community kernels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b4e22e66-621d-ea54-903d-30e5454600ce"
      },
      "outputs": [],
      "source": [
        "# Feature enginnering and preprocessing ############################################################################################################################################\n",
        "\n",
        "# https://www.kaggle.com/bguberfain/two-sigma-financial-modeling/univariate-model-with-clip/run/482189/code\n",
        "# Clipped target value range to use\n",
        "low_y_cut = -0.08\n",
        "high_y_cut = 0.08\n",
        "\n",
        "y_is_above_cut = (train.y > high_y_cut)\n",
        "y_is_below_cut = (train.y < low_y_cut)\n",
        "y_is_within_cut = (~y_is_above_cut & ~y_is_below_cut)\n",
        "\n",
        "# Select the features to use\n",
        "excl = ['id', 'sample', 'y', 'timestamp']\n",
        "#feature_vars = [c for c in train.columns if c not in excl]\n",
        "target_var = 'y'\n",
        "\n",
        "targets = train.loc[y_is_within_cut, target_var]\n",
        "y_train = targets.values\n",
        "\n",
        "del y_is_above_cut, y_is_below_cut, excl, target_var, targets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "582ea5e6-f00c-aa48-676d-9e9ea26195ad"
      },
      "source": [
        "Here we are using some ensemble models from XGBoost. Using a linear models ensemble plus a trees regressor ensemble to try to manage overfitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "996ad30f-3ed3-ade2-17d5-34b24a256789"
      },
      "outputs": [],
      "source": [
        "# Model training routine ###########################################################################################################################################################\n",
        "\n",
        "# Univariate linear models, first layer <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
        "\n",
        "features_ulm = ['technical_20', 'technical_19', 'technical_27', 'technical_30', 'technical_2', 'technical_36']\n",
        "\n",
        "features_ulm_train = train.loc[y_is_within_cut, features_ulm]\n",
        "feature_ulm_names = features_ulm_train.columns\n",
        "X_ulm = features_ulm_train.values\n",
        "\n",
        "# Train dataset\n",
        "xglin_train = xgb.DMatrix(X_ulm, label=y_train, feature_names=feature_ulm_names)\n",
        "\n",
        "# XGb model params\n",
        "params_xglin = {'booster'         :'gblinear',\n",
        "                'objective'       :'reg:linear',\n",
        "                'eta'             : 0.1,\n",
        "                'max_depth'       : 4,\n",
        "                'subsample'       : 0.9,\n",
        "                'min_child_weight': 1000,\n",
        "                'seed'            : 42,\n",
        "                'base_score'      : 0\n",
        "                }\n",
        "\n",
        "print (\"Training linear models\")\n",
        "t0 = time()\n",
        "bslin = xgb.train(params_xglin, xglin_train, 10)\n",
        "print(\"Done: %.1fs\" % (time() - t0))\n",
        "\n",
        "# Boosted trees ensemble, first layer <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
        "\n",
        "# https://www.kaggle.com/fernandocanteruccio/two-sigma-financial-modeling/xgboost-feature-importance-analysis\n",
        "features_bt = ['technical_35', 'fundamental_37', 'technical_20', 'technical_36', 'fundamental_36', 'fundamental_53',\n",
        "                'fundamental_35', 'fundamental_11', 'fundamental_50', 'fundamental_34']\n",
        "\n",
        "features_bt_train = train.loc[y_is_within_cut, features_bt]\n",
        "feature_bt_names = features_bt_train.columns\n",
        "X_bt = features_bt_train.values\n",
        "\n",
        "# Train dataset\n",
        "xgtrees_train = xgb.DMatrix(X_bt, label=y_train, feature_names=feature_bt_names)\n",
        "\n",
        "# XGb model params\n",
        "params_xgtrees = {'objective'       :'reg:linear',\n",
        "                  'eta'             : 0.1,\n",
        "                  'max_depth'       : 4,\n",
        "                  'subsample'       : 0.9,\n",
        "                  'min_child_weight': 1000,\n",
        "                  'seed'            : 42,\n",
        "                  'base_score'      : 0\n",
        "                   }\n",
        "\n",
        "print (\"Training boosted trees\")\n",
        "t0 = time()\n",
        "bst = xgb.train(params_xgtrees, xgtrees_train, 10)\n",
        "print(\"Done: %.1fs\" % (time() - t0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "dad0e9e5-697f-cfae-0c3d-947901d4e593"
      },
      "source": [
        "For this simple exploration model, the final ensemble prediction is just the unweighted mean of the first layer models predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "0c6a2aa0-9900-1147-b008-92946a507371"
      },
      "outputs": [],
      "source": [
        "# Predict-step-predict routine ####################################################################################################################################################\n",
        "def gen_predictions(update_threshold, print_info=True):\n",
        "    \n",
        "    global bslin, bst\n",
        "    \n",
        "    env = kagglegym.make()\n",
        "\n",
        "    # We get our initial observation by calling \"reset\"\n",
        "    observation = env.reset()\n",
        "\n",
        "    train = observation.train\n",
        "\n",
        "    params_xglin.update({'process_type': 'update',\n",
        "                         'updater'     : 'refresh',\n",
        "                         'refresh_leaf': False})\n",
        "\n",
        "    params_xgtrees.update({'process_type': 'update',\n",
        "                           'updater'     : 'refresh',\n",
        "                           'refresh_leaf': False})\n",
        "\n",
        "    # init aux vars\n",
        "    reward = 0.0\n",
        "    reward_log = []\n",
        "    timestamps_log = []\n",
        "    pos_count = 0\n",
        "    neg_count = 0\n",
        "\n",
        "    total_pos = []\n",
        "    total_neg = []\n",
        "\n",
        "    print(\"Predicting\")\n",
        "    t0= time()\n",
        "    while True:\n",
        "    #    observation.features.fillna(mean_values, inplace=True)\n",
        "\n",
        "        # Predict with univariate linear models\n",
        "        features_ulm_pred = observation.features.loc[:,features_ulm].values\n",
        "        X_ulm_pred = xgb.DMatrix(features_ulm_pred, feature_names=feature_ulm_names)\n",
        "\n",
        "        y_ulm_pred = bslin.predict(X_ulm_pred).clip(low_y_cut, high_y_cut)\n",
        "\n",
        "        # Predict with boosted trees\n",
        "        features_bt_pred = observation.features.loc[:,features_bt].values\n",
        "        X_bt_pred = xgb.DMatrix(features_bt_pred, feature_names=feature_bt_names)\n",
        "\n",
        "        y_bt_pred = bst.predict(X_bt_pred).clip(low_y_cut, high_y_cut)\n",
        "\n",
        "        # Average the predictions\n",
        "        averaged_out = np.mean(np.vstack((y_ulm_pred, y_bt_pred)), axis=0)\n",
        "\n",
        "        # Fill target df with predictions \n",
        "        observation.target.y = averaged_out\n",
        "\n",
        "        observation.target.fillna(0, inplace=True)\n",
        "        target = observation.target\n",
        "        timestamp = observation.features[\"timestamp\"][0]\n",
        "        obs_old = observation\n",
        "        observation, reward, done, info = env.step(target)\n",
        "\n",
        "        if update_threshold is not None:\n",
        "            if (reward > update_threshold):\n",
        "                # update boosted trees model\n",
        "                xgtrees_update = xgb.DMatrix(obs_old.features.loc[:,features_bt].values, averaged_out, feature_names=feature_bt_names)\n",
        "\n",
        "                bst = xgb.train(params_xgtrees, xgtrees_update, 10, xgb_model=bst)\n",
        "\n",
        "                # update boosted linear model \n",
        "                xglin_update = xgb.DMatrix(obs_old.features.loc[:,features_ulm].values, averaged_out, feature_names=feature_ulm_names)\n",
        "\n",
        "                bslin = xgb.train(params_xglin, xglin_update, 10, xgb_model=bslin)\n",
        "\n",
        "        \n",
        "        timestamps_log.append(timestamp)\n",
        "        reward_log.append(reward)\n",
        "\n",
        "        if (reward < 0):\n",
        "            neg_count += 1\n",
        "        else:\n",
        "            pos_count += 1\n",
        "\n",
        "        total_pos.append(pos_count)\n",
        "        total_neg.append(neg_count)\n",
        "        \n",
        "        if timestamp % 100 == 0:\n",
        "            if print_info:\n",
        "                print(\"Timestamp #{}\".format(timestamp))\n",
        "                print(\"Step reward:\", reward)\n",
        "                print(\"Mean reward:\", np.mean(reward_log[-timestamp:]))\n",
        "                print(\"Positive rewards count: {0}, Negative rewards count: {1}\".format(pos_count, neg_count))\n",
        "                print(\"Positive reward %:\", pos_count / (pos_count + neg_count) * 100)\n",
        "\n",
        "            pos_count = 0\n",
        "            neg_count = 0\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "    print(\"Done: %.1fs\" % (time() - t0))\n",
        "    print(\"Total reward sum:\", np.sum(reward_log))\n",
        "    print(\"Final reward mean:\", np.mean(reward_log))\n",
        "    print(\"Total positive rewards count: {0}, Total negative rewards count: {1}\".format(np.sum(total_pos), np.sum(total_neg)))\n",
        "    print(\"Final positive reward %:\", np.sum(total_pos) / (np.sum(total_pos) + np.sum(total_neg)) * 100)\n",
        "    print(info)\n",
        "\n",
        "    return reward_log, timestamps_log, info['public_score']\n",
        "\n",
        "reward_log, timestamps_log, score = gen_predictions(None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "1c174a41-b755-2f47-518a-56e1a709e6bb"
      },
      "source": [
        "With the model predictions in hands, lets plot some rewards distributions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d0b83462-c340-5194-bc26-4e80ff17390e"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "sns.set(style=\"whitegrid\");"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ccffb5c2-60c4-b8dc-4145-7e7a207f9bb5"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(12,7))\n",
        "ax.set_title(\"Rewards distribution\");\n",
        "sns.distplot(reward_log, kde=True);\n",
        "print(\"Rewards count:\",np.array(reward_log).shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "d13b5ddb-f965-daba-2013-185b26635206"
      },
      "source": [
        "As seen in the histogram, we are getting most of the reward signals around -0.1. Lets find out how it changes over time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "3e703f66-e4f3-2727-e9e8-084c040076aa"
      },
      "outputs": [],
      "source": [
        "def moving_average(a, n=3) :\n",
        "    ret = np.cumsum(a, dtype=float)\n",
        "    ret[n:] = ret[n:] - ret[:-n]\n",
        "    return ret[n - 1:] / n\n",
        "\n",
        "ma_window = 33\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12,7))\n",
        "ax.set_xlabel(\"Timestamp\");\n",
        "ax.set_title(\"Reward signal over time\");\n",
        "sns.tsplot(reward_log,timestamps_log,ax=ax,color='b');\n",
        "sns.tsplot(np.hstack((np.zeros(ma_window-1),moving_average(reward_log, ma_window)))\n",
        "           ,timestamps_log,ax=ax,color='r');\n",
        "ax.set_ylabel('Reward');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "d62a64d8-a6c7-654e-ded5-bf309276cd3b"
      },
      "source": [
        "No real trend here. Lets try to refresh the models trees during the prediction fase and see what happens with the reward signal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "2655e1e0-c9e5-f0ce-562d-475415b008cf"
      },
      "outputs": [],
      "source": [
        "reward_log_2, timestamps_log, score_2 = gen_predictions(0.01,print_info=False)\n",
        "print(\"Percent change:\", (score_2 - score) / score * 100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "e120ffd6-ef40-c06d-bc9e-1b478b28dafe"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(12,7))\n",
        "ax.set_title(\"Rewards distribution\");\n",
        "sns.distplot(reward_log, kde=True, ax=ax, label='Without update',color='b');\n",
        "sns.distplot(reward_log_2, kde=True, ax=ax, label='With update',color='g');\n",
        "plt.legend();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "fc3fa294-3880-f5d8-e00a-78b6049a54f2"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(12,7))\n",
        "ax.set_xlabel(\"Timestamp\");\n",
        "ax.set_title(\"Averaged reward signal over time (window = 33)\");\n",
        "sns.tsplot(np.hstack((np.zeros(ma_window-1),moving_average(reward_log, ma_window)))\n",
        "           ,timestamps_log,ax=ax,color='b');\n",
        "sns.tsplot(np.hstack((np.zeros(ma_window-1),moving_average(reward_log_2, ma_window)))\n",
        "           ,timestamps_log,ax=ax,color='g');\n",
        "ax.set_ylabel('Reward');\n",
        "ax.set_ylim([-0.23, -0.08]);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "813f0227-4a7f-080c-8ea0-0388530c948c"
      },
      "source": [
        "Not much of a difference on the rewards log, yet the public score jumped. This is an ~15% increase in the score. What if we do more agressive updates?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "bf2d0fcd-1802-d61e-5ac2-d09b71b8b10c"
      },
      "outputs": [],
      "source": [
        "reward_log_3, timestamps_log, score_3 = gen_predictions(-0.03,print_info=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "90b61a04-5364-3d4a-2115-cf219269a7b0"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(12,7))\n",
        "ax.set_title(\"Rewards distribution\");\n",
        "sns.distplot(reward_log, kde=True, ax=ax, label='Without update',color='b');\n",
        "sns.distplot(reward_log_2, kde=True, ax=ax, label='With update',color='g');\n",
        "sns.distplot(reward_log_3, kde=True, ax=ax, label='More agressive update',color='r');\n",
        "plt.legend();\n",
        "print(\"Percent change:\", (score_3 - score) / score * 100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "7026394b-dc79-66cf-be19-69ef2c629515"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(12,7))\n",
        "ax.set_xlabel(\"Timestamp\");\n",
        "ax.set_title(\"Averaged reward signal over time (window = 33)\");\n",
        "sns.tsplot(np.hstack((np.zeros(ma_window-1),moving_average(reward_log, ma_window)))\n",
        "           ,timestamps_log,ax=ax,color='b');\n",
        "sns.tsplot(np.hstack((np.zeros(ma_window-1),moving_average(reward_log_2, ma_window)))\n",
        "           ,timestamps_log,ax=ax,color='g');\n",
        "sns.tsplot(np.hstack((np.zeros(ma_window-1),moving_average(reward_log_3, ma_window)))\n",
        "           ,timestamps_log,ax=ax,color='r');\n",
        "ax.set_ylabel('Averaged Reward');\n",
        "ax.set_ylim([-0.23, -0.08]);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "173e3e80-4275-ccaf-4756-c1d015095395"
      },
      "source": [
        "Again, not much of a chance but the score increased about ~15.4% from the static model. With so many params to tune, it will be nice to do some grid-search with cross-validation. Also, it will be probably better it the ensemble predictions is weighted by some model instead of simple averaging."
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}