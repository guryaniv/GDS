{"cells":[{"metadata":{"_uuid":"220460d0f628f4cc19be1d2e77e1fdd1609b2055"},"cell_type":"markdown","source":"**Introduction: Movie Review Sentiment Analysis**\n\nThis notebook is for those who are new to machine learning and deep learning industry. I want to introduce the basic concept to learn data, understand the data using some statistics and virtualization tools in order to start machine learning instead of jumping to a complicated model. Any suggestion is much appreciated.\n\nSentiment analysis is an example of supervised machine learning task a labelled dataset which containing text documents and their labels is used for a train a classifier.\n\nFor feature engineering, I suggest you to please take a look at this notebook: [Movie Reviews: Feature Engineering\n](https://www.kaggle.com/bhavikapanara/movie-reviews-feature-engineering)"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n#NLP\nfrom nltk.corpus import stopwords\neng_stopwords = stopwords.words(\"english\")\nfrom nltk.stem import WordNetLemmatizer\nimport textblob\nimport re\nfrom sklearn.preprocessing import OneHotEncoder\n\nprint(\"Loading data...\")\ntrain = pd.read_csv(\"../input/movie-review-sentiment-analysis-kernels-only/train.tsv\", sep=\"\\t\")\nprint(\"Train shape:\", train.shape)\ntest = pd.read_csv(\"../input/movie-review-sentiment-analysis-kernels-only/test.tsv\", sep=\"\\t\")\nprint(\"Test shape:\", test.shape)\n\nenc = OneHotEncoder(sparse=False)\nenc.fit(train[\"Sentiment\"].values.reshape(-1, 1))\nprint(\"Number of classes:\", enc.n_values_[0])\n\nprint(\"Class distribution:\\n{}\".format(train[\"Sentiment\"].value_counts()/train.shape[0]))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"collapsed":true},"cell_type":"code","source":"print(\"Ratio of test set examples which occur in the train set: {0:.2f}\".format(len(set(train[\"Phrase\"]).intersection(set(test[\"Phrase\"])))/test.shape[0]))\ntest = pd.merge(test, train[[\"Phrase\", \"Sentiment\"]], on=\"Phrase\", how=\"left\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"8b71c56b2f64beb3725a4cb52c2121996f9be0c8"},"cell_type":"code","source":"def clean_text(text):\n    text = text.lower()\n    text = re.sub(r\"what's\", \"what is \", text)\n    text = re.sub(r\"\\'s\", \" \", text)\n    text = re.sub(r\"\\'ve\", \" have \", text)\n    text = re.sub(r\"can't\", \"cannot \", text)\n    text = re.sub(r\"n't\", \" not \", text)\n    text = re.sub(r\"i'm\", \"i am \", text)\n    text = re.sub(r\"\\'re\", \" are \", text)\n    text = re.sub(r\"\\'d\", \" would \", text)\n    text = re.sub(r\"\\'ll\", \" will \", text)\n    text = re.sub(r\"\\'scuse\", \" excuse \", text)\n    text = re.sub('\\W', ' ', text)\n    text = re.sub('\\s+', ' ', text)\n    text = text.strip(' ')\n    return text\n\ntrain['Phrase'] = train['Phrase'].map(lambda com : clean_text(com))\ntest['Phrase'] = test['Phrase'].map(lambda com : clean_text(com))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fd418563c6f08ce93d964cc8d854bf6a31f78876","collapsed":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\n\ncv1 = CountVectorizer()\ncv1.fit(train[\"Phrase\"])\n\ncv2 = CountVectorizer()\ncv2.fit(test[\"Phrase\"])\n\nprint(\"Train Set Vocabulary Size:\", len(cv1.vocabulary_))\nprint(\"Test Set Vocabulary Size:\", len(cv2.vocabulary_))\nprint(\"Number of Words that occur in both:\", len(set(cv1.vocabulary_.keys()).intersection(set(cv2.vocabulary_.keys()))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7d93acdf6e1cbb7a2284ea60cb6c780d4dfd72c1","collapsed":true},"cell_type":"code","source":"def transform(df):\n    df[\"phrase_count\"] = df.groupby(\"SentenceId\")[\"Phrase\"].transform(\"count\")\n    df[\"word_count\"] = df[\"Phrase\"].apply(lambda x: len(x.split()))\n    df[\"has_upper\"] = df[\"Phrase\"].apply(lambda x: x.lower() != x)\n    df[\"sentence_end\"] = df[\"Phrase\"].apply(lambda x: x.endswith(\".\"))\n    df[\"after_comma\"] = df[\"Phrase\"].apply(lambda x: x.startswith(\",\"))\n    df[\"Phrase\"] = df[\"Phrase\"].apply(lambda x: x.lower())\n    return df\n\ntrain = transform(train)\ntest = transform(test)\n\ndef getSentFeat(s , polarity):\n    sent = textblob.TextBlob(s).sentiment\n    if polarity:\n        return sent.polarity\n    else :\n        return sent.subjectivity\n    \ntrain['polarity'] = train['Phrase'].apply(lambda x: getSentFeat(x , polarity=True))\ntrain['subjectivity'] = train['Phrase'].apply(lambda x: getSentFeat(x , polarity=False))\n\ntest['polarity'] = test['Phrase'].apply(lambda x: getSentFeat(x , polarity=True))\ntest['subjectivity'] = test['Phrase'].apply(lambda x: getSentFeat(x , polarity=False))\n\ndense_features = [\"phrase_count\", \"word_count\", \"has_upper\", \"after_comma\", \"sentence_end\" ,\"polarity\",\"subjectivity\"]\n\ntrain.groupby(\"Sentiment\")[dense_features].mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"8363686be195146a5e89bee0d42f839399625444"},"cell_type":"code","source":"NUM_FOLDS = 5\n\ntrain[\"fold_id\"] = train[\"SentenceId\"].apply(lambda x: x%NUM_FOLDS)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5b3b8e4c52c35b4642cfda90b68ea97398d879f7","collapsed":true},"cell_type":"code","source":"EMBEDDING_FILE = \"../input/glove6b/glove.6B.100d.txt\"\nEMBEDDING_DIM = 100\n\nall_words = set(cv1.vocabulary_.keys()).union(set(cv2.vocabulary_.keys()))\n\ndef get_embedding():\n    embeddings_index = {}\n    f = open(EMBEDDING_FILE)\n    for line in f:\n        values = line.split()\n        word = values[0]\n        if len(values) == EMBEDDING_DIM + 1 and word in all_words:\n            coefs = np.asarray(values[1:], dtype=\"float32\")\n            embeddings_index[word] = coefs\n    f.close()\n    return embeddings_index\n\nembeddings_index = get_embedding()\nprint(\"Number of words that don't exist in GLOVE:\", len(all_words - set(embeddings_index)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2879a5b767d39d02f8e249358a3678ab0127fe6e","collapsed":true},"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\nMAX_SEQUENCE_LENGTH = 60\n\ntokenizer = Tokenizer(filters=\"\")\ntokenizer.fit_on_texts(np.append(train[\"Phrase\"].values, test[\"Phrase\"].values))\nword_index = tokenizer.word_index\n\nnb_words = len(word_index) + 1\nembedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n\nfor word, i in word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector\n        \nseq = pad_sequences(tokenizer.texts_to_sequences(train[\"Phrase\"]), maxlen=MAX_SEQUENCE_LENGTH)\ntest_seq = pad_sequences(tokenizer.texts_to_sequences(test[\"Phrase\"]), maxlen=MAX_SEQUENCE_LENGTH)\nseq.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"89e1c4a0e74378d7cdc2474c7142432fb93f4b58"},"cell_type":"code","source":"from keras.layers import *\nfrom keras.models import Model\nfrom keras.callbacks import EarlyStopping,ModelCheckpoint\n\ndef build_model():\n    embedding_layer = Embedding(nb_words,\n                                EMBEDDING_DIM,\n                                weights=[embedding_matrix],\n                                input_length=MAX_SEQUENCE_LENGTH,\n                                trainable=True)\n    dropout = SpatialDropout1D(0.2)\n    mask_layer = Masking()\n    lstm_layer = LSTM(50)\n    \n    seq_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=\"int32\")\n    dense_input = Input(shape=(len(dense_features),))\n    \n    dense_vector = BatchNormalization()(dense_input)\n    \n    phrase_vector = lstm_layer(mask_layer(dropout(embedding_layer(seq_input))))\n    \n    feature_vector = concatenate([phrase_vector, dense_vector])\n    feature_vector = Dense(50, activation=\"relu\")(feature_vector)\n    feature_vector = Dense(20, activation=\"relu\")(feature_vector)\n    \n    output = Dense(5, activation=\"softmax\")(feature_vector)\n    \n    model = Model(inputs=[seq_input, dense_input], outputs=output)\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f4b4ded968d35ab56cda2457e2743eafde97973b","collapsed":true},"cell_type":"code","source":"test_preds = np.zeros((test.shape[0], 5))\n\nfor i in range(NUM_FOLDS):\n    print(\"FOLD\", i+1)\n    \n    print(\"Splitting the data into train and validation...\")\n    train_seq, val_seq = seq[train[\"fold_id\"] != i], seq[train[\"fold_id\"] == i]\n    train_dense, val_dense = train[train[\"fold_id\"] != i][dense_features], train[train[\"fold_id\"] == i][dense_features]\n    y_train = enc.transform(train[train[\"fold_id\"] != i][\"Sentiment\"].values.reshape(-1, 1))\n    y_val = enc.transform(train[train[\"fold_id\"] == i][\"Sentiment\"].values.reshape(-1, 1))\n    \n    print(\"Building the model...\")\n    model = build_model()\n    model.compile(loss=\"categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"acc\"])\n    \n    early_stopping = EarlyStopping(monitor=\"val_acc\", patience=2, verbose=1)\n    file_path = \"best_model.hdf5\"\n    check_point = ModelCheckpoint(file_path, monitor = \"val_loss\", verbose = 1, save_best_only = True, mode = \"min\")\n    \n    print(\"Training the model...\")\n    model.fit([train_seq, train_dense], y_train, validation_data=([val_seq, val_dense], y_val),\n              epochs=15, batch_size=1024, shuffle=True, callbacks=[check_point,early_stopping], verbose=1)\n    \n    print(\"Predicting...\")\n    test_preds += model.predict([test_seq, test[dense_features]], batch_size=1024, verbose=1)\n    print()\n    \ntest_preds /= NUM_FOLDS","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a312327aa8dadee3a7e57a3b75d036fb18d868ba","collapsed":true},"cell_type":"code","source":"print(\"Select the class with the highest probability as prediction...\")\ntest[\"pred\"] = test_preds.argmax(axis=1)\n\nprint(\"Use these predictions for the phrases which don't exist in train set...\")\ntest.loc[test[\"Sentiment\"].isnull(), \"Sentiment\"] = test.loc[test[\"Sentiment\"].isnull(), \"pred\"]\n\nprint(\"Make the submission ready...\")\ntest[\"Sentiment\"] = test[\"Sentiment\"].astype(int)\ntest[[\"PhraseId\", \"Sentiment\"]].to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c9f496774554a8e772bbc9aac3f179a7dd67dd50"},"cell_type":"markdown","source":"Also, I want to give credit to this : [notebook](https://www.kaggle.com/divrikwicky/fast-basic-lstm-net-with-proper-k-fold-lb-0-682)"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"8224d179a3990df7d44fd9390b3625ae5036189d"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}