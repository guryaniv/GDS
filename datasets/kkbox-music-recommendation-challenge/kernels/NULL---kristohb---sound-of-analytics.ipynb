{"nbformat": 4, "nbformat_minor": 1, "cells": [{"cell_type": "markdown", "metadata": {"_cell_guid": "4a7c52a9-fdb8-488e-bc3f-3c260f0cfee4", "_uuid": "ce21c39a2161ee46d892f749d760019533ef1763"}, "source": ["# 1 Introduction to competitionm\n", "Now, the goal of the competition is to \"build a better music recommendation system\" using dataset from KKBOX. More specifically one is to predict the chances of a user listening to song repetitvely within a time period from an event triggered the first listening. \n", "\n", "Now, in the training set we are dealt\n", "* The user id\n", "* Song id\n", "* Where the event of listening to a new song was triggered\n", "* The layout name seen by the user\n", "* Where the song was found\n", "* Finally the target: 1 if the user listened to the same song within the same month and 0 if not. \n", "\n", "Additionally we have the songs dataset that describes the song id and the members dataset containing the user information.  We also have extra information about each song as well. \n", "\n", "Now let us load the modules and datasets.\n", "\n"]}, {"cell_type": "code", "outputs": [], "execution_count": null, "metadata": {"collapsed": true, "_cell_guid": "7b078300-ece1-47c6-982e-a3f55b9d19b4", "_uuid": "daf1256e7564f13e7f01f3368e284d52cfc4c0af"}, "source": ["# This Python 3 environment comes with many helpful analytics libraries installed\n", "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n", "# For example, here's several helpful packages to load in \n", "\n", "import numpy as np # linear algebra\n", "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n", "import xgboost as xgb\n", "from datetime import datetime\n", "import matplotlib.pyplot as plt\n", "import copy \n", "import re\n", "from sklearn import preprocessing \n", "from sklearn.model_selection import GridSearchCV, train_test_split, KFold, StratifiedKFold\n", "# Input data files are available in the \"../input/\" directory.\n", "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n", "\n", "from subprocess import check_output\n", "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n", "\n", "# Any results you write to the current directory are saved as output"]}, {"cell_type": "code", "outputs": [], "execution_count": null, "metadata": {"collapsed": true, "_cell_guid": "04b4b248-695e-4301-bfc3-7e19f761feae", "_uuid": "4c4269c690539fe4af90ac94dd5c71fd93530b00"}, "source": ["# LOad data ###\n", "train = pd.read_csv('../input/train.csv')\n", "test = pd.read_csv('../input/test.csv')\n", "songs = pd.read_csv('../input/songs.csv')\n", "song_info = pd.read_csv('../input/song_extra_info.csv')\n", "members = pd.read_csv('../input/members.csv')\n", "sample_submission = pd.read_csv('../input/sample_submission.csv')\n", "print(\"| fin\")"]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "744b7ebf-96bd-41d1-8800-592d98607ba0", "_uuid": "de08dd1cfa786ae78b37d7d71bd4edeba0911cad"}, "source": ["# 2.Study of users \n", "We can begin our study by exploring the training set"]}, {"cell_type": "code", "outputs": [], "execution_count": null, "metadata": {"collapsed": true, "_uuid": "78c100d8a1c8fbb9e82f657a96b1b8a1d0840640", "_cell_guid": "e93af957-7dd6-4f26-971d-4a2fbbe7ed44", "_kg_hide-output": true, "_kg_hide-input": true}, "source": ["print(\"Shape of Training set: \",train.shape)\n", "print(\"Column names: \",train.columns.values)\n", "users = train.loc[:,'msno'].values\n", "unique_users = list(set(users))\n", "print(\"Number of unique Users: \",len(unique_users))"]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "9cbafcc0-b135-42aa-8516-4d66ada3c3f9", "_uuid": "adf8a98aefaa880e8739375eb15119cc7ac2a955"}, "source": ["Hence there are 30,755 users divided over 7,377,418 observation. Talk about big data! Now we can merge on the extra data in order to get a complete dataset."]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "e5f7361a-2df0-4a53-a73d-e058c8af6581", "_uuid": "23930e84f11734c0100ed1b6302eeaf2862e8d2b"}, "source": ["# 3. Merge in members, songs and song info into the data\n", "\n", "We start by merging in the extra data in order to have a complete dataset"]}, {"cell_type": "code", "outputs": [], "execution_count": null, "metadata": {"collapsed": true, "_cell_guid": "5b86e0c4-b34b-4c16-8536-24880ed1fb64", "_uuid": "1dbebd43fa3895bdabed789cc6734e39e5d8a10c"}, "source": ["## Merge in data into the training set \n", "\n", "train = pd.merge(train,members,how=\"left\",on=\"msno\")\n", "print(\"Shape new train: \",train.shape)\n", "train = pd.merge(train,songs,how=\"left\",on=\"song_id\")\n", "print(\"Shape new train: \",train.shape)\n", "train = pd.merge(train,song_info,how=\"left\",on=\"song_id\")\n", "print(\"Shape new train: \",train.shape)\n"]}, {"cell_type": "code", "outputs": [], "execution_count": null, "metadata": {"collapsed": true, "_cell_guid": "b059f762-d365-48e8-9752-93deed3ca0e6", "_uuid": "612e3b64a2fc241ac49b5bbbca8f78bfb7b6adce"}, "source": ["# Merge test data as well \n", "test = pd.merge(test,members,how=\"left\",on=\"msno\")\n", "print(\"Shape new test: \",test.shape)\n", "test = pd.merge(test,songs,how=\"left\",on=\"song_id\")\n", "print(\"Shape new test: \",test.shape)\n", "test = pd.merge(test,song_info,how=\"left\",on=\"song_id\")\n", "\n", "print(\"Shape new test: \",test.shape)"]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "5d94fa22-b760-4e35-96e5-b35c5170dee1", "_uuid": "fa90f1d639f3c7dbc1517cfc957fca158f22d369"}, "source": ["# 4. Study the Missing Values"]}, {"cell_type": "code", "outputs": [], "execution_count": null, "metadata": {"collapsed": true, "_cell_guid": "468e141c-da69-4127-a442-bb1ce0818e54", "_uuid": "e54e9011f35c77c1dadca98fe1b46aaf6fb7e1fb"}, "source": ["## Missing data analysis curtesy https://www.kaggle.com/pmarcelino/comprehensive-data-exploration-with-python\n", "total = train.isnull().sum().sort_values(ascending=False)\n", "percent = (train.isnull().sum()/train.isnull().count()).sort_values(ascending=False)\n", "missing_data_train = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n", "print(\"| Top Missing data in Training data|\")\n", "print(missing_data_train.head(20))\n", "\n", "##\n", "total = test.isnull().sum().sort_values(ascending=False)\n", "percent = (test.isnull().sum()/test.isnull().count()).sort_values(ascending=False)\n", "missing_data_test = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n", "print(\"| Top Missing data in Test data |\")\n", "print(missing_data_test.head(20))\n"]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "27131b50-5cca-45c2-b4d5-07ce625e215b", "_uuid": "5ce710a605f0a4b95488ab1f1fd27e13772356bd"}, "source": ["Hence we see that there is a 12 variables that have missing values. 3 of these source_screen_name, source_system_tab and source types are found in the training set. We should not remove these columns but rather find values to insert instead of the NaNs. However \n", "\n", "Let us study the columns in order to find replacements for the NaNs."]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "4e75dcfd-ff97-4bfa-a02f-3cd8b5c54d8e", "_uuid": "dc9563be42104a1d1742e62dcc93300dfb539c70"}, "source": ["# 4.1 Study the types of data "]}, {"cell_type": "code", "outputs": [], "execution_count": null, "metadata": {"collapsed": true, "_cell_guid": "282cce9f-9747-46ba-a29f-2a7247f919a2", "_uuid": "7a1a139e1d200b5a45a12eb5c2ad58e77d3aa727"}, "source": ["train_objects = train.columns.to_series().groupby(train.dtypes).groups\n", "\n", "kys = list(train_objects.keys())\n", "print(train_objects)\n", "train_objects[kys[0]]\n", "train_cpy = train\n", "#for col in train_cpy.columns.values:\n", "#    print(train_cpy[col].dtype)\n", "#    print(\"Col: \",col,\" Number of values: \",train_cpy[col].value_counts().shape[0])\n", "#    print(train_cpy[col].value_counts())"]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "04eda4e6-8f8f-4fcb-9d9c-a8bc696c5f6b", "_uuid": "dea488dfa0466d4291bbc8d9675dbe0c166dc25f"}, "source": ["After Inspection we have that the columns\n", "* msno: 30,755 ids (strings)\n", "* song_id: 359,966 song ids (strings)\n", "* source_system_tab: 8 unique objects (strings)\n", "* source_screen_name: 20 unique objects(strings)\n", "* source_type: 12 unique objects(strings)\n", "* target: 2 unique ints\n", "* city: 21 unique ints \n", "* bd: 92 unique ints\n", "* gender: 2 unqiue values (strings)\n", "* registered_via: 5 unqiue ints\n", "* registration_init_time: 3811 unique ints (dates)\n", "* expiration_date: 1395 unique ints (dates)\n", "* song_length: 50266 unique doubles\n", "* genre_ids: 572 unique strings, but cases of int | int ... | ... int as the music overlaps genres (string)\n", "* artist_name: 40,582 strings\n", "* composer: 76,064 unqque strings\n", "* lyricist: 33,888 unqiue strings\n", "* language: 10 unqiue doubles. One -1 value \n", "* name: 234,144 unique song names (strings)\n", "* isrc: 269,760 unique strings\n", "\n", "Hence we should (1) encode the string objects:\n", "* source_system_tab: 8 unique objects (strings)\n", "* source_screen_name: 20 unique objects(strings)\n", "* source_type: 12 unique objects(strings)\n", "* gender: 2 unqiue values (strings)\n", "*\u00a0artist_name: 40,582 strings\n", "* composer: 76,064 unqque strings\n", "* lyricist: 33,888 unqiue strings\n", "* name: 234,144 unique song names (strings)\n", "\n", "to integer values. Additionally turn:\n", "* registration_init_time: 3811 unique ints (dates)\n", "* expiration_date: 1395 unique ints (dates)\n", "to pd datetimes (2). \n", "\n", "And extract information from the isrc\n", "* isrc: 269,760 unique strings\n", "as the isrc is on the form  \"CC-XXX-YY-NNNNN\" where CC is the country code, XXX is the 3 character registrant code. YY last two digits of reference year. NNNNN 5 digit identifying the song. \n", "\n", "and seperate the genre ids:\n", "* genre_ids: 572 unique strings, but cases of int | int ... | ... int as the music overlaps genres (string)\n", "\n", " Hence we will end up with many new categorical variables"]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "3feb5ff0-dadd-4399-89fe-8eb05f1ef344", "_uuid": "34d343cfa97de396282b33958a11cae80e19dbe7"}, "source": ["# 5. Process the Data"]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "8f5550dd-3a16-4d1f-b261-7e7587aa8cec", "_uuid": "46bfec88002889894b3365d839a32ebfed59470d"}, "source": ["# 5.1 Artist_name splitting\n", "\n", "It would be interesting to split the artist name if there is featured artists in the artist_name. It might be that a user is a big fan of Future (which is featured on everything) and thus if he appears on a track this might increase the chance of a second listen.\n", "\n", "I have only began experimenting with this and unfortunately the workbook does not allow me to run the code to the end. Probably because of inefficient coding, sorry."]}, {"cell_type": "code", "outputs": [], "execution_count": null, "metadata": {"collapsed": true, "_cell_guid": "5f58c774-c35d-4326-8d2c-a760e8af5d53", "_uuid": "d324c0623f8325f08d618e8fa805c715fb862bb3"}, "source": ["## Experiment with the regular expressions on the artist name\n", "\n", "'''\n", "print(re.split(\"(\\w+)?[feat||]*(\\w+?[\\s\\w+])\",\"sia feat kanye|rkelly|lil wayne|beyonce\"))\n", "print(re.split(\"(\\w+)?[featuring|feat|feat.||]*(\\w+?[\\s\\w+])\",\"sia featuring kanye|rkelly|lil wayne|beyonce\"))\n", "print(re.split(\"[?\\s+]feat?uring[?\\s+]|\\|\",\"sia feat. kanye | lil wayne\"))\n", "\n", "fit_label = preprocessing.LabelEncoder().fit(['one','two','three'])\n", "print(fit_label.transform(['one','one']))\n", "'''"]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "a8e47e3b-12e0-422c-bcf2-7e41cb4f7a3e", "_uuid": "6a394769d9c1abbc4034716b31f8c69ff4b0cc05"}, "source": ["We start by encoding the strings objects to ints"]}, {"cell_type": "code", "outputs": [], "execution_count": null, "metadata": {"collapsed": true, "_cell_guid": "b550c3fd-a4a0-4b49-9a3f-2d9202a48095", "_uuid": "23aab3c42389792d3f227fb1d874ec0584b75e1f"}, "source": ["'''\n", "## Study the string objects\n", "train[\"artist_name\"] = train[\"artist_name\"].str.lower()\n", "counts = train[\"artist_name\"].value_counts()\n", "uniq = set(list((counts.index)))\n", "#print(uniq)\n", "#print(re.findall(\"feat|(s+) ?\\|\",\"sia feat kanye, rkelly\"))\n", "\n", "#Have | and feat seperating artists\n", "where = train[\"artist_name\"].str.find(\"feat\")>0 \n", "print(\"..\")\n", "split_train = train[\"artist_name\"].str.split(\"[?\\s+]feat?uring.[?\\s+]|\\|\",expand=True)\n", "print(\"..\")\n", "split_train = split_train.apply(lambda x: x.str.strip())\n", "uniq_names = list(pd.unique(split_train.values.ravel('K')))\n", "print(\"..\")\n", "la = split_train.apply(lambda x: [uniq_names.index(v) for v in x] ,axis = 1)\n", "print(\"..\")\n", "la.colnames = ['MainArtist'] + [\"FeatArtist\"+str(c) for c in la.colnames.values[1:]]\n", "print(la)\n", "\n", "train[la.colnames] = la\n", "''''''"]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "76e5867c-dfe7-40eb-ba09-1d8f76c18213", "_uuid": "f88b2c7d04af86b836d8c3382a14c9c6a9816564"}, "source": ["# 5.2 Code the categorical variables as categorical variables\n", "A no brainer this one. "]}, {"cell_type": "code", "outputs": [], "execution_count": null, "metadata": {"collapsed": true, "_cell_guid": "59719488-7a34-4341-a9ba-80267a283aed", "_uuid": "0ff7ad771fb1f96baa9d0c64f51dae2230bdb655"}, "source": ["string_objs = [\"source_system_tab\",\"source_screen_name\",'source_type','gender','artist_name',\n", "              'composer','lyricist','name']\n", "for col in string_objs:\n", "    train[col] = train[col].astype('category').cat.codes\n", "train_objects = train.columns.to_series().groupby(train.dtypes).groups\n", "print(train_objects)"]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "9b0f040b-6b03-4a40-88a1-3477943e47f8", "_uuid": "77ea171872e1c6a076396588f0fffd3e6d64e0a5"}, "source": ["Next step is to turn the datetimes into dates"]}, {"cell_type": "code", "outputs": [], "execution_count": null, "metadata": {"collapsed": true, "_cell_guid": "25fddc01-aa87-45f3-b115-b0b32e9018a1", "_uuid": "3758243c24005a737a62a86ca31bc3e78f389f12"}, "source": ["date_objs = ['registration_init_time','expiration_date']\n", "\n", "train['registration_init_time']=pd.to_datetime(train['registration_init_time'], format='%Y%m%d')\n", "train['expiration_date']=pd.to_datetime(train['expiration_date'], format='%Y%m%d')\n"]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "d5c8d42e-dd21-4566-b3c8-7426e4ad4d97", "_uuid": "932822445d90b151b174c1d87cedc74ef2855a84"}, "source": ["# 5.3 Extra Information from isrc code\n", "The isrc code is \" International Standard Recording Code (ISRC) is an international standard code for uniquely identifying sound recordings and music video recordings\" according to wikipedia. \n", "\n", "You can find an explanation to what the code contains and finally how to split it for every song. "]}, {"cell_type": "code", "outputs": [], "execution_count": null, "metadata": {"collapsed": true, "_cell_guid": "3c8a0352-79e0-4b38-8a4f-df195c30d7a2", "_uuid": "aa804efee5ac9ec6d7aa1e0114dfbf8e8b520d9f"}, "source": ["isrc = ['isrc']\n", "print(\"| Example extraction from ISRC |\")\n", "print('ISRC:      ',train[isrc].values[0][0])\n", "print('Country:   ',train[isrc].values[0][0][0:2])\n", "print('Reg code:  ',train[isrc].values[0][0][2:5])\n", "print('Year:      ',train[isrc].values[0][0][5:7])\n", "print('Unique id: ',train[isrc].values[0][0][7:])\n", "print(\"|------------------------------|\")\n", "#print(train['isrc'].str.extract('(.{2,2})' * 4))\n", "isrc_cols = ['isrc_c','isrc_rc','isrc_yr','isrc_ud']\n", "train['isrc_c'],train['isrc_rc'],train['isrc_yr'],train['isrc_ud'] = map(train['isrc'].str.slice, [0, 2, 5,7], [2, 5, 7,12])\n", "train[isrc_cols] = train[isrc_cols].fillna(value=-1)\n", "\n", "train['isrc_c'] = train['isrc_c'].astype('category').cat.codes\n", "train['isrc_rc'] = train['isrc_rc'].astype('category').cat.codes\n", "train['isrc_yr'] = train['isrc_yr'].astype(\"int\")\n", "train['isrc_ud'] =train['isrc_ud'].astype('category').cat.codes\n", "#print([i[0:2] for i in train[isrc].values])"]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "ec34e5d5-bc42-4562-bfe8-8c7b95144820", "_uuid": "ea53fcb3c22bed1336a361e927098e52dc985892"}, "source": ["# 5.4 Split the genre id into several genres and encode them \n", "This goes back to the \"artists featured on a song\". The format of the gendre_id data is genre1|genre2|....|genren and could therefore be split into n columns each a integer. So we don't need to categorise the data, neat!"]}, {"cell_type": "code", "outputs": [], "execution_count": null, "metadata": {"collapsed": true, "_cell_guid": "ce757a19-5460-4b42-aa0c-7738b7395ea4", "_uuid": "991cd855253257aa4e22dee44d7b1db5a7f110bd"}, "source": ["print(train['genre_ids'].value_counts())\n", "genres_new = train['genre_ids'].str.split('|', expand=True)\n", "genres_new = genres_new.fillna(value=-1)\n", "genres_new.columns = [\"genre_\"+str(c) for c in genres_new.columns]\n", "\n", "train[genres_new.columns] = genres_new.astype(\"int64\")\n"]}, {"cell_type": "code", "outputs": [], "execution_count": null, "metadata": {"collapsed": true, "_cell_guid": "0d4c7d02-0434-4151-8eef-667c1af43f92", "_uuid": "16a0b6087b9cca01b2c2342d55ea6a9ce062459a"}, "source": ["train_objects = train.columns.to_series().groupby(train.dtypes).groups\n", "print(train_objects)"]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "6da78469-1545-4183-a421-0f530515e629", "_uuid": "78803023502c7c8e5ab1f6db6a36e0c5700e1d0c"}, "source": ["So finally we have turned all the dtype('O') objects to categorical variables. Now we don't need to keep genre_ids and isrc so we remove these."]}, {"cell_type": "code", "outputs": [], "execution_count": null, "metadata": {"collapsed": true, "_cell_guid": "8648b9de-0f61-4d11-8d57-2ba9188fefd8", "_uuid": "a5ac3ac54afbfce7e1fd6561f9d13de0b02cf1cb"}, "source": ["train = train.drop(['genre_ids','isrc'],axis=1)\n", "print(train.columns.values)"]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "e1806069-0fd3-43e8-a4bb-5d9e8f2b0681", "_uuid": "a15a550e813f22337fd070e4e1778678912c09b4"}, "source": ["# 6. NaN handeling\n", "This is partly dealt with by setting variables with NaNs to -1. "]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "61cf6bb9-625f-4321-a886-8392f6102712", "_uuid": "875520875cae91c2e5ff056683c2f886f18e1976"}, "source": ["# 7.  Feature engineering\n", "Now that we have identified some variables that might help explain whether a user repetitively listenings to the same song within a month. \n", "\n", "The natural next step is to create new feratures based on intuition that mighrt help explain whether a user will listen to a song a second time or not. "]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "50304bb4-b69a-4444-b017-48178c7f1c53", "_uuid": "4d615e6d66a270898eab08a67e7250c865fb5f3f"}, "source": ["# 7.1 Calculate the time of the subscription\n", "\n", "The logic goes that if the user have had a subscription over a longer time this might increase the chances of listening to the same song over again. "]}, {"cell_type": "code", "outputs": [], "execution_count": null, "metadata": {"collapsed": true, "_cell_guid": "0d4506e3-f21d-4818-8fda-10d09d2cb9d1", "_uuid": "7f9da346403c3a01d1cd8ef0f49daa583666940e"}, "source": ["## Feature Engineering ###\n", "# Days since registration\n", "diff_days = (train['expiration_date']-train['registration_init_time']).dt.days\n", "train['diff_days'] = diff_days"]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "e0e80fbd-a928-4d5e-955a-253bcc8b3c82", "_uuid": "ed2abc429fe13cdb1a4ffe5d059670833c7bc89c"}, "source": ["# 7.2 Calculate the number of genres each song belongs to\n", "Maybe some perfer to listen to music that spans various genres or not."]}, {"cell_type": "code", "outputs": [], "execution_count": null, "metadata": {"collapsed": true, "_cell_guid": "28d2644d-1010-443e-b14e-5ed52b634e66", "_uuid": "e76965f2c453d83a0856522ed8ff2419ded3cdeb"}, "source": ["# Number of Genres \n", "train['num_genres'] = (train[genres_new.columns]!=-1).sum(axis=1)"]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "18f194c6-2da8-4b3f-847b-acf094626ec1", "_uuid": "f6dea476fb52628d807c50f11e579a8a0c9bc593"}, "source": ["# 7.3 Split the songs length into n bins\n", "If we divide into n=3 bins we could have short songs, medium length songs and long songs. Maybe a user finds it easier to listen to a short song a second time than a long one. "]}, {"cell_type": "code", "outputs": [], "execution_count": null, "metadata": {"collapsed": true, "_cell_guid": "aee517f6-49b7-4c35-8b29-b72bf70d428c", "_uuid": "5058b5265fcbc677ec7e60530849e97b6400ae38"}, "source": ["# Split song length\n", "train['song_length_bins'] = pd.qcut(train['song_length'],10,labels=range(10))"]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "1ef0fc84-3b7f-4204-a1a3-84de76e73400", "_uuid": "b3cece0f39be859a40ea449f824a0eba6fd4c3c2"}, "source": ["# 7.4 Split the ages into bins\n", "This is done under the assumption that the chance of listening to a song over again is greater in one age group than another. This is can be caused by the intuition that use frequency is greater amongs people in their 20s than in their 60s. But hey it can prove not to be the case!"]}, {"cell_type": "code", "outputs": [], "execution_count": null, "metadata": {"collapsed": true, "_cell_guid": "92d0c350-4cf0-48cc-8568-37c6c98c3b2b", "_uuid": "21b61d7b2487d0effd4441522c58b55beab0426f"}, "source": ["# Age bins\n", "\n", "## Adjust ages ##\n", "#print(train.loc[train['bd'] >90,'bd'])\n", "print(\"| Ages less than 16: \", train.loc[train['bd'] <-1,'bd'].shape[0])\n", "print(\"| Ages greater than 90: \", train.loc[train['bd'] >90,'bd'].shape[0])\n", "train.loc[train['bd'] >90,'bd'] = -1\n", "train.loc[train['bd'] <10,'bd']= -1\n", "#train['bd_age'] = pd.qcut(train['bd'],bins,labels=False)"]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "c113fb27-9c4e-435a-8ab6-a297e86c291c", "_uuid": "9ec8441a54441c637035b9771370b4323e40adc4"}, "source": ["# 8.0 Random person analysis\n", "Let us now look at one random (lucky) person and see how the original and created variables helps explain the chance of listening to a song twice within a month. "]}, {"cell_type": "code", "outputs": [], "execution_count": null, "metadata": {"collapsed": true, "_cell_guid": "c682be25-593a-4ebe-96e2-6b44b4909e70", "scrolled": true, "_uuid": "e9027d78983130fcf99b5b28ec9ebd3976a95f68"}, "source": ["var = 'artist_name'\n", "random_user = train['msno'].iloc[0]\n", "random_user_data = train[train['msno']==random_user]\n", "random_user_data = random_user_data.drop([\"msno\",\"song_id\",'registration_init_time', 'expiration_date'],axis=1)\n", "print(random_user_data.columns.values)\n", "print(\"| Correlation study |\")\n", "print(random_user_data.corrwith(random_user_data['target'],axis=0).sort_values(ascending=False))\n", "tar_0 = random_user_data[random_user_data['target']==0]\n", "tar_1 = random_user_data[random_user_data['target']==1]\n", "val_0_var =tar_0[var].value_counts()\n", "val_0_var = val_0_var/val_0_var.sum(axis=0)\n", "val_1_var = tar_1[var].value_counts()\n", "val_1_var = val_1_var/val_1_var.sum(axis=0)\n", "comp = pd.concat([val_0_var, val_1_var],axis=1,names=['0','1'])\n", "comp.columns = ['target 0','target 1']\n", "comp['absdiff'] = abs(comp['target 0'] - comp['target 1'])\n", "print()\n", "print(\"| \",var,\" vs. target |\")\n", "print(comp.sort_values(by=\"absdiff\",ascending=False))\n", "#plt.scatter(random_user_data[var],random_user_data['target'])\n", "print(train[var].value_counts())"]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "b059bdbf-1bed-4de0-bcb1-13b6a0f0cbae", "_uuid": "8e2dbefce99986310760b5671b3db4fcb56e9895"}, "source": ["# 8. Complete dataset\n", "Finally we have a complete dataset which is ready to be trained and tested. As one might observe I have not carried out the feature engineering for the test set. This should however be straight forward: just change train to train + test in the beginning and your off. \n", "\n", "\n"]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "0b6a32c7-1262-4656-abda-f106e1756573", "_uuid": "9fff31211dd852ecfffe9a82e159bebbed633867"}, "source": ["# 9 Modelling of the data\n", "This is not currently finished. "]}, {"cell_type": "code", "outputs": [], "execution_count": null, "metadata": {"collapsed": true, "_cell_guid": "4a1a030e-c014-4c7f-b082-a6fc163a3f27", "_uuid": "a47c2b2195e0fae88054449f4b1b9b3e9642e84c"}, "source": ["'''\n", "## Run V fold CV for XGBOOST ##\n", "kfold = 5\n", "skf = StratifiedKFold(n_splits=kfold, random_state=42)\n", "for i, (train_index, test_index) in enumerate(skf.split(X_data, y_data)):\n", "    print(\"fold \",i,\" of \",kfold)\n", "    X_train, X_valid = X_data[train_index, :], X_data[test_index, :]\n", "    y_train, y_valid = y_data[train_index],    y_data[test_index]\n", "    \n", "    \n", "    d_train = xgb.DMatrix(X_train, y_train,missing=-999.0)\n", "    d_valid = xgb.DMatrix(X_valid, y_valid,missing=-999.0)\n", "    #d_test = xgb.DMatrix(test_data)\n", "\n", "    watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n", "\n", "    # Model XGBOOST\n", "    print(\" - model\")\n", "    xgb_mdl = xgb.train(xgb_params, d_train,100, watchlist, early_stopping_rounds=100, feval='auc', maximize=True,\n", "                        verbose_eval=1)\n", "    print(\" - predict\")\n", "    xgb_predict = xgb_mdl.predict(xgb.DMatrix(X_valid),ntree_limit=xgb_mdl.best_ntree_limit)\n", "'''"]}], "metadata": {"language_info": {"pygments_lexer": "ipython3", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "name": "python", "nbconvert_exporter": "python", "version": "3.6.3", "file_extension": ".py"}, "kernelspec": {"name": "python3", "language": "python", "display_name": "Python 3"}}}