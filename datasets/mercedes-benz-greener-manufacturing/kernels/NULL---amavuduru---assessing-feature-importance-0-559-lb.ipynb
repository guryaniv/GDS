{"nbformat": 4, "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3", "language": "python"}, "language_info": {"version": "3.6.1", "nbconvert_exporter": "python", "file_extension": ".py", "pygments_lexer": "ipython3", "name": "python", "codemirror_mode": {"name": "ipython", "version": 3}, "mimetype": "text/x-python"}}, "cells": [{"execution_count": null, "source": "# A Straightforward Approach to Feature Selection\nIn this notebook, I will demonstrate how to measure feature importance using a random forest from scikit-learn with a built-in feature_importances_attribute. Note that this can also be done using XGBoost's built in method for plotting importances. A key difficulty in this competition is that the features are anonymized so feature engineering is quite difficult. So in this case the best approach may be to get rid of features that are not important.", "cell_type": "markdown", "metadata": {"collapsed": false, "_cell_guid": "49927c79-6e1d-4e36-8d71-da0c7fe25dfe", "_uuid": "d78031f339c3ef8e51a73583be7006fa91e54754", "_execution_state": "idle"}, "outputs": []}, {"execution_count": null, "source": "\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import LabelEncoder\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\nfrom sklearn import metrics, model_selection\n\n# Any results you write to the current directory are saved as output.", "cell_type": "code", "metadata": {"trusted": false, "_execution_state": "idle", "_cell_guid": "8d334c73-c815-415b-bd39-c44944bda01d", "_uuid": "e1b1d31c643daf0f8d48c7c5ab5dfb29fc5d9e85"}, "outputs": []}, {"execution_count": null, "source": "train = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\ntrain.head()", "cell_type": "code", "metadata": {"trusted": false, "collapsed": false, "_cell_guid": "0cc999ce-fd2d-427a-a753-441edd976c80", "_uuid": "c2eb52108431c3d2051ef2547ad327116c4522ae", "_execution_state": "idle"}, "outputs": []}, {"execution_count": null, "source": "Since several of our features come as string values, we will have to perform some kind of label-encoding so that we can fit a RandomForest to the data.", "cell_type": "markdown", "metadata": {"collapsed": false, "_cell_guid": "cad1585c-a7a5-401e-85c1-8b9d08111398", "_uuid": "43817832dce0b708c5a1e67b96b9a45ab6ea34d7", "_execution_state": "idle"}, "outputs": []}, {"execution_count": null, "source": "for c in train.columns:\n    if train[c].dtype == 'object':\n        lbl = LabelEncoder()\n        lbl.fit(list(train[c].values) + list(test[c].values))\n        train[c] = lbl.transform(list(train[c].values))\n        test[c] = lbl.transform(list(test[c].values))", "cell_type": "code", "metadata": {"trusted": false, "collapsed": false, "_cell_guid": "c9fb3b59-5cbc-45ee-9e12-21662a23adf7", "_uuid": "5b1aa939f81f86fb52ef3783d7c1c3592bf46e8e", "_execution_state": "idle"}, "outputs": []}, {"execution_count": null, "source": "X = train.drop(['ID', 'y'], axis=1)\ny = train['y']", "cell_type": "code", "metadata": {"trusted": false, "collapsed": false, "_cell_guid": "397ea93f-fe12-479d-b356-25d7b03fb134", "_uuid": "2f10509cedf85bbc53cc0ed2e91bfa7be42c3bff", "_execution_state": "idle"}, "outputs": []}, {"execution_count": null, "source": "feature_labels = X.columns #This will give us a list of all of the anonymized features", "cell_type": "code", "metadata": {"trusted": false, "collapsed": false, "_cell_guid": "40e55971-1e6a-4feb-b23b-eb6c928ff30f", "_uuid": "61a2601db0ed63220ecbdb5006f2e4fb2d4c31ee", "_execution_state": "idle"}, "outputs": []}, {"execution_count": null, "source": "forest = RandomForestRegressor(n_estimators=1000, random_state=0, n_jobs=-1)\n# 1000 trees sounds like a lot but setting n_jobs=-1 makes the process a lot faster since the\n# trees will be constructed in parallel", "cell_type": "code", "metadata": {"trusted": false, "collapsed": false, "_cell_guid": "9ebdb587-40b4-4bba-838e-101bccfd44cb", "_uuid": "b54f82652291866b58e5cb4d44a29bd44d9fbe3f", "_execution_state": "idle"}, "outputs": []}, {"execution_count": null, "source": "forest.fit(X, y)    # Fits the Random Forest Regressor to the entire data set.\nimportances = forest.feature_importances_  # Sets importances equal to the feature importances of the model", "cell_type": "code", "metadata": {"trusted": false, "collapsed": false, "_cell_guid": "7bcea038-480d-4967-b0c7-e419dc940e14", "_uuid": "7f3794621806ea1991decaee69b6b851c66f4374", "_execution_state": "idle"}, "outputs": []}, {"execution_count": null, "source": "The code below prints the normalized feature importances in descending order so we can see which features are the most important.", "cell_type": "markdown", "metadata": {"collapsed": false, "_cell_guid": "69fe688b-d796-4d75-bfaa-19816c8fcd0c", "_uuid": "e362b8f0eb0f19ecb3acf5e9dd57299a94887702", "_execution_state": "idle"}, "outputs": []}, {"execution_count": null, "source": "indices = np.argsort(importances)[::-1]\norder_features = []\norder_importances = []\nfor f in range(X.shape[1]):\n    print(\"%2d) %-*s %f\" % (f+1, 30, feature_labels[f], importances[indices[f]]))\n    order_features.append(feature_labels[f])\n    order_importances.append(importances[indices[f]])", "cell_type": "code", "metadata": {"trusted": false, "collapsed": false, "_cell_guid": "edac6664-da4d-4559-b724-f7a5076ed4b2", "_uuid": "ba8339924def10b1f2c8a77b7551ceee2374fede", "_execution_state": "idle"}, "outputs": []}, {"execution_count": null, "source": "plt.figure(figsize=(10,10))\ntop_50_importances = order_importances[:50] #This will give us the top 50 features by importance\nplt.title('Top 50 Features By Importance')\nplt.bar(range(X.shape[1]-326), top_50_importances, color='lightblue', align='center')\nplt.xticks(range(X.shape[1]-326), order_features[:50], rotation=90)\nplt.xlim([-1, X.shape[1]-326])\nplt.show()", "cell_type": "code", "metadata": {"trusted": false, "collapsed": false, "_cell_guid": "3ac28eb6-4afb-4588-af08-4a41d2519c94", "_uuid": "cedd9e2c193213f1b3e211817763814c0d706e08", "_execution_state": "idle"}, "outputs": []}, {"execution_count": null, "source": "test = test.drop(order_features[250:], axis=1)\ntrain = train.drop(order_features[250:], axis=1) # Modify train to only take in the top 100 features and the target column y", "cell_type": "code", "metadata": {"trusted": false, "collapsed": false, "_cell_guid": "54b2e2be-20ed-4e92-9eb7-5b0050715995", "_uuid": "7e6734a1d599403811fd1fe921ad5196bfa93e0b", "_execution_state": "idle"}, "outputs": []}, {"execution_count": null, "source": "from sklearn.base import BaseEstimator,TransformerMixin, ClassifierMixin\nimport xgboost as xgb\nfrom sklearn.linear_model import ElasticNetCV, LassoLarsCV\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.pipeline import make_pipeline, make_union\nfrom sklearn.utils import check_array\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.random_projection import GaussianRandomProjection\nfrom sklearn.random_projection import SparseRandomProjection\nfrom sklearn.decomposition import PCA, FastICA\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.metrics import r2_score", "cell_type": "code", "metadata": {"trusted": false, "collapsed": false, "_cell_guid": "6889b67f-705c-41f1-9129-06ee22b53317", "_uuid": "aadbc59ca5f6eab19d821df958a0a937187ea3e4", "_execution_state": "idle"}, "outputs": []}, {"execution_count": null, "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np\nfrom sklearn.base import BaseEstimator,TransformerMixin, ClassifierMixin\nfrom sklearn.preprocessing import LabelEncoder\nimport xgboost as xgb\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.linear_model import ElasticNetCV, LassoLarsCV\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.pipeline import make_pipeline, make_union\nfrom sklearn.utils import check_array\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.random_projection import GaussianRandomProjection\nfrom sklearn.random_projection import SparseRandomProjection\nfrom sklearn.decomposition import PCA, FastICA\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.metrics import r2_score\n\n\n\nclass StackingEstimator(BaseEstimator, TransformerMixin):\n    \n    def __init__(self, estimator):\n        self.estimator = estimator\n\n    def fit(self, X, y=None, **fit_params):\n        self.estimator.fit(X, y, **fit_params)\n        return self\n    def transform(self, X):\n        X = check_array(X)\n        X_transformed = np.copy(X)\n        # add class probabilities as a synthetic feature\n        if issubclass(self.estimator.__class__, ClassifierMixin) and hasattr(self.estimator, 'predict_proba'):\n            X_transformed = np.hstack((self.estimator.predict_proba(X), X))\n\n        # add class prodiction as a synthetic feature\n        X_transformed = np.hstack((np.reshape(self.estimator.predict(X), (-1, 1)), X_transformed))\n\n        return X_transformed", "cell_type": "code", "metadata": {"trusted": false, "collapsed": false, "_cell_guid": "8a878317-93bd-4a30-80ff-77779aca809b", "_uuid": "73c2e4c6ec78e0af3941c370b7d92038465e3dab", "_execution_state": "idle"}, "outputs": []}, {"execution_count": null, "source": "n_comp = 12\n\n# tSVD\ntsvd = TruncatedSVD(n_components=n_comp, random_state=420)\ntsvd_results_train = tsvd.fit_transform(train.drop([\"y\"], axis=1))\ntsvd_results_test = tsvd.transform(test)\n\n# PCA\npca = PCA(n_components=n_comp, random_state=420)\npca2_results_train = pca.fit_transform(train.drop([\"y\"], axis=1))\npca2_results_test = pca.transform(test)\n\n# ICA\nica = FastICA(n_components=n_comp, random_state=420)\nica2_results_train = ica.fit_transform(train.drop([\"y\"], axis=1))\nica2_results_test = ica.transform(test)\n\n# GRP\ngrp = GaussianRandomProjection(n_components=n_comp, eps=0.1, random_state=420)\ngrp_results_train = grp.fit_transform(train.drop([\"y\"], axis=1))\ngrp_results_test = grp.transform(test)\n\n# SRP\nsrp = SparseRandomProjection(n_components=n_comp, dense_output=True, random_state=420)\nsrp_results_train = srp.fit_transform(train.drop([\"y\"], axis=1))\nsrp_results_test = srp.transform(test)\n\n#save columns list before adding the decomposition components\n\nusable_columns = list(set(train.columns) - set(['y']))", "cell_type": "code", "metadata": {"trusted": false, "collapsed": false, "_cell_guid": "f5a2937d-5f5f-4b54-849d-122a3b2ad734", "_uuid": "fdd8e1fbf48ba23e67331a0a296c0da08c818461", "_execution_state": "idle"}, "outputs": []}, {"execution_count": null, "source": "for i in range(1, n_comp + 1):\n    train['pca_' + str(i)] = pca2_results_train[:, i - 1]\n    test['pca_' + str(i)] = pca2_results_test[:, i - 1]\n\n    train['ica_' + str(i)] = ica2_results_train[:, i - 1]\n    test['ica_' + str(i)] = ica2_results_test[:, i - 1]\n\n    train['tsvd_' + str(i)] = tsvd_results_train[:, i - 1]\n    test['tsvd_' + str(i)] = tsvd_results_test[:, i - 1]\n\n    train['grp_' + str(i)] = grp_results_train[:, i - 1]\n    test['grp_' + str(i)] = grp_results_test[:, i - 1]\n\n    train['srp_' + str(i)] = srp_results_train[:, i - 1]\n    test['srp_' + str(i)] = srp_results_test[:, i - 1]\n", "cell_type": "code", "metadata": {"trusted": false, "collapsed": false, "_cell_guid": "62006920-8346-4ce0-ab0c-2e6fe12e0002", "_uuid": "8bae4d2e83f55ebf7e4cfa71de98aee84fcc6b3a", "_execution_state": "idle"}, "outputs": []}, {"execution_count": null, "source": "y_train = train['y'].values\ny_mean = np.mean(y_train)\nid_test = test['ID'].values\n#finaltrainset and finaltestset are data to be used only the stacked model (does not contain PCA, SVD... arrays) \nfinaltrainset = train[usable_columns].values\nfinaltestset = test[usable_columns].values", "cell_type": "code", "metadata": {"trusted": false, "collapsed": false, "_cell_guid": "b505bb81-20ad-4bc2-aacf-af30064207ae", "_uuid": "283bbdda7093df223b0d997003f64a8a8912af9d", "_execution_state": "idle"}, "outputs": []}, {"execution_count": null, "source": "'''Train the xgb model then predict the test data'''\n\nxgb_params = {\n    'n_trees': 520, \n    'eta': 0.0045,\n    'max_depth': 4,\n    'subsample': 0.93,\n    'objective': 'reg:linear',\n    'eval_metric': 'rmse',\n    'base_score': y_mean, # base prediction = mean(target)\n    'silent': 1\n}\n\n\ndtrain = xgb.DMatrix(train.drop('y', axis=1), y_train)\ndtest = xgb.DMatrix(test)\n\nnum_boost_rounds = 1250\n# train model\nmodel = xgb.train(dict(xgb_params, silent=0), dtrain, num_boost_round=num_boost_rounds)\ny_pred = model.predict(dtest)\n", "cell_type": "code", "metadata": {"trusted": false, "collapsed": false, "_cell_guid": "19e46fa8-fc9c-4529-9637-c402637a252e", "_uuid": "cd3aa46c32bbe8c5573aab01e18c4a2f4c9ba884", "_execution_state": "idle"}, "outputs": []}, {"execution_count": null, "source": "'''Train the stacked models then predict the test data'''\n\nstacked_pipeline = make_pipeline(\n    StackingEstimator(estimator=ElasticNetCV(normalize=True)),\n    StackingEstimator(estimator=GradientBoostingRegressor(learning_rate=0.001, loss=\"huber\", max_depth=5, max_features=0.55, min_samples_leaf=18, min_samples_split=14, subsample=0.7, n_estimators=200)),\n    ElasticNetCV()\n\n)\n\nstacked_pipeline.fit(finaltrainset, y_train)\nresults = stacked_pipeline.predict(finaltestset)\n\n'''R2 Score on the entire Train data when averaging'''\n\nprint('R2 score on train data:')\nprint(r2_score(y_train,stacked_pipeline.predict(finaltrainset)*0.2855 + model.predict(dtrain)*0.7145))\n\n\n\n'''Average the predictions test data of both models then save it on a csv file'''\n\nprint('Cross Validation')\nprint('................')\n\nn_folds = 5\nkf = model_selection.StratifiedKFold(n_splits=n_folds, random_state=1, shuffle=True)\n\nX = train.drop('y', axis=1).values\ny = train['y'].values\n\n\nfold = 0\nfor train_index, test_index in kf.split(X, y):\n    fold += 1\n    \n    X_training, X_valid = X[train_index], X[test_index]\n    y_training, y_valid = y[train_index], y[test_index]\n    \n    finaltrainset = train[usable_columns].values\n    final_train, final_valid = finaltrainset[train_index], finaltrainset[test_index]\n    \n    print(\"Fold\", fold, X_training.shape, X_valid.shape)\n    \n    print('Fitting XGBoost for Fold {}'.format(fold))\n    dtrain = xgb.DMatrix(X_training, y_training)\n    dtest = xgb.DMatrix(X_valid)\n    model = xgb.train(dict(xgb_params, silent=0), dtrain, num_boost_round=num_boost_rounds)\n    \n    print('Fitting stacked pipeline for Fold {}'.format(fold))\n    stacked_pipeline.fit(final_train, y_training)\n    \n    print(r2_score(y_valid,stacked_pipeline.predict(final_valid)*0.2855 + model.predict(dtest)*0.7145))\n\n    \nsub = pd.DataFrame()\nsub['ID'] = id_test\nsub['y'] = y_pred*0.75 + results*0.25\nsub.to_csv('stacked-models.csv', index=False)", "cell_type": "code", "metadata": {"trusted": false, "collapsed": false, "_cell_guid": "b82fbe6f-d59f-43c1-a925-a1228f0814b9", "_uuid": "6aa4382ba8b720b48cfd67a3f678b779865f7762", "_execution_state": "busy"}, "outputs": []}, {"execution_count": null, "source": "", "cell_type": "code", "metadata": {"trusted": false, "collapsed": false, "_cell_guid": "83694e02-9c94-4f6b-86d5-150eb2eea3b6", "_uuid": "d02f7137591c24369e5c05154fce81fc893131d9", "_execution_state": "idle"}, "outputs": []}], "nbformat_minor": 0}