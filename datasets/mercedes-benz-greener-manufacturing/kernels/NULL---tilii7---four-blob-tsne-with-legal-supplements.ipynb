{"cells": [{"outputs": [], "metadata": {"collapsed": false, "_cell_guid": "66cbe259-1289-47ea-8a12-b17ef777b223", "_execution_state": "idle", "_uuid": "d978d108a30a95cc5a929835a5d9555f10a25dcc"}, "cell_type": "markdown", "execution_count": null, "source": "There are four clusters when one plots train 'y' with train prediction. In Scirpus' [__original script__][1] this is pretty clear, yet the color-coding for y-values doesn't show all that well.\n\n\n  [1]: https://www.kaggle.com/scirpus/four-blob-tsne \"original script\""}, {"outputs": [], "metadata": {"collapsed": false, "_cell_guid": "a9924fce-3d76-4c0c-8134-aac8a34f17e1", "_execution_state": "idle", "trusted": false, "_uuid": "ff5d87f81a051dfa5be4ecd055186914f499b3da"}, "cell_type": "code", "execution_count": null, "source": "import numpy as np \nimport pandas as pd \nfrom sklearn.manifold import TSNE\nfrom sklearn.metrics import r2_score\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\n%matplotlib inline"}, {"outputs": [], "metadata": {"collapsed": false, "_cell_guid": "0716f5b5-8d6b-476f-a400-ca56d0ba2fe4", "_execution_state": "idle", "_uuid": "b66770648cb1ac3485f4af14520575b42d03a827"}, "cell_type": "markdown", "execution_count": null, "source": "Different color map.\n"}, {"outputs": [], "metadata": {"collapsed": false, "_cell_guid": "753acfa0-d165-4c6f-ad38-aec8fb701b0e", "_execution_state": "idle", "trusted": false, "_uuid": "841dbcea467eaa1cb8162c4aa5fccaef7b07d1c3"}, "cell_type": "code", "execution_count": null, "source": "cm = plt.cm.get_cmap('RdYlBu')"}, {"outputs": [], "metadata": {"collapsed": false, "_cell_guid": "6f75e2df-874c-4ecb-8688-3104be32766c", "_execution_state": "idle", "_uuid": "0809c2f42578e2d241b930c2526bfc88ee800797"}, "cell_type": "markdown", "execution_count": null, "source": "Five new features were added: X29, X48, X232, X236 and X263. All of them were found by genetic programming, just like the original set of Scirpus' features. I think this is justified as the scores will be better in the end. __X48 and X236 were subsequently removed.__"}, {"outputs": [], "metadata": {"_cell_guid": "3e840080-98cb-4711-8344-96f854884c1d", "_uuid": "2c4e6d1e77862d63e53a7868eb8545b43614e521", "_execution_state": "idle", "trusted": false}, "cell_type": "code", "execution_count": null, "source": "features = ['X118',\n            'X127',\n            'X47',\n            'X315',\n            'X311',\n            'X179',\n            'X314',\n### added by Tilii\n            'X232',\n            'X29',\n            'X263',\n###\n            'X261']"}, {"outputs": [], "metadata": {"collapsed": false, "_cell_guid": "481f8bfa-433a-4d1c-8011-f14d9235b59f", "_execution_state": "idle", "_uuid": "71a48abb76eb5d71fa2d92f256fece150e3592c8"}, "cell_type": "markdown", "execution_count": null, "source": "In Scirpus' [__original script__][1] the whole y-range is used, so the color-coding gets stretched because of the >250 outlier. Therefore, most of the y-values end up in the bottom half of the color range and the whole plot is just various shades of blue that are difficult to tell apart. In this script I clip y-values so that everything above 130 will be the same shade of BLUE.\n\n\n  [1]: https://www.kaggle.com/scirpus/four-blob-tsne \"original script\""}, {"outputs": [], "metadata": {"collapsed": false, "_cell_guid": "b09d48c6-ac76-4f77-883f-fd37fa500479", "_execution_state": "idle", "trusted": false, "_uuid": "6517d2975bd87e242cf59b0c2439b91a2342707f"}, "cell_type": "code", "execution_count": null, "source": "train = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\ny_clip = np.clip(train['y'].values, a_min=None, a_max=130)"}, {"outputs": [], "metadata": {"collapsed": false, "_cell_guid": "d7397551-1c29-46cb-a77c-c43fd6b56253", "_execution_state": "idle", "trusted": false, "_uuid": "9a4c29fc611ebec3825d1de01b54d5e0ba2ede1b"}, "cell_type": "code", "execution_count": null, "source": "tsne = TSNE(random_state=2016,perplexity=50,verbose=2)\nx = tsne.fit_transform(pd.concat([train[features],test[features]]))"}, {"outputs": [], "metadata": {"collapsed": false, "_cell_guid": "2a342186-b38a-4fff-971f-0cba65163763", "_execution_state": "idle", "trusted": false, "_uuid": "e47da2eee068d140bc6caa48b87b8e88cb5b06c8"}, "cell_type": "code", "execution_count": null, "source": "plt.figure(figsize=(12,10))\n# plt.scatter(x[train.shape[0]:,0],x[train.shape[0]:,1], cmap=cm, marker='.', s=15, label='test')\ncb = plt.scatter(x[:train.shape[0],0],x[:train.shape[0],1], c=y_clip, cmap=cm, marker='o', s=15, label='train')\nplt.colorbar(cb)\nplt.legend(prop={'size':15})\n#plt.title('t-SNE embedding of train & test data', fontsize=20)\nplt.title('t-SNE embedding of train data', fontsize=20)\nplt.savefig('four-blob-tSNE-01.png')\n"}, {"outputs": [], "metadata": {"collapsed": false, "_cell_guid": "02336a75-e4ce-4d71-8d02-7411dd61ea3f", "_execution_state": "idle", "_uuid": "fbf579015d40d97c92fb61caf80d5a86d712553c"}, "cell_type": "markdown", "execution_count": null, "source": "Pretty sure that the plot in this notebook will not look the same as my local plot. Not being paranoid - this is based on my experience with t-SNE Kaggle implementation from this [__script__][1]. Anyway, this is how my local plot looks like using the same script as in this notebook.\n\n![__t-SNE on raw data__][2]\n\nGiven that this t-SNE embedding was done on raw data, I think it compares quite nicely with the t-SNE embedding below which was done after neural network training. Four clusters should be obvious in both plots. One could even argue that there is a small 5th cluster.\n\n![__t-SNE after neural network training__][3]\n\n\n  [1]: https://www.kaggle.com/tilii7/you-want-outliers-we-got-them-outliers\n  [2]: https://i.imgur.com/JpmDztu.png\n  [3]: https://i.imgur.com/wRuOZkO.png"}, {"outputs": [], "metadata": {"collapsed": false, "_cell_guid": "c652415f-1b97-4b0b-9c7e-54edd4fa12a4", "_execution_state": "idle", "trusted": false, "_uuid": "6b662af8ed92fa82c37b5b6805a58e439f06f017"}, "cell_type": "code", "execution_count": null, "source": "from sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.model_selection import KFold"}, {"outputs": [], "metadata": {"collapsed": false, "_cell_guid": "18c17935-b787-44cd-8b2e-91a31b2c5ba3", "_execution_state": "idle", "trusted": false, "_uuid": "b5a321691f70ab9ab46988f59d5f21d44e38593f"}, "cell_type": "code", "execution_count": null, "source": "score = 0\nsplits = 5\nkf = KFold(n_splits=splits)\ny = train.y.ravel()\nfor train_index, test_index in kf.split(range(train.shape[0])):\n    blind = x[:train.shape[0]][test_index]\n    vis = x[:train.shape[0]][train_index]\n    knn = KNeighborsRegressor(n_neighbors=80,weights='uniform',p=2)\n    knn.fit(vis,y[train_index])\n    score +=(r2_score(y[test_index],(knn.predict(blind))))\nprint(score/splits)"}, {"outputs": [], "metadata": {"collapsed": false, "_cell_guid": "c67313ca-2117-43d3-806f-38b69b823988", "_execution_state": "idle", "trusted": false, "_uuid": "2966485e38be7bb46895a5843353d193966f8725"}, "cell_type": "code", "execution_count": null, "source": "score = 0\nsplits = 5\nkf = KFold(n_splits=splits)\ny = train.y.ravel()\nfor train_index, test_index in kf.split(range(train.shape[0])):\n    blind = train[features].loc[test_index]\n    vis = train[features].loc[train_index]\n    knn = KNeighborsRegressor(n_neighbors=80,weights='uniform',p=2)\n    knn.fit(vis,y[train_index])\n    score +=(r2_score(y[test_index],(knn.predict(blind))))\nprint(score/splits)"}], "metadata": {"kernelspec": {"display_name": "Python 3", "name": "python3", "language": "python"}, "language_info": {"version": "3.6.1", "nbconvert_exporter": "python", "file_extension": ".py", "pygments_lexer": "ipython3", "mimetype": "text/x-python", "name": "python", "codemirror_mode": {"version": 3, "name": "ipython"}}}, "nbformat": 4, "nbformat_minor": 0}