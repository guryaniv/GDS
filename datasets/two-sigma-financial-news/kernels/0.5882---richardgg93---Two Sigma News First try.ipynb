{
  "cells": [
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport datetime \n\nimport sklearn # ML\n\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.datasets import make_regression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error\n\nimport matplotlib.pyplot as plt\n\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\nfrom kaggle.competitions import twosigmanews\n\n# Any results you write to the current directory are saved as output.",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Retreive the environment of the competition\n# You can only call make_env() once, so don't lose it!\nenv = twosigmanews.make_env()\nprint('Data loaded!')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b5812371bff3ef083ea912bc8efc158b925156b6"
      },
      "cell_type": "code",
      "source": "# Retrieve all training data\n(market_train_df, news_train_df) = env.get_training_data()\nprint(\"Fetching training data finished... \")\nprint('Data obtained!')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a6788a5c8bac47eba68a79caa7897243470c17a2"
      },
      "cell_type": "code",
      "source": "# Preprocessing\n# Reorder universe so its right after assetName\n# cols=market_train_df.columns.tolist()\n# cols=cols[:3]+[cols[-1]]+cols[3:-1]\n# market_train_df = market_train_df[cols]\n# Remove universe, as it doesnt exist on the pred data\nmarket_train_df.drop(['universe'], axis=1, inplace=True)\n# Adding daily difference\nnew_col = market_train_df[\"close\"] - market_train_df[\"open\"]\nmarket_train_df.insert(loc=6, column=\"daily_diff\", value=new_col)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d5de254d7b6d9d055a82c621d036cc9d4ad15340"
      },
      "cell_type": "code",
      "source": "# Market data analysis\n# Types of the columns\nmarket_train_df.dtypes",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4aaca48059249ab7fa3708dbef402a8369b6fbfa"
      },
      "cell_type": "code",
      "source": "# Example of the columns\nmarket_train_df.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "36211bb5b878c4526c3f761825e5ef33b7d79067"
      },
      "cell_type": "code",
      "source": "# Variables description\nmarket_train_df.describe()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3e987f2249d053c0b0fd2c37948d2fcb000dcc9f"
      },
      "cell_type": "code",
      "source": "# Correlation between the numericals (except universe)\n# Note that this removes the null values from the computation\nmarket_train_df.iloc[:, 3:].corr(method='pearson')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5a05c364bf0f55933668ad1265d61ed7401f5a15"
      },
      "cell_type": "code",
      "source": "# Lets analyze the assets\nassets=market_train_df[\"assetCode\"].unique()\nprint(len(assets))\nprint(assets)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "94a579b53d082190922c1397f48d9c0415404d47"
      },
      "cell_type": "code",
      "source": "# Lets analyze further the target variable\n# Very big outliers, lets see their number and distribution\nfig, axes = plt.subplots(3,2, figsize=(20, 12)) # create figure and axes\nprint(\"# Rows with |value| > 1 =\", market_train_df[market_train_df[\"returnsOpenNextMktres10\"].abs()>1].shape[0])\nprint(\"# Rows with |value| > 0.5 =\", market_train_df[market_train_df[\"returnsOpenNextMktres10\"].abs()>0.5].shape[0])\nprint(\"# Rows with |value| > 0.25 =\", market_train_df[market_train_df[\"returnsOpenNextMktres10\"].abs()>0.25].shape[0])\nprint(\"# Rows with |value| > 0.1 =\", market_train_df[market_train_df[\"returnsOpenNextMktres10\"].abs()>0.1].shape[0])\n\n# Boxplot with all values\nmarket_train_df.boxplot(column=\"returnsOpenNextMktres10\", ax=axes.flatten()[0])\naxes.flatten()[0].set_xlabel('Boxplot with all values', fontsize=18)\n# Removing rows with outliers (bigger or smaller than 1)\nmarket_train_df[market_train_df[\"returnsOpenNextMktres10\"].abs()<1].boxplot(column=\"returnsOpenNextMktres10\", ax=axes.flatten()[1])\naxes.flatten()[1].set_xlabel('Boxplot with values such that |val| < 1', fontsize=18)\n# Removing rows with outliers (bigger or smaller than 0.5)\nmarket_train_df[market_train_df[\"returnsOpenNextMktres10\"].abs()<0.5].boxplot(column=\"returnsOpenNextMktres10\", ax=axes.flatten()[2])\naxes.flatten()[2].set_xlabel('Boxplot with values such that |val| < 0.5', fontsize=18)\n# Removing rows with outliers (bigger or smaller than 0.25)\nmarket_train_df[market_train_df[\"returnsOpenNextMktres10\"].abs()<0.25].boxplot(column=\"returnsOpenNextMktres10\", ax=axes.flatten()[3])\naxes.flatten()[3].set_xlabel('Boxplot with values such that |val| < 0.25', fontsize=18)\n# Removing rows with outliers (bigger or smaller than 0.1)\nmarket_train_df[market_train_df[\"returnsOpenNextMktres10\"].abs()<0.1].boxplot(column=\"returnsOpenNextMktres10\", ax=axes.flatten()[4])\naxes.flatten()[4].set_xlabel('Boxplot with values such that |val| < 0.1', fontsize=18)\n# Distribution of the target value (not including values bigger or smaller than 1)\nmarket_train_df[market_train_df[\"returnsOpenNextMktres10\"].abs()<0.25].hist(column=\"returnsOpenNextMktres10\", bins=100, ax=axes.flatten()[5])\naxes.flatten()[5].set_xlabel('Histogram for values such that |val| < 0.25', fontsize=18)\nprint(\"The variable is actually centered in 0 and only a few outliers higher than 0.1. This makes sense considering that the returns of the \\\nmarket for 10 days are really small. Our goal then should be to detect those times in which the wins or loses are really high by making \\\nuse of the news. A good approach for this could be an algorithm to control the small temporal oscilation of the market and then use the news \\\nto detect those imprevisible changes.\")\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "88f83ed2b535175cee54224582379415a4cc962b"
      },
      "cell_type": "code",
      "source": "# Number of null values\nmarket_train_df.isna().sum()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "89ece90896cb3b9bf3b66ac5b71b7962c4a938c2"
      },
      "cell_type": "code",
      "source": "# Where are those null values?\nrows_with_null=market_train_df[pd.isnull(market_train_df).any(axis=1)]\ndates_with_null=rows_with_null[\"time\"].unique()\nnulls_per_date=[rows_with_null[rows_with_null[\"time\"]==d].shape[0] for d in dates_with_null]\ncaca=pd.DataFrame({'date': dates_with_null, 'nulls': nulls_per_date })\ncaca",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6e6b3511fa48a46bb499feafe4848ab6674a1b8d"
      },
      "cell_type": "code",
      "source": "# Where are those null values?\nrows_with_null=market_train_df[pd.isnull(market_train_df).any(axis=1)]\nassets_with_null=rows_with_null[\"assetCode\"].unique()\nnulls_per_asset=[rows_with_null[rows_with_null[\"assetCode\"]==a].shape[0] for a in assets_with_null]\ncaca=pd.DataFrame({'asset': assets_with_null, 'nulls': nulls_per_asset})\ncaca.sort_values(by=['nulls'], ascending=False, inplace=True)\ncaca",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "aa58fc0ac68c9654850910d59726885bce60af87"
      },
      "cell_type": "code",
      "source": "# Possibilities to deal with missing values:\n# Remove the rows with missing values\n# We cannot follow this approach to predict, just to be sure while training\n#market_train_df.dropna(inplace=True)\n# Remove the cols with missing values\n# We cannot follow this approach to predict\n# What if the cols with missing vals are different? Or if those are relevant?\n# market_train_df.dropna(inplace=True, axis=1)\n# Fill in with shitty values such as -99999\n# Tricky, but model can learn it, overall decission trees\n# ... we will do this with sklearn\n# market_train_df.isna().sum()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d299d22b2b8cadd3fb12bfeb735babb9f695fe6f"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e6b65d1f518a12f738d06b3d9d34bff34674ee91"
      },
      "cell_type": "code",
      "source": "# News data analysis",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d5588128c77bda8a2acfcb9b3aba2621ec08bced"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6ccaec72fe64ef77a11fde9afe0fdd3845a90fb9"
      },
      "cell_type": "code",
      "source": "# Correlation of news data with our target\n# pd.merge()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "cedc43b8badcdf3929d986d3a9497ddb416e58d8"
      },
      "cell_type": "code",
      "source": "# Toy prediction example\n# Remove outliers to make some tests\nF = market_train_df[market_train_df[\"returnsOpenNextMktres10\"].abs()<0.25]\n# Imputer to remove nans\nimp = SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=-9999.99)\nT = pd.DataFrame(imp.fit_transform(F), columns=F.columns)\nT",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6796741549504b0139b28b80910328f0b3257d33"
      },
      "cell_type": "code",
      "source": "# Define data to use for X and y\nn = 1000000\nX = T.iloc[:n, 3:-1]\ny = T[T.columns[-1]][:n]\nprint(X.shape, y.shape)\n# Split into train and test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, shuffle=False)\n\n# Save cols order for the prediction data\ncols_order=X_train.columns\n\n# Predict\nregr = RandomForestRegressor(n_jobs=-1, max_depth=20, random_state=0, n_estimators=20)\nregr.fit(X_train, y_train)\ny_predicted=regr.predict(X_test)\nprint(\"Acabó\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "58fa0f100804a1c4fa40ee74c76bb8911509fd99"
      },
      "cell_type": "code",
      "source": "mean_absolute_error(y_test, y_predicted)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "37ddcf28d2b2c17640b975ea04a62ea17c15e51e"
      },
      "cell_type": "code",
      "source": "for i in range(X.shape[1]):\n    print(\"%s (%f)\" % (X.columns[i], regr.feature_importances_[i]))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "28c7ef03e78ae1f74d1c57efabb17c36d58ddae7"
      },
      "cell_type": "code",
      "source": "df_results = X_test\ndf_results.insert(loc=df_results.shape[1], column=\"y_real\", value=y_test)\ndf_results.insert(loc=df_results.shape[1], column=\"y_pred\", value=y_predicted)\ndf_results.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "af44ac0b6bcf7d9c182bd29d63035e3d781871b2"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ddd64b4a29f72dd39309ae899b309033354eecb3"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d43bc6685c54d3f05e667cc5c68963f9fddfbea3"
      },
      "cell_type": "code",
      "source": "# Retrieve all days to iterate through\n# You can only iterate through a result from `get_prediction_days()` once\n# so be careful not to lose it once you start iterating.\ndays = env.get_prediction_days()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8ead937c7e5621ae0e069f2fb166b363c252e353"
      },
      "cell_type": "code",
      "source": "def make_random_predictions(predictions_df):\n    predictions_df.confidenceValue = 2.0 * np.random.rand(len(predictions_df)) - 1.0",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "db387a58b542558d449c069af8c26845ace5c824"
      },
      "cell_type": "code",
      "source": "def rfr_predictions(market, news, predictions_template_df):\n    print(market[\"time\"][0])\n    copy=market.copy()\n    # Adding daily difference\n    new_col = copy[\"close\"] - copy[\"open\"]\n    copy.insert(loc=6, column=\"daily_diff\", value=new_col)\n    # Getting columns used on the training only and reorder\n    copy=copy[cols_order]\n    # Replacing missing values\n    copy = pd.DataFrame(imp.fit_transform(copy), columns=copy.columns)\n    # Predicting\n    y_predicted=regr.predict(copy)\n    mn=min(y_predicted)\n    mx=max(y_predicted)\n    # Converting into the confidence value, from -1 to 1\n    predictions_template_df.confidenceValue = [((y-(-0.25))/(0.25-(-0.25))*2-1) for y in y_predicted]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "73b9b54d5f7766f4ddeb72225d781bb5cc775328"
      },
      "cell_type": "code",
      "source": "# Generate the predictions\nfor (market_obs_df, news_obs_df, predictions_template_df) in days:\n    rfr_predictions(market_obs_df, news_obs_df, predictions_template_df)\n    env.predict(predictions_template_df)\nprint('Prediction finished!')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9c5f59bf9c2fe94319236c8c92c19cc560642130"
      },
      "cell_type": "code",
      "source": "#env.predict(predictions_template_df)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b63e0f6f54815dde268f1c83434ab2ce045a6ca4"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "78aeb2a6fd2e106110e54a89eea40b45a983e4e7"
      },
      "cell_type": "code",
      "source": "# Write submission file\n# Note that for submitting the results we have to commit and then upload the resulting csv file\nenv.write_submission_file()\nprint([filename for filename in os.listdir('.') if '.csv' in filename])\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4a00350200e37ed3893ecc44c8bb2d031a517092"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}