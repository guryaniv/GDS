{"cells": [{"cell_type": "markdown", "source": ["# Kaggle Toxic Comments Challenge"], "metadata": {}}, {"cell_type": "code", "source": ["import pandas as pd\n", "import numpy as np\n", "import gensim\n", "import keras"], "outputs": [], "execution_count": null, "metadata": {}}, {"cell_type": "markdown", "source": ["## Data Loading"], "metadata": {}}, {"cell_type": "code", "source": ["df = pd.read_csv('../input/train.csv',index_col='id')\n", "df_test = pd.read_csv('../input/test.csv', index_col = 'id')"], "outputs": [], "execution_count": null, "metadata": {}}, {"cell_type": "code", "source": ["df_test.isnull().sum()"], "outputs": [], "execution_count": null, "metadata": {}}, {"cell_type": "code", "source": ["df_test.fillna('', inplace = True)"], "outputs": [], "execution_count": null, "metadata": {"collapsed": true}}, {"cell_type": "code", "source": ["df.head()"], "outputs": [], "execution_count": null, "metadata": {}}, {"cell_type": "code", "source": ["df.shape"], "outputs": [], "execution_count": null, "metadata": {}}, {"cell_type": "code", "source": ["df.info()"], "outputs": [], "execution_count": null, "metadata": {}}, {"cell_type": "markdown", "source": ["## Preprocessing"], "metadata": {}}, {"cell_type": "code", "source": ["simple_tokens = df.comment_text.apply(gensim.utils.simple_preprocess)"], "outputs": [], "execution_count": null, "metadata": {"collapsed": true}}, {"cell_type": "code", "source": ["simple_tokens"], "outputs": [], "execution_count": null, "metadata": {}}, {"cell_type": "code", "source": ["df[df.index==999898414104]['comment_text'] # this is the actual conversion : [it, staying, let, move, on, corbett]"], "outputs": [], "execution_count": null, "metadata": {}}, {"cell_type": "code", "source": ["phrases = gensim.models.phrases.Phrases(simple_tokens)\n", "tokenizer = gensim.models.phrases.Phraser(phrases)"], "outputs": [], "execution_count": null, "metadata": {}}, {"cell_type": "code", "source": ["tokenized_text = list(tokenizer[simple_tokens]) # a 2D list of all the keywords from comment_text"], "outputs": [], "execution_count": null, "metadata": {}}, {"cell_type": "code", "source": ["tokenized_text[0]"], "outputs": [], "execution_count": null, "metadata": {}}, {"cell_type": "code", "source": ["corpus_dict = gensim.corpora.dictionary.Dictionary(tokenized_text)"], "outputs": [], "execution_count": null, "metadata": {"collapsed": true}}, {"cell_type": "code", "source": ["TARGET_CLASSES = df.columns[1:]"], "outputs": [], "execution_count": null, "metadata": {}}, {"cell_type": "code", "source": ["TARGET_CLASSES"], "outputs": [], "execution_count": null, "metadata": {}}, {"cell_type": "code", "source": ["targets = df[TARGET_CLASSES].values"], "outputs": [], "execution_count": null, "metadata": {}}, {"cell_type": "markdown", "source": ["## Analysis using seaborn"], "metadata": {}}, {"cell_type": "code", "source": ["import seaborn as sns\n", "import matplotlib.pyplot as plt\n", "%matplotlib inline"], "outputs": [], "execution_count": null, "metadata": {"collapsed": true}}, {"cell_type": "code", "source": ["sns.distplot([len(doc) for doc in tokenized_text], bins=100, kde=False, label='Number of tokens per comment.')\n", "plt.xlabel(\"Tokens in a comment\")\n", "plt.ylabel(\"Frequency\")\n", "plt.xlim((0, 400))"], "outputs": [], "execution_count": null, "metadata": {}}, {"cell_type": "markdown", "source": ["## Training word2vec on comment data"], "metadata": {}}, {"cell_type": "code", "source": ["word2vec = gensim.models.word2vec.Word2Vec(tokenized_text, window=5, size=100, min_count=2, workers=6)"], "outputs": [], "execution_count": null, "metadata": {"collapsed": true}}, {"cell_type": "code", "source": ["word2vec.wv.most_similar('popularity')"], "outputs": [], "execution_count": null, "metadata": {}}, {"cell_type": "code", "source": ["word2vec.wv.most_similar('idiot')"], "outputs": [], "execution_count": null, "metadata": {}}, {"cell_type": "markdown", "source": ["## word2vec-based based model."], "metadata": {}}, {"cell_type": "code", "source": ["features = np.zeros((len(tokenized_text), word2vec.vector_size))\n", "for i, tokens in enumerate(tokenized_text):\n", "    tokens = [t for t in tokens if t in word2vec.wv.vocab]\n", "    if tokens:\n", "        features[i, :] = np.mean([word2vec.wv[t] / word2vec.wv.vocab[t].count for t in tokens], axis=0)"], "outputs": [], "execution_count": null, "metadata": {"collapsed": true}}, {"cell_type": "code", "source": ["from keras.models import Sequential\n", "from keras.layers import Dense\n", "from keras.optimizers import Adam\n", "\n", "model = Sequential()\n", "model.add(Dense(256, activation='relu', input_shape=(word2vec.vector_size,)))\n", "model.add(Dense(128, activation='relu'))\n", "model.add(Dense(len(TARGET_CLASSES), activation='sigmoid'))\n", "model.compile(optimizer=Adam(lr=0.001), loss='binary_crossentropy', metrics=['accuracy'])"], "outputs": [], "execution_count": null, "metadata": {"collapsed": true}}, {"cell_type": "code", "source": ["model.fit(features, targets, epochs=10, validation_split=0.1)"], "outputs": [], "execution_count": null, "metadata": {}}, {"cell_type": "code", "source": ["# serialize model to YAML\n", "model_yaml = model.to_yaml()\n", "with open(\"model-baseline.yaml\", \"w\") as yaml_file:\n", "    yaml_file.write(model_yaml)\n", "    \n", "# serialize model to JSON\n", "model_json = model.to_json()\n", "with open(\"model-baseline.json\", \"w\") as json_file:\n", "    json_file.write(model_json)\n", "\n", "# serialize weights to HDF5\n", "model.save_weights(\"model-baseline.h5\")\n", "print(\"Saved model to disk\")"], "outputs": [], "execution_count": null, "metadata": {}}, {"cell_type": "markdown", "source": ["## Sequential models"], "metadata": {}}, {"cell_type": "code", "source": ["# Note: shifting indices by 1 as index 0 will be used for padding.\n", "docs = [[idx + 1 for idx in corpus_dict.doc2idx(doc)]  for doc in tokenized_text]"], "outputs": [], "execution_count": null, "metadata": {"collapsed": true}}, {"cell_type": "code", "source": ["MAX_SEQ_LEN = 50\n", "padded_docs = keras.preprocessing.sequence.pad_sequences(docs, maxlen=MAX_SEQ_LEN, truncating='post', value=0)"], "outputs": [], "execution_count": null, "metadata": {"collapsed": true}}, {"cell_type": "code", "source": ["max_idx = max(c for d in docs for c in d)\n", "max_idx"], "outputs": [], "execution_count": null, "metadata": {"scrolled": true}}, {"cell_type": "code", "source": ["embeddings = np.array([np.random.normal(size=word2vec.vector_size)]+ # for the '0' padding word\n", "                      [word2vec.wv[corpus_dict[idx]]\n", "                      if corpus_dict[idx] in word2vec.wv.vocab\n", "                      else np.random.normal(size=word2vec.vector_size)\n", "                      for idx in range(max_idx)])"], "outputs": [], "execution_count": null, "metadata": {"collapsed": true}}, {"cell_type": "markdown", "source": ["### CNN"], "metadata": {}}, {"cell_type": "code", "source": ["from keras.models import Sequential\n", "from keras.layers.embeddings import Embedding\n", "from keras.layers.recurrent import SimpleRNN\n", "from keras.layers.core import Dense, Dropout\n", "from keras.layers.wrappers import TimeDistributed\n", "from keras.layers import Convolution1D, MaxPool1D, Flatten, BatchNormalization\n", "\n", "model = Sequential()\n", "model.add(Embedding(max_idx + 1, word2vec.vector_size, input_length=MAX_SEQ_LEN))\n", "model.add(Dropout(0.5))\n", "model.add(BatchNormalization())\n", "model.add(Convolution1D(52, 5, padding='same',\n", "                        kernel_regularizer=keras.regularizers.l2(0.01)))\n", "model.add(MaxPool1D())\n", "model.add(Dropout(0.5))\n", "model.add(BatchNormalization())\n", "model.add(Convolution1D(128, 3, padding='same',\n", "                        kernel_regularizer=keras.regularizers.l2(0.01)))\n", "model.add(MaxPool1D())\n", "model.add(Flatten())\n", "model.add(Dense(len(TARGET_CLASSES), activation='sigmoid',\n", "                kernel_regularizer=keras.regularizers.l2(0.02)))\n", "model.compile(Adam(0.001), 'binary_crossentropy')"], "outputs": [], "execution_count": null, "metadata": {}}, {"cell_type": "code", "source": ["model.fit(padded_docs, targets, batch_size=512, epochs=20, validation_split=0.1)"], "outputs": [], "execution_count": null, "metadata": {}}, {"cell_type": "code", "source": ["# serialize model to YAML\n", "model_yaml = model.to_yaml()\n", "with open(\"model-cnn.yaml\", \"w\") as yaml_file:\n", "    yaml_file.write(model_yaml)\n", "    \n", "# serialize model to JSON\n", "model_json = model.to_json()\n", "with open(\"model-cnn.json\", \"w\") as json_file:\n", "    json_file.write(model_json)\n", "\n", "# serialize weights to HDF5\n", "model.save_weights(\"model-cnn.h5\")\n", "print(\"Saved model to disk\")"], "outputs": [], "execution_count": null, "metadata": {}}, {"cell_type": "markdown", "source": ["## Submission"], "metadata": {}}, {"cell_type": "code", "source": ["def comment_to_sequential_input(comment):\n", "    tokens = tokenizer[gensim.utils.simple_preprocess(comment)]\n", "    t_ids = [corpus_dict.token2id[t] + 1 for t in tokens if t in word2vec.wv.vocab and t in corpus_dict.token2id]\n", "    return keras.preprocessing.sequence.pad_sequences([t_ids], maxlen=MAX_SEQ_LEN)[0]"], "outputs": [], "execution_count": null, "metadata": {"collapsed": true}}, {"cell_type": "code", "source": ["test_input = [comment_to_sequential_input(\"You are a jerk you freakin indian.\").reshape(1, -1)]\n", "for target_class, score in zip(TARGET_CLASSES, model.predict(test_input)[0]):\n", "    print(\"{}: {:.2f}%\".format(target_class, score * 100))"], "outputs": [], "execution_count": null, "metadata": {}}, {"cell_type": "code", "source": ["test_inputs = np.array([comment_to_sequential_input(doc) for doc in df_test.comment_text])"], "outputs": [], "execution_count": null, "metadata": {"collapsed": true}}, {"cell_type": "code", "source": ["test_outputs = model.predict(test_inputs)"], "outputs": [], "execution_count": null, "metadata": {"collapsed": true}}, {"cell_type": "code", "source": ["test_outputs[0]"], "outputs": [], "execution_count": null, "metadata": {}}, {"cell_type": "code", "source": ["output_df = df_test.reset_index()[['id']].copy()"], "outputs": [], "execution_count": null, "metadata": {"collapsed": true}}, {"cell_type": "code", "source": ["for i, target_class in enumerate(TARGET_CLASSES):\n", "    output_df[target_class] = test_outputs[:, i]"], "outputs": [], "execution_count": null, "metadata": {"collapsed": true}}, {"cell_type": "code", "source": ["output_df[output_df.toxic > 0.5].sample(10, random_state=0).merge(df_test.reset_index(), on='id')"], "outputs": [], "execution_count": null, "metadata": {}}, {"cell_type": "code", "source": ["output_df.to_csv('cnn-pred.csv', index=False)"], "outputs": [], "execution_count": null, "metadata": {}}], "nbformat": 4, "nbformat_minor": 1, "metadata": {"language_info": {"file_extension": ".py", "version": "3.6.0", "codemirror_mode": {"version": 3, "name": "ipython"}, "name": "python", "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "mimetype": "text/x-python"}, "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}}}