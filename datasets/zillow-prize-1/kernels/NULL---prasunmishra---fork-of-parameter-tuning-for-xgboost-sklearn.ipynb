{"metadata": {"kernelspec": {"language": "python", "display_name": "Python 3", "name": "python3"}, "language_info": {"pygments_lexer": "ipython3", "name": "python", "file_extension": ".py", "mimetype": "text/x-python", "version": "3.6.1", "nbconvert_exporter": "python", "codemirror_mode": {"name": "ipython", "version": 3}}}, "cells": [{"metadata": {"_execution_state": "busy", "trusted": false, "_uuid": "2d6c5889a6856bf45fc171274db1032ec86b88d9", "collapsed": true, "_cell_guid": "1ac56572-0c4b-4488-804f-d12458a49fc4"}, "execution_count": null, "cell_type": "code", "source": "# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Wed Aug  2 12:40:13 2017\nThis notebook is for parameter tuning of XGBoost\n@author: prasun.mishra\n\"\"\"\n\n#Import libraries:\nimport pandas as pd\nimport numpy as np\nfrom xgboost.sklearn import XGBClassifier\nfrom sklearn import cross_validation, metrics   #Additional scklearn functions\nfrom sklearn.grid_search import GridSearchCV   #Perforing grid search\nfrom sklearn.preprocessing import LabelEncoder\nimport matplotlib.pylab as plt\n%matplotlib inline\nfrom matplotlib.pylab import rcParams\nrcParams['figure.figsize'] = 12, 4\n\n\n##################################################\n\nprint(\"\\nReading data :\")\nprop = pd.read_csv('../input/properties_2016.csv')\ntrain_2016 = pd.read_csv(\"../input/train_2016_v2.csv\", parse_dates=[\"transactiondate\"])\nsample = pd.read_csv('../input/sample_submission.csv')\n\nprint('\\nBinding to float32')\n\nfor c, dtype in zip(prop.columns, prop.dtypes):\n    if dtype == np.float64:\n        prop[c] = prop[c].astype(np.float32)\n        \nprint('\\nFitting Label Encoder on prop')\nfor c in prop.columns:\n    prop[c]=prop[c].fillna(-1)\n    if prop[c].dtype == 'object':\n        lbl = LabelEncoder()\n        lbl.fit(list(prop[c].values))\n        prop[c] = lbl.transform(list(prop[c].values))\n\n###################################################        \n\ntrain = train_2016.merge(prop, how='left', on='parcelid')\ntrain.fillna(train.mean(),inplace = True)\ntrain = train.drop(['parcelid','transactiondate', 'propertyzoningdesc', 'propertycountylandusecode','fireplacecnt', 'fireplaceflag'], axis=1)\ntarget = 'logerror'\nIDcol = 'ParcelId'\n##################################################\n\ndef modelfit(alg, dtrain, predictors,useTrainCV=True, cv_folds=5, early_stopping_rounds=50):\n    print('*** Here in modelfit ******* Point 1')\n    if useTrainCV:\n        print('*** Here in modelfit ******* Point 2')            \n        xgb_param = alg.get_xgb_params()\n        print('*** Here in modelfit ******* Point 3')\n        xgtrain = xgb.DMatrix(dtrain[predictors].values, label=dtrain[target].values)\n        print('*** Here in modelfit ******* Point 4')\n        cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds,\n            metrics='mae', early_stopping_rounds=early_stopping_rounds, verbose_eval=10)\n        alg.set_params(n_estimators=cvresult.shape[0])\n        print('*** Here in modelfit ******* Point 5')\n    \n    #Fit the algorithm on the data\n    print('*** Here in modelfit ******* Point 5.5')\n    print (\"Here predictors are:\",predictors )\n    alg.fit(dtrain[predictors], dtrain['logerror'],eval_metric='mae')\n    #alg.fit(dtrain[predictors], dtrain['logerror'])\n    print('*** Here in modelfit ******* Point 6')\n        \n    #Predict training set:\n    dtrain_predictions = alg.predict(dtrain[predictors])\n    print('*** Here in modelfit ******* Point 7')\n    dtrain_predprob = alg.predict_proba(dtrain[predictors])[:,1]\n    print('*** Here in modelfit ******* Point 8')\n        \n    #Print model report:\n    print (\"\\nModel Report\")\n    print (\"\\nAccuracy : %.4g\" % metrics.accuracy_score(dtrain['logerror'].values, dtrain_predictions))\n    print (\"\\nMAE Score (Train): %f\" % metrics.mean_absolute_error(dtrain['logerror'], dtrain_predprob))\n                    \n    feat_imp = pd.Series(alg.booster().get_fscore()).sort_values(ascending=False)\n    print('*** Here in modelfit ******* Point 9')\n    feat_imp.plot(kind='bar', title='Feature Importances')\n    print('*** Here in modelfit ******* Point 10')\n    plt.ylabel('Feature Importance Score')\n    print('*** Here in modelfit ******* Point 11')\n    \n#Choose all predictors except target & IDcols\nprint('****** This is point 1')\npredictors = [x for x in train.columns if x not in [target, IDcol]]\nxgb1 = XGBClassifier(\n learning_rate =0.1,\n n_estimators=1000,\n max_depth=5,\n min_child_weight=1,\n gamma=0,\n subsample=0.8,\n colsample_bytree=0.8,\n objective= 'reg:linear',\n nthread=4,\n scale_pos_weight=1,\n seed=27)\nmodelfit(xgb1, train, predictors)\n\nprint('****** This is point 2')\n\nparam_test1 = {\n 'max_depth':range(3,10,2),\n 'min_child_weight':range(1,6,2)\n}\n\nprint('****** This is point 3')\ngsearch1 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=140, max_depth=5,\n min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8,\n objective= 'reg:linear', nthread=4, scale_pos_weight=1, seed=27), \n param_grid = param_test1, scoring='mae',n_jobs=4,iid=False, cv=5)\n\nprint('****** This is point 4')\ngsearch1.fit(train[predictors],train[target])\nprint('****** This is point 5')\ngsearch1.grid_scores_, gsearch1.best_params_, gsearch1.best_score_\nprint('****** This is point 6')\n\n\nparam_test2 = {\n 'max_depth':[4,5,6],\n 'min_child_weight':[4,5,6]\n}\ngsearch2 = GridSearchCV(estimator = XGBClassifier( learning_rate=0.1, n_estimators=140, max_depth=5,\n min_child_weight=2, gamma=0, subsample=0.8, colsample_bytree=0.8,\n objective= 'reg:linear', nthread=4, scale_pos_weight=1,seed=27), \n param_grid = param_test2, scoring='mae',n_jobs=4,iid=False, cv=5)\ngsearch2.fit(train[predictors],train[target])\ngsearch2.grid_scores_, gsearch2.best_params_, gsearch2.best_score_\n\nparam_test2b = {\n 'min_child_weight':[6,8,10,12]\n}\ngsearch2b = GridSearchCV(estimator = XGBClassifier( learning_rate=0.1, n_estimators=140, max_depth=4,\n min_child_weight=2, gamma=0, subsample=0.8, colsample_bytree=0.8,\n objective= 'reg:linear', nthread=4, scale_pos_weight=1,seed=27), \n param_grid = param_test2b, scoring='mae',n_jobs=4,iid=False, cv=5)\ngsearch2b.fit(train[predictors],train[target])\n\nmodelfit(gsearch2b.best_estimator_, train, predictors)\ngsearch2b.grid_scores_, gsearch2b.best_params_, gsearch2b.best_score_\n\n\nparam_test3 = {\n 'gamma':[i/10.0 for i in range(0,5)]\n}\ngsearch3 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=140, max_depth=4,\n min_child_weight=6, gamma=0, subsample=0.8, colsample_bytree=0.8,\n objective= 'reg:linear', nthread=4, scale_pos_weight=1,seed=27), \n param_grid = param_test3, scoring='mae',n_jobs=4,iid=False, cv=5)\ngsearch3.fit(train[predictors],train[target])\ngsearch3.grid_scores_, gsearch3.best_params_, gsearch3.best_score_\n\nxgb2 = XGBClassifier(\n learning_rate =0.1,\n n_estimators=1000,\n max_depth=4,\n min_child_weight=6,\n gamma=0,\n subsample=0.8,\n colsample_bytree=0.8,\n objective= 'reg:linear',\n nthread=4,\n scale_pos_weight=1,\n seed=27)\nmodelfit(xgb2, train, predictors)\n\nparam_test4 = {\n 'subsample':[i/10.0 for i in range(6,10)],\n 'colsample_bytree':[i/10.0 for i in range(6,10)]\n}\ngsearch4 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=177, max_depth=4,\n min_child_weight=6, gamma=0, subsample=0.8, colsample_bytree=0.8,\n objective= 'reg:linear', nthread=4, scale_pos_weight=1,seed=27), \n param_grid = param_test4, scoring='mae',n_jobs=4,iid=False, cv=5)\ngsearch4.fit(train[predictors],train[target])\ngsearch4.grid_scores_, gsearch4.best_params_, gsearch4.best_score_\nparam_test5 = {\n 'subsample':[i/100.0 for i in range(75,90,5)],\n 'colsample_bytree':[i/100.0 for i in range(75,90,5)]\n}\ngsearch5 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=177, max_depth=4,\n min_child_weight=6, gamma=0, subsample=0.8, colsample_bytree=0.8,\n objective= 'reg:linear', nthread=4, scale_pos_weight=1,seed=27), \n param_grid = param_test5, scoring='mae',n_jobs=4,iid=False, cv=5)\ngsearch5.fit(train[predictors],train[target])\n\n\nparam_test6 = {\n 'reg_alpha':[1e-5, 1e-2, 0.1, 1, 100]\n}\ngsearch6 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=177, max_depth=4,\n min_child_weight=6, gamma=0.1, subsample=0.8, colsample_bytree=0.8,\n objective= 'reg:linear', nthread=4, scale_pos_weight=1,seed=27), \n param_grid = param_test6, scoring='mae',n_jobs=4,iid=False, cv=5)\ngsearch6.fit(train[predictors],train[target])\ngsearch6.grid_scores_, gsearch6.best_params_, gsearch6.best_score_\n\nparam_test7 = {\n 'reg_alpha':[0, 0.001, 0.005, 0.01, 0.05]\n}\ngsearch7 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=177, max_depth=4,\n min_child_weight=6, gamma=0.1, subsample=0.8, colsample_bytree=0.8,\n objective= 'reg:linear', nthread=4, scale_pos_weight=1,seed=27), \n param_grid = param_test7, scoring='mae',n_jobs=4,iid=False, cv=5)\ngsearch7.fit(train[predictors],train[target])\ngsearch7.grid_scores_, gsearch7.best_params_, gsearch7.best_score_\n\n\nxgb3 = XGBClassifier(\n learning_rate =0.1,\n n_estimators=1000,\n max_depth=4,\n min_child_weight=6,\n gamma=0,\n subsample=0.8,\n colsample_bytree=0.8,\n reg_alpha=0.005,\n objective= 'reg:linear',\n nthread=4,\n scale_pos_weight=1,\n seed=27)\nmodelfit(xgb3, train, predictors)\n\nxgb4 = XGBClassifier(\n learning_rate =0.01,\n n_estimators=5000,\n max_depth=4,\n min_child_weight=6,\n gamma=0,\n subsample=0.8,\n colsample_bytree=0.8,\n reg_alpha=0.005,\n objective= 'reg:linear',\n nthread=4,\n scale_pos_weight=1,\n seed=27)\nmodelfit(xgb4, train, predictors)\n\n\n", "outputs": []}], "nbformat_minor": 1, "nbformat": 4}