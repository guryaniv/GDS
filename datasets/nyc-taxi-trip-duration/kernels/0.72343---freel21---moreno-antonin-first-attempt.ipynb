{"cells":[{"metadata":{"_uuid":"eb215bbb77762d7d2d8e9073fae3506d4350eb38"},"cell_type":"markdown","source":"# 1 - Importing the magic stuff"},{"metadata":{"trusted":true,"_uuid":"b810fedd153f8d18b10ba514e3e0099b6e31377f"},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import cross_val_score, train_test_split\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c5b0c824cf9cdc288a217449dab44c08f78bd72e"},"cell_type":"markdown","source":"# 2 - Data exploration"},{"metadata":{"trusted":true,"_uuid":"ba35d683c5f57fe48b59a8dec72408dcf6e9d059"},"cell_type":"code","source":"df_train = pd.read_csv('../input/train.csv')\ndf_test = pd.read_csv('../input/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eaa364bd567cc0ad93013f1c9cccab90f6148b15"},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"00d7dc705f5135da2d192c1eb3ea7133cabb6208"},"cell_type":"code","source":"df_train.size\n\n# Holy Pepperoni, that's big","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bb34c652c5015f6a7e7d2be62d0d8df5f0614a54"},"cell_type":"code","source":"print(df_train.columns)\n\n# in order to get an idea of the futur features to take in count for the prediction\n# ... and because I love to print columns anyway, hope you don't mind","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"851a4c59a310fb840411c380f60547294d5df6fb"},"cell_type":"code","source":"df_train.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f08a68b0af12d03abfbb021ac081e51147c9a0c6"},"cell_type":"markdown","source":"# 3 - Further exploration"},{"metadata":{"trusted":true,"_uuid":"1fbdbb4141f420cb768709d8df3328a6b51dc46a"},"cell_type":"code","source":"df_train.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0c2e2ad4bc4a88ebf7de74da1158195807c2e476"},"cell_type":"code","source":"df_train.isnull().sum()\n\n# a data with no NaN value, what is this sorcery","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c7abd13acc985324495233ba9ce4bfa905705214"},"cell_type":"code","source":"# just to be sure\n\ndf_train.duplicated().sum()\n\n# sweet","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5315ab0ec738786e9d0cf9f462c0a53d19b0be7f"},"cell_type":"markdown","source":"# 4 - Data cleaning"},{"metadata":{"trusted":true,"_uuid":"9c23c3ffcaee8c39a575d71d2686116fb5e0e329"},"cell_type":"code","source":"# First things first, we need to get rid of outliers in the trip duration feature\n\nplt.subplots(figsize=(20,10))\nplt.title(\"Top Outliers repartition in the trip duration feature\")\ndf_train.boxplot();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"727ec36c8c872d5b99d900c617074ae1b71f17ad"},"cell_type":"code","source":"# 1 minute of silence for the people who forgot to turn off the taxi counter\n# We have to get rid of these values in order to make correct predictions\n\ndf_train = df_train.loc[df_train['trip_duration']< 300000]\n\n# The \"top\" outliers are the easiest to deal with. It gets more complicated with the \"bottom\" outliers\n# In the following, we will consider any trip duration below 5 minutes as outliers\n# haters gonna hate, I know, who would take the taxi for a trip duration less than 5 minutes anyway\n\ndf_train = df_train.loc[(df_train['trip_duration'] > 300) & (df_train['trip_duration'] < 300000)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c405a9bd8478603e421d639b4d274d218004ac54"},"cell_type":"code","source":"# Now we also need to get rid of outliers in the geographical place data section where people are picked up\n\ndf_train.plot.scatter(x='pickup_longitude',y='pickup_latitude');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b2fdff4e10073601d40a77793703393d395f6eb3"},"cell_type":"code","source":"df_train = df_train.loc[(df_train['pickup_longitude']> -90) & (df_train['pickup_latitude']< 46)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e9892bad7f6eff5aa42fb6638603d3e328030f7f"},"cell_type":"code","source":"# Same goes with the place people are dropped off\n\ndf_train.plot.scatter(x='dropoff_longitude',y='dropoff_latitude');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9bfc6abac8e7b0a0930ca207c6928a5a3d60188b"},"cell_type":"code","source":"df_train = df_train.loc[(df_train['dropoff_longitude']< -70) & (df_train['dropoff_latitude']> 35)]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9c7b3c216d6c0996103284d7b4fa3cf3f5d90671"},"cell_type":"markdown","source":"# Features creation, selection and extraction"},{"metadata":{"trusted":true,"_uuid":"06603d58ff436a75c01b6dc288eb1885a106878b"},"cell_type":"code","source":"# In prevision of the prediction model, we are going to create 7 more features : DateTime, Month, Day, Hour, Minute, Time, Distance\n\ndf_train['Time'] = df_train['pickup_datetime'].apply(lambda x: int(x.split()[1][0:2]))\n\ndf_train['Distance'] = np.sqrt((df_train['pickup_latitude']-df_train['dropoff_latitude'])**2 + (df_train['pickup_longitude']-df_train['dropoff_longitude'])**2) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"114c0256ffadc74b638937bc0a80d01965ae9278"},"cell_type":"code","source":"y = df_train[\"trip_duration\"] # <-- target\nX = df_train[[\"passenger_count\", \"pickup_longitude\", \"pickup_latitude\", \"dropoff_longitude\",\"dropoff_latitude\",\"Time\",\"Distance\"]] # <-- features\n\nX_train, X_valid, y_train, y_valid = train_test_split(X,y, test_size=0.2, random_state=42)\nX_train.shape, X_valid.shape, y_train.shape, y_valid.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4ec9671436eeaf24521628a0643f2eafd3dea7cd"},"cell_type":"code","source":"m1 = RandomForestRegressor()\nm1.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e1c509dd8f1ba1672f891a337d2b69a5e4f38a7c"},"cell_type":"markdown","source":"# Validation"},{"metadata":{"trusted":true,"_uuid":"cf435d942a9011af2794dd586b4af84cd6bf4045"},"cell_type":"code","source":"m1_scores = cross_val_score(m1, X, y, cv=5, scoring =\"neg_mean_squared_log_error\")\nm1_scores","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3126b90095405d6d1e986ae96991ea06519ecbce"},"cell_type":"code","source":"for i in range(len(m1_scores)):\n    m1_scores[i] = np.sqrt(abs(m1_scores[i]))\nprint(m1_scores)\nprint(np.mean(m1_scores))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ee9bbb0a3a8cea047ef7b58d3a0510c2582fa642"},"cell_type":"code","source":"df_test['Time'] = df_test['pickup_datetime'].apply(lambda x: int(x.split()[1][0:2]))\n\ndf_test['Distance'] = np.sqrt((df_test['pickup_latitude']-df_test['dropoff_latitude'])**2 + (df_test['pickup_longitude']-df_test['dropoff_longitude'])**2) \ndf_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fc1db391567adc59e68c3061591d43a3286afdae"},"cell_type":"code","source":"X_test = df_test[[\"passenger_count\", \"pickup_longitude\", \"pickup_latitude\", \"dropoff_longitude\",\"dropoff_latitude\",\"Time\",\"Distance\"]]\npredicted_duration = m1.predict(X_test)\nprint(predicted_duration)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fa99b32dc0cf134a12bed8ec66653688643fd73f"},"cell_type":"code","source":"My_Submission = pd.DataFrame({'id': df_test.id, 'trip_duration': predicted_duration})\nprint(My_Submission)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b887b8a691579276a064f72f6bf8318f975128b6"},"cell_type":"code","source":"My_Submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"817236d71f7f29ac80352cb9210894952fe5569e"},"cell_type":"markdown","source":"### Thanks for the time you spent reading this Kernel. I'm completely new to machine learning and this is my first attempt on a Kaggle dataset. If you have any suggestion on how I can improve myself, please feel free to share it ! "}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}