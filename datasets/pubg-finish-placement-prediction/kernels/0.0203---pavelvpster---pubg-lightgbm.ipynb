{"cells":[{"metadata":{"_uuid":"63ead0525bfccba8005636a9bbdf7009db3ef389"},"cell_type":"markdown","source":"# PUBG: LightGBM"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport warnings\nwarnings.simplefilter('ignore')\n\nfrom copy import deepcopy\n\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import StandardScaler\n\nimport gc, sys\ngc.enable()\n\nimport os\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f0bffc9af1f00c088f80d27174b1a3206f748bcb"},"cell_type":"markdown","source":"## Framework"},{"metadata":{"_uuid":"9dab86d68fca059efa941bee23b5782d0816d2ad"},"cell_type":"markdown","source":"### Common functions"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"code","source":"# Thanks to https://www.kaggle.com/gemartin/load-data-reduce-memory-usage\n\ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n#        else:\n#            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"405544b512bd3e3d02f40599342cfb4dcd27bbd3"},"cell_type":"markdown","source":"### Data preparation"},{"metadata":{"trusted":true,"_uuid":"3ab128856bd13b595315a4b5f8f3e07655c7c030"},"cell_type":"code","source":"def take_part_of_data(df, part):\n    \n    match_ids = df['matchId'].unique()\n    match_ids_part = np.random.choice(match_ids, int(part * len(match_ids)))\n    \n    df = df[df['matchId'].isin(match_ids_part)]\n    \n    del match_ids\n    del match_ids_part","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0f0d4e4fbc5780116c5c220844d7641da175c858"},"cell_type":"markdown","source":"### Feature engineering"},{"metadata":{"trusted":true,"_uuid":"6547db0628265bc1b402ba7ed5563f8df0ce3382"},"cell_type":"code","source":"def add_new_features_1(df):\n    \n    # calculate total distance\n    df['totalDistance'] = df['rideDistance'] + df['walkDistance'] + df['swimDistance']\n    \n    # sum heals and boosts\n    df['healsAndBoosts'] = df['heals'] + df['boosts']\n    \n    # headshot rate\n    df['headshotKillsOverKills'] = df['headshotKills'] / df['kills']\n    df['headshotKillsOverKills'].fillna(0, inplace=True)\n    \n    # kill streake rate\n    df['killStreaksOverKills'] = df['killStreaks'] / df['kills']\n    df['killStreaksOverKills'].fillna(0, inplace=True)\n    \n    # kills and assists\n    df['killsAndAssists'] = df['kills'] + df['assists']\n    \n    # teamwork\n    df['assistsAndRevives'] = df['assists'] + df['revives']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4c0d8350cd1f98447a7140627ea38043b2034143"},"cell_type":"code","source":"def add_new_features_2(df):\n    \n    # number of players joined\n    df['playersJoined'] = df.groupby('matchId')['matchId'].transform('count')\n    \n    # normalize features by number of players joined\n    df['killsAndAssistsOverPlayersJoined'] = df['killsAndAssists'] * ((100 - df['playersJoined']) / 100 + 1)\n    df['matchDurationOverPlayersJoined'] = df['matchDuration'] * ((100 - df['playersJoined']) / 100 + 1)\n    df['damageDealtOverPlayersJoined'] = df['damageDealt'] * ((100 - df['playersJoined']) / 100 + 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ad8b5633b6713f7e7354299cc21d5aea7f904751"},"cell_type":"code","source":"def add_new_features_3(df):\n    \n    # total distance over kills and assists\n    df['totalDistanceOverKillsAndAssists'] = df['totalDistance'] / df['killsAndAssists']\n    df['totalDistanceOverKillsAndAssists'].fillna(0, inplace=True)\n    df['totalDistanceOverKillsAndAssists'].replace(np.inf, 0, inplace=True)\n    \n    # total distance over heals and boosts\n    df['totalDistanceOverHealsAndBoosts'] = df['totalDistance'] / df['healsAndBoosts']\n    df['totalDistanceOverHealsAndBoosts'].fillna(0, inplace=True)\n    df['totalDistanceOverHealsAndBoosts'].replace(np.inf, 0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4f0170444ecaa12dbcc2e0ae2ed53579ec705d7d"},"cell_type":"code","source":"def add_new_features_4(df):\n    \n    df['headshotRate'] = df['kills'] / df['headshotKills']\n    df['killStreakRate'] = df['killStreaks'] / df['kills']\n    df['healsAndBoosts'] = df['heals'] + df['boosts']\n    df['totalDistance'] = df['rideDistance'] + df['walkDistance'] + df['swimDistance']\n    df['killPlaceOverMaxPlace'] = df['killPlace'] / df['maxPlace']\n    df['headshotKillsOverKills'] = df['headshotKills'] / df['kills']\n    df['distanceOverWeapons'] = df['totalDistance'] / df['weaponsAcquired']\n    df['walkDistanceOverHeals'] = df['walkDistance'] / df['heals']\n    df['walkDistanceOverKills'] = df['walkDistance'] / df['kills']\n    df['killsPerWalkDistance'] = df['kills'] / df['walkDistance']\n    df[\"skill\"] = df['headshotKills'] + df['roadKills']\n    \n    df[df == np.Inf] = np.NaN\n    df[df == np.NINF] = np.NaN\n    \n    df.fillna(0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8c4af3fc227c11763012e7520c47bcf608d91923"},"cell_type":"code","source":"def feature_engineering(df, is_train=True):\n    \n    # fix rank points\n    df['rankPoints'] = np.where(df['rankPoints'] <= 0, 0, df['rankPoints'])\n    \n    features = list(df.columns)\n    features.remove(\"matchId\")\n    features.remove(\"groupId\")\n    features.remove(\"matchDuration\")\n    features.remove(\"matchType\")\n    if 'winPlacePerc' in features:\n        features.remove('winPlacePerc')\n    \n    y = None\n    \n    # average y for training dataset\n    if is_train:\n        y = df.groupby(['matchId','groupId'])['winPlacePerc'].agg('mean')\n    elif 'winPlacePerc' in df.columns:\n        y = df['winPlacePerc']\n    \n    # mean by match and group\n    agg = df.groupby(['matchId','groupId'])[features].agg('mean')\n    agg_rank = agg.groupby('matchId')[features].rank(pct=True).reset_index()\n    \n    if is_train:\n        df_out = agg.reset_index()[['matchId','groupId']]\n    else:\n        df_out = df[['matchId','groupId']]\n    \n    df_out = df_out.merge(agg.reset_index(), suffixes=[\"\", \"\"], how='left', on=['matchId', 'groupId'])\n    df_out = df_out.merge(agg_rank, suffixes=[\"_mean\", \"_mean_rank\"], how='left', on=['matchId', 'groupId'])\n    \n    # max by match and group\n    agg = df.groupby(['matchId','groupId'])[features].agg('max')\n    agg_rank = agg.groupby('matchId')[features].rank(pct=True).reset_index()\n    \n    df_out = df_out.merge(agg.reset_index(), suffixes=[\"\", \"\"], how='left', on=['matchId', 'groupId'])\n    df_out = df_out.merge(agg_rank, suffixes=[\"_max\", \"_max_rank\"], how='left', on=['matchId', 'groupId'])\n    \n    # max by match and group\n    agg = df.groupby(['matchId','groupId'])[features].agg('min')\n    agg_rank = agg.groupby('matchId')[features].rank(pct=True).reset_index()\n    \n    df_out = df_out.merge(agg.reset_index(), suffixes=[\"\", \"\"], how='left', on=['matchId', 'groupId'])\n    df_out = df_out.merge(agg_rank, suffixes=[\"_min\", \"_min_rank\"], how='left', on=['matchId', 'groupId'])\n    \n    # number of players in group\n    agg = df.groupby(['matchId','groupId']).size().reset_index(name='group_size')\n    \n    df_out = df_out.merge(agg, how='left', on=['matchId', 'groupId'])\n    \n    # mean by match\n    agg = df.groupby(['matchId'])[features].agg('mean').reset_index()\n    \n    df_out = df_out.merge(agg, suffixes=[\"\", \"_match_mean\"], how='left', on=['matchId'])\n    \n    # number of groups in match\n    agg = df.groupby(['matchId']).size().reset_index(name='match_size')\n    \n    df_out = df_out.merge(agg, how='left', on=['matchId'])\n    \n    # drop match id and group id\n    df_out.drop([\"matchId\", \"groupId\"], axis=1, inplace=True)\n    \n    del agg, agg_rank\n    \n    return df_out, y","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9c40e27705574c955035ea1fd14270ad70da7c4c"},"cell_type":"markdown","source":"### Machine learning"},{"metadata":{"trusted":true,"_uuid":"d55b06b68252eaf889144db7f81bf3827f80c772"},"cell_type":"code","source":"class Estimator(object):\n    \n    def fit(self, x_train, y_train, x_valid, y_valid):\n        raise NotImplementedException\n    \n    def predict(self, x):\n        raise NotImplementedException","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ddd0403c8918043e99b2230a3423caebba37643f"},"cell_type":"code","source":"class ScikitLearnEstimator(Estimator):\n    \n    def __init__(self, estimator):\n        self.estimator = estimator\n    \n    def fit(self, x_train, y_train, x_valid, y_valid):\n        self.estimator.fit(x_train, y_train)\n    \n    def predict(self, x):\n        return self.estimator.predict(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1c4771d7271e62d2e7c94fced64cb2df83129f4d"},"cell_type":"code","source":"def fit_predict_step(estimator, x_train, y_train, train_idx, valid_idx, x_test, oof):\n    \n    # prepare train and validation data\n    x_train_train = x_train[train_idx]\n    y_train_train = y_train[train_idx]\n    x_train_valid = x_train[valid_idx]\n    y_train_valid = y_train[valid_idx]\n    \n    # fit estimator\n    estimator.fit(x_train_train, y_train_train, x_train_valid, y_train_valid)\n    \n    # collect OOF\n    oof_part = estimator.predict(x_train_valid)\n    \n    print('MAE:', mean_absolute_error(y_train_valid, oof_part))\n    \n    oof[valid_idx] = oof_part\n    \n    # make predictions for test data\n    y_part = estimator.predict(x_test)\n    \n    return y_part","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"15979e3e270158f5378898802076ae1409778295"},"cell_type":"code","source":"def fit_predict(estimator, x_train, y_train, x_test):\n    \n    oof = np.zeros(x_train.shape[0])\n    \n    y = np.zeros(x_test.shape[0])\n    \n    kf = KFold(n_splits=5, random_state=42)\n    \n    for train_idx, valid_idx in kf.split(x_train):\n        \n        y_part = fit_predict_step(estimator, x_train, y_train, train_idx, valid_idx, x_test, oof)\n        \n        # average predictions for test data\n        y += y_part / kf.n_splits\n    \n    print('Final MAE:', mean_absolute_error(y_train, oof))\n    \n    return oof, y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"abbe240c0871b331025310a1dba0d31732a17952"},"cell_type":"code","source":"def fit_step(estimator, x_train, y_train, train_idx, valid_idx, oof):\n    \n    # prepare train and validation data\n    x_train_train = x_train[train_idx]\n    y_train_train = y_train[train_idx]\n    x_train_valid = x_train[valid_idx]\n    y_train_valid = y_train[valid_idx]\n    \n    # fit estimator\n    estimator.fit(x_train_train, y_train_train, x_train_valid, y_train_valid)\n    \n    # collect OOF\n    oof_part = estimator.predict(x_train_valid)\n    \n    mae = mean_absolute_error(y_train_valid, oof_part)\n    print('MAE:', mae)\n    \n    oof[valid_idx] = oof_part\n    \n    return estimator, mae","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f7f1212587f761871dc47c733df3b5455776a236"},"cell_type":"code","source":"def fit(estimator, x_train, y_train):\n    \n    oof = np.zeros(x_train.shape[0])\n    \n    kf = KFold(n_splits=5, random_state=42)\n    \n    trained_estimators = []\n    \n    for train_idx, valid_idx in kf.split(x_train):\n        \n        e, mae = fit_step(estimator, x_train, y_train, train_idx, valid_idx, oof)\n        \n        trained_estimators.append(deepcopy(e))\n    \n    print('Final MAE:', mean_absolute_error(y_train, oof))\n    \n    return oof, trained_estimators","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c74c38d8bcc2b9025c7d5263ea7793120973d6d3"},"cell_type":"code","source":"def predict(trained_estimators, x_test):\n    \n    y = np.zeros(x_test.shape[0])\n    \n    for estimator in trained_estimators:\n        \n        y_part = estimator.predict(x_test)\n        \n        # average predictions for test data\n        y += y_part / len(trained_estimators)\n    \n    return y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e7487e584ff933aa3e15ff062d6aa41d48b5f163"},"cell_type":"code","source":"def pipeline_fit(estimator, df_train, scaler=None):\n    \n    # add new features\n    add_new_features_4(df_train)\n    \n    # feature engineering\n    x_train, y_train = feature_engineering(df_train, is_train=True)\n    x_train = reduce_mem_usage(x_train)\n    gc.collect()\n    \n    # scale\n    if not (scaler is None):\n        scaler.fit(x_train)\n        scaled_x_train = scaler.transform(x_train)\n    else:\n        scaled_x_train = x_train.values\n    \n    del x_train\n    gc.collect()\n    \n    # fit\n    oof, trained_estimators = fit(estimator, scaled_x_train, y_train.values)\n    \n    del scaled_x_train\n    del y_train\n    gc.collect()\n    \n    return oof, trained_estimators","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"894510801b8d26c462d97ab6387d85bc5cb47a87"},"cell_type":"code","source":"def pipeline_predict(trained_estimators, df_test, scaler=None):\n    \n    # add new features\n    add_new_features_4(df_test)\n    \n    # feature engineering\n    x_test, _ = feature_engineering(df_test, is_train=False)\n    x_test = reduce_mem_usage(x_test)\n    gc.collect()\n    \n    # scale\n    if not (scaler is None):\n        scaled_x_test = scaler.transform(x_test)\n    else:\n        scaled_x_test = x_test.values\n    \n    del x_test\n    gc.collect()\n    \n    # predict\n    y = predict(trained_estimators, scaled_x_test)\n    \n    del scaled_x_test\n    gc.collect()\n    \n    return y","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ddd2a0f1f94da41a5435e96c870178851e9ce8aa"},"cell_type":"markdown","source":"## Load train data"},{"metadata":{"_uuid":"8dcf02134aeb11d024f71f3ffbfff7de0050d1b6","trusted":false},"cell_type":"code","source":"df_train = pd.read_csv('../input/train_V2.csv', index_col='Id')\ndf_train.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fadd8e0fb1cd32eb1b216351dc7cd0b4f3497dec","trusted":false},"cell_type":"code","source":"df_train = reduce_mem_usage(df_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f291168dc2a98bc7d1da94ac7685c9249758f7b0","trusted":false},"cell_type":"code","source":"df_train.head().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"22b7c447bc854d02d381da7d96c7a8808615dcbe"},"cell_type":"code","source":"gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"84ed416c46b4b7f4fe7cf46e325759d6dfbba38f"},"cell_type":"markdown","source":"## Remove bad row"},{"metadata":{"_uuid":"0626ac7f1e3b0bef491d806699ebd0d44148fc84","trusted":false},"cell_type":"code","source":"df_train.drop(df_train[df_train['winPlacePerc'].isnull()].index, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"91c8912fc72ae57c3693baa463deb91ad688f1b7"},"cell_type":"markdown","source":"## LightGBM"},{"metadata":{"trusted":true,"_uuid":"3dea8c672bb3192bf3cd6edaffeca159f2e9d798"},"cell_type":"code","source":"import lightgbm as lgb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"25e6ed7204184f8b852ff0604bd6830f1460981f"},"cell_type":"code","source":"class LightGBM(Estimator):\n    \n    def __init__(self, params):\n        self.params = params\n    \n    def fit(self, x_train, y_train, x_valid, y_valid):\n        \n        lgb_train = lgb.Dataset(data=x_train.astype('float32'), label=y_train.astype('float32'))\n        lgb_valid = lgb.Dataset(data=x_valid.astype('float32'), label=y_valid.astype('float32'))\n        \n        self.lgb_model = lgb.train(self.params, lgb_train, valid_sets=lgb_valid, verbose_eval=1000)\n    \n    def predict(self, x):\n        return self.lgb_model.predict(x.astype('float32'), num_iteration=self.lgb_model.best_iteration)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"875df566c0e878c8a1d51cd3493ea22134f28a23","trusted":false},"cell_type":"code","source":"params = {'objective': 'regression',\n          'metric': 'mae',\n          'n_estimators': 10000,\n          'early_stopping_rounds': 100,\n          'num_leaves': 300,\n          'max_depth': 14,\n          'bagging_fraction': 0.9,\n          'learning_rate': 0.03,\n          'bagging_seed': 0,\n          'num_threads': 4,\n          'colsample_bytree': 0.7}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ddbe1123947740e29df386759d3bf11fd08e9714"},"cell_type":"code","source":"%%time\n\n# scaler = StandardScaler()\noof, trained_estimators = pipeline_fit(LightGBM(params), df_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"adccd19657c827f3ee8210942bf23eb8c0419968"},"cell_type":"code","source":"del df_train\n\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f9474ba1092da2e75b7da08ab84a1ac26ca258e6"},"cell_type":"markdown","source":"## Load test data"},{"metadata":{"trusted":true,"_uuid":"2092078035ca23b3379791ba304f2d3a1866790c"},"cell_type":"code","source":"df_test = pd.read_csv('../input/test_V2.csv', index_col = 'Id')\ndf_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7c975322a04f728d9cf55755ea9cd32c433c9ef2"},"cell_type":"code","source":"df_test = reduce_mem_usage(df_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"87229511d8ecc3a2fab75fe7b77d402b1503211f"},"cell_type":"code","source":"df_test_id = pd.DataFrame(index=df_test.index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"389a18bbfdf3f74371dd0ac9031b080581470a45"},"cell_type":"code","source":"gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"97c8a971f0f76c8ecbfe8f50a14d1410230423d1"},"cell_type":"markdown","source":"## LightGBM"},{"metadata":{"trusted":true,"_uuid":"c9aa2889da46692c4775e20de8803648094f061e"},"cell_type":"code","source":"y = pipeline_predict(trained_estimators, df_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"265d83c053decec08880916f13cefbea1b63d0cc"},"cell_type":"code","source":"del df_test\n\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bf2e72f157fc61658b2bd8301fd4e5612dac9022"},"cell_type":"markdown","source":"## Save OOF"},{"metadata":{"trusted":true,"_uuid":"a9a11e401c048cd2c9f4bdeb2e91a85478b28bd8"},"cell_type":"code","source":"df_oof = pd.DataFrame()\ndf_oof['lgb_oof'] = oof\ndf_oof.to_csv('light_gbm_oof.csv', index_label='id')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b75a27aa546a89e286c5880bfe77c16870a7ed58"},"cell_type":"markdown","source":"## Submission"},{"metadata":{"_uuid":"c464dad858f0459d7d6fe51ebb6183241f3f4685","trusted":false},"cell_type":"code","source":"df_submission = pd.DataFrame(index=df_test_id.index)\ndf_submission['winPlacePerc'] = y\ndf_submission.to_csv('light_gbm_raw.csv', index_label='Id')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6bb4d91e1e5ec277bbf45acf1fd9da76c5d541a2"},"cell_type":"markdown","source":"## Adjust predictions"},{"metadata":{"trusted":true,"_uuid":"351f02453a9c738b0d8fb2c58ff2c248d1436c22"},"cell_type":"code","source":"df_test = pd.read_csv('../input/test_V2.csv')\ndf_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"418d75174f19aae312be01190e984b6164617578"},"cell_type":"code","source":"df_submission = df_submission.merge(df_test[['Id', 'matchId', 'groupId', 'maxPlace', 'numGroups']], on='Id', how='left')\ndf_submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fdc2d9d28a1aaf2a97a21dadd9d6814ce2c77419"},"cell_type":"code","source":"df_submission_group = df_submission.groupby(['matchId', 'groupId']).first().reset_index()\n\ndf_submission_group['rank'] = df_submission_group.groupby(['matchId'])['winPlacePerc'].rank()\n\ndf_submission_group = df_submission_group.merge(df_submission_group.groupby('matchId')['rank'].max().to_frame('max_rank').reset_index(), on='matchId', how='left')\n\ndf_submission_group['adjusted_perc'] = (df_submission_group['rank'] - 1) / (df_submission_group['numGroups'] - 1)\n\ndf_submission = df_submission.merge(df_submission_group[['adjusted_perc', 'matchId', 'groupId']], on=['matchId', 'groupId'], how='left')\n\ndf_submission['winPlacePerc'] = df_submission['adjusted_perc']\n\ndf_submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"be3fb18dc7898013a7fd35772481bcd68780b747"},"cell_type":"code","source":"df_submission.loc[df_submission.maxPlace == 0, 'winPlacePerc'] = 0\ndf_submission.loc[df_submission.maxPlace == 1, 'winPlacePerc'] = 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4ad135bdb072cad6858051907e74a338044940d5"},"cell_type":"code","source":"# Thanks to https://www.kaggle.com/anycode/simple-nn-baseline-4\nt = df_submission.loc[df_submission.maxPlace > 1]\ngap = 1.0 / (t.maxPlace.values - 1)\nfixed_perc = np.around(t.winPlacePerc.values / gap) * gap\ndf_submission.loc[df_submission.maxPlace > 1, 'winPlacePerc'] = fixed_perc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ab461e77ecc27914b27e68ba577d43d26b237a6e"},"cell_type":"code","source":"df_submission.loc[(df_submission.maxPlace > 1) & (df_submission.numGroups == 1), 'winPlacePerc'] = 0\n\nassert df_submission['winPlacePerc'].isnull().sum() == 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"87fc7a4385a949af7de4152987a2b890f1946639"},"cell_type":"code","source":"df_submission[['Id', 'winPlacePerc']].to_csv('light_gbm_adjusted.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}