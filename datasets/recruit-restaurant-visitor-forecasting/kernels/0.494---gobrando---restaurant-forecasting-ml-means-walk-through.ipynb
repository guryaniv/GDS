{"cells":[{"metadata":{"_uuid":"3a077f16bd54a7cbedf253defeea7d20c9dbe2d1"},"cell_type":"markdown","source":"# Predicting Restaurant Demand: A Walk-Through For Beginners and The Curious "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"markdown","source":"*This kernel draws from work by DSEverything, JdPaletto, the1owl1, and hklee and aims to explain some of the methods they took for solving this problem with some tweaks of my own of course :D"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"baced04a904917911c106482171c847229d020e8"},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn import ensemble, neighbors, linear_model, metrics, preprocessing\nfrom datetime import datetime\nimport glob, re","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d6aca4887cd91c4cdcba728fed8954e40e54d497"},"cell_type":"markdown","source":"# Our Methodology\n#### I'm very excited to share this work with you as it combines two very different approaches to predicting future customer demand in a way that is both educational and performs quite well!\n #### 1. The machine learning approach of building one big dataframe, feeding that dataframe to a model (our ensemble of models in our case), and letting it predict future demand\n #### 2. The old-school and dare I say elegant inferential statistics approach of time weighted means averaging to predict future demand\n #### Think of it like taking a high level approach then getting down in the weeds of the math and statistics to really understand what machine learning approaches are really built on. Should be interesting!\n #### If you have any ideas on how to improve this notebook let me know in the comments. "},{"metadata":{"_uuid":"324f411253ac5ae02052b721894d88fa8cd67b3e"},"cell_type":"markdown","source":"# Approach 1 (ML Model Ensembling)"},{"metadata":{"_uuid":"24b646af928156aa8c5096a7799c6badc12e98dc"},"cell_type":"markdown","source":"## Cleaning, Feature Engineering & Merging (...But Mostly Cleaning)"},{"metadata":{"_uuid":"9db025a3e87237821ef704918d55f4ba0a017264"},"cell_type":"markdown","source":"We'll begin by collecting all our datasets in one dictionary"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"collapsed":true},"cell_type":"code","source":"data = {\n    'ar': pd.read_csv('../input/air_reserve.csv'),\n    'as': pd.read_csv('../input/air_store_info.csv'),\n    'tra': pd.read_csv('../input/air_visit_data.csv'),\n    'hol': pd.read_csv('../input/date_info.csv').rename(columns={'calendar_date': 'visit_date'}),\n    'hr': pd.read_csv('../input/hpg_reserve.csv'),\n    'hs': pd.read_csv('../input/hpg_store_info.csv'),\n    'tes': pd.read_csv('../input/sample_submission.csv'),\n    'id': pd.read_csv('../input/store_id_relation.csv')\n}","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5c0dd89669e2922bdf76e4efef9bd95d21d3ecd4"},"cell_type":"markdown","source":"Then we'll go through each dataset looking where we can merge them together, sort them for understanding, and feature engineer them for new patterns. In the end we should have one well optimized dataset to feed our machine learning model."},{"metadata":{"trusted":true,"_uuid":"3667b33cfc484a0ef2f196b0752ec39260370cd6"},"cell_type":"code","source":"data['id'].sample(3)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d7ca782001e758de6d3ac2a95f5e10e908a3189d"},"cell_type":"markdown","source":"Our id set has both restaruant ids in it. Making it the perfect candidate for linking hpg data to air data. Let's start merging!"},{"metadata":{"trusted":true,"_uuid":"dc2eeb3c56589f2acb3b152a43337799755db596"},"cell_type":"code","source":"data['hr'] = data['hr'].merge(data['id'], how='inner', on=['hpg_store_id'])\ndata['hr'].sample(5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a016fc377664f94bc89bf9bf09cead9828e13c99"},"cell_type":"markdown","source":"Next we need to convert the datetime columns to a pandas datetime so we can extract our most valuable piece of data in this competition. The date each customer came to eat! (we'll be doing this a lot so you know). We'll engineer our own potentially useful feature as well; tracking how many days in advance each customer reserved their table. Could be interesting!"},{"metadata":{"trusted":true,"_uuid":"dfb46743e60baa652951018d167e357402766ea7"},"cell_type":"code","source":"# Notice datetime initially an object. We will fix that\nnp.dtype(data['hr']['visit_datetime'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ca7ada20bca06648742996cc755a6ebdd0f7906a"},"cell_type":"code","source":"for df in ['ar', 'hr']:\n    data[df]['visit_datetime'] = pd.to_datetime(data[df]['visit_datetime'])\n    data[df]['visit_datetime'] = data[df]['visit_datetime'].dt.date\n    data[df]['reserve_datetime'] = pd.to_datetime(data[df]['reserve_datetime'])\n    data[df]['reserve_datetime'] = data[df]['reserve_datetime'].dt.date\n    data[df]['reserve_datetime_diff'] = data[df].apply(\n        lambda r: (r['visit_datetime'] - r['reserve_datetime']).days, axis=1)\n    data[df] = data[df].groupby(['air_store_id', 'visit_datetime'], as_index=False)[['reserve_datetime_diff',\n                                                                                     'reserve_visitors']].sum().rename(columns={'visit_datetime':'visit_date'})\ndata['hr'].sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4edcb32f401d40b67b08405a1ecdf4e854730eef"},"cell_type":"code","source":"data['ar'].sample(4)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"be48fecbf04ee3cd772f6b9fbde2027a19067f5e"},"cell_type":"markdown","source":"'hr' and 'ar' look good, next on the list is 'as'. Let's take a peek."},{"metadata":{"trusted":true,"_uuid":"f47e2a1a745835d6f126cc359e6b2f08700cee9f"},"cell_type":"code","source":"data['as'].sample(5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"53254aacd455c1cca8828d006427ba5e80232bda"},"cell_type":"markdown","source":"### **Why Not Weather?**\nI see a lot of people getting fancy mapping out the location of every restaurant on a map of Japan which is pretty cool to witness but not very useful to our problem so I won't be going that direction.\n\nI've also seen a lot of kernels incorporating the weather down to the area of every restaurant and looking to see if the weather forecast might spur or disincentivise going out to eat. Unfortunately, from what I've seen no one has been able to gather any significant results from this method. This makes sense as from what we've seen so far everyone seems to book a table in advance and thus is probably serious about showing up. \n\nThe uselessness of weather in our forecasting should also be viewed from a business angle. We can only know the weather a few days in advance and with not great accuracy. That's not something stable to forecast our future staffing or food supply from. \n\n### **Back to Cleaning**\nInstead we'll drawl more stable features from the 'as' set to base our model off of. The type of cuisine served and area the restaurant is located in are classic indicators of forecasting future demand. We'll have to encode those columns so our machine can read  them. "},{"metadata":{"trusted":true,"_uuid":"e3ff1efb2076e1a67c2c52d5b72f5591e009c1d7"},"cell_type":"code","source":"lbl = preprocessing.LabelEncoder()\ndata['as']['air_genre_name'] = lbl.fit_transform(data['as']['air_genre_name'])\ndata['as']['air_area_name'] = lbl.fit_transform(data['as']['air_area_name'])\ndata['as'].sample(3)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"46c7869b2f245ac3108be9338933e43413f3e15c"},"cell_type":"markdown","source":"Now to the 'tra' dataset"},{"metadata":{"trusted":true,"_uuid":"007881e3614f65aac0c7b4f588bce5aa7cf34c36"},"cell_type":"code","source":"data['tra'].sample(4)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3193bcf26b1910c1ea5adcc7134e32fee3fa76c7"},"cell_type":"markdown","source":"This time we'll use the pandas datetime method a little differently. Instead of having it pull apart a date & time stamp into just the date, we'll have it extract a day of the week, month, year AND date from  a date stamp. Very useful library. "},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"f8c29c8c3b9330f342c9c1584b746cd409aa0031"},"cell_type":"code","source":"data['tra']['visit_date'] = pd.to_datetime(data['tra']['visit_date'])\ndata['tra']['day_of_week'] = data['tra']['visit_date'].dt.dayofweek\ndata['tra']['month'] = data['tra']['visit_date'].dt.month\ndata['tra']['year'] = data['tra']['visit_date'].dt.year\ndata['tra']['visit_date'] = data['tra']['visit_date'].dt.date\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ef4b5aa27ea515851d31d088964f137c10d218ce"},"cell_type":"code","source":"data['tra'].sample(4)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6d3e2acc1e5fbd91f5a55d39c24075e513128e44"},"cell_type":"markdown","source":"Looks good. For day_of_week, Monday=0 if you're wondering. Moving on"},{"metadata":{"trusted":true,"_uuid":"0ddf69f97d1c662ccd108dab68d7d52e6f578783"},"cell_type":"code","source":"data['hol'].sample(5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bb53c5aa0efd6a532d154bf25a0c77cd33530a74"},"cell_type":"markdown","source":"Our date info looks pretty good. Let's just convert it to a datetime object so it can merge with our other table's visit_date later on. We'll also drop the day_of_week column as we already built one in 'tra' and what matters most about this table is the holiday flag data"},{"metadata":{"trusted":true,"_uuid":"79f67491e00fddbdf4fa6b8593fab121f1c83e53"},"cell_type":"code","source":"data['hol'] = data['hol'].drop(columns=['day_of_week'])\ndata['hol']['visit_date'] = pd.to_datetime(data['hol']['visit_date'])\ndata['hol']['visit_date'] = data['hol']['visit_date'].dt.date\ndata['hol'].sample(4)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ed5ec8fc08c796369ca6ffe54c088d1819a60238"},"cell_type":"markdown","source":"All right, let's see what's in the next table"},{"metadata":{"trusted":true,"_uuid":"47d438492e9bddcd5f7212175f77d940ef56dd80"},"cell_type":"code","source":"data['hs'].sample(4)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c67fdb76724b443b082de329daf566264eaaf74a"},"cell_type":"markdown","source":"I wish I had some use for this data. All of this data is already in the 'as' table though. The id column is unique but does not provide any additional info for solving the problem. Adding it in only adds redundency and increases the risk of overfitting our model. Looks like we're gona have to walk away from this one. "},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"f20939a39586030e6a1894c436333fb0436fea01"},"cell_type":"markdown","source":"And our last dataset to clean up is 'tes'"},{"metadata":{"trusted":true,"_uuid":"e0a17a8543db740cbc6f67b5f88f308113e6c093"},"cell_type":"code","source":"data['tes'].sample(3)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"186eb9153885b8a03829b4e5f469c95b38a85801"},"cell_type":"markdown","source":"It might not look like there's much for us to work with but if you look closer there's a date stamp in the id we can pull out and extract more data from. Feature engineering to the rescue!"},{"metadata":{"trusted":true,"_uuid":"615c9ad6eccab852ab781b0a9c75cc810b1f84c9"},"cell_type":"code","source":"data['tes']['visit_date'] = data['tes']['id'].map(lambda x: str(x).split('_')[2])\ndata['tes']['air_store_id'] = data['tes']['id'].map(lambda x: '_'.join(x.split('_')[:2]))\ndata['tes']['visit_date'] = pd.to_datetime(data['tes']['visit_date'])\ndata['tes']['day_of_week'] = data['tes']['visit_date'].dt.dayofweek\ndata['tes']['month'] = data['tes']['visit_date'].dt.month\ndata['tes']['year'] = data['tes']['visit_date'].dt.year\ndata['tes']['visit_date'] = data['tes']['visit_date'].dt.date\ndata['tes'].sample(4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"5f7f44ce29dec8ea4e7181d27bac37075b0f52ec"},"cell_type":"markdown","source":"With the datasets now all clean and related to one another. Now we can merge them all into a nice train and test set with our 'tra' and 'tes' files acting as the base we'll merge all the other datasets onto. "},{"metadata":{"trusted":true,"_uuid":"72ba479f824319d57e1127e7cd324db228c28709","collapsed":true},"cell_type":"code","source":"# Store each unique restaurant in an array\nunique_stores = data['tes']['air_store_id'].unique()\n# Break each restaurant into 7 rows to track each day of the week\nstores = pd.concat([pd.DataFrame({'air_store_id': unique_stores, \n                                  'day_of_week': [i]*len(unique_stores)}) for i in range(7)], \n                   axis=0, ignore_index=True)\n# Make a temporary variable to store new 'tra' features on visitors\n# Then merge that new feature into our stores dataframe \ntmp = data['tra'].groupby(['air_store_id', 'day_of_week'], \n                          as_index=False)['visitors'].min().rename(columns={'visitors':'min_visitors'})\nstores = stores.merge(tmp, how='left', on=['air_store_id','day_of_week'])\n# Continue this process for mean, max, and count of visitors\ntmp = data['tra'].groupby(['air_store_id', 'day_of_week'],\n                         as_index=False)['visitors'].mean().rename(columns={'visitors':'mean_visitors'})\nstores = stores.merge(tmp, how='left', on=['air_store_id', 'day_of_week'])\ntmp = data['tra'].groupby(['air_store_id', 'day_of_week'], \n                          as_index=False)['visitors'].median().rename(columns={'visitors':'median_visitors'})\nstores = stores.merge(tmp, how='left', on=['air_store_id', 'day_of_week'])\ntmp = data['tra'].groupby(['air_store_id', 'day_of_week'], \n                          as_index=False)['visitors'].max().rename(columns={'visitors':'max_visitors'})\nstores = stores.merge(tmp, how='left', on=['air_store_id', 'day_of_week'])\ntmp = data['tra'].groupby(['air_store_id', 'day_of_week'], \n                          as_index=False)['visitors'].count().rename(columns={'visitors':'visitor count'})\nstores = stores.merge(tmp, how='left', on=['air_store_id', 'day_of_week'])\n# Now we'll merge 'as' in\nstores = stores.merge(data['as'], how='left', on=['air_store_id'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d398104cd12ac007ce1a433fc32dcd16c220d02f"},"cell_type":"code","source":"# Check everything looks good and is machine readable\nstores.head(4)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"18874d1a64797f8c383a5be785b2b6c42a91d73c"},"cell_type":"markdown","source":"Great. Now we'll build the actual train and test variables to store our new stores table data and add in the remaining tables"},{"metadata":{"trusted":true,"_uuid":"ee2e7f9a869cbf250c7e9721c93ee92b09e9483e"},"cell_type":"code","source":"train = pd.merge(data['tra'], data['hol'], how='left', on=['visit_date'])\ntrain = pd.merge(data['tra'], stores, how='left', on=['air_store_id', 'day_of_week'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"61a68183f9bc0d54a854d60b6832f8eaed661187","scrolled":true},"cell_type":"code","source":"test = pd.merge(data['tes'], data['hol'], how='left', on=['visit_date'])\ntest = pd.merge(data['tes'], stores, how='left', on=['air_store_id', 'day_of_week'])\ntest.sample(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e84f26083085611f1e6462ba3e9496a51da07387"},"cell_type":"code","source":"data['ar'] = data['ar'].merge(data['hr'], how='left', on=['air_store_id', 'visit_date', 'reserve_datetime_diff', 'reserve_visitors'])\ndata['ar'].sample(4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"36cdd1315677adab7900514851ef3a5a358d9b31","scrolled":true,"collapsed":true},"cell_type":"code","source":"train = train.merge(data['ar'], how='left', on=['air_store_id', 'visit_date'])\ntest = test.merge(data['ar'], how='left', on=['air_store_id', 'visit_date'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8d1e6846ab86784032a63f74434f26d121d303a4"},"cell_type":"code","source":"train.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"83609e123f9db4c9a4c4fc8f3e4dd0b0bf345cdf"},"cell_type":"markdown","source":"With our tables all merged and cleaned, the last bit of prep work we'll need is to convert null values to -1. Our machine doesn't like NaNs and replacing with -1 will ensure those previously empty values don't bias our result. And separate out the columns we want our machine to make predictions from. Notice we're mostly dropping columns we needed for joining disparate tables but do not add value in a regression analysis."},{"metadata":{"trusted":true,"_uuid":"9c4537dabcb00b5a3a443672b7fd37cbac106b54"},"cell_type":"code","source":"col = [c for c in train if c not in ['id', 'air_store_id', 'visit_date', 'visitors']]\ntrain = train.fillna(-1)\ntest = test.fillna(-1)\ntrain.sample(4)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ff01dc246a4bd384b6e35a8b331d10595cc3470a"},"cell_type":"markdown","source":"## Modeling & Ensembling"},{"metadata":{"_uuid":"fb63d1a620bf04891602f3760761d4b2c1ff9de8"},"cell_type":"markdown","source":"Now to the cool stuff. I won't say the fun stuff since cleaning and merging can be fun as well in a piecing the puzzle together sort of way. \n\nWe're going to make our first demand forecase by running our train dataframe through an Extremely Randomized Trees Regressor and a KNN Regressor. Then taking the average (ensemble) of the two predictions to use are our initial submission file. \n\nWe chose these two models as both are robust and complement each other well as one tends to have low bias while the other has low variance (our attempt at leveling bias-variance tradeoff). Ensembling models is the name of the game these days and if I was going to pick just two for a problem like ours (numerical time series forecasting) I'd go with these two. Of course feel free to try other combinations of models. If another combination works better please share in the comments."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"c34ad7cb20d2ed0678d2974de2bdf439a3584320"},"cell_type":"code","source":"def RMSLE(y, pred):\n    return metrics.mean_squared_error(y, pred)**0.5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3fbbd7bf8e854af61ecb2eedfdd064be8ff6b164"},"cell_type":"code","source":"etc = ensemble.ExtraTreesRegressor(n_estimators=225, max_depth=5, n_jobs=-1, random_state=3)\nknn = neighbors.KNeighborsRegressor(n_jobs=-1, n_neighbors=4)\netc.fit(train[col], np.log1p(train['visitors'].values))\nknn.fit(train[col], np.log1p(train['visitors'].values))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"723bc125640b5bada5a59a2139bd84c6afe89abe"},"cell_type":"code","source":"test['visitors'] = (etc.predict(test[col]) / 2) + (knn.predict(test[col]) / 2)\ntest['visitors'] = np.expm1(test['visitors']).clip(lower=0.)\nsub1 = test[['id', 'visitors']].copy()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7b917fb434d23e29cb253bfccaa976d327297f54"},"cell_type":"markdown","source":"And with that we're ready for the second approach. Hope you're ready for some handmade weights, log transformations, and pythagorean means!"},{"metadata":{"_uuid":"674abfa13f4acc95631d0ef925136242eb1cca1e"},"cell_type":"markdown","source":"# Approach 2 (Good Ole' Statistics)"},{"metadata":{"_uuid":"d202fa5eb55c173ffe559e221424ac593d8ffb08"},"cell_type":"markdown","source":"Now begins our second approach that does away with fancy ML trees and neighbors replacing it with a simple yet effective approach of taking the average visitor count for each day of the week and nudging that average a little to better reflect recent visits than older ones. \n\nIf you're into deep learning think of it like the weight bias we apply to each data point and alter as we learn more. The only difference is we're deciding what the weight value is going to be upfront instead of using backpropogation to continual refine it.\n\nAs you'll see, the results of both approaches are not that different.\n\nTo start, we'll need a clean set of tables again."},{"metadata":{"trusted":true,"_uuid":"d0dea2641631ea8d5c5d273f8d94436a03e5ffd4","collapsed":true},"cell_type":"code","source":"# Create a glob expression for finding and reading all .csv files in the data warehouse\ndfs = {re.search('/([^/\\.]*)\\.csv', fn).group(1):\n      pd.read_csv(fn) for fn in glob.glob('../input/*.csv')}\n\n# store the CSVs locally for quick access\nfor k, v in dfs.items(): locals()[k] = v","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"701baae3049a02537b1450535f40ae3deb8f7ab4"},"cell_type":"markdown","source":"## More Cleaning and Feature Engineering\n\nNext we'll make our weights. We want our weights to discount older dates more than newer ones and we want to do so using a multiplicative function so that the relationship between weights is consistent (this will matter when we convert our visitor data to log scale).\n\nIn terms of what exponent to choose hklee already tested out a few and found 5 to be best. I left their test results commented out if you're interested"},{"metadata":{"trusted":true,"_uuid":"bc7fe18c7090ee76503cbe11865e567ce363b83f","collapsed":true},"cell_type":"code","source":"#date_info['weight'] = ((date_info.index + 1) / len(date_info))       # LB 0.509\n#date_info['weight'] = ((date_info.index + 1) / len(date_info)) ** 2  # LB 0.503\n#date_info['weight'] = ((date_info.index + 1) / len(date_info)) ** 3  # LB 0.500\n#date_info['weight'] = ((date_info.index + 1) / len(date_info)) ** 4  # LB 0.498\ndate_info['weight'] = ((date_info.index + 1) / len(date_info)) ** 5 # LB 0.497","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6549eee1b7f5ee4f3208c0aaad4a13d9e86f823e"},"cell_type":"markdown","source":"With weight values decided we'll now merge them based on date with the newest dates getting the smallest weights. We'll then convert our visitor count data to log to reduce skew before applying the weights."},{"metadata":{"trusted":true,"_uuid":"8d994cfc5c14c87bb593a1083576ea23b915d006"},"cell_type":"code","source":"visit_data = air_visit_data.merge(date_info, left_on = 'visit_date', right_on = 'calendar_date', how = 'left')\nvisit_data.drop('calendar_date', axis=1, inplace = True)\nvisit_data['visitors'] = visit_data.visitors.map(pd.np.log1p)\nvisit_data.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8b82e4562c9cc18bfcac86271077799cdd37d257"},"cell_type":"markdown","source":"All right we got our weights and visitors under the same roof. We're ready to apply the weights and update our visitor count based off the result. The formula we'll be using is called weighted arithmetic mean and is worth remembering."},{"metadata":{"trusted":true,"_uuid":"1293fa627b1d9f6ffa4fe440dc473a38019fc127"},"cell_type":"code","source":"wmean = lambda x:((x.weight * x.visitors).sum() / x.weight.sum())\nvisitors = visit_data.groupby(['air_store_id', 'day_of_week', 'holiday_flg']).apply(wmean).rename('visitors').reset_index()\nvisitors.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e3da3dff3a63fe9b9cd16b23e1359e6517849b4d"},"cell_type":"markdown","source":"Looks great. Now lets apply those visitor values where it counts; the sample_submission table."},{"metadata":{"trusted":true,"_uuid":"459ac2b95b0d958a9209661693020e2b17ffe208","collapsed":true},"cell_type":"code","source":"sample_submission['air_store_id'] = sample_submission.id.map(lambda x: '_'.join(x.split('_')[:-1]))\nsample_submission['calendar_date'] = sample_submission.id.map(lambda x: x.split('_')[2])\nsample_submission.drop('visitors', axis=1, inplace=True)\nsample_submission = sample_submission.merge(date_info, on='calendar_date', how='left')\nsample_submission = sample_submission.merge(visitors, on=[\n    'air_store_id', 'day_of_week', 'holiday_flg'], how='left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"689eb60a29f82cda06b8e592a2141be01e6c6e2c"},"cell_type":"code","source":"sample_submission.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"efe0953081326e3c68d14a77523882c339bed6ad"},"cell_type":"markdown","source":"Now most of our store dates and days have visitor count predictions set but a quick call of null values will show our merging strategy still left some holes in the data.\n\nOur first move will be to fill holidays dates that are missing data with the weighted mean of that stores normal day of the week. So a Monday holiday will get replaced with a normal Monday. If you look on your own you'll find this isn't a big deal as holiday visitor counts are not all that different than normal visitor counts. \n\n'visitor_y' is specified because to make the comparison along the holiday_flg we'll end up duplicating holiday_flg and visitors. Here we are saying we want to use the values of the second visitor count which is the value from our visitors dataframe."},{"metadata":{"trusted":true,"_uuid":"57def9c3e660c6712a9038e5b0942b94b7e41254"},"cell_type":"code","source":"sample_submission.visitors.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0c4aa938203c94cb98405e8afeebdf66b14efaad","collapsed":true},"cell_type":"code","source":"missings = sample_submission.visitors.isnull()\nsample_submission.loc[missings, 'visitors'] = sample_submission[missings].merge(\n    visitors[visitors.holiday_flg == 0], on=('air_store_id', 'day_of_week'), \n    how='left')['visitors_y'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c47ec4694ff725fd412e055b2ba09c2e478ad96b"},"cell_type":"code","source":"sample_submission.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"60dbe9bb3011e6721daf3bbf39184718d235ca49"},"cell_type":"markdown","source":"Notice we still have a lot of values where the merge criteria we outlined above didn't line up. That's all right, we will fill these remaining null values with a less precise but still pretty good approximation based on the overall average visitors for the store"},{"metadata":{"trusted":true,"_uuid":"a0db3d0859b309b449e84797c47b2ecdceead437","collapsed":true},"cell_type":"code","source":"missings = sample_submission.visitors.isnull()\nsample_submission.loc[missings, 'visitors'] = sample_submission[missings].merge(\n    visitors[['air_store_id', 'visitors']].groupby('air_store_id').mean().reset_index(), \n    on='air_store_id', how='left')['visitors_y'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a00c53cfb9625c4bc9c1e2a48f09e47c4b06ed95"},"cell_type":"code","source":"# Double check we filled all the missing values\nsample_submission.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"54d43dc87dc70b72a1f3f5318cac9f964471a4ee"},"cell_type":"markdown","source":"## Final Model For Submission\n\nWith every air_store_id day given a prediction it's time to store the results in variable 'sub2' which we'll then merge with 'sub1' from our machine learning ensemble."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"75cc7fefa6e2db97985df996e56969e4f5e646bf"},"cell_type":"code","source":"sample_submission['visitors'] = sample_submission.visitors.map(pd.np.expm1)\nsub2 = sample_submission[['id', 'visitors']].copy()\nsub_merge = pd.merge(sub1, sub2, on='id', how='inner')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fcfefe798c8a6587487d79f055f31bf82c2ce2e1"},"cell_type":"markdown","source":"Now it is time to make our final merge! Here I chose the geometric mean as my mathematical average of choice as it is less vulnerable to being skewed by the larger value of the two and is best used when averaging values of a multiplicative nature which our weighted means model definitely is. I'll also leave the harmonic mean function below in case anyone wants to try that method instead. \n\nAnd with that, I hope you learned a thing or two about merging dataframes, data transformations, or building a predictive model by hand with some classic statistics. If you see any spots in this notebook that could use improvement or further clarification please share in the comments! Happy modeling!"},{"metadata":{"trusted":true,"_uuid":"b4bcff10972ca4815f8e74ed725a5590507cd576"},"cell_type":"code","source":"## Geometric Mean  \nsub_merge['visitors'] = (sub_merge['visitors_x'] * sub_merge['visitors_y']) ** (1/2)\nsub_merge[['id', 'visitors']].to_csv('sub_geo_mean.csv', index = False)\nsub_merge[['id', 'visitors']].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"df07c2ad2e4762e14b43dfdb497aeffd30486c75"},"cell_type":"code","source":"## Harmonic Mean \nsub_merge['visitors'] = 2/(1/sub_merge['visitors_x'] + 1/sub_merge['visitors_y'])\nsub_merge[['id', 'visitors']].to_csv('sub_hrm_mean.csv', index = False)\nsub_merge[['id', 'visitors']].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"553577d7df46e92ff72382a6819c23639f3e59e0"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}