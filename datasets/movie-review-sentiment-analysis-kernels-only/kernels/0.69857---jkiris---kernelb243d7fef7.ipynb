{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport re\nimport sys\nimport pickle\nimport time\nimport math\nimport random\nimport datetime\nimport shutil\nimport textblob\n\nimport numpy as np \nimport pandas as pd \nimport tensorflow as tf\nfrom tensorflow.contrib import learn\nfrom tensorflow.contrib.training import HParams\nfrom nltk.tokenize import TweetTokenizer\nfrom tensorflow.contrib.learn import preprocessing\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import KFold\nfrom sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom sklearn.preprocessing import OneHotEncoder","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"_WORD_RE = dict()\n\ndef _get_regexp(pat):\n    if pat not in _WORD_RE:\n        _WORD_RE[pat] = re.compile(pat)\n    return _WORD_RE[pat]\n\ndef word_cut(sentence, cut_by=' '):\n    # words = filter(bool, str_decode(sentence, coding).split(' '))\n\n    words = filter(bool, _get_regexp(cut_by).split(sentence))\n    return words\n\n\nclass WordVocabProcessor(preprocessing.VocabularyProcessor):\n    def __init__(self, max_document_length, min_frequency=0,\n                 vocabulary=None, cut_by=' ', **kwargs):\n        super(WordVocabProcessor, self).__init__(max_document_length, min_frequency,\n                                                  vocabulary, tokenizer_fn=None)\n        self.cut_by = cut_by\n\n    def fit(self, raw_documents, unused_y=None):\n        self.vocabulary_.add('<PAD>')\n        for tokens in self.tokenizer_(raw_documents):\n            for token in tokens:\n                self.vocabulary_.add(token)\n\n        if self.min_frequency > 0:\n            self.vocabulary_.trim(self.min_frequency)\n\n        self.vocabulary_.freeze()\n        return self\n\n    def tokenizer_(self, documents):\n        for doc in documents:\n            yield word_cut(doc, cut_by=self.cut_by)\n\n    def transform(self, raw_documents):\n        for tokens in self.tokenizer_(raw_documents):\n            word_ids = np.zeros(self.max_document_length, np.int64)\n            for idx, token in enumerate(tokens):\n                if idx >= self.max_document_length:\n                    break\n                word_ids[idx] = self.vocabulary_.get(token)\n\n            yield word_ids\n\n    @property\n    def vocab_size(self):\n        return len(self.vocabulary_)\n\ndef load_train_df():\n    df = pd.read_csv('../input/movie-review-sentiment-analysis-kernels-only/train.tsv', '\\t')\n#     df['Phrase'] = df['Phrase'].str.lower()\n    return df\n\ndef load_test_df():\n    df = pd.read_csv('../input/movie-review-sentiment-analysis-kernels-only/test.tsv', '\\t')\n#     df['Phrase'] = df['Phrase'].str.lower()\n    return df\n\ndef load_made_df():\n    return pd.concat([pd.read_csv('../input/movie-review-sentiment-analysis-man-made/trans_0.csv', ','), \n                         pd.read_csv('../input/movie-review-sentiment-analysis-man-made/trans_1.csv', ',')])\n\ndef build_word_vocab():\n    df_train = load_train_df()\n    df_test = load_test_df()\n    texts = list(df_train.Phrase) + list(df_test.Phrase)\n    \n    vocab = WordVocabProcessor(max_document_length=60, cut_by=r'[ \\-\\\\/]')\n    vocab.fit(texts)\n    print('Build word vocab with size: %s' % vocab.vocab_size)\n    return vocab","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1fc444de7f3979984badf394aa48c6c8d8974ea1"},"cell_type":"code","source":"class UniformRandomEmbedding(object):\n    def __init__(self, vocab_size, embedding_size):\n        self.vocab_size = vocab_size\n        self.embedding_size = embedding_size\n        self.W_ = None\n\n    def W(self):\n        if self.W_ is None:\n            self.W_ = np.random.uniform(-1.0, 1.0, (self.vocab_size, self.embedding_size))\n\n        return self.W_.astype(\"float32\")\n\n    def save(self, filename):\n        if self.W_ is None:\n            self.W()\n\n        with open(filename, 'wb') as f:\n            f.write(pickle.dumps(self))\n\n    @classmethod\n    def restore(cls, filename):\n        with open(filename, 'rb') as f:\n            return pickle.loads(f.read())\n\n        \nclass FastTextEmbedding(UniformRandomEmbedding):\n    def __init__(self, vocab):\n        self.vocab = vocab\n        super(FastTextEmbedding, self).__init__(vocab.vocab_size, 300 + 2)\n\n    def W(self):\n        vocab_size = self.vocab.vocab_size\n        \n        if self.W_ is None:\n            words_need = set([self.vocab.vocabulary_.reverse(idx) for idx in range(vocab_size)])\n            words_need |= set([w.lower() for w in words_need])\n            words_need.add(\"n't\")\n            \n            word_vec = dict()\n            with open('../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec') as f:\n                f.readline()\n                for line in f:\n                    dd = line.strip().split(' ')\n                    word, vec = dd[0], np.asarray(dd[1:], dtype=\"float32\")\n                    if word in words_need:\n                        word_vec[word] = vec\n\n            W = super(FastTextEmbedding, self).W()\n            exists_cnt = 0\n            for idx in range(vocab_size):\n                word = self.vocab.vocabulary_.reverse(idx)\n                sent = textblob.TextBlob(word).sentiment\n                W[idx, -2:] = [sent.polarity, sent.subjectivity]\n\n                if word == \"n't\":\n                    word = 'not'\n                    print('redirect n\\'t to not')\n\n                if word in word_vec:\n                    exists_cnt += 1\n                    W[idx, :-2] = word_vec[word]\n                elif word.lower() in word_vec:\n                    exists_cnt += 1\n                    W[idx, :-2] = word_vec[word.lower()]\n                else:\n                    print(\"Can not find %s in fast text embedding.\" % word)\n            print('Found %s / %s in fast-text embedding vocab.' % (exists_cnt, vocab_size))\n\n            del word_vec\n\n            self.W_ = W\n\n        return self.W_.astype(\"float32\")\n\n    \ndef build_fast_text_embedding(vocab):\n    embedding = FastTextEmbedding(vocab)\n    embedding.W()\n    return embedding\n\n\nclass GloveEmbedding(UniformRandomEmbedding):\n    def __init__(self, vocab, embedding_size=300):\n        assert embedding_size in (50, 100, 200, 300)\n        self.vocab = vocab\n        self.embedding_size_raw = embedding_size\n        super(GloveEmbedding, self).__init__(vocab.vocab_size, embedding_size + 2)\n\n    def W(self):\n        vocab_size = self.vocab.vocab_size\n        \n        if self.W_ is None:\n            words_need = set([self.vocab.vocabulary_.reverse(idx) for idx in range(vocab_size)])\n            words_need |= set([w.lower() for w in words_need])\n            words_need.add(\"n't\")\n            \n            word_vec = dict()\n            with open('../input/glove6b/glove.6B.%sd.txt' % self.embedding_size_raw) as f:\n                for line in f:\n                    dd = line.strip().split(' ')\n                    word, vec = dd[0], np.array(list(map(float, dd[1:])))\n                    if word in words_need:\n                        word_vec[word] = vec\n\n            W = super(GloveEmbedding, self).W()\n            exists_cnt = 0\n            for idx in range(vocab_size):\n                word = self.vocab.vocabulary_.reverse(idx)\n                sent = textblob.TextBlob(word).sentiment\n                W[idx, -2:] = [sent.polarity, sent.subjectivity]\n                \n                if word in word_vec:\n                    exists_cnt += 1\n                    W[idx, :-2] = word_vec[word]\n                elif word.lower() in word_vec:\n                    exists_cnt += 1\n                    W[idx, :-2] = word_vec[word.lower()]\n                else:\n                    print(\"Can not find %s in glove embedding.\" % word)\n            print('Found %s / %s in glove embedding vocab.' % (exists_cnt, vocab_size))\n\n            del word_vec\n\n            self.W_ = W\n\n        return self.W_.astype(\"float32\")\n    \n\ndef build_glove_embedding(vocab):\n    embedding = GloveEmbedding(vocab, 100)\n    embedding.W()\n    return embedding","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cd156322490454b9596b4f4098fbbfb89a6113d7"},"cell_type":"code","source":"class LabelEncoder(object):\n    def __init__(self):\n        self.labels_ = None\n\n    def _check_fit(self):\n        assert self.labels_, 'LabelEncode not fit yet.'\n\n    def fit(self, y):\n        self.labels_ = list(set(y))\n        return self\n\n    def transform(self, y):\n        self._check_fit()\n\n        labels_d = dict([(l, i) for i, l in enumerate(self.labels_)])\n\n        labels_m = np.zeros((len(y), len(self.labels_)))\n        for idx, label in enumerate(y):\n            labels_m[idx, labels_d.get(label)] = 1.\n\n        return labels_m\n\n    def fit_transform(self, y):\n        self.fit(y)\n        return self.transform(y)\n\n    def reverse(self, logits):\n        self._check_fit()\n\n        return [self.labels_[idx] for idx in np.argmax(logits, 1)]\n\n    @property\n    def num_classes(self):\n        self._check_fit()\n\n        return len(self.labels_)\n\n    def save(self, filename):\n        with open(filename, 'wb') as f:\n            f.write(pickle.dumps(self))\n\n    @classmethod\n    def restore(cls, filename):\n        with open(filename, 'rb') as f:\n            return pickle.loads(f.read())\n\ndef build_label_encoder():\n    df_train = load_train_df()\n    labels = list(df_train.Sentiment)\n    \n    label_ed = LabelEncoder()\n    label_ed.fit(labels)\n    print('Encode %s classes.' % label_ed.num_classes)\n\n    return label_ed\n\n\nclass BinaryLabelEncoder(LabelEncoder):\n    def __init__(self):\n        self.labels_ = [0, 2]\n\n    def _check_fit(self):\n        assert self.labels_, 'LabelEncode not fit yet.'\n\n    def fit(self, y):\n        return self\n\n    def transform(self, y):\n        self._check_fit()\n\n        labels_m = np.zeros((len(y), len(self.labels_)))\n        for idx, label in enumerate(y):\n            if label == 2:\n                labels_m[idx, 1] = 1.\n\n        return labels_m\n\n\ndef build_binary_label_encoder():\n    label_ed = BinaryLabelEncoder()\n    return label_ed\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6979ac3d395aa405f2fa4a603705a9ca0f5d9de3","scrolled":true},"cell_type":"code","source":"vocab = build_word_vocab()\n# embedding_g = build_glove_embedding(vocab)\nembedding_ft = build_fast_text_embedding(vocab)\nlabel_ed = build_label_encoder()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dce9ed7793c7a14f5c12b9279fad564618e1e015"},"cell_type":"code","source":"def computeTF(df):\n    dicts = []\n    for i in range(0, 5):\n        tf_dict = {}\n        sentences = list(df[df['Sentiment']==i]['Phrase'].values)\n        for sentence in sentences:\n            sentence = sentence.lower()\n            words = sentence.split()\n            for word in words:\n                if word not in tf_dict:\n                    tf_dict[word] = 1\n                else:\n                    tf_dict[word] += 1\n        total_words = sum(tf_dict.values())\n        for word, val in tf_dict.items():\n            tf_dict[word] = val * 1.0/total_words\n        dicts.append(tf_dict)\n    return dicts\n\ndef computeIDF(tf_dicts):\n    keys_all = []\n    idf_dict = {}\n    for i in range(0, 5):\n        keys_all += list(tf_dicts[i].keys())\n    for key in keys_all:\n        if key not in idf_dict:\n            idf_dict[key] = 1\n        else:\n            idf_dict[key] += 1\n    for word, val in idf_dict.items():\n            idf_dict[word] = math.log(5.0 / idf_dict[word])\n    return idf_dict\n\ndef computeTFIDF(tf_dicts, idf_dict):\n    tfidf_dicts = []\n    for i in range(0, 5):\n        tfidf_dict = {}\n        for word, val in tf_dicts[i].items():\n            tfidf_dict[word] = tf_dicts[i][word] * idf_dict[word]\n        tfidf_dicts.append(tfidf_dict)\n    return tfidf_dicts\n\ndef prepare_keywords():\n    df = load_train_df()\n    tf_dicts = computeTF(df)\n    idf_dict = computeIDF(tf_dicts)\n    tfidf_dicts = computeTFIDF(tf_dicts, idf_dict)\n\n    keywords = []\n    for i in range(0, 5):\n        important_words = sorted(tfidf_dicts[i].items(), key=lambda x: x[1], reverse=True)[1:100]\n        keywords.append([item[0] for item in important_words])\n    all_keywords = [keyword for keyword_list in keywords for keyword in keyword_list]\n    \n    return keywords, all_keywords\n\nkeywords, all_keywords = prepare_keywords()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"aeeb47241c2db9631eede1f9e6b45960ed68e892"},"cell_type":"code","source":"class Node(object):\n    def __init__(self, parent, pid, phrase):\n        self.parent = parent\n        self.children = []\n        self.pid = pid\n        self.phrase = phrase\n    \n    def add_child(self, node):\n        self.children.append(node)\n\ndef build_sentence_tree(dd):\n    root, current = None, None\n    \n    for _, row in dd.iterrows():\n        pid, phrase = row.PhraseId, row.Phrase\n        if not root:\n            root = Node(None, pid, phrase)\n            current = root\n            continue\n        \n        while True:\n            if current is None:\n                break\n            if phrase in current.phrase:\n                node = Node(current, pid, phrase)\n                current.add_child(node)\n                current = node\n                break\n            else:\n                current = current.parent\n                \n    pids_start, current = [], root\n    while current:\n        pids_start.append(current.pid)\n        if not current.children:\n            break\n        current = current.children[0]\n    \n    pids_end, current = [], root\n    while current:\n        pids_end.append(current.pid)\n        if not current.children:\n            break\n        current = current.children[-1]\n    \n    pid_level_dct = dict()\n    level, lid, pids_leaf = [root], 1, []\n    while level:\n        pids_leaf += [n.pid for n in level if not n.children]\n        for n in level:\n            pid_level_dct[n.pid] = lid\n        \n        level = [n for r in level for n in r.children]\n        lid += 1\n        \n    return pd.DataFrame([(pid,\n                          1 if pid in pids_start else 0, \n                          1 if pid in pids_end else 0, \n                          1 if pid in pids_leaf else 0, \n                          pid_level_dct.get(pid, lid)) for pid in dd.PhraseId], \n                        columns=['PhraseId', 'is_start', 'is_end', 'is_leaf', 'tree_level'], \n                        index=dd.index)\n\ndef tree_feat(df):\n    tree_feat = pd.concat([build_sentence_tree(df[df.SentenceId == sid]) for sid in df.SentenceId.unique()])\n    assert tree_feat.shape[0] == df.shape[0]\n    return tree_feat  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"83fcbd3020d6e67426dbbb051bc99660959e78ec"},"cell_type":"code","source":"FEATS_DENCE = [\"phrase_count\", \"word_count\", \"has_upper\", \"sentence_end\", \"after_comma\", \n              \"sentence_start\", 'is_start', 'is_end', 'is_leaf', 'tree_level', \n              \"sentiment0_words\", \"sentiment1_words\",\"sentiment3_words\", \"sentiment4_words\", \"no_sentiment_words\"]\n\ndef transform_dence(df):\n    dence_df = pd.DataFrame({\n        \"PhraseId\": df.PhraseId,\n        \"phrase_count\": df.groupby(\"SentenceId\")[\"Phrase\"].transform(\"count\"),\n        \"word_count\": df[\"Phrase\"].apply(lambda x: len(x.split())),\n        \"has_upper\": df[\"Phrase\"].apply(lambda x: x.lower() != x),\n        \"sentence_end\": df[\"Phrase\"].apply(lambda x: x.endswith(\".\")),\n        \"after_comma\": df[\"Phrase\"].apply(lambda x: x.startswith(\",\")),\n        \"sentence_start\": df[\"Phrase\"].apply(lambda x: \"A\" <= x[0] <= \"Z\"),\n        \"sentiment0_words\": df[\"Phrase\"].apply(lambda x: len(set(x.split()).intersection(set(keywords[0])))),\n        \"sentiment1_words\": df[\"Phrase\"].apply(lambda x: len(set(x.split()).intersection(set(keywords[1])))),\n        \"sentiment3_words\": df[\"Phrase\"].apply(lambda x: len(set(x.split()).intersection(set(keywords[3])))),\n        \"sentiment4_words\": df[\"Phrase\"].apply(lambda x: len(set(x.split()).intersection(set(keywords[4])))),\n        \"no_sentiment_words\": df[\"Phrase\"].apply(lambda x: len(set(x.split()))-len(set(x.split()).intersection(set(all_keywords)))),\n    })\n    \n    tree_df = tree_feat(df)  \n    \n    df_d = pd.merge(dence_df, tree_df, on=\"PhraseId\")\n    assert df_d.shape[0] == df.shape[0]\n    return df_d.loc[:, FEATS_DENCE]\n\n\ndef prepare_train(df, random_state=None):\n    df_d = transform_dence(df)\n    \n    df = pd.concat([df, df_d], axis=1)\n    \n    if random_state is None:\n        random_state = random.randint(0, 1000)\n\n    print('Shuffle with random_state: %s' % random_state)\n    \n    df = df.sample(frac=1, random_state=random_state)\n    \n    return np.array(df.Phrase), np.array(df.SentenceId), np.array(df.Sentiment), np.array(df.loc[:, FEATS_DENCE])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1d7262adf2300e77b7424370a4600386a87cba8c"},"cell_type":"code","source":"class TrainInputFeeder(object):\n    def __init__(self, X, y, vocab, label_ed=None, num_epochs=5, batch_size=128, shuffle=True):\n        self.X = np.array(X)\n        self.y = np.array(y)\n\n        self.vocab = vocab\n        self.label_ed = label_ed\n        self.num_epochs = num_epochs\n        self.batch_size = batch_size\n        self.shuffle = shuffle\n\n    def sample_weight(self, y):\n        y = np.array(y)\n        return np.ones((y.shape[0], 1))\n    \n    def transform_X(self, X):\n        return np.array(list(self.vocab.transform(X)))\n    \n    def transform_y(self, y):\n        if self.label_ed is None:\n            return y\n        \n        return self.label_ed.transform(y)\n    \n    def __iter__(self):\n        X_size = len(self.X)\n        num_batches_per_epoch = int((X_size - 1) / self.batch_size) + 1\n        print('num batches per epoch: %s' % num_batches_per_epoch)\n\n        for epoch in range(self.num_epochs):\n            print\n            print('start epoch %s' % (epoch + 1))\n            print\n\n            if self.shuffle:\n                shuffle_indices = np.random.permutation(np.arange(X_size))\n                X, y = self.X[shuffle_indices], self.y[shuffle_indices]\n            else:\n                X, y = self.X, self.y\n\n            for batch_num in range(num_batches_per_epoch):\n                start_index = batch_num * self.batch_size\n                end_index = min((batch_num + 1) * self.batch_size, X_size)\n                yield (self.transform_X(X[start_index:end_index]), \n                        self.transform_y(y[start_index:end_index]))\n\n                \nclass TrainInputFeeder2X(object):\n    def __init__(self, X1, X2, y, num_epochs=5, batch_size=128, shuffle=True):\n        self.X1 = np.array(X1)\n        self.X2 = np.array(X2)\n        self.y = np.array(y)\n\n        self.num_epochs = num_epochs\n        self.batch_size = batch_size\n        self.shuffle = shuffle\n\n    def sample_weight(self, y):\n        y = np.array(y)\n        return np.ones((y.shape[0], 1))\n    \n    def __iter__(self):\n        X_size = len(self.X1)\n        num_batches_per_epoch = int((X_size - 1) / self.batch_size) + 1\n        print('num batches per epoch: %s' % num_batches_per_epoch)\n\n        for epoch in range(self.num_epochs):\n            print\n            print('start epoch %s' % (epoch + 1))\n            print\n\n            if self.shuffle:\n                shuffle_indices = np.random.permutation(np.arange(X_size))\n                X1, X2, y = self.X1[shuffle_indices], self.X2[shuffle_indices], self.y[shuffle_indices]\n            else:\n                X1, X2, y = self.X1, self.X2, self.y\n\n            for batch_num in range(num_batches_per_epoch):\n                start_index = batch_num * self.batch_size\n                end_index = min((batch_num + 1) * self.batch_size, X_size)\n                yield (X1[start_index:end_index], X2[start_index:end_index], \n                        y[start_index:end_index])\n                \n\nclass PredictInputFeeder2X(object):\n    def __init__(self, X1, X2, batch_size=128):\n        self.X1 = np.array(X1)\n        self.X2 = np.array(X2)\n        self.result = []\n\n        self.batch_size = batch_size\n\n    def __iter__(self, merge=True):\n        X_size = len(self.X1)\n        num_batches = int((X_size - 1) / self.batch_size) + 1\n\n        for batch_num in range(num_batches):\n            start_index = batch_num * self.batch_size\n            end_index = min((batch_num + 1) * self.batch_size, X_size)\n            yield self.X1[start_index:end_index], self.X2[start_index:end_index]\n\n    def feed_result(self, result):\n        self.result.extend(result)\n\nclass PredictInputFeeder(object):\n    def __init__(self, X, vocab, batch_size=128):\n        self.X = np.array(X)\n        self.result = []\n\n        self.vocab = vocab\n        self.batch_size = batch_size\n    \n    def transform_X(self, X):\n        return np.array(list(self.vocab.transform(X)))\n\n    def __iter__(self, merge=True):\n        X_size = len(self.X)\n        num_batches = int((X_size - 1) / self.batch_size) + 1\n\n        for batch_num in range(num_batches):\n            start_index = batch_num * self.batch_size\n            end_index = min((batch_num + 1) * self.batch_size, X_size)\n            yield self.transform_X(self.X[start_index:end_index])\n\n    def feed_result(self, result):\n        self.result.extend(result)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"06035ad2f4a5d224fd6376d19b6656d6048a99be"},"cell_type":"code","source":"def _single_cell(unit_type, num_units):\n    \"\"\"Create an instance of a single RNN cell.\"\"\"\n\n    if unit_type == \"lstm\":\n        single_cell = tf.contrib.rnn.BasicLSTMCell(num_units)\n    elif unit_type == \"gru\":\n        single_cell = tf.contrib.rnn.GRUCell(num_units)\n    elif unit_type == \"nas\":\n        single_cell = tf.contrib.rnn.NASCell(num_units)\n    else:\n        raise ValueError(\"Unknown unit type %s!\" % unit_type)\n\n    return single_cell\n\n\ndef build_cell(unit_type, num_units, num_layers):\n    cell_list = []\n    for i in range(num_layers):\n        single_cell = _single_cell(\n            unit_type=unit_type,\n            num_units=num_units,\n        )\n        cell_list.append(single_cell)\n\n    if len(cell_list) == 1:  # Single layer.\n        return cell_list[0]\n    else:  # Multi layers\n        return tf.contrib.rnn.MultiRNNCell(cell_list)\n\n    \ndef batch_norm(x, variance_epsilon=0.001):\n    shape_x = x.get_shape()\n    with tf.name_scope(\"bn\"):\n        axis = list(range(len(shape_x) - 1))\n        wb_mean, wb_var = tf.nn.moments(x, axis)\n        scale = tf.Variable(tf.ones(shape_x[-1:]))\n        offset = tf.Variable(tf.zeros(shape_x[-1:]))\n\n        h_bn = tf.nn.batch_normalization(x, wb_mean, wb_var, offset, scale, variance_epsilon)\n\n    return h_bn\n\n\ndef attention(inputs, attention_size, time_major=False, return_alphas=False):\n    if isinstance(inputs, tuple):\n        # In case of Bi-RNN, concatenate the forward and the backward RNN outputs.\n        inputs = tf.concat(inputs, 2)\n\n    if time_major:\n        # (T,B,D) => (B,T,D)\n        inputs = tf.array_ops.transpose(inputs, [1, 0, 2])\n\n    hidden_size = inputs.shape[2].value  # D value - hidden size of the RNN layer\n\n    # Trainable parameters\n    w_omega = tf.Variable(tf.random_normal([hidden_size, attention_size], stddev=0.1))\n    b_omega = tf.Variable(tf.random_normal([attention_size], stddev=0.1))\n    u_omega = tf.Variable(tf.random_normal([attention_size], stddev=0.1))\n\n    with tf.name_scope('v'):\n        # Applying fully connected layer with non-linear activation to each of the B*T timestamps;\n        #  the shape of `v` is (B,T,D)*(D,A)=(B,T,A), where A=attention_size\n        v = tf.tanh(tf.tensordot(inputs, w_omega, axes=1) + b_omega)\n\n    # For each of the timestamps its vector of size A from `v` is reduced with `u` vector\n    vu = tf.tensordot(v, u_omega, axes=1, name='vu')  # (B,T) shape\n    alphas = tf.nn.softmax(vu, name='alphas')         # (B,T) shape\n\n    # Output of (Bi-)RNN is reduced with attention vector; the result has (B,D) shape\n    output = tf.reduce_sum(inputs * tf.expand_dims(alphas, -1), 1)\n\n    if not return_alphas:\n        return output\n    else:\n        return output, alphas\n\n\nclass TextRnnCnn(object):\n    def __init__(self, sequence_length, num_classes, embedding_W,\n                 num_units, attention_size, filter_sizes, num_filters, \n                 num_outputs_fc1, num_outputs_fc2, num_outputs_fc3):\n        self.input_x = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x\")\n        self.input_y = tf.placeholder(tf.float32, [None, num_classes], name=\"input_y\")\n        self.input_d = tf.placeholder(tf.float32, [None, len(FEATS_DENCE)], name=\"input_d\")\n\n        embedding_inputs = self.embed(embedding_W)\n        seq_length = self._seq_length(self.input_x)\n\n        self.embedding_keep_prob = tf.placeholder(tf.float32, name=\"embedding_keep_prob\")\n        self.embedding_inputs = tf.nn.dropout(embedding_inputs, self.embedding_keep_prob)\n\n        rnn_output_lstm, rnn_output_lstm_last = self.build_rnn(self.embedding_inputs, 'lstm', num_units, \n                                                               seq_length, variable_scope='lstm')\n        rnn_output_lstm_attention, _ = attention(rnn_output_lstm, attention_size, return_alphas=True)\n        cnn_output_lstm = self.build_cnn(rnn_output_lstm, sequence_length, filter_sizes, num_filters,\n                                         variable_scope='lstm-cnn')\n\n        rnn_output_gru, rnn_output_gru_last = self.build_rnn(self.embedding_inputs, 'gru', num_units, \n                                                             seq_length, variable_scope='gru')\n        rnn_output_gru_attention, _ = attention(rnn_output_gru, attention_size, return_alphas=True)\n        cnn_output_gru = self.build_cnn(rnn_output_gru, sequence_length, filter_sizes, num_filters,\n                                         variable_scope='gru-cnn')\n\n        self.fc_keep_prob = tf.placeholder(tf.float32, name=\"fc_keep_prob\")\n\n#         cnn_output = tf.concat([cnn_output_lstm, cnn_output_gru], 1)\n        cnn_output = tf.concat([cnn_output_lstm, cnn_output_gru, self.input_d], 1)\n#         cnn_output = tf.concat([rnn_output_lstm_attention, rnn_output_gru_attention, self.input_d], 1)\n        self.cnn_output = tf.nn.dropout(cnn_output, self.fc_keep_prob)\n\n        with tf.name_scope(\"fc1\"):\n            h_fc1_bn = batch_norm(self.cnn_output)\n            print(\"Level fc1 shape: %s\" % h_fc1_bn.get_shape()[1])\n            h_fc1 = tf.contrib.layers.fully_connected(h_fc1_bn, num_outputs_fc1, activation_fn=tf.nn.relu)\n            h_fc1 = tf.concat([h_fc1, self.input_d], 1)\n            h_fc2_d = tf.nn.dropout(h_fc1, self.fc_keep_prob)\n        \n#         with tf.name_scope(\"fc2\"):\n#             h_fc2_bn = batch_norm(h_fc1_d)\n#             print(\"Level fc2 shape: %s\" % h_fc2_bn.get_shape()[1])\n#             h_fc2 = tf.contrib.layers.fully_connected(h_fc2_bn, num_outputs_fc2, activation_fn=tf.nn.relu)\n#             h_fc2 = tf.concat([h_fc2, self.input_d], 1)\n#             h_fc2_d = tf.nn.dropout(h_fc2, self.fc_keep_prob)\n\n        with tf.name_scope(\"fc3\"):\n            h_fc3_bn = batch_norm(h_fc2_d)\n            print(\"Level fc3 shape: %s\" % h_fc3_bn.get_shape()[1])\n            self.h_out = tf.contrib.layers.fully_connected(h_fc3_bn, num_outputs_fc3, activation_fn=tf.nn.relu)\n            self.h_out = tf.concat([self.h_out, self.input_d], 1)\n            # self.h_out = tf.nn.dropout(h_fc2, self.fc_keep_prob)\n\n        # Final (unnormalized) logits and predictions\n        with tf.name_scope(\"output\"):\n            self.h_out = batch_norm(self.h_out)\n            print(\"Level output shape: %s\" % self.h_out.get_shape()[1])\n            self.logits = tf.contrib.layers.fully_connected(self.h_out, num_classes)\n            self.predictions = tf.argmax(self.logits, 1, name=\"predictions\")\n\n        # Calculate mean cross-entropy loss\n        with tf.name_scope(\"loss\"):\n            losses = tf.nn.softmax_cross_entropy_with_logits(logits=self.logits, labels=self.input_y)\n            self.loss = tf.reduce_mean(losses)\n\n        # Accuracy\n        with tf.name_scope(\"accuracy\"):\n            correct_predictions = tf.equal(self.predictions, tf.argmax(self.input_y, 1))\n            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")\n\n    def embed(self, embedding_W):\n        with tf.device('/cpu:0'), tf.name_scope(\"embedding\"):\n            self.W = tf.Variable(embedding_W, name=\"W\")\n            embedding_inputs = tf.nn.embedding_lookup(self.W, self.input_x)\n\n        return embedding_inputs\n\n    def build_cnn(self, rnn_outputs, sequence_length, filter_sizes, num_filters, variable_scope='cnn'):\n        rnn_outputs = tf.expand_dims(rnn_outputs, -1)\n\n        # Create a convolution + maxpool layer for each filter size\n        pooled_outputs = []\n        for i, filter_size in enumerate(filter_sizes):\n            with tf.variable_scope(\"%s-conv-%s\" % (variable_scope, filter_size),\n                                   initializer=tf.keras.initializers.he_uniform()):\n                # Convolution Layer\n                filter_shape = [filter_size, int(rnn_outputs.get_shape()[2]), 1, num_filters]\n                W = tf.get_variable(\"W\", shape=filter_shape)\n                b = tf.get_variable(\"b\", shape=[num_filters])\n                conv = tf.nn.conv2d(\n                    rnn_outputs,\n                    W,\n                    strides=[1, 1, 1, 1],\n                    padding=\"VALID\",\n                    name=\"conv\")\n                # Apply nonlinearity\n                h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n                # Maxpooling over the outputs\n                pooled_max = tf.nn.max_pool(\n                    h,\n                    ksize=[1, sequence_length - filter_size + 1, 1, 1],\n                    strides=[1, 1, 1, 1],\n                    padding='VALID',\n                    name=\"pool\")\n                pooled_outputs.append(pooled_max)\n\n                pooled_avg = tf.nn.avg_pool(\n                    h,\n                    ksize=[1, sequence_length - filter_size + 1, 1, 1],\n                    strides=[1, 1, 1, 1],\n                    padding='VALID',\n                    name=\"pool\")\n                pooled_outputs.append(pooled_avg)\n\n        # Combine all the pooled features\n        num_filters_total = num_filters * len(pooled_outputs)\n        h_pool = tf.concat(pooled_outputs, 3)\n\n        h_pool_flat = tf.reshape(h_pool, [-1, num_filters_total])\n\n        return h_pool_flat\n\n    def build_rnn(self, embedding_inputs, unit_type, num_units, seq_length, variable_scope=\"rnn\"):\n        with tf.variable_scope(variable_scope):#, initializer=tf.orthogonal_initializer()):\n            cell_fw = build_cell(unit_type, num_units, 1)\n            cell_bw = build_cell(unit_type, num_units, 1)\n\n            bi_outputs, bi_state = tf.nn.bidirectional_dynamic_rnn(\n                cell_fw, cell_bw, embedding_inputs,\n                dtype=tf.float32, sequence_length=seq_length,\n                swap_memory=True)\n\n            rnn_outputs = tf.concat(bi_outputs, 2)\n\n#             cell = build_cell(unit_type, num_units, 1)\n#             rnn_outputs, state = tf.nn.dynamic_rnn(\n#                 cell, embedding_inputs, dtype=tf.float32, sequence_length=seq_length,\n#                 swap_memory=True)\n\n        return rnn_outputs, self.last_relevant(rnn_outputs, seq_length)\n\n    @staticmethod\n    def _seq_length(seq):\n        relevant = tf.sign(tf.abs(seq))\n        length = tf.reduce_sum(relevant, reduction_indices=1)\n        length = tf.cast(length, tf.int32)\n        return length\n\n    @staticmethod\n    def last_relevant(seq, length):\n        batch_size = tf.shape(seq)[0]\n        max_length = int(seq.get_shape()[1])\n        input_size = int(seq.get_shape()[2])\n        index = tf.range(0, batch_size) * max_length + tf.maximum(length - 1, 0)\n        flat = tf.reshape(seq, [-1, input_size])\n        return tf.gather(flat, index)\n\n    \ndef build_model(hparams, vocab, label_ed, embedding_W):\n    return TextRnnCnn(\n            vocab.max_document_length, label_ed.num_classes, embedding_W,\n            hparams.num_units, hparams.attention_size, \n            hparams.filter_sizes, hparams.num_filters,\n            hparams.num_outputs_fc1, hparams.num_outputs_fc2, hparams.num_outputs_fc3\n    )\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9a495521341231dc9f42575a94ea4d1117889fa3"},"cell_type":"code","source":"class EarlyStopError(Exception):\n    pass\n\n\nclass EarlyStopMonitor(object):\n    BEST_INIT_VALUE = -999.0\n\n    def __init__(self, sess, saver, mode='min', min_delta=0, patience=0, baseline=None):\n        self.sess = sess\n        self.saver = saver\n\n        self.baseline = baseline\n        self.patience = patience\n        self.min_delta = min_delta\n\n        if mode not in ['min', 'max']:\n            warnings.warn('EarlyStopping mode %s is unknown, '\n                          'fallback to auto mode.' % mode,\n                          RuntimeWarning)\n\n        if mode == 'min':\n            self.monitor_op = np.less\n        elif mode == 'max':\n            self.monitor_op = np.greater\n\n        self.wait = 0\n        self.best = None\n        if self.get_best() is None and self.baseline is not None:\n            self.set_best(self.baseline)\n\n    def get_best(self):\n        return self.best\n\n    def set_best(self, value):\n        self.best = value\n\n    def reset_wait(self):\n        self.wait = 0\n\n    def get_wait(self):\n        return self.wait\n\n    def incr_wait(self):\n        self.wait += 1\n\n    def save_checkpoint(self, *args, **kwargs):\n        path = self.saver.save(self.sess, *args, **kwargs)\n        print(\"Saved model checkpoint to {}\\n\".format(path))\n\n    def on_eval(self, current, *args, **kwargs):\n        best = self.get_best()\n        if best is None:\n            self.set_best(current)\n            self.reset_wait()\n            self.save_checkpoint(*args, **kwargs)\n            return\n\n        if self.monitor_op(current - self.min_delta, best):\n            self.set_best(current)\n            self.save_checkpoint(*args, **kwargs)\n            self.reset_wait()\n        else:\n            self.incr_wait()\n            if self.get_wait() >= self.patience:\n                raise EarlyStopError()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"1fa28671a03a90024f8dc5fb6fb92086b4ae89e7"},"cell_type":"code","source":"def train(hparams, feeder, X_dev, d_dev, y_dev, embedding_W,\n          num_checkpoints=3, evaluate_every=200, patience=5, \n          out_dir=None):\n    timestamp = str(int(time.time()))\n    if out_dir is None:\n        name = '%s_%s' % (hparams.out_prefix, timestamp)\n        out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", name))\n    \n    with tf.Graph().as_default():\n        sess = tf.Session()\n\n        with sess.as_default():\n            # Define Training procedure\n            rnn = build_model(hparams, vocab, label_ed, embedding_W)\n\n            global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n\n            learning_rate = hparams.learning_rate\n            learning_rate_t = tf.placeholder(tf.float32, name=\"learning_rate\")\n            optimizer = tf.train.AdamOptimizer(learning_rate_t)\n            grads_and_vars = optimizer.compute_gradients(rnn.loss)\n            train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n\n            # Output directory for models and summaries\n            print(\"Writing to {}\\n\".format(out_dir))\n            if not os.path.exists(out_dir):\n                os.makedirs(out_dir)\n\n            # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n            checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n            checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n            if not os.path.exists(checkpoint_dir):\n                os.makedirs(checkpoint_dir)\n            saver = tf.train.Saver(tf.global_variables(), max_to_keep=num_checkpoints)\n            monitor = EarlyStopMonitor(sess, saver, mode='max', patience=patience, min_delta=0.)\n\n            ckpt = tf.train.get_checkpoint_state(checkpoint_dir)\n            # if that checkpoint exists, restore from checkpoint\n            if ckpt and ckpt.model_checkpoint_path:\n                saver.restore(sess, ckpt.model_checkpoint_path)\n            else:\n                # Initialize all variables\n                sess.run(tf.global_variables_initializer())\n\n            def train_step(x_batch, d_batch, y_batch):\n                \"\"\"\n                A single training step\n                \"\"\"\n                feed_dict = {\n                    rnn.input_x: x_batch,\n                    rnn.input_y: y_batch,\n                    rnn.input_d: d_batch,\n                    rnn.embedding_keep_prob: hparams.embedding_keep_prob,\n                    rnn.fc_keep_prob: hparams.fc_keep_prob,\n                    learning_rate_t: learning_rate,\n                }\n\n                _, step, loss, accuracy = sess.run(\n                    [train_op, global_step, rnn.loss, rnn.accuracy],\n                    feed_dict)\n\n                if hparams.verbose > 1:\n                    time_str = datetime.datetime.now().isoformat()\n                    print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n\n            def dev_step(x_batch, d_batch, y_batch):\n                \"\"\"\n                Evaluates model on a dev set\n                \"\"\"\n                feed_dict = {\n                    rnn.input_x: x_batch,\n                    rnn.input_y: y_batch,\n                    rnn.input_d: d_batch,\n                    rnn.embedding_keep_prob: 1.0,\n                    rnn.fc_keep_prob: 1.0,\n                }\n                step, loss, accuracy = sess.run(\n                    [global_step, rnn.loss, rnn.accuracy],\n                    feed_dict)\n                if hparams.verbose > 0:\n                    time_str = datetime.datetime.now().isoformat()\n                    print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n                return accuracy\n\n            for X_batch, d_batch, y_batch in feeder:\n                train_step(X_batch, d_batch, y_batch)\n\n                current_step = tf.train.global_step(sess, global_step)\n                if current_step % evaluate_every == 0:\n                    print(\"\\nEvaluation:\")\n                    accuracy = dev_step(X_dev, d_dev, y_dev)\n                    print(\"\")\n\n                    try:\n                        monitor.on_eval(accuracy, checkpoint_prefix, global_step=current_step)\n                    except EarlyStopError as e:\n                        break\n\n    return out_dir","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4bbf9b4150f424a6f8d903f5b610d940a260c4c8"},"cell_type":"code","source":"def predict(hparams, feeder, out_dir, vocab, label_ed, embedding_W):\n    with tf.Graph().as_default():\n        sess = tf.Session()\n\n        with sess.as_default():\n            # Define Training procedure\n            rnn = build_model(hparams, vocab, label_ed, embedding_W)\n\n            sess.run(tf.global_variables_initializer())\n            saver = tf.train.Saver()\n\n            writer = tf.summary.FileWriter(out_dir, sess.graph)\n\n            checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n\n            ckpt = tf.train.get_checkpoint_state(checkpoint_dir)\n            # if that checkpoint exists, restore from checkpoint\n            if ckpt and ckpt.model_checkpoint_path:\n                saver.restore(sess, ckpt.model_checkpoint_path)\n\n            for X_batch, d_batch in feeder:\n                feed_dict = {\n                    rnn.input_x: X_batch,\n                    rnn.input_d: d_batch,\n                    rnn.embedding_keep_prob: 1.0,\n                    rnn.fc_keep_prob: 1.0,\n                }\n\n                logits = sess.run(rnn.logits, feed_dict)\n                feeder.feed_result(logits)\n    \n    return feeder.result\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"336d13221a3039fcd8ed8e72ee155c565570f77e"},"cell_type":"code","source":"def train_predict_cv(hparams, feeder_train, texts_dev, label_dev, dence_dev, feeder_pred):\n    assert hparams.embed_type in ('fast-text', 'glove')\n\n    if hparams.embed_type == 'fast-text':\n        print('Train Use Fast Text embedding.')\n        embedding_W = embedding_ft.W() \n    else:\n        print('Train Use Glove embedding.')\n        embedding_W =  embedding_g.W()\n    \n    if not hparams.out_dir:\n        hparams.out_dir = train(hparams, feeder_train, texts_dev, dence_dev, label_dev, embedding_W, \n                                evaluate_every=hparams.evaluate_every, patience=3, \n                                out_dir=None)\n\n    return np.array(predict(hparams, feeder_pred, hparams.out_dir, vocab, label_ed, embedding_W))\n\n\ndef kfold_by_sentence(sid, n_splits):\n    for i in range(n_splits):\n        dev_index = sid % n_splits == i\n        train_index = (~dev_index)\n        yield train_index, dev_index\n\n\ndef blend_cv(hparams, n_splits, path_predict):\n    df_train = load_train_df()\n    df_test = load_test_df()\n    \n    df_train['PhraseLower'] = df_train[\"Phrase\"].str.lower()\n    df_test['PhraseLower'] = df_test[\"Phrase\"].str.lower()\n    phrase_inter = set(df_train[\"PhraseLower\"]).intersection(set(df_test[\"PhraseLower\"]))\n    print(\"Ratio of test set examples which occur in the train set: {0:.2f}\".format(len(phrase_inter)/df_test.shape[0]))\n    df_test = pd.merge(df_test, df_train[[\"PhraseLower\", \"Sentiment\"]], on=\"PhraseLower\", how=\"left\")\n\n    texts_test = np.array(list(vocab.transform(list(df_test.Phrase))))\n    dence_test = transform_dence(df_test)\n    \n    texts, sid, label, dence = prepare_train(df_train)\n    texts, label = np.array(list(vocab.transform(texts))), label_ed.transform(label)\n\n    results = []\n    for train_index, dev_index in kfold_by_sentence(sid, n_splits):\n        texts_train, label_train, dence_train = texts[train_index], label[train_index], dence[train_index]\n        texts_dev, label_dev, dence_dev = texts[dev_index], label[dev_index], dence[dev_index]\n        \n        hparams_cv = HParams(**hparams.values())\n        feeder_train = TrainInputFeeder2X(texts_train, dence_train, label_train, \n                                            num_epochs=hparams_cv.num_epochs, batch_size=hparams_cv.batch_size, \n                                            shuffle=hparams_cv.shuffle)\n        feeder_pred = PredictInputFeeder2X(texts_test, dence_test, batch_size=2048)\n        \n        results.append(train_predict_cv(hparams_cv, feeder_train, texts_dev, label_dev, dence_dev, feeder_pred))\n    \n    result = sum(results)\n    labels_pred = label_ed.reverse(result)\n    \n    df_test['pred'] = labels_pred\n    df_test.loc[df_test[\"Sentiment\"].isnull(), \"Sentiment\"] = df_test.loc[df_test[\"Sentiment\"].isnull(), \"pred\"]\n    \n    df_test['Sentiment'] = df_test.Sentiment.astype(int)\n    df = df_test.loc[:, ('PhraseId', 'Sentiment')]\n\n    df.to_csv(path_predict, ',', index=False)\n    \n    if os.path.exists(os.path.join(\".\", \"runs\")):\n        shutil.rmtree(os.path.join(\".\", \"runs\"))\n    \n    print('Submissions: %s' % path_predict)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"160402557ddd2e46d999f5062d457311cce1031a"},"cell_type":"code","source":"hparams = HParams(\n        embed_type='fast-text', embedding_keep_prob=0.5,\n        num_units=128, attention_size=128,\n        filter_sizes=(2, 3), num_filters=128,\n        num_outputs_fc1=256, num_outputs_fc2=128, num_outputs_fc3=50, fc_keep_prob=0.8,\n        learning_rate=0.001, batch_size=1024, num_epochs=15, shuffle=True, evaluate_every=138, verbose=1,\n        out_prefix='rnn_cnn', out_dir=None\n    )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2b66023183e896443f5610f29924834fdcc37b3e"},"cell_type":"code","source":"# df_train = load_train_df()\n# df_test = load_test_df()\n\n# df_train['PhraseLower'] = df_train[\"Phrase\"].str.lower()\n# df_test['PhraseLower'] = df_test[\"Phrase\"].str.lower()\n# phrase_inter = set(df_train[\"PhraseLower\"]).intersection(set(df_test[\"PhraseLower\"]))\n# print(\"Ratio of test set examples which occur in the train set: {0:.2f}\".format(len(phrase_inter)/df_test.shape[0]))\n# df_test = pd.merge(df_test, df_train[[\"PhraseLower\", \"Sentiment\"]], on=\"PhraseLower\", how=\"left\")\n\n# texts_test = np.array(list(vocab.transform(list(df_test.Phrase))))\n# dence_test = transform_dence(df_test)\n\n# texts, sid, label, dence = prepare_train(df_train)\n# texts, label = np.array(list(vocab.transform(texts))), label_ed.transform(label)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6f5473f2c374a92ce2a0faee75a4309362edfd70","scrolled":true},"cell_type":"code","source":"# hparams.verbose = 2\n# for train_index, dev_index in kfold_by_sentence(sid, 10):\n#     texts_train, label_train, dence_train = texts[train_index], label[train_index], dence[train_index]\n#     texts_dev, label_dev, dence_dev = texts[dev_index], label[dev_index], dence[dev_index]\n        \n#     hparams_cv = HParams(**hparams.values())\n\n#     feeder_train = TrainInputFeeder2X(texts_train, dence_train, label_train, \n#                                             num_epochs=hparams_cv.num_epochs, batch_size=hparams_cv.batch_size, \n#                                             shuffle=hparams_cv.shuffle)\n#     feeder_pred = PredictInputFeeder2X(texts_test, dence_test, batch_size=2048)\n        \n#     train_predict_cv(hparams_cv, feeder_train, texts_dev, label_dev, dence_dev, feeder_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"57ab07a4ef5a0a00d64ca3b72af857a977abaa72"},"cell_type":"code","source":"# shutil.rmtree(os.path.join(\".\", \"runs\"))\nblend_cv(hparams, 10, 'submission_20181004_1.csv')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}