{"cells":[{"metadata":{"_cell_guid":"8e9abf83-9a78-4210-a36f-980f6de76d21","_uuid":"f933629c7434ad2449c157fab6f040fdb3f8282b"},"cell_type":"markdown","source":"# Credit Fraud Detection: EDA & Baseline Prediction\nThe objective of this analysis is to explore the Credit Fraud Detection dataset, collected from European Cardholders in Spetember 2013 and develope a baseline for detection of fraud using Machine Learning Algorithms. The Data has been anonimatized and preprocessed, such that each record has 28 PCA-transformed fields, plus Time and Amount, together with a Class (0 if regular transaction, 1 for fraud). \nThe Following is the roadmap of the study\n* [Data Assessment](#section_DataAssessment)\n* [Data Exploration](#section_DataExploration)\n* [Predictive Models](#section_PredictiveModels)\n * [XGBoost](#section_XGBoost)\n * [XGBoost - Parameter Optimization](#section_XGBoost_Parameter_Optimization)\n* [Results](#section_Results)\n* [Conclusion and Next Steps](#section_Conclusion)"},{"metadata":{"_cell_guid":"237095ee-92d5-484b-8b56-7057d34b0a06","_uuid":"40b48fda694733e9640005ec1825f881d30db88d"},"cell_type":"markdown","source":"<a id=\"section_DataAssesment\"></a>\n## **Data Assessment** \nLet's have a quick look at what the data can tell us, using basic-but-powerfull analytics. We need to load all the relevant libraries."},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","collapsed":true,"trusted":true},"cell_type":"code","source":"# importing relevant libraries\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport pylab as plt # plots\nfrom matplotlib.colors import LogNorm # logarithmic norm\nplt.rcParams['figure.figsize'] = [10, 5] # set plot size\nfrom sklearn.preprocessing import StandardScaler # needed to normalize data\nfrom sklearn.model_selection import train_test_split # needed to split data into train and test\nfrom sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc # relevant performance metrics\nfrom sklearn.model_selection import GridSearchCV # needed to optimize hyperparameters \nfrom sklearn.cluster import Birch # needed for clustering \nimport xgboost as xgb # gradient boosting library","execution_count":1,"outputs":[]},{"metadata":{"_cell_guid":"64f2a02c-bda7-4512-991c-90923d7487d9","_uuid":"c7a4c43130310fa2f9084449acaf635e9904663e","collapsed":true},"cell_type":"markdown","source":"The data was given as csv, so we can read it direcly with pandas"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","scrolled":false,"trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/creditcard.csv') # read the data\nprint('The dataset consists of %d rows and %d columns' % data.shape)\ndata.head() # A first look at the data","execution_count":2,"outputs":[]},{"metadata":{"_cell_guid":"3964475b-d68c-4fe4-acd7-dcb8776f9e8c","_uuid":"51e4bc9de7e4ce967db8693a89d57691c8295a17"},"cell_type":"markdown","source":"The mean value of features V1-V28 is 0 with standard deviation decreasing from 1.959 to 0.33"},{"metadata":{"_cell_guid":"97d157f7-c913-4288-b1f6-f9cad902dea2","_uuid":"a7132a97a881474782aff5dea91166bb2e96574e","trusted":true},"cell_type":"code","source":"lFEATURE = []\nlMEAN = []\nfor ii in range(1,10):\n    plt.scatter([ii for jj in range(len(data['V' + str(ii)]))],data['V' + str(ii)],alpha=0.5,color='blue')\n    lFEATURE.append(ii)\n    lMEAN.append(data['V' + str(ii)].mean())\nplt.scatter(lFEATURE,lMEAN,marker = '+', color = 'red', s = 100)\nplt.xlabel('Feature')\nplt.show()","execution_count":3,"outputs":[]},{"metadata":{"_cell_guid":"0717ec00-24ef-47f0-a4bb-dec1383388e7","_uuid":"9b1d8abd050d6434309290d8dd1cae1c29d32f9f"},"cell_type":"markdown","source":"For a closer look at the distributions we can plot them one by one."},{"metadata":{"_cell_guid":"ca340c16-a39c-4d7f-8bda-959a62c28244","_uuid":"173494afb3c3e094c404b7a026215bfb8107cab6","trusted":true},"cell_type":"code","source":"data[data.columns.values[1:5]].hist()\n#data[data.columns.values[5:9]].hist()\nplt.show()","execution_count":4,"outputs":[]},{"metadata":{"_cell_guid":"d6527142-c7ab-4599-9fbe-9ae73861a8b0","_uuid":"61031627928502a00133b3658fe84e4a0e3f9ab1","trusted":true},"cell_type":"code","source":"data.isnull().sum()","execution_count":5,"outputs":[]},{"metadata":{"_cell_guid":"2f19e255-825b-4869-af06-c3ec20e71683","_uuid":"7df04b91a4ecbc9dca3f2f9e35f4c3b7e46b51fe"},"cell_type":"markdown","source":"There are no null values on any of the fields. So far we know that, all the preprocessed data has been normalized so the mean value is 0 and the standard deviation is less than 2. In the next section we will examine the data in more detail"},{"metadata":{"_cell_guid":"766e9506-6366-4c07-ba8b-f5172eb92e3f","_uuid":"b33747bf5c3a4fa5285ff2099f535d699ff7f6dd"},"cell_type":"markdown","source":"<a id=\"section_DataExploration\"></a>\n## **Data Exploration** \nDue to privacy, the 28 data features have been preprocessed and anonimized so we only have access to the post principal component analysis (PCA) data. However we still have time and amount as descriptive fields. "},{"metadata":{"_cell_guid":"18283805-6fb6-428d-a9ca-da4687b481e1","_uuid":"a1c1a16edc670fbe386c06b695857d0bacd03c86","trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1,2)\nax[0].hist(data['Time']/(3600),bins=48)\nax[0].set_xlabel('Time[hours]')\nax[1].hist(data['Amount'],bins=100)\nax[1].set_xlabel('Amount[CUR]')\nplt.show()","execution_count":6,"outputs":[]},{"metadata":{"_cell_guid":"e91856a4-d00d-4ec7-909c-7b1a9c0d0016","_uuid":"877590f58947d327f7b301b839bac59f1322fbee"},"cell_type":"markdown","source":"The Time distribution in hours presents two main peaks, roughly arounf the hour 15 and 40 (about 24h difference) which suggests that there is a daily modulation of the traffic, assuming that the data was captured from a single time-zone (**Assumption A**)\nOn the other hand, most of the Amount values are grouped near small values. However, a transformation can provide more insights about the real distribution."},{"metadata":{"_cell_guid":"eaf80bb2-f050-4c86-a622-ae7f54433dc7","_uuid":"fb1ac4ca4f59ddfe6582ef262a10864417a87714","trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1,2)\nax[0].hist(data['Time']/(3600),bins=48)\nax[0].set_xlabel('Time[hours]')\nax[1].hist(data['Amount'],bins=np.logspace(np.log10(0.001),np.log10(25000), 50))\nax[1].set_xlabel('Amount[CUR]')\nax[1].set_xscale(\"log\")\nplt.show()","execution_count":7,"outputs":[]},{"metadata":{"_cell_guid":"7de674ac-0e9b-4607-bc20-3ae0e2128623","_uuid":"f95599feb23da0d88f76347833d4247b9cbb0d8f"},"cell_type":"markdown","source":"Let's explore the Amount field. From the histogram, it is clear that the best transformation to perform on the Amount is a logarithm. However, this will compromise the negative and zero values. Fortunately, there are no negative values and the number of records with zero amount is pretty small. So we can select the positive values of Amount and work on a specific model for the zero case, being the smallest non-zero value 0.01, consistent with one cent of the currency (**Assumption B**)."},{"metadata":{"_cell_guid":"5a6a5de4-b5e4-4ae3-9680-61f9fd204bf5","_uuid":"a38242d877e943ddfee53069614d539423931368","trusted":true},"cell_type":"code","source":"print('There are %d (%f) records with zero amount, and the minimum non-zero value is %f' \n      % (len(data[data['Amount']==0]),1.*len(data[data['Amount']==0])/len(data),data[data['Amount']>0]['Amount'].min()))","execution_count":8,"outputs":[]},{"metadata":{"_cell_guid":"83825a0a-e4a0-4e9c-8aed-0504035f9779","_uuid":"b6b89e318204329e5e9541e3d9c66f8ea2189820"},"cell_type":"markdown","source":"The above discution motivates the first cut in our analysis: Consider records with a minimum of 0.01 CUR."},{"metadata":{"_cell_guid":"1a492974-8cdb-42a0-ba1b-39ff31b1d7f5","_uuid":"4eaa2f8d58ec4b90b11edc50ab0ca0c306e8c287","collapsed":true,"trusted":true},"cell_type":"code","source":"data_clean = data[data['Amount']>0].copy()","execution_count":9,"outputs":[]},{"metadata":{"_cell_guid":"5b1d1679-0166-442a-a69c-8b26070d59bc","_uuid":"56cdbc8b020252292007661aca322ff4a9af8470"},"cell_type":"markdown","source":"### Imbalanced dataset\nThe data set is clearly inbalanced in the sense that the number of negative examples  (class 0) is overwhelmly larger than the number of positive examples. This suggests that we need a way even the contributions from positive and negative examples in our predictors."},{"metadata":{"_cell_guid":"197a1088-926a-4664-b45d-5182be35ed63","_uuid":"9e02ed201aff0009de6733d088a88387364b4373","trusted":true},"cell_type":"code","source":"data_clean['Class'].value_counts()","execution_count":10,"outputs":[]},{"metadata":{"_cell_guid":"a72584e0-408a-4335-91fb-b6e0ff93bdf7","_uuid":"3f845b88235d2d798292236271768baff42c8f4e"},"cell_type":"markdown","source":"Let's consider the behavior of the rate of positive (number of positive over total records) as a function of Time."},{"metadata":{"_cell_guid":"ef4a1ae9-acf9-49ef-a188-eaa2cc68eb9c","_uuid":"bc391486a485aaa778aa234db16471e02d701510","trusted":true},"cell_type":"code","source":"fig, ax1 = plt.subplots()\nhist_time_count, hist_time_bin, hist_time_img = plt.hist(data_clean['Time']/3600, bins=48,label='All Traffic')\nhist_time_count_1, hist_time_bin_1, hist_time_img_1 = plt.hist(data_clean[data_clean['Class']==1]['Time']/3600, bins=48)\nax2 = ax1.twinx()\nplt.plot(hist_time_count_1/hist_time_count, marker='+',linestyle='None',color='red',label='Fraud Ratio')\nax1.set_xlabel('Time[hours]')\nax2.tick_params('y', colors='r')\nplt.legend()\nplt.show()\nmean_pos_rate = np.mean(hist_time_count_1/hist_time_count)\nmax_pos_rate = np.max(hist_time_count_1/hist_time_count)","execution_count":11,"outputs":[]},{"metadata":{"_cell_guid":"0f47236a-b08e-48dc-979f-f93b232d4101","_uuid":"94ec216cb14f4b215fe05461153a705c5b209f0f"},"cell_type":"markdown","source":"We can see that there is a higher positive rate during the times of lower traffic. "},{"metadata":{"_cell_guid":"72473102-ce93-498c-a8ea-9ced76530195","_uuid":"570536db828d4327c015b027fef4b9815dac033e","trusted":true},"cell_type":"code","source":"print('Mean value of pos rate is %f and the max value if pos rate is %f making it %f times higher' % (mean_pos_rate,max_pos_rate,max_pos_rate/mean_pos_rate))","execution_count":12,"outputs":[]},{"metadata":{"_cell_guid":"add3e697-2f76-4be0-b149-8e04e13ca38b","_uuid":"6faa64a609968520bae524d6e3eb91f1bff03a19"},"cell_type":"markdown","source":"Let's consider now the Amount and the rate of positive as a function of the Amount"},{"metadata":{"_cell_guid":"075e8271-aa9c-40c2-b397-556a38c45700","_uuid":"02684bf98add3054f3415b344ecbb8d6c75c4be0","trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1,2)\nax[0].hist(data_clean['Amount'],bins=np.logspace(np.log10(0.001),np.log10(25000), 50))\nax[0].set_xlabel('Amount[CUR]')\nax[0].set_xscale(\"log\")\nax[1].hist(data_clean[data_clean['Class']==1]['Amount'],bins=np.logspace(np.log10(0.001),np.log10(25000), 50))\nax[1].set_xlabel('Amount[CUR]')\nax[1].set_xscale(\"log\")\nplt.show()","execution_count":13,"outputs":[]},{"metadata":{"_cell_guid":"ab5f8db3-a37b-4fc4-b0cd-526c3d9d2870","_uuid":"53e742abe541b04aff57af977ad0833c8490b698","trusted":true},"cell_type":"code","source":"fig, ax1 = plt.subplots()\nhist_amount_count, hist_amount_bin, hist_amount_img = plt.hist(data_clean['Amount'],bins=np.logspace(np.log10(0.001),np.log10(25000), 50),label='All Traffic')\nhist_amount_count_1, hist_amount_bin_1, hist_amount_img_1 = plt.hist(data_clean[data_clean['Class']==1]['Amount'], bins=np.logspace(np.log10(0.001),np.log10(25000), 50))\nax1.set_xscale(\"log\")\nax2 = ax1.twinx()\nplt.plot(hist_amount_bin_1[:-1],1.*hist_amount_count_1/hist_amount_count, marker='+',linestyle='None',color='red',label='Fraud Ratio')\nax1.set_xscale(\"log\")\nax1.set_xlabel('Amount[CUR]')\nax2.tick_params('y', colors='r')\nax2.set_xscale(\"log\")\nplt.legend()\nplt.show()","execution_count":14,"outputs":[]},{"metadata":{"_cell_guid":"3f1be04e-4a1a-4d56-9b9d-429ac8016e69","_uuid":"46bceccbbd0a085cd17fd30dd2cf9c8412e0195c","collapsed":true},"cell_type":"markdown","source":"Actionable insights\n* Special attention to low traffic hours needed to identify fraud records. If possible add resources to monitor during down time\n* The highest fraude rates are associated to:\n * minimum contributions (Amount = 0.01), possibly tests\n * small contributions (Amount between 0.1 and 1), possibly tests\n * medium contributions (Amount between 1 and 100)\n * large contributions (Amount between 100 and 2000)\n* No significant fraud observed for Amounts larger than 2000\n \n"},{"metadata":{"_cell_guid":"761da381-9b31-4c11-8c9a-069669cac7e2","_uuid":"9fdb3395f192cf4c6a8bd4142d8119e20939210d","collapsed":true},"cell_type":"markdown","source":"<a id=\"section_DataExploration\"></a>\n## **Predictive Models** \nClassification with unbalanced data set needs a way to mitigate the disbalance and we will try an undersampling. First we will define a few new features like the logarithm of the Amount, the normalized logarithm of the Amount and the normalized Time."},{"metadata":{"_cell_guid":"4128afd5-8c4c-4b25-a9ab-7de1fb255aa5","_uuid":"23e8af66674dd2f9371c39495bba5ecca9dfc5c5","collapsed":true,"trusted":true},"cell_type":"code","source":"data_clean['logAmount'] = np.log(data_clean['Amount'])/np.log(10)","execution_count":15,"outputs":[]},{"metadata":{"_cell_guid":"d6ae80e5-9905-4742-94c5-f5793ce3adc9","_uuid":"123c907a6f91dd9cc0746d407e54d3060cdf3e05","collapsed":true,"trusted":true},"cell_type":"code","source":"data_clean['normAmount'] = StandardScaler().fit_transform(data_clean['logAmount'].values.reshape(-1, 1))\ndata_clean['normTime'] = StandardScaler().fit_transform(data_clean['Time'].values.reshape(-1, 1))","execution_count":16,"outputs":[]},{"metadata":{"_cell_guid":"a0d5e46e-ca00-4a9b-96a2-142f3dcb2000","_uuid":"620e840d9332a272e672b89145d75eb3ee24586c"},"cell_type":"markdown","source":"We will now delete the fields that are not normalized"},{"metadata":{"_cell_guid":"c8d49776-a1ac-4f41-aced-5e2ce55c4a61","_uuid":"b2a742e4f5a9d0b33ea92129b8770a696c91b7ac","collapsed":true,"trusted":true},"cell_type":"code","source":"data_sel = data_clean.drop(['Time','Amount','logAmount'],axis=1)","execution_count":17,"outputs":[]},{"metadata":{"_cell_guid":"8af88740-6ea4-46be-8aab-c66ca2365e69","_uuid":"b00428b5c870830573a7be7db7961313c68d3b84"},"cell_type":"markdown","source":"We will set the ratio of positive to negative records as 1 but it can be changed for further explorations"},{"metadata":{"_cell_guid":"2d0d82d6-d16b-4185-b01d-9b12fa9d5557","_uuid":"ae3a7326111478188255fb154a34b4572f5ad790","collapsed":true,"trusted":true},"cell_type":"code","source":"ratio_pos_neg = 1","execution_count":18,"outputs":[]},{"metadata":{"_cell_guid":"61eb748e-6064-4287-ae25-bab2d5e2c5ba","_uuid":"6736bf2bd91d0370cb5a7384444d21bc98e1c1c8"},"cell_type":"markdown","source":"We now separate the data into 0-class and 1-class. Then we will randomly sample the 0-class set and select a set with the same size as the 1-class set. The new balanced 1-class and 0-class will make up the data_model"},{"metadata":{"_cell_guid":"ca9ec185-7603-4f8a-9ca0-f199b50747f3","_uuid":"3f143f06994a0c820535bf59120933e5a63d2362","collapsed":true,"trusted":true},"cell_type":"code","source":"data_sel_0 = data_sel[data_sel['Class']==0]\ndata_sel_1 = data_sel[data_sel['Class']==1]","execution_count":19,"outputs":[]},{"metadata":{"_cell_guid":"adf24f66-2b54-493b-af27-20d7f0164851","_uuid":"3971d066fdbbaff396c8f3b99d8dba05c56e3114","collapsed":true,"trusted":true},"cell_type":"code","source":"data_balanced = data_sel_0.sample(int(len(data_sel_1)*ratio_pos_neg))","execution_count":20,"outputs":[]},{"metadata":{"_cell_guid":"4347baed-c170-4041-b2d9-113a91b78d10","_uuid":"39c03ac530ccf24e26adb23cb11d0a938eb93237","collapsed":true,"trusted":true},"cell_type":"code","source":"data_model = pd.concat([data_balanced,data_sel_1]).sample(frac=1)","execution_count":21,"outputs":[]},{"metadata":{"_cell_guid":"3b3ea43b-6d44-4df0-99e5-120fa6ae8ab5","_uuid":"3e38ae5d7cdd7b98ace53949cf7bd6c16710db8e"},"cell_type":"markdown","source":"Now we select the dependent features X and the feature to predict y, and split the set into training and testing (2/3 and 1/3 respectively)"},{"metadata":{"_cell_guid":"d356b727-f4da-4067-9611-fcd9dfae820b","_uuid":"7d374cee050490219bed04758f8c560ce1f02d01","collapsed":true,"trusted":true},"cell_type":"code","source":"X = data_model.loc[:, data_model.columns != 'Class']\ny = data_model.loc[:, data_model.columns == 'Class']","execution_count":22,"outputs":[]},{"metadata":{"_cell_guid":"65e1d527-1ba2-4c89-a9b1-b1ebd96b17b5","_uuid":"defe274e1649c8bfd4765703bfc606ef767d2527","collapsed":true,"trusted":true},"cell_type":"code","source":"y = np.ravel(y)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)","execution_count":23,"outputs":[]},{"metadata":{"_cell_guid":"3b74833e-e11e-4af2-96f2-d9b8804e8622","_uuid":"9175cec62f7caffbeb2affc31645bfcadcc6e413"},"cell_type":"markdown","source":"<a id=\"section_XGBoost\"></a>\n### XGBoost\n[XGBoost](http://xgboost.readthedocs.io/en/latest/) is an implementation of [Gradient Boosting](https://en.wikipedia.org/wiki/Gradient_boosting) that is designed for speed processing of boosted trees algorithms.  We use first a baseline implementation with some initial parameters such as n_estimators =200, max_depth=3 and learning_rate=0.01. We fit the training data and then use the testing data to evaluate the performance of the algorithm."},{"metadata":{"_cell_guid":"5298b3c3-395a-43ca-bc69-238ecd76e2aa","_kg_hide-output":true,"_uuid":"7d1451a49b519b4d3ce991a89db846946dafeb68","trusted":true},"cell_type":"code","source":"model = xgb.XGBClassifier(max_depth=3, n_estimators=200, learning_rate=0.01)\nmodel.fit(X_train,y_train)\ny_pred_xgb = model.predict(X_test)\ny_score_xgb = model.predict_proba(X_test)[:,1]\nfpr_xgb, tpr_xgb, _ = roc_curve(y_test, y_score_xgb)","execution_count":24,"outputs":[]},{"metadata":{"_cell_guid":"cd8914cf-9f9e-4d80-ae57-e6516a7ee17f","_uuid":"8592a84fb95717fe608684f3c0295b5a6724d9d4","trusted":true},"cell_type":"code","source":"print('XGBoost ROC AUC', auc(fpr_xgb, tpr_xgb))\nprint(classification_report(y_test, y_pred_xgb, target_names=['NoFraud','Fraud']))","execution_count":31,"outputs":[]},{"metadata":{"_cell_guid":"e867fbb1-5a17-40b7-b1d6-f876ff1b9a29","_uuid":"97374624be565e081e37184d45eaf13ed34a31f4"},"cell_type":"markdown","source":"<a id=\"section_XGBoost_ParameterOptimization\"></a>\n### XGBoost - Parameter optimization\nWe can use a grid search optimization to find the set of parameter with the optimal performance"},{"metadata":{"_cell_guid":"5d2a8686-6cfc-4fa2-be9a-73aab2bbe406","_kg_hide-output":true,"_uuid":"9da269a2dc7c92ca3d307ed95caf0a3aab4b2713","trusted":true},"cell_type":"code","source":"cv_params = {'max_depth': [3,5,7], 'min_child_weight': [1,3,5]}\nind_params = {'learning_rate': 0.1, 'n_estimators': 100, 'seed':0, 'subsample': 0.8, 'colsample_bytree': 0.8, 'objective': 'binary:logistic'}\noptimized_model = GridSearchCV(xgb.XGBClassifier(**ind_params), \n                            cv_params, \n                             scoring = 'accuracy', cv = 5, n_jobs = -1) \noptimized_model.fit(X_train,y_train)","execution_count":32,"outputs":[]},{"metadata":{"_cell_guid":"90a5fe5b-cc52-4775-8c0c-a34a20386571","_uuid":"97141f29f328cac3d6e217c1989d36e7cff831d2","collapsed":true,"trusted":true},"cell_type":"code","source":"best_optimized_model = optimized_model.best_estimator_","execution_count":33,"outputs":[]},{"metadata":{"_cell_guid":"a55d0ceb-7e9b-4457-bee2-40763a431dfd","_uuid":"0f08cb1da425a33efc31371cffaf06bef189e502","trusted":true},"cell_type":"code","source":"y_pred_xgb = best_optimized_model.predict(X_test)\ny_score_xgb = optimized_model.predict_proba(X_test)[:,1]\nfpr_xgb, tpr_xgb, _ = roc_curve(y_test, y_score_xgb)","execution_count":34,"outputs":[]},{"metadata":{"_cell_guid":"b194aafb-9929-47db-b1b1-892705eab260","_uuid":"da3814b14af190cc50f4c18887cbbd16452f0c2f","trusted":true},"cell_type":"code","source":"print('XGBoost ROC AUC', auc(fpr_xgb, tpr_xgb))\nprint(classification_report(y_test, y_pred_xgb, target_names=['NoFraud','Fraud']))","execution_count":35,"outputs":[]},{"metadata":{"_cell_guid":"fdc1eda5-5e6f-42ed-ad1c-03a08e67a94f","_uuid":"836bfacb076d88363bfd2c20e670df823447f63d","trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots()\nplt.plot([0, 1], [0, 1], 'k--')\nplt.plot(fpr_xgb, tpr_xgb, label='XGB')\nax.set_aspect('equal')\nplt.legend()\nplt.show()","execution_count":36,"outputs":[]},{"metadata":{"_cell_guid":"a4c0e6e0-7b86-44f1-9411-a6fda2d90b86","_uuid":"b351dd647446c26afa653a0e609746d4476eb77c"},"cell_type":"markdown","source":"The performance improves as we scanned over the possible parameter space, with a very high precision and accuracy. From the ROC curve we can see that when the False Positive Rate (FPR) is close to zero ."},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"da0507af17c74fd3c7c574474b9e720a0f057ff9"},"cell_type":"code","source":"features = X.columns.values.tolist()","execution_count":37,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"30ce8c6c555da6d38977dd10866eba5ddf1dd654"},"cell_type":"code","source":"def getAUC_XGB(i_index):\n    slFIELDS = features[:i_index] + features[i_index+1:]\n    X = data_model[slFIELDS]\n    y = data_model['Class']\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n    xgbc_best = best_optimized_model\n    xgbc_best.fit(X_train, y_train)\n    y_pred_xgb = xgbc_best.predict(X_test)\n    y_score_xgb = xgbc_best.predict_proba(X_test)[:,1]\n    fpr_xgb, tpr_xgb, _ = roc_curve(y_test, y_score_xgb)\n    return auc(fpr_xgb, tpr_xgb)","execution_count":38,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5ad6616fd452cdc1da26ab0ee8d50c8d1ae6ffcd"},"cell_type":"code","source":"getAUC_XGB(i_index=0)","execution_count":39,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"8d2f0cae850fc6772642820ff90f072645d43b68"},"cell_type":"code","source":"lAUC = []\nlIND = []\nfor ii in range(len(features)):\n    lIND.append(features[ii])\n    lAUC.append(getAUC_XGB(ii))","execution_count":40,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"688b07f12633a5ab2ab59f222c1521b889e7aed7"},"cell_type":"code","source":"plt.scatter(range(len(lIND)),lAUC)\nplt.xticks([ii for ii in range(len(features))], features, rotation='vertical')\nplt.show()","execution_count":41,"outputs":[]},{"metadata":{"_uuid":"13389460da10e0d0083da4982c788c75174d3551"},"cell_type":"markdown","source":"We can see how excluding the field V14 brings the AUC-ROC down, suggesting this feature carries a higher weight in importance in the interpretation of the model."},{"metadata":{"_cell_guid":"833c5c23-1e8d-4d8b-a1d9-512fec19c7fa","_uuid":"bf26f315246693a02fffdbbc04a4f9ce1fd6d71f"},"cell_type":"markdown","source":"<a id=\"section_XGBoost_ParameterOptimization\"></a>\n### XGBoost - Parameter optimization & Clustering of 0-class records\nIn this section we will explore the clustering of the 0-class events into representative records for each segment. The idea is that the certain 0-class events are indeed similar and can be grouped. Instead of randomly under-sampling records, we can use the mean value of the segments. For this particular apporach, we will explore the [BIRCH clustering approach](https://en.wikipedia.org/wiki/BIRCH), that works very well with large data sets defining the number of cluster to be equal to the number of 1-class records. This needs to be revisited and will be part of the next steps."},{"metadata":{"_cell_guid":"fef8c462-e75c-40fd-bd9f-6a4b04a5b7d9","_uuid":"9b7159424243a53dc3ef682de7c580e5b216941d"},"cell_type":"markdown","source":"Due to computation constrains from the kernel we wont cluster the entire 0-class set, but only a fraction, randomly selected. "},{"metadata":{"_cell_guid":"51c69ca3-e2ce-4ef7-9df8-9f7ec1e4ae6a","_uuid":"98165310f6ef5eec08b6b34dc6cdd232dc8613b6","collapsed":true,"trusted":true},"cell_type":"code","source":"data_Birch = data_sel_0.loc[:, data_model.columns != 'Class'].sample(frac=0.1)","execution_count":42,"outputs":[]},{"metadata":{"_cell_guid":"ae1971b2-ad95-4cba-8430-91b1ae8f8990","_uuid":"ac821cd036e18d01f0f878e35052896d131befd7"},"cell_type":"markdown","source":"Next we define the number of clusters as the multiple of the numbr of 1-class events. In this case it will be equal."},{"metadata":{"_cell_guid":"e4bbd278-9d05-4aba-91bf-5ac46082cb22","_uuid":"b44e1fde463d19118d19334417fe643ac5419099","collapsed":true,"trusted":true},"cell_type":"code","source":"ratio_pos_neg = 1\nbrc = Birch(n_clusters=len(data_sel_1)*ratio_pos_neg)","execution_count":43,"outputs":[]},{"metadata":{"_cell_guid":"5cffec30-7861-4ddc-a3a0-72ed7cc9e60e","_uuid":"d38138f78ab8a49241a1582e8210b7ebcdeb7342"},"cell_type":"markdown","source":"We now fit the selected 0-class set with the BIRCH algorithm and add a new variable Label to the data set that will be used to aggregate the events with the same label and produce the mean value of each variable as the representative event."},{"metadata":{"_cell_guid":"a8971333-4979-4eb0-8392-0f46c3404cca","_uuid":"7d650b3a7e2a2bc8225c251608541e1cce02a51a","trusted":true},"cell_type":"code","source":"brc.fit(data_Birch)","execution_count":44,"outputs":[]},{"metadata":{"_cell_guid":"d0e0da0a-82ae-4166-bbb6-061382c534cc","_uuid":"6bfbd13bb0d1e2d08cd30bacd4de00765ed2d142","collapsed":true,"trusted":true},"cell_type":"code","source":"data_Birch['Label'] = brc.labels_","execution_count":45,"outputs":[]},{"metadata":{"_cell_guid":"898f4a39-cc41-4c3c-a592-b5eb0c8c31f9","_uuid":"3cb5853e024a7ca049c097dd798b9ff9980d5f1d","collapsed":true,"trusted":true},"cell_type":"code","source":"data_Birch_mean = data_Birch.groupby('Label').mean().reset_index().drop('Label',axis=1)\ndata_Birch_mean['Class'] = 0","execution_count":46,"outputs":[]},{"metadata":{"_cell_guid":"5c7f0cee-fb32-4e81-a9ff-4102b49b2cf2","_uuid":"abf3f86efdc021c4c1519d29bad27b2f52f30c2a","collapsed":true,"trusted":true},"cell_type":"code","source":"data_model_Birch = pd.concat([data_Birch_mean,data_sel_1]).sample(frac=1)","execution_count":47,"outputs":[]},{"metadata":{"_cell_guid":"bf17c852-6186-43ff-bb06-3bc19949a385","_uuid":"3075410a53b173b805234b86273d5327b8427b96"},"cell_type":"markdown","source":"Following a similar approach as above we will split the set into training and test, train the XGBoost algorithm and test the performance on the test set computing the relevant metrics."},{"metadata":{"_cell_guid":"63f807ba-1c25-4490-aade-84a4860541a3","_uuid":"77bb67ae3e4c1a55d8408527e0ec0e52f8757f4e","collapsed":true,"trusted":true},"cell_type":"code","source":"X = data_model_Birch.loc[:, data_model_Birch.columns != 'Class']\ny = data_model_Birch.loc[:, data_model_Birch.columns == 'Class']\ny = np.ravel(y)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)","execution_count":48,"outputs":[]},{"metadata":{"_cell_guid":"65adb0e3-ac6a-42cd-989e-ac3e855f31ae","_uuid":"124d3f279807e05ffd4a6c73fba4d2e26caf4c5b","trusted":true},"cell_type":"code","source":"model = xgb.XGBClassifier(max_depth=3, n_estimators=200, learning_rate=0.01)\nmodel.fit(X_train,y_train)\ny_pred_xgb = model.predict(X_test)\ny_score_xgb = model.predict_proba(X_test)[:,1]\nfpr_xgb, tpr_xgb, _ = roc_curve(y_test, y_score_xgb)","execution_count":49,"outputs":[]},{"metadata":{"_cell_guid":"7ba9d3a9-1a30-4fc6-9192-c7eac08f907d","_uuid":"88da6e24b31830ab039cd3041d1e083b83c1e192","trusted":true},"cell_type":"code","source":"print('XGBoost ROC AUC', auc(fpr_xgb, tpr_xgb))\nprint(classification_report(y_test, y_pred_xgb, target_names=['NoFraud','Fraud']))","execution_count":50,"outputs":[]},{"metadata":{"_cell_guid":"39262213-1e22-4a7f-b6f5-f6d6c81b91e3","_uuid":"09846b526c5c6f27462a429c67eef73948e21780","trusted":true},"cell_type":"code","source":"cv_params = {'max_depth': [3,5,7], 'min_child_weight': [1,3,5]}\nind_params = {'learning_rate': 0.1, 'n_estimators': 100, 'seed':0, 'subsample': 0.8, 'colsample_bytree': 0.8, 'objective': 'binary:logistic'}\noptimized_model = GridSearchCV(xgb.XGBClassifier(**ind_params), \n                            cv_params, \n                             scoring = 'accuracy', cv = 5, n_jobs = -1) \noptimized_model.fit(X_train,y_train)","execution_count":51,"outputs":[]},{"metadata":{"_cell_guid":"b4f7ee49-c377-4816-99d8-683d1435aa82","_uuid":"7a9d085223987726cbe989dd34562dd3651c775d","collapsed":true,"trusted":true},"cell_type":"code","source":"best_optimized_model = optimized_model.best_estimator_","execution_count":52,"outputs":[]},{"metadata":{"_cell_guid":"dc7a182e-91f7-4c1e-acab-e40b07054535","_uuid":"625a84bc02084793a85091f013455f71e5547bdf","trusted":true},"cell_type":"code","source":"y_pred_xgb = best_optimized_model.predict(X_test)\ny_score_xgb = optimized_model.predict_proba(X_test)[:,1]\nfpr_xgb, tpr_xgb, _ = roc_curve(y_test, y_score_xgb)","execution_count":53,"outputs":[]},{"metadata":{"_cell_guid":"ecedd707-21b1-49a6-92b1-58ef414b8447","_uuid":"1d52deef2ac92f13d59add63ae29e2093fa048dd","trusted":true},"cell_type":"code","source":"print('XGBoost ROC AUC', auc(fpr_xgb, tpr_xgb))\nprint(classification_report(y_test, y_pred_xgb, target_names=['NoFraud','Fraud']))","execution_count":54,"outputs":[]},{"metadata":{"_cell_guid":"ab35e167-b50a-404d-a548-1f24068658e9","_uuid":"e4d40efcf6fd640f1848cbbc7e192b5390890e13","collapsed":true},"cell_type":"markdown","source":"We can see that the new procedure improves the performance of the predictor."},{"metadata":{"_cell_guid":"1c45940f-4db8-4aec-96e3-f5fdf56a6173","_uuid":"122542cef6a0213671deb43d36a9e37e66a42019"},"cell_type":"markdown","source":"<a id=\"section_DataExploration\"></a>\n## **Results** \n\nThe predictive model based on XGBoost sets a very good baseline in terms of precision, accuracy and AUC-ROC. Implementing a Search Grid optimization improves the overall performance. The clustering of the 0-class is a procedure that aims to provide the most relevant and complete sample of the regular events based on similarity of records ans further improves the performance of the predictor to a precision in of 0.96 and recall of 0.95 which is a very good performance. "},{"metadata":{"_cell_guid":"fef657c0-7f67-4630-9936-8827ee072e4b","_uuid":"a9ea8eb974e31f6a424c57f9561a640afb1efea4"},"cell_type":"markdown","source":"<a id=\"section_DataExploration\"></a>\n## **Conclusions and Next Steps** \n\nThe detection of fraud in credit transations requires special attention to the inbalance problem, the overwhelming disproportion of positive and negative examples. This rare-event detection can be improved by undersampling the 0-class events and using XGBoost, grid search optimization and selecting representative records from clusters of 0-class events, the overall performance achieved was 0.96/0.95 precision and recall. \n\nTo further undersatnd and interpret the model, in a further study we will consider the characteristics of each cluster, quantifying the quality of the clustering methodology, compare to other suitable culstering methodology to improve the selection of representative events. \n\nThe special case of records with amount zero needs to be treated similarly. It is possible to add a fake non-zero but very small amount and add it to the procedure. "},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"7a83b5d404f1ff0b5c55aabf3442b67f5f896ec5"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}