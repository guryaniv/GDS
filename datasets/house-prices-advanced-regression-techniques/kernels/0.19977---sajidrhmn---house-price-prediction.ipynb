{"cells":[{"metadata":{"_cell_guid":"e81ee64d-e474-4662-9036-ce23df615199","_uuid":"b6269c0e8f417f82daf093dda8fa0da6d2c57d86"},"cell_type":"markdown","source":"# House Price Prediction\n**Workspace for the [Machine Learning course](https://www.kaggle.com/learn/machine-learning).**\n\n\nReading the csv files and printing basic details about the data"},{"metadata":{"_cell_guid":"86b26423-563a-4fa1-a595-89e25ff93089","_uuid":"1c728098629e1301643443b1341556a15c089b2b","trusted":true},"cell_type":"code","source":"import pandas as pd\n\npd.set_option('display.max_rows', 5)\nmain_file_path = '../input/house-prices-advanced-regression-techniques/train.csv' # this is the path to the Iowa data that you will use\ntest = pd.read_csv('../input/house-prices-advanced-regression-techniques/test.csv')\n# test = pd.read_csv('test.csv')\n# main_file_path = \"train.csv\"\ndata = pd.read_csv(main_file_path)\n\nprint(data.describe())\nprint(data.head())\nprint(data.columns)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"301e52efb0e3377d0696988ce6f1443b3bb647c1"},"cell_type":"markdown","source":"**Correlation Plot**"},{"metadata":{"trusted":true,"_uuid":"9f94f3e24b658cf89ba24a37dc981fe29d5bd411"},"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\ncorrmat = data.corr()\nf, ax = plt.subplots(figsize=(20, 9))\nsns.heatmap(corrmat, vmax=.8, annot=True);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c88963228fc130d94b6e438f4d17040125374dca"},"cell_type":"code","source":"sns.set()\ncols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt']\nsns.pairplot(data[cols], size = 2.5)\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0217b0d8c5b58e2d5f26e919dae02216cf38de63"},"cell_type":"markdown","source":"**Analysis on SalePrice**"},{"metadata":{"trusted":true,"_uuid":"eb683355e8976f78595755041c733ae97e82a21e"},"cell_type":"code","source":"from scipy import stats\nfrom scipy.stats import norm, skew #for some statistics\n\nsns.distplot(data['SalePrice'] , fit=norm);\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(data['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\nfig = plt.figure()\nres = stats.probplot(data['SalePrice'], plot=plt)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"792aa4995ca5406b44a559169fbafaea8541efcb"},"cell_type":"markdown","source":"**Checking Sale Price Column**"},{"metadata":{"_uuid":"c26a29f8bbc6f7f02eb3dfe247e2fa7853ff3aa7","trusted":true},"cell_type":"code","source":"print(data[\"SalePrice\"].describe())\nprint(data[\"SalePrice\"].head())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dfd012b992e9c6483616c9dcfc41f14f2b04bc33"},"cell_type":"markdown","source":"# Feature Selection"},{"metadata":{"_uuid":"0228424b9427c2e5ffaf3c49f47e5e845e4f2fc5","trusted":true},"cell_type":"code","source":"feature_list = [\"LotArea\",\"YearBuilt\",\"1stFlrSF\",\"2ndFlrSF\",\"FullBath\",\"BedroomAbvGr\",\"TotRmsAbvGrd\"]\nX = data[feature_list]\ny = data[\"SalePrice\"]\nX.head()\ny.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"350e35aa38face46e8a90d60d626c2c6fc0334d6"},"cell_type":"markdown","source":"# Decision Tree"},{"metadata":{"_uuid":"829db6a909e7527b3c3983d35d4cb34552d5fcc3","trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor as dt\n\niowa_model = dt()\niowa_model.fit(X,y)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"15ff447f854bc8730b84e838283ca932ee478c44","trusted":true},"cell_type":"code","source":"iowa_model.predict(X.head())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3b1b790fd06e89ebc2c377cda433d8a2ef34fdcd"},"cell_type":"markdown","source":"**Checking MAE using built-in method**"},{"metadata":{"_uuid":"3b47e6aaed2f6dd2f7db194ea2a0d4f0860e1c19","trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_absolute_error\n\npredicted_home_prices = iowa_model.predict(X)\nmean_absolute_error(y, predicted_home_prices)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"70fa230605d1c598990187ea330e41405e3b43bb"},"cell_type":"markdown","source":"**Splitting into test-train sets**"},{"metadata":{"_uuid":"b7aba0537a831d78c78d0b89deb093b95ec77700","trusted":true},"cell_type":"code","source":"#split training and validation data using scikit-learns inbuilt function\n\nfrom sklearn.model_selection import train_test_split\ntrain_X, val_X, train_y, val_y = train_test_split(X, y,random_state = 0)\n\niowa_model.fit(train_X,train_y)\npredicted_home_prices = iowa_model.predict(val_X)\nmean_absolute_error(val_y,predicted_home_prices)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4c45c7ed7a799301ec10fa984c7289d763dd4faf"},"cell_type":"markdown","source":"**Searching for optimal leaf nodes**"},{"metadata":{"_uuid":"bd6575d906f6db7b85d6a9cd059606ca71c25eb1","trusted":true},"cell_type":"code","source":"def get_mae(max_leaf_nodes, predictors_train, predictors_val, targ_train, targ_val):\n    model = dt(max_leaf_nodes=max_leaf_nodes, random_state=0)\n    model.fit(predictors_train, targ_train)\n    preds_val = model.predict(predictors_val)\n    mae = mean_absolute_error(targ_val, preds_val)\n    return(mae)\n\nfor max_leaf_nodes in [5, 50, 500, 5000]:\n    my_mae = get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y)\n    print(\"Max leaf nodes: %d  \\t\\t Mean Absolute Error:  %d\" %(max_leaf_nodes, my_mae))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b17e0184032c22ebf6839595194260f4a1499878"},"cell_type":"markdown","source":"# Ramdom Forest"},{"metadata":{"_uuid":"278d95c0da357b9684cfc5911ec55ac943585741","trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\n\nforest_model = RandomForestRegressor()\nforest_model.fit(train_X, train_y)\nmelb_preds = forest_model.predict(val_X)\nprint(mean_absolute_error(val_y, melb_preds))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2488bd19aacfc5ce13fd0be2f2ef91c740586c3c","trusted":true},"cell_type":"code","source":"test_features = test[feature_list]\npredicted_prices = forest_model.predict(test_features)\nprint(predicted_prices)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"86964abd49a52f2970fa15d16d9a1551de8b1892","collapsed":true,"trusted":true},"cell_type":"code","source":"#creat submission file called submission.csv\nmy_submission = pd.DataFrame({'Id': test.Id, 'SalePrice': predicted_prices})\nmy_submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"895df7f1-dab8-4c54-ab7e-9a865146deac","_uuid":"704e07440d7d4ef7ad3cf25c0a966c000bb8eeef"},"cell_type":"markdown","source":"# Handling Missing Values"},{"metadata":{"_uuid":"e0b31e9a93bc01b5f5ec9e3caaf4fb494969027a"},"cell_type":"markdown","source":"**Drop Columns with Missing Values**"},{"metadata":{"trusted":true,"_uuid":"97d4ce13a772d4eeb7756ba3afd3fa73d32af164"},"cell_type":"code","source":"data_without_missing_values = data.dropna(axis=1)\n\ncols_with_missing = [col for col in data.columns if data[col].isnull().any()]\nreduced_X_train = train_X.drop(cols_with_missing, axis=1)\nreduced_X_test  = val_X.drop(cols_with_missing, axis=1)\n\nprint(\"Mean Absolute Error from dropping columns with Missing Values:\")\nprint(score_dataset(reduced_X_train, reduced_X_test, y_train, y_test))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1f1a4f243b7254b285ff989c7388a6516dbcfed0"},"cell_type":"markdown","source":"**Imputation**\n\nImputation fills in the missing value with some number. The default behavior fills in the mean value for imputation. Statisticians have researched more complex strategies, but those complex strategies typically give no benefit once you plug the results into sophisticated machine learning models.\n\nOne (of many) nice things about Imputation is that it can be included in a scikit-learn Pipeline. Pipelines simplify model building, model validation and model deployment."},{"metadata":{"trusted":true,"_uuid":"8aaa6b83560f683124fcbcb634afe34118689563"},"cell_type":"code","source":"from sklearn.preprocessing import Imputer\n\ndef score_dataset(X_train, X_test, y_train, y_test):\n    model = RandomForestRegressor()\n    model.fit(X_train, y_train)\n    preds = model.predict(X_test)\n    return mean_absolute_error(y_test, preds)\n\nmy_imputer = Imputer()\nimputed_X_train = my_imputer.fit_transform(train_X)\nimputed_X_test = my_imputer.transform(val_X)\nprint(\"Mean Absolute Error from Imputation:\")\nprint(score_dataset(imputed_X_train, imputed_X_test, train_y, val_y))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"63409b2dd29d7a0e2960265042baa7df647e22ed"},"cell_type":"code","source":"data","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c8e6c312c74b79d96340ee475a70340c8a89a109"},"cell_type":"markdown","source":"**One Hot Encoding for Categorical Data**\n\nUse pd.get_dummies() to get encodings"},{"metadata":{"trusted":true,"_uuid":"27b58cbbb8afeb5afeba904180dd1db76ae354f4"},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestRegressor\n\ndef get_mae(X, y):\n    # multiple by -1 to make positive MAE score instead of neg value returned as sklearn convention\n    return -1 * cross_val_score(RandomForestRegressor(50), \n                                X, y, \n                                scoring = 'neg_mean_absolute_error').mean()\n\npredictors_without_categoricals = X.select_dtypes(exclude=['object'])\n\nmae_without_categoricals = get_mae(predictors_without_categoricals, y)\n\none_hot_encoded_training_predictors = pd.get_dummies(X)\nmae_one_hot_encoded = get_mae(one_hot_encoded_training_predictors, y)\n\nprint('Mean Absolute Error when Dropping Categoricals: ' + str(int(mae_without_categoricals)))\nprint('Mean Abslute Error with One-Hot Encoding: ' + str(int(mae_one_hot_encoded)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8fd23125357037ef00ee8250fbc6c7fef6269489"},"cell_type":"code","source":"print(one_hot_encoded_training_predictors)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fffc80b8a059718f22eb220d6e9393b3654e3b94"},"cell_type":"markdown","source":"# XGBoost - Xtreme Gradient Boosting"},{"metadata":{"trusted":true,"_uuid":"dd3535d12d34df8c7e98691cfa9515625011fdc3"},"cell_type":"code","source":"from xgboost import XGBRegressor\n\nmy_model = XGBRegressor(n_estimators=1000, learning_rate=0.05)\nmy_model.fit(train_X, train_y, early_stopping_rounds=5, eval_set=[(val_X, val_y)], verbose=False)\npredictions = my_model.predict(val_X)\nprint(\"Mean Absolute Error : \" + str(mean_absolute_error(predictions, val_y)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"7bb42a7e7d78633528f680ac8d91b892d16f07ff"},"cell_type":"code","source":"#creat submission file called submission.csv\ntest_features = test[feature_list]\npredicted_prices = forest_model.predict(test_features)\nmy_submission = pd.DataFrame({'Id': test.Id, 'SalePrice': predicted_prices})\nmy_submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a6f7b20ca6b7e296322d88a943811a13ac0dcd47"},"cell_type":"markdown","source":"**XGBoost with Hot Encoding**"},{"metadata":{"trusted":true,"_uuid":"69f16684a0dad9476f4db2077032ddc0cfc175fb"},"cell_type":"code","source":"my_model.fit(one_hot_encoded_training_predictors, train_y, early_stopping_rounds=5, eval_set=[(val_X, val_y)], verbose=False)\npredictions = my_model.predict(val_X)\nprint(\"Mean Absolute Error : \" + str(mean_absolute_error(predictions, val_y)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"86af470b7d30f6c148be37e7bb87714d6a80a992"},"cell_type":"markdown","source":"# Partial Dependence Plot"},{"metadata":{"_uuid":"e24afa0ab71dece2cb8c9a17de80aef2a10b508e"},"cell_type":"markdown","source":"**The partial dependence plot is calculated only after the model has been fit.**\n\nNote: scikit-learn originally implemented partial dependence plots only for Gradient Boosting models. This was due to an implementation detail, and a future release will support all model types."},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"b44fb539e1cea6b6e4c04ebc972b54176f541e04"},"cell_type":"code","source":"from sklearn.ensemble.partial_dependence import partial_dependence, plot_partial_dependence\nfrom sklearn.ensemble import GradientBoostingRegressor\nmy_model = GradientBoostingRegressor()\n# fit the model as usual\nmy_model.fit(X, y)\nmy_plots = plot_partial_dependence(my_model,       \n                                   features=[0, 2], # column numbers of plots we want to show\n                                   X=X,            # raw predictors data.\n                                   feature_names=['Distance', 'Landsize', 'BuildingArea'], # labels on graphs\n                                   grid_resolution=10) # number of values to plot on x axis","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"145dae20909a95d60e0fc86df38c4e8237369ea8"},"cell_type":"markdown","source":"# Pipelines\n\nPipelines are a simple way to keep your data processing and modeling code organized. Specifically, a pipeline bundles preprocessing and modeling steps so you can use the whole bundle as if it were a single step."},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"1fa3b55d5f76bca509814df17d046a5a41c8fe76"},"cell_type":"code","source":"from sklearn.pipeline import make_pipeline\n\nmy_pipeline = make_pipeline(Imputer(), RandomForestRegressor())","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"f6e52913f7d5d29dea020d1da699bb05799c1733"},"cell_type":"code","source":"my_pipeline.fit(train_X,train_y)\npredictions = my_pipeline.predict(val_X)\npredictions","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"41c45133f9fc56b609018fecca383ff6363e8200"},"cell_type":"markdown","source":"**Understanding Pipelines**\n\nMost scikit-learn objects are either transformers or models.\n\nTransformers are for pre-processing before modeling. The Imputer class (for filling in missing values) is an example of a transformer. Over time, you will learn many more transformers, and you will frequently use multiple transformers sequentially.\n\nModels are used to make predictions. You will usually preprocess your data (with transformers) before putting it in a model.\n\nYou can tell if an object is a transformer or a model by how you apply it. After fitting a transformer, you apply it with the transform command. After fitting a model, you apply it with the predict command. Your pipeline must start with transformer steps and end with a model. This is what you'd want anyway.\n\nEventually you will want to apply more transformers and combine them more flexibly. We will cover this later in an Advanced Pipelines tutorial."},{"metadata":{"_uuid":"7e288acb37833fc9efb7cea8d9339c409cbce904"},"cell_type":"markdown","source":"# Cross Validation\n\nIn cross-validation, we run our modeling process on different subsets of the data to get multiple measures of model quality. For example, we could have 5 folds or experiments. We divide the data into 5 pieces, each being 20% of the full dataset."},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"105e66765dd4c55e494616e331b4a4ceeb92bfd1"},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\nscores = cross_val_score(my_pipeline, X, y, scoring='neg_mean_absolute_error')\nprint(scores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"37f83912a7bb4139b1c9461d7c38e1b139da889f"},"cell_type":"code","source":"print('Mean Absolute Error %2f' %(-1 * scores.mean()))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e7e11c55eb0e76c9a396e5980e553e857a1c6df4"},"cell_type":"markdown","source":"# Data Leakage\n\n**Leaky Predictors**\nThis occurs when your predictors include data that will not be available at the time you make predictions.\nCheck which predictors might depend on the target variable and drop them from training.\n\n**Leaky Validation Strategy**\nA much different type of leak occurs when you aren't careful distinguishing training data from validation data.\nBe careful to not add testing data in fitting."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.2"}},"nbformat":4,"nbformat_minor":1}