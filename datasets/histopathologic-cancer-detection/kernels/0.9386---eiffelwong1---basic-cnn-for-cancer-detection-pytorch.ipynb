{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"scrolled":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d18642c1bcaec05236c28de039e7ce2f6512f6d3"},"cell_type":"markdown","source":"so we have 4 files to work with.\nand we will load in some librarys, there will probably mix of pytourch, sklearn."},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true,"_uuid":"834a681c1d797530f254b850145abb4378111a99"},"cell_type":"code","source":"import csv\nfrom PIL import Image\nimport numpy as np\nimport pandas as pd\nimport os\nimport cv2\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom sklearn.model_selection import train_test_split\nimport torchvision.transforms as transforms\n\nimport torch\nfrom torch.utils.data import TensorDataset, DataLoader,Dataset\nfrom torch.utils.data.sampler import SubsetRandomSampler\n\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.autograd import Variable\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import sampler\nimport torchvision\nimport torchvision.datasets as dset\nimport torchvision.transforms as T\nimport timeit\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"af2dc7552bd0c835fdb741396c052757d189c8b8"},"cell_type":"code","source":"class CancerDataset(Dataset):\n    def __init__(self, datafolder, datatype='train', transform = transforms.Compose([transforms.ToTensor()]), labels_dict={}):\n        self.datafolder = datafolder\n        self.datatype = datatype\n        self.image_files_list = [s for s in os.listdir(datafolder)]\n        self.transform = transform\n        self.labels_dict = labels_dict\n        if self.datatype == 'train':\n            self.labels = [labels_dict[i.split('.')[0]] for i in self.image_files_list]\n        else:\n            self.labels = [0 for _ in range(len(self.image_files_list))]\n\n    def __len__(self):\n        return len(self.image_files_list)\n\n    def __getitem__(self, idx):\n        img_name = os.path.join(self.datafolder, self.image_files_list[idx])\n        image = Image.open(img_name)\n        image = self.transform(image)\n        img_name_short = self.image_files_list[idx].split('.')[0]\n\n        if self.datatype == 'train':\n            label = self.labels_dict[img_name_short]\n        else:\n            label = 0\n        return image, label","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"IMAGE_NOT_FOUND_COUNTER = 0\n\nlabels = pd.read_csv('../input/train_labels.csv')\n\ndata_transforms = transforms.Compose([\n    #transforms.CenterCrop(32),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomVerticalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n    ])\ndata_transforms_test = transforms.Compose([\n    #transforms.CenterCrop(32),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n    ])\n\n\n\ntr, val = train_test_split(labels.label, stratify=labels.label, test_size=0.1)\nprint(\"number of training data: \",len(tr))\nprint(\"number of testing  data: \",len(val))\n# dictionary with labels and ids of train data\nimg_class_dict = {k:v for k, v in zip(labels.id, labels.label)}\n\ntrain_sampler = SubsetRandomSampler(list(tr.index))\nvalid_sampler = SubsetRandomSampler(list(val.index))\nbatch_size = 256\nnum_workers = 0\n\ndataset = CancerDataset(datafolder='../input/train/', datatype='train', transform=data_transforms, labels_dict=img_class_dict)\ntest_set = CancerDataset(datafolder='../input/test/', datatype='test', transform=data_transforms_test)\n# prepare data loaders (combine dataset and sampler)\ntrain_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=train_sampler, num_workers=num_workers)\nvalid_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=valid_sampler, num_workers=num_workers)\ntest_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, num_workers=num_workers)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"90bae2c4a31beaf2945d2f54c9f092155bfed1a9"},"cell_type":"code","source":"class Flatten(nn.Module):\n    def forward(self, x):\n        N, C, H, W = x.size() # read in N, C, H, W\n        return x.view(N, -1)  # \"flatten\" the C * H * W values into a single vector per image","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9112b0d404cc38ead7693b7ee6c9cb5cf792d4b4"},"cell_type":"code","source":"avg_loss_list = []\nacc_list = []\n\ndef train(model, train_loader ,loss_fn, optimizer, num_epochs = 1):\n    total_loss =0\n\n    for epoch in range(num_epochs):\n        print('Starting epoch %d / %d' % (epoch + 1, num_epochs))\n        model.train()\n        for t, (x, y) in enumerate(train_loader):\n            x_var = Variable(x.type(gpu_dtype))\n            y_var = Variable(y.type(gpu_dtype).long())\n\n            scores = model(x_var)\n            loss = loss_fn(scores, y_var)\n            total_loss += loss.data\n            \n            if (t + 1) % print_every == 0:\n                avg_loss = total_loss/print_every\n                print('t = %d, avg_loss = %.4f' % (t + 1, avg_loss) )\n                avg_loss_list.append(avg_loss)\n                total_loss = 0\n                \n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n        acc = check_accuracy(fixed_model_gpu, valid_loader)\n        print('acc = %f' %(acc))\n            \ndef check_accuracy(model, loader):\n    print('Checking accuracy on test set')   \n    num_correct = 0\n    num_samples = 0\n    model.eval() # Put the model in test mode (the opposite of model.train(), essentially)\n    for x, y in loader:\n        x_var = Variable(x.type(gpu_dtype))\n\n        scores = model(x_var)\n        _, preds = scores.data.cpu().max(1)\n        num_correct += (preds == y).sum()\n        num_samples += preds.size(0)\n    acc = float(num_correct) / num_samples\n    acc_list.append(acc)\n    return acc\n    print('Got %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"cdcb7f6c9adffc8b1b2862773997e7f32726f5fc"},"cell_type":"code","source":"from torchvision import models\n\nprint_every = 20\ngpu_dtype = torch.cuda.FloatTensor\n\nout_1 = 32\nout_2 = 64\nout_3 = 128\nout_4 = 256\n\nk_size_1 = 3\npadding_1 = 1\n\n\nnum_epochs = 6\n\nfixed_model_base = nn.Sequential( # You fill this in!\n                nn.Conv2d(3, out_1, padding= padding_1, kernel_size=k_size_1, stride=1), # out_1-k_size_1+1 = 26\n                nn.ReLU(inplace=True),\n                nn.BatchNorm2d(out_1),\n                nn.Conv2d(out_1 , out_1, padding= padding_1, kernel_size=k_size_1, stride=1), #26 - 4 + 1 = 23\n                nn.ReLU(inplace=True),\n                nn.BatchNorm2d(out_1),\n                nn.Conv2d(out_1 , out_1, padding= padding_1, kernel_size=k_size_1, stride=1), # 23 -3 = 20\n                nn.ReLU(inplace=True),\n                nn.BatchNorm2d(out_1),\n    \n                nn.MaxPool2d(2, stride=2),\n    \n                nn.Conv2d(out_1 , out_2, padding= padding_1, kernel_size=k_size_1, stride=1), # 20 -3 = 17\n                nn.ReLU(inplace=True),\n                nn.BatchNorm2d(out_2),\n                nn.Conv2d(out_2 , out_2, padding= padding_1, kernel_size=k_size_1, stride=1), # 17 -3 = 14\n                nn.ReLU(inplace=True), \n                nn.BatchNorm2d(out_2),\n                nn.Conv2d(out_2 , out_2, padding= padding_1, kernel_size=k_size_1, stride=1), # 17 -3 = 14\n                nn.ReLU(inplace=True),\n                nn.BatchNorm2d(out_2),\n    \n                nn.MaxPool2d(2, stride=2),\n    \n                nn.Conv2d(out_2 , out_3, padding= padding_1, kernel_size=k_size_1, stride=1),\n                nn.ReLU(inplace=True),\n                nn.BatchNorm2d(out_3),\n                nn.Conv2d(out_3 , out_3, padding= padding_1, kernel_size=k_size_1, stride=1), # 17 -3 = 14\n                nn.ReLU(inplace=True),\n                nn.BatchNorm2d(out_3),\n                nn.Conv2d(out_3 , out_3, padding= padding_1, kernel_size=k_size_1, stride=1), # 17 -3 = 14\n                nn.ReLU(inplace=True),\n                nn.BatchNorm2d(out_3),\n    \n                nn.MaxPool2d(2, stride=2),\n    \n                nn.Conv2d(out_3 , out_4, padding= padding_1, kernel_size=k_size_1, stride=1), # 17 -3 = 14\n                nn.ReLU(inplace=True),\n                nn.BatchNorm2d(out_4),\n                nn.Conv2d(out_4 , out_4, padding= padding_1, kernel_size=k_size_1, stride=1), # 17 -3 = 14\n                nn.ReLU(inplace=True),\n                nn.BatchNorm2d(out_4),\n                nn.Conv2d(out_4 , out_4, padding= padding_1, kernel_size=k_size_1, stride=1), # 17 -3 = 14\n                nn.ReLU(inplace=True),\n                nn.BatchNorm2d(out_4),\n    \n                #nn.Conv2d(out_11 , out_12, padding= padding_1, kernel_size=k_size_1, stride=1), # 17 -3 = 14\n                #nn.ReLU(inplace=True),\n                #nn.BatchNorm2d(out_12),\n    \n                nn.MaxPool2d(2, stride=2), #17/2 = 7\n                Flatten(),\n                \n                nn.Linear(9216,512 ), # affine layer\n                nn.ReLU(inplace=True),\n                nn.Linear(512,10), # affine layer\n                nn.ReLU(inplace=True),\n                nn.Linear(10,2), # affine layer\n            )\nfixed_model_gpu = fixed_model_base.type(gpu_dtype)\nprint(fixed_model_gpu)\nloss_fn = nn.modules.loss.CrossEntropyLoss()\noptimizer = optim.RMSprop(fixed_model_gpu.parameters(), lr = 1e-3)\n\ntrain(fixed_model_gpu, train_loader ,loss_fn, optimizer, num_epochs=num_epochs)\ncheck_accuracy(fixed_model_gpu, valid_loader)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6b2f26fc0e0f59be1eba5774d5d68c1b6f581ef6"},"cell_type":"code","source":"print(avg_loss_list,acc_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6849f23812e7deb06c2500395f80015115bfce2f"},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.plot([print_every*batch_size*(i+1)/len(tr) for i in range((len(avg_loss_list)))],avg_loss_list)\nplt.plot([i+1 for i in range((len(acc_list)))],acc_list)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6088c735b19731d3c5cfdd28811ceca346cabc66"},"cell_type":"code","source":"fixed_model_gpu.eval()\npreds = []\nfor batch_i, (data, target) in enumerate(test_loader):\n    data, target = data.cuda(), target.cuda()\n    output = fixed_model_gpu(data)\n\n    pr = output[:,1].detach().cpu().numpy()\n    for i in pr:\n        preds.append(i)\n        \ntest_preds = pd.DataFrame({'imgs': test_set.image_files_list, 'preds': preds})\n\ntest_preds['imgs'] = test_preds['imgs'].apply(lambda x: x.split('.')[0])\n\ndata_to_submit = pd.read_csv('../input/sample_submission.csv')\ndata_to_submit = pd.merge(data_to_submit, test_preds, left_on='id', right_on='imgs')\ndata_to_submit = data_to_submit[['id', 'preds']]\ndata_to_submit.columns = ['id', 'label']\ndata_to_submit.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4011f2fe1ba4c87cd64cc792ee37fd7977b65432"},"cell_type":"code","source":"data_to_submit.to_csv('csv_to_submit.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7a4baf77ec397302c7c35af473595f74b6cf240c"},"cell_type":"markdown","source":"#citation\n* data parsing and code for submittion are taken from: https://www.kaggle.com/artgor/simple-eda-and-model-in-pytorch"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}