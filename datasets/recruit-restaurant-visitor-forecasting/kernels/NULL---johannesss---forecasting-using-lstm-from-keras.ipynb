{"nbformat": 4, "nbformat_minor": 1, "cells": [{"cell_type": "code", "execution_count": null, "source": ["import numpy as np # linear algebra\n", "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n", "from sklearn.preprocessing import LabelEncoder\n", "from sklearn.preprocessing import MinMaxScaler\n", "from sklearn.metrics import mean_squared_error\n", "\n", "from keras.models import Sequential\n", "from keras.layers import Dense, LSTM\n", "from subprocess import check_output"], "metadata": {"_uuid": "0a3131f5b1e272426033b62d82d40be37bf52e65", "_cell_guid": "eb7eb04d-42c7-4971-a6d2-79f482ce0f25"}, "outputs": []}, {"cell_type": "code", "execution_count": null, "source": ["data = {\n", "    'tra': pd.read_csv('../input/air_visit_data.csv'),\n", "    'as': pd.read_csv('../input/air_store_info.csv'),\n", "    'hs': pd.read_csv('../input/hpg_store_info.csv'),\n", "    'ar': pd.read_csv('../input/air_reserve.csv'),\n", "    'hr': pd.read_csv('../input/hpg_reserve.csv'),\n", "    'id': pd.read_csv('../input/store_id_relation.csv'),\n", "    'tes': pd.read_csv('../input/sample_submission.csv'),\n", "    'hol': pd.read_csv('../input/date_info.csv').rename(columns={'calendar_date':'visit_date'})\n", "    }\n", "data['hr'] = pd.merge(data['hr'], data['id'], how='inner', on=['hpg_store_id'])\n", "data['hr'].drop('hpg_store_id',  axis=1, inplace=True)\n", "data['ar'] = data['ar'].append(data['hr'])\n", "data['tes']['air_store_id'] = data['tes']['id'].map(lambda x: '_'.join(x.split('_')[:2]))\n", "data['tes']['visit_date'] = data['tes']['id'].map(lambda x: str(x).split('_')[2])\n", "data['tes'].drop('id', axis=1, inplace=True)\n", "print ('Data loaded')"], "metadata": {"_uuid": "df069fc9a7396acba16d2cbb5c8742ca9941adc5", "_cell_guid": "750dab05-4563-4f05-874f-53eda6e9d6b0"}, "outputs": []}, {"cell_type": "code", "execution_count": null, "source": ["# Create single data set with all relevant base data:\n", "data['tra']['visit_datetime'] = pd.to_datetime(data['tra']['visit_date'])\n", "data['tra']['visit_date']     = data['tra']['visit_datetime'].dt.date\n", "data['ar']['res_visit_datetime'] = pd.to_datetime(data['ar']['visit_datetime'])\n", "data['ar']['reserve_datetime']   = pd.to_datetime(data['ar']['reserve_datetime'])\n", "data['ar']['visit_date']         = data['ar']['res_visit_datetime'].dt.date\n", "data['ar']['reserve_diff'] = data['ar'].apply(lambda r: (r['res_visit_datetime']\n", "                                                         - r['reserve_datetime']).days, \n", "                                        axis=1)\n", "data['ar'].drop('visit_datetime',  axis=1, inplace=True)\n", "data['ar'].drop('reserve_datetime',  axis=1, inplace=True)\n", "data['ar'].drop('res_visit_datetime',  axis=1, inplace=True)\n", "avg_reserv = data['ar'].groupby(['air_store_id','visit_date'], \n", "                                as_index=False).mean().reset_index()\n", "data['ar'] = data['ar'].groupby(['air_store_id','visit_date'], \n", "                                as_index=False).sum().reset_index()\n", "data['ar'] = data['ar'].drop(['reserve_diff'],axis=1)\n", "data['ar'] = data['ar'].drop(['index'],axis=1)\n", "data['ar']['reserve_diff'] = avg_reserv['reserve_diff']                            \n", "    \n", "data['hol']['visit_date'] = pd.to_datetime(data['hol']['visit_date'])\n", "data['hol']['visit_date'] = data['hol']['visit_date'].dt.date\n", "\n", "data['tes']['visit_datetime'] = pd.to_datetime(data['tes']['visit_date'])\n", "data['tes']['visit_date'] = data['tes']['visit_datetime'].dt.date\n", "\n", "prep_df = pd.merge(data['tra'], data['ar'],  how='left', on=['air_store_id', 'visit_date'])\n", "prep_df = pd.merge(prep_df,     data['as'],  how='inner', on='air_store_id')\n", "prep_df = pd.merge(prep_df,     data['hol'], how='left',  on='visit_date')\n", "\n", "predict_data = pd.merge(data['tes'],  data['ar'],   how='left', on=['air_store_id', 'visit_date'])\n", "predict_data = pd.merge(predict_data, data['as'],   how='inner', on='air_store_id')\n", "predict_data = pd.merge(predict_data, data['hol'],  how='left', on='visit_date')\n", "\n", "#print(len(prep_df[prep_df.air_store_id == \"air_35512c42db0868da\"]))\n", "#print(len(data['tra'][data['tra'].air_store_id == \"air_35512c42db0868da\"]))"], "metadata": {"_uuid": "492e59e2738e1c5db6a7fa81371daa1fdcc01753", "collapsed": true, "_cell_guid": "13033c0f-d4fc-4e43-8126-efc11a71e0f6"}, "outputs": []}, {"cell_type": "code", "execution_count": null, "source": ["# Encode fields:\n", "prep_df['month'] = prep_df['visit_datetime'].dt.month\n", "prep_df['day']   = prep_df['visit_datetime'].dt.day\n", "prep_df.drop('visit_datetime',      axis=1, inplace=True)   \n", "predict_data['month'] = predict_data['visit_datetime'].dt.month\n", "predict_data['day']   = predict_data['visit_datetime'].dt.day\n", "predict_data.drop('visit_datetime', axis=1, inplace=True)\n", "\n", "# Encode labels of categorical columns:\n", "cat_features = [col for col in ['air_genre_name', 'air_area_name', 'day_of_week']]\n", "for column in cat_features:\n", "    temp_prep = pd.get_dummies(pd.Series(prep_df[column]))\n", "    prep_df = pd.concat([prep_df,temp_prep],axis=1)\n", "    prep_df = prep_df.drop([column],axis=1)\n", "    temp_predict = pd.get_dummies(pd.Series(predict_data[column]))\n", "    predict_data = pd.concat([predict_data,temp_predict],axis=1)\n", "    predict_data = predict_data.drop([column],axis=1)\n", "    for missing_col in temp_prep:     # Make sure the columns of train and test are identical\n", "        if missing_col not in predict_data.columns:\n", "            predict_data[missing_col] = 0\n", "    \n", "prep_df['visitors'] = np.log1p(prep_df['visitors'])\n", "prep_df.fillna(0, inplace=True)\n", "predict_data.fillna(0, inplace=True)\n", "print('Done')"], "metadata": {"_uuid": "8251d594c4d3a507bdb36c5429bf08648721ad57", "_cell_guid": "75456b4f-afe0-41d3-9576-088871eb2b80"}, "outputs": []}, {"cell_type": "code", "execution_count": null, "source": ["air_ids = [air for air in prep_df['air_store_id'].unique()]\n", "mult_series = dict()\n", "scaler = MinMaxScaler(feature_range=(0, 1))\n", "\n", "store_key = prep_df[['air_store_id', 'visit_date']]\n", "store_key_predict = predict_data[['air_store_id', 'visit_date']]\n", "prep_df.drop(['air_store_id', 'visit_date'], axis=1, inplace=True)  \n", "predict_data.drop(['air_store_id', 'visit_date'], axis=1, inplace=True) \n", "cols = prep_df.columns\n", "cols_predict = predict_data.columns\n", "scaler.fit(prep_df)\n", "scaled_prep_df      = pd.DataFrame(scaler.transform(prep_df), columns=cols)\n", "scaled_predict_data = pd.DataFrame(scaler.transform(predict_data), columns=cols_predict)\n", "scaled_prep_df['air_store_id'] = store_key['air_store_id']\n", "scaled_prep_df['visit_date']   = store_key['visit_date']\n", "scaled_predict_data['air_store_id'] = store_key_predict['air_store_id']\n", "scaled_predict_data['visit_date']   = store_key_predict['visit_date']\n", "scaled_predict_data['visitors'] = 0\n", "\n", "for air_id in air_ids:\n", "    tmp = pd.DataFrame(scaled_prep_df[scaled_prep_df['air_store_id'] == air_id]).sort_values('visit_date')\n", "    tmp.drop('air_store_id', axis=1, inplace=True)  \n", "    tmp.set_index('visit_date', inplace=True)\n", "    mult_series[str(air_id)] = tmp.astype('float32')\n", "\n", "mult_series['air_ee3a01f0c71a769f'].head(10)  # Print data for sample restaurant\n", "#list(mult_series.keys())\n", "# Target:  y = prep_df['visitors'].values"], "metadata": {"_uuid": "0110301907bfea1240e6093d489c348f1535ecd2", "_cell_guid": "a59a6680-18a5-4109-a008-14cb976595d4"}, "outputs": []}, {"cell_type": "code", "execution_count": null, "source": ["# From https://machinelearningmastery.com/convert-time-series-supervised-learning-problem-python/\n", "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n", "    \"\"\"\n", "    Frame a time series as a supervised learning dataset.\n", "    Arguments:\n", "        data: Sequence of observations as a list or NumPy array.\n", "        n_in: Number of lag observations as input (X).\n", "        n_out: Number of observations as output (y).\n", "        dropnan: Boolean whether or not to drop rows with NaN values.\n", "    Returns:\n", "        Pandas DataFrame of series framed for supervised learning.\n", "    \"\"\"\n", "    n_vars = 1 if type(data) is list else data.shape[1]\n", "    df = pd.DataFrame(data)\n", "    cols, names = list(), list()\n", "    # Input sequence (t-n, ... t-1)\n", "    for i in range(n_in, 0, -1):\n", "        cols.append(df.shift(i))\n", "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n", "    # Forecast sequence (t, t+1, ... t+n)\n", "    for i in range(0, n_out):\n", "        cols.append(df.shift(-i))\n", "        if i == 0:\n", "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n", "        else:\n", "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n", "    # Put it all together\n", "    agg = pd.concat(cols, axis=1)\n", "    agg.columns = names\n", "    # Drop rows with NaN values\n", "    if dropnan:\n", "        agg.dropna(inplace=True)\n", "    return agg\n", "\n", "# Submissions are evaluated using RMSLE:\n", "def RMSLE(y, pred):\n", "    return mean_squared_error(y, pred)**0.5"], "metadata": {"_uuid": "fcae5885004264bbc541d2891195f9d300e76e4b", "collapsed": true, "_cell_guid": "185c0363-bfca-471a-94fe-e44bed734c63"}, "outputs": []}, {"cell_type": "code", "execution_count": null, "source": ["# Convert data series for supervised learning:\n", "tmp = pd.DataFrame(series_to_supervised(mult_series['air_ee3a01f0c71a769f'], 1, 1))\n", "tmp.drop(tmp.columns[[i for i in range(133,264)]], axis=1, inplace=True)\n", "super_data = tmp\n", "for air_id in air_ids:\n", "    tmp = series_to_supervised(mult_series[str(air_id)], 1, 1)\n", "    # Drop columns that should not be predicted (column #103 is number of visitors:\n", "    tmp.drop(tmp.columns[[i for i in range(133,264)]], axis=1, inplace=True)\n", "    super_data = super_data.append(tmp)\n", "super_data.head(10)"], "metadata": {"_uuid": "a0e51933fad965a99850bbe126270d5e854b227d", "_cell_guid": "7f154d13-2ed9-4f9a-91d9-be8f9eddba79"}, "outputs": []}, {"cell_type": "code", "execution_count": null, "source": ["# Prepare LSTM training, split up records into training and test data:\n", "train_size = int(len(super_data) * 0.7)\n", "test_size = len(super_data) - train_size\n", "\n", "train = super_data[:train_size].values\n", "test  = super_data[train_size:].values\n", "\n", "# Split into input and outputs\n", "train_X, train_y = train[:,:-1], train[:,-1]\n", "test_X, test_y = test[:, :-1], test[:, -1]\n", "\n", "# LSTM requires 3D data sets: [samples, timesteps, features]\n", "train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\n", "test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\n", "print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)"], "metadata": {"_uuid": "cddfbecbd3f530e0226d4c107543dd85b27c2fd0", "_cell_guid": "cd3af536-c427-4af2-ba3c-9b4d3cc2ef85"}, "outputs": []}, {"cell_type": "code", "execution_count": null, "source": ["# Train model:\n", "multi_model = Sequential()\n", "btc_size = 50\n", "#multi_model.add(LSTM(4, input_shape=(train_X.shape[1], train_X.shape[2])))\n", "multi_model.add(LSTM(4, batch_input_shape=(btc_size, train_X.shape[1], train_X.shape[2]), \n", "                     stateful=True))\n", "multi_model.add(Dense(1))\n", "multi_model.compile(loss='mse', optimizer='adam')\n", "for i in range(int(train_X.shape[0] / btc_size)):\n", "    this_X = train_X[(i * btc_size):((i + 1) * btc_size)][:][:]\n", "    this_y = train_y[(i * btc_size):((i + 1) * btc_size)]\n", "    multi_history = multi_model.fit(this_X, this_y, epochs=10, \n", "                                batch_size=btc_size, \n", "                                verbose=0, shuffle=False)\n", "    multi_model.reset_states()\n", "  "], "metadata": {"_uuid": "65dbec7eb6305636a7abf1ccfb74ed9570ade2cd", "collapsed": true, "_cell_guid": "8bd9ed1a-225b-45aa-9650-c1447fad54ae"}, "outputs": []}, {"cell_type": "code", "execution_count": null, "source": ["# Make predictions:\n", "y_pred = [test_X.shape[0]]\n", "for i in range(int(test_X.shape[0] / btc_size)):\n", "    this_X = test_X[(i * btc_size):((i + 1) * btc_size)][:][:]\n", "    this_pred = multi_model.predict(this_X, batch_size=btc_size)    \n", "    y_pred[(i * btc_size):((i + 1) * btc_size)] = this_pred\n", "\n", "test_X_nn = test_X.reshape((test_X.shape[0], test_X.shape[2]))\n", "# Invert scaling for forecast\n", "inv_y_pred = np.concatenate((y_pred, test_X_nn[:, 1:]), axis=1)\n", "inv_y_pred = scaler.inverse_transform(inv_y_pred)\n", "inv_y_pred = inv_y_pred[:,0]\n", "# Invert scaling for actual\n", "test_y_nn = test_y.reshape((len(test_y), 1))\n", "inv_y = np.concatenate((test_y_nn, test_X_nn[:, 1:]), axis=1)\n", "inv_y = scaler.inverse_transform(inv_y)\n", "inv_y = inv_y[:,0]\n", "\n", "print(inv_y_pred[:10])\n", "print(inv_y[:10])\n", "rmsle = RMSLE(inv_y, inv_y_pred)\n", "print('Test RMSLE: %.3f' % rmsle)"], "metadata": {"_uuid": "22b05d3d0b0a0c2279750562dc74f540df33451b", "_cell_guid": "5bab96fe-ab28-4f66-9016-6bd402f4dc3d"}, "outputs": []}], "metadata": {"kernelspec": {"name": "python3", "language": "python", "display_name": "Python 3"}, "language_info": {"name": "python", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "nbconvert_exporter": "python", "file_extension": ".py", "version": "3.6.3", "pygments_lexer": "ipython3"}}}