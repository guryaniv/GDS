{"cells":[{"metadata":{"_uuid":"0c7cbf17b5808ba0765bd9fc521b0f125ba54609"},"cell_type":"markdown","source":"# Load Data, Data Wrangling, Train Dataset and Test Dataset Preparation"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"ebb1e9bd534a98253ed20cbccff7865075b4a9c6"},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport time\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import tree\nfrom sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import accuracy_score, confusion_matrix, zero_one_loss, precision_recall_fscore_support, roc_curve, auc\n\na = time.time()","execution_count":2,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"8348e96abcfbee19923901ec7b2bb65854b1fe13"},"cell_type":"code","source":"# Select only those files with DETAILED RESULTS\nRegularSeason_df = pd.read_csv('Data/RegularSeasonDetailedResults.csv')\nTourney_df = pd.read_csv('Data/TourneyDetailedResults.csv')","execution_count":3,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"0ca79ae550f6c2e1fa2a89a0f26369be836ff304"},"cell_type":"code","source":"# Drop unnecessary columns, season index, day number, win location, numbers of overtime\nRegularSeason_df = RegularSeason_df.drop(['Season', 'Daynum', 'Wloc', 'Numot'], axis = 1)\nTourney_df = Tourney_df.drop(['Season', 'Daynum', 'Wloc', 'Numot'], axis = 1)","execution_count":4,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"f501f69984fecc4d4d43b830a655b405c9abdc21"},"cell_type":"code","source":"# Training Data\nWinLosePair_df = RegularSeason_df\n# WinLosePair_df","execution_count":5,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"e5a89ba1bff0df0ac1f6151ea789d233a79c70cd"},"cell_type":"code","source":"# Rename\nTrainDataOriginal_1_df = pd.DataFrame(WinLosePair_df[\\\n                                           ['Wfgm', 'Wfga', 'Wfgm3', 'Wfga3', 'Wftm', 'Wfta', 'Wor', 'Wdr', 'Wast', 'Wto', 'Wstl', 'Wblk', 'Wpf',\\\n                                           'Lfgm', 'Lfga', 'Lfgm3', 'Lfga3', 'Lftm', 'Lfta', 'Lor', 'Ldr', 'Last', 'Lto', 'Lstl', 'Lblk', 'Lpf']])\\\n.rename(columns = {'Wfgm':'fgm_x', 'Wfga':'fga_x', 'Wfgm3':'fgm3_x', 'Wfga3':'fga3_x', 'Wftm':'ftm_x', 'Wfta':'fta_x', 'Wor':'or_x', 'Wdr':'dr_x', 'Wast':'ast_x', 'Wto':'to_x', 'Wstl':'stl_x', 'Wblk':'blk_x', 'Wpf':'pf_x',\\\n                  'Lfgm':'fgm_y', 'Lfga':'fga_y', 'Lfgm3':'fgm3_y', 'Lfga3':'fga3_y', 'Lftm':'ftm_y', 'Lfta':'fta_y', 'Lor':'or_y', 'Ldr':'dr_y', 'Last':'ast_y', 'Lto':'to_y', 'Lstl':'stl_y', 'Lblk':'blk_y', 'Lpf':'pf_y'})\\\n.reset_index(drop = True)\n\nTrainDataOriginal_2_df = pd.DataFrame(WinLosePair_df[\\\n                                           ['Lfgm', 'Lfga', 'Lfgm3', 'Lfga3', 'Lftm', 'Lfta', 'Lor', 'Ldr', 'Last', 'Lto', 'Lstl', 'Lblk', 'Lpf',\\\n                                           'Wfgm', 'Wfga', 'Wfgm3', 'Wfga3', 'Wftm', 'Wfta', 'Wor', 'Wdr', 'Wast', 'Wto', 'Wstl', 'Wblk', 'Wpf']])\\\n.rename(columns = {'Lfgm':'fgm_x', 'Lfga':'fga_x', 'Lfgm3':'fgm3_x', 'Lfga3':'fga3_x', 'Lftm':'ftm_x', 'Lfta':'fta_x', 'Lor':'or_x', 'Ldr':'dr_x', 'Last':'ast_x', 'Lto':'to_x', 'Lstl':'stl_x', 'Lblk':'blk_x', 'Lpf':'pf_x',\\\n                  'Wfgm':'fgm_y', 'Wfga':'fga_y', 'Wfgm3':'fgm3_y', 'Wfga3':'fga3_y', 'Wftm':'ftm_y', 'Wfta':'fta_y', 'Wor':'or_y', 'Wdr':'dr_y', 'Wast':'ast_y', 'Wto':'to_y', 'Wstl':'stl_y', 'Wblk':'blk_y', 'Wpf':'pf_y'})\\\n.reset_index(drop = True)\n\n\nTrainData_df = TrainDataOriginal_1_df.append(TrainDataOriginal_2_df).reset_index(drop = True)\nTrainData = TrainData_df.values\nTrainLabel = np.ones((len(WinLosePair_df)*2))\nTrainLabel[len(WinLosePair_df):] = 2","execution_count":6,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"a408ec1b75c09e5a41a893092297b4838552f513"},"cell_type":"code","source":"# Test Data\n# Select those rows whose 'School' index has substring 'NCAA'\n# Select and then rstrip\n\nSeason2018_df = pd.read_csv('Data/2018.csv', skiprows = 1)\nNCAA_df = Season2018_df[Season2018_df['School'].str.contains('NCAA')]\nNCAA_df['School'] = NCAA_df['School'].map(lambda x: x.rstrip(' NCAA'))\n# NCAA_df","execution_count":7,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"1d437e72751d5ea41e758f414ca6549a590984a9"},"cell_type":"code","source":"# Annotated manually\nncaa2018_df = pd.read_csv('Data/ncaa2018.csv')\n# ncaa2018_df","execution_count":8,"outputs":[]},{"metadata":{"_uuid":"4542f4f97e7c8968901b1bb8a9b3ae1d7861d46a"},"cell_type":"markdown","source":"## Testing on the all the tournaments together using best performing Adaboost classifier"},{"metadata":{"trusted":false,"_uuid":"5f101ac73bc7a4ef2494eeabf9f8e003e080e262"},"cell_type":"code","source":"Season2018_stat_df = NCAA_df[['G', 'FG', 'FGA', '3P', '3PA', 'FT', 'FTA', 'ORB', 'TRB', 'AST', \n                              'TOV', 'STL', 'BLK', 'PF']]\nSeason2018_stat_df['TRB'] = Season2018_stat_df['TRB'] - Season2018_df['ORB']\nSeason2018_stat_df.rename(columns = {'TRB': 'DRB'}, inplace = True)\nSeason2018_avg_df = Season2018_stat_df.div(Season2018_stat_df.G, axis = 0).join(NCAA_df['School'])\nSeason2018_avg_df.drop('G', axis=1, inplace=True)\nSeason2018_avg_df.columns = ['fgm', 'fga', 'fgm3', 'fga3', 'ftm', 'fta', 'or', 'dr', 'ast', 'to', \n                            'stl', 'blk', 'pf', 'School']\n# Season2018_avg_df","execution_count":9,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"f8a0d804f6d7f38f69fba61dcbfc8b147ef12a60"},"cell_type":"code","source":"# half with first team winning, half with second team winning, to make the class more balanced\nTourney_df1 = Tourney_df.iloc[0 : int(len(Tourney_df)/2)]\nTourney_df2 = Tourney_df.iloc[int(len(Tourney_df)/2) : ]\nTestDataOriginal_1_df = pd.DataFrame(Tourney_df1[\\\n                                           ['Wfgm', 'Wfga', 'Wfgm3', 'Wfga3', 'Wftm', 'Wfta', 'Wor', 'Wdr', 'Wast', 'Wto', 'Wstl', 'Wblk', 'Wpf',\\\n                                           'Lfgm', 'Lfga', 'Lfgm3', 'Lfga3', 'Lftm', 'Lfta', 'Lor', 'Ldr', 'Last', 'Lto', 'Lstl', 'Lblk', 'Lpf']])\\\n.rename(columns = {'Wfgm':'fgm_x', 'Wfga':'fga_x', 'Wfgm3':'fgm3_x', 'Wfga3':'fga3_x', 'Wftm':'ftm_x', 'Wfta':'fta_x', 'Wor':'or_x', 'Wdr':'dr_x', 'Wast':'ast_x', 'Wto':'to_x', 'Wstl':'stl_x', 'Wblk':'blk_x', 'Wpf':'pf_x',\\\n                  'Lfgm':'fgm_y', 'Lfga':'fga_y', 'Lfgm3':'fgm3_y', 'Lfga3':'fga3_y', 'Lftm':'ftm_y', 'Lfta':'fta_y', 'Lor':'or_y', 'Ldr':'dr_y', 'Last':'ast_y', 'Lto':'to_y', 'Lstl':'stl_y', 'Lblk':'blk_y', 'Lpf':'pf_y'})\\\n.reset_index(drop = True)\n\nTestDataOriginal_2_df = pd.DataFrame(Tourney_df2[\\\n                                           ['Lfgm', 'Lfga', 'Lfgm3', 'Lfga3', 'Lftm', 'Lfta', 'Lor', 'Ldr', 'Last', 'Lto', 'Lstl', 'Lblk', 'Lpf',\\\n                                           'Wfgm', 'Wfga', 'Wfgm3', 'Wfga3', 'Wftm', 'Wfta', 'Wor', 'Wdr', 'Wast', 'Wto', 'Wstl', 'Wblk', 'Wpf']])\\\n.rename(columns = {'Lfgm':'fgm_x', 'Lfga':'fga_x', 'Lfgm3':'fgm3_x', 'Lfga3':'fga3_x', 'Lftm':'ftm_x', 'Lfta':'fta_x', 'Lor':'or_x', 'Ldr':'dr_x', 'Last':'ast_x', 'Lto':'to_x', 'Lstl':'stl_x', 'Lblk':'blk_x', 'Lpf':'pf_x',\\\n                  'Wfgm':'fgm_y', 'Wfga':'fga_y', 'Wfgm3':'fgm3_y', 'Wfga3':'fga3_y', 'Wftm':'ftm_y', 'Wfta':'fta_y', 'Wor':'or_y', 'Wdr':'dr_y', 'Wast':'ast_y', 'Wto':'to_y', 'Wstl':'stl_y', 'Wblk':'blk_y', 'Wpf':'pf_y'})\\\n.reset_index(drop = True)\n\nTestData_df = TestDataOriginal_1_df.append(TestDataOriginal_2_df).reset_index(drop = True)\n# TestData_df","execution_count":10,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"623872ee7065c883079bd3330d424bed98634e1e"},"cell_type":"code","source":"\nTest_df = pd.merge(ncaa2018_df, Season2018_avg_df, left_on = 'School_x', right_on = 'School', how = 'inner')\nTest_df = pd.merge(Test_df, Season2018_avg_df, left_on = 'School_y', right_on = 'School', how = 'inner')\nResults = Test_df['Results'].values\n\n# creating labels for tournament testing data\nTestLabel1 = np.ones((1, int(TestData_df.shape[0] / 2)))\nTestLabel2 = np.ones((1, int(TestData_df.shape[0] / 2))) * 2\nTestLabel = np.append(TestLabel1 , TestLabel2)\nTestLabel = np.append(Results, TestLabel)\n\nTest_df = Test_df.drop(['School_x', 'School_y', 'Rk_x', 'G_x', 'Results', 'Rk_y', 'G_y'], axis = 1)\n\n# add the past tournament data\nTest_df = Test_df.append(TestData_df)\n\nTest_df = Test_df.astype(int)\nTestData = Test_df.values\n# Test_df","execution_count":11,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"75b23f72d2c6263d934ccfa03f1c1411575ccee8"},"cell_type":"code","source":"# Logistic Regression\nlog_clf = LogisticRegression()\nlog_clf.fit(TrainData, TrainLabel)\nprint ('Logistic Regression')\nprint ('Accuracy: ', end = '')\nprint (accuracy_score(TestLabel, log_clf.predict(TestData)))\nprint ('Precision-Recall-F1')\nprint (precision_recall_fscore_support(TestLabel, log_clf.predict(TestData), average = \"weighted\"))\nprint ('----------------------------------')\n\n# LDA\nlda = LinearDiscriminantAnalysis()\nlda.fit(TrainData, TrainLabel)\nprint ('LDA')\nprint ('Accuracy: ', end = '')\nprint (accuracy_score(TestLabel, lda.predict(TestData)))\nprint ('Precision-Recall-F1')\nprint (precision_recall_fscore_support(TestLabel, lda.predict(TestData), average = \"weighted\"))\nprint ('----------------------------------')\n\n# Naive Bayes\ngnb = GaussianNB()\ngnb.fit(TrainData, TrainLabel)\nprint ('Naive Bayes')\nprint ('Accuracy: ', end = '')\nprint (accuracy_score(TestLabel, gnb.predict(TestData)))\nprint ('Precision-Recall-F1')\nprint (precision_recall_fscore_support(TestLabel, gnb.predict(TestData), average = \"weighted\"))\nprint ('----------------------------------')\n\n# KNN\nknn = KNeighborsClassifier(n_neighbors = 2)\nknn.fit(TrainData, TrainLabel)\nprint ('KNN')\nprint ('Accuracy: ', end = '')\nprint (accuracy_score(TestLabel, knn.predict(TestData)))\nprint ('Precision-Recall-F1')\nprint (precision_recall_fscore_support(TestLabel,knn.predict(TestData), average = \"weighted\"))\nprint ('----------------------------------')\n\n# SVM\nsvm = SVC()\nsvm.fit(TrainData, TrainLabel)\nprint ('SVM')\nprint ('Accuracy: ', end = '')\nprint (accuracy_score(TestLabel, svm.predict(TestData)))\nprint ('Precision-Recall-F1')\nprint (precision_recall_fscore_support(TestLabel, svm.predict(TestData), average = \"weighted\"))\nprint ('----------------------------------')\n\n# Decision Tree\ndt = tree.DecisionTreeClassifier()\ndt.fit(TrainData, TrainLabel)\nprint ('Decision Tree')\nprint ('Accuracy: ', end = '')\nprint (accuracy_score(TestLabel, dt.predict(TestData)))\nprint ('Precision-Recall-F1')\nprint (precision_recall_fscore_support(TestLabel, dt.predict(TestData), average = \"weighted\"))\nprint ('----------------------------------')\n\n# Random Forest\nrf = RandomForestClassifier(n_estimators = 100)\nrf.fit(TrainData, TrainLabel)\nprint ('Accuracy: ', end = '')\nprint (accuracy_score(TestLabel, rf.predict(TestData)))\nprint ('Precision-Recall-F1')\nprint (precision_recall_fscore_support(TestLabel, rf.predict(TestData), average = \"weighted\"))\nprint ('----------------------------------')\n\n# Adaboost\nadb = AdaBoostClassifier(n_estimators=100)\nadb.fit(TrainData, TrainLabel)\nprint ('Adaboost')\nprint ('Accuracy: ', end = '')\nprint (accuracy_score(TestLabel, adb.predict(TestData)))\nprint ('Precision-Recall-F1')\nprint (precision_recall_fscore_support(TestLabel, adb.predict(TestData), average = \"weighted\"))\nprint ('----------------------------------')\n\n# Gradient Boosting\ngb = GradientBoostingClassifier(n_estimators=100)\ngb.fit(TrainData, TrainLabel)\nprint ('Gradient Boosting')\nprint ('Accuracy: ', end = '')\nprint (accuracy_score(TestLabel, gb.predict(TestData)))\nprint ('Precision-Recall-F1')\nprint (precision_recall_fscore_support(TestLabel, gb.predict(TestData), average = \"weighted\"))","execution_count":14,"outputs":[]},{"metadata":{"_uuid":"6ac6a9cf23bb8304e4637236dd04df4fd90a706e"},"cell_type":"markdown","source":"# Visualization of the classifier performance"},{"metadata":{"_uuid":"8b9b8863609377799d9818e6d57478b03d11d7df"},"cell_type":"markdown","source":"## Comparing four classifiers"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"70d5129b885ae7bc08f3556aefa974d88032f3cf"},"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.model_selection import ShuffleSplit\n\n\ndef plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n    \"\"\"\n    Generate a simple plot of the test and training learning curve.\n\n    Parameters\n    ----------\n    estimator : object type that implements the \"fit\" and \"predict\" methods\n        An object of that type which is cloned for each validation.\n\n    title : string\n        Title for the chart.\n\n    X : array-like, shape (n_samples, n_features)\n        Training vector, where n_samples is the number of samples and\n        n_features is the number of features.\n\n    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n        Target relative to X for classification or regression;\n        None for unsupervised learning.\n\n    ylim : tuple, shape (ymin, ymax), optional\n        Defines minimum and maximum yvalues plotted.\n\n    cv : int, cross-validation generator or an iterable, optional\n        Determines the cross-validation splitting strategy.\n        Possible inputs for cv are:\n          - None, to use the default 3-fold cross-validation,\n          - integer, to specify the number of folds.\n          - An object to be used as a cross-validation generator.\n          - An iterable yielding train/test splits.\n\n        For integer/None inputs, if ``y`` is binary or multiclass,\n        :class:`StratifiedKFold` used. If the estimator is not a classifier\n        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n\n        Refer :ref:`User Guide <cross_validation>` for the various\n        cross-validators that can be used here.\n\n    n_jobs : integer, optional\n        Number of jobs to run in parallel (default 1).\n    \"\"\"\n    plt.figure()\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_error_mean = np.mean(1-train_scores, axis=1)\n    train_error_std = np.std(1-train_scores, axis=1)\n    test_error_mean = np.mean(1-test_scores, axis=1)\n    test_error_std = np.std(1-test_scores, axis=1)\n    plt.grid()\n\n    plt.fill_between(train_sizes, train_error_mean - train_error_std,\n                     train_error_mean + train_error_std, alpha=0.1, color=\"r\"\n                     )\n    plt.fill_between(train_sizes, test_error_mean - test_error_std,\n                     test_error_mean + test_error_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_error_mean, 'o-', color=\"r\",\n             label=\" Training error\")\n    plt.plot(train_sizes, test_error_mean, 'o-', color=\"g\",\n             label=\" Cross-validation error\")\n    plt.legend(loc=\"best\")\n    plt.show()\n#     return plt\n\n\nX_test,y_test = TestData, TestLabel\nX_train,y_train = TrainData, TrainLabel\n\nprint(\"start\")\n\ntitle = \"Learning Curves for Random Forest\"\n# Cross validation with 100 iterations to get smoother mean test and train\n# score curves, each time with 20% data randomly selected as a validation set.\ncv = ShuffleSplit(n_splits=100, test_size=0.2, random_state=0)\nestimator = RandomForestClassifier()\nplot_learning_curve(estimator, title, X_train, y_train, ylim=(0, 0.35), cv=cv, n_jobs=4)\n\n\ntitle = \"Learning Curves for Adaboost Classifier\"\nestimator = AdaBoostClassifier()\nplot_learning_curve(estimator, title, X_train, y_train, ylim=(0, 0.35), cv=cv, n_jobs=4)\n\n\ntitle = \"Learning Curves for Gradient Boosting Tree\"\n#cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\nestimator = GradientBoostingClassifier()\nplot_learning_curve(estimator, title, X_train, y_train, ylim=(0, 0.35), cv=cv, n_jobs=4)\n\n\ntitle = \"Learning Curves for Logistic Regression\"\nestimator = LogisticRegression()\nplot_learning_curve(estimator, title, X_train, y_train, ylim=(0, 0.35), cv=cv, n_jobs=-1)\n\n\n# plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"76a4b6e385d5d918119fed03ec1ee1809621782e"},"cell_type":"markdown","source":"## Comparing adaboost and gradient boosting tree"},{"metadata":{"trusted":false,"_uuid":"9b22cba0c562896a62f097e9e677f3e18fb9cd71"},"cell_type":"code","source":"n_estimators = 400\nlearning_rate = 1\nX_test,y_test = TestData, TestLabel\nX_train,y_train = TrainData, TrainLabel\n\ndt_stump=DecisionTreeClassifier(max_depth=1,min_samples_leaf=1)\ndt_stump.fit(X_train,y_train)\ndt_stump_err=1.0-dt_stump.score(X_test,y_test)\n \n# dt=DecisionTreeClassifier(max_depth=9,min_samples_leaf=1)\n# dt.fit(X_train,y_train)\n# dt_err=1.0-dt.score(X_test,y_test)\n \n# ada_discrete=AdaBoostClassifier(base_estimator=dt_stump,learning_rate=learning_rate,n_estimators=n_estimators,algorithm='SAMME')\n# ada_discrete.fit(X_train,y_train)\n\n# Random Forest\nrf_clf = RandomForestClassifier(n_estimators=n_estimators)\nrf_clf.fit(X_train,y_train)\n\n# Gradient Boosting Tree\ngbm_clf = GradientBoostingClassifier(n_estimators=n_estimators) # default n_estimator is 100\ngbm_clf.fit(X_train,y_train)\n \n# base_estimator=dt_stump\nada_real=AdaBoostClassifier(learning_rate=learning_rate,n_estimators=n_estimators,algorithm='SAMME.R')\nada_real.fit(X_train,y_train)\n \nfig=plt.figure()\nax=fig.add_subplot(111)\n# ax.plot([1,n_estimators],[dt_stump_err]*2,'k-',label='Decision Stump Error')\n# ax.plot([1,n_estimators],[dt_err]*2,'k--',label='Decision Tree Error')\n \n# ada_discrete_err=np.zeros((n_estimators,))\n# for i,y_pred in enumerate(ada_discrete.staged_predict(X_test)):\n#     ada_discrete_err[i]=zero_one_loss(y_pred,y_test)    ######zero_one_loss\n# ada_discrete_err_train=np.zeros((n_estimators,))\n# for i,y_pred in enumerate(ada_discrete.staged_predict(X_train)):\n#     ada_discrete_err_train[i]=zero_one_loss(y_pred,y_train)\n    \nada_real_err=np.zeros((n_estimators,))\nfor i,y_pred in enumerate(ada_real.staged_predict(X_test)):\n    ada_real_err[i]=zero_one_loss(y_pred,y_test)\nada_real_err_train=np.zeros((n_estimators,))\nfor i,y_pred in enumerate(ada_real.staged_predict(X_train)):\n    ada_real_err_train[i]=zero_one_loss(y_pred,y_train)\n    \ngbm_err=np.zeros((n_estimators,))\nfor i,y_pred in enumerate(gbm_clf.staged_predict(X_test)):\n    gbm_err[i]=zero_one_loss(y_pred,y_test)\ngbm_err_train=np.zeros((n_estimators,))\nfor i,y_pred in enumerate(gbm_clf.staged_predict(X_train)):\n    gbm_err_train[i]=zero_one_loss(y_pred,y_train)\n\nax.plot(np.arange(n_estimators)+1,ada_real_err,label='Real AdaBoost Test Error',color='orange')\nax.plot(np.arange(n_estimators)+1,ada_real_err_train,label='Real AdaBoost Train Error',color='green')\n# ax.plot(np.arange(n_estimators)+1,rf_err,label='Random Forest Test Error',color='red')\n# ax.plot(np.arange(n_estimators)+1,rf_err_train,label='Random Forest Train Error',color='blue')\nax.plot(np.arange(n_estimators)+1,gbm_err,label='Gradient Boosting Tree Test Error',color='black')\nax.plot(np.arange(n_estimators)+1,gbm_err_train,label='Gradient Boosting Tree Train Error',color='yellow')\n \nax.set_ylim((0.0,0.5))\nax.set_xlabel('n_estimators')\nax.set_ylabel('error rate')\n \nleg=ax.legend(loc='upper right',fancybox=True)\nleg.get_frame().set_alpha(0.7)\nb=time.time()\nprint('total running time of this example is :',b-a)\nplt.show()\n\n","execution_count":12,"outputs":[]},{"metadata":{"_uuid":"80304d56d3ef1674fea3aa5d20a033b6c008fe34"},"cell_type":"markdown","source":"## ROC curve for logistic and adaboost"},{"metadata":{"trusted":false,"_uuid":"dda3f78af4742896e0d6d9d7aff769f5f90262dd"},"cell_type":"code","source":"fpr, tpr, thresholds = roc_curve(TestLabel, log_clf.predict(TestData), pos_label=2)\n\nroc_auc = auc(fpr, tpr)\n\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.title(\"Logistic Regression ROC curve\")\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","execution_count":16,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"fe8d3ce5e39450f19a5c58cac902bbf8e3fca39d"},"cell_type":"code","source":"fpr, tpr, thresholds = roc_curve(TestLabel, ada_real.predict(TestData), pos_label=2)\n\nroc_auc = auc(fpr, tpr)\n\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.title(\"Adaboost ROC curve\")\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","execution_count":17,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"69e453f69b846a26cf76096deed16e194c995f41"},"cell_type":"code","source":"import scikitplot as skplt\nimport matplotlib.pyplot as plt\n\ny_true = TestLabel\ny_probas = log_clf.predict_proba(TestData)\nskplt.metrics.plot_roc_curve(y_true, y_probas)\nplt.title(\"Logistic Regression smooth ROC curve\")\nplt.show()","execution_count":18,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"ff25a428eab13538b16932c65eef5b88eb1bd217"},"cell_type":"code","source":"import scikitplot as skplt\nimport matplotlib.pyplot as plt\n\ny_true = TestLabel\ny_probas = ada_real.predict_proba(TestData)\nskplt.metrics.plot_roc_curve(y_true, y_probas)\nplt.title(\"Adaboost smooth ROC curve\")\nplt.show()","execution_count":19,"outputs":[]},{"metadata":{"_uuid":"c73dc77ac415564509ef6a5c4796fec85c02d5e0"},"cell_type":"markdown","source":"# Testing on this year's tournament on some single games"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"dfdc0dcb44526aeae3e332c1624a1e81e7600766"},"cell_type":"code","source":"# random game from this year's tournament (UMBC vs. Virginia)\nt1 = Season2018_avg_df[Season2018_avg_df['School'] == 'Maryland-Baltimore County']\nt2 = Season2018_avg_df[Season2018_avg_df['School'] == 'Virginia']\ncolumns_use = t1.columns.tolist()\n#columns_use.remove('G')\ncolumns_use.remove('School')\nt1_test = t1[columns_use].reset_index(drop=True)\nt2_test = t2[columns_use].reset_index(drop=True)\ntest_game = pd.concat([t1_test,t2_test], axis=1)","execution_count":20,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"4e1a450ff542e3a39580bbccda92b22f0603d92e"},"cell_type":"code","source":"log_clf.predict(test_game) # UMBC won, predicted correctly","execution_count":22,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"da5e251e9e6642f174a6cf86b17615ff405063a0"},"cell_type":"code","source":"# random game from this year's tournament (Missouri vs. Florida State)\nt1 = Season2018_avg_df[Season2018_avg_df['School'] == 'Missouri']\nt2 = Season2018_avg_df[Season2018_avg_df['School'] == 'Florida State']\ncolumns_use = t1.columns.tolist()\ncolumns_use.remove('School')\nt1_test = t1[columns_use].reset_index(drop=True)\nt2_test = t2[columns_use].reset_index(drop=True)\ntest_game = pd.concat([t1_test,t2_test], axis=1)","execution_count":23,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"1f1c933cedcef7e1d531144a1eb36af29c063e8f"},"cell_type":"code","source":"log_clf.predict(test_game) # FSU won, predicted correctly","execution_count":25,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"b4f0b299502f7de411b808821e60813b59ecc5e1"},"cell_type":"code","source":"# random game from this year's tournament (Duke vs. Iona)\nt1 = Season2018_avg_df[Season2018_avg_df['School'] == 'Duke']\nt2 = Season2018_avg_df[Season2018_avg_df['School'] == 'Iona']\ncolumns_use = t1.columns.tolist()\n#columns_use.remove('G')\ncolumns_use.remove('School')\nt1_test = t1[columns_use].reset_index(drop=True)\nt2_test = t2[columns_use].reset_index(drop=True)\ntest_game = pd.concat([t1_test,t2_test], axis=1)","execution_count":26,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"1c0382bc6083aef1acfdc7c6e21735fe46a07418"},"cell_type":"code","source":"log_clf.predict(test_game) # Duke won, predicted correctly","execution_count":28,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"0dc6fb379c126ea91ebcc578c18a13b7c820c3b5"},"cell_type":"code","source":"# random game from this year's tournament (Arizona vs. Buffalo)\nt1 = Season2018_avg_df[Season2018_avg_df['School'] == 'Arizona']\nt2 = Season2018_avg_df[Season2018_avg_df['School'] == 'Buffalo']\ncolumns_use = t1.columns.tolist()\n#columns_use.remove('G')\ncolumns_use.remove('School')\nt1_test = t1[columns_use].reset_index(drop=True)\nt2_test = t2[columns_use].reset_index(drop=True)\ntest_game = pd.concat([t1_test,t2_test], axis=1)","execution_count":29,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"ac0917e0fb5ac13845591a78a18216125c02b256"},"cell_type":"code","source":"log_clf.predict(test_game) # Buffalo won, predicted correctly","execution_count":31,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"7b1deb5fa581e3feb4baa9494237d31a33452f3b"},"cell_type":"code","source":"# random game from this year's tournament (Villanova vs. Radford)\nt1 = Season2018_avg_df[Season2018_avg_df['School'] == 'Radford']\nt2 = Season2018_avg_df[Season2018_avg_df['School'] == 'Villanova']\ncolumns_use = t1.columns.tolist()\n#columns_use.remove('G')\ncolumns_use.remove('School')\nt1_test = t1[columns_use].reset_index(drop=True)\nt2_test = t2[columns_use].reset_index(drop=True)\ntest_game = pd.concat([t1_test,t2_test], axis=1)","execution_count":32,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"0a59d714a091f3b9c27efd4a0cea04abd7f4b4c7"},"cell_type":"code","source":"log_clf.predict(test_game) # Villanova won, predicted correctly","execution_count":34,"outputs":[]},{"metadata":{"_uuid":"8d561e2ea2d48d24f37039ca9cdfbab465a6f123"},"cell_type":"markdown","source":"# Generating plots between game statistics and game outcomes"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"efeb3a0e595f0e222c4587f11b6251dbdb8dbcc3"},"cell_type":"code","source":"plotlist = ['fgm', 'fga', 'fgm3', 'fga3', 'ftm', 'fta', 'or', 'dr', 'ast', 'to', 'stl', 'blk', 'pf']\n\nfor key in plotlist:\n    x = WinLosePair_df['W' + key]\n    y = WinLosePair_df['L' + key]\n    plt.figure()\n    plt.plot(range(len(WinLosePair_df['W' + key])), x, 'o-', color = \"r\", label=\" Win\")\n    plt.plot(range(len(WinLosePair_df['L' + key])), y, '*-', color = \"g\", label=\" Lose\")\n    plt.xlabel(\"Team\")\n    plt.ylabel(\"Value\")\n    plt.legend(loc=\"best\")\n    plt.title(key + '_Comparison')\n    plt.savefig(key + '_Comparison')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"889e4e3ac6eeaf3df110c1074832f83e8e6d3a97"},"cell_type":"code","source":"x = WinLosePair_df['Wfgm'] / WinLosePair_df['Wfga']\ny = WinLosePair_df['Lfgm'] / WinLosePair_df['Lfga']\nplt.figure()\nplt.plot(range(len(WinLosePair_df['Wfgm'])), x, 'o-', color = \"r\", label=\" Win\")\nplt.plot(range(len(WinLosePair_df['Lfgm'])), y, '*-', color = \"g\", label=\" Lose\")\nplt.xlabel(\"Team\")\nplt.ylabel(\"Value\")\nplt.legend(loc=\"best\")\nplt.title('ShootingAverage_Comparison')\nplt.savefig('ShootingAverage_Comparison')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"85b16e614e8ad58a3491f0038fd162d574b4ba9c"},"cell_type":"markdown","source":"# Plotting differences between teams in game statistics and corresponding winning probabilities generated by our chosen classifier"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"8d145879c7333ec035d1065dcff70af171ed8b4b"},"cell_type":"code","source":"winning_probabilities = log_clf.predict_proba(TestData)","execution_count":35,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"6ef1f434e4084f58249c9c4748b727f12c656d14"},"cell_type":"code","source":"win_prob_1 = winning_probabilities[:, 0]","execution_count":36,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"e761cb9af7356b698cfa5d89088e5f9f0b5bc40a"},"cell_type":"code","source":"# Total Rebound\nplt.plot(Test_df['or_x']+Test_df['dr_x']-Test_df['or_y']-Test_df['dr_y'], win_prob_1)\nplt.ylabel(\"Winning Probability\")\nplt.xlabel(\"Total Rebound\")\nplt.show()","execution_count":37,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"17fea8f6ec92b88522b3eaafd9c42b1db4c9782f"},"cell_type":"code","source":"# Assist-to-Turnover Ratio\nplt.plot(Test_df['ast_x']/Test_df['to_x']-Test_df['ast_y']/Test_df['to_y'], win_prob_1)\nplt.ylabel(\"Winning Probability\")\nplt.xlabel(\"Assist-to-Turnover Ratio\")\nplt.show()","execution_count":38,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"9537d2055cb6775124fd61fefdbfebff739898b9"},"cell_type":"code","source":"# Turnover and Foul\nplt.plot(Test_df['to_x']+Test_df['pf_x']-Test_df['to_y']-Test_df['pf_y'], win_prob_1)\nplt.ylabel(\"Winning Probability\")\nplt.xlabel(\"Turnover and Foul\")\nplt.show()","execution_count":39,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"dedb9694d752e7ab9231f86029e4b34605054c1d"},"cell_type":"code","source":"# Steal and Block\nplt.plot(Test_df['stl_x']*Test_df['blk_x']-Test_df['stl_y']*Test_df['blk_y'], win_prob_1)\nplt.ylabel(\"Winning Probability\")\nplt.xlabel(\"Steal and Block\")\nplt.show()","execution_count":40,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":1}