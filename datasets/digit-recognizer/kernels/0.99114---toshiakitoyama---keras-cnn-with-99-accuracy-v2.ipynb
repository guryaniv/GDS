{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "b8f23803-f7c9-c890-22dd-eab6672e749c"
      },
      "source": [
        "This is a modified version of my Keras CNN for the MNIST dataset. I saw my score drop on the leaderboard and decided to tweak things a bit. Unfortunately, the LB seems to be dominated by suspicious 100% result nowadays, so this might not help ... \n",
        "\n",
        "Here goes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "19d8eb6b-9b71-029c-06e1-7e74975ae669"
      },
      "outputs": [],
      "source": [
        "import numpy as np # linear algebra\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "76adcf6f-d109-e47d-d8f0-c7a26e986df5"
      },
      "outputs": [],
      "source": [
        "from keras.utils import np_utils\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, BatchNormalization\n",
        "from keras.optimizers import Adam\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.callbacks import LearningRateScheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "249daad0-7a0d-ad43-2bf9-4861ffac0d90"
      },
      "outputs": [],
      "source": [
        "train_file = \"../input/train.csv\"\n",
        "test_file = \"../input/test.csv\"\n",
        "output_file = \"submission.csv\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "b3a8472f-d3a1-949d-3bb4-2935b53c4f17"
      },
      "source": [
        "## Load the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b8f5eb0f-6bd4-5484-0a89-d7f814678c80"
      },
      "outputs": [],
      "source": [
        "raw_data = np.loadtxt(train_file, skiprows=1, dtype='int', delimiter=',')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "95cd483d-23ae-fa98-3823-724e41ed4b5b"
      },
      "outputs": [],
      "source": [
        "x_train, x_val, y_train, y_val = train_test_split(\n",
        "    raw_data[:,1:], raw_data[:,0], test_size=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "fdc43c64-0655-e91d-0659-ceb97a6f3532"
      },
      "outputs": [],
      "source": [
        "x_train = x_train.astype(\"float32\")/255.0\n",
        "x_val = x_val.astype(\"float32\")/255.0\n",
        "y_train = np_utils.to_categorical(y_train)\n",
        "y_val = np_utils.to_categorical(y_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "1d2ceaed-52a4-581b-51af-c139e4913ee4"
      },
      "outputs": [],
      "source": [
        "n_train = x_train.shape[0]\n",
        "n_val = x_val.shape[0]\n",
        "x_train = x_train.reshape(n_train, 28, 28, 1)\n",
        "x_val = x_val.reshape(n_val, 28, 28, 1)\n",
        "n_classes = y_train.shape[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "82eb74e2-1f8d-02e9-cf9b-cbdd6c7d371d"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "363e8bc8-a731-01da-9a39-d3914ea7a5c8"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(filters = 16, kernel_size = (3, 3), activation='relu',\n",
        "                 input_shape = (28, 28, 1)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(filters = 16, kernel_size = (3, 3), activation='relu'))\n",
        "model.add(MaxPool2D(strides=(2,2)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Conv2D(filters = 32, kernel_size = (3, 3), activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(filters = 32, kernel_size = (3, 3), activation='relu'))\n",
        "model.add(MaxPool2D(strides=(2,2)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Dense(1024, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(10, activation='softmax'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "bb0658c9-59f1-5336-9e5d-abcd82010238"
      },
      "source": [
        "If you run this on a strong machine, over hundreds of epochs, augmentation will improve your performance. This means randomly perturbing the images to prevent overfitting, and Keras has a simple function for this. \n",
        "\n",
        "Here in the Kernel, we will only look at each image 4-5 times, so augmentation will only add a little "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c34d0682-2777-382d-ab1c-fa0186fc0ae7"
      },
      "outputs": [],
      "source": [
        "datagen = ImageDataGenerator(zoom_range = 0.1,\n",
        "                            height_shift_range = 0.1,\n",
        "                            width_shift_range = 0.1,\n",
        "                            rotation_range = 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "1e6d722a-80fb-48a7-8766-80e6d8c83711"
      },
      "source": [
        "These parameters here again chosen for the Kaggle kernel. For better performance, try reducing the learning rate and increase the number of epochs. I was able to reach 99.5% with a similar net on longer training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "86757f02-d7ff-cf2b-e567-7644a8983faa"
      },
      "outputs": [],
      "source": [
        "#Start with a slightly lower learning rate\n",
        "model.compile(loss='categorical_crossentropy', optimizer = Adam(lr=3e-5), metrics = [\"accuracy\"])\n",
        "\n",
        "hist = model.fit_generator(datagen.flow(x_train, y_train, batch_size = 16),\n",
        "                           steps_per_epoch = 1000, \n",
        "                           epochs = 1, verbose = 2,\n",
        "                           validation_data = (x_val[:400,:], y_val[:400,:]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "21ab56fa-c6fb-830a-47d3-3bc40c447ec9"
      },
      "source": [
        "We will use a small learning rate for the first epoch:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d0acbcf9-0be3-712f-222d-58e6670dac58"
      },
      "outputs": [],
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer = Adam(lr=3e-5), metrics = [\"accuracy\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "e87ab385-9290-9b87-3b78-716188a140cc"
      },
      "source": [
        "Then we speed things up, only to reduce it by 10% each epoch. This Keras function does all this for us:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "7f5eea4e-0ae6-0948-2c05-400b8bd854be"
      },
      "outputs": [],
      "source": [
        "annealer = LearningRateScheduler(lambda x: 1e-3 * 0.9**x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b8d960a3-09ed-0802-2b23-4ed583f7492b"
      },
      "outputs": [],
      "source": [
        "hist = model.fit_generator(datagen.flow(x_train, y_train, batch_size = 16),\n",
        "                           steps_per_epoch = 1000, #Increase this when not on Kaggle kernel\n",
        "                           epochs = 9, #Increase this when not on Kaggle kernel\n",
        "                           verbose = 2,  #verbose=1 outputs ETA, but doesn't work well in the cloud\n",
        "                           validation_data = (x_val[:400,:], y_val[:400,:]), #To evaluate faster\n",
        "                           callbacks = [annealer])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "9ceb04e7-b196-b2ca-ce2b-abcdc8870bcc"
      },
      "source": [
        "## Evaluate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "c64f7ec3-6af5-d3e6-e0e4-21f8e29d969c"
      },
      "source": [
        "We only used a subset of the validation set during training, to save time. Now let's check performance on the whole validation set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d3a2a555-ef78-1cce-3971-0ea4f7bb0c1d"
      },
      "outputs": [],
      "source": [
        "model.evaluate(x_val, y_val, verbose=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "49fd9696-826d-f95c-3fe6-94fe4d1f0a6c"
      },
      "outputs": [],
      "source": [
        "plt.plot(hist.history['loss'], color='b')\n",
        "plt.plot(hist.history['val_loss'], color='r')\n",
        "plt.show()\n",
        "plt.plot(hist.history['acc'], color='b')\n",
        "plt.plot(hist.history['val_acc'], color='r')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "0783efce-d6b9-6d98-3824-0b45260a0f23"
      },
      "outputs": [],
      "source": [
        "y_hat = model.predict(x_val)\n",
        "y_pred = np.argmax(y_hat, axis=1)\n",
        "y_true = np.argmax(y_val, axis=1)\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "print(cm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "5375f3d0-51fd-7199-4109-b32bcb4a4e63"
      },
      "source": [
        "Not too bad, considering the minimal amount of training so far. In fact, we have only gone through the training data approximately five times. With proper training we should get really good results, but then we might need to add more Dropout to prevent overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "0bd63110-f8f6-6f39-3225-c2f941184f3b"
      },
      "source": [
        "## Submit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "002ce4ad-9a8f-8a1e-c658-40f9c371f91d"
      },
      "outputs": [],
      "source": [
        "mnist_testset = np.loadtxt(test_file, skiprows=1, dtype='int', delimiter=',')\n",
        "x_test = mnist_testset.astype(\"float32\")/255.0\n",
        "n_test = x_test.shape[0]\n",
        "x_test = x_test.reshape(n_test, 28, 28, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "7413800d-21c3-654f-8b39-27e3df9c4391"
      },
      "outputs": [],
      "source": [
        "y_hat = model.predict(x_test, batch_size=64)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "074aabf5-4a9a-bfdb-cbdd-0a8f06dc5fae"
      },
      "source": [
        "y_test consists of class probabilities, I now select the class with highest probability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "575d9348-99ec-9b8c-f5a5-ceac44c5308d"
      },
      "outputs": [],
      "source": [
        "y_pred = np.argmax(y_hat,axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "cbf31eb1-2ffd-efb5-e2b2-b4dda99fa1da"
      },
      "outputs": [],
      "source": [
        "with open(output_file, 'w') as f :\n",
        "    f.write('ImageId,Label\\n')\n",
        "    for i in range(0, n_test) :\n",
        "        f.write(\"\".join([str(i+1),',',str(y_pred[i]),'\\n']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "580a57a8-c8b2-cb5b-9962-a743f50eb5a4"
      },
      "source": [
        "Submitting from this notebook usually gives slightly lower than 99%, but I achieved 99.3% by averaging over 5 good runs. And you can get higher than that if you train overnight."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "2a092a95-f80b-fc66-882b-9ee851ef6938"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}