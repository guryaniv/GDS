{"metadata": {"language_info": {"pygments_lexer": "ipython3", "mimetype": "text/x-python", "codemirror_mode": {"version": 3, "name": "ipython"}, "name": "python", "nbconvert_exporter": "python", "version": "3.6.3", "file_extension": ".py"}, "kernelspec": {"language": "python", "display_name": "Python 3", "name": "python3"}}, "cells": [{"metadata": {"_uuid": "3ae4f06c309793cb1b206300f9c0f56c5fd2e85c", "_cell_guid": "88b36a6b-6bb5-4cdd-8fd4-0f5c43bf1903"}, "source": ["# Overview\n", "The basic idea of this notebook is to transform the data from a sequence of letters into possible categories using a CNN. We use letters instead of words since we say in the [mothjer](https://www.kaggle.com/fcostartistican/don-t-mess-with-my-mothjer) notebook that words are often misspelled or written differently so looking at character level correlations might work better.  We utilize Atrous Convolutions since they can account for larger spacings between relevant words and ideas. For the model we focus on individual letters and ngrams sized 1-10, but the model could easily be expanded to handle larger differences."], "cell_type": "markdown"}, {"metadata": {"_uuid": "e41e62413248c1a87934d5c5e8bb0390b2481add", "collapsed": true, "_cell_guid": "d06b6ce7-d93b-424b-ace1-2464e13f90ae"}, "execution_count": null, "source": ["import numpy as np # linear algebra\n", "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n", "from keras.models import Model\n", "from keras.layers import Dense, Embedding, Input\n", "from keras.layers import Conv1D, MaxPooling1D, GlobalMaxPool1D, Dropout, concatenate\n", "from keras.preprocessing import text as keras_text, sequence as keras_seq\n", "from keras.callbacks import EarlyStopping, ModelCheckpoint"], "outputs": [], "cell_type": "code"}, {"metadata": {"_uuid": "f99670815ef29c169168b34a817b60783421e8d1", "collapsed": true, "_cell_guid": "71ec20e2-3f9b-49f8-81ea-479747ab3728"}, "execution_count": null, "source": ["# define network parameters\n", "max_features = 64\n", "maxlen = 512"], "outputs": [], "cell_type": "code"}, {"metadata": {"_uuid": "96d5f7c706b6a11fbc2a2530c97dfefb4faf209d", "_cell_guid": "6ddb5d07-801c-4c82-8fb4-24fd71567dc0"}, "source": ["# Load and Preprocessing Steps\n", "Here we load the data and fill in the misisng values"], "cell_type": "markdown"}, {"metadata": {"_uuid": "d6b8d61f1c16e0a7e1713d31428c5d01097a985c", "collapsed": true, "_cell_guid": "c8bbaffa-b53b-4840-b47f-2d5bb117fc53"}, "execution_count": null, "source": ["%%time\n", "train = pd.read_csv(\"../input/train.csv\")\n", "test = pd.read_csv(\"../input/test.csv\")\n", "train = train.sample(frac=1)\n", "\n", "list_sentences_train = train[\"comment_text\"].fillna(\"unknown\").values\n", "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n", "y = train[list_classes].values\n", "list_sentences_test = test[\"comment_text\"].fillna(\"unknown\").values"], "outputs": [], "cell_type": "code"}, {"metadata": {"_uuid": "ee41735cbf0933aec3590d0da6a8b6d59c7ea608", "_cell_guid": "ebe64376-9126-4f7f-b10d-388a8f3bc77d"}, "source": ["## Sequence Generation\n", "Here we take the data and generate sequences from the data"], "cell_type": "markdown"}, {"metadata": {"_uuid": "bfde767157b1af853bf66ae4a99ec551b486e7d8", "collapsed": true, "_cell_guid": "6c84aaf5-155d-475c-be20-d69fa2b7e8db"}, "execution_count": null, "source": ["tokenizer = keras_text.Tokenizer(char_level = True)\n", "tokenizer.fit_on_texts(list(list_sentences_train))\n", "# train data\n", "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n", "X_t = keras_seq.pad_sequences(list_tokenized_train, maxlen=maxlen)\n", "# test data\n", "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n", "X_te = keras_seq.pad_sequences(list_tokenized_test, maxlen=maxlen)"], "outputs": [], "cell_type": "code"}, {"metadata": {"_uuid": "467cc9819d7545bff9eb2a18dc51780d92408f8e", "collapsed": true, "_cell_guid": "9f1ff657-6129-4c00-9756-1915b6db7389"}, "execution_count": null, "source": ["def build_model(conv_layers = 2, \n", "                dilation_rates = [0, 2, 4, 8, 16], \n", "                embed_size = 256):\n", "    inp = Input(shape=(None, ))\n", "    x = Embedding(input_dim = len(tokenizer.word_counts)+1, \n", "                  output_dim = embed_size)(inp)\n", "    prefilt_x = Dropout(0.25)(x)\n", "    out_conv = []\n", "    # dilation rate lets us use ngrams and skip grams to process \n", "    for dilation_rate in dilation_rates:\n", "        x = prefilt_x\n", "        for i in range(2):\n", "            if dilation_rate>0:\n", "                x = Conv1D(16*2**(i), \n", "                           kernel_size = 3, \n", "                           dilation_rate = dilation_rate,\n", "                          activation = 'relu',\n", "                          name = 'ngram_{}_cnn_{}'.format(dilation_rate, i)\n", "                          )(x)\n", "            else:\n", "                x = Conv1D(16*2**(i), \n", "                           kernel_size = 1,\n", "                          activation = 'relu',\n", "                          name = 'word_fcl_{}'.format(i))(x)\n", "        out_conv += [Dropout(0.5)(GlobalMaxPool1D()(x))]\n", "    x = concatenate(out_conv, axis = -1)    \n", "    x = Dense(64, activation='relu')(x)\n", "    x = Dropout(0.1)(x)\n", "    x = Dense(32, activation='relu')(x)\n", "    x = Dropout(0.1)(x)\n", "    x = Dense(6, activation='sigmoid')(x)\n", "    model = Model(inputs=inp, outputs=x)\n", "    model.compile(loss='binary_crossentropy',\n", "                  optimizer='adam',\n", "                  metrics=['accuracy'])\n", "    return model\n", "\n", "model = build_model()\n", "model.summary()"], "outputs": [], "cell_type": "code"}, {"metadata": {"_uuid": "dcd322903096a49e1401bd1bf5fa073e9ecf0b11", "_cell_guid": "3052969a-a72b-49c2-a595-e75b8573c369"}, "source": ["# Train the Model\n", "Here we train the model and use model checkpointing and early stopping to keep only the best version of the model"], "cell_type": "markdown"}, {"metadata": {"_uuid": "a4e639ce318affd65cb2dfc141fd76ac60bccbbf", "_cell_guid": "11dd7653-7bda-4611-9eb1-beeb24f06cbd"}, "source": ["## Hold-out\n", "We create a hold-out group of data for having a set of data the model was never exposed to for testing it. We add all of the possible categories together as a cheap hack for ensuring groups are somewhat stratified."], "cell_type": "markdown"}, {"metadata": {"_uuid": "a7e9d58c9698de39f32e5c0600ea474bd9f5d1cb", "collapsed": true, "_cell_guid": "2d550f74-32c7-44e6-8398-8dfb4d9b762e"}, "execution_count": null, "source": ["from sklearn.model_selection import train_test_split\n", "any_category_positive = np.sum(y,1)\n", "print('Distribution of Total Positive Labels (important for validation)')\n", "print(pd.value_counts(any_category_positive))\n", "X_t_train, X_t_test, y_train, y_test = train_test_split(X_t, y, \n", "                                                        test_size = 0.2, \n", "                                                        stratify = any_category_positive,\n", "                                                       random_state = 2017)\n", "print('Training:', X_t_train.shape)\n", "print('Testing:', X_t_test.shape)"], "outputs": [], "cell_type": "code"}, {"metadata": {"_uuid": "2c60302e2756b9fb5137503e29db220ffce955bf", "collapsed": true, "_cell_guid": "7b5e74a1-d92e-4705-b478-606121443909"}, "execution_count": null, "source": ["batch_size = 128 # large enough that some other labels come in\n", "epochs = 1\n", "\n", "file_path=\"best_weights.h5\"\n", "checkpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n", "early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=20)\n", "\n", "callbacks_list = [checkpoint, early] #early\n", "model.fit(X_t_train, y_train, \n", "          validation_data=(X_t_test, y_test),\n", "          batch_size=batch_size, \n", "          epochs=epochs, \n", "          shuffle = True,\n", "          callbacks=callbacks_list)"], "outputs": [], "cell_type": "code"}, {"metadata": {"_uuid": "58de3d5b2443643c4548e5a6c6dc256b7c49ce49", "_cell_guid": "f6792563-9c96-4e2f-b9a9-9d5dd53b2665"}, "source": ["# Make Predictions\n", "Load the model and make predictions on the test dataset"], "cell_type": "markdown"}, {"metadata": {"_uuid": "92532dfaa0211e9b01d9ee3d78f86b3f1e570a3c", "collapsed": true, "_cell_guid": "a3442437-db15-4e64-b377-c8bd010e7378"}, "execution_count": null, "source": ["model.load_weights(file_path)\n", "y_test = model.predict(X_te)\n", "sample_submission = pd.read_csv(\"../input/sample_submission.csv\")\n", "sample_submission[list_classes] = y_test\n", "sample_submission.to_csv(\"predictions.csv\", index=False)"], "outputs": [], "cell_type": "code"}, {"metadata": {"_uuid": "1dbe8f9f111c55e385d917c591a6c27fcdcc1906", "collapsed": true, "_cell_guid": "90de75ae-3a9c-4b62-b165-ced65afbe463"}, "execution_count": null, "source": [], "outputs": [], "cell_type": "code"}], "nbformat": 4, "nbformat_minor": 1}