{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "4f68454a-c55b-bfe6-b273-9aad5896ebbb"
      },
      "source": [
        "This notebook presents several approaches followed to classify the data in the Leaf dataset. In addition a brief EDA is included.\n",
        "\n",
        "The data provided by Kaggle consists basically on two different types: Binary pictures of the leaf-samples and vectors of these leaf-samples containing pre-extracted features.\n",
        "\n",
        "In particular, the training set consists of N = 990 samples. For each sample, we have a binary image of the leaf of variable size and an d-dimensional feature vectors, where d = 192."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "a74b6ce5-497a-712a-40bd-cfa62fe899eb"
      },
      "source": [
        "#1. Feature vectors\n",
        "We will first work with the feature vectors of the leaf-samples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "605673ff-e4a4-03f0-e21c-1f336a71b29c"
      },
      "source": [
        "## 1.1 Prepare workspace\n",
        "Let us begin by loading some basic libraries and some of the data that we will be using throughout this lab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "6d07e5de-149c-4d8c-cc06-8ac1abd9c011"
      },
      "outputs": [],
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load in \n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import matplotlib.pyplot as plt # plot\n",
        "\n",
        "# Input data files are available in the \"../input/\" directory.\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
        "\n",
        "from subprocess import check_output\n",
        "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n",
        "\n",
        "# Any results you write to the current directory are saved as output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "82da7fc6-c128-8849-36f8-8307fe24b56b"
      },
      "outputs": [],
      "source": [
        "# Load feature vectors\n",
        "df = pd.read_csv(\"../input/train.csv\")\n",
        "# Display some of the feature vectors\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "b2215a07-2553-9fca-b926-eb73d43fe8c6"
      },
      "source": [
        "## 1.2 Exploratory Data Analysis\n",
        "First of all, let us have a brief look at the feature vectors. In this regard, we will use simple approaches such as PCA and t-SNE. Note that there are several classes, which makes the visualisation a bit difficult."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "2c7d445e-8c5b-2a8d-08bf-719525a9085a"
      },
      "source": [
        "### 1.2.1 PCA\n",
        "First, we will use Principal Component Analysis to obtain an idea of how is our data distributed. PCA basically consists on finding the dimensions that retain the maximum variance of the data. For it to be easily visualized, we usually choose to find 2 dimensions. Thus, we can say that we are performing dimensionality reduction (we go from the original 192 dimensions to just 2)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "05fce6d2-074f-662e-eee4-c56fda7b4f49"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=3)\n",
        "\n",
        "# Obtain list of features\n",
        "features = list(df)\n",
        "del(features[:2]) # Delete id and species from features\n",
        "\n",
        "# Apply PCA to our data\n",
        "pca_result = pca.fit_transform(df[features].values)\n",
        "\n",
        "# Define dataframe containing low-dimension representation of original data\n",
        "pca_df = pd.DataFrame()\n",
        "pca_df['species'] = df['species']\n",
        "pca_df['pca-one'] = pca_result[:,0]\n",
        "pca_df['pca-two'] = pca_result[:,1] \n",
        "\n",
        "# How much variance does this new representation retain from the original data?\n",
        "print('Explained variation per principal component: {}'.format(pca.explained_variance_ratio_))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "b1a8c21e-3141-5d75-8ad2-42fea6dc73f0"
      },
      "source": [
        "From the previous results, we observe that the two principal components retain roughly a 25% of the total variance of the data. It looks like they are good representatives!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "e28df08f-9270-6d08-e1be-b85fdfb4e6aa"
      },
      "outputs": [],
      "source": [
        "from ggplot import *\n",
        "chart = ggplot( pca_df, aes(x='pca-one', y='pca-two', color='species') ) \\\n",
        "        + geom_point(size=75,alpha=0.8) \\\n",
        "        + ggtitle(\"First and Second Principal Components colored by species\")\n",
        "chart"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "d95e10f1-9412-dfd6-2338-ffe5d42ba547"
      },
      "source": [
        "### 1.2.2 t-SNE\n",
        "After working with PCA, let us dive into t-SNE. The key difference to PCA is that it follows a probabilistic approach rather than a mathematical one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "415d22db-ca00-0cf0-f6fd-89fb59da7cb1"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# Start t-SNE algorithm\n",
        "time_start = time.time()\n",
        "tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=400)\n",
        "tsne_results = tsne.fit_transform(df[features].values)\n",
        "\n",
        "print('t-SNE done! Time elapsed: {} seconds'.format(time.time()-time_start))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "bf9abab0-3f2a-471a-8970-cd326137cefb"
      },
      "outputs": [],
      "source": [
        "# Create dataframe for the new representation of the data\n",
        "df_tsne = pd.DataFrame()\n",
        "df_tsne['species'] = df['species']\n",
        "df_tsne['x-tsne'] = tsne_results[:,0]\n",
        "df_tsne['y-tsne'] = tsne_results[:,1]\n",
        "\n",
        "# Create chart and display it\n",
        "chart = ggplot( df_tsne, aes(x='x-tsne', y='y-tsne', color='species') ) \\\n",
        "        + geom_point(size=70,alpha=0.1) \\\n",
        "        + ggtitle(\"tSNE dimensions colored by species\")\n",
        "chart"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "b6c72990-7cf0-96b8-c289-f3aa0426e1da"
      },
      "source": [
        "A common approach prior to perform t-SNE is to obtain a low-dimensionality representation of the data using PCA (say with 50 principal components) and then apply t-SNE on top of this. In this regard, let us apply PCA on our original data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "a47cce38-8fe3-4f68-ab72-cbc96d1b32f5"
      },
      "outputs": [],
      "source": [
        "pca_50 = PCA(n_components=50)\n",
        "pca_result_50 = pca_50.fit_transform(df[features].values)\n",
        "\n",
        "print('Explained variation per principal component (PCA): {}'.format(np.sum(pca_50.explained_variance_ratio_)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "31f98912-cd10-70bb-8c9d-525209c92765"
      },
      "source": [
        "We observer that more than 95% of the variance is retained in 50 dimensions! Let's apply t-SNE!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "dddcbbb6-fd61-ffcc-5cb1-f34d4427d13c"
      },
      "outputs": [],
      "source": [
        "time_start = time.time()\n",
        "\n",
        "tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300)\n",
        "tsne_pca_results = tsne.fit_transform(pca_result_50)\n",
        "\n",
        "print('t-SNE done! Time elapsed: {} seconds'.format(time.time()-time_start))\n",
        "\n",
        "# Create dataframe for the new representation of the data\n",
        "df_tsne = pd.DataFrame()\n",
        "df_tsne['species'] = df['species']\n",
        "df_tsne['x-tsne'] = tsne_results[:,0]\n",
        "df_tsne['y-tsne'] = tsne_results[:,1]\n",
        "\n",
        "# Create chart and display it\n",
        "chart = ggplot( df_tsne, aes(x='x-tsne', y='y-tsne', color='species') ) \\\n",
        "        + geom_point(size=70,alpha=0.1) \\\n",
        "        + ggtitle(\"tSNE dimensions colored by species\")\n",
        "chart"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "15fb0e52-64cd-26bf-c605-350be6a21d01"
      },
      "source": [
        "## 1.3 Feed Forward Neural Network\n",
        "Now, let us proceed by applying some fancy Neural Network to this dataset. A Feed Forward Neural Network is a Neural Network that is built by _layers_. Each layer consists of a set of units (often refered to as _neurons_). Particularity of these networks is that units within the same layer do not interact between each other. The following figure illustrates the architecture of these networks\n",
        "\n",
        "![](http://cse22-iiith.vlabs.ac.in/exp4/images/structure.png)\n",
        "\n",
        "The input layer is where we introduce the training data. In this first attempt, we will use the feature vectors. Thus, the input layer has 192 units. In Classification problems, the _output layer_ has as many units as classes we have in our problem (in our case, we have 99). In between, we have the _hidden layers_ (the number of hidden layers might vary from task to task). \n",
        "\n",
        "In general, at each layer these networks compute linear combinations of the output vector of the previous layer and subsequently apply non-linearities (also known as activation functions). In our case, we will be using the ReLu activation function, which has been proven in the literature to be robust to the vanishing gradient problem and has a faster convergence than other standard activation functions (e.g. sigmoid, tanh).\n",
        "\n",
        "In the following, we initialise the model parameters and define the network architecture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ac7f3c17-14fc-48b0-f0da-a4df4c66f59f"
      },
      "outputs": [],
      "source": [
        "#\u00a0Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# Parameters\n",
        "learning_rate = 0.0001\n",
        "training_epochs = 200\n",
        "batch_size = 10\n",
        "display_step = 10\n",
        "\n",
        "# Network Parameters\n",
        "n_features = 192\n",
        "n_classes = 99\n",
        "neurons_layers = [n_features, 256, 512, 256, n_classes] # Number of features in input, hidden and output layers\n",
        "\n",
        "# Load Train data\n",
        "train_data = {}\n",
        "df = pd.read_csv(\"../input/train.csv\") # Load training set\n",
        "tmp = df.as_matrix()\n",
        "train_data['samples'] = tmp[:, 2:] # Obtain all the feature vectors (without ids and species)\n",
        "train_data['labels'] = tmp[:, 1] # Obtain labels (i.e. species)\n",
        "N_train = len(train_data['samples']) # Number of training samples\n",
        "\n",
        "# Obtain list of features\n",
        "features = list(df)\n",
        "del(features[:2]) # Delete id and species from features\n",
        "\n",
        "# Load Test data\n",
        "test_data = {}\n",
        "df = pd.read_csv(\"../input/test.csv\")\n",
        "tmp = df.as_matrix()\n",
        "test_data['samples'] = tmp[:, 1:]\n",
        "test_ids = df.pop('id')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "068ff5e7-8538-39da-e73c-616c2b07d414"
      },
      "source": [
        "The label of the samples comes as string format. However, we would prefer to have it in numerical values. Even more, we would like to use **one-hot encoding**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "5a0a6764-633e-f15e-36b2-f12272fb0458"
      },
      "outputs": [],
      "source": [
        "# One hot encoding map\n",
        "enc = np.eye(n_classes)\n",
        "sparse2dense = {i: enc[i] for i in range(n_classes)}\n",
        "\n",
        "# Map class names to one-hot representation\n",
        "class_names = np.unique(train_data['labels'])\n",
        "class_encodings = {}\n",
        "for i in range(n_classes):\n",
        "    class_encodings[class_names[i]] = sparse2dense[i]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "f4edd885-50cf-e8aa-b7f5-345f5667d3e6"
      },
      "source": [
        "To evaluate how good our model is performing, we would like to have a validation set we can run our model on. Only assessing the performance of our model on the training set could easily lead to overfitting. A first approach is to take a fraction of the training set to train the model and the remaining as the validation set. However, we do not have much data and hence should be thinking of another approach. \n",
        "\n",
        "Another option is to use k-fold Cross Validation. This technique is based on splitting the training set in _k_ subsets and pick _1_ as the validation set and the remaining as training set.  This is done for all subsets. The following picture illustrates this idea.\n",
        "\n",
        "![](https://static.oschina.net/uploads/img/201609/26155106_OfXx.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "80c836d7-282e-c32f-f231-1d66c622ed4f"
      },
      "source": [
        "To assess the performance of our model, typically a validation set is used. Evaluating how good our model is on the training set can easily lead to overfitting. However, in our scenario the training data is to scarce. Hence, another approach is considered: **k-fold Cross Validation**. This technique is based on splitting the training set in _k_ subsets, where one of them is used as the validation set and the rest are merged and used to train the model. This procedure is ran several times iterating the fold assigned to the test set. The following picture illustrates this idea.\n",
        "\n",
        "![](https://static.oschina.net/uploads/img/201609/26155106_OfXx.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "dc154362-6ea7-1f94-2ca0-1fd7bc74a9e7"
      },
      "outputs": [],
      "source": [
        "# Create Validation and Training sets\n",
        "tmp = pd.read_csv(\"../input/train.csv\").as_matrix()\n",
        "\n",
        "idx = np.random.choice(N_train, N_train, replace=False)\n",
        "N_train = int(0.75*N_train)\n",
        "train_data['samples'] = tmp[idx[:N_train], 2:]\n",
        "train_data['labels'] = np.array([class_encodings[t] for t in tmp[idx[:N_train], 1]])\n",
        "\n",
        "val_data = {}\n",
        "val_data['samples'] = tmp[idx[N_train:], 2:]\n",
        "val_data['labels'] = np.array([class_encodings[t] for t in tmp[idx[N_train:], 1]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "915d7102-baa0-1ee8-c5ad-f863606e9b1d"
      },
      "source": [
        "We define our first Tensorflow variables. In particular we define the placeholders for the input and output vectors, with their corresponding sizes.\n",
        "\n",
        "Furthermore, we also initialise the model parameters, i.e. the weight matrices and bias vectors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "1da4cde8-35b0-25d6-6931-64ed070ba8bf"
      },
      "outputs": [],
      "source": [
        "# tf Graph input\n",
        "x = tf.placeholder(tf.float32, [None, n_features])\n",
        "y = tf.placeholder(tf.float32, [None, n_classes])\n",
        "\n",
        "# Model parameters (weights, biases)\n",
        "weights = [\n",
        "    tf.Variable(tf.random_normal([neurons_layers[k], neurons_layers[k + 1]])) for k in range(len(neurons_layers)-1)\n",
        "]\n",
        "\n",
        "biases = [\n",
        "    tf.Variable(tf.random_normal([neurons_layers[k+1]])) for k in range(len(neurons_layers)-1)\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "75963312-bfe8-12aa-7a14-be1adc5367d5"
      },
      "source": [
        "Now, we define a method that is responsible for building a fully connected feed forward NN with Relu activations given a predefined model parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "aaa243ad-f41f-0189-9aac-2b14263c2a75"
      },
      "outputs": [],
      "source": [
        "# Function implementing a fully connected feed forward NN, with relu activations\n",
        "def multilayer_perceptron(x, weights, biases):\n",
        "    \n",
        "    #\u00a0Compute number of hidden layers\n",
        "    n_hidden = len(weights)-1\n",
        "    \n",
        "    # Input layer to hidden layer\n",
        "    out = tf.add(tf.matmul(x, weights[0]), biases[0])\n",
        "    \n",
        "    # Check that there are hidden layers\n",
        "    if n_hidden > 0:\n",
        "    \n",
        "        # Iterate over all hidden layers\n",
        "        for k in range(1, n_hidden+1):\n",
        "            out = tf.nn.relu(out) # Apply activation fct on previous layer output\n",
        "            out = tf.add(tf.matmul(out, weights[k]), biases[k]) # linear combination\n",
        "               \n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "cba76e24-ace3-67ee-e84b-2e141ba50173"
      },
      "outputs": [],
      "source": [
        "for k in range(1,1):\n",
        "   print(k)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "81d6d329-2081-7252-659a-aed112382ee6"
      },
      "source": [
        "Last step prior to the training is to define the **forward** and **backward pass** of the network. For the forward pass we will use the method `multilayer_perceptron` and for the backward pass we will use the softmax-cross entropy loss and the Adam optimiser."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "cd9000dd-c8fe-42ea-9423-110ababbe657"
      },
      "outputs": [],
      "source": [
        "# Construct model\n",
        "y_pred = multilayer_perceptron(x, weights, biases)\n",
        "p_pred = tf.nn.softmax(y_pred)\n",
        "\n",
        "# Define cross entropy loss\n",
        "cost = tf.nn.softmax_cross_entropy_with_logits(logits=y_pred, labels=y)\n",
        "cost = tf.reduce_mean(cost)*batch_size\n",
        "\n",
        "# Define optimizer\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
        "\n",
        "# Evaluation of the model on a set\n",
        "correct_prediction = tf.equal(tf.argmax(y_pred, 1), tf.argmax(y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "35d6b896-c2e8-324d-5cd6-998d4efd32b8"
      },
      "outputs": [],
      "source": [
        "# Arrays containing cost for each epoch\n",
        "cost_train = np.zeros([training_epochs,1])\n",
        "cost_val = np.zeros([training_epochs,1])\n",
        "\n",
        "# Initializing the variables\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "\n",
        "# Launch the graph\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "\n",
        "    # Training cycle\n",
        "    for epoch in range(training_epochs):\n",
        "        avg_cost = 0.\n",
        "        total_batch = int(N_train/batch_size)\n",
        "        # Loop over all batches\n",
        "        for i in range(total_batch):\n",
        "            idx = range(i*batch_size, (i+1)*batch_size)\n",
        "            # Train samples\n",
        "            batch_x = train_data['samples'][idx]\n",
        "            #\u00a0Train labels\n",
        "            #print(idx)\n",
        "            batch_y = train_data['labels'][idx]\n",
        "\n",
        "            ## Run optimization op (backprop) and cost op (to get loss value)\n",
        "            _, c = sess.run([optimizer, cost], feed_dict={x: batch_x,\n",
        "                                                          y: batch_y})\n",
        "            # Compute average loss\n",
        "            avg_cost += c / total_batch\n",
        "            \n",
        "        # Display logs per epoch step\n",
        "        if epoch % display_step == 0:\n",
        "            print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \\\n",
        "                \"{:.9f}\".format(avg_cost))\n",
        "            print(\"Accuracy:\", accuracy.eval({x: val_data['samples'], y: val_data['labels']}))\n",
        "        cost_train[epoch] = avg_cost\n",
        "        cost_val[epoch] = cost.eval({x: val_data['samples'], y: val_data['labels']})\n",
        "        \n",
        "    print(\"Optimization Finished!\")\n",
        "\n",
        "    \n",
        "    \n",
        "    p = sess.run([p_pred], {x: test_data['samples']})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c3e12f91-b786-d284-a90b-06e1ddfc64cd"
      },
      "outputs": [],
      "source": [
        "val_data['samples'].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "a0d4362f-377b-e892-d38c-b1e9207d8a63"
      },
      "source": [
        "After the training, it is a good practice to visualise how the cost curve evolved as the number of epochs increased"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "61dbfaae-d20d-5ced-63a7-2e139be738ea"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "loss_train_curve, = plt.plot(cost_train, label='Training Set')\n",
        "loss_val_curve, = plt.plot(cost_val, label='Validation Set')\n",
        "plt.legend(handles=[loss_train_curve, loss_val_curve])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "7e440944-5021-3a69-5109-cbf12bd6f5ac"
      },
      "outputs": [],
      "source": [
        "# prepare csv for submission\n",
        "submission = pd.DataFrame(p[0], index=test_ids, columns=class_names)\n",
        "submission.to_csv('submission.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "9c9f550f-2b8d-5258-96d4-2aa8e6664e85"
      },
      "source": [
        "## ConvNets\n",
        "\n",
        "This part is still to do"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c59073b2-ccb9-4d93-2bf2-6e8abba5640a"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import glob\n",
        "image_list = []\n",
        "for filename in glob.glob('../input/images/*.jpg'): #assuming gif\n",
        "    im=Image.open(filename)\n",
        "    image_list.append(im)"
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}