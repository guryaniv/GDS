{"metadata": {"language_info": {"nbconvert_exporter": "python", "version": "3.6.3", "codemirror_mode": {"name": "ipython", "version": 3}, "mimetype": "text/x-python", "pygments_lexer": "ipython3", "file_extension": ".py", "name": "python"}, "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}}, "nbformat": 4, "cells": [{"metadata": {"_cell_guid": "9e5bbde9-c23a-476f-9bfe-38461f6fd504", "_uuid": "737885ce5287fc7e6e403ce2a0c7a415228c4bc9"}, "cell_type": "markdown", "source": ["Hi all,\n", "\n", "I have seen some kernels here that try to estimate the median of the future time series by means of taking the median of the historical data into account. Some kernels in previous competions like [webtraffic prediction](https://www.kaggle.com/c/web-traffic-time-series-forecasting) were also having acceptable results by this approach. But, there, the goal was to minimze an MAE based error function, while here, we should minimze an MSE based error function.\n", "\n", "So, I have a theory. I think for MAE based error functions median estimation is the right approach but for MSE based functions mean estimation is a better approach.\n", "\n", "Here I show it with one numerical simulation and then prove it analytically in the next section.\n", "\n", "so, let's make a signal with some slow and fast oscillations with some trend and randomness:\n", "\n"]}, {"metadata": {"collapsed": true, "_cell_guid": "fa5bf279-26b6-4f54-a316-06897141634b", "_uuid": "820ac240ce7b5ffae58f8863322943a02bcf18b4"}, "cell_type": "code", "source": ["import pandas as pd\n", "import pylab as pl\n", "import seaborn as sns\n", "from scipy.stats import mode\n", "\n", "def analyzer(n, Seed, Res, Plot):\n", "    pl.seed(Seed)\n", "    x = pl.arange(n)\n", "    y = pl.zeros(n)  # let's make up our signal\n", "    y += pl.sin(pl.pi * x / 100)  # slow oscillation\n", "    y += pl.sin(pl.pi * x / 5)  # fast oscillation\n", "    y += .01*x  # some trend\n", "    y += .5*pl.cumsum(pl.randn(n))  # some randomness by randomwalk\n", "    y += pl.exp(pl.randn(n))*pl.randn(n)  # some outliers\n", "\n", "    Avg = pl.mean(y)\n", "    Med = pl.median(y)\n", "    bins = pl.linspace(y.min(), y.max(), 20)\n", "    m = mode(pl.digitize(y, bins))\n", "    Mod = bins[m[0]]-(bins[1]-bins[0])/2\n", "\n", "    Res = Res.append(pd.DataFrame([[Seed,'Avg',((y-Avg)**2).mean(),abs(y-Avg).mean()]], columns=ResCols))\n", "    Res = Res.append(pd.DataFrame([[Seed,'Med',((y-Med)**2).mean(),abs(y-Med).mean()]], columns=ResCols))\n", "    Res = Res.append(pd.DataFrame([[Seed,'Mod',((y-Mod)**2).mean(),abs(y-Mod).mean()]], columns=ResCols))\n", "\n", "    if Plot:\n", "        pl.figure(figsize=(10,10))\n", "        pl.subplot(2,1,1)\n", "        pl.hist(y)\n", "        pl.xlabel('y values')\n", "        pl.ylabel('histogram')\n", "\n", "        pl.subplot(2,1,2)\n", "        pl.plot(x, y)\n", "        pl.plot(x[[0,-1]], [Avg, Avg])\n", "        pl.plot(x[[0,-1]], [Med, Med])\n", "        pl.plot(x[[0,-1]], [Mod, Mod])\n", "        pl.legend(('data','Mean','Median','Mode'))\n", "        pl.xlabel('samples')\n", "        pl.ylabel('y values')\n", "\n", "    return Res\n", "\n", "\n", "n = 500\n", "Seed = 1\n", "ResCols = ('Seed','Estimate','MSE','MAE')\n", "Res = pd.DataFrame([],columns=ResCols)\n", "Res = analyzer(n, Seed, Res, Plot=True)\n", "print(Res)\n"], "execution_count": null, "outputs": []}, {"metadata": {"_cell_guid": "0067d3fe-e772-455d-bdcd-9e5d273eca74", "_uuid": "349ea2598d2e6b8a53d65f465a2a07ca1f0f509a"}, "cell_type": "markdown", "source": ["The table shows for MSE average is a better estimate and for MAE median is doing better, and mode is not working good in either case.\n", "\n", "Let's run some statistics:"]}, {"metadata": {"collapsed": true, "_cell_guid": "6082fa00-93ca-4875-8c05-2e099bcf2cfd", "_uuid": "7160d007a9a44b0d9c07ff8a33d0296521fe4ee2"}, "cell_type": "code", "source": ["Res = pd.DataFrame([],columns=ResCols)\n", "for Seed in range(100):\n", "    Res = analyzer(n, Seed, Res, Plot=False)\n", "\n", "Res2=Res.pivot(columns='Estimate',index='Seed')\n", "print(all(Res2[('MAE','Med')]<=Res2[('MAE','Avg')]))\n", "print(all(Res2[('MSE','Med')]>=Res2[('MSE','Avg')]))\n"], "execution_count": null, "outputs": []}, {"metadata": {"_cell_guid": "c9c679c6-ea73-4feb-b994-05a0e05fd6ae", "_uuid": "3ecb13254ba999b8a74999d4e386063fbac4afb5"}, "cell_type": "markdown", "source": ["so, it's clear that for MSE mean estimate works the best and for MAE median is the best estimate.\n", "\n", "And of course, the difference between the median and mean depends on the distribution of the values of our time series.\n", "\n", "Here are some figures:"]}, {"metadata": {"collapsed": true, "_cell_guid": "2ef03de8-8b56-43d5-b190-8e27327532bf", "_uuid": "bf4cc9a175cf82291d8a3292a96d04c220adad03"}, "cell_type": "code", "source": ["pl.figure(figsize=(10,10))\n", "ax=pl.subplot(2,2,1)\n", "sns.violinplot(x='Estimate', y='MSE', data=Res, ax=ax)\n", "sns.pointplot(x='Estimate', y='MSE', data=Res, ax=ax, color='y', markers=\".\")\n", "\n", "ax=pl.subplot(2,2,2)\n", "sns.violinplot(x='Estimate', y='MAE', data=Res, ax=ax)\n", "sns.pointplot(x='Estimate', y='MAE', data=Res, ax=ax, color='y', markers=\".\")\n", "\n", "pl.subplot(2,2,3)\n", "pl.plot(Res2[('MSE','Avg')],Res2[('MSE','Med')], '.')\n", "pl.plot([0,Res.MSE.max()],[0,Res.MSE.max()],'r')\n", "pl.xlabel('Median')\n", "pl.ylabel('Average')\n", "pl.title('MSE')\n", "\n", "pl.subplot(2,2,4)\n", "pl.plot(Res2[('MAE','Avg')],Res2[('MAE','Med')], '.')\n", "pl.plot([0,Res.MAE.max()],[0,Res.MAE.max()],'r')\n", "pl.xlabel('Median')\n", "pl.ylabel('Average')\n", "pl.title('MAE')\n"], "execution_count": null, "outputs": []}, {"metadata": {"_cell_guid": "645fa143-66e4-4588-939c-eebdb8c494a3", "_uuid": "5cc6c77f1b50c486605bc4ceeb378649bd439910"}, "cell_type": "markdown", "source": ["The figures show slight differences in the errors for median and mean estimations, and as I said it depends on the distribution of the signal.\n", "\n", "and here is the proof that I promissed:\n", "\n", "We are going to determine what is the best estimate of the signal, if we take some error function.Let's first do it for MSE.\n", "\n", "$MSE = \\frac{1}{n}\\sum_{i=1}^n {{\\big(y_i - m\\big)}^2} $\n", "\n", "by expanding this formula we get:\n", "\n", "$MSE = \\frac{\\sum_{i=1}^n y_i^2}{n} + \\frac{nm^2}{n} + -2m\\frac{\\sum_{i=1}^n y_i}{n} $\n", "\n", "and in fact it is a function of m that we are going to minimize with respect to m i.e. find the best m that has the lowest MSE:\n", "\n", "$MSE(m) = \\mathbb{E}[y^2] + m^2 + -2mAvg $\n", "\n", "the extremum values of this function are where the derivative is zero:\n", "\n", "$\\frac{\\mathrm d}{\\mathrm d m}  MSE  = 2m-2Avg $ \n", "\n", "so clearly the extremum is where m is the average!\n", "\n", "Let's do the same thing for MAE:\n", "\n", "$MAE = \\frac{1}{n} \\sum_{i=1}^n {\\big|y_i - m\\big|} $\n", "\n", "It's derivative is:\n", "\n", "$\\frac{\\mathrm d}{\\mathrm d m}  MAE  = \\frac{1}{n} \\Big( \\frac{y_1-m}{|y_1-m|} + \\frac{y_2-m}{|y_2-m|} + ...+ \\frac{y_n-m}{|y_n-m|} \\Big) $\n", "\n", "and these  $\\frac{y_i-m}{|y_i-m|}$  terms are +1s and -1s. Therefore, this summation is only minimum if we have the same number of +1s and -1s, which implies m should be the median of this series!"]}], "nbformat_minor": 1}