{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\n# numpy and pandas for data manipulation\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# sklearn preprocessing for dealing with categorical variables\nfrom sklearn.preprocessing import LabelEncoder\n\n# File system management\nimport os\n\n# Suppress warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# matplotlib and seaborn for plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# List files available\nprint(os.listdir(\"../input/\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"77e49e69db7e0f8732c7bb316f06ded470c550fb"},"cell_type":"code","source":"# Training data\napp_train = pd.read_csv('../input/application_train.csv')\nprint(\"Training data shape: \", app_train.shape)\napp_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d493ba5a0648f7fdc7a124d26c0c0fb67fc4879b"},"cell_type":"code","source":"app_train.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d8032852a83719aaec86f7df2d642ef5c03d59ed"},"cell_type":"code","source":"# Test data\napp_test = pd.read_csv('../input/application_test.csv')\nprint(\"Test data shape: \", app_test.shape)\napp_test.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"82a108dd1d4b4c71e3b5887a228564ea8160f85d"},"cell_type":"markdown","source":"# Exploratory Data Analysis (EDA)\nEDA is an open-ended process where we calculate statistics and anomalies to discover what data can tell us. For example, the author looks at the \"TARGET\" column to discover that we have an imbalance class problem. I will need to understand exactly what it is and why it is a problem by reading the article he provides."},{"metadata":{"trusted":true,"_uuid":"255cdc6cc889828a0e4fe510051fb7c12aef96b4"},"cell_type":"code","source":"# this just count how many observations in the column\napp_train['TARGET'].count()\n# this counts how many observation of each type in the column\napp_train['TARGET'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0ac403cbd774e9da2b9a03f543b9c825e671eafb","scrolled":true},"cell_type":"code","source":"# plot the histogram of the \"TARGET\" column\napp_train['TARGET'].astype(int).plot.hist()\nplt.show()\n# diagram shows an imblance class problem\n# Focus 1: Implement several current research on how to deal with this type of problem","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"7cccc8733df6482fd84f7700b7c82fadc9ae0e9f"},"cell_type":"markdown","source":"# Checkpoint 1: Write a function that returns summary of missing values\nSave this code for my future reference"},{"metadata":{"trusted":true,"_uuid":"ca0a50162b3068fe8a89deef9e93634b7cc6215b"},"cell_type":"code","source":"def missing_values_table(df):\n    \"\"\"\n    missing_values_table counts the total number of missing values and their percentages\n    in each column.\n    Inputs:\n    - df--pandas dataframe: a data frame of which its missing values are under investigation.\n    \n    Outputs:\n    - missing_values_tab--pandas dataframe: table of total number of missing values together \n    with their percentages in each column and some printout details.\n    \n    Acknowledgement: Based on Will Koehrsen's Kaggle kernel with slight simplifications.\n    \"\"\"\n    # Count the number of missing values in each column\n    missing_value_counts = df.isnull().sum() # pandas series\n    \n    # Find the percentages of missing values within each column\n    missing_value_percentage = (100 * missing_value_counts/len(df)).round(1) # pandas series\n    \n    # Make a table of missing values with keys to acces. \n    # pd.concat helps concatenate two pandas series.\n    missing_values_tab = pd.concat([missing_value_counts, missing_value_percentage], \n                                   axis = 1, keys=['Missing Values','% of Total Values'])\n    \n    # Sort the table by percentage of missing descending. There are tons of methods.\n    # One method is sort_values(inplace=True). Here, try a new method just learned\n    missing_values_tab = missing_values_tab[missing_values_tab.iloc[:,1] != 0].sort_values(\n                         '% of Total Values', ascending=False)\n    \n    # Print some summary information\n    print(\"Your selected data frame has %d columns.\" %(df.shape[1]))\n    print(\"There are %d columns that have missing values.\" %(missing_values_tab.shape[0]))\n    \n    return missing_values_tab","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"487cd4e250715aa48ad85a97212c40fec55b1376","scrolled":false},"cell_type":"code","source":"missing_values = missing_values_table(app_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e793558fe478200d40a8251968752dc1808d4e37"},"cell_type":"markdown","source":"# Encoding Categorical Variables"},{"metadata":{"trusted":true,"_uuid":"1d79fa907185199838e75c5f382b19e3ede700ca"},"cell_type":"code","source":"# Number of each type of column\napp_train.dtypes.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"94b409bde5b09894d4e09c695b2e775d112131e7"},"cell_type":"code","source":"# Number of unique classes in each object.\n# Note that the apply method takes in a function. Also it ignores NaN values!\n# select_dtypes select the data of the type we want. Here, type object\napp_train.select_dtypes('object').apply(pd.Series.nunique, axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4c185281e4fbc015a2e84d74873ec17eefab0abf"},"cell_type":"code","source":"# One-hot encoding: this method encodes hot for a specific type and 0 for the others\n# It is the safest approach when dealing with categorical values since it doesn't\n# impose arbitrary values to categories.\n# Problem: add extra dimensions to the data set which makes our models be exposed to noise.\n# To avoid: can use PCA and other dimensionality reduction methods\n# For now we will use Label Encoding for categorical variables with only 2 categories and\n# One-Hot Encoding for other categorical variables.\nlb = LabelEncoder()\nlb_count = 0\nlb_name = []\n# Iterate through the columns\nfor col in app_train:\n    if app_train[col].dtype == 'object':\n        # if 2 or fewer unique categories\n        if len(list(app_train[col].unique())) <= 2:\n            # train on the training data\n            lb.fit(app_train[col]) # use entries in the colummn to define appropriate labels\n            # transform both training and test data\n            app_train[col] = lb.transform(app_train[col])\n            app_test[col] = lb.transform(app_test[col])\n            \n            # keep track of how many columns were label encoded\n            lb_count += 1\n            lb_name.append(col)\n\nprint(\"Number of columns were label-encoded:\", lb_count)\nprint(\"Name of columns were label-encoded:\", lb_name)\n\n# Note that the data set has been modified, so if we rerun this cell, it will show 0.\n# Note also that although EMERGENCYSTATE_MODE above shows 2 types of value, it actually has 3 types of values. The value that\n# was not shown was NaN and was ignored in when we called the apply method.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c27b35f9f8d7c0ad47d2cc935239e25469a41833"},"cell_type":"code","source":"# Implement one-hot encoding to our training data. Note that NaN values are still ignored\ndim_before = len(app_train.columns)\nprint(\"Number of columns of training data BEFORE one-hot encoding:\", dim_before)\n\napp_train = pd.get_dummies(app_train)\napp_test = pd.get_dummies(app_test) # do the same thing for app_test\n\ndim_after = len(app_train.columns)\nprint(\"Number of columns of training data AFTER one-hot encoding:\", dim_after)\n\nprint(\"\\nNumber of columns of test data after one-hot encoding: \", len(app_test.columns))\n\n# There is a mismatch in the number of dimensions of training data vs test data. Check which one.\nfor col in app_train.columns:\n    if col not in app_test.columns:\n        print(col)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c2c7cfb06d9fcef2bfb870350415f06f1d692e1d"},"cell_type":"code","source":"# Aligning training and test data process. Very very cool!\n\n# Extract target variable\ntrain_labels = app_train['TARGET']\n# Align the training and test data. Don't forget to set axis = 1 to align columns only\napp_train, app_test = app_train.align(app_test, axis = 1, join = 'inner')\n# Put the target back in\napp_train['TARGET'] = train_labels\n\nprint('Shape of training data:', app_train.shape)\nprint('Shape of test data:', app_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ab43e95c9fdd36d3de4401a62d3fe5c66250de35"},"cell_type":"markdown","source":"# Handle Anomalies\nWe want to identify anomalies by looking at each column"},{"metadata":{"trusted":true,"_uuid":"cfb75f1f3c20d5ce704d8915a36043c9a8c7e950"},"cell_type":"code","source":"# Inspect DAYS_BIRTH anomalies in training data. This looks normal\n(app_train['DAYS_BIRTH'] / -365).describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"35ef409f3544d712198eaccc71451e2c39fb7403"},"cell_type":"code","source":"# Inspect DAYS_EMPLOYED and observe that the column doesn't look normal. The max should be negative and should not be that big.\napp_train['DAYS_EMPLOYED'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c04c3a1ade163987b549bd85f1db856feb306f13"},"cell_type":"code","source":"app_train['DAYS_EMPLOYED'].plot.hist(title = \"Days Employment Histogram\")\nplt.xlabel(\"Days Employment\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8bafcbd48b46153c6579c0e2f09ed517f83ab4dc"},"cell_type":"code","source":"# Out of curiosity, let's see if the clients of anomaly subset tend to have higher defaults than those of non-anomaly subset\nanom = app_train[app_train['DAYS_EMPLOYED'] == 365243]\nnon_anom = app_train[app_train['DAYS_EMPLOYED'] != 365243]\nprint(\"The non-anomalies default on %0.2f%% of loans\" %(100 * non_anom['TARGET'].mean()))\nprint(\"The anomalies default on %0.2f%% of loans\" %(100 * anom['TARGET'].mean()))\nprint(\"There are %d anomalous data points\" %(len(anom)))\n\n# It looks like anomalies less default on loans than non-anomalies","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"76ae64d7eff7658f708ebb486a5d3ac0ee312f7e"},"cell_type":"code","source":"# Handling anomalies depends on the exact situation. However,one of the safest approaches is to set them to missing\n# values and then have them filled in (using imputation) before machine learning.\n# In this case, since all the anomalies share the same value, we want to fill them with one common value.\n# First, we'll fill these anomalies with np.nan. Later, we'll replace NaN's with an appropriate number.\n# Thus, we also need to create an idicating column for these anomalies\n\n# Create an anomalous flag column\napp_train['DAYS_EMPLOYED_ANOM'] = app_train['DAYS_EMPLOYED'] == 365243\n\n# Replace anomalous values with nan\napp_train['DAYS_EMPLOYED'].replace({365243: np.nan}, inplace = True)\n\n# Plot the histogram\napp_train['DAYS_EMPLOYED'].plot.hist(title = 'Days Employment Histogram')\nplt.xlabel('Days employed')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e1b9b524d760c1ffb7a91323986bd02e9648c211"},"cell_type":"code","source":"# Do the same thing to test data. This is very important but people just keep forget all the time!\n# Most machine learning platforms now will spit out some results no matter what, which is extremely dangerous!\n# Is there anyway to check for dimensions of traning data and test data to make sure that this won't happen???\n\napp_test['DAYS_EMPLOYED_ANOM'] = app_test['DAYS_EMPLOYED'] == 365243\napp_test['DAYS_EMPLOYED'].replace({365243: np.nan}, inplace= True)\nprint('There are %d anomalies in the test data out of %d data points' %(app_test['DAYS_EMPLOYED'].isnull().sum(), len(app_test)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1cc5b5ec94b8da6946970ddbcff16f58f9f43d2e"},"cell_type":"markdown","source":"# Study Correlations\nAnother way for us to understand more about the data is looking at the correlations between features and the target"},{"metadata":{"trusted":true,"_uuid":"e28df062d05221c7740cecf6905f7b11da222863"},"cell_type":"code","source":"# Find the correlations with the target and sort\ncorrelations = app_train.corr()['TARGET'].sort_values()\n\n# Display correlations\nprint('Most Positive Correlations: \\n', correlations.tail(15))\nprint('\\nMost Negative Correlations: \\n', correlations.head(15))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"98fa2515580edf0a85ba906d1419871c054144ec"},"cell_type":"code","source":"# Note that TARGET and DAYS_BIRTH are highly correlated.\n# This makes sense since as people get older, they are more responsible with their loans.\n# Note that since 1 represents default and 0 represents repaid, we need to adjust our data\n# a little bit\napp_train['DAYS_BIRTH'] = abs(app_train['DAYS_BIRTH'])\napp_train['DAYS_BIRTH'].corr(app_train['TARGET'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b0fe3420068e3c467cb0b3c9a95ea1a4883d0b66"},"cell_type":"code","source":"# We now study the distribution of DAYS_BIRTH\n\n# Set the style of plots\nplt.style.use('fivethirtyeight')\n\n# Plot the distribution of ages in years\nplt.hist(app_train['DAYS_BIRTH'] / 365, edgecolor = 'k', bins = 25)\nplt.title('Age of Client'); plt.xlabel('Age (years)'); plt.ylabel('Count')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9cba1d0a9704e1da8eeacd60f1471843d7d0255d"},"cell_type":"code","source":"# Use kde to plot the distribution of DAYS_BIRTH\nplt.figure(figsize=(10,8))\n\n# kde plot of loans that were repaid on time\nsns.kdeplot(app_train.loc[app_train['TARGET'] == 0, 'DAYS_BIRTH'] / 365, label = 'target == 0')\n\n# kde plot of loans that were default\nsns.kdeplot(app_train.loc[app_train['TARGET'] == 1, 'DAYS_BIRTH'] / 365, label = 'target == 1')\n\n# Labeling of plot\nplt.title('Distribution of Age'); plt.xlabel('Age (years)'); plt.ylabel('Density')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c47fcb03f5a05838cce3717cb31c16a0fc766a7c"},"cell_type":"code","source":"# Average failure to repay loans by age bracket\n\n# Separate age information into a separate dataframe\nage_data = app_train[['TARGET', 'DAYS_BIRTH']]\nage_data['YEARS_BIRTH'] = age_data['DAYS_BIRTH'] / 365\n\n# Bin the age data. I just learned more about np.linspace\nage_data['YEARS_BINNED'] = pd.cut(age_data['YEARS_BIRTH'], bins = np.linspace(20, 70, num = 11))\n\nage_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4321a704772023fc62357297c022a92debc67006"},"cell_type":"code","source":"age_groups = age_data.groupby('YEARS_BINNED').mean()\nage_groups","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e9200e02aa11ebff079ea2726c33f712da89fec4"},"cell_type":"code","source":"plt.figure(figsize = (8, 8))\n\n# Graph the age bins and the average of the target as a bar plot\nplt.bar(age_groups['TARGET'].index.astype(str), 100 * age_groups['TARGET'])\n\n# Adjust x axis\nplt.xticks(rotation = 75); plt.xlabel('Age Group (years)'); plt.ylabel('Failure to Repay (%)')\nplt.title('Failure to Repay by Age Group')\nplt.show()\n\n# This shows that the youngest folks are not very responsible for their loans at all","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ef03339fabad994c876d8847f20a011b0c102740"},"cell_type":"code","source":"# We now study external sources because they have the strongest \next_data = app_train[['TARGET', 'EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH']]\next_data_corrs = ext_data.corr()\next_data_corrs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ea5e7fa6720b80e2b5828d7b72eb5f2e69330462"},"cell_type":"code","source":"# Express ext_data_corrs in heatmap style\nplt.figure(figsize = (8,6))\n\nsns.heatmap(ext_data_corrs, cmap = plt.cm.RdYlBu_r, vmin = -0.25, annot = True, vmax = 0.6)\nplt.title('Correlation Heatmap')\nplt.show()\n\n# Note that all three external sources are negatively correlated with the TARGET variable, which\n# indicates that as the (absolute) value increases, the applicant is more likely to repay the loan","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"53b4d046337412a126873e3df1300b3b878552b0"},"cell_type":"code","source":"# Next we plot the distributions of external data\nplt.figure(figsize = (10, 12))\n\n# Iterate through the sources\nfor idx, source in enumerate(['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']):\n    \n    # Create a new subplot for each source\n    plt.subplot(3, 1, idx + 1)\n    # Plot the distribution of source that were repaid\n    sns.kdeplot(ext_data.loc[ext_data['TARGET'] == 0, source], label = 'target == 0') # repaid\n    # Plot the distribution of source that were default\n    sns.kdeplot(ext_data.loc[ext_data['TARGET'] == 1, source], label = 'target == 1') # default\n    \n    plt.title('Distribution of %s by Target Value' % source)\n    plt.xlabel('%s' % source); plt.ylabel('Density')\n    \nplt.tight_layout(h_pad = 4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"022083fd5f952e9a631287e28cb09f89b5e7feae"},"cell_type":"code","source":"# Copy the data for plotting\nplot_data = ext_data.drop(columns = ['DAYS_BIRTH']).copy()\n\n# Add in the age of the client in years\nplot_data['YEARS_BIRTH'] = age_data['YEARS_BIRTH']\n\n# Drop na values and limit to first 100000 rows\nplot_data = plot_data.dropna().loc[:100000, :]\n\n# Function to calculate correlation coefficient between two columns\ndef corr_func(x, y, **kwargs):\n    r = np.corrcoef(x, y)[0][1]\n    ax = plt.gca()\n    ax.annotate(\"r = {:.2f}\".format(r),\n                xy=(.2, .8), xycoords=ax.transAxes,\n                size = 20)\n\n# Create the pairgrid object\ngrid = sns.PairGrid(data = plot_data, size = 3, diag_sharey=False,\n                    hue = 'TARGET', \n                    vars = [x for x in list(plot_data.columns) if x != 'TARGET'])\n\n# Upper is a scatter plot\ngrid.map_upper(plt.scatter, alpha = 0.2)\n\n# Diagonal is a histogram\ngrid.map_diag(sns.kdeplot)\n\n# Bottom is density plot\ngrid.map_lower(sns.kdeplot, cmap = plt.cm.OrRd_r);\n\nplt.suptitle('Ext Source and Age Features Pairs Plot', size = 32, y = 1.05);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a2917087910cc01c8bc740111a3c4387eb74c47c"},"cell_type":"markdown","source":"# Feature Engineering\nTo cite a quote from Andrew Ng, \"Applied machine learning is basically feature engineering.\""},{"metadata":{"trusted":true,"_uuid":"8584b990cea7ee02d792d7696f0cfeab77134920"},"cell_type":"code","source":"# First, we need to fill out missing values (finally!!!)\n# Remember two things: (1) never ever touch the target variable,\n# and (2) always do the same thing with test data\n\n# Get features that we want to transform\npoly_features = app_train[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH', 'TARGET']]\npoly_features_test = app_test[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH']]\n\n\n# Imputer for handling missing values\nfrom sklearn.preprocessing import Imputer\nimputer = Imputer(strategy = 'median') # multiple imputation common in statistics\n\npoly_target = poly_features['TARGET']\npoly_features = poly_features.drop(columns = ['TARGET'])\n\n# Need to impute missing values\npoly_features = imputer.fit_transform(poly_features)\npoly_features_test = imputer.fit_transform(poly_features_test)\n\n# Now we can transform our features\n# Create the polynomial object with specified degree\nfrom sklearn.preprocessing import PolynomialFeatures\npoly_transformer = PolynomialFeatures(degree = 3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2104a10933f7abf3221af9ab8f6db1faef5b4f2a"},"cell_type":"code","source":"# Train the polynomial features\npoly_transformer.fit(poly_features)\n\n# Transform the features\npoly_features = poly_transformer.transform(poly_features)\npoly_features_test = poly_transformer.transform(poly_features_test)\nprint('Polynomial Features shape: ', poly_features.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c6565a06288f7acba48c801f14dab2b184ff666a"},"cell_type":"code","source":"feature_names = poly_transformer.get_feature_names(input_features = ['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH'])\nprint(feature_names)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cb72b13fb96bd9d26bc4ba72e109482d557c055b"},"cell_type":"code","source":"# Now want to see if any of these features are correlated with the target.\n\n# Convert np array to dataframe\npoly_features = pd.DataFrame(poly_features, columns = feature_names)\n# Add in the target\npoly_features['TARGET'] = poly_target\n# Find correlations with the target variable\npoly_corrs = poly_features.corr()['TARGET'].sort_values()\n# Display most negative and most positive\nprint(poly_corrs.head(10))\nprint(poly_corrs.tail())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ad039e1cea87635d0285291e966a6766c631e489"},"cell_type":"code","source":"# Put test features into dataframe\npoly_features_test = pd.DataFrame(poly_features_test, columns = feature_names)\n\n# Merge polynomial features into training dataframe\nkey = 'SK_ID_CURR'\npoly_features[key] = app_train[key] # key to join\napp_train_poly = app_train.merge(poly_features, on = key, how = 'left')\n\n# Merge polynomial features into test dataframe\npoly_features_test[key] = app_test[key]\napp_test_poly = app_test.merge(poly_features_test, on = key, how = 'left')\n\n# Algin the dataframes\napp_train_poly, app_test_poly = app_train_poly.align(app_test_poly, join = 'inner', axis = 1)\n\n# Print out the new shapes\nprint('Training data with polynomial features shape:', app_train_poly.shape)\nprint('Test data with polynomial features shape:', app_test_poly.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6ee4295c3fedbfc400bce72107491a0e61eb226e"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"687402f6f33065af15fbcdc5615c7a119172ab49"},"cell_type":"markdown","source":"# Domain knowledge feature\nCopy these codes because we don't have that kind of expertise. Note that we add columns to the original"},{"metadata":{"trusted":true,"_uuid":"87d2fbe672d8d6800e8ddf06fe1c7a67c3b3a0d7"},"cell_type":"code","source":"# For training data. Think how I can write a function for this. I don't like repeating like this!\napp_train_domain = app_train.copy()\napp_test_domain = app_test.copy()\n\napp_train_domain['CREDIT_INCOME_PERCENT'] = app_train_domain['AMT_CREDIT'] / app_train_domain['AMT_INCOME_TOTAL']\napp_train_domain['ANNUITY_INCOME_PERCENT'] = app_train_domain['AMT_ANNUITY'] / app_train_domain['AMT_INCOME_TOTAL']\napp_train_domain['CREDIT_TERM'] = app_train_domain['AMT_ANNUITY'] / app_train_domain['AMT_CREDIT']\napp_train_domain['DAYS_EMPLOYED_PERCENT'] = app_train_domain['DAYS_EMPLOYED'] / app_train_domain['DAYS_BIRTH']\n\n# For test data\napp_test_domain['CREDIT_INCOME_PERCENT'] = app_test_domain['AMT_CREDIT'] / app_test_domain['AMT_INCOME_TOTAL']\napp_test_domain['ANNUITY_INCOME_PERCENT'] = app_test_domain['AMT_ANNUITY'] / app_test_domain['AMT_INCOME_TOTAL']\napp_test_domain['CREDIT_TERM'] = app_test_domain['AMT_ANNUITY'] / app_test_domain['AMT_CREDIT']\napp_test_domain['DAYS_EMPLOYED_PERCENT'] = app_test_domain['DAYS_EMPLOYED'] / app_test_domain['DAYS_BIRTH']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9caf7c313763a1c14599cff73879f9c21d4cd95c"},"cell_type":"code","source":"print(app_train_domain.shape)\nprint(app_test_domain.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c1604048c7765c7791fefe4ace02eeb97263f4f2"},"cell_type":"code","source":"# Let's visualize these new variables by plotting their distributions with kde method\n\n# Creat a new plot\nplt.figure(figsize = (10, 12))\n\n# Plot the distributions with kde method in seaborn library\nfor idx, source in enumerate(['CREDIT_INCOME_PERCENT', 'ANNUITY_INCOME_PERCENT',\n                             'CREDIT_TERM', 'DAYS_EMPLOYED_PERCENT']):\n    # Create a subplot for each source\n    plt.subplot(4, 1, idx + 1)\n    # Distribution of repaid loans\n    sns.kdeplot(app_train_domain.loc[app_train_domain['TARGET'] == 0, source], label = 'target == 0')\n    # Distribution of default loans\n    sns.kdeplot(app_train_domain.loc[app_train_domain['TARGET'] == 1, source], label = 'target == 1')\n    # Label each subplot\n    plt.xlabel('%s' % source); plt.ylabel('Density')\n    plt.title('Distribution of %s by Target Value' % source)\n    \nplt.tight_layout(h_pad = 2.5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c30e96193cf2a3395ad318283ec53b8c68e36296"},"cell_type":"markdown","source":"# Classification tasks\nThe following list all the models that will be used. For each model, we will simultaneously try on the original data, data with polynomial features, and data with domain-knowledge features.\n1. Baseline model\n2. Logistic regression"},{"metadata":{"trusted":true,"_uuid":"a06e43d585fc15a7cee071b2ee77195a3de0b269"},"cell_type":"code","source":"# 1. Baseline model\n# This is purely guessing. For each case, flip a coin. If, say, H, then declare 0. If, say, T, declare, 0.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"04acac1b835c24a2ac6085927fd959892e899412","scrolled":true},"cell_type":"code","source":"# 2. Logistic regression\n# We'll start by imputing missing values in app_train. Ahhh! Never ever touch the target variable\n\nfrom sklearn.preprocessing import MinMaxScaler, Imputer\n\n# Isolate the target variable\nif 'TARGET' in app_train: # This code is smart because it is polymorphic\n    train = app_train.drop(columns=['TARGET'])\n    print('TARGET was in app_train')\n    print('TARGET would be dropped from app_train')\nelse:\n    train = app_train.copy()\n    print('TARGET was not in app_train')\n    print('TARGET would be added to app_train before being dropped for train')\n\nprint('\\n----------------------NEXT STEP----------------------\\n')\n\n# Feature names\nfeatures = list(train.columns)\n\n# Imputation\nimputer = Imputer(strategy = 'median')\nimputer.fit(train)\nprint('BEFORE IMPUTATION...')\nmissing_values_table(train)\nprint()\nprint('AFTER IMPUTATION...')\ntrain = imputer.transform(train)\nmissing_values_table(pd.DataFrame(train))\n\n# Scaling\nscaler = MinMaxScaler()\nscaler.fit(train)\ntrain = scaler.transform(train)\n\n# Do the same thing for test data\ntest = imputer.transform(app_test)\ntest = scaler.transform(test)\n\nprint()\nprint('Training data shape:', train.shape)\nprint('Test data shape:', test.shape)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"08356a3559e5c6ed675f01f249052b1678d8d022"},"cell_type":"code","source":"# Now logistic regression\nfrom sklearn.linear_model import LogisticRegression\n# Step 1: Create the model object\nlog_reg = LogisticRegression(C = 0.001) # C is the regularization parameter\n# Step 2: Fit on training data\nlog_reg.fit(train, train_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"83cc1ef1f485bdb3f964aa758dceeb04371aae55"},"cell_type":"code","source":"# Step 3: Predict\nlog_reg_pred = log_reg.predict_proba(test)[:, 1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dfccee1231c0a2b071493bd5780b1c21a72a7d4d"},"cell_type":"code","source":"# Submission dataframe\nsubmit = app_test[['SK_ID_CURR']]\nsubmit['TARGET'] = log_reg_pred\nsubmit.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"72c6b7030ab2fd25c6c4628be1ff0011f2111b59"},"cell_type":"code","source":"# Save the submission to a csv file\nsubmit.to_csv('log_reg_baseline.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1371307d862b347f7b3c3472b4c09959f362ab81"},"cell_type":"code","source":"# Random Forest\n\nfrom sklearn.ensemble import RandomForestClassifier # Note that RandomForestRegressor is for regression tasks\n\n# Step 1: Define the model\nrandom_forest = RandomForestClassifier(n_estimators = 100, random_state = 50, verbose = 1, n_jobs = -1)\n\n# Step 2: Fit the model on training data\nrandom_forest.fit(train, train_labels)\n\n# Extract feature importances\nfeature_importance_values = random_forest.feature_importances_\nfeature_importances = pd.DataFrame({'feature': features, 'importance': feature_importance_values})\n\n# Step 3: Make predictions on the test data\npredictions = random_forest.predict_proba(test)[:, 1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a7c7ff765f07edc3137310863b4b2a4637fae43f"},"cell_type":"code","source":"# Make the submission dataframe\nsubmit = app_test[['SK_ID_CURR']]\nsubmit['TARGET'] = predictions\n\n# Save the submission dataframe\nsubmit.to_csv('random_forest_baseline.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"02084d712553f16b5d501aa2fe322fd0a04bf8ad"},"cell_type":"code","source":"submit.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5fdbabde1c64e10af35892da61b1316e54edb60d"},"cell_type":"code","source":"feature_importances.sort_values('importance', ascending=False).head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e318645b8f677ac30250db2bd2875a4ab8024d1a"},"cell_type":"code","source":"# Let's predict on polynomial features\n\n# Extract feature names\npoly_features_names = list(app_train_poly.columns)\n\n# Imputation\nimputer = Imputer(strategy='median')\npoly_features_train = imputer.fit_transform(app_train_poly)\npoly_features_test  = imputer.transform(app_test_poly)\n\n# Scale the polynomial features\nscaler = MinMaxScaler()\npoly_features_train = scaler.fit_transform(poly_features_train)\npoly_features_test  = scaler.transform(poly_features_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2e67f21a5b0b099c6ca558248e1c2450d0cf950c"},"cell_type":"code","source":"# Step 1: Define the model\nrandom_forest_poly = RandomForestClassifier(n_estimators=100, random_state=50, verbose=1, n_jobs=-1)\n\n# Step 2: Fit the model on training data\nrandom_forest_poly.fit(poly_features_train, train_labels)\n\n# Step 3: Make predictions on the test data\npoly_predictions = random_forest_poly.predict_proba(poly_features_test)[:, 1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8099a5551a559dcec1d9001574e316b6db6732d6"},"cell_type":"code","source":"submit = app_test[['SK_ID_CURR']]\nsubmit['TARGET'] = predictions\n\n# Save the submission dataframe\nsubmit.to_csv('random_forest_baseline_engineered.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"65684f3e8f0c2706f9ad5daacd0b7f28bfb4a040"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7ddc73ab1dea60044ed3bf0b9f27cc93b34c506f"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8204e476e1d29a94ada5e5bbb19cb5bf3afcd578"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7a4d786bad32558e85935d381c18499a2e91d928"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ba6e3ac6dc4f71e78e4fb13088131f23341cce63"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"34cbd4f1df03f8e9241b61dd5db9b15cfa243069"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"41555a330f91b574fdcd41d53fb93f2f8cc8ba7d"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b1a9a1f6188890aaccdfb6b7c16f5d005fb1e165"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1f47a7ded553e2824643c67fb3bff64933e467ca"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}