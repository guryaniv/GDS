{"cells":[{"metadata":{"_uuid":"e2ae927e6cba0d1772a1f58cc3c67b0c08da782f","trusted":true,"collapsed":true},"cell_type":"code","source":"# Import data manipulation libraries\nimport pandas as pd\nimport numpy as np\nimport datetime\n\n# Visualization libaries\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Machine Learning\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn import tree\nfrom sklearn.linear_model import SGDRegressor\nimport xgboost as xgb","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dbd184aa2231625c46e7c1e73171ad3d2dd1ab54","trusted":true},"cell_type":"code","source":"# Import training and test data\ntrain = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\ncombine = [train, test]\n\nprint(train.head(3))\nprint(test.head(3))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3647dc9c82bb499b52ea9338215a005910b2c71a","trusted":true},"cell_type":"code","source":"# Define column date as datatype date and define new date features\nfor dataset in combine:\n    dataset['date'] = pd.to_datetime(dataset['date'])\n    dataset['year'] = dataset.date.dt.year\n    dataset['month'] = dataset.date.dt.month\n    dataset['day'] = dataset.date.dt.day\n    dataset['dayofyear'] = dataset.date.dt.dayofyear\n    dataset['dayofweek'] = dataset.date.dt.dayofweek\n    dataset['weekofyear'] = dataset.date.dt.weekofyear\n    \n    # Additional date features\n    dataset['log_dayofyear'] = np.log(dataset['dayofyear'])\n    dataset['day_power_year'] = np.log((np.log(dataset['dayofyear'] + 1)) ** (dataset['year'] - 2000))\n    dataset['day_week_power_year'] = np.log(np.log(dataset['dayofyear'] + 1) * (np.log(dataset['weekofyear'] + 1)) ** (dataset['year'] - 2000))\n    \n    # Drop date\n    dataset.drop('date', axis=1, inplace=True)\n    \ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3040d86ffbf73b6ad01c9049a85dbbcb3dee26fd","trusted":true,"collapsed":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"369154287c168f6854e898d6b4c184b4192a62b2","trusted":true,"collapsed":true},"cell_type":"code","source":"# Add features, such as average sales pr. day, average sales pr. month, rolling mean 90 periods\ndef add_avg(x):\n    x['daily_avg']=x.groupby(['item','store','dayofweek'])['sales'].transform('mean')\n    x['monthly_avg']=x.groupby(['item','store','month'])['sales'].transform('mean')\n    return x\ntrain = add_avg(train).dropna()\n\ndaily_avg = train.groupby(['item','store','dayofweek'])['sales'].mean().reset_index()\nmonthly_avg = train.groupby(['item','store','month'])['sales'].mean().reset_index()\n\ndef merge(x,y,col,col_name):\n    x =pd.merge(x, y, how='left', on=None, left_on=col, right_on=col,\n            left_index=False, right_index=False, sort=True,\n             copy=True, indicator=False,validate=None)\n    x=x.rename(columns={'sales':col_name})\n    return x\n\ntest = merge(test, daily_avg,['item','store','dayofweek'],'daily_avg')\ntest = merge(test, monthly_avg,['item','store','month'],'monthly_avg')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8cadedb4820e4f8436752d0113c9d340443b02ae","trusted":true},"cell_type":"code","source":"# Adding rolling mean feature to train\ndf = train.groupby(['item'])['sales'].rolling(10).mean().reset_index().drop('level_1', axis=1)\ntrain['rolling_mean'] = df['sales']\n\n# Adding last 3 months of rolling mean from training to test data \n# (Doing this and shifting rolling mean 3 months in training data)\nrolling_mean_test = train.groupby(['item','store'])['rolling_mean'].tail(90).copy().reset_index().drop('index', axis=1)\ntest['rolling_mean'] = rolling_mean_test\n\n# Shifting rolling mean 3 months\ntrain['rolling_mean'] = train.groupby(['item'])['rolling_mean'].shift(90)\ntrain.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0206ffa8b19769200b8e18ac09bd89114cc383c1"},"cell_type":"code","source":"combine = [train, test]\n\nfor dataset in combine:\n    dataset['item_times_rolling_mean'] = dataset['item'] * dataset['rolling_mean']\n    dataset['store_times_rolling_mean'] = dataset['store'] * dataset['rolling_mean']\n    dataset['dayofyear_times_rolling_mean'] = dataset['dayofyear'] * dataset['rolling_mean']\n    \ntrain.columns,test.columns","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9620dcef756f5a034158db6b75bb76780dbbdd87","trusted":true,"scrolled":true},"cell_type":"code","source":"# Let's check how the features correlate\ncolormap = plt.cm.RdBu\nplt.figure(figsize=(14,12))\nplt.title('Pearson Correlation of Features', y=1.05, size=15)\nsns.heatmap(train.corr(),linewidths=0.1,vmax=1.0, \n            square=True, cmap=colormap, linecolor='white', annot=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"64cca0b45fca7068872f166bae654077bceb7b99","trusted":true},"cell_type":"code","source":"# Seems like dayofyear and weekofyear has high internal correlation and correlates highly with month, so let's drop those.\n# All average/mean features also correlated heavily. Since Monthly Average correlates most with sales, we keep this.\ncombine = [train, test]\n\nfor dataset in combine:\n    dataset.drop(['dayofyear', \n                  'weekofyear',\n                  'daily_avg',\n                  'day',\n                  'month',\n                  'item',\n                  'store',\n                  'day_week_power_year',\n                  'log_dayofyear',\n                  #'monthly_avg',\n                  'dayofyear_times_rolling_mean',\n                  #'store_times_rolling_mean',\n                  #'rolling_mean',\n                  'item_times_rolling_mean'],\n                  #'year'],\n                  #'dayofweek',\n                  #'day_power_year'], \n                 axis=1, \n                 inplace=True)\n    \n# Let's check the correlation with all dropped features\ncolormap = plt.cm.RdBu\nplt.figure(figsize=(14,12))\nplt.title('Pearson Correlation of Features', y=1.05, size=15)\nsns.heatmap(train.corr(),linewidths=0.1,vmax=1.0, \n            square=True, cmap=colormap, linecolor='white', annot=True)\n\ntrain.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"26ba0b77eb820ff7e4e76c7d6efae3be484bbb00"},"cell_type":"code","source":"# Feature scaling for faster algorithm optimum\ntemp_sales = train['sales']\ntemp_id = test['id']\ntrain = (train - train.mean()) / train.std()\ntest = (test - test.mean()) / test.std()\ntrain['sales'] = temp_sales\ntest['id'] = temp_id\n\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d15bfb231c61ea5119829807cfcb3e166c620655","trusted":true,"scrolled":true},"cell_type":"code","source":"# Let's prepare the training and data set\nx_train = train.drop('sales', axis=1).dropna()\ny_train = train['sales']\ntest.sort_values(by=['id'], inplace=True)\nx_test = test.drop('id', axis=1)\n\nx_pred = test.drop('id', axis=1)\ndf = train\n\ntrain.tail()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"30e927cce4967ef0dd3c0bb6b927e422704a5587","trusted":true,"scrolled":true,"collapsed":true},"cell_type":"code","source":"# Linear Regression Model\n#regr = LinearRegression()\n#regr.fit(x_train, y_train)\n#prediction = regr.predict(x_test)\n\n# Decision Tree (Best performer so far)\n# max_depth = 100, min_samples_leaf = 10\n#clf = tree.DecisionTreeRegressor()\n#clf.fit(x_train, y_train)\n#prediction = clf.predict(x_test)\n\n# SGD Stochastic Gradient Descent Regressor\n#combine = [x_train, x_test]\n\n#for dataset in combine:\n#    dataset['day_power_year'] = (dataset['day_power_year'] - dataset['day_power_year'].mean()) / dataset['day_power_year'].std()\n#    dataset['monthly_avg'] = (dataset['monthly_avg'] - dataset['monthly_avg'].mean()) / dataset['monthly_avg'].std()\n#sgd = SGDRegressor(max_iter = 1000, alpha = .0003, learning_rate='constant', verbose=1)\n#sgd.fit(x_train, y_train)\n#prediction = sgd.predict(x_test)\n\n# Let's run XGBoost and predict those sales!\nx_train,x_test,y_train,y_test = train_test_split(df.drop('sales',axis=1),df.pop('sales'),random_state=123,test_size=0.2)\n\ndef XGBmodel(x_train,x_test,y_train,y_test):\n    matrix_train = xgb.DMatrix(x_train,label=y_train)\n    matrix_test = xgb.DMatrix(x_test,label=y_test)\n    model=xgb.train(params={'objective':'reg:linear','eval_metric':'mae'}\n                    ,dtrain=matrix_train,num_boost_round=500, \n                    early_stopping_rounds=20,evals=[(matrix_test,'test')],)\n    return model\n\nmodel=XGBmodel(x_train,x_test,y_train,y_test)\ny_pred = model.predict(xgb.DMatrix(x_pred), ntree_limit = model.best_ntree_limit)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b65e530a03b02cd595782f3bf45bbf94fb4c0a9a","trusted":true,"collapsed":true},"cell_type":"code","source":"# Add to submission\n\nsubmission = pd.DataFrame({\n        \"id\": test['id'],\n        \"sales\": y_pred.round()\n})\n\nsubmission.to_csv('sub500.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c33c48eef949936e37058ef34379c397958cf700","trusted":true,"scrolled":true,"collapsed":true},"cell_type":"code","source":"submission.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"83ff6d232fe815f845b137cfd3927f7ff70ac15d","collapsed":true,"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"2e6f17323603b1067db1d5e3d94c0a61efafbf46"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}