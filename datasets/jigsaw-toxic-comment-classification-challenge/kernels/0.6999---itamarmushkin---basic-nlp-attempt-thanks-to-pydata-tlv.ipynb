{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"#This submission is based on NLP workshop by the PyData team from Tel Aviv! Thanks a lot!\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os, sys, re, collections, string, itertools\n\nfrom sklearn.feature_extraction import text\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, classification_report\n\nfrom sklearn.linear_model import SGDClassifier, LogisticRegression\nfrom sklearn.svm import LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\n\nfrom gensim.models import Word2Vec\n\nprint(os.listdir(\"../input\"))\ntrain_data=pd.read_csv('../input/train.csv')\ntest_data=pd.read_csv('../input/test.csv')\n\ntrain_data.head() #showing some sample toxic comments; \n#learning that every toxic comment is tagged as 'toxic' with our without some sub-tag\n\nX=train_data['comment_text']\nX_test=test_data['comment_text']\n\nlabels=train_data.columns.values[2:]\ntoxic_sublabels=train_data.columns.values[3:]\n\nys=train_data[labels]\ny0=train_data['toxic']\ntoxic_ys=train_data[toxic_sublabels][y0==1]\ntoxic_comments=X[y0==1]\n\ntoxic_comments.head()\ntoxic_ys.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d10db236d9dd9e384de9d43dd774f2240f9574a7","collapsed":true},"cell_type":"code","source":"test_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cbdb816ffd0d5bd8ca27bed9c9a19b58c8b21b50","collapsed":true},"cell_type":"code","source":"def clean_text(text):\n    text=text.str.lower()\n    digits = re.compile(r\"\\d[\\d\\.\\$]*\")\n    not_allowed = re.compile(r\"[^\\s\\w<>_]\")\n    text=text.str.replace(digits,\"<NUM>\")\n    text=text.str.replace(not_allowed,\"\")\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"55403f13a2ad8e82c10e409cd76f34d7249cb402","collapsed":true},"cell_type":"code","source":"X=clean_text(X)\nX_test=clean_text(X_test)\nX_train, X_crossval, y_trains, y_crossvals = train_test_split(X, ys, test_size=0.3, random_state=20180301)\n\nvectorizer = text.CountVectorizer()\nvectorizer = text.TfidfVectorizer(max_features=1000, max_df=0.05)\nvectorizer.fit(X_train)\nX_train = vectorizer.transform(X_train)\nX_crossval=vectorizer.transform(X_crossval)\nX_test=vectorizer.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"2b8fb5bb1ee546e9b8d054dad7c6ee87f44ad7ce","collapsed":true},"cell_type":"code","source":"model = LinearSVC()\n\nfor label in labels:\n    y_train=y_trains[label]\n    y_crossval=y_crossvals[label]\n    model.fit(X_train, y_train)\n    yh_train = model.predict(X_train)\n    yh_crossval = model.predict(X_crossval)\n    print(label)\n    print(classification_report(y_crossval, yh_crossval))\n    yh_test=model.predict(X_test)\n    test_data[label]=yh_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c6fb50e334790af1ab924aa043b699dcfc403a22","collapsed":true},"cell_type":"code","source":"my_submission = test_data\nmy_submission.drop('comment_text',axis=1,inplace=True)\nmy_submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}