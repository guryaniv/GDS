{"cells":[{"metadata":{"_uuid":"28ad10cb1e737615ac62206c44d3f61ed8c4b833"},"cell_type":"markdown","source":"# Contents\n\n1. [The Problem](#The-Problem)\n2. [Common Libraries](#Common-Libraries)\n3. [Import Data](#Import-Data)\n4. [Explore Data](#Explore-Data)\n5. [Prepare Data](#Prepare-Data)\n6. [Shortlist Models](#Shortlist-Models)\n7. [Tune Models](#Tune-Models)\n8. [Submission](#Submission)"},{"metadata":{"_uuid":"d4bba7a5ce3e587cf3a00b3185a9212d81051c56"},"cell_type":"markdown","source":"# The Problem\n\nFor now, you can read about the problem on Kaggle. \n\nBased on that, this qualifies as a supervised multivariate classification problem in which we'll use batch learning from the supplied dataset.\n\n## Performance evaluation \nAs described in the Kaggle description, the performance will be evaluated using the Macro F1 Score. \n\nTodo:\n* Describe the F1 score and evaluate if it's a good fit.\n* Is the right measure given the business objectives?\n* Minimumm performance required?\n* List assumptions"},{"metadata":{"_uuid":"307e35e37a98881213e58315dfccd3c578f5d84d"},"cell_type":"markdown","source":"# Common Libraries"},{"metadata":{"_uuid":"2d04376e5953cdeaf0f912889db6ebd31afd2bfb","trusted":false},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Set up Seaborn with default theme, scaling, and color palette\nsns.set()\n\n#Scikit-learn common imports\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.pipeline import Pipeline, make_pipeline","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bdb2899ad39e69f7cb5c49f3625f12d2b7a137e0"},"cell_type":"markdown","source":"# Import Data"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":false},"cell_type":"code","source":"def import_train_test():\n    train = pd.read_csv('../input/train.csv')\n    test = pd.read_csv('../input/test.csv')\n    \n    return train, test\n\ntrain, test = import_train_test()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true},"cell_type":"markdown","source":"# Explore Data"},{"metadata":{"_uuid":"864c6d75e9f1443b87d8d41fcf6f4fc5429d6fe7","trusted":false},"cell_type":"code","source":"print(train.shape)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3e59243b076bc3c103ca8a058d5c793db2673b97","trusted":false},"cell_type":"code","source":"print(test.shape)\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"aba9e552f84b7988f660064b47d8c33356342d0d"},"cell_type":"markdown","source":"Okay, first things we notice is that we have 9,557 rows in our training set and 23,856 in our test set. Each row represents an individual and the household they correspond it is indicated by their `idhogar` value. According to the business objective, we are only interested in classifying households, the `Target` should be the same for every individual in a household. Let's see how many households are in each set."},{"metadata":{"_uuid":"b43463fda5039df3686ce4f79cb9c803b87cf453","trusted":false},"cell_type":"code","source":"print(f\"There are {len(train.idhogar.unique())} unique households in the training set.\")\nprint(f\"There are {len(test.idhogar.unique())} unique households in the test set.\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a7045c5e3aa3bb869790db085be0be2ae6b81b61"},"cell_type":"markdown","source":"Almost 2.5 times the number of households in the test set. This means we should be very cautious to make sure that our model doesn't overfit the training set because if we do, we'll likely score pretty poorly on the test submission.\n\nLet's take a look at the label distirbution both for individuals and household."},{"metadata":{"_uuid":"86567e641ef322c6ffbd747317df931e9ac6d57a","trusted":false},"cell_type":"code","source":"fig, axs = plt.subplots(1,2)\nfig.tight_layout()\n\nsns.countplot(x='Target', data=train, ax=axs[0])\naxs[0].set_title('Train')\n\nsns.countplot(x='Target', data=train.copy().groupby('idhogar').first(), ax=axs[1])\naxs[1].set_title('Train Households')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e6695e63d5fe4d3881d4a1fb20011db494d6ffee"},"cell_type":"markdown","source":"Okay, looks like we're okay in terms of the distributions between the train set and the extracted households. We should take not that our classes are very imbalanced. If the test set follows the same distribution, we could use a naive model that simply predicts the `Target` to be 4 and we'll be right over 65% of the time. This certainly won't help IDB figure out how to focus their support, since most households would be classified as non-vulnerable and, therefore, not in need of additional support. Still, it's good to be aware of this as we evaluate our models.\n\nIt seems like a lot of the features in the dataset are boolean. To start exploring the data and distributions, let's take a look at all the numeric attributes that aren't boolean to see what their distributions are and see if we can identify anything interesting about the data we're working with."},{"metadata":{"_uuid":"505b1e2b8e51346e3ee85d4c554db02bfc9bc8ee","trusted":false},"cell_type":"code","source":"def is_boolean_column(data, column):\n    return set(data[column].value_counts().keys()) == {0,1}\n    \nnumeric_non_boolean_attributes = [column for column in train.columns if not is_boolean_column(train, column)]\n\ntrain.hist(numeric_non_boolean_attributes, bins=50, figsize=(20,15))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9f92c0ab5a33ef52d3cb291ff01c753d52f70462"},"cell_type":"markdown","source":"From the above histograms, we can see a couple interesting things that we should be aware of and possibly address before training our models. Specifically:\n\n* While a lot of the attributes resemble a normal distribution, many have long tails and some are skewed heavily \n* `elimbasu5`, which shouldn't be in here since it is a boolean field, but by being in here we can see that all of the values are 0. If that's the case, this doesn't tell us anything, so we shouldn't pass that to our model.\n* There is a lot of variance in the values, so we'll need to scale these before training our model."},{"metadata":{"_uuid":"0d1521e549a1fd99b0e8d8c6346878a7bc7a014e"},"cell_type":"markdown","source":"## Outliers\nLet's find any attribute that has values outside the IQR.\n"},{"metadata":{"trusted":false,"_uuid":"04d9516c76e83e82fb587182ff386cef237bf478"},"cell_type":"code","source":"q1s = train[numeric_non_boolean_attributes].quantile(0.25)\nq3s = train[numeric_non_boolean_attributes].quantile(0.75)\n\niqrs = pd.DataFrame([q1s, q3s], index=['q1', 'q3']).transpose().drop('Target')\niqrs['iqr'] = iqrs.q3 - iqrs.q1\niqrs['iqr_amplified'] = iqrs.iqr * 1.5\niqrs['outlier_min'] = iqrs.q1 - iqrs.iqr_amplified\niqrs['outlier_max'] = iqrs.q3 + iqrs.iqr_amplified\n\ndef min_outlier_count(iqr_row):\n    return len(train[train[iqr_row.name] < iqrs.loc[[iqr_row.name]].outlier_min[0]])\n\ndef max_outlier_count(iqr_row):\n    return len(train[train[iqr_row.name] > iqrs.loc[[iqr_row.name]].outlier_max[0]])\n\niqrs['min_outlier_count'] = iqrs.apply(min_outlier_count, axis=1)\niqrs['max_outlier_count'] = iqrs.apply(max_outlier_count, axis=1)\niqrs[(iqrs.min_outlier_count > 0) | (iqrs.max_outlier_count > 0)]\n\nwith pd.option_context('display.max_columns', len(iqrs)):\n    print(train[iqrs.index].describe())","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"66a5f22fda8155683be8d1d08c4334df01902d3e"},"cell_type":"code","source":"with pd.option_context('display.max_columns', len(iqrs)):\n    print(train[iqrs.index].describe())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"53c0378cf5531c21e21494df16464dcef1083492"},"cell_type":"markdown","source":"While there are a few columns with outliers, if you simply defined that as outside 1.5 times the IQR, this is mostly because most of these distributions aren't gaussian and have long tails. When looking at the specifics, I don't see anything that's so far beyond reasonable as to be completely unexpected—depressing if you think about the real people behind these numbers, though. \n\nAside from still thinking we should exlude `elimbasu5`, I don't think we should remove or constrain any outliers in this first pass."},{"metadata":{"_uuid":"ec69e66cb3bebb59141ec0604fd5db816791f787"},"cell_type":"markdown","source":"## Attribute Correlation\nLet's also evaluate if there's any obvious correlation between any of the attributes and the target variable that we should be aware of at this point."},{"metadata":{"_uuid":"88e353372b9cc800b9db2b4d80a8b055910bfeae","trusted":false},"cell_type":"code","source":"correlations = train.copy().corr()\n\ncorrelations['Target'].where(correlations['Target'].abs() > 0.25).dropna().sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"aa8c33dc852ed8b1dd887b67261d0f21915bcb1e"},"cell_type":"markdown","source":"We can see that there a few weakly correlated attributes with the target variabe, `Target`. That's good news, otherwise we'd have a tough time training a model worth anything. \n\nOne other thing I notice here is that the squared eqivalent attributes, e.g. `SQBescolari` to `escolari` seem to have a lower correlation to the un-squared attribute. We might want to omit the squared variables.\n\n`meaneduc`, average years of education for adults (18+), and `hogar_nin`, Number of children 0 to 19 in household, are the most correlated attributes, so we should pay extra attention to those two concepts."},{"metadata":{"_uuid":"c8b70cc7f4c3ebd3a532444b2db3b6e13a8e2d04","trusted":false},"cell_type":"code","source":"fig, axs = plt.subplots(1,2)\nfig.tight_layout()\n\nsns.regplot(x='meaneduc', y='Target', data=train, ax=axs[0])\nsns.regplot(x='hogar_nin', y='Target', data=train, ax=axs[1])\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9a04f4aafba2c86d9c669668b9a7c2df85627a07"},"cell_type":"markdown","source":"# Data Cleaning\n\nFirst, let's create some reference dictionaries we'll use for the features"},{"metadata":{"_uuid":"95fb36b8bd43fb8905740b2949cba70db9f4d5d4","trusted":false},"cell_type":"code","source":"id_features = ['Id', 'idhogar']\n\nfeatures_and_descriptions = [('v2a1', 'Monthly rent payment'),\n                             ('hacdor', '=1 Overcrowding by bedrooms'),\n                             ('rooms', 'number of all rooms in the house'),\n                             ('hacapo', '=1 Overcrowding by rooms'),\n                             ('v14a', '=1 has bathroom in the household'),\n                             ('refrig', '=1 if the household has refrigerator'),\n                             ('v18q', 'owns a tablet'),\n                             ('v18q1', 'number of tablets household owns'),\n                             ('r4h1', 'Males younger than 12 years of age'),\n                             ('r4h2', 'Males 12 years of age and older'),\n                             ('r4h3', 'Total males in the household'),\n                             ('r4m1', 'Females younger than 12 years of age'),\n                             ('r4m2', 'Females 12 years of age and older'),\n                             ('r4m3', 'Total females in the household'),\n                             ('r4t1', 'persons younger than 12 years of age'),\n                             ('r4t2', 'persons 12 years of age and older'),\n                             ('r4t3', 'Total persons in the household'),\n                             ('tamhog', 'size of the household'),\n                             ('tamviv', 'number of persons living in the household'),\n                             ('escolari', 'years of schooling'),\n                             ('rez_esc', 'Years behind in school'),\n                             ('hhsize', 'household size'),\n                             ('paredblolad', '=1 if predominant material on the outside wall is block or brick'),\n                             ('paredzocalo', '\"=1 if predominant material on the outside wall is socket (wood,  zinc or absbesto\"'),\n                             ('paredpreb', '=1 if predominant material on the outside wall is prefabricated or cement'),\n                             ('pareddes', '=1 if predominant material on the outside wall is waste material'),\n                             ('paredmad', '=1 if predominant material on the outside wall is wood'),\n                             ('paredzinc', '=1 if predominant material on the outside wall is zink'),\n                             ('paredfibras', '=1 if predominant material on the outside wall is natural fibers'),\n                             ('paredother', '=1 if predominant material on the outside wall is other'),\n                             ('pisomoscer', '\"=1 if predominant material on the floor is mosaic,  ceramic,  terrazo\"'),\n                             ('pisocemento', '=1 if predominant material on the floor is cement'),\n                             ('pisoother', '=1 if predominant material on the floor is other'),\n                             ('pisonatur', '=1 if predominant material on the floor is  natural material'),\n                             ('pisonotiene', '=1 if no floor at the household'),\n                             ('pisomadera', '=1 if predominant material on the floor is wood'),\n                             ('techozinc', '=1 if predominant material on the roof is metal foil or zink'),\n                             ('techoentrepiso', '\"=1 if predominant material on the roof is fiber cement,  mezzanine \"'),\n                             ('techocane', '=1 if predominant material on the roof is natural fibers'),\n                             ('techootro', '=1 if predominant material on the roof is other'),\n                             ('cielorazo', '=1 if the house has ceiling'),\n                             ('abastaguadentro', '=1 if water provision inside the dwelling'),\n                             ('abastaguafuera', '=1 if water provision outside the dwelling'),\n                             ('abastaguano', '=1 if no water provision'),\n                             ('public', '\"=1 electricity from CNFL,  ICE,  ESPH/JASEC\"'),\n                             ('planpri', '=1 electricity from private plant'),\n                             ('noelec', '=1 no electricity in the dwelling'),\n                             ('coopele', '=1 electricity from cooperative'),\n                             ('sanitario1', '=1 no toilet in the dwelling'),\n                             ('sanitario2', '=1 toilet connected to sewer or cesspool'),\n                             ('sanitario3', '=1 toilet connected to  septic tank'),\n                             ('sanitario5', '=1 toilet connected to black hole or letrine'),\n                             ('sanitario6', '=1 toilet connected to other system'),\n                             ('energcocinar1', '=1 no main source of energy used for cooking (no kitchen)'),\n                             ('energcocinar2', '=1 main source of energy used for cooking electricity'),\n                             ('energcocinar3', '=1 main source of energy used for cooking gas'),\n                             ('energcocinar4', '=1 main source of energy used for cooking wood charcoal'),\n                             ('elimbasu1', '=1 if rubbish disposal mainly by tanker truck'),\n                             ('elimbasu2', '=1 if rubbish disposal mainly by botan hollow or buried'),\n                             ('elimbasu3', '=1 if rubbish disposal mainly by burning'),\n                             ('elimbasu4', '=1 if rubbish disposal mainly by throwing in an unoccupied space'),\n                             ('elimbasu5', '\"=1 if rubbish disposal mainly by throwing in river,  creek or sea\"'),\n                             ('elimbasu6', '=1 if rubbish disposal mainly other'),\n                             ('epared1', '=1 if walls are bad'),\n                             ('epared2', '=1 if walls are regular'),\n                             ('epared3', '=1 if walls are good'),\n                             ('etecho1', '=1 if roof are bad'),\n                             ('etecho2', '=1 if roof are regular'),\n                             ('etecho3', '=1 if roof are good'),\n                             ('eviv1', '=1 if floor are bad'),\n                             ('eviv2', '=1 if floor are regular'),\n                             ('eviv3', '=1 if floor are good'),\n                             ('dis', '=1 if disable person'),\n                             ('male', '=1 if male'),\n                             ('female', '=1 if female'),\n                             ('estadocivil1', '=1 if less than 10 years old'),\n                             ('estadocivil2', '=1 if free or coupled uunion'),\n                             ('estadocivil3', '=1 if married'),\n                             ('estadocivil4', '=1 if divorced'),\n                             ('estadocivil5', '=1 if separated'),\n                             ('estadocivil6', '=1 if widow/er'),\n                             ('estadocivil7', '=1 if single'),\n                             ('parentesco1', '=1 if household head'),\n                             ('parentesco2', '=1 if spouse/partner'),\n                             ('parentesco3', '=1 if son/doughter'),\n                             ('parentesco4', '=1 if stepson/doughter'),\n                             ('parentesco5', '=1 if son/doughter in law'),\n                             ('parentesco6', '=1 if grandson/doughter'),\n                             ('parentesco7', '=1 if mother/father'),\n                             ('parentesco8', '=1 if father/mother in law'),\n                             ('parentesco9', '=1 if brother/sister'),\n                             ('parentesco10', '=1 if brother/sister in law'),\n                             ('parentesco11', '=1 if other family member'),\n                             ('parentesco12', '=1 if other non family member'),\n                             ('idhogar', 'Household level identifier'),\n                             ('hogar_nin', 'Number of children 0 to 19 in household'),\n                             ('hogar_adul', 'Number of adults in household'),\n                             ('hogar_mayor', '# of individuals 65+ in the household'),\n                             ('hogar_total', '# of total individuals in the household'),\n                             ('dependency', 'Dependency rate, calculated = (number of members of the household younger than 19 or older than 64)/(number of member of household between 19 and 64)'),\n                             ('edjefe', 'years of education of male head of household, based on the interaction of escolari (years of education), head of household and gender, yes=1 and no=0'),\n                             ('edjefa', 'years of education of female head of household, based on the interaction of escolari (years of education), head of household and gender, yes=1 and no=0'),\n                             ('meaneduc', 'average years of education for adults (18+)'),\n                             ('instlevel1', '=1 no level of education'),\n                             ('instlevel2', '=1 incomplete primary'),\n                             ('instlevel3', '=1 complete primary'),\n                             ('instlevel4', '=1 incomplete academic secondary level'),\n                             ('instlevel5', '=1 complete academic secondary level'),\n                             ('instlevel6', '=1 incomplete technical secondary level'),\n                             ('instlevel7', '=1 complete technical secondary level'),\n                             ('instlevel8', '=1 undergraduate and higher education'),\n                             ('instlevel9', '=1 postgraduate higher education'),\n                             ('bedrooms', 'number of bedrooms'),\n                             ('overcrowding', '# persons per room'),\n                             ('tipovivi1', '=1 own and fully paid house'),\n                             ('tipovivi2', '\"=1 own,  paying in installments\"'),\n                             ('tipovivi3', '=1 rented'),\n                             ('tipovivi4', '=1 precarious'),\n                             ('tipovivi5', '\"=1 other(assigned,  borrowed)\"'),\n                             ('computer', '=1 if the household has notebook or desktop computer'),\n                             ('television', '=1 if the household has TV'),\n                             ('mobilephone', '=1 if mobile phone'),\n                             ('qmobilephone', '# of mobile phones'),\n                             ('lugar1', '=1 region Central'),\n                             ('lugar2', '=1 region Chorotega'),\n                             ('lugar3', '=1 region PacÃƒÂ­fico central'),\n                             ('lugar4', '=1 region Brunca'),\n                             ('lugar5', '=1 region Huetar AtlÃƒÂ¡ntica'),\n                             ('lugar6', '=1 region Huetar Norte'),\n                             ('area1', '=1 zona urbana'),\n                             ('area2', '=2 zona rural'),\n                             ('age', 'Age in years'),\n                             ('SQBescolari', 'escolari squared'),\n                             ('SQBage', 'age squared'),\n                             ('SQBhogar_total', 'hogar_total squared'),\n                             ('SQBedjefe', 'edjefe squared'),\n                             ('SQBhogar_nin', 'hogar_nin squared'),\n                             ('SQBovercrowding', 'overcrowding squared'),\n                             ('SQBdependency', 'dependency squared'),\n                             ('SQBmeaned', 'square of the mean years of education of adults (>=18) in the household'),\n                             ('agesq', 'Age squared')]\n\nall_features = [feature for feature, description in features_and_descriptions]\n\n\nhousehold_features_and_descriptions = [('v2a1', 'Monthly rent payment'),\n                             ('hacdor', '=1 Overcrowding by bedrooms'),\n                             ('rooms', 'number of all rooms in the house'),\n                             ('hacapo', '=1 Overcrowding by rooms'),\n                             ('v14a', '=1 has bathroom in the household'),\n                             ('refrig', '=1 if the household has refrigerator'),\n                             ('v18q1', 'number of tablets household owns'),\n                             ('r4h1', 'Males younger than 12 years of age'),\n                             ('r4h2', 'Males 12 years of age and older'),\n                             ('r4h3', 'Total males in the household'),\n                             ('r4m1', 'Females younger than 12 years of age'),\n                             ('r4m2', 'Females 12 years of age and older'),\n                             ('r4m3', 'Total females in the household'),\n                             ('r4t1', 'persons younger than 12 years of age'),\n                             ('r4t2', 'persons 12 years of age and older'),\n                             ('r4t3', 'Total persons in the household'),\n                             ('tamhog', 'size of the household'),\n                             ('tamviv', 'number of persons living in the household'),\n                             ('hhsize', 'household size'),\n                             ('paredblolad', '=1 if predominant material on the outside wall is block or brick'),\n                             ('paredzocalo', '\"=1 if predominant material on the outside wall is socket (wood,  zinc or absbesto\"'),\n                             ('paredpreb', '=1 if predominant material on the outside wall is prefabricated or cement'),\n                             ('pareddes', '=1 if predominant material on the outside wall is waste material'),\n                             ('paredmad', '=1 if predominant material on the outside wall is wood'),\n                             ('paredzinc', '=1 if predominant material on the outside wall is zink'),\n                             ('paredfibras', '=1 if predominant material on the outside wall is natural fibers'),\n                             ('paredother', '=1 if predominant material on the outside wall is other'),\n                             ('pisomoscer', '\"=1 if predominant material on the floor is mosaic,  ceramic,  terrazo\"'),\n                             ('pisocemento', '=1 if predominant material on the floor is cement'),\n                             ('pisoother', '=1 if predominant material on the floor is other'),\n                             ('pisonatur', '=1 if predominant material on the floor is  natural material'),\n                             ('pisonotiene', '=1 if no floor at the household'),\n                             ('pisomadera', '=1 if predominant material on the floor is wood'),\n                             ('techozinc', '=1 if predominant material on the roof is metal foil or zink'),\n                             ('techoentrepiso', '\"=1 if predominant material on the roof is fiber cement,  mezzanine \"'),\n                             ('techocane', '=1 if predominant material on the roof is natural fibers'),\n                             ('techootro', '=1 if predominant material on the roof is other'),\n                             ('cielorazo', '=1 if the house has ceiling'),\n                             ('abastaguadentro', '=1 if water provision inside the dwelling'),\n                             ('abastaguafuera', '=1 if water provision outside the dwelling'),\n                             ('abastaguano', '=1 if no water provision'),\n                             ('public', '\"=1 electricity from CNFL,  ICE,  ESPH/JASEC\"'),\n                             ('planpri', '=1 electricity from private plant'),\n                             ('noelec', '=1 no electricity in the dwelling'),\n                             ('coopele', '=1 electricity from cooperative'),\n                             ('sanitario1', '=1 no toilet in the dwelling'),\n                             ('sanitario2', '=1 toilet connected to sewer or cesspool'),\n                             ('sanitario3', '=1 toilet connected to  septic tank'),\n                             ('sanitario5', '=1 toilet connected to black hole or letrine'),\n                             ('sanitario6', '=1 toilet connected to other system'),\n                             ('energcocinar1', '=1 no main source of energy used for cooking (no kitchen)'),\n                             ('energcocinar2', '=1 main source of energy used for cooking electricity'),\n                             ('energcocinar3', '=1 main source of energy used for cooking gas'),\n                             ('energcocinar4', '=1 main source of energy used for cooking wood charcoal'),\n                             ('elimbasu1', '=1 if rubbish disposal mainly by tanker truck'),\n                             ('elimbasu2', '=1 if rubbish disposal mainly by botan hollow or buried'),\n                             ('elimbasu3', '=1 if rubbish disposal mainly by burning'),\n                             ('elimbasu4', '=1 if rubbish disposal mainly by throwing in an unoccupied space'),\n                             ('elimbasu5', '\"=1 if rubbish disposal mainly by throwing in river,  creek or sea\"'),\n                             ('elimbasu6', '=1 if rubbish disposal mainly other'),\n                             ('epared1', '=1 if walls are bad'),\n                             ('epared2', '=1 if walls are regular'),\n                             ('epared3', '=1 if walls are good'),\n                             ('etecho1', '=1 if roof are bad'),\n                             ('etecho2', '=1 if roof are regular'),\n                             ('etecho3', '=1 if roof are good'),\n                             ('eviv1', '=1 if floor are bad'),\n                             ('eviv2', '=1 if floor are regular'),\n                             ('eviv3', '=1 if floor are good'),\n                             ('hogar_nin', 'Number of children 0 to 19 in household'),\n                             ('hogar_adul', 'Number of adults in household'),\n                             ('hogar_mayor', '# of individuals 65+ in the household'),\n                             ('hogar_total', '# of total individuals in the household'),\n                             ('dependency', 'Dependency rate, calculated = (number of members of the household younger than 19 or older than 64)/(number of member of household between 19 and 64)'),\n                             ('edjefe', 'years of education of male head of household, based on the interaction of escolari (years of education), head of household and gender, yes=1 and no=0'),\n                             ('edjefa', 'years of education of female head of household, based on the interaction of escolari (years of education), head of household and gender, yes=1 and no=0'),\n                             ('meaneduc', 'average years of education for adults (18+)'),\n                             ('bedrooms', 'number of bedrooms'),\n                             ('overcrowding', '# persons per room'),\n                             ('tipovivi1', '=1 own and fully paid house'),\n                             ('tipovivi2', '\"=1 own,  paying in installments\"'),\n                             ('tipovivi3', '=1 rented'),\n                             ('tipovivi4', '=1 precarious'),\n                             ('tipovivi5', '\"=1 other(assigned,  borrowed)\"'),\n                             ('computer', '=1 if the household has notebook or desktop computer'),\n                             ('television', '=1 if the household has TV'),\n                             ('mobilephone', '=1 if mobile phone'),\n                             ('qmobilephone', '# of mobile phones'),\n                             ('lugar1', '=1 region Central'),\n                             ('lugar2', '=1 region Chorotega'),\n                             ('lugar3', '=1 region PacÃƒÂ­fico central'),\n                             ('lugar4', '=1 region Brunca'),\n                             ('lugar5', '=1 region Huetar AtlÃƒÂ¡ntica'),\n                             ('lugar6', '=1 region Huetar Norte'),\n                             ('area1', '=1 zona urbana'),\n                             ('area2', '=2 zona rural')]\n\nhousehold_features = [feature for feature, description in household_features_and_descriptions]\n\nindividual_features_and_descriptions = [('v18q', 'owns a tablet'),\n                             ('escolari', 'years of schooling'),\n                             ('rez_esc', 'Years behind in school'),\n                             ('dis', '=1 if disable person'),\n                             ('male', '=1 if male'),\n                             ('female', '=1 if female'),\n                             ('estadocivil1', '=1 if less than 10 years old'),\n                             ('estadocivil2', '=1 if free or coupled uunion'),\n                             ('estadocivil3', '=1 if married'),\n                             ('estadocivil4', '=1 if divorced'),\n                             ('estadocivil5', '=1 if separated'),\n                             ('estadocivil6', '=1 if widow/er'),\n                             ('estadocivil7', '=1 if single'),\n                             ('parentesco1', '=1 if household head'),\n                             ('parentesco2', '=1 if spouse/partner'),\n                             ('parentesco3', '=1 if son/doughter'),\n                             ('parentesco4', '=1 if stepson/doughter'),\n                             ('parentesco5', '=1 if son/doughter in law'),\n                             ('parentesco6', '=1 if grandson/doughter'),\n                             ('parentesco7', '=1 if mother/father'),\n                             ('parentesco8', '=1 if father/mother in law'),\n                             ('parentesco9', '=1 if brother/sister'),\n                             ('parentesco10', '=1 if brother/sister in law'),\n                             ('parentesco11', '=1 if other family member'),\n                             ('parentesco12', '=1 if other non family member'),\n                             ('instlevel1', '=1 no level of education'),\n                             ('instlevel2', '=1 incomplete primary'),\n                             ('instlevel3', '=1 complete primary'),\n                             ('instlevel4', '=1 incomplete academic secondary level'),\n                             ('instlevel5', '=1 complete academic secondary level'),\n                             ('instlevel6', '=1 incomplete technical secondary level'),\n                             ('instlevel7', '=1 complete technical secondary level'),\n                             ('instlevel8', '=1 undergraduate and higher education'),\n                             ('instlevel9', '=1 postgraduate higher education'),\n                             ('age', 'Age in years')]\n\nindividual_features = [feature for feature, description in individual_features_and_descriptions]\n\nsquared_features_and_descriptions = [('SQBescolari', 'escolari squared'),\n                             ('SQBage', 'age squared'),\n                             ('SQBhogar_total', 'hogar_total squared'),\n                             ('SQBedjefe', 'edjefe squared'),\n                             ('SQBhogar_nin', 'hogar_nin squared'),\n                             ('SQBovercrowding', 'overcrowding squared'),\n                             ('SQBdependency', 'dependency squared'),\n                             ('SQBmeaned', 'square of the mean years of education of adults (>=18) in the household'),\n                             ('agesq', 'Age squared')]\n\nsquared_features = [feature for feature, description in squared_features_and_descriptions]\n\nsquared_household_features_and_descriptions = [('SQBhogar_total', 'hogar_total squared'),\n                             ('SQBedjefe', 'edjefe squared'),\n                             ('SQBhogar_nin', 'hogar_nin squared'),\n                             ('SQBovercrowding', 'overcrowding squared'),\n                             ('SQBdependency', 'dependency squared'),\n                             ('SQBmeaned', 'square of the mean years of education of adults (>=18) in the household')]\n\nsquared_household_features = [feature for feature, description in squared_household_features_and_descriptions]\n\nsquared_individual_features_and_descriptions = [('SQBescolari', 'escolari squared'),\n                             ('SQBage', 'age squared'),\n                             ('agesq', 'Age squared')]\n\nsquared_individual_features = [feature for feature, description in squared_individual_features_and_descriptions]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f73584dce54d46f065b654b31a6855e1f73ecc6d","trusted":false},"cell_type":"code","source":"# Verify that seemingly duplicate attributes, SQBage and agesq, \n# are in fact duplicates and contain duplicate values in both train and test sets.\n\nfor df, name in [(train, 'train'), (test, 'test')]:\n    assert df.agesq.equals(df.SQBage), f\"agesq is not equivalent with SQBage in the {name} set\"\n    \n# Remove duplicate column agesq from feature lists\nlists_with_agesq = [features_and_descriptions, squared_features_and_descriptions, squared_individual_features_and_descriptions]\nfor feature_list in lists_with_agesq:\n    try:\n        feature_list.remove(('agesq', 'Age squared'))\n    except:\n        continue\n\n# Verify we deleted them all\nfor feature_list in lists_with_agesq:\n    assert 'agesq' not in {x for x, y in feature_list}, 'Duplicated column agesq is still in feature list.'","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"97d46a8db78062a5c64c34d1c12de25f709acc06"},"cell_type":"markdown","source":"## Attributes with NaNs"},{"metadata":{"_uuid":"3202cbc740d00c2be4dd10e63d72bca62563120c","trusted":false},"cell_type":"code","source":"def print_nan_counts(df):\n    nan_counts = df.isna().sum()\n    print(nan_counts[nan_counts > 0].sort_values(ascending = False))\n    \nprint_nan_counts(train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f348abb8d75c0b1b72155174f0c0a4802aea4a33"},"cell_type":"markdown","source":"Good news is that for most features, there aren't a lot of missing values. Let's figure out how best to handle each of these.\n\n### rez_esc\n\nFirst, `rez_esc`. The description of this field is \"Years behind in school\".  Let's take a look to see if we can figure out why there are so many empty values here. My intuition is that it has something to do with whether the individual is in school or not. For example, a two year old can't really be \"behind in school\". Nor can a graduate. We'll also check to see if there's been any discussion for how this field was created by IDB."},{"metadata":{"_uuid":"b1053930e2620d1a9dfa0ea3813d8dd0da9e9251","trusted":false},"cell_type":"code","source":"print('Stats for potentially related characteristics for individuals where rez_esc isna.')\nprint(train[train.rez_esc.isna()][['age', 'escolari', 'meaneduc']].describe())\n\nprint('')\nprint('Stats for potentially related characteristics for individuals where rez_esc is not null.')\nprint(train[train.rez_esc.notna()][['age', 'escolari', 'meaneduc']].describe())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dbd2dd09cf66cea36c095f29fa799c0cf09b501f"},"cell_type":"markdown","source":"The thing that jumps out from this analysis is that range for the age of individuals where `rez_esc` is not na is between 7 and 17. The other variables don't show an obvious difference. Indeed, IDB clarifies in a discussion about this variable: \n\n> This variable is only collected for people between 7 and 19 years of age and it is the difference between the years of education a person should have and the years of education he/she has. it is capped at 5.\n\nAs a first step, then, let's create a custom transformer that fills in 0 for anyone who is younger than 7 or older than 19."},{"metadata":{"_uuid":"73c042da8a18c6803500e79963825fc959e98fb8","trusted":false},"cell_type":"code","source":"class ZerofillRezEscOutOfBounds(BaseEstimator, TransformerMixin):\n    \"\"\"Zerofill rez_esc for any row where age is < 7 or age is > 19.\"\"\"\n    def __init__(self):\n        pass\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X, y=None):\n        X.loc[X.rez_esc.isna() & ((X.age < 7) | (X.age > 19)), 'rez_esc'] = 0\n        return X\n        \n    \nzerofiller = ZerofillRezEscOutOfBounds()\ntrain_zerofilled = zerofiller.transform(train.copy())\n\nassert not train_zerofilled[(train_zerofilled.age < 7) | (train_zerofilled.age > 19)].rez_esc.isna().any(), 'There are individuals younger than 7 or older than 19 with a na value for rez_esc.'\nprint(f\"There are {train_zerofilled.rez_esc.isna().sum()} individuals with a na value for rez_esc.\")\nprint(f\"There are {train_zerofilled[(train_zerofilled.age < 7) | (train_zerofilled.age > 19)].rez_esc.isna().sum()} individuals younger than 7 or older than 19 with a na value for rez_esc.\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"70d9ed3f648fdd7eaac63f378f260eb2f1b81034"},"cell_type":"markdown","source":"For the remaining 350 individuals between 7 and 19, we'll begin by simply imputing their values as part of our data preprocessing pipeline step. If needed, we might look further into this and see if can make a more meanginful inference from other data. For now, though, we'll spend the time on getting a working model and just use a basic imputer.\n\nI see in the IDB response that this should be capped at 5. We should check to see that everyone is capped correctly or evaluate whether this matters."},{"metadata":{"trusted":false,"_uuid":"599a50a98c57a85886a5b39f33ea732b0b1d5fa3"},"cell_type":"code","source":"train.loc[train.rez_esc > 5, id_features + individual_features].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"9238542967aa0d29f2f149ec1457ae64c09e2539"},"cell_type":"code","source":"test.loc[test.rez_esc > 5, id_features + individual_features].head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3183af17c5afd7c233dbd642737bc7666a6b913d"},"cell_type":"markdown","source":"It looks like there's one value in the test set that has a value higher than the cap. I would assume that a value of 99 should just be set to the max, but in this case, this seems like bad input. It's for a 8-year old boy, who has an `instlevel2`, or \"incomplete primary\". This person actually seems right on track. Even if he was just starting kindergarten, he wouldn't be 5 years behind in school. For now, we'll set anything higher than 5 to 0."},{"metadata":{"trusted":false,"_uuid":"3f8295e36d15c16d240d051a23e392c7d5b879d5"},"cell_type":"code","source":"class ZeroMaxRezEsc(BaseEstimator, TransformerMixin):\n    \"\"\"Zero out rez_esc for any row where rez_esc is > the prescribed max of 5.\"\"\"\n    def __init__(self):\n        pass\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X, y=None):\n        X.loc[X.rez_esc > 5, 'rez_esc'] = 0\n        return X\n        \n    \nzero_rez_esc = ZeroMaxRezEsc()\ntest_zero_rez_esc = zero_rez_esc.transform(test.copy())\n\nassert len(test_zero_rez_esc[test_zero_rez_esc.rez_esc > 5]) == 0, 'There are individuals with greater than 5 rez_esc'","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bfe0c95b55547ec4674c4fe95b135682e2f65776"},"cell_type":"markdown","source":"### v18q1\n\nNext, let's look at **v18q1**. The description for this field is \"number of tablets household owns\". There's also a field `v18q`, which represents if the individual \"owns a tablet\".  Theoretically, `v18q1` would be a derivative field summing the `v18q` for the individuals in the household. It's possible a blank `v18q1` could  represent households without a tablet. Let's see if we can confirm these before plowing forward."},{"metadata":{"_uuid":"fbab84061930d0971e4b0ec3b01161ce16a03379","trusted":false},"cell_type":"code","source":"train.loc[train.v18q1 > 1, ['Id', 'idhogar', 'parentesco1', 'v18q', 'v18q1']].head(20)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"59f40ce07f0c014d5d51c72941bfda22982a17d7"},"cell_type":"markdown","source":"Wrong! We can immediately see that the hypothesis that v18q1, the \"number of tablets household owns\" is *not* the sum of `v18q` for each individual in the household. Household 28ec0c747 lists 2 for `v18q1`, but a 1 for each of the 6 individual's `v18q`.\n\nI'm now realizing that `v18q` is probably a boolean field indicating whether the household for that individual owns a tablet, similar to many of the other fields in the dataset. We can check."},{"metadata":{"_uuid":"351c45cc2b61fcd350a8b7925824bf788e90e75d","trusted":false},"cell_type":"code","source":"train.v18q.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2c6b4173fadf252dadae28e12b7a67a1a0b4f2ac"},"cell_type":"markdown","source":"Bingo! All of the values in the `v18q` column are 0s or 1s. ***And...*** that 7342 count rings a bell. In fact, it's the count of the number of NaN values in the `v18q1` column. It's now pretty reasonable that any field that has a 0 in `v18q` is part of a houseold that doesn't own a tablet and, therefore, the count of tablets in the house, `v18q1` should be set to 0. First, let's make sure this isn't a fluke and that there are households with more than 1 tablet. "},{"metadata":{"_uuid":"97a214336e0b398a3a42f4b5e0934028050dd6d9","trusted":false},"cell_type":"code","source":"print(train.v18q1.value_counts())\nprint(len(train.loc[(train.v18q == 0 & train.v18q1.isna())]))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3731be92d5c8b44227322eb7f3019812f09f5f4e"},"cell_type":"markdown","source":"Yep, I feel comfortable now creating a transformer to set those 0s. And, moreover, we might want to consider excluding `v18q` from the final features used in the models, since it's redundant with `v18q1`—0 for `v18q1` is equivalent with 0 `v18q` and anything else is equivalent to 1 `v18q`"},{"metadata":{"_uuid":"82c97a1533b9783e76e83cc2e652ebb42d8a0eb1","trusted":false},"cell_type":"code","source":"class ZerofillV18q1ForFalseV18Q(BaseEstimator, TransformerMixin):\n    \"\"\"Zerofill v18q1 for any row where v18q is 0.\"\"\"\n    def __init__(self):\n        pass\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X, y=None):\n        X.loc[X.v18q == 0, 'v18q1'] = 0\n        return X\n        \n    \nzerofiller = ZerofillV18q1ForFalseV18Q()\ntrain_zerofilled = zerofiller.transform(train.copy())\n\nassert not train_zerofilled.v18q1.isna().any(), 'There are individuals with na value for v18q1.'","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"69bee67513cffd06fc578250e59689f2f122e734"},"cell_type":"markdown","source":"### v21a1\n\nNext up, let's look at the other attribute that had a lot of NaN values, `v2a1`, described as \"Monthly rent payment\". Why would someone not have a monthly rent payment? I can only think of a few valid reasons, they own a house outright, live with someone else rent-free, or are homeless. If we can't figure out how to fill in some of these values, we might have to ignore this field. That would be a shame, though, since there's probably a decent correlation with a household's poverty level and their rent (or whether they own their home). \n\nAgain, the representitive from IDB offers some help in the [Kaggle discussions](https://www.kaggle.com/c/costa-rican-household-poverty-prediction/discussion/61403#360694):\n\n> In the example of v2a1 you should first filter by tipoviv3. Most households that don't have information will be households that do not rent or are not paying loans (tipoviv2). If a household is tipoviv3 and does not have information then you need to make a choice about the treatment of the variable either assume 0, impute a value or not use the variable. Basically in those cases we don't know the value.\n\nLet's see what we can find out following that path."},{"metadata":{"_uuid":"ba5c19027bf802c96bef101b0985b47f93890e22","trusted":false},"cell_type":"code","source":"for v in ['tipovivi1', 'tipovivi2', 'tipovivi3', 'tipovivi4', 'tipovivi5']:\n    print(f\"Value counts for {v} for individuals with missing v2a1:\")\n    print(train.loc[train.v2a1.isna(), v].value_counts())\n    print('')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1bd355971a8df083f2390575b3cac8dcb3be309c"},"cell_type":"markdown","source":"We can see that most of the individuals who have a missing `v2a1` have a 1 for `tipovivi1`. Looking at the data definitions, this indicates individuals who \"own and fully paid house\". This corresponds to one of the hypotheses we had for why `v2a1` could be legitimately be blank. For these, we should fill with 0s.\n\nWhat about the others with a missing `v2a1`? A few have a 1 for `tipovivi4` and the rest have a 1 for `tipovivi5`. These represent \"precarious\" and \"other(assigned,  borrowed)\", respectively. I'm not sure exactly what precarious refers to, but `tipovivi5`, homes that are assigned or borrowed probably can also be assigned a 0 value for `v2a1`. Since `tipovivi4` represents such a small percentage of this set, we could choose to impute those values or set them to 0 as well. As a first round, we're going to leave them as blank and impute them as part of our preprocessing pipeline, but should flag this as something we might want to revisit as we try to squeeze more performance from our models. We should probably do some research to find out if that is a common designation for housing. "},{"metadata":{"_uuid":"7b36e87a53e6730f8ac30804a56c9bbed9ae4489","trusted":false},"cell_type":"code","source":"class ZerofillV2a1(BaseEstimator, TransformerMixin):\n    \"\"\"Zerofill v2a1 if tipovivi1 or tipovivi5 is 1.\"\"\"\n    def __init__(self):\n        pass\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X, y=None):\n        X.loc[(X.v2a1.isna()) & ((X.tipovivi1 == 1) | (X.tipovivi5 == 1)), 'v2a1'] = 0\n        return X\n        \n    \nzerofiller = ZerofillV2a1()\ntrain_zerofilled = zerofiller.transform(train.copy())\n\n# Assert that everyone that isn't a tipovivi4 doesn't have a NaN value for v2a1\nassert not train_zerofilled[train_zerofilled.tipovivi4 == 0].v2a1.isna().any(), 'There are individuals with na value for v2a1.'","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3424eda6dfe7e698fdef83096878ab81f5b03487"},"cell_type":"markdown","source":"Let's verify we've handled all the missing values we can."},{"metadata":{"_uuid":"2411df3a84fccf778224a9056701111e7a658d2f","trusted":false},"cell_type":"code","source":"nan_pipeline = make_pipeline(ZerofillRezEscOutOfBounds(), ZerofillV18q1ForFalseV18Q(), ZerofillV2a1())\ntrain_nan = nan_pipeline.fit_transform(train.copy())\n\nprint_nan_counts(train_nan)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d7d9d4eefd3c675a52d1743faac1f5245b35d7de"},"cell_type":"markdown","source":"We've addressed all of the columns with a lot of missing values. We will address the remaining missing values by imputing them as part of our preprocessing pipeline. And the other two attributes are so small that we can either impute them as well or drop those rows. For later tuning, we may want to look at them further to see if we can make some smart decisions about what they should be. For now, I think we've successfully built some transformer to handle NaN values. Next up, let's handle text and categorical attributes."},{"metadata":{"_uuid":"c298dfcbcb22aee0393484be5beb1f8eaa4d2cea"},"cell_type":"markdown","source":"## Text & Categorical Attributes\n\nThe way we typically handle text columns is eather one-hot encoding them or transforming them into an obvious numeric value, usually if the values are ordinal in nature. Let's see what we've got here."},{"metadata":{"_uuid":"c9b8c66ffc52c741a473a81bcab58c1c9d1d7f67","trusted":false},"cell_type":"code","source":"train.select_dtypes(include='object').head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ff40aa50d9f3af69023cd9bc9229083ac586a31e"},"cell_type":"markdown","source":"Aside from the IDs of the individual and household, there are really three columns, each that look to have a mix between numbers and yes, nos. According the data descriptions:\n\n> dependency, Dependency rate, calculated = (number of members of the household younger than 19 or older than 64)/(number of member of household between 19 and 64)\n\n> edjefe, years of education of male head of household, based on the interaction of escolari (years of education), head of household and gender, yes=1 and no=0\n\n> edjefa, years of education of female head of household, based on the interaction of escolari (years of education), head of household and gender, yes=1 and no=0\n\nIt seems like for `edjefe` and `edjefa` they added \"no\" for anyone who had 0 years of education and \"yes\" for anyone with 1. We can easily transform those back to numeric columns. I am a little suspicious of why they would do the latter and would want to validate with the stakeholders that they didn't transform anyone with more than 1 years of education into a \"yes\". For now, we'll respect the data definition.\n\nDependency is a rate, though and doesn't list this rule. Luckily, I found in the discussion that this is a general rule and can be applied to the `dependency` field as well:\n\n> yes it is a general rule we applied to the database. In these three cases (edjefe, edjefa, dependency), when converting formats the variables were not delabeled for some reason. In the database yes is always 1 and no is always 0."},{"metadata":{"_uuid":"276e3a32e94e80a1deeaf6ca4083a8df2f62bf6b","trusted":false},"cell_type":"code","source":"class TransformYesNoToNumeric(BaseEstimator, TransformerMixin):\n    \"\"\"Transform edjefe, edjefa, and dependencey yes/no values to numeric values.\n    yes=1 and no=0.\"\"\"\n    def __init__(self):\n        pass\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X, y=None):\n        replacements = {'yes': 1, 'no': 0}\n        columns = [('edjefe', 'uint8'), \n                   ('edjefa', 'uint8'), \n                   ('dependency', 'float16')]\n    \n        for column, converted_type in columns:    \n            if X[column].dtype == 'object':\n                X[column] = X[column].replace(replacements).astype(converted_type)\n        \n        return X\n\nyes_no_transformer = TransformYesNoToNumeric()\ntrain_yes_no_transformed = yes_no_transformer.transform(train.copy())\n\n# Assert that all columns aside from Id and idhogar are numeric\nassert train_yes_no_transformed.select_dtypes(include='object').columns.values.tolist() == ['Id', 'idhogar'], 'There are columns aside from Id and idhogar that are type object.'","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"00078f8337509c77332756e62552e883daa5c3e6"},"cell_type":"markdown","source":"## Aggregating Individual Features to Household Features\n\nSince the primary business objective is concerned with households, we're going to be training our model just on households. It would be a shame to lose all of the information embedded in all of the individual attributes. To address this, let's add some features that aggregate the individual attributes to the household level by applying a few different statistics"},{"metadata":{"_uuid":"08cd0f1c16b87a219925d390233e5694060c7e03","trusted":false},"cell_type":"code","source":"class AggregateIndividualFeatures(BaseEstimator, TransformerMixin):\n    \"\"\"Aggregate individual features per household by grouping them by idhogar and\n    applying sum, min, max, std.\n    \n    New features will be added as {feature name}-{aggregation type}, e.g. age-std\n    \"\"\"\n    def __init__(self):\n        self.excluded_individual_features = ['v18q', 'male', 'female', 'Target']\n        self.individual_features_to_aggregate = [feature for feature in individual_features if feature not in self.excluded_individual_features]\n        self.aggregations = ['sum', 'min', 'max', 'std', 'mean', 'median']\n        self.aggregated_features = []\n            \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X, y=None):\n        if 'instlevel' in X.columns and 'instlevel' not in self.individual_features_to_aggregate:\n            self.individual_features_to_aggregate.append('instlevel')\n            \n        aggregates = X[id_features + self.individual_features_to_aggregate].groupby(\"idhogar\").agg(self.aggregations)\n        aggregates.columns = ['-'.join(column).strip() for column in aggregates.columns.values]\n        self.aggregated_features = aggregates.columns.values.tolist()\n    \n        return X.merge(aggregates, on='idhogar', how='left')\n\n    \nind_aggregator = AggregateIndividualFeatures()\ntrain_ind_aggregated = ind_aggregator.transform(train.copy())\n\n# To be exhaustive, we could loop through all households, but this spost check should \n# give us a lot more confidence and be a lot faster.\nhousehold_id = '6893e65ca'\n\nfor aggregation in ind_aggregator.aggregations:\n    manual = train.loc[train.idhogar == household_id].age.apply(aggregation)\n    aggregated = train_ind_aggregated.loc[(train_ind_aggregated.idhogar == household_id), [f\"age-{aggregation}\"]].iloc[0].values[0]\n    assert manual == aggregated, f\"Calculated {aggregation} for age doesn't match aggregation for household {household_id}\"\n    print(f\"Calculated {aggregation} for age, {aggregated}, matches manual aggregation, {manual}, for household {household_id}\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dfad60c3b0aa5d2b25713afa63b29360ac1ffadc"},"cell_type":"markdown","source":"## Ordinalize features"},{"metadata":{"_uuid":"39420d08da3be73d3db5e7a7159a4e1135f5083a"},"cell_type":"markdown","source":"If we look at some of the features names, we can see that there are some feature groups that have been broken out into a series of boolean values, e.g. instlevel[1-9]. This makes sense for any categorical feature when there isn't really any order to the categories. In the case of instlevel, though, each category is related to other categories in an orderly fashion. If we remap these to values that capture the scale, we'll be able to reduce the dimensionality of our models as well as pass it more information. \n\nFor now, we'll use a linear scaling, though I could also see an argument that exponential growth could be more accurate in the case of instlevel. "},{"metadata":{"_uuid":"f91c0c20eaaf502645678c7f74a2fd2b7ea2dfe8","trusted":false},"cell_type":"code","source":"class Ordinalizer(BaseEstimator, TransformerMixin):\n    \"\"\"Add an ordered numeric attribute for previously broken out boolean attributes\"\"\"\n    \n    def __init__(self):\n        pass\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X, y=None):\n        features = ['instlevel', 'epared', 'etecho', 'eviv']\n\n        for feature in features:\n            columns = [column for column in all_features if column.startswith(feature)]\n            X[feature] = X.loc[:, columns].idxmax(1).apply(lambda x: columns.index(x) + 1)\n            \n            # Don't drop instleveli columns since we'll use them in household aggregation\n            if feature != 'instlevel':\n                X.drop(columns, axis=1, inplace=True)\n                \n        return X\n    \n    \n# Tests\nordinalizer = Ordinalizer()\ntrain_ordinalized = ordinalizer.transform(train.copy())\n    \n# Test 10 random rows\nfor index, individual in train_ordinalized.sample(10).iterrows():\n    for feature in ['instlevel', 'epared', 'etecho', 'eviv']:\n        assert train.loc[index, f\"{feature}{individual[feature]}\"] == 1, f\"Ordinalized {feature} doesn't match original boolean instlevel\"\n                      \n# Test dropped columns\nfor feature in ['epared', 'etecho', 'eviv']:\n    feature_columns = [column for column in train_ordinalized.columns if column.startswith(feature)]\n    assert len(feature_columns) == 1, f\"{feature} column is still present.\"\n\n# Test AggregateIndividualFeatures picks up instlevel for aggregation\nhousehold_id = \"6893e65ca\"\n\ninstlevel_ind_aggregator = AggregateIndividualFeatures()\ntrain_instlevel_ind_aggregated = instlevel_ind_aggregator.transform(train_ordinalized)\n                      \nassert 'instlevel-sum' in instlevel_ind_aggregator.aggregated_features, \"instlevel is not in aggregated features.\"\n                      \nfor aggregation in instlevel_ind_aggregator.aggregations:\n    manual = train_ordinalized.loc[train_ordinalized.idhogar == household_id].instlevel.apply(aggregation)\n    aggregated = train_instlevel_ind_aggregated.loc[(train_instlevel_ind_aggregated.idhogar == household_id), [f\"instlevel-{aggregation}\"]].iloc[0].values[0]\n    assert manual == aggregated, f\"Calculated {aggregation} for instlevel doesn't match aggregation for household {household_id}\"\n    print(f\"Calculated {aggregation} for instlevel, {aggregated}, matches manual aggregation, {manual}, for household {household_id}\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"37cf7df648ade0596bb0fd34696488d0f55365db"},"cell_type":"markdown","source":"## Feature engineering"},{"metadata":{"trusted":false,"_uuid":"d5dc680014be7d60054bb9d7fd04b21baa945cde"},"cell_type":"code","source":"class FeatureCreator(BaseEstimator, TransformerMixin):\n    \"\"\"Adds additional features\"\"\"\n    \n    def __init__(self):\n        self.created_features = []\n    \n    def fit(self, X, y=None):\n        self.created_features = []\n        \n        self._add_feature('rent_per_room', lambda X: X.v2a1 / X.rooms)\n        self._add_feature('rent_per_hhsize', lambda X: X.v2a1 / X.hhsize)\n        self._add_feature('tablets_per_hhsize', lambda X: X.v18q1 / X.hhsize)\n        self._add_feature('tablets_per_adult', lambda X: X.v18q1 / X.hogar_adul)\n        self._add_feature('escolari-mean_to_age_mean', lambda X: X['escolari-mean'] / X['age-mean'])\n        self._add_feature('rez_esc-mean_to_age_mean', lambda X: X['rez_esc-mean'] / X['age-mean'])\n        self._add_feature('males_to_females', lambda X: X.r4h3 / X.r4m3)\n        self._add_feature('under12_to_over12', lambda X: X.r4t1 / X.r4t2)\n        \n        return self\n    \n    def transform(self, X, y=None):\n        for label, calculation in self.created_features:\n            X[label] = calculation(X)\n        \n#         # Address NaNs and infinity probably on a per feature basis\n# #         X.replace({'males_to_females': {np.inf: np.nan},\n# #                    'under12_to_over12': {np.inf: np.nan}}, inplace=True)\n        X.replace(np.inf, np.nan, inplace=True)\n        \n        return X\n        \n    def _add_feature(self, label, calculation):\n        self.created_features.append((label, calculation))\n\n        \n# Tests\nind_aggregator = AggregateIndividualFeatures()\ntrain_ind_aggregated = ind_aggregator.transform(train.copy())\nfeature_creator = FeatureCreator()\nfeatures_created = feature_creator.fit_transform(train_ind_aggregated.copy())\n\nassert features_created.rent_per_room.equals(features_created.v2a1 / features_created.rooms), \"rent_per_room wasn't created correctly.\"\nassert len(features_created.columns) - len(train_ind_aggregated.columns) == 8, \"Didn't create the expected number of new features\"\n\nfeatures_created[['rent_per_room', 'v2a1', 'rooms']].head()\nassert len(features_created.columns[features_created[features_created == np.inf].any()]) == 0, \"inifinity values present\"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"43af838c15f23c0ac54ffb15745c1e9ec0ba0ec7"},"cell_type":"markdown","source":"## Household split"},{"metadata":{"_uuid":"0fa99768a9af39c0634b232716ba8d6b58f6bc52","trusted":false},"cell_type":"code","source":"household_groups = train.groupby('idhogar')\n\n# Verify that all the rows in a household group have the same value for the household features\nassert len(household_groups) == len(train.idhogar.unique()), \"Length of household groups is not the same as the number of unique household ids.\"\nassert not household_groups[[\"idhogar\"] + household_features].var().any().any(), \"Not all rows for a group have the same values for a household feature\"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2ffe886ee9053380d85f0e4e8bac86a08c06b355","trusted":false},"cell_type":"code","source":"# Todo: I don't love that the separating of the target variable happens\n# here. Seems like it should be an explicit step in the pipeline or that\n# We should have a better way of getting the household targets. Probably \n# the latter.\n\nclass ExtractHouseholds(BaseEstimator, TransformerMixin):\n    \"\"\"Returns dataframe for households.\"\"\"\n    \n    def __init__(self, individual_aggregator=None, feature_creator=None):\n        self.individual_aggregator = individual_aggregator\n        self.feature_creator = feature_creator\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X, y=None):\n        self.included_features = household_features + squared_household_features\n        \n        if self.individual_aggregator:\n            self.included_features = self.included_features + self.individual_aggregator.aggregated_features\n            \n        if self.feature_creator:\n            self.included_features = self.included_features + [feature for feature, _ in self.feature_creator.created_features]\n            \n        # Don't include features that have been dropped earlier in the pipeline\n        self.included_features = [feature for feature in self.included_features if feature in X]\n    \n        return X.groupby('idhogar')[self.included_features].first()\n \n# Tests\nhousehold_extractor = ExtractHouseholds()\ntrain_households = household_extractor.transform(train.copy())\nassert len(train_households) == len(train.idhogar.unique()), f\"Extracted households length ({len(train_households)}) doesn't match unique household indentifiers in train set ({len(train.idhogar.unique())}).\"\n\nhousehold_extractor = ExtractHouseholds(individual_aggregator=instlevel_ind_aggregator)\ntrain_households = household_extractor.transform(train_instlevel_ind_aggregated.copy())\nassert len(train_households) == len(train_instlevel_ind_aggregated.idhogar.unique()), f\"Extracted households length ({len(train_households)}) doesn't match unique household indentifiers in train set ({len(train_instlevel_ind_aggregated.idhogar.unique())}).\"\n\ntrain_households.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a81e963fc65e0a1027c664803d61bd03ac9da8e4"},"cell_type":"markdown","source":"# Shortlist Models\n\nAt this point, we can get a basic model trained and we can begin trying to find a shortlist of potential models to fine tune."},{"metadata":{"_uuid":"de862f25776ddb56cea8bc770fad600cf6afed71","trusted":false},"cell_type":"code","source":"import scipy\n\n# Transformers\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\n\n# Cross validation\nfrom sklearn.model_selection import cross_validate,  RandomizedSearchCV\nfrom sklearn.metrics.scorer import make_scorer\nfrom sklearn.metrics import f1_score\n\n# Models\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.neural_network import MLPClassifier","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ac9673b68878522db53b2c6826335f04b91c2243","trusted":false},"cell_type":"code","source":"class FeatureSelector(BaseEstimator, TransformerMixin):\n    \"\"\"Returns NumPy array corresponding to the selected features.\"\"\"\n    \n    def __init__(self, selected_features=[], excluded_features=[]):\n        self.selected_features = selected_features\n        self.excluded_features = excluded_features\n        \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X, y=None):\n        if not self.selected_features:\n            self.selected_features = X.columns.tolist()\n            \n        if self.excluded_features:\n            self.selected_features = [feature for feature in self.selected_features if feature not in self.excluded_features]\n\n        return X[self.selected_features]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"26721215029e7e899176f98de2fa7da1ac91848d","trusted":false},"cell_type":"code","source":"from sklearn.exceptions import NotFittedError\n\nclass ModelEvaluator():\n    def __init__(self, pipeline, estimators, features=[], excluded_features=[]):\n        self.pipeline = pipeline\n        self.estimators = [self._init_estimator(estimator) for estimator in estimators]\n        self._preprocessed = False\n        self.features = features\n        self.excluded_features = excluded_features\n        \n        # Ensure we're working with clean data and that none of our previous work\n        # has leaked into the datasets.\n        self.train, self.test = import_train_test()\n        self.results = []\n        \n        \n    def _init_estimator(self, estimator):\n        return {'estimator': estimator,\n                'fit': False,\n                'cv_results': {},\n                'feature_importances': None,\n                'tuned': False,\n                'tuned_best_estimator': None,\n                'tuned_best_score': None,\n                'tuned_best_params': None,\n                'tuned_cv_results': {}}\n        \n    def _preprocess_data(self):\n        try:\n            self.pipeline.named_steps['featureselector'].selected_features = self.features\n            self.pipeline.named_steps['featureselector'].excluded_features = self.excluded_features\n        except KeyError:\n            pass\n        \n        self._trainX_preprocessed = pipeline.fit_transform(self.train.copy())\n\n        # Since X is now households, we need the corresponding targets to those households\n        self._trainY_preprocessed = self.train.copy().groupby('idhogar')['Target'].first()\n        \n        self._testX_preprocessed = self.pipeline.transform(self.test.copy())\n\n\n        self._preprocessed = True\n        \n        \n    def evaluate(self):\n        if not self._preprocessed:\n            self._preprocess_data()\n            \n        print(f\"Evaluating on {self._trainX_preprocessed.shape[1]} features and {self._trainX_preprocessed.shape[0]} samples.\")\n        print(\"\")\n        \n        for estimator in self.estimators:\n            estimator['cv_results'] = self._cross_validate(estimator)\n            estimator['feature_importances'] = self._calculate_feature_importances(estimator)\n            \n        self._print_cv_results()\n\n        \n    def _cross_validate(self, estimator):\n        return cross_validate(estimator['estimator'], \n                              self._trainX_preprocessed, \n                              self._trainY_preprocessed, \n                              cv=5, \n                              scoring='f1_macro',\n                              n_jobs=-1,\n                              return_train_score=False)\n        \n        \n    def _fit_estimator(self, estimator):\n        estimator['estimator'].fit(self._trainX_preprocessed, self._trainY_preprocessed)\n        estimator['fit'] = True\n\n        \n    def _calculate_feature_importances(self, estimator):\n        if not estimator['fit']:\n            self._fit_estimator(estimator)\n            \n        try:\n            return self._feature_importances_dataframe(estimator['estimator'].feature_importances_)\n        except AttributeError:\n            return None\n            \n        \n    def _feature_importances_dataframe(self, feature_importances):\n        # Get the features we actually trained on\n        # Todo - consider moving this to preprocessing step\n        trained_features = self.pipeline.named_steps['featureselector'].selected_features\n\n        return pd.DataFrame(data={'importance': feature_importances}, \n                            index=trained_features).sort_values(by='importance', ascending=False)\n    \n    def _print_cv_results(self):\n        for estimator in self.estimators:\n            print(f\"Scores for {estimator['estimator']}\")\n            print(f\"Mean Macro F1 Score: {estimator['cv_results']['test_score'].mean()}, SD={estimator['cv_results']['test_score'].std()}\")\n            print(\"\")\n                  \n                  \n    def tune_hyperparameters(self, estimator, param_dist, n_iter=20, cv=5):\n        random_search = RandomizedSearchCV(estimator['estimator'], \n                                           param_distributions=param_dist,\n                                           n_iter=n_iter, \n                                           cv=cv, \n                                           scoring='f1_macro',\n                                           n_jobs=-1,\n                                           verbose=1)\n                  \n        random_search.fit(self._trainX_preprocessed, self._trainY_preprocessed)\n                  \n        estimator['tuned'] = True\n        estimator['tuned_best_estimator'] = random_search.best_estimator_\n        estimator['tuned_best_score'] = random_search.best_score_\n        estimator['tuned_cv_results'] = random_search.cv_results_\n        estimator['tuned_best_params'] = random_search.best_params_\n        estimator['tuned_best_index'] = random_search.best_index_\n                  \n        return random_search\n                  \n        \n    def prepare_submissions(self):\n        \"\"\"Prepare a submission csv for every tuned estimator\"\"\"\n        for estimator in self.estimators:\n            if not estimator['tuned']:\n                continue\n            \n            self._save_submission(estimator)\n    \n                  \n    def _save_submission(self, estimator):\n        if not estimator['tuned_best_estimator']:\n            pass\n                  \n        # Make predictions on test set\n        predictions = self.test.copy().groupby('idhogar').first().reset_index()\n        predictions['Target'] = estimator['tuned_best_estimator'].predict(self._testX_preprocessed)\n\n        # # Merge household predictions back to individuals in test set\n        test_results = test[['Id', 'idhogar']].copy()\n        test_results = test_results.merge(predictions[['idhogar', 'Target']].copy(), on=\"idhogar\", how=\"left\").drop(\"idhogar\", axis=1)\n\n        assert test_results.shape[0] == self.test.shape[0], \"Number of results don't match number of test samples.\"\n\n        filename = f\"{estimator['tuned_best_estimator'].__class__.__name__}-tuned-{estimator['tuned_best_score']:.3f}.csv\"\n        self._save_predictions(test_results, filename)\n        \n                  \n    def _save_predictions(self, predictions, filename):\n        \"\"\"Write results to csv file.\"\"\"\n        predictions.to_csv(filename, index=False)\n        print(f\"Wrote results to {filename}\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"93c5a0cdcc809956e64ad43083b360a8327fca8a","trusted":false},"cell_type":"code","source":"# store this so we can pass it into household extractor\nindividual_aggregator = AggregateIndividualFeatures()\nfeature_creator = FeatureCreator()\n\n# Create pipeline\npipeline = make_pipeline(ZerofillRezEscOutOfBounds(),\n                         ZeroMaxRezEsc(),\n                         ZerofillV18q1ForFalseV18Q(), \n                         ZerofillV2a1(),\n                         TransformYesNoToNumeric(),\n                         Ordinalizer(), # Make sure this happens before individual aggregation\n                         individual_aggregator,\n                         feature_creator,\n                         ExtractHouseholds(individual_aggregator=individual_aggregator, \n                                           feature_creator=feature_creator),\n                         FeatureSelector(),\n                         SimpleImputer(),\n                         MinMaxScaler()\n                        )\n\n# Estimators we want to evaluate\nestimators = [DecisionTreeClassifier(random_state=42),\n              KNeighborsClassifier(),\n              LogisticRegression(random_state=42, solver='lbfgs', multi_class='multinomial', max_iter=1000),\n              RandomForestClassifier(random_state=42, n_estimators=100),\n              AdaBoostClassifier(random_state=42),\n              GradientBoostingClassifier(random_state=42),\n              SGDClassifier(random_state=42, max_iter=1000, tol=1e-3),\n              MLPClassifier(random_state=42, solver='lbfgs')]\n\nevaluator = ModelEvaluator(pipeline, estimators)\nevaluator.evaluate()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a37aedb0347b7d76ab95b06c1ce0a24055a3a9f2"},"cell_type":"markdown","source":"### Detect correlated features\nLet's see if any features are highly correlated with each other and can be removed"},{"metadata":{"trusted":false,"_uuid":"851fa7607a7d2e126e366a71deade2a5bcbb29bc"},"cell_type":"code","source":"# Build a pipeline that performs everything except imputation and scaling\npipeline = make_pipeline(ZerofillRezEscOutOfBounds(),\n                         ZeroMaxRezEsc(),\n                         ZerofillV18q1ForFalseV18Q(), \n                         ZerofillV2a1(),\n                         TransformYesNoToNumeric(),\n                         Ordinalizer(), # Make sure this happens before individual aggregation\n                         individual_aggregator,\n                         feature_creator,\n                         ExtractHouseholds(individual_aggregator=individual_aggregator, \n                                           feature_creator=feature_creator)\n                        )\n\nevaluator_corr = ModelEvaluator(pipeline, estimators)\nevaluator_corr._preprocess_data()\n\ncorr = evaluator_corr._trainX_preprocessed.corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"86d32a7f54f10ae1a590bcfb5868dfbda2d17d51"},"cell_type":"code","source":"print(evaluator_corr.train.columns)\nprint(evaluator_corr._trainX_preprocessed.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"e90bc643d315aa48878fcd0a11db0b13e4d8543c"},"cell_type":"code","source":"correlated_features = corr[corr.abs().gt(0.95)].count() > 1\ncorr.loc[correlated_features, correlated_features]\n\nplt.figure(figsize=(25,25))\nsns.heatmap(corr.loc[correlated_features, correlated_features])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"2451155dbe05b807e897b28c70239b6cf6bce41d"},"cell_type":"code","source":"for attribute in corr.loc[correlated_features, correlated_features]:\n    print(corr.loc[attribute, corr.loc[attribute].abs().gt(0.95)])\n    print()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5c078375c2e4ad2253eee188d4e56224fed72665"},"cell_type":"markdown","source":"So, we definitely have some features that are highly correlated with others. When looking through the list and at the heatmap, it's clear that a lot of the correlations stem from the aggregation step on the individuals. There are a lot of correlations between an individual aggregation's `sum`, `max`, and `std`. We should probably only include one of these, maybe two, but definitely not all three.\n\ninstlevel and escolari mins and sums are highly correlated.\n\n`area1` and `area2` are 100% inversely correlated, so we should only include one of them.\n\n`public` and `coopele` aren't as inversely correlated, but still highly correlated, so we shoud only include one of those.\n\n`r4t3`, `tamhog`, `hhsize`, and `hogar_total` are also all highly correlated and seem to all be about total household size. We should just include one of them."},{"metadata":{"trusted":false,"_uuid":"78c23b97114a28704f9c46c84ef01ed05dd5f26f"},"cell_type":"code","source":"variance = len(train.loc[train[['r4t3', 'tamhog', 'hhsize', 'hogar_total']].var(axis=1) != 0, ['r4t3', 'tamhog', 'hhsize', 'hogar_total']])\nprint(f\"Variance in train between 'r4t3', 'tamhog', 'hhsize', 'hogar_total': {variance}\")\n      \nvariance = len(test.loc[test[['r4t3', 'tamhog', 'hhsize', 'hogar_total']].var(axis=1) != 0, ['r4t3', 'tamhog', 'hhsize', 'hogar_total']])\nprint(f\"Variance in test between 'r4t3', 'tamhog', 'hhsize', 'hogar_total': {variance}\")\n\nvariance = len(train.loc[train[['tamhog', 'hhsize', 'hogar_total']].var(axis=1) != 0, ['tamhog', 'hhsize', 'hogar_total']])\nprint(f\"Variance in train between 'tamhog', 'hhsize', 'hogar_total': {variance}\")\n\nvariance = len(test.loc[test[['tamhog', 'hhsize', 'hogar_total']].var(axis=1) != 0, ['tamhog', 'hhsize', 'hogar_total']])\nprint(f\"Variance in test between 'tamhog', 'hhsize', 'hogar_total': {variance}\")\n      \ntrain.loc[train[['r4t3', 'tamhog', 'hhsize', 'hogar_total']].var(axis=1) != 0, ['r4t3', 'tamhog', 'hhsize', 'hogar_total','Target']].head(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dd452f105000d5ce2f89f89dfd5e2c006b6f780c"},"cell_type":"markdown","source":"Well, in 48 of the rows in train set and 101 rows in the test set, these 4 attributes aren't equal. After inspecting the rows, it looks like `r4t3` is the one that is different and the other 3 are equal in all cases. With this knowledge, we should probably keep `r4t3` and one of the other three. We should probably also create a feature for the differnce between these two.\n\nThere's one other related attribute that seems to be related even though it's not showing up as being correlated, `tamviv`, defined as \"number of persons living in the household\". I'm not sure why that's different than `r4t3`, \"Total persons in the household\". Surely, `r4t3` isn't including skeletons in the closet, right? Maybe semi-permanent guests? Let's see how they differ."},{"metadata":{"trusted":false,"_uuid":"794ac924552697f28c37af33c9e7fa9a371e7801"},"cell_type":"code","source":"print(train[['tamviv', 'r4t3', 'tamhog', 'hhsize', 'hogar_total']].corr())\nprint(test[['tamviv', 'r4t3', 'tamhog', 'hhsize', 'hogar_total']].corr())","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"78a6f96684d21efc53a3b4663814e113831be571"},"cell_type":"code","source":"print(train.loc[(train.tamviv != train.r4t3), ['tamviv', 'r4t3', 'tamhog', 'hhsize', 'hogar_total', 'Target']].head())\nprint(train.loc[(train.tamviv > train.r4t3), ['tamviv', 'r4t3', 'tamhog', 'hhsize', 'hogar_total', 'Target']].head())\n\nequality = train.loc[(train.tamviv != train.r4t3)].equals(train.loc[(train.tamviv > train.r4t3)])\nprint(f\"When tamviv isn't equal to r4t3, tamviv is greater than tamviv, train: {equality}\")\n\nequality = test.loc[(test.tamviv != test.r4t3)].equals(test.loc[(test.tamviv > test.r4t3)])\nprint(f\"When tamviv isn't equal to r4t3, tamviv is greater than tamviv, test: {equality}\")\n\n# Whenever r4t3 is different than the hhsize attributes, is tamviv always equal to r4t3?\nprint(len(train.loc[(train.r4t3 != train.hhsize) & (train.tamviv != train.r4t3), ['tamviv', 'r4t3', 'tamhog', 'hhsize', 'hogar_total', 'Target']]))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"682f2fbf3e9d9e4a43050d3c5082e5d8e76ed3f3"},"cell_type":"markdown","source":"So what to do about `tamviv` and `r4t3`. The correlation isn't high enough to hit our 95% threshold in our automatic correlation detection above, but there's still high correlation. It seems as if when there are more people living in the household than are currently part of the household?\n\nIt's hard to say exaclty what's going on here and what the difference is, if there is a legitimate one, without talking to the stakeholders, but I think the best thing to do is merge the two columns together taking the max between the two into a new column. We'll create a new attribute `tamviv_r4t3_combined` and experiment with how it performs leaving only it in vs. all three. Worst case, we'll end up dropping it if the models don't use it."},{"metadata":{"trusted":false,"_uuid":"d30765ff04006ba7e685a1d2cc12b8df7756daf5"},"cell_type":"code","source":"class TamvivR4t3Combined(BaseEstimator, TransformerMixin):\n    \"\"\"Adds a new attribute with the max of tamviv and r4t3\"\"\"\n    \n    def __init__(self):\n        self.created_features = []\n        \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X, y=None):\n        X['tamviv_r4t3_combined'] = X[['r4t3', 'tamviv']].max(axis=1)\n        \n        self.created_features = ['tamviv_r4t3_combined']\n\n        return X\n    \n    \ntamviv_r4t3_combiner = TamvivR4t3Combined()\ntamviv_r4t3_preprocessed = tamviv_r4t3_combiner.transform(train.copy())\n\nmax_check = tamviv_r4t3_preprocessed.tamviv_r4t3_combined.equals(tamviv_r4t3_preprocessed[['r4t3', 'tamviv']].max(axis=1))\nassert max_check, \"TamvivR4t3Combiner didn't create the correct max column\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"42b539a08701d099e580359523b954b51669f107"},"cell_type":"code","source":"# store this so we can pass it into household extractor\nindividual_aggregator = AggregateIndividualFeatures()\nfeature_creator = FeatureCreator()\n\n# Create pipeline\npipeline = make_pipeline(ZerofillRezEscOutOfBounds(),\n                         ZeroMaxRezEsc(),\n                         ZerofillV18q1ForFalseV18Q(), \n                         ZerofillV2a1(),\n                         TransformYesNoToNumeric(),\n                         Ordinalizer(), # Make sure this happens before individual aggregation\n                         individual_aggregator,\n                         feature_creator,\n                         ExtractHouseholds(individual_aggregator=individual_aggregator, \n                                           feature_creator=feature_creator),\n                         TamvivR4t3Combined(),\n                         FeatureSelector(),\n                         SimpleImputer(),\n                         MinMaxScaler()\n                        )\n\ncorrelated_features_list = correlated_features[correlated_features == True].index\n\n# sum_features = [feature for feature in correlated_features_list if feature[-4:] == '-sum']\n# max_features = [feature for feature in correlated_features_list if feature[-4:] == '-max']\n# std_features = [feature for feature in correlated_features_list if feature[-4:] == '-std']\n\nexcluded_features = ['elimbasu5', 'area2', 'r4t3', 'tamviv', 'coopele', 'SQBdependency', 'hogar_total', 'tamhog'] #+ squared_features + sum_features + max_features\nkept_features = ['area1', 'public', 'hhsize', 'dependency']\n\n# Exclude the aggregation features more selectively than excluding them all\nfor feature in correlated_features_list:\n    if feature not in excluded_features + kept_features:\n        for excluded_feature in corr.loc[feature, corr.loc[feature].abs().between(0.95, 0.9999999)].index:\n            excluded_features.append(excluded_feature)\n            \n        kept_features.append(feature)\n        \n# print(f\"Excluding features: {excluded_features}\")\n\nevaluator_excluded = ModelEvaluator(pipeline, estimators, excluded_features=excluded_features)\nevaluator_excluded.evaluate()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bccc44f37daf9e23a974ba3c615459bc86c0b6f2","trusted":false},"cell_type":"code","source":"def select_important_features(feature_importances, threshold=0.9):\n    total_importance = 0\n    important_features = []\n\n    for feature, importance in feature_importances.iterrows():\n        total_importance += importance[0]\n        important_features.append(feature)\n\n        if total_importance >= threshold:\n            break\n        \n    return important_features","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a751b79b416d38cdfcdb102c1144cabf3a877061","trusted":false},"cell_type":"code","source":"# Get important features from estimator\nfeature_importance_estimator = [estimator for estimator in evaluator_excluded.estimators if estimator['estimator'].__class__.__name__ == 'GradientBoostingClassifier'][0]\nimportant_features = select_important_features(feature_importance_estimator['feature_importances'], threshold=0.95)\n\nevaluator_important_features = ModelEvaluator(pipeline, estimators, important_features)\nevaluator_important_features.evaluate()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"369f1f6cc69119c110f5fa207e8f7001b010d1f4"},"cell_type":"markdown","source":"# Tune Models\n\nAt this point, it seems like the GradientBoostingClassifier and AdaBoostClassifiers stand out and we should turn our attention to tuning those models."},{"metadata":{"trusted":false,"_uuid":"1f47a5c1f0aa5ebf825d79f4eb55de31f4ef1dcd"},"cell_type":"code","source":"estimators = [GradientBoostingClassifier(random_state=42)]\ntuning_evaluator = ModelEvaluator(pipeline, estimators, important_features)\ntuning_evaluator.evaluate()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"6e91d5e0dd88462c267ef8d1b73573fe0c9d5357"},"cell_type":"code","source":"param_dist = {'learning_rate': scipy.stats.truncnorm(-0.995, 2, loc=0.1, scale=0.05),\n              'n_estimators': scipy.stats.randint(80, 1000),\n              'max_depth': scipy.stats.randint(3, 8),\n              'min_samples_split': scipy.stats.randint(2, 100),\n              'min_samples_leaf': scipy.stats.randint(20, 60),\n              'max_features': ['sqrt', None, 'log2'],\n              'subsample': scipy.stats.uniform(loc=0.6, scale=0.4)}\n\nresults = tuning_evaluator.tune_hyperparameters(tuning_evaluator.estimators[0], param_dist, n_iter=20, cv=5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"f3af1d0e86d21e6dc24fa77c6afa72d0a49663bb"},"cell_type":"code","source":"print(results.best_params_)\nprint(results.best_score_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"710b527810a2d55f308f02e7c2af834ff2ca46c3"},"cell_type":"code","source":"# tuning_evaluator.estimators[0]['tuned_cv_results']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ee73c2b1e8227a39b1b90045ffeef278ee0dc8f5"},"cell_type":"markdown","source":"# Submit"},{"metadata":{"_uuid":"a2aec79e7d67ff0764efa2ff9ae3baa1ab542967","trusted":false},"cell_type":"code","source":"tuning_evaluator.prepare_submissions()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"13e9b82e6b0284d20c199a18a60e5e9522b4c2b3"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"f0dc7753275a137ea70194a7582e399482d1f13f"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.7"}},"nbformat":4,"nbformat_minor":1}