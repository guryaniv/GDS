{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"collapsed":true},"cell_type":"code","source":"import xgboost as xgb\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import f1_score\nimport gc\n\n# Any results you write to the current directory are saved as output.\n\ntrain_df = pd.read_csv(\"../input/train.csv\")\ntest_df = pd.read_csv(\"../input/test.csv\")\nid_test = test_df['Id']\nprint(train_df.shape)\nprint(test_df.shape)\n\nlabels = train_df['Target'].values\ntrain_df.drop('Target',axis=1, inplace=True)\n# concatenate both train and test data\nalldata = pd.concat([train_df, test_df])\nprint(alldata.shape)\n\n# encoding categorical columns\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nalldata['idhogar'] = le.fit_transform(alldata['idhogar'])\n\n\n# handle missing values\nmissing_cols = alldata.columns[alldata.isnull().any()].tolist()\nprint(missing_cols)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a9fdf8c2492880848f9be062637a8c1a6c01a2ca","collapsed":true},"cell_type":"code","source":"# We first examine 'v2a1' which is the monthly rent amount\n# Clearly that people who rent a house have to rent, so let's check this probability\nalldata[alldata['tipovivi3']==1].shape[0]/alldata.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"b2c78e5f42775f930331fbaeaa3e96c278ca9f33"},"cell_type":"code","source":"# only 17% people rent a house, so most people do not have to pay\n# we therefore set this to be 0\nalldata['v2a1'].fillna(0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a2b1241efca68d328c2d9458e84cf996adcf1c5d","collapsed":true},"cell_type":"code","source":"# fill missing meaneduc and SQBmeaned\n# Clearly, we can use SQBmeaned to infer meaneduc and vice-versa\nids = alldata['SQBmeaned'].notnull() & alldata['meaneduc'].isnull()\nalldata.loc[ids, 'meaneduc'] = np.sqrt(alldata[ids]['SQBmeaned'])\n\nids = alldata['SQBmeaned'].isnull() & alldata['meaneduc'].notnull()\nalldata.loc[ids, 'SQBmeaned'] = np.square(alldata[ids]['meaneduc'])\nalldata[alldata['meaneduc'].isnull()].shape[0]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"2c879dcca91f818119f408e18176d362523fa5a2"},"cell_type":"code","source":"# there are still 36 missing values\n# we can use the average years of schooling as mean of years of education, intuitively\ndf=alldata.groupby('idhogar')['escolari'].mean().reset_index().rename(columns={'escolari':'escolari_mean'})\nalldata = alldata.merge(df, how='left', on='idhogar')\nmean_ids = alldata['meaneduc'].isnull()\nsq_mean_ids = alldata['SQBmeaned'].isnull()\nalldata.loc[mean_ids, 'meaneduc'] = alldata.loc[mean_ids,'escolari_mean']\nalldata.loc[sq_mean_ids,'SQBmeaned'] = np.square(alldata.loc[sq_mean_ids,'meaneduc'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3a8cde113eee79c3c3886e7d5946227cb5fe64d7","collapsed":true},"cell_type":"code","source":"# let's check possible values of this column\nalldata['rez_esc'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"72be153a70f95c748088234c2cc0d6d59d12c04b","collapsed":true},"cell_type":"code","source":"# there is only one row with value of 99 which does not seem to be right (indeed, the corresponding age is only 8), let's set it to nan\nalldata.loc[alldata['rez_esc']>5, 'rez_esc'] = np.nan\n# we could see the the majority has value of 0, but let's see the range of ages for which rez_esc > 0 \n#alldata.groupby('rez_esc')['age'].mean().reset_index()\nmin_age=alldata[alldata['rez_esc']>0]['age'].min()\nmax_age=alldata[alldata['rez_esc']>0]['age'].max()\nprint (\"min age: {}, max age: {}\".format( min_age, max_age))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b00a7761a70b05847b5005a718a357682b9bc059","collapsed":true},"cell_type":"code","source":"# easiest way is to set any age outside of the above range to 0\nalldata.loc[alldata['rez_esc'].isnull() & ((alldata['age']<8)|(alldata['age']>17)), 'rez_esc'] = 0\nmissing_left = list(alldata[alldata['rez_esc'].isnull()]['age'])\nprint(missing_left)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ef079b57f2174d0a30d18bbfdc3187eae13f5ce8","collapsed":true},"cell_type":"code","source":"# after filling, we have 5 missing values left\n# of course, it would not make much difference which value we choose for this particular instance, so we can choose 0\n# but let's examine the probability that a certain age corresponds to a particular value of res_esc\nfor age in set(missing_left):\n    ids = (alldata['age']==age)&(alldata['rez_esc'].notnull())\n    df = alldata[ids].groupby('rez_esc')['Id'].count().reset_index()\n    df['Id'] = df['Id']/alldata[ids].shape[0]\n    #rez_esc_dict.update({age:list(df['Id'])})\n    print(\"age {} : {}\".format(age, list(df['Id'])))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"4891ac587cea19f1bb4721779f35c0425fb4c5e9"},"cell_type":"code","source":"# we can see that rez_esc = 0 is the majority in all cases\n# so let's set the remaining missing value to 0\nalldata['rez_esc'].fillna(0,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1ca6f7904c4f9e89681df1fe4b5892f2972cedf8","collapsed":true},"cell_type":"code","source":"# filling missing values for v18q1\n# of course, one does not own a tablet should have 0 tablets\nalldata.loc[alldata['v18q']==0,'v18q1']=0\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f741781a1124ff2016c13b4c4ec5bb2855556b03","collapsed":true},"cell_type":"code","source":"# the majority is that one owns and has exactly 1 tablet\nalldata.isnull().any().any()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c35061d77b7e98ef2641b23f23c77e5dbb4e9080","collapsed":true},"cell_type":"code","source":"# we remove 'yes' and 'no' from numerical columns using the coressponding square\nalldata.loc[alldata['edjefe']=='no','edjefe'] = 0\nalldata.loc[alldata['edjefe']=='yes','edjefe'] = 1\nalldata['edjefe'] = alldata['edjefe'].astype(int)\n\nalldata.loc[alldata['edjefa']=='no','edjefa'] = 0\nalldata.loc[alldata['edjefa']=='yes','edjefa'] = 1\nalldata['edjefa'] = alldata['edjefa'].astype(int)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"83e87ff6e0af670848a07dc42ee499eba8b103c4"},"cell_type":"code","source":"text_ids = (alldata['dependency']=='yes')|(alldata['dependency']=='no')\nalldata.loc[text_ids,'dependency'] = np.sqrt(alldata.loc[text_ids,'SQBdependency'])\nalldata['dependency'] = alldata['dependency'].astype(float)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"af9291d721c3b06eb15fd47e41179e9ba4696571","collapsed":true},"cell_type":"code","source":"# we explore data before performing feature engineering\n# here I tried with dependency as an example\n# note that dependency rate = (people with age <=19 and >=64)/(people with age within [19,64])\n# we create a new column for people within [19,64] called working_people\nalldata['working_people'] = alldata['hogar_adul'] - alldata['hogar_mayor']\n# we create a columns for people <=19 and >=64\nalldata['not_working_people'] = alldata['hogar_nin']+alldata['hogar_mayor']\n# it should hold that dependency = not_working_people/working_people, so let's check that\n# now, we check that\nalldata[['not_working_people','working_people','dependency']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"50f1b4fc57a7a443a0aa9338f2437145bc76961a","collapsed":true},"cell_type":"code","source":"# oh, they do not agree on many rows\n# let's check at which value of dependency we have the difference\nalldata[(alldata['not_working_people']%alldata['working_people']!=0)&(np.abs(alldata['not_working_people']-(alldata['working_people']*alldata['dependency']))>0.0001)][['not_working_people','working_people','dependency']]\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b8e7bffee9cef8295ff9717b3533f24db716614d","collapsed":true},"cell_type":"code","source":"# interesting! they differ only when dependency = 8 and working_people = 0\n# of course, when no people are working then the dependency should be maximal, but in this case the dependency rate should depends on the number of non_working_people\nids = alldata['dependency']>7 # not that comparison with float is dangerous, so we do it in a safer way\nalldata.loc[ids, 'dependency'] = alldata.loc[ids,'not_working_people']+8","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"da3112a2cf25804652786732f6af79a09add0990","collapsed":true},"cell_type":"code","source":"# add new features\ndef statsFeature(df, grp_cols, stats_col, stats='sum'):\n    prefix = '_'.join(grp_cols+[stats_col])\n    new_col = prefix+\"_\"+stats\n    if stats=='min':\n        new_df = df.groupby(grp_cols)[stats_col].min().reset_index().rename(columns={stats_col:new_col})\n    elif stats=='max':\n        new_df = df.groupby(grp_cols)[stats_col].max().reset_index().rename(columns={stats_col:new_col})\n    elif stats=='mean':\n        new_df = df.groupby(grp_cols)[stats_col].mean().reset_index().rename(columns={stats_col:new_col})\n    elif stats=='median':    \n        new_df = df.groupby(grp_cols)[stats_col].median().reset_index().rename(columns={stats_col:new_col})\n    elif stats=='count':\n        new_df = df.groupby(grp_cols)[stats_col].count().reset_index().rename(columns={stats_col:new_col})\n    else:\n        new_df = df.groupby(grp_cols)[stats_col].sum().reset_index().rename(columns={stats_col:new_col})\n    df = df.merge(new_df, how = 'left', on=grp_cols)\n    return df\n\n# generate counting features\nalldata = statsFeature(alldata, ['idhogar'],'Id','count')\n\n# generate statistical features\n#estadocivil_cols = ['estadocivil'+str(i) for i in range(1,8)]\n#parentesco_cols = ['parentesco'+str(i) for i in range(1,13)]\n#instlevel_cols = ['instlevel'+str(i) for i in range(1,10)]\nmean_cols = ['age','rez_esc','escolari']\nfor col in mean_cols:\n    alldata = statsFeature(alldata,['idhogar'],col,'mean')\n\n# new relational features\nalldata['bedroom_per_room'] = alldata['bedrooms']/alldata['rooms']\nalldata['mobile_per_person'] = alldata['qmobilephone']/alldata['tamviv']\nalldata['mobile_per_adult'] = alldata['qmobilephone']/alldata['r4t2']\nalldata['tablet_per_adult'] = alldata['v18q1']/alldata['r4t2']\nalldata['room_area'] = alldata['hhsize']/alldata['rooms']\nalldata['female_ratio'] = alldata['r4m3']/alldata['tamviv']\nalldata['male_ratio'] = alldata['r4h3']/alldata['tamviv']\nalldata['young_people_ratio'] = alldata['r4t1']/alldata['tamviv']\nalldata['young_people_less_than_19_ratio'] = alldata['hogar_nin']/alldata['tamviv']\nalldata['old_people_ratio_1'] = alldata['hogar_mayor']/alldata['tamviv']\nalldata['old_people_ratio_2'] = alldata['hogar_mayor']/(1+alldata['working_people'])\nalldata['adult_ratio'] = alldata['hogar_adul']/alldata['tamviv']\nalldata['working_people_ratio'] = alldata['working_people']/alldata['tamviv']\nalldata['rent_per_person'] = alldata['v2a1']/alldata['tamviv']\nalldata['rent_per_room'] = alldata['v2a1']/alldata['rooms']\nalldata['rent_per_working_people'] = alldata['v2a1']/(1+alldata['working_people'])\nalldata['rent_area'] = alldata['v2a1']/alldata['hhsize']\nalldata['person_area'] = alldata['tamviv']/alldata['hhsize']\nalldata['person_per_room'] = alldata['tamviv']/alldata['rooms']\nalldata['person_per_bedroom'] = alldata['tamviv']/alldata['bedrooms']\nalldata['working_people_area'] = alldata['working_people']/alldata['hhsize']\nalldata['working_people_per_room'] = alldata['working_people']/alldata['rooms']\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c3c22fe9455372fc6c1f1be915ee7a20aa4a3688","collapsed":true},"cell_type":"code","source":"# feature importances\nimport lightgbm as lgb\n# drop id and unnecessary columns\nredundant_cols=['Id','agesq','tamviv']+[col for col in alldata.columns if col[:3]=='SQB']\nalldata.drop(redundant_cols, axis=1, inplace=True)\n# split to train and test data\ntrain_df = alldata[:train_df.shape[0]]\ntest_df = alldata[-test_df.shape[0]:]\ndel alldata\ngc.collect()\n\ndef lgb_f1_macro(preds, dtrain):  \n    labels = dtrain.get_label()\n    preds = preds.reshape(-1, 4).argmax(axis=1)\n    f_score = f1_score(preds, labels, average = 'macro')\n    return 'f1_score', f_score, True\n\n\nlgb_params ={'colsample_bytree': 0.952164731370897, \n                 'scale_pos_weight':1, \n                 'min_child_samples': 111, \n                 'min_child_weight': 0.01, \n                 'num_leaves': 38, \n                 'reg_alpha': 0, \n                 'reg_lambda': 0.1, \n                 'subsample': 0.3029313662262354,\n                'boosting_type': 'gbdt',\n                'objective': 'multiclass',\n                'metric': 'f1_macro',\n                'max_depth': 5,\n                'learning_rate': 0.004,\n                'bagging_fraction': 0.8, \n                'feature_fraction': 0.9,\n                'bagging_freq': 5,\n                'verbose': -1,\n                'num_threads': 6,\n                'lambda_l2': 1.0,\n                'min_gain_to_split': 0,\n                'num_class': 4,\n                'class_weight':'balanced'\n            }\n\nX = train_df.values\ny = labels-1\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=2018, stratify=y)\n# create dataset for lightgbm\nlgb_train = lgb.Dataset(X_train, y_train)\nlgb_test = lgb.Dataset(X_test, y_test, reference=lgb_train)\n\ngbm = lgb.train(lgb_params,\n                lgb_train,\n                num_boost_round=5000,\n                feval=lgb_f1_macro,\n                valid_sets=[lgb_train, lgb_test],\n                feature_name=list(train_df.columns),\n                categorical_feature='auto',\n                verbose_eval=10)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c874f7a3c6454ca24806d969248e193656a74b5a","collapsed":true},"cell_type":"code","source":"feature_score = pd.DataFrame(gbm.feature_importance(), columns=['score'])\nfeature_score['feature'] = train_df.columns\nfeature_score = feature_score.sort_values(by=['score'], ascending=False)\n#print(feature_score)\npositive_features = feature_score[feature_score['score']>0]['feature']\n# take 80% the best features\nfeature_names = list(positive_features[-int(len(positive_features)*0.8):])\n#print(feature_names)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"93ae53aec72a9a4f724adb045fd75d2648de996f","collapsed":true},"cell_type":"code","source":"print(feature_names)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f74fb1c78da9d5c41357b407951623a856f10c1a","collapsed":true},"cell_type":"code","source":"# define f1 score for multiclass \ndef xgb_f1_score(y_pred, dtrain):\n    y_true = dtrain.get_label()\n    return 'f1_macro', f1_score(y_true, y_pred, average='macro')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"342f825217ff9f5fe817ff31e821d08ec8932d8f","collapsed":true},"cell_type":"code","source":"def xgb_predict(model,X, y, cvFolds=5, early_stopping_rounds=40):\n    xgb_param = model.get_xgb_params()\n    xgTrain = xgb.DMatrix(X, label=y)\n    cvresult = xgb.cv(xgb_param,\n                  xgTrain,\n                  num_boost_round=5000,\n                  nfold=cvFolds,\n                  stratified=True,\n                  metrics={'mlogloss'},\n                  feval=xgb_f1_score,\n                  early_stopping_rounds=early_stopping_rounds,\n                  seed=0,     \n                  callbacks=[xgb.callback.print_evaluation(show_stdv=False),xgb.callback.early_stop(3)])\n    model.set_params(n_estimators=cvresult.shape[0])\n    # Fit the algorithm\n    model.fit(X, y)\n    return model.predict(test_df)\n\nxgb_params = {'n_estimators':20, \n          'max_depth':5,\n          'min_child_weight':6,\n          'num_class':4,\n          'gamma':0.41,\n          'learning_rate':0.004,\n          'subsample':0.8,\n          'colsample_bytree':0.8,\n          'objective':'multi:softmax',\n          'scale_pos_weight':1,\n          'seed':27}\n\nxgb_model = XGBClassifier(**xgb_params)\ntrain_df = train_df[feature_names]\ntest_df = test_df[feature_names]\npred = xgb_predict(xgb_model, train_df, y)\nsubm = pd.DataFrame()\nsubm['Id'] = id_test\nsubm['Target'] = pred+1\nsubm.to_csv('submission.csv', index=False)\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}