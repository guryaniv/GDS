{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"markdown","source":"# Fast.ai/Pytorch Starter\n\nAlthough most people (including me) seem to be doing much better with Boosting Trees, I think it is worth the time to explore a Neural Network solution to the  Home Credit Default Risk competition. Besides, a Kaggle competition is always a good opportunity to test what one is currently learning. I did two \"cool\"  things in this kernel:\n\n* Categorical Embeddings.\n* Custom loss functions with different weights for each class to try to manage the imbalance in `TARGET`. \n\nI had to tweak the fast.ai library a little bit, but all things considered, it is extraordinary how little code you actually have to write to get some model going. \n\n## Load Data\n\nTo aggregate the various tables available in the competition I followed the next heuristic:\n\n* If the variable was continuous, I aggregated it using its mean. \n* If the variable was categorical, I aggregated it using its mode. If I were to one-hot-encode the variables and aggregate them using their mean, there wouldn't be categorical variables (besides the one in the main table) for which to create categorical embeddings. \n\nThe mode computation is very, very slow, so I did it all of this in another Kaggle kernel.\n\n## Imports\n"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"from fastai.imports import *\nfrom fastai.structured import *\nfrom fastai.column_data import *\nfrom torch.nn import functional as F\nimport gc \nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import train_test_split","execution_count":1,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c23817e7b2279270f1a1a2bbcf234471fe769b7a"},"cell_type":"code","source":"!ls ../input/","execution_count":2,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2df01493fae9666b612557581c89be3205269ea5"},"cell_type":"code","source":"df_train = pd.read_csv('../input/introduction-to-manual-feature-engineering/train_bureau_corrs_removed.csv')\ndf_test = pd.read_csv('../input/introduction-to-manual-feature-engineering/test_bureau_corrs_removed.csv')\ndf_train.head()","execution_count":8,"outputs":[]},{"metadata":{"_uuid":"04867915d576cb3cbb34a0853de8e1d5bcaa7699"},"cell_type":"markdown","source":"## What type of variables do we have?"},{"metadata":{"trusted":true,"_uuid":"166e9ec0956507959d1afc7f4ac09a1c0fb5dc37"},"cell_type":"code","source":"df_train.dtypes.value_counts()","execution_count":9,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4a2de280e875125d1b0865bd3431d468907a323f","collapsed":true},"cell_type":"code","source":"cat_vars = [col for col in df_train if df_train[col].dtype.name != 'float64' and df_train[col].dtype.name != 'float32' and len(df_train[col].unique()) < 150]\ncat_vars.remove('TARGET')\nfor v in cat_vars: df_train[v] = df_train[v].astype('category').cat.as_ordered()\ncat_sz = [(c, len(df_train[c].cat.categories)+1) for c in cat_vars]","execution_count":10,"outputs":[]},{"metadata":{"_uuid":"469514a8d5cbd798e67556c4711259caf53940bb"},"cell_type":"markdown","source":"Which variables are we going to treat as categorical?"},{"metadata":{"trusted":true,"_uuid":"69081617002e7c700ed74812063f4a2866dc327c"},"cell_type":"code","source":"cat_sz","execution_count":11,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5d4439e1809b5bd9199eb2c763398c4c57647f7d"},"cell_type":"code","source":"cat_vars","execution_count":12,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"2f47b52a2d07e8fa6b004b08f10ce7a76a3d38bf","_kg_hide-input":true},"cell_type":"code","source":"apply_cats(df_test, df_train)","execution_count":13,"outputs":[]},{"metadata":{"_uuid":"d80db730bd8830942a96c8bfd63075aa5722a8e5"},"cell_type":"markdown","source":"## Pre-processing \n\nThe fast.ai library handles NA values for us:"},{"metadata":{"trusted":true,"_uuid":"73094dc36af6d27ce324d07480eddb5d442bae39"},"cell_type":"code","source":"%time df, y, nas, mapper = proc_df(df_train, 'TARGET', do_scale=True, skip_flds=['SK_ID_CURR'])\n%time df_test_md, _, nas, mapper = proc_df(df_test, do_scale=True, na_dict=nas, mapper=mapper, skip_flds=['SK_ID_CURR'])","execution_count":14,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"85bb24bdb20e05ac1d448575a0008c22842863d6","collapsed":true},"cell_type":"code","source":"df_to_nn_train, df_to_nn_valid, y_train, y_valid = train_test_split(df, y, test_size=0.33, random_state=23, stratify = y)","execution_count":15,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9522391664a0f13e08e00768ed72bb6d4cfda804","_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"for v in cat_vars: df_to_nn_train[v] = df_to_nn_train[v].astype('category').cat.as_ordered()\nfor v in cat_vars: df_to_nn_valid[v] = df_to_nn_valid[v].astype('category').cat.as_ordered()    ","execution_count":16,"outputs":[]},{"metadata":{"_uuid":"352852dd63bf9f25842626900130f205bc113e1e"},"cell_type":"markdown","source":"The embedding sizes we are going to use for each category:"},{"metadata":{"trusted":true,"_uuid":"f11f979d1ae30817a9000a2a8a8a564557ffda70","collapsed":true},"cell_type":"code","source":"emb_szs = [(c, min(50, (c+1)//2)) for _,c in cat_sz]","execution_count":17,"outputs":[]},{"metadata":{"_uuid":"47483c8ca3797c77cccc4b4f8382c303bbc3826e"},"cell_type":"markdown","source":"## PyTorch/Fast.ai\n\nDefine the data loader:"},{"metadata":{"trusted":true,"_uuid":"77b4b6e7e60be736f4355b34de3a7b460b83a1a3","collapsed":true},"cell_type":"code","source":"md  = ColumnarModelData.from_data_frames('', trn_df = df_to_nn_train, val_df = df_to_nn_valid, trn_y = y_train.astype('int'), val_y = y_valid.astype('int'), cat_flds=cat_vars, bs=128, is_reg= False)","execution_count":18,"outputs":[]},{"metadata":{"_uuid":"ef952815b0c2874e6e6957f80967fca90f758918"},"cell_type":"markdown","source":"There's no easy way of using the fast.ai library (that I know) to predict structured data in a classification problem. Besides, the fast.ai package that Kaggle is running is not the same as the source code in GitHub. Thus, I read a little bit of the code and tweaked it to create the model that we are going to use."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"881c7933b477f474655be4f55474b82644f605f2"},"cell_type":"code","source":"class MixedInputModel(nn.Module):\n    def __init__(self, emb_szs, n_cont, emb_drop, out_sz, szs, drops,\n                 y_range=None, use_bn=False, is_reg=True, is_multi=False):\n        super().__init__()\n        self.embs = nn.ModuleList([nn.Embedding(c, s) for c,s in emb_szs])\n        for emb in self.embs: emb_init(emb)\n        n_emb = sum(e.embedding_dim for e in self.embs)\n        self.n_emb, self.n_cont= n_emb, n_cont\n        szs = [n_emb + n_cont] + szs\n        self.lins = nn.ModuleList([\n            nn.Linear(szs[i], szs[i+1]) for i in range(len(szs)-1)])\n        self.bns = nn.ModuleList([\n            nn.BatchNorm1d(sz) for sz in szs[1:]])\n        for o in self.lins: kaiming_normal(o.weight.data)\n        self.outp = nn.Linear(szs[-1], out_sz)\n        kaiming_normal(self.outp.weight.data)\n\n        self.emb_drop = nn.Dropout(emb_drop)\n        self.drops = nn.ModuleList([nn.Dropout(drop) for drop in drops])\n        self.bn = nn.BatchNorm1d(n_cont)\n        self.use_bn,self.y_range = use_bn,y_range\n        self.is_reg = is_reg\n        self.is_multi = is_multi\n\n    def forward(self, x_cat, x_cont):\n        x = []\n        for i,e in enumerate(self.embs):\n            x.append(e(x_cat[:,i]))\n        x = torch.cat(x, 1)\n        x = self.emb_drop(x)\n        x2 = self.bn(x_cont)\n        x = torch.cat([x, x2], 1)\n        for l,d,b in zip(self.lins, self.drops, self.bns):\n            x = F.relu(l(x))\n            if self.use_bn: x = b(x)\n            x = d(x)\n        x = self.outp(x)\n        x = F.log_softmax(x)\n        return x","execution_count":19,"outputs":[]},{"metadata":{"_uuid":"320f477b11d99be017d56a4301689727683d2849"},"cell_type":"markdown","source":"Besides the embedding, 3 fully connected layers:"},{"metadata":{"trusted":true,"_uuid":"d2dcee58a192073da17e32b9c2c720e537f3707e","collapsed":true},"cell_type":"code","source":"m = MixedInputModel(emb_szs, n_cont = len(df.columns)-len(cat_vars),\n                   emb_drop = 0, out_sz = 2, szs = [100, 100, 100], drops = [0, 0, 0],y_range = None, use_bn = False, is_reg = False, is_multi = False)","execution_count":20,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1ed97b69062e147544695f291f3683abce592a0a","collapsed":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"bm = BasicModel(m.cuda(), 'binary_classifier')","execution_count":21,"outputs":[]},{"metadata":{"_uuid":"22a9cbd77fb51a5cbf5d2f91636bfc99e2b119e8"},"cell_type":"markdown","source":"We define our learner's loss function:"},{"metadata":{"trusted":true,"_uuid":"31f94b39a811234109d125f657dcd9398f110dd2","collapsed":true},"cell_type":"code","source":"# you can simply create learner with any custom model and data\n# source code is here\nclass StructuredLearner(Learner):\n    def __init__(self, data, models, **kwargs):\n        super().__init__(data, models, **kwargs)\n        self.crit = F.nll_loss\n\n\nlearn = StructuredLearner(md, bm)","execution_count":22,"outputs":[]},{"metadata":{"_uuid":"c49ca38c946afe68f802167b5c24920339212fd1"},"cell_type":"markdown","source":"Now, let's do some fitting:"},{"metadata":{"trusted":true,"_uuid":"4d6337fb7e894784c84944404d3d8ff9691632e6"},"cell_type":"code","source":"learn.lr_find()","execution_count":23,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"246e3597413bd2d99aaa02a0a2906a07a6e16b95","_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"def roc_auc_own(y_score, y_true):\n    y_score = np.exp(y_score[:,1])\n    return roc_auc_score(y_true, y_score)","execution_count":24,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"31770ad7dad4f2455a4f0af1c0b50e0143d20a0e"},"cell_type":"code","source":"learn.sched.plot(100)","execution_count":25,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9b6653330732ce4c010ddab8e01724715c9bf78a","collapsed":true},"cell_type":"code","source":"lr = 0.0005","execution_count":26,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"91a3873b7965b1a3d19b22bdae48ff2ab1e56f83"},"cell_type":"code","source":"learn.fit(lr, 3, metrics=[roc_auc_own])","execution_count":27,"outputs":[]},{"metadata":{"_uuid":"26de4474c232325f85783cee10d7c4e38e70b56e"},"cell_type":"markdown","source":"Let's do, just for the kicks, some Stochastic Gradient Descent with Restarts. If you do no know what it is, take the fast.ai MOOC! !!. "},{"metadata":{"trusted":true,"_uuid":"2061d696b7ade5e12934d0c7c81dcd7500658d8a"},"cell_type":"code","source":"learn.fit(lr, 3, metrics=[roc_auc_own], cycle_len=1, cycle_mult=2)","execution_count":28,"outputs":[]},{"metadata":{"_uuid":"2a9a5549da281f03d925bd2f4de4a97fac9e58e5"},"cell_type":"markdown","source":"## Tackling the imbalance problem\n\nSeems we've exhausted what this model can learn, as the changes from `val_loss` and `roc` have hit decreasing returns. Let's try to correct for the imbalance in `TARGET` by creating a loss function with weights. "},{"metadata":{"trusted":true,"_uuid":"8e2fc13b929aa573deea0e548290c18a0e61a490","collapsed":true},"cell_type":"code","source":"class StructuredLearner(Learner):\n    def __init__(self, data, models, **kwargs):\n        super().__init__(data, models, **kwargs)\n        self.crit = torch.nn.NLLLoss(weight= torch.FloatTensor([0.1, 0.9]).cuda())","execution_count":29,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"eca09374b0c59aa8872c01bd78892827ecc0d91a","_kg_hide-input":true},"cell_type":"code","source":"class ColumnarDataset(Dataset):\n    def __init__(self, cats, conts, y, is_reg, is_multi):\n        n = len(cats[0]) if cats else len(conts[0])\n        self.cats  = np.stack(cats,  1).astype(np.int64)   if cats  else np.zeros((n,1))\n        self.conts = np.stack(conts, 1).astype(np.float32) if conts else np.zeros((n,1))\n        self.y     = np.zeros((n,1))                       if y is None else y\n        if is_reg:\n            self.y =  self.y[:,None]\n        self.is_reg = is_reg\n        self.is_multi = is_multi\n\n    def __len__(self): return len(self.y)\n\n    def __getitem__(self, idx):\n        return [self.cats[idx], self.conts[idx], self.y[idx]]\n\n    @classmethod\n    def from_data_frames(cls, df_cat, df_cont, y=None, is_reg=True, is_multi=False):\n        cat_cols = [c.values for n,c in df_cat.items()]\n        cont_cols = [c.values for n,c in df_cont.items()]\n        return cls(cat_cols, cont_cols, y, is_reg, is_multi)\n\n    @classmethod\n    def from_data_frame(cls, df, cat_flds, y=None, is_reg=True, is_multi=False):\n        return cls.from_data_frames(df[cat_flds], df.drop(cat_flds, axis=1), y, is_reg, is_multi)\n\nclass ColumnarModelData(ModelData):\n    def __init__(self, path, trn_ds, val_ds, bs, test_ds=None, shuffle=True):\n        test_dl = DataLoader(test_ds, bs, shuffle=False, num_workers=1) if test_ds is not None else None\n        super().__init__(path, DataLoader(trn_ds, bs, shuffle=shuffle, num_workers=1),\n            DataLoader(val_ds, bs*2, shuffle=False, num_workers=1), test_dl)\n    @classmethod\n    def from_data_frames(cls, path, trn_df, val_df, trn_y, val_y, cat_flds, bs, is_reg = False, is_multi = False, test_df=None):\n        trn_ds  = ColumnarDataset.from_data_frame(trn_df,  cat_flds, trn_y, is_reg, is_multi)\n        val_ds  = ColumnarDataset.from_data_frame(val_df,  cat_flds, val_y, is_reg, is_multi)\n        test_ds = ColumnarDataset.from_data_frame(test_df, cat_flds, None,  is_reg, is_multi) if test_df is not None else None\n        return cls(path, trn_ds, val_ds, bs, test_ds=test_ds)","execution_count":30,"outputs":[]},{"metadata":{"_uuid":"7c43677a75cd9cf4a3ccaf1a0a6349aec7fdd335"},"cell_type":"markdown","source":"Let's define the data loader, this time with the test set:"},{"metadata":{"trusted":true,"_uuid":"373fc5b1e1eb10d98b327305ccef6072faacb838","collapsed":true,"_kg_hide-input":true},"cell_type":"code","source":"md  = ColumnarModelData.from_data_frames('', trn_df = df_to_nn_train, val_df = df_to_nn_valid, trn_y = y_train.astype('int'), val_y = y_valid.astype('int'), cat_flds=cat_vars, bs=128, is_reg = False, test_df=df_test_md)","execution_count":31,"outputs":[]},{"metadata":{"_uuid":"e2564c31f78357b24c1fc308fd8851393c5e4170"},"cell_type":"markdown","source":"Same model as before, this time with some dropout:"},{"metadata":{"trusted":true,"_uuid":"6d6f965d659c8f73ebbe91d320b4c96054792e51","collapsed":true,"_kg_hide-input":false},"cell_type":"code","source":"m = MixedInputModel(emb_szs, n_cont = len(df.columns)-len(cat_vars),\n                   emb_drop = 0.05, out_sz = 2, szs = [100, 100, 100], drops = [0.05, 0.05, 0.05],y_range = None, use_bn = False, is_reg = False, is_multi = False)\nbm = BasicModel(m.cuda(), 'binary_classifier')\nlearn = StructuredLearner(md, bm)","execution_count":32,"outputs":[]},{"metadata":{"_uuid":"ff536b2ed71bc8c45fe08472064cfc856d190075"},"cell_type":"markdown","source":"Let's do the fitting:"},{"metadata":{"trusted":true,"_uuid":"45d6efae013d7ea982967d574bb0736d0bda0335"},"cell_type":"code","source":"learn.lr_find()\nlearn.sched.plot(100)","execution_count":33,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3d32a39929f4543f2df8a071f9c5d479a635e156"},"cell_type":"code","source":"lr = 1e-2\nlearn.fit(lr, 3, metrics=[roc_auc_own])","execution_count":34,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cf951184777b23b0dce2a780c615997a0b171fd4"},"cell_type":"code","source":"learn.fit(lr, 5, metrics=[roc_auc_own])","execution_count":35,"outputs":[]},{"metadata":{"_uuid":"3f4efed0be9fc9eeb282e4d16c9a9a58ba193acc"},"cell_type":"markdown","source":"Seems that is the best the model can deliver without torturing too much. The loss function with weights does seem to help it to improve the AUC in validation. However, not good enough compared to Boosting trees. Let's evaluate our predictions a little bit more in depth:"},{"metadata":{"trusted":true,"_uuid":"53823c052eff3d752797a630a2dcfe4af069d415"},"cell_type":"code","source":"logpreds = learn.predict(is_test=True)\npreds = np.exp(logpreds[:,1])\npreds","execution_count":36,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"collapsed":true,"_uuid":"2525c942e8ef69a04fdace6b1322a762b6785cb4"},"cell_type":"code","source":"def plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","execution_count":37,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"862f8662ad39cb4e17708fa97c7cfe41f903f019"},"cell_type":"code","source":"logpreds_valid = learn.predict(is_test = False)\npreds_valid = np.exp(logpreds_valid[:,1])\npreds_binary = (preds_valid >= 0.5).astype(np.int)\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_valid, preds_binary)\nplot_confusion_matrix(cm, [0, 1])","execution_count":38,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d3205fd551d4da879be354ee6aaec890a487f9d3"},"cell_type":"code","source":"from sklearn.metrics import classification_report\nprint(classification_report(y_valid,\n                            preds_binary,\n                            target_names= ['0', '1']))","execution_count":39,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"21b221edc005296f2db6474ff923f81b8ae7892b"},"cell_type":"code","source":"from sklearn.metrics import roc_curve\nfalse_positive_rate, true_positive_rate, threshold = roc_curve(y_valid,\n                                                               preds_valid)\n# Plot ROC curve\nplt.title(\"Receiver Operating Characteristic\")\nplt.plot(false_positive_rate, true_positive_rate)\nplt.plot([0, 1], ls=\"--\")\nplt.plot([0, 0], [1, 0] , c=\".7\"), plt.plot([1, 1] , c=\".7\")\nplt.ylabel(\"True Positive Rate\")\nplt.xlabel(\"False Positive Rate\")\nplt.show()\n","execution_count":40,"outputs":[]},{"metadata":{"_uuid":"6176b232ed86e1d64a840e3a9fd02dcfa51f3469"},"cell_type":"markdown","source":"Even though the loss function with weights did help, the model is still having trouble predicting the positive class: it is predicting it way too much.  Maybe the weight in the positive class was too big. We could always set it as any other hyperparameter by tweaking and comparing the different results in the validation set.\n\n## Wrap it up!"},{"metadata":{"trusted":true,"_uuid":"ec7498d203704336a21574dab00f323f2714627f","collapsed":true},"cell_type":"code","source":"submission = pd.DataFrame({'SK_ID_CURR': df_test['SK_ID_CURR'],\n              'TARGET': preds})\nsubmission.to_csv('submission.csv', index=False, float_format='%.8f')","execution_count":41,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"76398b8c339c79ae64294ff05f8d56797d6e19fa"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"a23c03c63ff579b279a513b4b01edae5401d17b4"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"92ff4f48b426b35d444a145ac4eadbd80e7a2a4e"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}