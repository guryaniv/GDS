{"cells":[{"metadata":{"_uuid":"597f4fd896cc4d77e693cd0199289490ee40f07e"},"cell_type":"markdown","source":"# CSU Machine Learning Team \nThis notebook is a collection of the code we are writing to learn and practice data science through the CSU Bioinformatics Club. Machine Learning team meets once a week to work on the Kaggle PetFinder competition for the first half of the Spring 2019 semester. \n\nThis notebook contains all the code I am writing in Python to compliment Steven's R code during meetups in case anyone wants to follow along with us in either R or Python to learn."},{"metadata":{"_uuid":"c36f100205461ce41bfef5d0022eccf57312c63a"},"cell_type":"markdown","source":"## Exploratory Data Analysis in Python \n### Week 1\nFirst meeting 02/01/2019 to discuss the basics of how the ML team meetups and Kaggle competition will work. \n\nWe first took a basic look at the data following code in R that Steven was writing to get a feel for the data. Here I am writing the same basic analysis in Python and making a few additional plots. This code should work in the Kaggle Kernel as well. "},{"metadata":{"trusted":true,"_uuid":"b2e182bad2a8f3282e847fb35a554fbe75545c09"},"cell_type":"code","source":"# First import the appropriate libraries\nimport os\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns \nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9100f73e2ad987596ef83537c9e9fa3a79d87d13"},"cell_type":"code","source":"# Take a look at the data we have available in the input directory\nprint(os.listdir('../input/'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"88653ef0f1473b96bd0e7eb9e652ce9e84b43355"},"cell_type":"code","source":"# Read in the label and training data\nstates = pd.read_csv('../input/state_labels.csv')\ncolors = pd.read_csv('../input/color_labels.csv')\nbreeds = pd.read_csv('../input/breed_labels.csv')\ntrain = pd.read_csv('../input/train/train.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7a2910e4f11703ae84845f378a4bdb706db0662a"},"cell_type":"code","source":"# Check out the contents\nprint(\"State Labels\")\nstates.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fb9f0bd1d817b94706d253f339ef85218c673b01"},"cell_type":"code","source":"print(\"Color Labels\")\ncolors.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"322b858167a621888125ce7b06d984c3e5b7af14"},"cell_type":"code","source":"print(\"Breed Labels\")\nbreeds.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3d610cb5576cd7485de7a5745b85787fa2aee6ff"},"cell_type":"code","source":"print(\"Training Data\")\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dabdf8624ba884c54967ba8da455088b6555e4ae"},"cell_type":"code","source":"# Look at summary statistics of training data\ntrain.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8d18934a00dec34e980bbb3074353f279c731e48"},"cell_type":"code","source":"# Variables we can look at from column names\nvar_name = list(train.columns)\n\n# Data type of each based on first entry\ndata_type = train.dtypes\n\nnum_unique = pd.Series({x: len(train.loc[:,x].unique()) for x in var_name})\n\n# Get summary stats from the describe table\nmin_val = train.describe().loc[\"mean\", :]\nmax_val = train.describe().loc[\"max\", :]\n\n# Get median via function to return NaN for non-numeric objects\ndef get_median(x, df, dtype):\n    if dtype[x] != 'object':\n        return df.loc[:,x].median()\n    else:\n        return np.NaN\n    \nmed_val = pd.Series({x: get_median(x,train,data_type) for x in var_name})\n\n# Get number of missing values\nnum_missing = pd.Series({x: train.loc[:,x].isnull().sum() for x in var_name})\n\n\n# Create a new data frame with some descriptive measures by merging these series\ndesc_measures = pd.DataFrame({\n    \"data_type\": data_type,\n    \"num_unique\": num_unique,\n    \"min\": min_val,\n    \"max\": max_val,\n    \"med\": med_val,\n    \"num_missing\": num_missing\n}, index=var_name)\n\n# Check contents\ndesc_measures","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3aa27ff19229bf73e27907a5125d57d16c8e73d2"},"cell_type":"code","source":"# Now to plot the number of animals being adopted based on categories of speed\nsns.set_style('darkgrid')\nax = sns.countplot(train.AdoptionSpeed, palette='Set2')\nax.set_xticklabels(labels=(\"Same Day\", \n                           \"1-7 days\", \n                           \"8-30 days\", \n                           \"31-90 days\", \n                           \"Not adopted after 100 days\"), rotation=60)\nax.set_xlabel(\"Adoption Speed\")\nax.set_ylabel(\"Animals Adopted\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9ec5b98e1847d0aefb69c80d4782799fb7b9e746"},"cell_type":"code","source":"# Split the plot into categories, is there one category that is being adopted faster/slower than others?\n# First check out the categories with only a few unique options \nfig, ax = plt.subplots(2,2)\nsns.countplot(x=\"AdoptionSpeed\", hue=\"Type\", data=train, ax=ax[0,0], palette='Set2')\nsns.countplot(x=\"AdoptionSpeed\", hue=\"Gender\", data=train, ax=ax[0,1], palette='Set2')\nsns.countplot(x=\"AdoptionSpeed\", hue=\"Health\", data=train, ax=ax[1,0], palette='Set2')\nsns.countplot(x=\"AdoptionSpeed\", hue=\"MaturitySize\", data=train, ax=ax[1,1], palette='Set2')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"214fcea25b0eb44628679e1d77fc8ecbf5449636"},"cell_type":"markdown","source":"## Applying a Random Forest Model \n### Week 2\n\nMeeting on 02/08/19. We applied a random forest model to the data. The purpose was to start figuring out what variables seem to be important for predicting adoption speed and continue exploring the data. My random forest model did not predict as well as Steven's did for some reason. The application of sklearn is based on the lecture on Random Forests from last semester when Steven was going through the Titanic competition."},{"metadata":{"trusted":true,"_uuid":"8e7e464d379e8bcb5345657e3c5abdcde032f5d0"},"cell_type":"code","source":"# Week 2 libraries to import - normally this should be at the very beginning, \n# but I am separating by week so you can see what libraries get added each week\n# We do use libraries from week 1 though\nimport warnings\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import export_graphviz\n\n# Ignore warnings from sklearn (omit this if you're still experimenting with code)\ndef warn(*args, **kwargs):\n    pass\nwarnings.warn = warn","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ccbe0f7518cd697d305a96a0281e8b3fd816e4a2"},"cell_type":"code","source":"# pull out the outcomes we want to predict. Since the data is skewed, look at values\n# greater than zero\ntrain_g0 = train.where(train.loc[:,\"AdoptionSpeed\"] > 0)\ncat_train_g0 = train.where(train.loc[:,\"Type\"] == 2)\ndog_train_g0 = train.where(train.loc[:,\"Type\"] == 1)\n\n# note: we dropped Breed1 and RescuerID, but need to revisit these\ncat_train_g0_subset = cat_train_g0.loc[:,(\"Age\", 'Gender', 'Vaccinated', \n                                          'Dewormed', 'Health', 'Fee', 'PhotoAmt',\n                                          'Sterilized', 'FurLength', 'Color1',\n                                          'MaturitySize', 'AdoptionSpeed')].dropna()\n\ncat_outcomes_g0 = cat_train_g0_subset.loc[:,'AdoptionSpeed']\n\ndog_train_g0_subset = dog_train_g0.loc[:,(\"Age\", 'Gender', 'Vaccinated', \n                                          'Dewormed', 'Health', 'Fee', 'PhotoAmt',\n                                          'Sterilized', 'FurLength', 'Color1',\n                                          'MaturitySize', 'AdoptionSpeed')].dropna()\n\ndog_outcomes_g0 = dog_train_g0_subset.loc[:,'AdoptionSpeed']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d41382172236dfe9c3248e127813006bb9645cd1"},"cell_type":"code","source":"# Fix the data types to contain categorical variables\ncategory_cols = {'Age': np.int32,\n    'Gender': 'category',\n    'Vaccinated': 'category',\n    'Dewormed': 'category',\n    'Health': 'category',\n    'Fee': np.int32,\n    'PhotoAmt': np.int32,\n    'Sterilized': 'category',\n    'FurLength': 'category',\n    'Color1': 'category',\n    'MaturitySize': 'category',\n    'AdoptionSpeed': np.int32\n    }\n\nfor col,dtype in category_cols.items():\n    cat_train_g0_subset.loc[:,col] = cat_train_g0_subset.loc[:,col].astype(dtype)\n    dog_train_g0_subset.loc[:,col] = dog_train_g0_subset.loc[:,col].astype(dtype)\n    \ncat_train_g0_subset.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bf646b3c277e0df5d992af130ae77501d5d863c3"},"cell_type":"code","source":"# Run a random forest model to determine what is driving empirical models\n# Helpful in feature engineering\n\n# Used the titanic code Steven shared with us as a baseline \n# Split the data into training and testing sets (20% of the data for testing)\nXtrain, Xtest, ytrain, ytest = train_test_split(cat_train_g0_subset.drop('AdoptionSpeed',axis=1), cat_outcomes_g0, test_size=0.20, random_state=154)\n\n# Train and score the classifier\nclassifier = RandomForestClassifier(criterion='gini', n_jobs=4, random_state=154, n_estimators=100, oob_score=True)\nclassifier.fit(Xtrain, ytrain)\nscores = classifier.score(Xtest, ytest)\n\nprint('The score of this Random Forest Classifier is {:.3f}'.format(scores))\nprint('The OOB score of this Random Forest Classifier is {:.3f}'.format(classifier.oob_score_))\n\n# Feature importance values\nimportances = list(classifier.feature_importances_)\nx_values = list(range(len(importances)))\n\n# Feature importance plot\nplt.figure(num=None, figsize=(8, 6), dpi=80)\nplt.bar(x_values, importances, orientation='vertical')\nplt.xticks(x_values, Xtrain.columns, rotation='vertical')\nplt.ylabel('Importance')\nplt.xlabel('Feature')\nplt.title('Feature Importances: Gini Criterion')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6aa13fa49acfe07b5d8f8cdd40a023c9ef1a8772"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false}},"nbformat":4,"nbformat_minor":1}