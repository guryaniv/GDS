{"cells": [{"metadata": {"_cell_guid": "1b40d022-263c-4146-aeac-4d7bcbcc7d02", "_uuid": "49667d621599935f6c3124c31f79cbd68227d775"}, "source": ["Some ideas to consider\n", "\n", "* Word Frequency (WF)\n", "* Sentence Length (SL)\n", "* Paragraph Length (PL)\n", "* Vocabulary Complexity (VC) (does the author use simple words, or more elegant ones?)\n", "    [Pattern](https://www.clips.uantwerpen.be/pattern) ? This my be better known as 'lexical richness' \n", "* Bayesian Analysis (BA)\n", "* Semanitc Analysis (SA) (What mood does the author write in?)\n", "\n", "I would like to explore each of these, but the initial focus will be attemting to do some level semantic analysis to determine the author of a particular phrase.  Semantic analysis alone will not be enough here, but could be an important factor to consider.\n", "\n", "Also, will be pursuing the use of image classification for this problem.  Using https://pypi.python.org/pypi/pylinkgrammar we can geenrate sentence diagrams.  We can them trun these diagrams into images.  Then we can train an image classifier against these images,"], "cell_type": "markdown"}, {"metadata": {"_cell_guid": "9f0732b5-97f8-4b3b-8705-2f27c8e2bd3b", "_uuid": "53b11b12d1cd13459bc3b0644a2c12adeeebac82"}, "source": ["This will be our block for all imports.  We can do them inline, this just keeps them in one spot."], "cell_type": "markdown"}, {"outputs": [], "execution_count": null, "source": ["import numpy as np\n", "import pandas as pd\n", "import nltk\n", "import matplotlib.pyplot as plt\n", "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n", "from wordcloud import WordCloud, STOPWORDS"], "metadata": {"_cell_guid": "c3bff9cb-567a-4700-8739-9acba49af9e1", "_uuid": "eb00b391db46997ac612e26c46960ec65df25a0c", "collapsed": true, "_kg_hide-output": true}, "cell_type": "code"}, {"metadata": {"_cell_guid": "0f566f6a-42e7-4bf1-8319-ece414eeab4b", "_uuid": "21dcd431cce6a8c1691df59fbd6b8127c601cb91"}, "source": ["Some obligatory 'first steps'.  Load our data and show a sample."], "cell_type": "markdown"}, {"outputs": [], "execution_count": null, "source": ["train = pd.read_csv(\"../input/train.csv\")\n", "train.head()"], "metadata": {"_cell_guid": "680e71b6-11ab-44f2-9a9a-5c3cc1f44a45", "_uuid": "801a80b9b06da30bc1504ce2066d20bed3ff4e31", "collapsed": true}, "cell_type": "code"}, {"metadata": {"_cell_guid": "1e1ba728-dd18-4cfa-9e01-9829249ec948", "_uuid": "4a11ac578ea89d52ca24ee3a22cc3fbde2ec3932"}, "source": ["Show the 'shape' of the data i this case 19759 passages grouped by three authors."], "cell_type": "markdown"}, {"outputs": [], "execution_count": null, "source": ["print(train.shape)"], "metadata": {"_cell_guid": "4b808fcf-835c-4e5c-a845-9a1fdfd1c331", "_uuid": "f1dac802ea48f3dc19e2839398af2b94f0bea0e0", "collapsed": true}, "cell_type": "code"}, {"metadata": {"_cell_guid": "6df5bc31-9435-4df5-ae7a-519450de031c", "_uuid": "2d7dd327bcac00ea0e141d32ab25254d483b7649"}, "source": ["Here we just extract the text values by author, and print one to show some content."], "cell_type": "markdown"}, {"outputs": [], "execution_count": null, "source": ["eap = train[train.author==\"EAP\"][\"text\"].values; print(eap)\n", "hpl = train[train.author==\"HPL\"][\"text\"].values\n", "mws = train[train.author==\"MWS\"][\"text\"].values"], "metadata": {"_cell_guid": "0d004ccd-316a-46e1-8eef-93c2640d3a7c", "_uuid": "2c1d3c04069de46095e49fd1e07f4bca300ea448", "collapsed": true}, "cell_type": "code"}, {"metadata": {"_cell_guid": "50fd8351-4fc2-462b-a1bb-f2239416b9a2", "_uuid": "4e318a65f11209c894f18ec078aee305d7a0d7de"}, "source": ["Cloud words are a cool visual, and I copied this straight from another notebook."], "cell_type": "markdown"}, {"outputs": [], "execution_count": null, "source": ["def make_cloud(terms):\n", "    wc = WordCloud(background_color=\"black\", max_words=10000, \n", "               stopwords=STOPWORDS, max_font_size= 40)\n", "    wc.generate(\" \".join(terms))\n", "    return wc"], "metadata": {"_cell_guid": "3f2b5712-2bd1-44a8-9b26-673499eab60d", "_uuid": "6dfff8327e58c04d7a846636612bd6b037f91240", "collapsed": true}, "cell_type": "code"}, {"outputs": [], "execution_count": null, "source": ["plt.figure(figsize=(14,11))\n", "plt.title(\"HP Lovecraft\", fontsize=16)\n", "plt.imshow(make_cloud(hpl).recolor( colormap= 'Pastel2' , random_state=17), alpha=0.9)"], "metadata": {"_cell_guid": "f59953dc-97e6-44a2-8cd1-8b4a4538dd0c", "_uuid": "e74d6a852e364d87aef5c35526dff1db7fde404d", "scrolled": true, "collapsed": true}, "cell_type": "code"}, {"metadata": {"_cell_guid": "b3fe0ec8-a86e-429a-bc62-f752e24fc7ae", "_uuid": "60d0aa811007646b8ab972b2ddf4763644263a85"}, "source": ["Here is a function that will 'clean' the test a bit.  It will tokenize and remove all stop words"], "cell_type": "markdown"}, {"outputs": [], "execution_count": null, "source": ["def clean_text(text):\n", "    text_list = nltk.word_tokenize(text)\n", "    stopwords = nltk.corpus.stopwords.words('english')\n", "    return [word for word in text_list if word.lower() not in stopwords]"], "metadata": {"_cell_guid": "d4b99708-c1b1-412f-a8a3-0132879e793c", "_uuid": "33986a3cf28018bb22744b6aa2834f4dec4ae716", "collapsed": true}, "cell_type": "code"}, {"metadata": {"_cell_guid": "951cbd4c-0983-46d6-93d8-d7854efbcb22", "_uuid": "fe83372a7e283c192cf9a8ab410ed61afe4ed50a"}, "source": ["Next we'll create a sentiment analyzer"], "cell_type": "markdown"}, {"outputs": [], "execution_count": null, "source": ["analyzer = SentimentIntensityAnalyzer()"], "metadata": {"_cell_guid": "c9c1e7d8-e929-4ba9-a4b9-0656a29cb4c6", "_uuid": "82c92af9e2a5a750fbf0487e04cdc76787c7ea43", "collapsed": true}, "cell_type": "code"}, {"metadata": {"_cell_guid": "5ac1027e-381f-4197-84ad-8981ac50a954", "_uuid": "f1e7cde1b426e6aa0e09378e7173ed172895a9ed"}, "source": ["Here we calculate the 'avg semantic value' for an author. (This may be a poor approach, but I am just starting. :) ) Additionally we are tracking the average word and sentence lengths."], "cell_type": "markdown"}, {"outputs": [], "execution_count": null, "source": ["def calc_avg_stats(author, text):\n", "    pos = 0 \n", "    neg = 0 \n", "    neu = 0\n", "    word_lens = 0 \n", "    total_words = 0\n", "    length = text.size\n", "    print('Analyzing %d passages for author: %s...' % (length, author))\n", "    for s in text:\n", "        res = analyzer.polarity_scores(s)\n", "        pos += res['pos']\n", "        neg += res['neg']\n", "        neu += res['neg']\n", "        words = clean_text(s)\n", "        word_lens += sum([len(w) for w in words])\n", "        total_words += len(words)\n", "    return {\n", "        'avg_pos': (pos/length), #avg positive semantic score\n", "        'avg_neg': (neg/length), #avg negative semantic score\n", "        'avg_neu': (neu/length), #avg neutral semantic score\n", "        'avg_wlen': (word_lens/total_words), #avg word length\n", "        'lex_rich': (total_words/length) #lexical richness\n", "        }"], "metadata": {"_cell_guid": "47225e09-e335-4628-9029-899b752b318c", "_uuid": "cb95d8e9e0260ab488e8c98d20a0612e49e17cc7", "collapsed": true}, "cell_type": "code"}, {"outputs": [], "execution_count": null, "source": ["print(calc_avg_stats('HPL', hpl))\n", "print(calc_avg_stats('MWS', mws))\n", "print(calc_avg_stats('EAP', eap))"], "metadata": {"_cell_guid": "d811c467-e080-4401-9023-ba100293039b", "_uuid": "20d37432dfca1195ae2579a358a8e43cb5e2c58e", "collapsed": true}, "cell_type": "code"}, {"outputs": [], "execution_count": null, "source": [], "metadata": {"_cell_guid": "7507b031-7f43-49dd-9423-fd6057408b29", "_uuid": "08d493dfeb31bdf06fbb20be7b3c917bb403f2ec", "collapsed": true}, "cell_type": "code"}], "metadata": {"kernelspec": {"language": "python", "display_name": "Python 3", "name": "python3"}, "language_info": {"codemirror_mode": {"version": 3, "name": "ipython"}, "name": "python", "mimetype": "text/x-python", "pygments_lexer": "ipython3", "version": "3.6.3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat_minor": 1, "nbformat": 4}