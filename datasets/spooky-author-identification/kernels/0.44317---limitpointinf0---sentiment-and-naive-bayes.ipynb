{"cells":[{"metadata":{"_cell_guid":"484a78e8-c63b-465c-8269-1515b85878b2","_uuid":"8b06a49d22d891117c976add949f5b90a352948a"},"cell_type":"markdown","source":"## Examine Raw Data"},{"metadata":{"_kg_hide-input":true,"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport string\nimport matplotlib.pyplot as plt\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nfrom nltk.tokenize import word_tokenize\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nimport warnings \nimport seaborn as sns\nwarnings.filterwarnings(\"ignore\")\n\ndef clean_ColText(data, col, stem=True):\n    \"\"\"Takes dataframe and column name and returns a dataframe with cleaned strings in the form of a list. Stemming is an option.\"\"\"\n    df = data.copy()\n    table = str.maketrans('', '', string.punctuation)\n    df[col] = df[col].map(lambda x: x.translate(table)) #remove punctuation\n    df[col] = df[col].map(lambda x: x.lower()) #lowercase\n    df[col] = df[col].apply(word_tokenize) #tokenize\n    stop_words = set(stopwords.words('english'))\n    df[col] = df[col].map(lambda x: [y for y in x if not y in stop_words]) #remove stop words\n    df[col] = df[col].map(lambda x: [y for y in x if y not in [\"’\",\"’\",\"”\",\"“\",\"‘\",\"—\"]]) #remove smart quotes and other non alphanums\n    if stem:\n        porter = PorterStemmer()\n        df[col] = df[col].map(lambda x: [porter.stem(y) for y in x])\n        return df\n    return df\n\npd.read_csv('../input/train.csv').head(10)","execution_count":1,"outputs":[]},{"metadata":{"_cell_guid":"dc92f6c3-d7d0-409a-b899-c8cc21b81584","_uuid":"06fb63d93bb5c673f3f889fcd12554ada02eda10"},"cell_type":"markdown","source":"## Adding Sentiment Features"},{"metadata":{"_kg_hide-input":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"sid = SentimentIntensityAnalyzer()\ndf = pd.read_csv('../input/train.csv')\ndf.id = df.id.map(lambda x: int(x.replace('id','')))\ndf['sent'] = df['text'].map(lambda x: sid.polarity_scores(x)['compound'])\ndf = clean_ColText(df, 'text', stem=True)\ndf = df.drop('id', axis=1)\ndf.head(10)","execution_count":12,"outputs":[]},{"metadata":{"_cell_guid":"f1e053db-6490-4688-ac47-4eed8fdd4aca","_uuid":"47121fcf5f4489b7c252fdda837ff5e56ee7e882"},"cell_type":"markdown","source":"Lets look at how sentiment distribution differs amongst authors."},{"metadata":{"_kg_hide-input":true,"_cell_guid":"0055bf84-4fc9-44b0-9811-dd65052859c4","_uuid":"77414bdffeee5abd1b42b6fb6d718fc93c61179e","trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,5))\nplt.title('Sentiments of Authors')\nsns.boxplot(x='author', y='sent', data=df)\nplt.show()","execution_count":3,"outputs":[]},{"metadata":{"_cell_guid":"090e7b04-b0e0-41e1-9129-2880108b4a7e","_uuid":"a3318eb83d97f49905507faba7c6ec19c69a846b"},"cell_type":"markdown","source":"## Preparing Data For Modeling"},{"metadata":{"_kg_hide-input":true,"_cell_guid":"d6ebefd5-a0b3-45d9-9d29-2ac53c27e3be","_uuid":"97a62a8ef9fe4c23789ea9d2dec10e5fdc054ae9","trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\n\ndf['text'] = df.text.map(lambda x: ' '.join(x))\nsent = np.array(df.sent) + 1\nX = df.drop(['author','sent'], axis=1)\ny = df.author\n\ncount_vect = CountVectorizer()\ntfidf_transformer = TfidfTransformer()\nX = count_vect.fit_transform(X.text)\nX = tfidf_transformer.fit_transform(X)\nX = X.toarray()\nprint(X.shape)","execution_count":13,"outputs":[]},{"metadata":{"_cell_guid":"b9c8fa79-79c4-4ad5-ac45-2cb316d5b899","_uuid":"c1637ed194e684e408104b9114cea82ebe46d60a"},"cell_type":"markdown","source":"Adding sentiment column to numpy array."},{"metadata":{"_cell_guid":"6e0cda0d-328d-41b3-9854-6a01c00a08ea","_uuid":"1c2578c5d2a959d2d3eb3627e505327cd0e8630b","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from scipy import sparse\n\nsent = sent.reshape((sent.shape[0],1))\nX = np.hstack((X, sent))\nX = sparse.csr_matrix(X)\nprint(X.shape)","execution_count":14,"outputs":[]},{"metadata":{"_uuid":"891e897a1fb6f3ceab8726960f6b657db9226076"},"cell_type":"markdown","source":"# Classification with NB"},{"metadata":{"_uuid":"ce21729e407707dac216cd4a01ecf39251b86c13"},"cell_type":"markdown","source":"Testing with Cross Validation"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"634e18438b7859055a4d7c68145234db02d02651"},"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\nfrom sklearn.model_selection import cross_val_score\n\nclf = MultinomialNB(alpha=0.1)\nscores = cross_val_score(clf, X, df.author, cv=5)\nprint('accuracy CV:',scores)","execution_count":38,"outputs":[]},{"metadata":{"_uuid":"de907242bac3e21e09bcd67a922e1061ebbde983"},"cell_type":"markdown","source":"Final Fitting"},{"metadata":{"trusted":true,"_uuid":"14e8425330257f438d2e6d6bbc2a90f03501fc76"},"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\nfrom sklearn.model_selection import cross_val_score\n\nfinal = MultinomialNB(alpha=0.1)\nfinal.fit(X, df.author)\n","execution_count":39,"outputs":[]},{"metadata":{"_uuid":"f7a8390261b54e2d42daabb8b4301c5ee58f75b4"},"cell_type":"markdown","source":"Processing test data for prediction. I want to use the same processing I used on my training data."},{"metadata":{"trusted":true,"_uuid":"101c4102af7695bd4b0b81ad4a5dfb693f983180"},"cell_type":"code","source":"df_t = pd.read_csv('../input/test.csv')\n\ndf_t['sent'] = df_t['text'].map(lambda x: sid.polarity_scores(x)['compound'])\ndf_t = clean_ColText(df_t, 'text', stem=True)\ndf_t['text'] = df_t.text.map(lambda x: ' '.join(x))\n\nX_t = df_t.drop(['sent','id'], axis=1).text\nX_t = count_vect.transform(X_t)\nX_t = tfidf_transformer.transform(X_t)\nX_t = X_t.toarray()\nsent_t = np.array(df_t.sent) + 1\nsent_t = sent_t.reshape((sent_t.shape[0],1))\nprint(X_t.shape, sent_t.shape)\nX_t = np.hstack((X_t, sent_t))\n\nX_t = sparse.csr_matrix(X_t)\nprint(X_t.shape)","execution_count":53,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"474e42e454a3a317fdf77e8fbc2b7adf79a32504"},"cell_type":"code","source":"preds = final.predict_proba(X_t)\ndf_t['EAP'] = preds[:,0]\ndf_t['HPL'] = preds[:,1]\ndf_t['MWS'] = preds[:,2]\ndf_t.head()","execution_count":59,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dc447f67cef38455353cf0a165c862c852a6e16d"},"cell_type":"code","source":"df_t = df_t.drop(['text', 'sent'], axis=1)\ndf_t.to_csv('results.csv', index=False)","execution_count":62,"outputs":[]}],"metadata":{"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}