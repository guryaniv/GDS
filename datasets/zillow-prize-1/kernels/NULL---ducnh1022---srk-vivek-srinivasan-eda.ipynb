{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "5b000e33-f0c2-9e94-14b3-ea474d5154c3"
      },
      "outputs": [],
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load in \n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "import missingno as msno\n",
        "from datetime import datetime\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import kendalltau\n",
        "import warnings\n",
        "\n",
        "color = sns.color_palette()\n",
        "\n",
        "# Input data files are available in the \"../input/\" directory.\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
        "\n",
        "from subprocess import check_output\n",
        "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n",
        "\n",
        "# Any results you write to the current directory are saved as output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "8623318b-8184-2f20-4827-258a31c6861b"
      },
      "outputs": [],
      "source": [
        "train_df = pd.read_csv(\"../input/train_2016.csv\", parse_dates=[\"transactiondate\"])\n",
        "prop_df = pd.read_csv(\"../input/properties_2016.csv\")\n",
        "train_df = pd.merge(train_df, prop_df, on='parcelid', how='left')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "57cfa686-3236-8e2c-b838-7d94bff32867"
      },
      "outputs": [],
      "source": [
        "# initial\n",
        "col_tar = \"logerror\"\n",
        "col_id = \"parcelid\"\n",
        "print(\"SHAPE: \" + str(train_df.shape))\n",
        "print(\"\\n\" + \"=\"*90)\n",
        "print(\"\\nSAMPLE DATA:\" )\n",
        "with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
        "    print(train_df.head(2).transpose())\n",
        "    \n",
        "    \n",
        "# target distribution\n",
        "\n",
        "ulimit = np.percentile(train_df[col_tar].values, 99)\n",
        "llimit = np.percentile(train_df[col_tar].values, 1)\n",
        "train_df[col_tar].ix[train_df[col_tar]>ulimit] = ulimit\n",
        "train_df[col_tar].ix[train_df[col_tar]<llimit] = llimit\n",
        "\n",
        "fig,ax = plt.subplots()\n",
        "fig.set_size_inches(20,5)\n",
        "sns.distplot(train_df[col_tar].values, bins=50,kde=False,color=\"#34495e\",ax=ax)\n",
        "ax.set(xlabel=col_tar, ylabel='VIF Score',title=\"Distribution Of Dependent Variable\")\n",
        "plt.show()\n",
        "\n",
        "# dtype print, you should encode this   \n",
        "\n",
        "dtype_df = train_df.dtypes.reset_index()\n",
        "dtype_df.columns = [\"count\", \"column_type\"]\n",
        "print(\"\\n\" + \"=\"*90)\n",
        "print(\"\\n DTYPE_DF: \" )\n",
        "with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
        "    print(dtype_df)\n",
        "\n",
        "dtype_count_df = dtype_df.groupby(\"column_type\").aggregate('count').reset_index()\n",
        "print(\"\\n\" + \"=\"*90)\n",
        "print(\"\\n DTYPE_COUNT_DF: \" )\n",
        "with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
        "    print(dtype_count_df)\n",
        "    \n",
        "fig,ax = plt.subplots()\n",
        "fig.set_size_inches(20,len(dtype_count_df))\n",
        "plt.ylabel('column_type', fontsize=20)\n",
        "plt.xlabel('count', fontsize=20)\n",
        "plt.title(\"Dtype Count\", fontsize=30)\n",
        "sns.barplot(data=dtype_count_df,y=\"column_type\",x=\"count\",ax=ax,palette=\"Blues_d\",orient=\"h\")\n",
        "plt.show()\n",
        "\n",
        "# missing values\n",
        "\n",
        "missing_df = train_df.isnull().sum(axis=0).reset_index()\n",
        "missing_df.columns = ['column_name', 'missing_count']\n",
        "missing_df['missing_ratio'] = missing_df['missing_count'] / train_df.shape[0]\n",
        "missing_df_top = missing_df.ix[missing_df['missing_ratio']>0.8].sort_values(by=\"missing_ratio\", ascending=False)\n",
        "print(\"\\n\" + \"=\"*90)\n",
        "print(\"\\n MISSING_DF: \" )\n",
        "with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
        "    print(missing_df_top)\n",
        "\n",
        "fig,ax = plt.subplots()\n",
        "fig.set_size_inches(20,len(missing_df_top))\n",
        "sns.barplot(data=missing_df_top,y=\"column_name\",x=\"missing_count\",ax=ax,color=\"#34495e\",orient=\"h\")\n",
        "plt.ylabel('column_type', fontsize=20)\n",
        "plt.xlabel('count', fontsize=20)\n",
        "plt.title(\"Missing Value Count\", fontsize=30)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "# dtype print, you should encode this \n",
        "# missing values\n",
        "# drop col Object\n",
        "# detect col Date, generate year, month, day_month, day_week, plot with target value and # of occurences\n",
        "# detect long, lat and plot\n",
        "# correlation, plot, imp1\n",
        "# make predict with xgboost and random_forest, plot importance, imp2, imp3\n",
        "# imp1, imp2, imp3 => 10 most important feature\n",
        "#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "349fec29-ba58-0eca-44e7-87c25e50a86e"
      },
      "outputs": [],
      "source": [
        "# dtype print, you should encode this \n",
        "# missing values\n",
        "# drop col Object\n",
        "dropping = dtype_df[dtype_df['column_type'] == 'object']['count'].values.tolist() + missing_df_top[missing_df_top['missing_ratio'] > 0.9]['column_name'].values.tolist()\n",
        "for drop in dropping:\n",
        "    print(\"Drop: \"+ str(drop))\n",
        "\n",
        "train_df_0 = train_df.drop(labels=dropping,axis=1)\n",
        "train_df_0.head(2)\n",
        "# detect col Date, generate year, month, day_month, day_week, plot with target value and # of occurences\n",
        "# detect long, lat and plot\n",
        "# correlation, plot, imp1\n",
        "# make predict with xgboost and random_forest, plot importance, imp2, imp3\n",
        "# imp1, imp2, imp3 => 10 most important feature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "e526d296-a681-89cb-6679-f117612c16f1"
      },
      "outputs": [],
      "source": [
        "col_date = dtype_df[dtype_df['column_type'] == 'datetime64[ns]']['count'].values.tolist()[0]\n",
        "\n",
        "train_date_df = train_df.copy()\n",
        "\n",
        "train_date_df[col_date+'_year'] = train_date_df[col_date].dt.year\n",
        "train_date_df[col_date+'_month'] = train_date_df[col_date].dt.month\n",
        "train_date_df[col_date+'_day'] = train_date_df[col_date].dt.day\n",
        "train_date_df[col_date+'_dayofyear'] = train_date_df[col_date].dt.dayofyear\n",
        "train_date_df[col_date+'_weekday'] = train_date_df[col_date].dt.weekday\n",
        "\n",
        "for col in [col_date+'_year',col_date+'_month',col_date+'_day',col_date+'_dayofyear',col_date+'_weekday']:\n",
        "    print(col)\n",
        "    cnt_srs = train_date_df[col].value_counts()\n",
        "    plt.figure(figsize=(12,6))\n",
        "    sns.barplot(cnt_srs.index, cnt_srs.values, alpha=0.8, color=\"#34495e\")\n",
        "    plt.xticks(rotation='vertical')\n",
        "    plt.xlabel(col + ' of transaction', fontsize=12)\n",
        "    plt.ylabel('Number of Occurrences', fontsize=12)\n",
        "    plt.show()\n",
        "\n",
        "    train_group_temp = train_date_df.groupby(col)['logerror'].mean().to_frame().reset_index()\n",
        "    plt.figure(figsize=(12,6))\n",
        "    sns.pointplot(x=train_group_temp[col], y=train_group_temp[\"logerror\"], data=train_group_temp, join=True,color=\"#34495e\")\n",
        "    plt.xlabel(col + ' of transaction', fontsize=12)\n",
        "    plt.ylabel('Mean target', fontsize=12)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "3d2cb46e-54b6-baa3-e879-ccb44056a16b"
      },
      "outputs": [],
      "source": [
        "lng = False\n",
        "lat = False\n",
        "for col in train_df.columns:\n",
        "    if \"longitude\" in col:\n",
        "        lng = True\n",
        "    if \"latitude\" in col:\n",
        "        lat = True\n",
        "if(lng and lat):\n",
        "    plt.figure(figsize=(12,12))\n",
        "    sns.jointplot(x=prop_df.latitude.values, y=prop_df.longitude.values, size=10)\n",
        "    plt.ylabel('Longitude', fontsize=12)\n",
        "    plt.xlabel('Latitude', fontsize=12)\n",
        "    plt.show()\n",
        "# detect long, lat and plot\n",
        "# correlation, plot, imp1\n",
        "# make predict with xgboost and random_forest, plot importance, imp2, imp3\n",
        "# imp1, imp2, imp3 => 10 most important feature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "2c3c345d-82c0-aab4-a29a-19211babcf10"
      },
      "outputs": [],
      "source": [
        "# Let us just impute the missing values with mean values to compute correlation coefficients #\n",
        "mean_values = train_df.mean(axis=0)\n",
        "train_df_new = train_df.fillna(mean_values, inplace=True)\n",
        "\n",
        "# Now let us look at the correlation coefficient of each of these variables #\n",
        "x_cols = [col for col in train_df_new.columns if col not in [col_tar] if train_df_new[col].dtype=='float64']\n",
        "\n",
        "labels = []\n",
        "values = []\n",
        "for col in x_cols:\n",
        "    labels.append(col)\n",
        "    values.append(np.corrcoef(train_df_new[col].values, train_df_new[col_tar].values)[0,1])\n",
        "corr_df = pd.DataFrame({'col_labels':labels, 'corr_values':values})\n",
        "corr_df = corr_df.sort_values(by='corr_values')\n",
        "\n",
        "ind = np.arange(len(labels))\n",
        "width = 0.9\n",
        "fig, ax = plt.subplots(figsize=(12,40))\n",
        "rects = ax.barh(ind, np.array(corr_df.corr_values.values), color='#34495e')\n",
        "ax.set_yticks(ind)\n",
        "ax.set_yticklabels(corr_df.col_labels.values, rotation='horizontal')\n",
        "ax.set_xlabel(\"Correlation coefficient\")\n",
        "ax.set_title(\"Correlation coefficient of the variables\")\n",
        "#autolabel(rects)\n",
        "plt.show()\n",
        "\n",
        "corr_df[\"abs\"] = np.abs(corr_df[\"corr_values\"])\n",
        "col_use_0 = corr_df.sort_values(by=\"abs\",ascending=False)[:10][\"col_labels\"].tolist()\n",
        "\n",
        "temp_df = train_df[col_use_0 + [col_tar]]\n",
        "corrmat = temp_df.corr(method='spearman')\n",
        "f, ax = plt.subplots(figsize=(8, 8))\n",
        "\n",
        "# Draw the heatmap using seaborn\n",
        "sns.heatmap(corrmat, vmax=1., square=True)\n",
        "plt.title(\"Important variables correlation map\", fontsize=15)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# correlation, plot, imp1\n",
        "# make predict with xgboost and random_forest, plot importance, imp2, imp3\n",
        "# imp1, imp2, imp3 => 10 most important feature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "86ece756-d470-0c34-b0f1-9b8145c9973d"
      },
      "outputs": [],
      "source": [
        "from sklearn import model_selection, preprocessing\n",
        "import xgboost as xgb\n",
        "\n",
        "train_df_new = train_df_new.fillna(-999)\n",
        "for f in train_df_new.columns:\n",
        "    if train_df_new[f].dtype=='object':\n",
        "        lbl = preprocessing.LabelEncoder()\n",
        "        lbl.fit(list(train_df_new[f].values)) \n",
        "        train_df_new[f] = lbl.transform(list(train_df_new[f].values))\n",
        "        \n",
        "train_y = train_df_new[col_tar].values\n",
        "train_X = train_df_new.drop([col_tar,col_id,col_date], axis=1)\n",
        "\n",
        "xgb_params = {\n",
        "    'eta': 0.05,\n",
        "    'max_depth': 8,\n",
        "    'subsample': 0.7,\n",
        "    'colsample_bytree': 0.7,\n",
        "    'objective': 'reg:linear',\n",
        "    'eval_metric': 'rmse',\n",
        "    'silent': 1\n",
        "}\n",
        "dtrain = xgb.DMatrix(train_X, train_y, feature_names=train_X.columns.values)\n",
        "model = xgb.train(dict(xgb_params, silent=0), dtrain, num_boost_round=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d03bb6af-e204-5cee-e4e5-4e30016bdef7"
      },
      "outputs": [],
      "source": [
        "featureImportance = model.get_fscore()\n",
        "features = pd.DataFrame()\n",
        "features['features'] = featureImportance.keys()\n",
        "features['importance'] = featureImportance.values()\n",
        "features.sort_values(by=['importance'],ascending=False,inplace=True)\n",
        "fig,ax= plt.subplots()\n",
        "fig.set_size_inches(20,10)\n",
        "plt.xticks(rotation=90)\n",
        "sns.barplot(data=features.head(15),x=\"importance\",y=\"features\",ax=ax,orient=\"h\",color=\"#34495e\")\n",
        "col_use_1 = features[:10][\"features\"].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "79ac2fda-82f9-3f9f-c229-3b51a703d9cd"
      },
      "outputs": [],
      "source": [
        "from sklearn import ensemble\n",
        "model0 = ensemble.ExtraTreesRegressor(n_estimators=25, max_depth=30, max_features=0.3, n_jobs=-1, random_state=0)\n",
        "model0.fit(train_X, train_y)\n",
        "\n",
        "importances = model0.feature_importances_\n",
        "std = np.std([tree.feature_importances_ for tree in model0.estimators_], axis=0)\n",
        "indices = np.argsort(importances)[::-1][:20]\n",
        "feat_names = train_X.columns\n",
        "plt.figure(figsize=(12,12))\n",
        "plt.title(\"Feature importances\")\n",
        "plt.bar(range(len(indices)), importances[indices], color=\"#34495e\", yerr=std[indices], align=\"center\")\n",
        "plt.xticks(range(len(indices)), feat_names[indices], rotation='vertical')\n",
        "plt.xlim([-1, len(indices)])\n",
        "plt.show()\n",
        "col_use_2 = feat_names[indices][:10].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "270f452b-66b8-2922-dffd-588dfbfe4920"
      },
      "outputs": [],
      "source": [
        "cols = {}\n",
        "for col in (col_use_0 + col_use_1 + col_use_2):\n",
        "    if col not in cols.keys():\n",
        "        cols[col] = 1\n",
        "    else:\n",
        "        cols[col] = cols[col] + 1\n",
        "\n",
        "col_df = pd.DataFrame(cols,index=[0]).transpose()\n",
        "col_df.columns = [\"count\"]\n",
        "col_df.sort_values(by=\"count\",inplace=True,ascending=False)\n",
        "col_list = col_df.index[:10].tolist()\n",
        "col_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "8a684345-20ed-bb01-190d-75a7b40dd44b"
      },
      "outputs": [],
      "source": [
        "col_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c9618e4e-97f8-8741-bbfe-e60c75aa1893"
      },
      "outputs": [],
      "source": [
        "for col in col_list:\n",
        "    uniq = len(train_df[col].unique())\n",
        "    print(uniq)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "0ef0211f-55b0-1d9c-73de-0d57b74d921f"
      },
      "outputs": [],
      "source": ""
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}