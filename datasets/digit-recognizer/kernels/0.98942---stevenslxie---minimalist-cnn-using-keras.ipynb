{"metadata": {"language_info": {"pygments_lexer": "ipython3", "file_extension": ".py", "mimetype": "text/x-python", "version": "3.6.3", "nbconvert_exporter": "python", "name": "python", "codemirror_mode": {"version": 3, "name": "ipython"}}, "kernelspec": {"display_name": "Python 3", "name": "python3", "language": "python"}}, "nbformat": 4, "nbformat_minor": 1, "cells": [{"cell_type": "markdown", "source": ["# Minimalist CNN using Keras\n", "#### A simple yet effective guide to Keras implementation of CNN\n", "by @stevenslxie\n", "\n", "#### Accuracy: 0.98942\n", "\n", "Note that the data preprocessing part of this guide is mainly from [Tensorflow Deep NN](https://www.kaggle.com/kakauandme/tensorflow-deep-nn)\n", "\n", "## Libraries and settings"], "metadata": {"_cell_guid": "2bb191db-8382-4272-a5a8-9c428da9ca2e", "_uuid": "8b832e99e39543728f0bcb11c57e88a806ab7f81"}}, {"cell_type": "code", "execution_count": null, "source": ["import numpy as np\n", "import pandas as pd\n", "import tensorflow as tf\n", "\n", "from keras import layers\n", "from keras.layers import Input, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D\n", "from keras.layers import AveragePooling2D, MaxPooling2D, Dropout, GlobalMaxPooling2D, GlobalAveragePooling2D\n", "from keras.models import Model\n", "from keras.preprocessing import image\n", "from keras.utils import layer_utils\n", "from keras.utils.data_utils import get_file\n", "from keras.applications.imagenet_utils import preprocess_input\n", "from IPython.display import SVG\n", "from keras.utils.vis_utils import model_to_dot\n", "from keras.utils import plot_model\n", "from keras.utils.np_utils import to_categorical\n", "\n", "\n", "import keras.backend as K\n", "K.set_image_data_format('channels_last')\n", "\n", "from matplotlib.pyplot import imshow\n", "%matplotlib inline\n", "import matplotlib.pyplot as plt\n", "import matplotlib.cm as cm\n", "\n", "# settings\n", "EPOCH = 20    \n", "BATCH_SIZE = 128\n", "VALIDATION_SIZE = 2000 # set to 0 to train on all available data\n", "\n"], "outputs": [], "metadata": {"_cell_guid": "1a7f72ae-cef7-404d-a9c3-4ee9769ba690", "_uuid": "13b8ed82adad5a52580260ce5da2e6143898c6b9", "collapsed": true}}, {"cell_type": "markdown", "source": ["## Data preparation\n", "To start, we read provided data. The *train.csv* file contains 42000 rows and 785 columns. Each row represents an image of a handwritten digit and a label with the value of this digit."], "metadata": {"_cell_guid": "a24744de-631c-4089-bfa4-4de621d31027", "_uuid": "d935bf88e94595b92b5bb5690a5c92cfb2957ba2"}}, {"cell_type": "code", "execution_count": null, "source": ["# read training data from CSV file \n", "data = pd.read_csv('../input/train.csv')\n", "\n", "print(data.shape)\n", "data.head()"], "outputs": [], "metadata": {"_cell_guid": "e7bd208e-a399-4901-858c-c486d7d451dd", "_uuid": "eca19cb57347ef9f6ca9618e51fb9e7f52eb1791", "collapsed": true}}, {"cell_type": "markdown", "source": ["Every image is a \"stretched\" array of pixel values."], "metadata": {"_cell_guid": "e10823f8-02b6-4cf5-9e1d-02f0deb02b2f", "_uuid": "ecf4d0edc5dc456ec7999bcf31690ebeb7163f06"}}, {"cell_type": "code", "execution_count": null, "source": ["images = data.iloc[:,1:].values\n", "images = images.astype(np.float)\n", "\n", "# convert from [0:255] => [0.0:1.0]\n", "images = np.multiply(images, 1.0 / 255.0)\n", "\n", "print(images.shape)"], "outputs": [], "metadata": {"_cell_guid": "ff9d3d23-5eab-4674-b32b-40b96cf44037", "_uuid": "f82f39248a6c5922c0279ae8053babf46b4cd1f8", "collapsed": true}}, {"cell_type": "markdown", "source": ["In this case it's 784 pixels => 28 * 28px"], "metadata": {"_cell_guid": "a14191d2-4811-43a3-ab45-3988c0ec3500", "_uuid": "d2b2c93c047664849c7c5be7ce46e7853f62749f"}}, {"cell_type": "code", "execution_count": null, "source": ["# in this case all images are square\n", "# get the width and height of the image\n", "image_size = images.shape[1]\n", "image_width = image_height = np.ceil(np.sqrt(image_size)).astype(np.uint8)\n", "\n", "print ('image_width => {0}\\nimage_height => {1}'.format(image_width,image_height))"], "outputs": [], "metadata": {"_cell_guid": "55a513d7-0f93-4ece-90d2-877f9aabbf0e", "_uuid": "2e5df0c012347e0a13f98d8a6c795b88453739a4", "collapsed": true}}, {"cell_type": "markdown", "source": ["Reshape the images from flatten representation to 28*28\n", "We also specify the number of channels. In this case, the images are grayscale so there is only one channel "], "metadata": {"_cell_guid": "8ee454ee-d24c-42d9-b504-224b29ee714b", "_uuid": "6943844828e980d0dfc6a9bd9f58f266b4b4e114"}}, {"cell_type": "code", "execution_count": null, "source": ["images_r = images.reshape(images.shape[0], image_height, image_width, 1)\n", "print(images_r.shape)"], "outputs": [], "metadata": {"_cell_guid": "528e7666-797c-4a5e-8cc1-1bad74f124f8", "_uuid": "c4c474704318c3b950243426f4ef4ff24606a32a", "collapsed": true}}, {"cell_type": "markdown", "source": ["Next we transform the flat labels to one-hot vectors. \n", "For example, a label value of 0 will be transform to a vector of [1, 0, 0, 0, 0, 0, 0, 0, 0, 0] "], "metadata": {"_cell_guid": "88557d4f-eaf9-4f84-b6a9-711fa65a145a", "_uuid": "9d82c1a961918513103ec3c11bf2def05f8a278b"}}, {"cell_type": "code", "execution_count": null, "source": ["#labels_flat = data[[0]].values.ravel()\n", "labels_flat = data.iloc[:,0].values.ravel()\n", "\n", "labels= to_categorical(labels_flat)\n", "labels.shape\n"], "outputs": [], "metadata": {"_cell_guid": "1d0b2946-c5bb-4d1c-bb07-3dc534903898", "_uuid": "a07be6743cdc6755d524e8a961633dd55981718d", "collapsed": true}}, {"cell_type": "markdown", "source": ["Lastly we set aside data for validation. It's essential in machine learning to have a separate dataset which doesn't take part in the training and is used to make sure that what we've learned can actually be generalised."], "metadata": {"_cell_guid": "18a23776-cfef-46d4-801f-b5565b4b2400", "_uuid": "c5f06941d1ed4d0e5520d0474ded98e34ae87b8a"}}, {"cell_type": "code", "execution_count": null, "source": ["# split data into training & validation\n", "#print(images_r.shape)\n", "validation_images = images_r[:VALIDATION_SIZE]\n", "validation_labels = labels[:VALIDATION_SIZE]\n", "\n", "train_images = images_r[VALIDATION_SIZE:]\n", "train_labels = labels[VALIDATION_SIZE:]\n", "\n", "train_images.shape"], "outputs": [], "metadata": {"_cell_guid": "30f74c3d-9fcf-49f0-aa63-bb6cdb5080d3", "_uuid": "71375e452f9aaf2f631aff4324a897bc89c59bc5", "collapsed": true}}, {"cell_type": "markdown", "source": ["Next we define the Keras model\n", "Here we define 2 convolution layers, followed by fully connected layers. \n", "Feel free to try out different parameters, such as the filter size, stride, etc"], "metadata": {"_cell_guid": "c10e620d-e2ed-4763-be42-197255876005", "_uuid": "53df0d8cb9aafda4f24332b1e7551ee42926fdcb"}}, {"cell_type": "code", "execution_count": null, "source": ["# define the Keras model\n", "def train_model(input_shape):\n", "    \"\"\"\n", "    Implementation of the train_model.\n", "    \n", "    Arguments:\n", "    input_shape -- shape of the images of the dataset\n", "\n", "    Returns:\n", "    model -- a Model() instance in Keras\n", "    \"\"\"\n", "     \n", "    # Define the input placeholder as a tensor with shape input_shape. Think of this as your input image!\n", "    X_input = Input(input_shape)\n", "\n", "    # Zero-Padding: pads the border of X_input with zeroes\n", "    X = ZeroPadding2D((1, 1))(X_input)\n", "\n", "    # CONV -> BN -> RELU Block applied to X\n", "    X = Conv2D(16, (5, 5), strides = (1, 1), name = 'conv0')(X)\n", "    X = BatchNormalization(axis = 3, name = 'bn0')(X)\n", "    X = Activation('relu')(X)\n", "\n", "    # MAXPOOL\n", "    X = MaxPooling2D((2, 2), name='max_pool')(X)\n", "\n", "    # CONV -> BN -> RELU Block applied to X\n", "    X = Conv2D(64, (11, 11), strides = (1, 1), name = 'conv1')(X)\n", "    X = BatchNormalization(axis = 3, name = 'bn1')(X)\n", "    X = Activation('relu')(X)\n", "\n", "    # MAXPOOL\n", "    X = MaxPooling2D((2, 2), name='max_pool2')(X)\n", "    \n", "    # DROPOUT\n", "    X = Dropout(0.25)(X)\n", "    \n", "    # FLATTEN X (means convert it to a vector) + FULLYCONNECTED\n", "    X = Flatten()(X)\n", "    X = Dense(128, activation='relu', name='fc1')(X)\n", "     \n", "    # DROPOUT\n", "    X = Dropout(0.5)(X)\n", "        \n", "    X = Dense(10, activation='softmax', name='fc2')(X)\n", "\n", "    # Create model. \n", "    model = Model(inputs = X_input, outputs = X, name='NMIST')\n", "\n", "    return model\n", "    "], "outputs": [], "metadata": {"_cell_guid": "525a1ce6-f328-4663-901c-4d79cdd55a22", "_uuid": "93ac8cee72b0253a52326513795e12148d5104d0", "collapsed": true}}, {"cell_type": "markdown", "source": ["Next we instantiate a model and specify the optimizer and loss function.\n", "ADAM is used here. "], "metadata": {"_cell_guid": "fbc8db6d-5ee6-450b-a8aa-baa8dd5f8278", "_uuid": "97bd7438e4cce7e2cffe845e390ba41076565575"}}, {"cell_type": "code", "execution_count": null, "source": ["NN_model = train_model((28,28,1))\n", "NN_model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\",metrics = [\"accuracy\"])"], "outputs": [], "metadata": {"_cell_guid": "338f4205-737c-48d0-9614-c56804f6ef3a", "_uuid": "ca5c093639174bf7335a0b54f57d81651bd63865", "collapsed": true}}, {"cell_type": "markdown", "source": ["Then we train the model."], "metadata": {"_cell_guid": "5278d43d-180b-4dbb-a1e1-3aae4cf82b66", "_uuid": "1dfbe79b18a16d70c771ef597be1070992b47a51"}}, {"cell_type": "code", "execution_count": null, "source": ["NN_model.fit(x=train_images,y=train_labels,epochs=EPOCH,batch_size=BATCH_SIZE)"], "outputs": [], "metadata": {"_cell_guid": "c7ae8e90-ead4-4dc2-8fdf-203ec4a76101", "_uuid": "ce99cce41a289c5699950e30187e1ba5a11369b0", "collapsed": true}}, {"cell_type": "markdown", "source": ["One thing good about Keras is that we can print the details of the model to get a clear picture of what you have defined."], "metadata": {"_cell_guid": "b8ae811c-0540-474c-bf61-8103024667c4", "_uuid": "0a90171acc9f49950684342b986fe90bda2a5488"}}, {"cell_type": "code", "execution_count": null, "source": ["NN_model.summary()"], "outputs": [], "metadata": {"_cell_guid": "f15c8b98-2706-4335-acc7-d48ea096a117", "_uuid": "5dd614a8949edf319a53d8cc19137cc3cf168ebe", "collapsed": true}}, {"cell_type": "code", "execution_count": null, "source": ["NN_model.evaluate(x=validation_images, y =validation_labels)"], "outputs": [], "metadata": {"_cell_guid": "e9992fc4-bb31-4c01-ba83-9d203f3876d6", "_uuid": "ff9769d8ce67c463e19a04dcbc58ef137e305550", "collapsed": true}}, {"cell_type": "code", "execution_count": null, "source": ["# read test data from CSV file \n", "test_images = pd.read_csv('../input/test.csv').values\n", "test_images = test_images.astype(np.float)\n", "\n", "# convert from [0:255] => [0.0:1.0]\n", "test_images = np.multiply(test_images, 1.0 / 255.0)\n", "test_images_r =  test_images.reshape(test_images.shape[0], image_height, image_width, 1)\n", "\n", "\n", "print(test_images_r.shape)\n", "\n", "predicted_labels = NN_model.predict(test_images_r)\n", "\n"], "outputs": [], "metadata": {"_cell_guid": "123614bb-a2c7-4d02-b51a-18042c58c36d", "_uuid": "204d6c31477e7dd0bc447dc8386f7b73c073b3f0", "collapsed": true}}, {"cell_type": "code", "execution_count": null, "source": ["predicted_labels_ind = np.argmax(predicted_labels,axis=-1)\n", "\n", "# save results\n", "np.savetxt('submission_softmax.csv', \n", "           np.c_[range(1,len(test_images)+1),predicted_labels_ind], \n", "           delimiter=',', \n", "           header = 'ImageId,Label', \n", "           comments = '', \n", "           fmt='%d')"], "outputs": [], "metadata": {"_cell_guid": "5c000cd6-af62-485e-83fb-5d83aa542fb9", "_uuid": "6c56e1cca3e43c5e8fca4bd2b52c277b540e3f7c", "collapsed": true}}]}