{"cells": [{"cell_type": "markdown", "source": ["In this notebook I'll try to use RNN on fasttext embeddings (seems like for this case it'll may be good idea to use char-based embedding - at lear as I can see by mine previous kernel with logisitc regressions over word / chars tf-idfs).\n", "So you'll need fasttext installed (for Windows I used this build - http://cs.mcgill.ca/~mxia3/FastText-for-Windows/).\n", "\n", "**Also - there is no fasttext installed at Kaggle, so you'll need to run notebook on your machine.**\n", "\n", "# Data import"], "metadata": {"_cell_guid": "448bfb2b-b05d-49da-99de-229ce294a20d", "_uuid": "8ab3a26879ccb6ba1939efa2a43384d8224908bb"}}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true, "_cell_guid": "6c13feb2-1234-4dc5-aa8b-ad438236a44a", "_uuid": "94f7a22e45a5a0163ee30c5d732c76732c618115"}, "source": ["import pandas as pd\n", "import numpy as np\n", "from itertools import chain\n", "from nltk.tokenize import wordpunct_tokenize\n", "from keras.preprocessing import text, sequence\n", "from keras.layers import Dense, Embedding, Dropout, LSTM, Bidirectional, GlobalMaxPool1D, InputLayer, BatchNormalization, Activation\n", "from keras.models import Sequential\n", "from keras.callbacks import EarlyStopping, ModelCheckpoint\n", "from sklearn.model_selection import train_test_split\n", "from subprocess import call\n", "from sklearn.utils import compute_sample_weight\n", "from sklearn.metrics import confusion_matrix, log_loss\n", "from collections import OrderedDict"], "outputs": []}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true, "_cell_guid": "0d90b317-42b2-4486-8051-28284853ec32", "_uuid": "22dcdf2010fccf25945187cf45634a562c22ca47"}, "source": ["train = pd.read_csv(\"../input/train.csv\")\n", "train.fillna(\"nan\")\n", "train.head()"], "outputs": []}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true, "_cell_guid": "51432730-607b-483a-9deb-98245f8f865f", "_uuid": "799800abdeb3b8f64f7b7d159bc97744eb653264"}, "source": ["test = pd.read_csv(\"../input/test.csv\")\n", "test.fillna(\"nan\")\n", "test.head()"], "outputs": []}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true, "_cell_guid": "832c4a28-394b-4c9d-9c27-571050534bcb", "_uuid": "722d3d53ae18044a451c6222da24cee306186c91"}, "source": ["submission = pd.read_csv('../input/sample_submission.csv')\n", "submission.head()"], "outputs": []}, {"cell_type": "markdown", "source": ["Some values interpreted as float, so I'll convert it to strings:"], "metadata": {"_cell_guid": "c5791ee8-919a-42df-9b4e-c26f97713ee0", "_uuid": "7d574a17ff913dbbd1deeecb309198bb0b489534"}}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true, "_cell_guid": "675c8a41-49da-4fa1-934b-68c6f89566db", "_uuid": "0cb38ad3d25ac6d7354088dde5f18e4951d42f5e"}, "source": ["train['comment_text'] = train['comment_text'].apply(str)\n", "test['comment_text'] = test['comment_text'].apply(str)"], "outputs": []}, {"cell_type": "markdown", "source": ["# train/validation split\n", "Let's split data to train/validation set.\n", "\n", "I used next method so save each class distribution:\n", "\n", "- build all possible labels combinations\n", "- excluded combination that seen l;east then 2 times\n", "- replaced label combination with combination index\n", "- build indices for stratified split based on combination indices"], "metadata": {"_cell_guid": "e634d87f-754d-4804-8959-0cc66da42fea", "_uuid": "a98745649d60d06d9bb2b946a4618bf5eeaf840a"}}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true, "_cell_guid": "0baafe20-b15f-47f6-94d8-3b190770f93e", "_uuid": "e95420630fcc2179c6ef4cfc48f8a756500267a2"}, "source": ["targets = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']"], "outputs": []}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true, "_cell_guid": "6c424491-2cc9-44fd-92ad-8eb4347e12d2", "_uuid": "20096b1e8a5b6ba101e84a13dea9e959dc17318e"}, "source": ["y = np.array(train[targets])\n", "texts = np.array(train['comment_text'])\n", "texts_test = np.array(test['comment_text'])"], "outputs": []}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true, "_cell_guid": "54b3a248-d0ee-4f41-8463-1875fb47aa6f", "_uuid": "331c0f7aa53471da60e811d8b585657ff10fd8f1"}, "source": ["# Some mappings exlucded because have only 1 sample.\n", "label_mapping = np.array([\n", "    [0, 0, 0, 0, 0, 0],\n", "    [0, 0, 0, 0, 0, 1],\n", "    [0, 0, 0, 0, 1, 0],\n", "    [0, 0, 0, 0, 1, 1],\n", "    [0, 0, 0, 1, 0, 0],\n", "    [0, 0, 0, 1, 0, 1],\n", "    [0, 0, 0, 1, 1, 0],\n", "    [0, 0, 0, 1, 1, 1],\n", "    [0, 0, 1, 0, 0, 0],\n", "    #[0, 0, 1, 0, 0, 1],\n", "    [0, 0, 1, 0, 1, 0],\n", "    [0, 0, 1, 0, 1, 1],\n", "    #[0, 0, 1, 1, 0, 0],\n", "    [0, 0, 1, 1, 0, 1],\n", "    #[0, 0, 1, 1, 1, 0],\n", "    [0, 0, 1, 1, 1, 1],\n", "    [0, 1, 0, 0, 0, 0],\n", "    [0, 1, 0, 0, 0, 1],\n", "    [0, 1, 0, 0, 1, 0],\n", "    [0, 1, 0, 0, 1, 1],\n", "    [0, 1, 0, 1, 0, 0],\n", "    [0, 1, 0, 1, 0, 1],\n", "    [0, 1, 0, 1, 1, 0],\n", "    [0, 1, 0, 1, 1, 1],\n", "    [0, 1, 1, 0, 0, 0],\n", "    [0, 1, 1, 0, 0, 1],\n", "    [0, 1, 1, 0, 1, 0],\n", "    [0, 1, 1, 0, 1, 1],\n", "    [0, 1, 1, 1, 0, 0],\n", "    [0, 1, 1, 1, 0, 1],\n", "    [0, 1, 1, 1, 1, 0],\n", "    [0, 1, 1, 1, 1, 1],\n", "    [1, 0, 0, 0, 0, 0],\n", "    [1, 0, 0, 0, 0, 1],\n", "    [1, 0, 0, 0, 1, 0],\n", "    [1, 0, 0, 0, 1, 1],\n", "    [1, 0, 0, 1, 0, 0],\n", "    [1, 0, 0, 1, 0, 1],\n", "    [1, 0, 0, 1, 1, 0],\n", "    [1, 0, 0, 1, 1, 1],\n", "    [1, 0, 1, 0, 0, 0],\n", "    [1, 0, 1, 0, 0, 1],\n", "    [1, 0, 1, 0, 1, 0],\n", "    [1, 0, 1, 0, 1, 1],\n", "    [1, 0, 1, 1, 0, 0],\n", "    [1, 0, 1, 1, 0, 1],\n", "    [1, 0, 1, 1, 1, 0],\n", "    [1, 0, 1, 1, 1, 1],\n", "    [1, 1, 0, 0, 0, 0],\n", "    [1, 1, 0, 0, 0, 1],\n", "    [1, 1, 0, 0, 1, 0],\n", "    [1, 1, 0, 0, 1, 1],\n", "    [1, 1, 0, 1, 0, 0],\n", "    [1, 1, 0, 1, 0, 1],\n", "    [1, 1, 0, 1, 1, 0],\n", "    [1, 1, 0, 1, 1, 1],\n", "    [1, 1, 1, 0, 0, 0],\n", "    [1, 1, 1, 0, 0, 1],\n", "    [1, 1, 1, 0, 1, 0],\n", "    [1, 1, 1, 0, 1, 1],\n", "    [1, 1, 1, 1, 0, 0],\n", "    [1, 1, 1, 1, 0, 1],\n", "    [1, 1, 1, 1, 1, 0],\n", "    [1, 1, 1, 1, 1, 1],\n", "])\n", "y_converted = np.zeros([len(y)])\n", "for i in range(len(label_mapping)):\n", "    idx = (y == label_mapping[i]).sum(axis=1) == 6\n", "    y_converted[idx] = i\n", "train_indices, val_indices, _, _ = train_test_split(np.fromiter(range(len(y)), dtype=np.int32),\n", "                                                    y_converted,\n", "                                                    test_size=0.1,\n", "                                                    stratify=y_converted)"], "outputs": []}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true, "_cell_guid": "b2946515-6297-4960-90d2-8a8d59799461", "_uuid": "096550f7553a611b4e53e808bacb4be87ce7034b"}, "source": ["texts_train, texts_val = texts[train_indices], texts[val_indices]\n", "y_train, y_val = y[train_indices], y[val_indices]"], "outputs": []}, {"cell_type": "markdown", "source": ["# Embedding training\n", "\n", "Now I'll prepare texts from train subset to use in fasttext train:"], "metadata": {"_cell_guid": "90bd2f50-2815-4463-a079-3349e9599c80", "_uuid": "7d81a479a5f596fdb1b355c4cc08f7037b7aece2"}}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true, "_cell_guid": "4decd929-045c-4b92-81d6-0aca53799e0c", "_uuid": "58294f30891b179c1e272fe36165502ac1f1532f"}, "source": ["with open('fasttext-embedding-train.txt', 'w', encoding='utf-8') as target:\n", "    for text in texts_train:\n", "        target.write('__label__0\\t{0}\\n'.format(text.strip()))"], "outputs": []}, {"cell_type": "markdown", "source": ["And - with next command I'll start fasttext model train:\n", "\n", "For linux system similar command will be \n", "\n", "    fasttext skipgram -input fasttext-embedding-train.txt -output embedding-model > /dev/null 2>&1"], "metadata": {"_cell_guid": "31ef2d6d-d384-468f-89de-e069573cecc5", "_uuid": "aa2cbc2d80f0157277844fe52b46a6399ca8c20e"}}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true, "_cell_guid": "4c5e1525-84b5-4a11-b6d4-1ff55f7e8fcf", "_uuid": "e96de2cbde3012e2ce2a3d3220650ecfcad5a08c"}, "source": ["!fasttext skipgram -input fasttext-embedding-train.txt -output embedding-model >nul 2>&1"], "outputs": []}, {"cell_type": "markdown", "source": ["Now I need to:\n", "- prepare list of words from train/validation/test sets\n", "- calculate vectors for each word\n", "- load vectors in mine model"], "metadata": {"_cell_guid": "ca96b197-63e3-4a8d-9220-e3662e30342b", "_uuid": "4fb58a047cc86184d4ca7067f85f780badbf73b7"}}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true, "_cell_guid": "50792345-d852-476e-b675-5a58c6071422", "_uuid": "122fff20980ae778e45b5c2c1ac649fd93aae169"}, "source": ["train_texts_tokenized = map(wordpunct_tokenize, train['comment_text'])\n", "test_texts_tokenized = map(wordpunct_tokenize, train['comment_text'])\n", "train_text_tokens = set(chain(*train_texts_tokenized))\n", "test_text_tokens = set(chain(*test_texts_tokenized))\n", "text_tokens = sorted(train_text_tokens | test_text_tokens)\n", "with open(\"fasttext-words.txt\", \"w\", encoding=\"utf-8\") as target:\n", "    for word in text_tokens:\n", "        target.write(\"{0}\\n\".format(word.strip()))"], "outputs": []}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true, "_cell_guid": "8129aa3f-3420-4883-a367-179d7c73b522", "_uuid": "23241e099d3b8a8569873ed39f4916ffcf1f8cfe"}, "source": ["!fasttext print-word-vectors embedding-model.bin < fasttext-words.txt > fasttext-vectors.txt"], "outputs": []}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true, "_cell_guid": "2dd0b692-077a-46dd-99b6-29975d9f6627", "_uuid": "a54d009e967551b9ba5ab42c32985783772ef1ce"}, "source": ["embedding_matrix = np.zeros([len(text_tokens) + 1, 100])\n", "word2index = {}\n", "with open(\"fasttext-vectors.txt\", \"r\", encoding=\"utf-8\") as src:\n", "    for i, line in enumerate(src):\n", "        parts = line.strip().split(' ')\n", "        word = parts[0]\n", "        vector = map(float, parts[1:])\n", "        word2index[word] = len(word2index)\n", "        embedding_matrix[i] = np.fromiter(vector, dtype=np.float)"], "outputs": []}, {"cell_type": "markdown", "source": ["And finally I'll replace words in text with embedding vector indices:"], "metadata": {"_cell_guid": "0dcfc4f3-72f4-44e5-a589-4b45c1de1819", "_uuid": "bc4dba8de20ba2e29bedd31b2bd3636ac0a38937"}}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true, "_cell_guid": "74ee53e7-30a1-4209-a085-673325b3e2d3", "_uuid": "653bd7d40e4afbff9f885e76146ce29f8d12ec8e"}, "source": ["def text2sequence(text):\n", "    return list(map(lambda token: word2index.get(token, len(word2index) - 1), wordpunct_tokenize(str(text))))\n", "\n", "\n", "X_train = sequence.pad_sequences(list(map(text2sequence, texts_train)), maxlen=100)\n", "X_val = sequence.pad_sequences(list(map(text2sequence, texts_val)), maxlen=100)\n", "X_test = sequence.pad_sequences(list(map(text2sequence, texts_test)), maxlen=100)"], "outputs": []}, {"cell_type": "markdown", "source": ["# Model\n", "\n", "Let's build and train model:"], "metadata": {"_cell_guid": "e3abd0e4-0d18-4880-853a-4076fe9091c5", "_uuid": "75aacd572adb78d187a8bbd1edf25f862f57fa0c"}}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true, "_cell_guid": "3d9250fb-7539-48d2-93cc-a2dfea43f671", "_uuid": "bfe146cbad9cf8c32a4965762a4ae134c65db0a0"}, "source": ["embed_size = 100\n", "model = Sequential([\n", "    InputLayer(input_shape=(100,), dtype='int32'),\n", "    Embedding(len(embedding_matrix), embed_size),\n", "    Bidirectional(LSTM(50, return_sequences=True)),\n", "    GlobalMaxPool1D(),\n", "    Dropout(0.3),\n", "    Dense(50, activation='relu'),\n", "    Dropout(0.3),\n", "    Dense(6, activation='sigmoid')\n", "])\n", "embedding = model.layers[1]\n", "embedding.set_weights([embedding_matrix])\n", "embedding.trainable = False\n", "model.compile(loss='binary_crossentropy',\n", "              optimizer='adam',\n", "              metrics=['accuracy'])\n", "model.summary()"], "outputs": []}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true, "_cell_guid": "262aa817-0b1a-4092-807e-694b2b872a3d", "_uuid": "9074989c6340e88f097203bbb2d618784c145b2b"}, "source": ["model.fit(X_train, y_train, \n", "          batch_size=64, \n", "          epochs=10, \n", "          validation_data=(X_val, y_val), \n", "          verbose=True, \n", "          callbacks=[\n", "              ModelCheckpoint('model.h5', save_best_only=True),\n", "              EarlyStopping(patience=3)\n", "          ])"], "outputs": []}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true, "_cell_guid": "f20233b8-5373-48b3-bf20-6fbd3a193d60", "_uuid": "5c34d1b7ddf3888ef2839605bd844ee9d770f9f0"}, "source": ["model.load_weights('model.h5')"], "outputs": []}, {"cell_type": "markdown", "source": ["# Test prediction"], "metadata": {"_cell_guid": "46f0afba-2fbe-4528-8bdd-bb1cb4171ef9", "_uuid": "ded2fbf486f2229329083eeef2713bdb9b657621"}}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true, "_cell_guid": "cafcb869-dc2e-4607-a07f-9bd2bcc1300b", "_uuid": "cbe29ef4194cc8e9c2f8a17a98351aa6fedf09c1"}, "source": ["test_prediction = model.predict(X_test, verbose=True)"], "outputs": []}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true, "_cell_guid": "9856adc1-d0cd-43d0-a940-bf9e1dea00ef", "_uuid": "6aab85f2e7ee36cd036ad9e77ddb2b8b49be3190"}, "source": ["for i, label in enumerate(['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']):\n", "    submission[label] = test_prediction[:, i]"], "outputs": []}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true, "_cell_guid": "07eecc24-a4e9-40f0-85ae-ec662f7776a5", "_uuid": "e93bd5aacf22cf4df1ca7099a022b44a6ddaa7b9"}, "source": ["submission.head()"], "outputs": []}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true, "_cell_guid": "3acf264a-6088-490a-8ee9-1edd5fa9eb16", "_uuid": "a4101fd96b5de4c0c7efdb74d3e7624bc44acd74"}, "source": ["submission.to_csv('output.csv', index=None)"], "outputs": []}, {"cell_type": "markdown", "source": ["# Validation error analysis\n", "\n", "Let's make prediction on validation set - and see what kind of errors we making with different classes:"], "metadata": {"_cell_guid": "833c80a5-c47e-40b7-b64d-42d125ff0f40", "_uuid": "018bfb05a32e8602e757f36e0febafff97844b78"}}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true, "_cell_guid": "dafdce80-022d-401a-afe6-9453549a4615", "_uuid": "66c0a3fa72b974fd6edf885dcf8a61ade1f6993c"}, "source": ["val_prediction = model.predict(X_val, verbose=True)"], "outputs": []}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true, "_cell_guid": "7944aacb-a1ea-4d6e-82a8-75c28961de61", "_uuid": "20b3ba0784698d6e2fab4a9baa464f862cd6d4ce"}, "source": ["def show_confustion_matrix(y_true, y_pred):\n", "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n", "    df = pd.DataFrame(OrderedDict([\n", "        (\"true-class\", [\"negative\", \"positive\"]),\n", "        (\"negative-classified\", [tn, fn]),\n", "        (\"positive-classified\", [fp, tp]),\n", "    ]))\n", "    return df.set_index(\"true-class\")"], "outputs": []}, {"cell_type": "markdown", "source": ["## Toxic"], "metadata": {"_cell_guid": "1fc33ef6-05be-4c99-86d6-6fec97880c19", "_uuid": "9cdc24342847209e2d6493e862fc53f61e01ad92"}}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true, "_cell_guid": "0e340e59-e66f-48af-a263-e240346b8153", "_uuid": "633cde6f47564d0447d9de4fd24dc8a6c6933fdc"}, "source": ["log_loss(y_val[:, 0], val_prediction[:, 0])"], "outputs": []}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true, "_cell_guid": "f671b4ac-d846-495f-8366-6fbdf2613b8c", "_uuid": "14231514a2e8f54a76c33585bbfbf2d660dca798"}, "source": ["show_confustion_matrix(y_val[:, 0], val_prediction[:, 0] > 0.5)"], "outputs": []}, {"cell_type": "markdown", "source": ["## Severe toxic"], "metadata": {"_cell_guid": "7fc15747-1bd8-48d8-955c-6e9ffab5f59d", "_uuid": "d1a13de9157d6e6df19fb4fbda0faab7d7aa6e50"}}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true, "_cell_guid": "f9a22b0d-898d-4836-af22-14020dceac3a", "_uuid": "74a2409d8f3a1a345d694d4d255cf8fd26841221"}, "source": ["log_loss(y_val[:, 1], val_prediction[:, 1])"], "outputs": []}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true, "_cell_guid": "d74fb1c3-51eb-413d-9ad4-20a2a8726368", "_uuid": "76a4f826cc9891c6bb0f87e58e5f86b5b15a0d47"}, "source": ["show_confustion_matrix(y_val[:, 1], val_prediction[:, 1] > 0.5)"], "outputs": []}, {"cell_type": "markdown", "source": ["## Obscene"], "metadata": {"_cell_guid": "63619573-e008-404f-af72-926ee98e8d26", "_uuid": "15d7c0e8538d2433ead178e0dfbd07dc93bc267f"}}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true, "_cell_guid": "9d81efdb-7501-4459-a540-80b23233cd25", "_uuid": "5d874d017026a0829d5597b8837d643d4b7ad7fa"}, "source": ["log_loss(y_val[:, 2], val_prediction[:, 2])"], "outputs": []}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true, "_cell_guid": "cc838025-45d3-4779-8ee5-8b3c233ab21e", "_uuid": "9ba314ce76ae333dfaf725ae552e11da2e2b2520"}, "source": ["show_confustion_matrix(y_val[:, 2], val_prediction[:, 2] > 0.5)"], "outputs": []}, {"cell_type": "markdown", "source": ["## Threat"], "metadata": {"_cell_guid": "fe8ce8d6-f146-4b19-abe7-0d7f2166e5a7", "_uuid": "747889f380c116e82252f479b7d22e322c23dcb7"}}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true, "_cell_guid": "6750911e-7c1c-40ae-9414-a45496d90d54", "_uuid": "30eadb532d448bb9047c17b72ec91a191735fda5"}, "source": ["log_loss(y_val[:, 3], val_prediction[:, 3])"], "outputs": []}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true, "_cell_guid": "31b4970f-bd36-4c6a-b11c-45d30368bc59", "_uuid": "24b18bb136fd1c3e418ce6752a0f6733ec64a914"}, "source": ["show_confustion_matrix(y_val[:, 3], val_prediction[:, 3] > 0.5)"], "outputs": []}, {"cell_type": "markdown", "source": ["## Insult"], "metadata": {"_cell_guid": "ae7c7836-03d6-4481-8afc-68a5eaf2d498", "_uuid": "dfee179cf599f6fe0e492c364d2f54da470d3a87"}}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true, "_cell_guid": "f917cfc2-1a77-4736-aacd-9af6525dc3e8", "_uuid": "07fc59488018ecebb2b36770fbebf85dd56d9b2d"}, "source": ["log_loss(y_val[:, 4], val_prediction[:, 4])"], "outputs": []}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true, "_cell_guid": "79be8fda-04ed-482b-ac1b-f4cf796a4d94", "_uuid": "a41ef273426733250ce158f1fce5a967d3e25a7d"}, "source": ["show_confustion_matrix(y_val[:, 4], val_prediction[:, 4] > 0.5)"], "outputs": []}, {"cell_type": "markdown", "source": ["## Identity hate"], "metadata": {"_cell_guid": "92640725-a370-41b9-99a6-ed79cdddc2f8", "_uuid": "da268f03da2f5d6d85933840d95c4c99792416d0"}}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true, "_cell_guid": "1d195f26-af09-4f2f-b88f-21b1773af9e4", "_uuid": "172c6cf6fc654320c5fb248987ca5a7b8715e876"}, "source": ["log_loss(y_val[:, 5], val_prediction[:, 5])"], "outputs": []}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true, "_cell_guid": "8d9d700a-997a-4965-94d5-828b94eb2dfd", "_uuid": "e58615d421a3bb61242191283b2894e7846b2d42"}, "source": ["show_confustion_matrix(y_val[:, 5], val_prediction[:, 5] > 0.5)"], "outputs": []}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true, "_cell_guid": "7de1cc68-c0bb-455c-856d-5869c14725c1", "_uuid": "374ce534e43ba5cee3d608aaa778a265a2167ac7"}, "source": [], "outputs": []}], "metadata": {"language_info": {"nbconvert_exporter": "python", "pygments_lexer": "ipython3", "mimetype": "text/x-python", "file_extension": ".py", "version": "3.6.3", "name": "python", "codemirror_mode": {"version": 3, "name": "ipython"}}, "kernelspec": {"name": "python3", "language": "python", "display_name": "Python 3"}}, "nbformat_minor": 1, "nbformat": 4}