{"cells":[{"metadata":{"_cell_guid":"ebba0809-6591-4386-8e4a-ba8d1935dd57","_uuid":"af1935f19e6ffeca47d2933c39bfcaf5a816535a"},"cell_type":"markdown","source":"Original kernel is [Teaching notebook for total imaging newbies]((https://www.kaggle.com/stkbailey/teaching-notebook-for-total-imaging-newbies).\n\nThis kernel will implement classical image techniques and will hopefully serve as a useful primer to people who have never worked with image data before. Ultimately, we will develop a simple pipeline using `scipy` and `numpy` (and a little bit of `scikit-image`) that we can apply to the test images -- in fact, we won't even use the training images except to optimize parameters.\n\nI'll keep updating this notebook to try and improve it - user Gabro Vecsei takes a similar approach and scores 0.22 in [this kernel]((https://www.kaggle.com/gaborvecsei/basic-pure-computer-vision-segmentation-lb-0-229). My main intention here, though, is to help out people who are new to analyses, not to score highly. "},{"metadata":{"_cell_guid":"2b168289-7746-4255-b933-356bd04914b7","_uuid":"25fa830192ea11807891ec2f71487dbeb18364e3","trusted":true},"cell_type":"code","source":"import pathlib\nimport imageio\nimport numpy as np\nimport pandas as pd\n\n# Glob the training data and load a single image path\ntraining_paths = pathlib.Path('../input/stage1_train').glob('*/images/*.png')\n\ntraining_sorted = sorted([x for x in training_paths])\nim_path = training_sorted[45]\nim = imageio.imread(str(im_path))\nprint(\"Total training images:%i\"%len(training_sorted))\nprint('im :'+str(format(im.shape)))","execution_count":4,"outputs":[]},{"metadata":{"_cell_guid":"aca64183-1c16-474c-88d6-6574a9dcf56e","_uuid":"1e41cf0c93629a559d0e62423b60b1f67691d9fc"},"cell_type":"markdown","source":"## Lectura de las máscaras"},{"metadata":{"_cell_guid":"bb23d8c6-8474-40aa-9113-10708b2c746c","_uuid":"de680bf8740e070c7daa01d4f74acd88bd75983e","trusted":true},"cell_type":"code","source":"#Función para obtener la mascara en una sola imagen a partir del directorio de la imagen\ndef getMask(im_path):\n    yy=(im_path.parents[1]/'masks').glob('*.png')\n    yy=[x for x in yy]\n    im_mask=imageio.imread(str(yy[0]))\n    for dirIm in yy:\n        im_t=imageio.imread(str(dirIm))\n        im_mask=im_t | im_mask\n    return im_mask\nprint('Done')","execution_count":5,"outputs":[]},{"metadata":{"_cell_guid":"505d8f16-1f54-4132-8a60-d73a145fee89","_uuid":"092bf1fb8a05ca9b975180d305928ab250306765"},"cell_type":"markdown","source":"## Visualización de una imagen"},{"metadata":{"_cell_guid":"00373a14-b85d-4a2f-9dad-cc9a66c37d34","_uuid":"a5e6e5b0d00730af7c7c2e242f09b58640d1fc3e","trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nim_path = training_sorted[0]\nim = imageio.imread(str(im_path))\nim_mask= getMask(im_path)\nplt.figure(figsize=(10,4))\n\nplt.subplot(1,3,1)\nplt.imshow(im[:,:,1],cmap='gray')\nplt.axis('off')\nplt.title('Original Image')\n\nplt.subplot(1,3,2)\nplt.imshow(im_mask)\nplt.axis('off')\nplt.title('Mask')\n\n# fondo\nfondo= im[:,:,1] & (~ im_mask )\nplt.subplot(1,3,3)\nplt.imshow(fondo)\nplt.axis('off')\nplt.title('fondo')\n\nplt.tight_layout()\nplt.show()","execution_count":6,"outputs":[]},{"metadata":{"_cell_guid":"4f6cff77-6108-427e-a555-ac12020b2c90","_uuid":"2c17d6e81a47698ad91cccef3800241539c8b185"},"cell_type":"markdown","source":"# Normalizando las imágenes\n\nEn el conjunto de datos se hallan diferentes tipos de imágenes, RGB,  y en escala de grises."},{"metadata":{"_cell_guid":"6164a0b8-f893-44ea-af99-91c838e91f45","_uuid":"ce4ecbb0178436bf399396312c4ebd5f7b70210f","trusted":true},"cell_type":"code","source":"# Lectura de las imagenes y las máscaras\ntipos={1:'gray',3:'rgb',4:'rgba'}\ntIm=[]\nimagenes=[]\nmascaras=[]\nfor dfile in training_sorted:\n    im=imageio.imread(str(dfile))\n    imagenes.append(im)\n    mascaras.append(getMask(dfile))\n    height, width, levels = im.shape\n    m=[int(im[:,:,c].mean()) for c in range(levels)]\n    typeIm='';\n    if m[0]== m[1] and m[1] == m[2]:\n        typeIm=tipos[1]\n    else:\n        typeIm=tipos[3]\n    tIm.append((width,height,typeIm))\nprint('Done')","execution_count":7,"outputs":[]},{"metadata":{"_cell_guid":"be1c69c3-66ea-43ed-a4ab-d1c7a0711320","_uuid":"9e058a0e7ecf866fd7b34f2cae9341f9f51b27a7"},"cell_type":"markdown","source":"Visión general de los datos de entrenamiento:"},{"metadata":{"_kg_hide-input":true,"_cell_guid":"6c2f2b51-8289-41fb-bba7-9a0b92de9625","_uuid":"57e3f692a2d75e003cdeaed0b657775faf1f97d2","trusted":true},"cell_type":"code","source":"dfTypes=pd.DataFrame(tIm,columns=['width','height','type'])\nagrupaciones=dfTypes.groupby(['width','height'])\ndimensiones=agrupaciones.groups.keys()\ncantidades=agrupaciones.size()\nfor i, t in enumerate(list(dimensiones)):\n    print('%i-width x height: %s   total:%i'%(i+1,str(t),cantidades[i]))\nprint('gray :%i'%len(dfTypes[dfTypes.type=='gray']))\nprint('rgb :%i'%len(dfTypes[dfTypes.type=='rgb']))\n","execution_count":8,"outputs":[]},{"metadata":{"_cell_guid":"44b1ed94-4062-4ac3-920f-3df140c5b801","_uuid":"fadbb89bce0619d2236c9232c2cc10d9c3e32522","trusted":true},"cell_type":"code","source":"dfTypes.head(4)","execution_count":9,"outputs":[]},{"metadata":{"_cell_guid":"cb2fc1a4-477b-4a14-bd73-a45e368a305c","_uuid":"cb1e9c57cfc8926e986ea073b72cd05e944781ff","trusted":true},"cell_type":"code","source":"imRgb=[imagenes[i] for i in dfTypes.index[dfTypes.type=='rgb'].tolist()]\nimRgbMask=[mascaras[i] for i in dfTypes.index[dfTypes.type=='rgb'].tolist()]\nprint('Total imágenes RGB: %i'%len(imRgb))","execution_count":10,"outputs":[]},{"metadata":{"_cell_guid":"20c1ded9-e5bc-4283-893a-8eda2cc025b7","_uuid":"f29feb7d5494fc62d3f4b391841798c866f1bcca","trusted":true},"cell_type":"code","source":"imRgb[0].shape","execution_count":11,"outputs":[]},{"metadata":{"_cell_guid":"16e4bbbe-56b9-45e5-8b89-f1640f81aca6","_uuid":"377ce06e319ba465654d918f3bf2c738de428a9f","trusted":true},"cell_type":"code","source":"imRgbMask[0].shape","execution_count":12,"outputs":[]},{"metadata":{"_cell_guid":"cb99a4da-1eeb-4278-85b2-75f3e991ca10","_uuid":"85148684a4f98b604f85e4a8496281b0d46eeec9"},"cell_type":"markdown","source":"## Invierte niveles de los canales\nSi el promedio de cada canal es superior a 128 se asume que el fondo es de color claro, por lo cual se invierten los niveles."},{"metadata":{"_cell_guid":"028faf41-174d-4e20-a3c8-8116112011e5","_uuid":"e59e15246991092617a6bec3fc930342c82e24bd","collapsed":true,"trusted":true},"cell_type":"code","source":"for i,img in enumerate( imRgb):\n    v=[ img[:,:,c].mean() for c in range(3)]\n    v=sum(v)/len(v)\n    if v>128:\n        imRgb[i]=255-img","execution_count":14,"outputs":[]},{"metadata":{"_cell_guid":"5b76b5df-7ef3-4e6c-9d1e-495db9d9fc88","_uuid":"1c9785aba26f5b7cfed1a3ce2ea12b70d336ed33"},"cell_type":"markdown","source":"## Descriptores para selección de canal\n"},{"metadata":{"_cell_guid":"2ad562f1-b124-4876-9ba8-58a25d868331","_uuid":"f506c35adc0a4c20c059716b42a88ad2373a3c2d","trusted":true},"cell_type":"code","source":"ch=imRgb[0][:,:,1]\nmedia= ch.mean()\nvmin=np.amin(ch)\nvmax=np.amax(ch)\nhist, bins= np.histogram(ch,256)\nhmaxA=hist[0:128].argmax()\nhmaxB=hist[128:].argmax()+128\n# Grafico del histograma\nplt.bar(bins[:-1],hist, width=1)\nplt.show()\nprint('media: %.2f   vmin: %i    vmax: %i   hmaxA: %i   hmaxB: %i'%(media, vmin, vmax, hmaxA,hmaxB))","execution_count":15,"outputs":[]},{"metadata":{"_cell_guid":"ba775e21-537d-4b3d-ad94-9f6976297761","_uuid":"b0003ec96c8960f426ee8b912d6fc6c7aa8e79ba","trusted":true},"cell_type":"code","source":"def descriptoresCh(ch):\n    hist, bins= np.histogram(ch,256)\n    hmaxA=hist[0:128].argmax()\n    hmaxB=hist[128:].argmax()+128\n    vmax=np.amax(ch)\n    vmin=np.amin(ch)\n    media=ch.mean()\n    return [media, vmin, vmax, hmaxA,hmaxB]\ndescriptores=[]\nfor i,img in enumerate( imRgb):\n    di=[]\n    di.extend(descriptoresCh(img[:,:,0]))\n    di.extend(descriptoresCh(img[:,:,1]))\n    di.extend(descriptoresCh(img[:,:,2]))\n    descriptores.append(di)\ncol=[]\nfor ch in ('r','g', 'b'):\n    for d in ['media', 'vmin', 'vmax', 'hmaxA', 'hmaxB']:\n        col.append('%s_%s'%(ch,d))\ndescriptores= pd.DataFrame(descriptores, columns=col)\ndescriptores.head(3)","execution_count":49,"outputs":[]},{"metadata":{"_cell_guid":"3f073097-c6d4-4869-a265-7362ba740813","_uuid":"999b14daea928b920f8042a8f31c2d67e97a952b"},"cell_type":"markdown","source":"## Identificación del mejor canal según las máscaras"},{"metadata":{"_cell_guid":"9e1b963d-5ab7-47d7-accc-57d17d7a352d","_uuid":"273948f568f2bdf15a7a90ded9f81723b06267ed","trusted":true},"cell_type":"code","source":"from skimage.filters import threshold_otsu\nimc=imRgb[0]\nmsk=imRgbMask[0]\nnmsk=np.sum(msk==255)\nprint(np.amax(msk))\nprint(imc.shape)\nttd=[]\nnwmsk=[]\ndfmsk=[]\nfor c in range(3):\n    imch=imc[:,:,c]\n    thresh_val = threshold_otsu(imch)\n    maskCh = np.where(imch > thresh_val, 255, 0)\n    dif= maskCh ^ msk\n    v= np.sum(dif==255)/nmsk\n    nwmsk.append(maskCh)\n    dfmsk.append(dif)\n    ttd.append(v)\nprint(ttd)\nplt.figure(figsize=(14,12))\nfor i, chName in enumerate(['R','G', 'B']):\n    plt.subplot(3,3,i+1)\n    plt.imshow(imc[:,:,i],cmap='gray')\n    plt.axis('off')\n    plt.title('Canal %s'%chName)\nfor i, chName in enumerate(['R','G', 'B']):\n    plt.subplot(3,3,i+4)\n    plt.imshow(nwmsk[i],cmap='gray')\n    plt.axis('off')\n    plt.title('Canal %s'%chName)\nfor i, chName in enumerate(['R','G', 'B']):\n    plt.subplot(3,3,i+7)\n    plt.imshow(dfmsk[i],cmap='gray')\n    plt.axis('off')\n    plt.title('Canal %s'%chName)\nplt.show()\nplt.figure(figsize=(4,4))\nplt.imshow(msk,cmap='gray')\nplt.axis('off')\nplt.title('Mascara')\nplt.show()","execution_count":17,"outputs":[]},{"metadata":{"_cell_guid":"7be7021c-2399-4ac3-b9fe-6965c55f56d4","_uuid":"54728c4829fbc2cec515d851d4cbd2ef135044a2","collapsed":true,"trusted":true},"cell_type":"code","source":"# Se escogerá el mejor canal en cada imagen a color\n# Se indica el canal con un indice 0 - R, 1 - G, 2 - B.\nmejorCH=[]\nfor i,img in enumerate( imRgb):\n    imc=img\n    msk=imRgbMask[i]\n    nmsk=np.sum(msk==255)\n    ttd=[] #Donde se almacena el porcentaje de no aciertos, que idealmente debe ser cero\n    for c in range(3):\n        imch=imc[:,:,c]\n        thresh_val = threshold_otsu(imch)\n        maskCh = np.where(imch > thresh_val, 255, 0)\n        dif= maskCh ^ msk\n        v= np.sum(dif==255)/nmsk\n        ttd.append(v)\n    mejorCH.append(ttd.index(min(ttd)))\n","execution_count":37,"outputs":[]},{"metadata":{"_cell_guid":"fda1fde0-6651-458d-a867-6a80ceca403a","_uuid":"abca43f93706e9effca21eceb0eb572878159b2d","trusted":true},"cell_type":"code","source":"fig = plt.figure(1,figsize=(7,4))\nplt.plot(mejorCH)\nplt.show()\nnp.histogram(mejorCH,[-1,0.5,1.5,2.5])","execution_count":19,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f168f396307851d71cc355e81273f32a1b94a232"},"cell_type":"code","source":"#Dado que el canal B no es muy común, y que solo hay dos casos, se descartará y se dejarán solo dos canales, se asume que para estos dos casos\n#El mejor canal es el verde G.\nmejorCH[mejorCH.index(2)]=1\nmejorCH[mejorCH.index(2)]=1","execution_count":38,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d1b1353c7fc63119d2273b463926ad17f6cbd749"},"cell_type":"code","source":"print(mejorCH)\nprint(type(mejorCH[0]))","execution_count":44,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"57c4cbd8a6f5ce5e2c631692f7450dbcec28523f"},"cell_type":"code","source":"dtf=descriptores\ndtf['canal']=pd.Series(mejorCH)\ndtf.head(5)","execution_count":51,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a726b0064815dcaa888926eebf5f8a0d49eedf13"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ntrain, test = train_test_split(dtf, test_size = 0.2,random_state=3)\nprint('(# Datos, # descriptores)\\n Entranimento: %s\\n Test: %s'%(str(train.shape),str(test.shape)))\ndescriptores=[]\nfor ch in ('r_','g_','b_'):\n    descriptores.append(ch+'media')\n    descriptores.append(ch+'vmin')\n    descriptores.append(ch+'vmax')\n    descriptores.append(ch+'hmaxA')\n    descriptores.append(ch+'hmaxB')\ntrain_X = train[descriptores]\ntrain_y=train.canal\ntest_X= test[descriptores] \ntest_y =test.canal  \nprint(len(test_y))","execution_count":54,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dad2dbcf6abb9d76b3714f26335e8a18d05d8a05"},"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\nfrom sklearn import metrics #for checking the model accuracy\nmodeloClasificador = GaussianNB()\nmodeloClasificador.fit(train_X, train_y)\nprediccion=modeloClasificador.predict(test_X) \nprint('The accuracy of the GaussianNB is:',metrics.accuracy_score(prediccion,test_y))","execution_count":55,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4ae75ebccf11c33e93edf26a713a02a060d60e34"},"cell_type":"code","source":"a=test_X.iloc[[3]]\nb= a.values.tolist()\nprint(b)\n#print(test_y.iloc[:])\n\nx=modeloClasificador.predict(b)\nprint(x[0])","execution_count":117,"outputs":[]},{"metadata":{"_cell_guid":"457b5a62-435d-44c5-a7d8-30367c5212f2","_uuid":"c6854798a17802a06c246173b84848011d266fda"},"cell_type":"markdown","source":"# Dealing with color\n\nThe images in this dataset can be in RGB, RGBA and grayscale format, based on the \"modality\" in which they are acquired. For color images, there is a third dimension which encodes the \"channel\" (e.g. Red, Green, Blue). To make things simpler for this first pass, we can coerce all these images into grayscale using the `rgb2gray` function from `scikit-image`."},{"metadata":{"_cell_guid":"3f389854-b251-4723-babf-9fd0d71e3805","_uuid":"3f03d9fd9cb23494b936e2ebd4535878efc2ccfd","trusted":true},"cell_type":"code","source":"# Print the image dimensions\nprint('Original image shape: {}'.format(im.shape))\n\n# Coerce the image into grayscale format (if not already)\nfrom skimage.color import rgb2gray\nim_gray = rgb2gray(im)\nprint('New image shape: {}'.format(im_gray.shape))","execution_count":56,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_cell_guid":"c298bcd0-2c4b-42aa-b0d1-f5b829515841","_uuid":"b1e6a69a5ae9121ff4f2f1a4fa96698f532a9375","trusted":true},"cell_type":"code","source":"# Now, let's plot the data\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(10,4))\n\nplt.subplot(1,2,1)\nplt.imshow(im)\nplt.axis('off')\nplt.title('Original Image')\n\nplt.subplot(1,2,2)\nplt.imshow(im_gray, cmap='gray')\nplt.axis('off')\nplt.title('Grayscale Image')\n\nplt.tight_layout()\nplt.show()","execution_count":57,"outputs":[]},{"metadata":{"_cell_guid":"5f5383ed-7c97-4c27-8ea4-ac4ae35bbd25","_uuid":"0a93026712b2a2ef54412c43873ac58197636917"},"cell_type":"markdown","source":"# Removing background\nPerhaps the simplest approach for this problem is to assume that there are two classes in the image: objects of interest and the background. Under this assumption, we would expect the data to fall into a bimodal distribution of intensities. If we found the best separation value, we could \"mask\" out the background data, then simply count the objects we're left with.\n\nThe \"dumbest\" way we could find the threshold value would be to use a simple descriptive statistic, such as the mean or median. But there are other methods: the \"Otsu\" method is useful because it models the image as a bimodal distribution and finds the optimal separation value. "},{"metadata":{"_cell_guid":"f145ff54-8e4d-4546-9913-e695bb7d9254","_uuid":"b76b12a4b26dc1a351d717a047bd90ea6aa3f7dd","collapsed":true,"trusted":true},"cell_type":"code","source":"from skimage.filters import threshold_otsu\nthresh_val = threshold_otsu(im_gray)\nmask = np.where(im_gray > thresh_val, 1, 0)\n\n# Make sure the larger portion of the mask is considered background\nif np.sum(mask==0) < np.sum(mask==1):\n    mask = np.where(mask, 0, 1)","execution_count":59,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_cell_guid":"65033fba-1681-46fd-aa6d-c3e3fec09c1f","_uuid":"14792479e2019e59f6cd0388b40798debb9e34dc","trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,4))\n\nplt.subplot(1,2,1)\nim_pixels = im_gray.flatten()\nplt.hist(im_pixels,bins=50)\nplt.vlines(thresh_val, 0, 100000, linestyle='--')\nplt.ylim([0,50000])\nplt.title('Grayscale Histogram')\n\nplt.subplot(1,2,2)\nmask_for_display = np.where(mask, mask, np.nan)\nplt.imshow(im_gray, cmap='gray')\nplt.imshow(mask_for_display, cmap='rainbow', alpha=0.5)\nplt.axis('off')\nplt.title('Image w/ Mask')\n\nplt.show()","execution_count":60,"outputs":[]},{"metadata":{"_cell_guid":"6b6afa37-f217-4c0b-a9d4-8e244d5384b9","_uuid":"47cc36785c751676f70dec7b25d2a4c56fe4cf0c"},"cell_type":"markdown","source":"# Deriving individual masks for each object\n\nFor this contest, we need to get a separate mask for each nucleus. One way we can do this is by looking for all objects in the mask that are connected, and assign each of them a number using `ndimage.label`.  Then, we can loop through each `label_id` and add it to an iterable, such as a list."},{"metadata":{"_cell_guid":"5c8b868f-1b2d-4939-914f-836d0aa7b50d","_uuid":"2475893dbeabff7c165283970eebea2fd2de9bd7","trusted":true},"cell_type":"code","source":"from scipy import ndimage\nlabels, nlabels = ndimage.label(mask)\n\nlabel_arrays = []\nfor label_num in range(1, nlabels+1):\n    label_mask = np.where(labels == label_num, 1, 0)\n    label_arrays.append(label_mask)\n\nprint('There are {} separate components / objects detected.'.format(nlabels))","execution_count":61,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_cell_guid":"14e0c916-d8c9-42df-acd0-3a89f4d0d072","_uuid":"53cb03a6457b538ad803ffe4fc1159b085f2a72e","trusted":true},"cell_type":"code","source":"# Create a random colormap\nfrom matplotlib.colors import ListedColormap\nrand_cmap = ListedColormap(np.random.rand(256,3))\n\nlabels_for_display = np.where(labels > 0, labels, np.nan)\nplt.imshow(im_gray, cmap='gray')\nplt.imshow(labels_for_display, cmap=rand_cmap)\nplt.axis('off')\nplt.title('Labeled Cells ({} Nuclei)'.format(nlabels))\nplt.show()","execution_count":62,"outputs":[]},{"metadata":{"_cell_guid":"e7b12a4f-c2d6-4a73-ab78-c85fe32de67e","_uuid":"a510167c5bc06a6843a4255d8e3505df23cb95f1"},"cell_type":"markdown","source":"A quick glance reveals two problems (in this very simple image): \n\n- There are a few individual pixels that stand alone (e.g. top-right)\n- Some cells are combined into a single mask (e.g., top-middle)\n    \nUsing `ndimage.find_objects`, we can iterate through our masks, zooming in on the individual nuclei found to apply additional processing steps.  `find_objects` returns a list of the coordinate range for each labeled object in your image."},{"metadata":{"_cell_guid":"3efbc350-ab52-4ee1-9dec-8a2e68d9a184","_uuid":"34ab91ec5564752381fe300645aa4e4732288e14","trusted":true},"cell_type":"code","source":"for label_ind, label_coords in enumerate(ndimage.find_objects(labels)):\n    cell = im_gray[label_coords]\n    \n    # Check if the label size is too small\n    if np.product(cell.shape) < 10: \n        print('Label {} is too small! Setting to 0.'.format(label_ind))\n        mask = np.where(labels==label_ind+1, 0, mask)\n\n# Regenerate the labels\nlabels, nlabels = ndimage.label(mask)\nprint('There are now {} separate components / objects detected.'.format(nlabels))","execution_count":63,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_cell_guid":"9a7a4b35-ceda-46bd-be8e-a5afe2437364","_uuid":"28cc9fda0ca6b5e0060591ca9ed7f41d52919a20","trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(1,6, figsize=(10,6))\n\nfor ii, obj_indices in enumerate(ndimage.find_objects(labels)[0:6]):\n    cell = im_gray[obj_indices]\n    axes[ii].imshow(cell, cmap='gray')\n    axes[ii].axis('off')\n    axes[ii].set_title('Label #{}\\nSize: {}'.format(ii+1, cell.shape))\n\nplt.tight_layout()\nplt.show()","execution_count":64,"outputs":[]},{"metadata":{"_cell_guid":"20b86040-6a0f-4084-ba70-263efc538465","_uuid":"c44a9f56a96eb0b65464f1b20a3738dc4051826d"},"cell_type":"markdown","source":"Label #2 has the \"adjacent cell\" problem: the two cells are being considered part of the same object. One thing we can do here is to see whether we can shrink the mask to \"open up\" the differences between the cells. This is called mask erosion. We can then re-dilate it to to recover the original proportions. "},{"metadata":{"_cell_guid":"20baf3f4-76b7-451e-b89d-566b17d879af","_uuid":"aea9549db2f19d7551fb2a021bd877572c8555fb","collapsed":true,"trusted":true},"cell_type":"code","source":"# Get the object indices, and perform a binary opening procedure\ntwo_cell_indices = ndimage.find_objects(labels)[1]\ncell_mask = mask[two_cell_indices]\ncell_mask_opened = ndimage.binary_opening(cell_mask, iterations=8)","execution_count":65,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_cell_guid":"d807c382-a1cb-4151-b45e-e7572436528f","_uuid":"cc2b1241e08673952bc6da9f4d05b95608a2c1e8","trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(1,4, figsize=(12,4))\n\naxes[0].imshow(im_gray[two_cell_indices], cmap='gray')\naxes[0].set_title('Original object')\naxes[1].imshow(mask[two_cell_indices], cmap='gray')\naxes[1].set_title('Original mask')\naxes[2].imshow(cell_mask_opened, cmap='gray')\naxes[2].set_title('Opened mask')\naxes[3].imshow(im_gray[two_cell_indices]*cell_mask_opened, cmap='gray')\naxes[3].set_title('Opened object')\n\n\nfor ax in axes:\n    ax.axis('off')\nplt.tight_layout()\nplt.show()","execution_count":66,"outputs":[]},{"metadata":{"_cell_guid":"ad2b865b-678c-41de-9db1-133462f20377","_uuid":"9f417451b0234f5f714be676cd5a217873b8fe98"},"cell_type":"markdown","source":"# Convert each labeled object to Run Line Encoding\nFinally, we need to encode each `label_mask` into a \"run line encoded\" string. Basically, we walk through the array, and when we find a pixel that is part of the mask, we index it and count how many subsequent pixels are also part of the mask. We repeat this each time we see new pixel start point.\n\nI found a nice function to do RLE from [Kaggle user Rakhlin's kernel](https://www.kaggle.com/rakhlin/fast-run-length-encoding-python), which I've copied here."},{"metadata":{"_cell_guid":"29ea6b76-766c-487d-8f88-d6b0e288e94f","_uuid":"4d64b21aa0f26d177863b3cae7fb91e80bc39180","trusted":true},"cell_type":"code","source":"def rle_encoding(x):\n    '''\n    x: numpy array of shape (height, width), 1 - mask, 0 - background\n    Returns run length as list\n    '''\n    dots = np.where(x.T.flatten()==1)[0] # .T sets Fortran order down-then-right\n    run_lengths = []\n    prev = -2\n    for b in dots:\n        if (b>prev+1): run_lengths.extend((b+1, 0))\n        run_lengths[-1] += 1\n        prev = b\n    return \" \".join([str(i) for i in run_lengths])\n\nprint('RLE Encoding for the current mask is: {}'.format(rle_encoding(label_mask)))","execution_count":67,"outputs":[]},{"metadata":{"_cell_guid":"42577f67-935c-4269-b6ef-d7c166723779","_uuid":"dfcd51eaaa5fdf144ce7e9de6397e8abea0b50b9","collapsed":true},"cell_type":"markdown","source":"# Combine it into a single function\nNow that we've seen the basic steps to processing an image in a \"dumb\" way, we can combine it all into a single function. This function will take an image path, perform the processes outlined above, and spit out a dataframe with the RLE strings for each mask found. \n\nWe also create a wrapper function that will spit out a single DataFrame for all images in the dataset."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"19536a3e3030ac1d13318601dad948af012547b9"},"cell_type":"code","source":"## Funciones para convertir adecuadamente una imagen a escala de grises\ndef descriptoresCh(ch):\n    hist, bins= np.histogram(ch,256)\n    hmaxA=hist[0:128].argmax()\n    hmaxB=hist[128:].argmax()+128\n    vmax=np.amax(ch)\n    vmin=np.amin(ch)\n    media=ch.mean()\n    return [media, vmin, vmax, hmaxA,hmaxB]\n\n#Funcion para convertir a escala de grises, usando el clasificador\ndef rgbToGray(im):\n    height, width, levels = im.shape\n    m=[int(im[:,:,c].mean()) for c in range(levels)]\n    typeIm='';\n    if m[0]== m[1] and m[1] == m[2]:\n        #La imagen esta en escala de grises\n        imG=im[:,:,0] #Toma un solo canal\n    else:\n        #En caso contrario usa el clasificador\n        di=[]#Descriptores de los diferentes canales\n        di.extend(descriptoresCh(img[:,:,0]))\n        di.extend(descriptoresCh(img[:,:,1]))\n        di.extend(descriptoresCh(img[:,:,2]))\n        x=modeloClasificador.predict([di])\n        imG=im[:,:,x[0]] #Toma un solo canal\n    return imG\n            ","execution_count":118,"outputs":[]},{"metadata":{"_cell_guid":"0d32b900-8f40-4eff-8465-0f715f24faf6","_uuid":"944f216004fa635201470b8eaa28a446d9679516","collapsed":true,"trusted":true},"cell_type":"code","source":"import pandas as pd\n\ndef analyze_image(im_path):\n    '''\n    Take an image_path (pathlib.Path object), preprocess and label it, extract the RLE strings \n    and dump it into a Pandas DataFrame.\n    '''\n    # Read in data and convert to grayscale\n    im_id = im_path.parts[-3]\n    im = imageio.imread(str(im_path))\n    #im_gray = rgb2gray(im)\n    im_gray = rgbToGray(im)#Cambio\n\n    # Mask out background and extract connected objects\n    thresh_val = threshold_otsu(im_gray)\n    mask = np.where(im_gray > thresh_val, 1, 0)\n    if np.sum(mask==0) < np.sum(mask==1):\n        mask = np.where(mask, 0, 1)    \n        labels, nlabels = ndimage.label(mask)\n    labels, nlabels = ndimage.label(mask)\n    \n    # Loop through labels and add each to a DataFrame\n    im_df = pd.DataFrame()\n    for label_num in range(1, nlabels+1):\n        label_mask = np.where(labels == label_num, 1, 0)\n        if label_mask.flatten().sum() > 10:\n            rle = rle_encoding(label_mask)\n            s = pd.Series({'ImageId': im_id, 'EncodedPixels': rle})\n            im_df = im_df.append(s, ignore_index=True)\n    \n    return im_df\n\n\ndef analyze_list_of_images(im_path_list):\n    '''\n    Takes a list of image paths (pathlib.Path objects), analyzes each,\n    and returns a submission-ready DataFrame.'''\n    all_df = pd.DataFrame()\n    for im_path in im_path_list:\n        im_df = analyze_image(im_path)\n        all_df = all_df.append(im_df, ignore_index=True)\n    \n    return all_df","execution_count":120,"outputs":[]},{"metadata":{"_cell_guid":"58b07c91-855f-4e6d-a9d8-457750d9ad4a","_uuid":"b7e21d3b95d7936904bfed088a880a5c78f0fa18","collapsed":true,"trusted":true},"cell_type":"code","source":"testing = pathlib.Path('../input/stage1_test/').glob('*/images/*.png')\ndf = analyze_list_of_images(list(testing))\ndf.to_csv('submission.csv', index=None)","execution_count":121,"outputs":[]},{"metadata":{"_cell_guid":"5b42577a-d990-40ae-aba3-03571ef16782","_uuid":"51fdaa6a207574dc67d497eca81eb8d757021fe4","collapsed":true,"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}