{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "365fab3b-fe40-0957-3dad-98aa199b3c09"
      },
      "source": [
        "# Tensorflow Multiple Layer Neural Network for Credit Card Fraud Detection   \n",
        "-- by Sijiang Du, May 2017 \n",
        " \n",
        "## Summary \n",
        " \n",
        "This Multi-layer neoral network is trained by the credit card transactions data set.  The raw data are labeled as two classes fraudulent or genuine. \n",
        "The challenge part of this classification is  how to identify a very small percentage of fraud cases among many normal cases. The total fraud transactions are less than 0.2% in total 284807 transaction records.  \n",
        "The tensorflow python  program constructs a neural network computation model to detect the fraud transactions. The overall accuracy is measured and reach above 98%. A better method, Credit card scoring and Kolmogorov Smirnov chart, is being used to evaluate the model. \n",
        " \n",
        "## Preprocessing Credit Card Data Set \n",
        " \n",
        "The following is the head of the csv file and one row data: \n",
        "\"Time\",\"V1\",\"V2\",\"V3\",\"V4\",\"V5\",\"V6\",\"V7\",\"V8\",\"V9\",\"V10\",\"V11\",\"V12\",\"V13\",\"V14\",\"V15\",\"V16\",\"V17\",\"V18\",\"V19\",\"V20\",\"\n",
        "V21\",\"V22\",\"V23\",\"V24\",\"V25\",\"V26\",\"V27\",\"V28\",\"Amount\",\"Class\" \n",
        " \n",
        "The last column of the row is \"Class\", where \"0\" is for genuine (true) transaction and \"1\" is for fraud transaction. The python program replace the \"Class\" column with two fabricated columns to indicate the likelihood of being fraud or genuine: \n",
        "      \"Fraud\" column:   0 if \"Class\" is 0, 1 if \"Class\" 1 \n",
        "       \"True\" column:  1 if \"Class\" is 0, 0 if \"Class\" 1 \n",
        "Therefore for the row value [0] -- >  [0,1], and [1] -- >  [1,0]  \n",
        "\n",
        "              csv_r = csv.reader(data) \n",
        "               for line in csv_r: \n",
        "                  if line[-1]=='0': \n",
        "                      line = line[:-1] + ['0','1']   \n",
        "                  elif line[-1]=='1': \n",
        "                      line = line[:-1] + ['1','0'] \n",
        " \n",
        "The data set is divided to two parts: 80% for training set and 20% for testing set  by \"csv_partition_train_test(csv_file_name, 0.8)\". \n",
        " \n",
        "## Tensorflow Input Pipeline \n",
        " \n",
        "Tensorflow input pipeline is experimented to provide practical sample codes for future use.\n",
        "\n",
        "The 80% training data are separated to two sets and save in two files: one is for \"true\" cases and the other is for \n",
        "\"fraud\" cases. \n",
        "\n",
        "Each training step is done in a batch input. Half of the batch data are fraud and another half are genuine (true). E.g., for a batch size 128, the input tensor has 128 rows, 64 rows are for \"true\" set, and 64 rows are from \"fraud\" set. \n",
        "They are mixed and shuffled to form a 128-row batch.   \n",
        "\n",
        "The tensorflow input pipeline organizes the feeding data by a list of files. And records can be repeatedly obtained \n",
        "for a defined number of epochs (or cycling forever if not defined) from an input queue after shuffling. \n",
        " \n",
        "## Multi-layer Graph \n",
        " \n",
        "The are \"layer_num\" hidden layers plus one output layer. Each layer has  \"neuron_num\" nodes.  Activation function \n",
        "is tf.identity.  The train( ) function is designed with parameters to specify the number of hidden layers and the \n",
        "number neurons in each layer. This is helpful to run experiments with variant range of sizes and depths.   \n",
        "\n",
        "E.g.  Call \"train(5,100)\", means 5 hidden layers, each layer has 100 neurons.  or, can put train(10, 2000) for a really \n",
        "large structure. The computation graph is constructed by while loop to generate multiple hidden layers. \n",
        "  \n",
        " ![enter image description here][1]\n",
        "\n",
        "      **Figure 1. Neural Network Graphs Generated by Tensorboard** \n",
        " \n",
        "\n",
        "## KS Graph to Evaluate Testing Results \n",
        " \n",
        "Experiments show that 3 epochs are sufficient to settle the model and achieve a stable accuracy. The testing result \n",
        "is evaluated by two measurements: overall accuracy (successful rate ) and KS value by credit card scoring.\n",
        "  \n",
        "Using the KS value is a more appropriate method to evaluate the performance because the percentage of fraud cases is too small. A high successful rate, for instance 99.0%, does not necessarily prove a strong evidence that a fraud detection can be detected.  \n",
        " \n",
        "Construct the Kolmogorov Smirnov chart: \n",
        "\n",
        "* Scorecard: the trained model generates a scorecard for each test sample, which contains a score indicates how \"creditable\" this sample is. The scores of test cases are comparable and sortable. The test samples are sorted by their scorecard scores. \n",
        "* Score: the value is computed from the two columns of the output layer. The first column indicates the likelihood of being fraud. The second column indicates the likelihood being genuine. The scorecard score is equal to the second column value minus the first column value. \n",
        "* Cumulative Percentages:  The blue lines shows the cumulative percentages of genuine cases predicted correctly, while the red lines shows the cumulative percentages of fraud cases predicted correctly, by the trained model.   \n",
        "* KS Value: max vertical distance in between blue and red lines.  Larger distance values mean better performances to identify fraud cases.\n",
        "  \n",
        "\n",
        "The length of green line is the KS value \n",
        "\n",
        "  ![enter image description here][2]\n",
        "\n",
        "         **Figure 2. KS Graph shows the capability of fraud detection** \n",
        "\n",
        "        The overall successful rate of this 4 hidden layer neural network is 99.0%.\n",
        "        Each hidden layer has 20 output channels (20 neurons). \n",
        "        The KS value (0.855 shows encouraging performance on the fraud detection. \n",
        "\n",
        "The test are done for many sizes and depths model. However, larger and deeper models do not generate better result.  Th result of 4 hidden layers with 200 neurones in each layer achieve almost same performance as the 4-layer-20-neuron model. \n",
        "\n",
        "## Amazing Result by 0-hidden Layer Test \n",
        " \n",
        "Run \"train(1,1)\", such that the hidden layer becomes one channel data pathway directly to output layer. That means the input vector is able to generate output only by one step linear transform( weights matrix multiplication). The result has same performance as the 4 layer 200 neurons model and 4 layer 20 neurons model.   \n",
        "  \n",
        "## Conclusion \n",
        " \n",
        "* There are obvious patterns on fraud and genuine transactions. The patterns are simple therefor an very simple one-layer trained NN  can obtain a high performance of fraud detection.  \n",
        "* The training does not require feature extraction/selection from the input data set. \n",
        "* The KS graph is a better way to evaluate the performance than just looking overall average accuracy on both classes. \n",
        "\n",
        "\n",
        " \n",
        " \n",
        "\n",
        "\n",
        "  [1]: https://github.com/sijiangdu/Tensorflow/blob/master/credit_card_fraud_detection/4_layer_20_Node.png?raw=true\n",
        "  [2]: https://github.com/sijiangdu/Tensorflow/blob/master/credit_card_fraud_detection/creditcard_ks_990_855_4_20_e3.png?raw=true"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "4a2d190f-6f0e-ed77-f0d6-e364c2ed8f0c"
      },
      "outputs": [],
      "source": [
        "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n",
        "# Modifications copyright (C) 2017 Sijiang Du\n",
        "# Licensed under the Apache License, Version 2.0 (the 'License');\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an 'AS IS' BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# ==============================================================================\n",
        "\"\"\"Credit Card Fraud Detection\n",
        "Multi-layer neoral network is trained by the data set ofaAnonymized credit card transactions labeled as fraudulent or genuine\n",
        "There are two classes of labeled rows: fraud and normal(true) transactions The total fraud trainsactions are less than 0.2%\n",
        "among overall 284807 records. The program constructs a neural network model to detect the fraud transactions. \n",
        "The training has several epochs. Each training step is done in a batch inputs. Half of the batch data are fraud data and another half\n",
        "are normal transactions data.\n",
        "The data set is divided to two parts: 80% for training set and 20% for testing set.\n",
        "The testing result is evaluated by two measurements: overall accuracy and KS value,\n",
        "The Kolmogorov Smirnov chart is ploted as result and a PNG file is saved in FLAGS.result_dir folder.\n",
        "\n",
        "Tensorflow input_pipeline is being used to streamline and randomlize inputs for the training and testing session.\n",
        "To start: call the train() function, e.g. \"train(5,100)\", means 5 hidden layers, each layer has 100 neurons.  \n",
        "\n",
        "Author: Sijiang Du, Updated in May 2, 2017\n",
        "\n",
        "This file is a modified file of https://www.github.com/tensorflow/tensorflow/blob/r1.1/tensorflow/examples/tutorials/mnist/mnist_with_summaries.py\n",
        "In this file:\n",
        "The input data are in \"creditcard.csv\" which is downloaded from https://www.kaggle.com/dalpozz/creditcardfraud\n",
        "\"\"\"\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import argparse\n",
        "import sys\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "\n",
        "FLAGS = None\n",
        "\n",
        "import numpy as np\n",
        "import os.path\n",
        "import csv\n",
        "import random\n",
        "\n",
        "csv_file_name = \"../input/creditcard.csv\"\n",
        "train_file_name_true = \"creditcard_train_true.csv\"\n",
        "train_file_name_fraud = \"creditcard_train_fraud.csv\"\n",
        "test_file_name_true = \"creditcard_test_true.csv\"\n",
        "test_file_name_fraud = \"creditcard_test_fraud.csv\"\n",
        "test_file_name_both =  \"creditcard_test_all.csv\"\n",
        "\n",
        "batch_size = 128\n",
        "input_channel_num = 30\n",
        "output_channel_num = 2\n",
        "hidden_layer_neuron = 200\n",
        "hidden_layer_num = 3\n",
        "train_epoch = 3\n",
        "csv_lines_train = 0\n",
        "csv_lines_test_both = 0\n",
        " \n",
        "# partition: randomly partion the input file to two files. E.g. partition = 0.8: 80% for training 20% for testing.\n",
        "def csv_partition_train_test(input_file, partition=0.8):\n",
        "  with open(input_file) as data:\n",
        "      with open(FLAGS.data_dir+test_file_name_true, 'w+') as test_true,\\\n",
        "           open(FLAGS.data_dir+test_file_name_fraud, 'w+') as test_fraud,\\\n",
        "           open(FLAGS.data_dir+test_file_name_both, 'w+') as test_both:\n",
        "          with open(FLAGS.data_dir+train_file_name_true, 'w+') as train_true, open(FLAGS.data_dir+train_file_name_fraud, 'w+') as train_fraud:\n",
        "              header = next(data)\n",
        " #             test.write(header)\n",
        " #             train.write(header)           \n",
        "              csv_r = csv.reader(data)\n",
        "              csv_w_train_true = csv.writer(train_true)\n",
        "              csv_w_train_fraud = csv.writer(train_fraud)\n",
        "              csv_w_test_true = csv.writer(test_true)\n",
        "              csv_w_test_fraud = csv.writer(test_fraud)\n",
        "              csv_w_test_both = csv.writer(test_both)             \n",
        "              global csv_lines_test_both\n",
        "              global csv_lines_train\n",
        "              \n",
        "              for line in csv_r:\n",
        "                \n",
        "                  if line[-1]=='0':\n",
        "                      line = line[:-1] + ['0','1']  \n",
        "                  elif line[-1]=='1':\n",
        "                      line = line[:-1] + ['1','0']\n",
        "                       \n",
        "                  if random.random() < partition:\n",
        "                    csv_lines_train +=1\n",
        "                    if line[-1]=='0':\n",
        "                       csv_w_train_fraud.writerow(line)\n",
        "                    else:\n",
        "                       csv_w_train_true.writerow(line)\n",
        "                       \n",
        "                  else:\n",
        "                    csv_w_test_both.writerow(line)\n",
        "                    csv_lines_test_both += 1\n",
        "                    if line[-1]=='0':\n",
        "                       csv_w_test_fraud.writerow(line)\n",
        "                    else:\n",
        "                       csv_w_test_true.writerow(line)\n",
        "\n",
        "\n",
        "def read_creditcard_csv(filename_queue):\n",
        "\n",
        "  reader = tf.TextLineReader(skip_header_lines=1)\n",
        "  key, value = reader.read(filename_queue)\n",
        "  record_defaults = [[0.0]for row in range(32)]\n",
        "  cols = tf.decode_csv(\n",
        "  value, record_defaults=record_defaults)\n",
        "  features = tf.stack(cols[:-2])\n",
        "  label = tf.stack([cols[30],cols[31]])\n",
        "  return features, label\n",
        "\n",
        "def input_pipeline(filenames, batch_size, num_epochs=None):\n",
        "  filename_queue = tf.train.string_input_producer(\n",
        "      filenames, num_epochs=num_epochs, shuffle=True)\n",
        "  example, label = read_creditcard_csv(filename_queue)\n",
        "  # min_after_dequeue defines how big a buffer we will randomly sample\n",
        "  #   from -- bigger means better shuffling but slower start up and more\n",
        "  #   memory used.\n",
        "  # capacity must be larger than min_after_dequeue and the amount larger\n",
        "  #   determines the maximum we will prefetch.  Recommendation:\n",
        "  #   min_after_dequeue + (num_threads + a small safety margin) * batch_size\n",
        "  min_after_dequeue = 10000\n",
        "  capacity = min_after_dequeue + 3 * batch_size\n",
        "  example_batch, label_batch = tf.train.shuffle_batch(\n",
        "      [example, label], batch_size=batch_size, capacity=capacity,\n",
        "      min_after_dequeue=min_after_dequeue)\n",
        "  return example_batch, label_batch\n",
        "\n",
        "def train(layer_num= hidden_layer_num, neuron_num = hidden_layer_neuron):\n",
        "\n",
        "  if    not os.path.exists(FLAGS.data_dir+train_file_name_true)\\\n",
        "     or not os.path.exists(FLAGS.data_dir+train_file_name_fraud)\\\n",
        "     or not os.path.exists(FLAGS.data_dir+test_file_name_true)\\\n",
        "     or not os.path.exists(FLAGS.data_dir+test_file_name_fraud):                    \n",
        "    csv_partition_train_test(csv_file_name, 0.8)\n",
        " \n",
        "  sess = tf.InteractiveSession()\n",
        "  # Create a multilayer model.\n",
        "\n",
        "  # Input placeholders\n",
        "  with tf.name_scope('input'):\n",
        "    x = tf.placeholder(tf.float32, [None, input_channel_num], name='x-input')\n",
        "    y_ = tf.placeholder(tf.float32, [None, output_channel_num], name='y-input')\n",
        "\n",
        "  def weight_variable(shape):\n",
        "    \"\"\"Create a weight variable with appropriate initialization.\"\"\"\n",
        "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
        "    return tf.Variable(initial)\n",
        "\n",
        "  def bias_variable(shape):\n",
        "    \"\"\"Create a bias variable with appropriate initialization.\"\"\"\n",
        "    initial = tf.constant(0.1, shape=shape)\n",
        "    return tf.Variable(initial)\n",
        "\n",
        "  def variable_summaries(var):\n",
        "    \"\"\"Attach a lot of summaries to a Tensor (for TensorBoard visualization).\"\"\"\n",
        "    with tf.name_scope('summaries'):\n",
        "      mean = tf.reduce_mean(var)\n",
        "      tf.summary.scalar('mean', mean)\n",
        "      with tf.name_scope('stddev'):\n",
        "        stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
        "      tf.summary.scalar('stddev', stddev)\n",
        "      tf.summary.scalar('max', tf.reduce_max(var))\n",
        "      tf.summary.scalar('min', tf.reduce_min(var))\n",
        "      tf.summary.histogram('histogram', var)\n",
        "\n",
        "  def nn_layer(input_tensor, input_dim, output_dim, layer_name, act=tf.nn.relu, keep_prob=1.0):\n",
        "    \"\"\"Reusable code for making a simple neural net layer.\n",
        "    It does a matrix multiply, bias add, and then uses relu to nonlinearize.\n",
        "    It also sets up name scoping so that the resultant graph is easy to read,\n",
        "    and adds a number of summary ops.\n",
        "    \"\"\"\n",
        "    # Adding a name scope ensures logical grouping of the layers in the graph.\n",
        "    with tf.name_scope(layer_name):\n",
        "      # This Variable will hold the state of the weights for the layer\n",
        "      with tf.name_scope('weights'):\n",
        "        weights = weight_variable([input_dim, output_dim])\n",
        "        print(weights)\n",
        "        variable_summaries(weights)\n",
        "      with tf.name_scope('biases'):\n",
        "        biases = bias_variable([output_dim])\n",
        "        variable_summaries(biases)\n",
        "      with tf.name_scope('Wx_plus_b'):\n",
        "        preactivate = tf.matmul(input_tensor, weights) + biases\n",
        "      activations = act(preactivate, name='activation')\n",
        "      tf.summary.histogram('activations', activations)\n",
        "      if FLAGS.dropout<1.0: dropped = tf.nn.dropout(activations, keep_prob)\n",
        "      return activations\n",
        "    \n",
        "  keep_prob = tf.placeholder(tf.float32)\n",
        "  \n",
        "  #Add layer_num layers of hidden layer\n",
        "  cur_layer = nn_layer(x, input_channel_num, neuron_num, 'layer_0', tf.identity, keep_prob)\n",
        "  for i in range(1,layer_num):\n",
        "    cur_layer = nn_layer(cur_layer, neuron_num, neuron_num, 'layer_'+str(i), tf.identity, keep_prob)\n",
        "\n",
        "  # the last layer is the output layer\n",
        "  y = nn_layer(cur_layer, neuron_num, output_channel_num, 'output_layer', act=tf.identity)\n",
        "  # scale the activation down by the size of hidden layer\n",
        "  y = y/neuron_num\n",
        "\n",
        "  with tf.name_scope('cross_entropy'):\n",
        "    diff = tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y)\n",
        "    with tf.name_scope('total'):\n",
        "      cross_entropy = tf.reduce_mean(diff)\n",
        "  tf.summary.scalar('cross_entropy', cross_entropy)\n",
        "\n",
        "  with tf.name_scope('train'):\n",
        "    train_step = tf.train.AdamOptimizer(FLAGS.learning_rate).minimize(\n",
        "        cross_entropy)\n",
        "\n",
        "  with tf.name_scope('accuracy'):\n",
        "    with tf.name_scope('correct_prediction'):\n",
        "      correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
        "    with tf.name_scope('accuracy'):\n",
        "      accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "  tf.summary.scalar('accuracy', accuracy)\n",
        "\n",
        "  #scorecard for the model evaluation. A lower score indictates a higher probability to be fraud.\n",
        "  with tf.name_scope('score_card'):\n",
        "    yc = tf.reshape( y[:,1]-y[:,0], (-1,1))\n",
        "    yc_ = tf.reshape( y_[:,1]-y_[:,0], (-1,1))\n",
        "    score_card = tf.concat([yc, yc_], 1)\n",
        "  \n",
        "  with tf.name_scope('input_examples'):\n",
        "      example_batch_train_true, label_batch_train_true = input_pipeline(tf.constant([FLAGS.data_dir+train_file_name_true]), round(batch_size/2))\n",
        "      example_batch_train_fraud, label_batch_train_fraud = input_pipeline(tf.constant([FLAGS.data_dir+train_file_name_fraud]), batch_size-round(batch_size/2))\n",
        "      example_batch_test_true, label_batch_test_true = input_pipeline(tf.constant([FLAGS.data_dir+test_file_name_true]), batch_size)\n",
        "      example_batch_test_fraud, label_batch_test_fraud = input_pipeline(tf.constant([FLAGS.data_dir+test_file_name_fraud]), batch_size)\n",
        "      example_batch_test_both, label_batch_test_both = input_pipeline(tf.constant([FLAGS.data_dir+test_file_name_both]), batch_size,1)\n",
        "      \n",
        "  init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
        "\n",
        "  # Merge all the summaries and write them out to /tmp/tensorflow/creditcard/logs/creditcard_with_summaries (by default)\n",
        "  merged = tf.summary.merge_all()\n",
        "  train_writer = tf.summary.FileWriter(FLAGS.log_dir + '/train', sess.graph)\n",
        "  test_writer_true = tf.summary.FileWriter(FLAGS.log_dir + '/test_true')\n",
        "  test_writer_fraud = tf.summary.FileWriter(FLAGS.log_dir + '/test_fraud')\n",
        "\n",
        "  sess.run(init_op)\n",
        "  # Start input enqueue threads.\n",
        "  coord = tf.train.Coordinator()\n",
        "  threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
        "\n",
        "  # Train the model, and also write summaries.\n",
        "  # Every 10th step, measure test-set accuracy, and write test summaries\n",
        "  # All other steps, run train_step on training data, & add training summaries\n",
        "\n",
        "  def feed_dict(train,test=0):\n",
        "    \"\"\"Make a TensorFlow feed_dict: maps data onto Tensor placeholders.\"\"\"\n",
        "    if train:\n",
        "      batch_xs_1, batch_ys_1 = sess.run([example_batch_train_true, label_batch_train_true])\n",
        "      batch_xs_2, batch_ys_2 = sess.run([example_batch_train_fraud, label_batch_train_fraud])\n",
        "\n",
        "      xs = np.concatenate ((batch_xs_1,batch_xs_2))\n",
        "      ys = np.concatenate ((batch_ys_1,batch_ys_2))\n",
        "      perm = np.random.permutation(xs.shape[0])\n",
        "      np.take(xs,perm,axis=0,out=xs)\n",
        "      np.take(ys,perm,axis=0,out=ys)\n",
        "      k = FLAGS.dropout\n",
        "    else:\n",
        "      if test==0:\n",
        "        xs, ys = sess.run([example_batch_test_fraud, label_batch_test_fraud])\n",
        "      elif test==1:\n",
        "        xs, ys = sess.run([example_batch_test_true, label_batch_test_true])\n",
        "      elif test==2:\n",
        "        xs, ys = sess.run([example_batch_test_both, label_batch_test_both])\n",
        "      k = 1.0\n",
        "    return {x: xs, y_: ys, keep_prob: k}\n",
        "\n",
        "  scorecard_all = [[0, 0]]\n",
        "  acc_all = []\n",
        "  print('[Training inputs: %s, epoch = %s], [Testing inputs: %s],' % (csv_lines_train, train_epoch,csv_lines_test_both))\n",
        "  try:      \n",
        "      i = 0\n",
        "      train_s = csv_lines_train*train_epoch/batch_size\n",
        "      # plotting\n",
        "      fig1 = plt.figure(1)\n",
        "      ax = fig1.add_subplot(111)\n",
        "      plt.ylim((-0.5, 1.5))\n",
        "      plt.ylabel('Accuracy')\n",
        "      plt.xlabel('Training Batches')\n",
        "      ax.text(0.05, 0.90,'Fraud: (red) --', ha='left', va='center', color='red', transform=ax.transAxes)\n",
        "      ax.text(0.05, 0.85,'Normal: (blue) --', ha='left', va='center', color='blue', transform=ax.transAxes)\n",
        "      fig1.suptitle('Tensorflow: Creditcard Fraud Detection Training')\n",
        "      while not coord.should_stop():\n",
        "            i = i+1\n",
        "\n",
        "            if i<=train_s :\n",
        "               #Record train set summaries, and train\n",
        "              if i % 100 == 0:  # Record execution stats\n",
        "                run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
        "                run_metadata = tf.RunMetadata()\n",
        "                summary, _= sess.run([merged, train_step],\n",
        "                                      feed_dict=feed_dict(True),\n",
        "                                      options=run_options,\n",
        "                                      run_metadata=run_metadata)\n",
        "                train_writer.add_run_metadata(run_metadata, 'step%03d' % i)\n",
        "                train_writer.add_summary(summary, i)\n",
        "                # print('Adding run metadata for', i)\n",
        "                \n",
        "              else:  # Record a summary\n",
        "                summary, _ = sess.run([merged, train_step], feed_dict=feed_dict(True))\n",
        "                train_writer.add_summary(summary, i)\n",
        "\n",
        "              if i % 10 == 0:  # Record summaries and test-set accuracy\n",
        "                  summary, acc0 = sess.run([merged, accuracy], feed_dict=feed_dict(False,0))\n",
        "                  test_writer_fraud.add_summary(summary, i)\n",
        "                  summary, acc1 = sess.run([merged, accuracy], feed_dict=feed_dict(False,1))\n",
        "                  test_writer_true.add_summary(summary, i)\n",
        "                  # print('%s: [%s], [%s]' % (i, acc0,acc1))\n",
        "                  \n",
        "                  # plotting\n",
        "                  plt.plot(list(range(i-10,i)),[acc0]*10, 'r', list(range(i-10,i)),[acc1]*10, 'b')\n",
        "                  if i%100 == 0:\n",
        "                    plt.draw()\n",
        "                    plt.pause(0.3)\n",
        "\n",
        "            else:\n",
        "              acc, scorecard = sess.run([accuracy,score_card], feed_dict=feed_dict(False,2))\n",
        "              acc_all.append(acc)\n",
        "              scorecard_all = np.concatenate ((scorecard_all,scorecard))              \n",
        "\n",
        "  except tf.errors.OutOfRangeError:\n",
        "      print('Done training and testing -- epoch limit reached')\n",
        "  finally:\n",
        "      # When done, ask the threads to stop.\n",
        "      coord.request_stop()\n",
        "\n",
        "  scorecard_all = np.delete(scorecard_all, (0), axis=0)\n",
        "  scorecard_all = scorecard_all[scorecard_all[:, 0].argsort()]\n",
        "  count_true = 0\n",
        "  count_fraud = 0\n",
        "  percent_true = np.zeros(scorecard_all.shape[0])\n",
        "  percent_fraud = np.zeros(scorecard_all.shape[0])\n",
        "  score_range = []\n",
        "  score_std = scorecard_all[:,0].std()\n",
        "  \n",
        "  for i in range(scorecard_all.shape[0]):\n",
        "      \n",
        "      if scorecard_all[i,1] > 0:\n",
        "          count_true +=1\n",
        "      else: \n",
        "          count_fraud +=1\n",
        "      percent_true[i] = count_true\n",
        "      percent_fraud[i] = count_fraud\n",
        "  ks = 0\n",
        "  ks_pos = 0\n",
        "  for i in range(scorecard_all.shape[0]):\n",
        "      percent_true[i]  /= count_true\n",
        "      percent_fraud[i] /= count_fraud\n",
        "      diff = percent_fraud[i] - percent_true[i]\n",
        "      if diff > ks:\n",
        "        ks = diff\n",
        "        ks_pos = i\n",
        "  accuracy_mean = np.mean(acc_all)\n",
        "  \n",
        "  acc = 'Accuracy (' + str(csv_lines_test_both) + ' samples): ' + str(accuracy_mean)\n",
        "  # print(acc)\n",
        "  fig2 = plt.figure(2,figsize=(10, 8))\n",
        "  ax = fig2.add_subplot(111)\n",
        "  plt.plot(list(scorecard_all[:, 0]),percent_fraud, 'r', list(scorecard_all[:, 0]),percent_true, 'b')\n",
        "  plt.plot([scorecard_all[ks_pos,0],scorecard_all[ks_pos,0]], [percent_true[ks_pos], percent_fraud[ks_pos]], 'g')\n",
        "  ax.text(0.05, 0.95,'Greenline: KS = '+ str(ks), ha='left', va='center', color='green', transform=ax.transAxes)\n",
        "  ax.text(0.05, 0.90,'Redline: Fraud', ha='left', va='center', color='red', transform=ax.transAxes)\n",
        "  ax.text(0.05, 0.85,'Blueline: Normal', ha='left', va='center', color='blue', transform=ax.transAxes)\n",
        "  ax.text(0.05, 0.80,acc, ha='left', va='center', color='black', transform=ax.transAxes)\n",
        "  plt.ylabel('Cumulative percentage')\n",
        "  plt.xlabel('Scorecard scoring')\n",
        "  fig2.suptitle('KS graph (Creditcard Fraud Detection by Tensorflow)\\n('+str(layer_num) +' hidden layers, '+str(neuron_num)+' neurons per layer)')\n",
        "  result_str = str(round(int(accuracy_mean*1000)))+'_'+str(int(round(ks*1000)))+'_'+str(layer_num)+'_'+str(neuron_num)+'_e'+str(train_epoch)\n",
        "  plt.draw()\n",
        "  plt.savefig(FLAGS.result_dir+'/creditcard_ks_'+result_str+'.png')\n",
        "  plt.pause(3) \n",
        "  # Wait for threads to finish.\n",
        "  coord.join(threads)\n",
        "\n",
        "  # Wait for threads to finish.\n",
        "  coord.join(threads)\n",
        "\n",
        "  sess.close()\n",
        "\n",
        "  train_writer.close()\n",
        "  test_writer_true.close()\n",
        "  test_writer_fraud.close()\n",
        "  return ks,accuracy_mean\n",
        "\n",
        "def main(_):\n",
        "  if tf.gfile.Exists(FLAGS.log_dir):\n",
        "    tf.gfile.DeleteRecursively(FLAGS.log_dir)\n",
        "  tf.gfile.MakeDirs(FLAGS.log_dir)\n",
        "  if tf.gfile.Exists(FLAGS.data_dir):\n",
        "    tf.gfile.DeleteRecursively(FLAGS.data_dir)\n",
        "  tf.gfile.MakeDirs(FLAGS.data_dir)\n",
        "  if not tf.gfile.Exists(FLAGS.result_dir):\n",
        "    tf.gfile.MakeDirs(FLAGS.result_dir)\n",
        " # 4 hidden layers, each layer has 20 output channels. One layer is a (20,20) tensor.\n",
        "  train(4, 20)\n",
        "if __name__ == '__main__':\n",
        "  parser = argparse.ArgumentParser()\n",
        "  parser.add_argument('--learning_rate', type=float, default=0.005,\n",
        "                      help='Initial learning rate')\n",
        "  parser.add_argument('--dropout', type=float, default= 1.0,\n",
        "                      help='Keep probability for training dropout.')\n",
        "  parser.add_argument('--data_dir', type=str, default='/tmp/tensorflow/creditcard/input_data/',\n",
        "                      help='Directory for storing input data')\n",
        "  parser.add_argument('--log_dir', type=str, default='/tmp/tensorflow/creditcard/logs/creditcard_with_summaries',\n",
        "                      help='Summaries log directory')\n",
        "  parser.add_argument('--result_dir', type=str, default='/tmp/tensorflow/creditcard/result',\n",
        "                      help='Summaries log directory')\n",
        "\n",
        "  FLAGS, unparsed = parser.parse_known_args()\n",
        "  tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "5eed28f9-13e4-53a6-82b3-8bc3bc9879e3"
      },
      "source": [
        "The above graph shows the training progress.  Each dot is a short test result by 128 test samples. \n",
        "  At the end of every 100 steps of training,  there is a test to show the accuracy for genuine cases and a test to show the accuracy for fraud cases.\n",
        "\n",
        "The dot shows an accuracy value beteen 0 and 1 for the training steps with batch size 128. \n",
        "The blue dots (genuine ) and red dots (fraud) reach above 90% after 2 epochs of training and settle at 99% at the end\n",
        "of 3rd epoch."
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}