{"metadata": {"language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "nbconvert_exporter": "python", "mimetype": "text/x-python", "pygments_lexer": "ipython3", "name": "python", "file_extension": ".py", "version": "3.6.1"}, "anaconda-cloud": {}, "kernelspec": {"display_name": "Python 3", "name": "python3", "language": "python"}}, "nbformat": 4, "nbformat_minor": 1, "cells": [{"metadata": {"_uuid": "d869fe3aeb537d998ab8029f40f1f3f9299decd4", "_cell_guid": "fbca962d-9131-4c49-91ea-cb5c9fcc76bf"}, "source": "# Kaggle Competition: New York City Taxi Trip Duration", "outputs": [], "cell_type": "markdown", "execution_count": null}, {"metadata": {"_uuid": "5b2f15becb68891ce33585cae7da11be3da12f9e", "_cell_guid": "a0b7bbf2-cdf9-48b1-a59d-a2c980a23bfe"}, "source": "The purpose of this analysis is to accurately predict the duration of taxi trips in New York City. This work is for a [Kaggle competition](https://www.kaggle.com/c/nyc-taxi-trip-duration). To make our predictions, we will use a feed-forward neural network using TensorFlow, a RandomForest Regressor, Lightgbm, and Catboost. Random search will be used to find the optimal network architecture and hyperparameter values for each model.\n\nThe sections of this analysis are:\n- Loading the Data\n- Cleaning the Data\n- Building the Neural Network\n- Training the Neural Network\n- Training the Other Models\n- Making Predictions\n- Summary", "outputs": [], "cell_type": "markdown", "execution_count": null}, {"metadata": {"_uuid": "1d8579e514096fe709ccf1f27ae99e4f0608f8b9", "collapsed": false, "trusted": false, "_cell_guid": "516ace71-24b2-4daa-a839-d1d9fc8a2bf0"}, "source": "import pandas as pd\npd.set_option('display.max_columns', None)\nimport numpy as np\nimport tensorflow as tf\n\nfrom sklearn.ensemble import RandomForestRegressor as RFR\nimport lightgbm as lgb\nfrom catboost import CatBoostRegressor\n\nfrom collections import namedtuple\nfrom pandas.tseries.holiday import USFederalHolidayCalendar\nfrom pandas.tseries.offsets import CustomBusinessDay\n\nimport time\nimport operator\nimport haversine\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom datetime import timedelta\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline  \n\nprint(tf.__version__)", "outputs": [], "cell_type": "code", "execution_count": 131}, {"metadata": {"_uuid": "eddc62384c5da1db7a999dc86ee3c0769fc52e44", "_cell_guid": "81b6f01e-2730-4207-85e6-39c632a931ed"}, "source": "## Loading the Data", "outputs": [], "cell_type": "markdown", "execution_count": null}, {"metadata": {"_uuid": "2ede60979365dcb490e9844b69b2b44dc7be8582", "collapsed": true, "trusted": false, "_cell_guid": "131da513-3165-41d6-af65-a03050f0bd14"}, "source": "train = pd.read_csv(\"../input/train.csv\")\ntest = pd.read_csv(\"../input/test.csv\")", "outputs": [], "cell_type": "code", "execution_count": 132}, {"metadata": {"_uuid": "d439c8b5c4e0c0b197bb9941e8df71b8bda970d9", "collapsed": false, "trusted": false, "_cell_guid": "4f7735e2-cf99-47d0-bb88-00b5f15b9008"}, "source": "train.head()", "outputs": [], "cell_type": "code", "execution_count": 133}, {"metadata": {"_uuid": "b0de994b752bf6581612f41c6f932ee22303333e", "collapsed": false, "trusted": false, "_cell_guid": "b1c39e9a-adae-492a-92fd-5d591ed9bf80"}, "source": "test.head()", "outputs": [], "cell_type": "code", "execution_count": 134}, {"metadata": {"_uuid": "74a0a17f6a576a848cbb0dfa4af8ca68e471e0bb", "collapsed": false, "trusted": false, "_cell_guid": "a9fafc5f-42cb-44f4-a323-1bbab295949b"}, "source": "print(train.shape)\nprint(test.shape)", "outputs": [], "cell_type": "code", "execution_count": 135}, {"metadata": {"_uuid": "be9d33f03a8463f3ef27e77e7bf7d6576ec46414", "collapsed": false, "trusted": false, "_cell_guid": "2ccb72f3-8ede-422e-afac-379cf1bb61ec"}, "source": "# Check for any duplicates\nprint(train.duplicated().sum())\nprint(train.id.duplicated().sum())\nprint(test.id.duplicated().sum())", "outputs": [], "cell_type": "code", "execution_count": 136}, {"metadata": {"_uuid": "8ee8a65e16dc1e967b6e6923ce51587da45dbf52", "collapsed": false, "trusted": false, "_cell_guid": "4b3657a7-bb0e-4566-a70f-ff8eddb3a63a"}, "source": "# Sanity check to ensure all trips are valid\nsum(train.dropoff_datetime < train.pickup_datetime)", "outputs": [], "cell_type": "code", "execution_count": 137}, {"metadata": {"_uuid": "3eb37438c50278bf6c6424cbde16a31cd7317f63", "_cell_guid": "e45277d4-359a-4b8f-b56a-0149afb62a54"}, "source": "## Cleaning the Data", "outputs": [], "cell_type": "markdown", "execution_count": null}, {"metadata": {"_uuid": "e30adf802be5747e46d970c63d91736142f6e31d", "collapsed": true, "trusted": false, "_cell_guid": "d93a53fa-773e-407b-b0a1-40e037a8e647"}, "source": "# drop feature since it will not be used to make any predictions.\n# it is not included in the test dataframe\ntrain = train.drop('dropoff_datetime',1)", "outputs": [], "cell_type": "code", "execution_count": 138}, {"metadata": {"_uuid": "74b9847d4e540256ca99252fc3165af448cc06eb", "collapsed": false, "trusted": false, "_cell_guid": "0f1df4a8-40bc-4e1d-b4aa-ef41561c7798"}, "source": "# Some of the journeys are very long\ntrain.trip_duration.describe()", "outputs": [], "cell_type": "code", "execution_count": 139}, {"metadata": {"_uuid": "3700887886702bc61439ac0bf618bb71fe287768", "collapsed": false, "trusted": false, "_cell_guid": "6f85db53-936e-4e74-80cc-cc953e4c1719"}, "source": "# Values are in minutes\nprint(np.percentile(train.trip_duration, 99)/60)\nprint(np.percentile(train.trip_duration, 99.5)/60)\nprint(np.percentile(train.trip_duration, 99.6)/60)\nprint(np.percentile(train.trip_duration, 99.8)/60)\nprint(np.percentile(train.trip_duration, 99.85)/60)\nprint(np.percentile(train.trip_duration, 99.9)/60)\nprint(np.percentile(train.trip_duration, 99.99)/60)\nprint(np.percentile(train.trip_duration, 99.999)/60)\nprint(np.percentile(train.trip_duration, 99.9999)/60)\nprint(train.trip_duration.max() / 60)", "outputs": [], "cell_type": "code", "execution_count": 140}, {"metadata": {"_uuid": "8f078f8b5544c08fd333eca27cf97769e297136a", "collapsed": false, "trusted": false, "_cell_guid": "8af95f7a-f16a-4e39-9c9f-b6945ffe6bea"}, "source": "# Check how many trips remain with each limit\nprint(len(train[train.trip_duration <= np.percentile(train.trip_duration, 99.9)]))\nprint(len(train[train.trip_duration <= np.percentile(train.trip_duration, 99.99)]))\nprint(len(train[train.trip_duration <= np.percentile(train.trip_duration, 99.999)]))", "outputs": [], "cell_type": "code", "execution_count": 141}, {"metadata": {"_uuid": "1c6f2b323155f0485d3c6377fd302ea5ed7102ba", "collapsed": false, "trusted": false, "_cell_guid": "c9d4c1dd-8986-44d2-92ed-eeb7c3020b48"}, "source": "# Remove outliers\ntrain = train[train.trip_duration <= np.percentile(train.trip_duration, 99.999)]", "outputs": [], "cell_type": "code", "execution_count": 142}, {"metadata": {"_uuid": "1b31da2041a44dbacee7336c9585f731c9b03f3a", "collapsed": false, "trusted": false, "_cell_guid": "b82b5fdd-b00b-4d73-a0ce-cdb620426b3a"}, "source": "# Plot locations - look for outliers\nn = 100000 # number of data points to display\n\nf, (ax1, ax2) = plt.subplots(1, 2, sharey=True, figsize=(10, 5))\nax1.scatter(train.pickup_longitude[:n], \n            train.pickup_latitude[:n],\n            alpha = 0.1)\nax1.set_title('Pickup')\nax2.scatter(train.dropoff_longitude[:n], \n            train.dropoff_latitude[:n],\n            alpha = 0.1)\nax2.set_title('Dropoff')", "outputs": [], "cell_type": "code", "execution_count": 143}, {"metadata": {"_uuid": "c574fd93b404c6a8aa3ba64cf79d3c08d94156bc", "collapsed": false, "trusted": false, "_cell_guid": "b31fadb1-3d2c-45ab-91a4-c541c76c61b9"}, "source": "# The values are not too wild, but we'll trim them back a little to be conservative\nprint(train.pickup_latitude.max())\nprint(train.pickup_latitude.min())\nprint(train.pickup_longitude.max())\nprint(train.pickup_longitude.min())\nprint()\nprint(train.dropoff_latitude.max())\nprint(train.dropoff_latitude.min())\nprint(train.dropoff_longitude.max())\nprint(train.dropoff_longitude.min())", "outputs": [], "cell_type": "code", "execution_count": 144}, {"metadata": {"_uuid": "331c5a8d0a64ef6d2806ba4c2a04ceedb209c55c", "collapsed": false, "trusted": false, "_cell_guid": "6c3e7acf-af07-4842-825b-de231c2a5cb7"}, "source": "# Find limits of location\nmax_value = 99.999\nmin_value = 0.001\n\nmax_pickup_lat = np.percentile(train.pickup_latitude, max_value)\nmin_pickup_lat = np.percentile(train.pickup_latitude, min_value)\nmax_pickup_long = np.percentile(train.pickup_longitude, max_value)\nmin_pickup_long = np.percentile(train.pickup_longitude, min_value)\n\nmax_dropoff_lat = np.percentile(train.dropoff_latitude, max_value)\nmin_dropoff_lat = np.percentile(train.dropoff_latitude, min_value)\nmax_dropoff_long = np.percentile(train.dropoff_longitude, max_value)\nmin_dropoff_long = np.percentile(train.dropoff_longitude, min_value)", "outputs": [], "cell_type": "code", "execution_count": 145}, {"metadata": {"_uuid": "2b7c79449dba883807822b649e1d9c4275474f5b", "collapsed": false, "trusted": false, "_cell_guid": "9313e9f7-1210-4958-a671-b412293d22b4"}, "source": "# Remove extreme values\ntrain = train[(train.pickup_latitude <= max_pickup_lat) & (train.pickup_latitude >= min_pickup_lat)]\ntrain = train[(train.pickup_longitude <= max_pickup_long) & (train.pickup_longitude >= min_pickup_long)]\n\ntrain = train[(train.dropoff_latitude <= max_dropoff_lat) & (train.dropoff_latitude >= min_dropoff_lat)]\ntrain = train[(train.dropoff_longitude <= max_dropoff_long) & (train.dropoff_longitude >= min_dropoff_long)]", "outputs": [], "cell_type": "code", "execution_count": 146}, {"metadata": {"_uuid": "bf4d6a72158b83efddd4e3c1841e7d5148382523", "collapsed": false, "trusted": false, "_cell_guid": "a503e4cc-fb37-4965-a7bf-c9d9350d790a"}, "source": "# Replot to see the differences - minimal, but there is some change\nf, (ax1, ax2) = plt.subplots(1, 2, sharey=True, figsize=(10, 5))\nax1.scatter(train.pickup_longitude[:n], \n            train.pickup_latitude[:n],\n            alpha = 0.1)\nax1.set_title('Pickup')\nax2.scatter(train.dropoff_longitude[:n], \n            train.dropoff_latitude[:n],\n            alpha = 0.1)\nax2.set_title('Dropoff')", "outputs": [], "cell_type": "code", "execution_count": 147}, {"metadata": {"_uuid": "8028bed4d7f8ce6e94b5ce0dc51c5278e6d15b97", "collapsed": true, "trusted": false, "_cell_guid": "5ea9c0e2-5847-49d0-8bf0-4f3ed9d69d71"}, "source": "# Concatenate the datasets for feature engineering\ndf = pd.concat([train,test])", "outputs": [], "cell_type": "code", "execution_count": 148}, {"metadata": {"_uuid": "b44ae36324a239595aa626e4465a28a40ace948f", "collapsed": false, "trusted": false, "_cell_guid": "9b29bd14-8096-460d-bd09-0d282ffea55b"}, "source": "df.shape", "outputs": [], "cell_type": "code", "execution_count": 149}, {"metadata": {"_uuid": "34c773c19f20e977404ba46dc619a9085da469bf", "collapsed": false, "trusted": false, "_cell_guid": "6ca7fbe5-1322-4765-9c88-b09047ef72bf"}, "source": "# Check for null values\n# trip_duration nulls to due to them not being present in the test set\ndf.isnull().sum()", "outputs": [], "cell_type": "code", "execution_count": 150}, {"metadata": {"_uuid": "d52a837ac24a8e3778cbde6cb9405691606755d1", "collapsed": false, "trusted": false, "_cell_guid": "5cfaf55b-eeba-49bd-8f7f-ac8c08d6768e"}, "source": "df.vendor_id.value_counts()", "outputs": [], "cell_type": "code", "execution_count": 151}, {"metadata": {"_uuid": "719116efa56eb09149b396d2610f222b1fafe403", "collapsed": false, "trusted": false, "_cell_guid": "552eeaa4-8a57-45bc-a482-8b4059ffc5c1"}, "source": "print(train.pickup_datetime.max())\nprint(train.pickup_datetime.min())\nprint()\nprint(test.pickup_datetime.max())\nprint(test.pickup_datetime.min())\nprint()\nprint(df.pickup_datetime.max())\nprint(df.pickup_datetime.min())", "outputs": [], "cell_type": "code", "execution_count": 153}, {"metadata": {"_uuid": "5964e46cc1043937673e989e24349e41c16124bc", "collapsed": true, "trusted": false, "_cell_guid": "33602fdb-8a55-4d3b-84a1-1da057f0bd8d"}, "source": "# Convert to datetime\ndf.pickup_datetime = pd.to_datetime(df.pickup_datetime)", "outputs": [], "cell_type": "code", "execution_count": 154}, {"metadata": {"_uuid": "2054fb5dce37e010ddff836cfa889bc42273bcd5", "collapsed": true, "trusted": false, "_cell_guid": "d0352c81-64f8-45e2-b1c2-a533bd22034a"}, "source": "# Calculate what minute in a day the pickup is at\ndf['pickup_minute_of_the_day'] = df.pickup_datetime.dt.hour*60 + df.pickup_datetime.dt.minute", "outputs": [], "cell_type": "code", "execution_count": 155}, {"metadata": {"_uuid": "34ae949dbbd3dc3fed3fc292996a5512bc9f787b", "collapsed": false, "trusted": false, "_cell_guid": "7a977aec-930a-4304-9fee-f0ef9ad7ec67"}, "source": "# Rather than use the standard 24 hours, group the trips into 24 groups that are sorted by KMeans\n# This should help 'rush-hour' rides to be in the same groups\nkmeans_pickup_time = KMeans(n_clusters=24, random_state=2).fit(df.pickup_minute_of_the_day[:500000].values.reshape(-1,1))", "outputs": [], "cell_type": "code", "execution_count": 156}, {"metadata": {"_uuid": "c16c282dc542294a254dc2529b85ecd1cde9770b", "collapsed": false, "trusted": false, "_cell_guid": "1cf05cda-9011-47cc-8351-104c4066a2e7"}, "source": "df['kmeans_pickup_time'] = kmeans_pickup_time.predict(df.pickup_minute_of_the_day.values.reshape(-1,1))", "outputs": [], "cell_type": "code", "execution_count": 157}, {"metadata": {"_uuid": "18e647a79a0c66fa31ce804730172e36d2e83b27", "collapsed": false, "trusted": false, "_cell_guid": "2b0a5b59-a1a8-4a3d-8f68-5d4d024981dc"}, "source": "# Compare the distribution of kmeans_pickup_time and the standard 24 hour breakdown\nn = 50000 # number of data points to plot\nf, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(10, 5))\n\nax1.scatter(x = df.pickup_minute_of_the_day[:n]/60, \n            y = np.random.uniform(0,1, n), \n            cmap = 'Set1',\n            c = df.kmeans_pickup_time[:n])\nax1.set_title('KMeans Pickup Time')\n\nax2.scatter(x = df.pickup_minute_of_the_day[:n]/60, \n            y = np.random.uniform(0,1, n), \n            cmap = 'Set1',\n            c = df.pickup_datetime.dt.hour[:n])\nax2.set_title('Pickup Hour')", "outputs": [], "cell_type": "code", "execution_count": 158}, {"metadata": {"_uuid": "c7333b03cd881305d18fb283784a3c2bc5c3889a", "collapsed": false, "trusted": false, "_cell_guid": "bc75a6e6-be67-459c-9d00-0ada5dcd4315"}, "source": "# Load a list of holidays in the US\ncalendar = USFederalHolidayCalendar()\nholidays = calendar.holidays()\n\n# Load business days\nus_bd = CustomBusinessDay(calendar = USFederalHolidayCalendar())\n# Set business_days equal to the work days in our date range.\nbusiness_days = pd.DatetimeIndex(start = df.pickup_datetime.min(), \n                                 end = df.pickup_datetime.max(), \n                                 freq = us_bd)\nbusiness_days = pd.to_datetime(business_days).date", "outputs": [], "cell_type": "code", "execution_count": 159}, {"metadata": {"_uuid": "991166a1bac8ef562fdd9985aa2ebce870a204ca", "collapsed": false, "trusted": false, "_cell_guid": "8460667e-aa8d-463f-b28c-782ed3d7cd12"}, "source": "# Create features relating to time\ndf['pickup_month'] = df.pickup_datetime.dt.month\ndf['pickup_weekday'] = df.pickup_datetime.dt.weekday\ndf['pickup_is_weekend'] = df.pickup_weekday.map(lambda x: 1 if x >= 5 else 0)\ndf['pickup_holiday'] = pd.to_datetime(df.pickup_datetime.dt.date).isin(holidays)\ndf['pickup_holiday'] = df.pickup_holiday.map(lambda x: 1 if x == True else 0)\n\n# If day is before or after a holiday\ndf['pickup_near_holiday'] = (pd.to_datetime(df.pickup_datetime.dt.date).isin(holidays + timedelta(days=1)) |\n                             pd.to_datetime(df.pickup_datetime.dt.date).isin(holidays - timedelta(days=1)))\ndf['pickup_near_holiday'] = df.pickup_near_holiday.map(lambda x: 1 if x == True else 0)\ndf['pickup_businessday'] = pd.to_datetime(df.pickup_datetime.dt.date).isin(business_days)\ndf['pickup_businessday'] = df.pickup_businessday.map(lambda x: 1 if x == True else 0)\n\n# Calculates what minute of the week it is\ndf['week_delta'] = (df.pickup_weekday + ((df.pickup_datetime.dt.hour + \n                                              (df.pickup_datetime.dt.minute / 60.0)) / 24.0))", "outputs": [], "cell_type": "code", "execution_count": 160}, {"metadata": {"_uuid": "19b96436c0beb11c89ef778c94f0156e4118ac7a", "collapsed": false, "trusted": false, "_cell_guid": "c1345d35-4fef-45bd-901f-8bffc53357e7"}, "source": "# Determines number of rides that occur during each specific time\n# Should help to determine traffic\nride_counts = df.groupby(['pickup_month', 'pickup_weekday','pickup_holiday','pickup_near_holiday',\n            'pickup_businessday','kmeans_pickup_time']).size()\nride_counts = pd.DataFrame(ride_counts).reset_index()\nride_counts['ride_counts'] = ride_counts[0]\nride_counts = ride_counts.drop(0,1)\n\n# Add `ride_counts` to dataframe\ndf = df.merge(ride_counts, on=['pickup_month',\n                          'pickup_weekday',\n                          'pickup_holiday',\n                          'pickup_near_holiday',\n                          'pickup_businessday',\n                          'kmeans_pickup_time'], how='left')", "outputs": [], "cell_type": "code", "execution_count": 161}, {"metadata": {"_uuid": "8c7ce84a5bd54b3e5ff04701b77e6d48b2e14fbc", "collapsed": true, "trusted": false, "_cell_guid": "6dd33a9d-3a41-421d-8163-9c9f5abe23c8"}, "source": "# Dont' need this feature any more\ndf = df.drop('pickup_datetime', 1)", "outputs": [], "cell_type": "code", "execution_count": 163}, {"metadata": {"_uuid": "2c5e1c85826cb085b366fd023ca98e0897d0adaa", "collapsed": false, "trusted": false, "_cell_guid": "db0b9f8a-ddd5-48f0-938b-03911ccf996a", "scrolled": true}, "source": "# Group pickup and dropoff locations into 15 groups\nkmeans_pickup = KMeans(n_clusters=15, random_state=2).fit(df[['pickup_latitude','pickup_longitude']][:500000])\nkmeans_dropoff = KMeans(n_clusters=15, random_state=2).fit(df[['dropoff_latitude','dropoff_longitude']][:500000])\n\ndf['kmeans_pickup'] = kmeans_pickup.predict(df[['pickup_latitude','pickup_longitude']])\ndf['kmeans_dropoff'] = kmeans_dropoff.predict(df[['dropoff_latitude','dropoff_longitude']])", "outputs": [], "cell_type": "code", "execution_count": 164}, {"metadata": {"_uuid": "2d4acbae7b2ccb7139562544faa8beca11964ca5", "collapsed": false, "trusted": false, "_cell_guid": "241edf84-0aea-41a6-bec5-124fbd9ef4ee"}, "source": "# Plot these 15 groups\n\nn = 100000 # Number of data points to plot\nf, (ax1, ax2) = plt.subplots(1, 2, sharey=True, figsize=(10, 5))\nax1.scatter(df.pickup_longitude[:n], \n            df.pickup_latitude[:n],\n            cmap = 'viridis',\n            c = df.kmeans_pickup[:n])\nax1.set_title('Pickup')\nax2.scatter(df.dropoff_longitude[:n], \n            df.dropoff_latitude[:n],\n            cmap = 'viridis',\n            c = df.kmeans_dropoff[:n])\nax2.set_title('Dropoff')", "outputs": [], "cell_type": "code", "execution_count": 166}, {"metadata": {"_uuid": "06c211db4cc9087207af48d56b463f393409d3ef", "collapsed": false, "trusted": false, "_cell_guid": "c7d625f6-8cdb-4f88-af92-f234742f55da"}, "source": "# Reduce pickup and dropoff locations to one value\npca = PCA(n_components=1)\ndf['pickup_pca'] = pca.fit_transform(df[['pickup_latitude','pickup_longitude']])\ndf['dropoff_pca'] = pca.fit_transform(df[['dropoff_latitude','dropoff_longitude']])", "outputs": [], "cell_type": "code", "execution_count": 168}, {"metadata": {"_uuid": "4fd6f1bf4e215ee90c67c7b519311c9c621e33f1", "collapsed": false, "trusted": false, "_cell_guid": "662fa476-097d-4090-8e35-235148a78a9e"}, "source": "# Create distance features\ndf['distance'] = np.sqrt(np.power(df['dropoff_longitude'] - df['pickup_longitude'], 2) + \n                         np.power(df['dropoff_latitude'] - df['pickup_latitude'], 2))\ndf['haversine_distance'] = df.apply(lambda r: haversine.haversine((r['pickup_latitude'],r['pickup_longitude']),\n                                                                  (r['dropoff_latitude'], r['dropoff_longitude'])), \n                           axis=1)\ndf['manhattan_distance'] = (abs(df.dropoff_longitude - df.pickup_longitude) +\n                            abs(df.dropoff_latitude - df.pickup_latitude))\ndf['log_distance'] = np.log(df['distance'] + 1)\ndf['log_haversine_distance'] = np.log(df['haversine_distance'] + 1)\ndf['log_manhattan_distance'] = np.log(df.manhattan_distance + 1)", "outputs": [], "cell_type": "code", "execution_count": 167}, {"metadata": {"_uuid": "8184ff84aa5050be8365570725ec366f789a7b9f", "collapsed": true, "trusted": false, "_cell_guid": "09b1b18e-c649-4ee6-af20-ac3e392256f3"}, "source": "def calculate_bearing(pickup_lat, pickup_long, dropoff_lat, dropoff_long):\n    '''Calculate the direction of travel in degrees'''\n    pickup_lat_rads = np.radians(pickup_lat)\n    pickup_long_rads = np.radians(pickup_long)\n    dropoff_lat_rads = np.radians(dropoff_lat)\n    dropoff_long_rads = np.radians(dropoff_long)\n    long_delta_rads = np.radians(dropoff_long_rads - pickup_long_rads)\n    \n    y = np.sin(long_delta_rads) * np.cos(dropoff_lat_rads)\n    x = (np.cos(pickup_lat_rads) * \n         np.sin(dropoff_lat_rads) - \n         np.sin(pickup_lat_rads) * \n         np.cos(dropoff_lat_rads) * \n         np.cos(long_delta_rads))\n    \n    return np.degrees(np.arctan2(y, x))", "outputs": [], "cell_type": "code", "execution_count": 169}, {"metadata": {"_uuid": "3e5bb027624f9a0f8598d4739c74e2c57fd1beb9", "collapsed": false, "trusted": false, "_cell_guid": "ad37fff0-fbee-4e39-91de-cf54719d0f0f"}, "source": "df['bearing'] = calculate_bearing(df.pickup_latitude,\n                                  df.pickup_longitude,\n                                  df.dropoff_latitude,\n                                  df.dropoff_longitude)", "outputs": [], "cell_type": "code", "execution_count": 170}, {"metadata": {"_uuid": "4fe52535c98419f940197ce67666d2dcd3edfbb4", "collapsed": false, "trusted": false, "_cell_guid": "cf90a87c-37cc-4812-881b-dab0edf9035f"}, "source": "df.passenger_count.value_counts()", "outputs": [], "cell_type": "code", "execution_count": 171}, {"metadata": {"_uuid": "b4078c43211e1636f1e54dc38f68087686f92e2c", "collapsed": false, "trusted": false, "_cell_guid": "0a8a5a39-6e3c-4061-916f-9eb48403912f"}, "source": "# Group passenger_count by type of group\ndf['no_passengers'] = df.passenger_count.map(lambda x: 1 if x == 0 else 0)\ndf['one_passenger'] = df.passenger_count.map(lambda x: 1 if x == 1 else 0)\ndf['few_passengers'] = df.passenger_count.map(lambda x: 1 if x > 1 and x <= 4 else 0)\ndf['many_passengers'] = df.passenger_count.map(lambda x: 1 if x >= 5 else 0)", "outputs": [], "cell_type": "code", "execution_count": 172}, {"metadata": {"_uuid": "861a59b18753864c9224d0bea4772fd542685fcf", "collapsed": false, "trusted": false, "_cell_guid": "7af5e562-608e-40d1-8f2f-18053485bb96", "scrolled": true}, "source": "df.store_and_fwd_flag = df.store_and_fwd_flag.map(lambda x: 1 if x == 'Y' else 0)", "outputs": [], "cell_type": "code", "execution_count": 173}, {"metadata": {"_uuid": "9bf3c1f4d4c0b22c59be1b9ecd8daddd3693ad6a", "collapsed": false, "trusted": false, "_cell_guid": "4837157f-2208-4bbe-b8b8-c30fe48c80fd", "scrolled": true}, "source": "# Create dummy features for these features, then drop these features\ndummies = ['kmeans_pickup_time','pickup_month','pickup_weekday','kmeans_pickup','kmeans_dropoff']\nfor feature in dummies:\n    dummy_features = pd.get_dummies(df[feature], prefix=feature)\n    for dummy in dummy_features:\n        df[dummy] = dummy_features[dummy]\n    df = df.drop([feature], 1)", "outputs": [], "cell_type": "code", "execution_count": 174}, {"metadata": {"_uuid": "3e70ea2a08b760c0ba018ac5dd88f9e4b2784ec3", "collapsed": false, "trusted": false, "_cell_guid": "de599d7d-26ab-4971-8ce5-586f74bcb378"}, "source": "# Check that all features look okay\ndf.head()", "outputs": [], "cell_type": "code", "execution_count": 175}, {"metadata": {"_uuid": "de6dc3a2bc5a0bdcb1c1a54c8579f73c261a4bae", "collapsed": false, "trusted": false, "_cell_guid": "37964642-85d1-4629-870f-a2fc715ca084"}, "source": "# Don't need this feature any more\ndf = df.drop(['id'],1)", "outputs": [], "cell_type": "code", "execution_count": 176}, {"metadata": {"_uuid": "8433e8c62a366a9c73b251952ad0234f69ff690a", "collapsed": false, "trusted": false, "_cell_guid": "188974a6-1e38-4daa-9d4c-ef7614be885f", "scrolled": true}, "source": "# Transform each feature to have a mean of 0 and standard deviation of 1\n# Help to train the neural network\nfor feature in df:\n    if feature == 'trip_duration':\n        continue\n    mean, std = df[feature].mean(), df[feature].std()\n    df.loc[:, feature] = (df[feature] - mean)/std", "outputs": [], "cell_type": "code", "execution_count": 177}, {"metadata": {"_uuid": "559c84ae591891602f4e4602f620d7f034090836", "collapsed": false, "trusted": false, "_cell_guid": "3c00c27b-ef1b-4e35-9a24-88f64e8bcfce"}, "source": "# Check that the transformation was carried out correctly\ndf.head()", "outputs": [], "cell_type": "code", "execution_count": 178}, {"metadata": {"_uuid": "1cd46c3c4cae0dfedcf6e4f83e0c576cba78faf6", "collapsed": false, "trusted": false, "_cell_guid": "2442c7e5-630b-4215-8d1d-cc88d13c96fe"}, "source": "# Return data into a training and testing set\ntrainFinal = df[:-len(test)]\ntestFinal = df[-len(test):]", "outputs": [], "cell_type": "code", "execution_count": 179}, {"metadata": {"_uuid": "5c01e83d28069c106c57429c5be9b25465d1ad74", "collapsed": false, "trusted": false, "_cell_guid": "0160a82a-10e3-4b47-8d68-d168eac3ddd5"}, "source": "# Check lengths of dataframes\nprint(len(trainFinal))\nprint(len(testFinal))\nprint(len(test))", "outputs": [], "cell_type": "code", "execution_count": 180}, {"metadata": {"_uuid": "b0dd2a3d09f34eb80a80c461c5ce7c41d9472258", "collapsed": false, "trusted": false, "_cell_guid": "ba04fafd-38ab-42f3-b027-da733d994f15"}, "source": "# Give trip_duration its own dataframe\n# Drop it from the other dataframes\nyFinal = pd.DataFrame(trainFinal.trip_duration)\ntrainFinal = trainFinal.drop('trip_duration',1)\ntestFinal = testFinal.drop('trip_duration',1)", "outputs": [], "cell_type": "code", "execution_count": 181}, {"metadata": {"_uuid": "f99625c01c364e04a19ee913ff7cc601312521db", "collapsed": true, "trusted": false, "_cell_guid": "efcfdefd-9ba9-4d55-a8db-f840a0d33882"}, "source": "# Sort data into training and testing sets\nx_trainFinal, x_testFinal, y_trainFinal, y_testFinal = train_test_split(trainFinal, \n                                                                        np.log(yFinal+1), \n                                                                        test_size=0.15, \n                                                                        random_state=2)\n\nx_train, x_test, y_train, y_test = train_test_split(x_trainFinal, \n                                                    y_trainFinal, \n                                                    test_size=0.15,\n                                                    random_state=2)", "outputs": [], "cell_type": "code", "execution_count": 182}, {"metadata": {"_uuid": "cbe44e285feab90597e5b4151abd902b17055558", "_cell_guid": "4139a712-19aa-438f-af97-3ee4719b6837"}, "source": "## Build the Neural Network", "outputs": [], "cell_type": "markdown", "execution_count": null}, {"metadata": {"_uuid": "bd7bc93d87ef603f41443dd85af5a721058acb49", "collapsed": true, "trusted": false, "_cell_guid": "f8057f40-aded-468c-9983-8ebc34e20ea3"}, "source": "def create_weights_biases(num_layers, n_inputs, multiplier, max_nodes):\n    '''Use the inputs to create the weights and biases for a network'''\n    \n    # Empty dictionaries to store the weights and biases for each layer\n    weights = {}\n    biases = {}\n    \n    # Create weights and biases for all layers, but the final layer\n    for layer in range(1,num_layers):\n        # The first layer needs to use the number of features that are in the dataframe\n        if layer == 1:\n            weights[\"h\"+str(layer)] = tf.Variable(tf.random_normal([num_features, n_inputs],\n                                                                   stddev=np.sqrt(1/num_features)))\n            biases[\"b\"+str(layer)] = tf.Variable(tf.random_normal([n_inputs],stddev=0))\n            # n_previous keeps track of the number of nodes in the previous layer\n            n_previous = n_inputs\n            \n        else:    \n            # To alter number of nodes in each layer, multiply n_previous by multiplier \n            n_current = int(n_previous * multiplier)\n            \n            # Limit the number of nodes to the maximum amount\n            if n_current >= max_nodes:\n                n_current = max_nodes\n                \n            weights[\"h\"+str(layer)] = tf.Variable(tf.random_normal([n_previous, n_current],\n                                                                       stddev=np.sqrt(1/n_previous)))\n            biases[\"b\"+str(layer)] = tf.Variable(tf.random_normal([n_current],stddev=0))\n            n_previous = n_current\n            \n    # Create weights for the final layer\n    n_current = int(n_previous * multiplier)\n    if n_current >= max_nodes:\n        n_current = max_nodes\n            \n    # The final layer only has 1 node since this is a regression task\n    weights[\"out\"] = tf.Variable(tf.random_normal([n_previous, 1], stddev=np.sqrt(1/n_previous)))\n    biases[\"out\"] = tf.Variable(tf.random_normal([1],stddev=0))\n                                                    \n    return weights, biases", "outputs": [], "cell_type": "code", "execution_count": 183}, {"metadata": {"_uuid": "6ba94ed979db8d093718010195ec173289d84b98", "collapsed": true, "trusted": false, "_cell_guid": "d5d4cfb5-8df1-45c9-b20d-d01d0e3e1955"}, "source": "def network(num_layers, n_inputs, weights, biases, rate, is_training, activation_function):\n    '''Add the required number of layers to the network'''\n    \n    for layer in range(1, num_layers):\n        if layer == 1:\n            current_layer = eval(activation_function + \"(tf.matmul(n_inputs, weights['h1']) + biases['b1'])\")\n            current_layer = tf.nn.dropout(current_layer, 1-rate)\n            previous_layer = current_layer\n        else:\n            current_layer = eval(activation_function + \"(tf.matmul(previous_layer,\\\n            weights['h'+str(layer)]) + biases['b'+str(layer)])\")\n            current_layer = tf.nn.dropout(current_layer, 1-rate)\n            previous_layer = current_layer\n\n    # Output layer with linear activation - because regression\n    out_layer = tf.matmul(previous_layer, weights['out']) + biases['out']\n    return out_layer", "outputs": [], "cell_type": "code", "execution_count": 184}, {"metadata": {"_uuid": "f167139d1ba20325ccb7bea1fbcb88169a34af99", "collapsed": true, "trusted": false, "_cell_guid": "8b51e9fd-af7f-4416-8181-36d60503b153"}, "source": "def model_inputs():\n    '''Create placeholders for model's inputs '''\n    \n    inputs = tf.placeholder(tf.float32, [None, None], name='inputs')\n    targets = tf.placeholder(tf.float32, [None, 1], name='targets')\n    learning_rate = tf.placeholder(tf.float32, name='learning_rate')\n    dropout_rate = tf.placeholder(tf.float32, name='dropout_rate')\n    is_training = tf.placeholder(tf.bool, name='is_training')\n    \n    return inputs, targets, learning_rate, dropout_rate, is_training", "outputs": [], "cell_type": "code", "execution_count": 185}, {"metadata": {"_uuid": "d7d7d65011b31547f7fa50f970dd1e43f4d1c829", "collapsed": true, "trusted": false, "_cell_guid": "82625d0d-0be4-4400-a9df-6fff16446a3f"}, "source": "def build_graph(num_layers,n_inputs,weights_multiplier,dropout_rate,learning_rate,max_nodes,activation_function):\n    '''Use inputs to build the graph and export the required features for training'''\n    \n    # Reset the graph to ensure it is ready for training\n    tf.reset_default_graph()\n    \n    # Get the inputs\n    inputs, targets, learning_rate, dropout_rate, is_training = model_inputs()\n    \n    # Get the weights and biases\n    weights, biases = create_weights_biases(num_layers, n_inputs, weights_multiplier, max_nodes)\n    \n    # Construct the network\n    preds = network(num_layers, inputs, weights, biases, dropout_rate, is_training, activation_function)    \n            \n    with tf.name_scope(\"cost\"):\n        # Cost function\n        cost = tf.sqrt(tf.losses.mean_squared_error(labels=targets, predictions=preds))\n        tf.summary.scalar('cost', cost)\n\n    with tf.name_scope(\"optimze\"):\n        optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n\n    # Merge all of the summaries\n    merged = tf.summary.merge_all()    \n\n    # Export the nodes \n    export_nodes = ['inputs','targets','dropout_rate','is_training','cost','preds','merged',\n                    'optimizer','learning_rate']\n    Graph = namedtuple('Graph', export_nodes)\n    local_dict = locals()\n    graph = Graph(*[local_dict[each] for each in export_nodes])\n\n    return graph", "outputs": [], "cell_type": "code", "execution_count": 186}, {"metadata": {"_uuid": "16ced33adc67028b09ddc711543a40d291122b5d", "_cell_guid": "dcc472e5-ae74-4376-a626-7d82c27246d1"}, "source": "## Training the Neural Network", "outputs": [], "cell_type": "markdown", "execution_count": null}, {"metadata": {"_uuid": "d2ca8fbdaaa521c790dd1f5ced57e6c754771b4d", "collapsed": true, "trusted": false, "_cell_guid": "54ed6b67-a973-4e4b-9ab3-8d3635e90823"}, "source": "def train(model, epochs, log_string, learning_rate):\n    '''Train the Network and return the average RMSE for each iteration of the model'''\n    \n    with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())\n\n        # Used to determine when to stop the training early\n        testing_loss_summary = []\n\n        iteration = 0 # Keep track of which batch iteration is being trained\n        stop_early = 0 # Keep track of how many consective epochs have not achieved a record low RMSE\n        stop = 5 # If the batch_loss_testing does not decrease in 5 consecutive epochs, stop training\n        per_epoch_training = 2 # Check training progress 2 times per epcoh\n        per_epoch_testing = 1 # Check testing progress 1 time per epoch\n        \n        # Decay learning rate after consective epochs of no improvements\n        learning_rate_decay_threshold = np.random.choice([2,3]) \n        original_learning_rate = learning_rate # Keep track of orginial learning rate for each split\n\n        print()\n        print(\"Training Model: {}\".format(log_string))\n\n        # Record progress to view with TensorBoard\n        train_writer = tf.summary.FileWriter('./logs/1/train/{}'.format(log_string), sess.graph)\n        test_writer = tf.summary.FileWriter('./logs/1/test/{}'.format(log_string))\n        \n        training_check = (len(x_train)//batch_size//per_epoch_training)-1 # Check training progress after this many batches\n        testing_check = (len(x_train)//batch_size//per_epoch_testing)-1 # Check testing results after this many batches\n\n        for epoch_i in range(1, epochs+1): \n            batch_loss = 0\n            batch_time = 0\n\n            for batch in range(int(len(x_train)/batch_size)):\n                batch_x = x_train[batch*batch_size:(1+batch)*batch_size]\n                batch_y = y_train[batch*batch_size:(1+batch)*batch_size]\n\n                start_time = time.time()\n\n                summary, loss, _ = sess.run([model.merged,\n                                             model.cost, \n                                             model.optimizer], \n                                             {model.inputs: batch_x,\n                                              model.targets: batch_y,\n                                              model.learning_rate: learning_rate,\n                                              model.dropout_rate: dropout_rate,\n                                              model.is_training: True})\n\n\n                batch_loss += loss\n                end_time = time.time()\n                batch_time += end_time - start_time\n\n                # Record the progress of training\n                train_writer.add_summary(summary, iteration)\n\n                iteration += 1\n\n                if batch % training_check == 0 and batch > 0:\n                    print('Epoch {:>3}/{} Batch {:>4}/{} - RMSE: {:>6.3f}, Seconds: {:>4.2f}'\n                          .format(epoch_i,\n                                  epochs, \n                                  batch, \n                                  len(x_train) // batch_size, \n                                  (batch_loss / training_check), \n                                  batch_time))\n                    batch_loss = 0\n                    batch_time = 0\n\n                #### Testing ####\n                if batch % testing_check == 0 and batch > 0:\n                    batch_loss_testing = 0\n                    batch_time_testing = 0\n                    for batch in range(int(len(x_test)/batch_size)):\n                        batch_x = x_test[batch*batch_size:(1+batch)*batch_size]\n                        batch_y = y_test[batch*batch_size:(1+batch)*batch_size]\n\n                        start_time_testing = time.time()\n                        summary, loss = sess.run([model.merged,\n                                                  model.cost], \n                                                     {model.inputs: batch_x,\n                                                      model.targets: batch_y,\n                                                      model.learning_rate: learning_rate,\n                                                      model.dropout_rate: 0,\n                                                      model.is_training: False})\n\n                        batch_loss_testing += loss\n                        end_time_testing = time.time()\n                        batch_time_testing += end_time_testing - start_time_testing\n\n                        # Record the progress of testing\n                        test_writer.add_summary(summary, iteration)\n\n                    n_batches_testing = batch + 1\n                    print('Testing RMSE: {:>6.3f}, Seconds: {:>4.2f}'\n                          .format(batch_loss_testing / n_batches_testing, \n                                  batch_time_testing))\n\n                    batch_time_testing = 0\n\n                    # If the batch_loss_testing is at a new minimum, save the model\n                    testing_loss_summary.append(batch_loss_testing)\n                    if batch_loss_testing <= min(testing_loss_summary):\n                        print('New Record!') \n                        lowest_loss_testing = batch_loss_testing/n_batches_testing\n                        stop_early = 0 # Reset stop_early if new minimum loss is found\n                        checkpoint = \"./{}.ckpt\".format(log_string)\n                        saver = tf.train.Saver()\n                        saver.save(sess, checkpoint)\n\n                    else:\n                        print(\"No Improvement.\")\n                        stop_early += 1 # Increase stop_early if no new minimum loss is found\n                        if stop_early % learning_rate_decay_threshold == 0:\n                            learning_rate *= learning_rate_decay\n                            print(\"New learning rate = \", learning_rate)\n                        elif stop_early == stop:\n                            break\n\n            if stop_early == stop:\n                print(\"Stopping training for this iteration.\")\n                print(\"Lowest RMSE =\", lowest_loss_testing)\n                print()\n                early_stop = 0\n                testing_loss_summary = []\n                break\n        \n    return lowest_loss_testing", "outputs": [], "cell_type": "code", "execution_count": 187}, {"metadata": {"_uuid": "6463c112e5cafb4752457e38d63ed049fb308771", "collapsed": false, "trusted": false, "_cell_guid": "953eeabb-71c7-4d79-95da-7d0a6ef51fe0", "scrolled": true}, "source": "# Use random search to choose the values for each iteration\n\nnum_iterations = 1 # Changed from 15 to 1 to save time when uploading to Kaggle\nresults = {} # Save the log_string and RMSE of each iteration\nfor i in range(num_iterations):\n    # (Randomly) choose the value for each input\n    num_features = x_train.shape[1]\n    epochs = 1 # Changed from 50 to 1 to save time when uploading to Kaggle\n    learning_rate = np.random.uniform(0.001, 0.1)\n    learning_rate_decay = np.random.uniform(0.1,0.5)\n    weights_multiplier = np.random.uniform(0.5,2)\n    n_inputs = np.random.randint(int(num_features)*0.1,int(num_features)*2)\n    num_layers = np.random.choice([2,3,4])\n    dropout_rate = np.random.uniform(0,0.3)\n    batch_size = np.random.choice([256,512,1024])\n    max_nodes = np.random.randint(16, 512)\n    activation_function = np.random.choice(['tf.nn.sigmoid',\n                                            'tf.nn.relu',\n                                            'tf.nn.elu'])\n\n    print(\"Starting iteration #\",i+1)\n    log_string = 'LR={},LRD={},WM={},NI={},NL={},DR={},BS={},MN={},AF={}'.format(learning_rate,\n                                                                                 learning_rate_decay,\n                                                                                 weights_multiplier,\n                                                                                 n_inputs,\n                                                                                 num_layers,\n                                                                                 dropout_rate,\n                                                                                 batch_size,\n                                                                                 max_nodes,\n                                                                                 activation_function) \n    \n    model = build_graph(num_layers, n_inputs, weights_multiplier, \n                        dropout_rate,learning_rate,max_nodes,activation_function)\n    result = train(model, epochs, log_string, learning_rate)\n    results[log_string] = result", "outputs": [], "cell_type": "code", "execution_count": 188}, {"metadata": {"_uuid": "bdd9eb5f04fbbdfafd014e505965885d61092206", "collapsed": true, "trusted": false, "_cell_guid": "2ec01bbf-7f1a-4856-bef8-e1533ca6a95f"}, "source": "def find_inputs(model):\n    '''Use the log_string from the model to extract the values for all of the model's inputs'''\n    \n    learning_rate_start = model.find('LR=') + 3\n    learning_rate_end = model.find(',LRD', learning_rate_start)\n    learning_rate = float(model[learning_rate_start:learning_rate_end])\n    \n    learning_rate_decay_start = model.find('LRD=') + 4\n    learning_rate_decay_end = model.find(',WM', learning_rate_decay_start)\n    learning_rate_decay = float(model[learning_rate_decay_start:learning_rate_decay_end])\n    \n    weights_multiplier_start = model.find('WM=') + 3\n    weights_multiplier_end = model.find(',NI', weights_multiplier_start)\n    weights_multiplier = float(model[weights_multiplier_start:weights_multiplier_end])\n    \n    n_inputs_start = model.find('NI=') + 3\n    n_inputs_end = model.find(',NL', n_inputs_start)\n    n_inputs = int(model[n_inputs_start:n_inputs_end])\n    \n    num_layers_start = model.find('NL=') + 3\n    num_layers_end = model.find(',DR', num_layers_start)\n    num_layers = int(model[num_layers_start:num_layers_end])\n    \n    dropout_rate_start = model.find('DR=') + 3\n    dropout_rate_end = model.find(',BS', dropout_rate_start)\n    dropout_rate = float(model[dropout_rate_start:dropout_rate_end])\n    \n    batch_size_start = model.find('BS=') + 3\n    batch_size_end = model.find(',MN', batch_size_start)\n    batch_size = int(model[batch_size_start:batch_size_end])\n    \n    max_nodes_start = model.find('MN=') + 3\n    max_nodes_end = model.find(',AF', max_nodes_start)\n    max_nodes = int(model[max_nodes_start:max_nodes_end])\n    \n    activation_function_start = model.find('AF=') + 3\n    activation_function = str(model[activation_function_start:])\n    \n    return (learning_rate, learning_rate_decay, weights_multiplier, n_inputs,\n            num_layers, dropout_rate, batch_size, max_nodes, activation_function)", "outputs": [], "cell_type": "code", "execution_count": 189}, {"metadata": {"_uuid": "2e6dfa740667edeb86b418b4bcbb83d078aca19c", "collapsed": false, "trusted": false, "_cell_guid": "a5add3b6-5141-463e-8706-a39f86feab88", "scrolled": true}, "source": "# Sort results by RMSE (lowest - highest)\nsorted_results_nn = sorted(results.items(), key=operator.itemgetter(1))", "outputs": [], "cell_type": "code", "execution_count": 190}, {"metadata": {"_uuid": "08a929e99bebae1fe3dbba5a4559a3be1a5bc1bb", "collapsed": true, "trusted": false, "_cell_guid": "f3a5b417-e4fe-41d0-9a76-694285480c8b"}, "source": "# Create an empty dataframe to contain all of the inputs for each iteration of the model\nresults_nn = pd.DataFrame(columns=[\"learning_rate\", \n                                   \"learning_rate_decay\", \n                                   \"weights_multiplier\", \n                                   \"n_inputs\",\n                                   \"num_layers\", \n                                   \"dropout_rate\", \n                                   \"batch_size\", \n                                   \"max_nodes\", \n                                   \"activation_function\"])\n\nfor result in sorted_results_nn:\n    # Find the input values for each iteration\n    learning_rate, learning_rate_decay, weights_multiplier, n_inputs,\\\n        num_layers, dropout_rate, batch_size, max_nodes, activation_function = find_inputs(result[0])\n    \n    # Find the Mean Squared Error for each iteration\n    RMSE = result[1]\n    \n    # Create a dataframe with the values above\n    new_row = pd.DataFrame([[RMSE,\n                             learning_rate, \n                             learning_rate_decay, \n                             weights_multiplier, \n                             n_inputs,\n                             num_layers, \n                             dropout_rate, \n                             batch_size, \n                             max_nodes, \n                             activation_function]],\n                     columns = [\"RMSE\",\n                                \"learning_rate\", \n                                \"learning_rate_decay\", \n                                \"weights_multiplier\", \n                                \"n_inputs\",\n                                \"num_layers\", \n                                \"dropout_rate\", \n                                \"batch_size\", \n                                \"max_nodes\", \n                                \"activation_function\"])\n    \n    # Append the dataframe as a new row in results_df\n    results_nn = results_nn.append(new_row, ignore_index=True)", "outputs": [], "cell_type": "code", "execution_count": 191}, {"metadata": {"_uuid": "34f6b90ca10ee763abf266b3e6fa4c7edb47f046", "collapsed": false, "trusted": false, "_cell_guid": "c4c2f7cd-6a98-4ca7-85f8-4916bb4e5ff2"}, "source": "# Look at the top five iterations\nresults_nn.head()", "outputs": [], "cell_type": "code", "execution_count": 192}, {"metadata": {"_uuid": "ade851958db594b3e9f2eedfda15e3a614a542a9", "collapsed": true, "trusted": false, "_cell_guid": "4ac8b9b0-dc76-4529-a958-df8bee8506dd"}, "source": "def make_predictions(data, batch_size):\n    '''\n    Restore a session to make predictions, then return these predictions\n    data: the data that will be used to make predictions.\n    '''\n    with tf.Session() as sess:\n        saver = tf.train.Saver()\n        saver.restore(sess, checkpoint)\n        predictions = [] # record the predictions\n\n        for batch in range(int(len(data)/batch_size)):\n            batch_x = data[batch*batch_size:(1+batch)*batch_size]\n\n            batch_predictions = sess.run([model.preds],\n                                   {model.inputs: batch_x,\n                                    model.learning_rate: learning_rate,\n                                    model.dropout_rate: 0,\n                                    model.is_training: False})\n\n            for prediction in batch_predictions[0]:\n                predictions.append(prediction)\n\n        return predictions", "outputs": [], "cell_type": "code", "execution_count": 193}, {"metadata": {"_uuid": "a570de5c89c1e7d9ca762791056ae50afb81054b", "collapsed": false, "trusted": false, "_cell_guid": "c261fa55-8ec3-4e43-84fd-e87daf93208a", "scrolled": true}, "source": "initial_preds = {} # stores the RMSE and predictions for x_testFinal\nfinal_preds = {} # store the predictions for testFinal, with x_testFinal's RMSE\n\niteration = 1 \n\nfor model, result in sorted_results_nn:\n    checkpoint = str(model) + \".ckpt\" \n    \n    # Aquire the inputs from the log_string\n    _, _, weights_multiplier, n_inputs, num_layers, _, _, max_nodes, activation_function = find_inputs(model)\n    \n    model = build_graph(num_layers,n_inputs,weights_multiplier,dropout_rate,\n                        learning_rate,max_nodes,activation_function)\n    \n    y_preds_nn = make_predictions(x_testFinal, 659)\n    RMSE_nn = np.sqrt(mean_squared_error(y_testFinal, y_preds_nn))\n    print(\"RMSE for iteration #{} is {}.\".format(iteration, RMSE_nn))\n    print()\n    initial_preds[RMSE_nn] = y_preds_nn\n    testFinal_preds_nn = make_predictions(testFinal, 258)\n    final_preds[RMSE_nn] = [testFinal_preds_nn]\n    iteration += 1", "outputs": [], "cell_type": "code", "execution_count": 194}, {"metadata": {"_uuid": "b889b40b411696b4411a02e2fe8ce82a76a1f494", "_cell_guid": "8300b51a-1545-48b8-9c9b-2bb896161aa6"}, "source": "## Training the Other Models", "outputs": [], "cell_type": "markdown", "execution_count": null}, {"metadata": {"_uuid": "4fd11ddea84ab159a35c36f0330af7439f382ece", "collapsed": false, "trusted": false, "_cell_guid": "f14648a6-e4d3-4a4e-b6d8-7ed841b39b6a", "scrolled": true}, "source": "# Create an empty dataframe to contain all of the inputs for each iteration of the model\nresults_rfr = pd.DataFrame(columns=[\"RMSE\",\n                                    \"n_estimators\", \n                                    \"max_depth\", \n                                    \"min_samples_split\"])\n\nfor i in range(num_iterations):\n    # Use random search to choose the inputs' values\n    n_estimators = np.random.randint(1,2) # Changed from (10,20) to (1,2) to save time whne uploading to Kaggle\n    max_depth = np.random.randint(6,12)\n    min_samples_split = np.random.randint(2,50)\n\n    rfr = RFR(n_estimators = n_estimators,\n          max_depth = max_depth,\n          min_samples_split = min_samples_split,\n          verbose = 2,\n          random_state = 2)\n    \n    rfr = rfr.fit(x_train, y_train.values)\n\n    y_preds_rfr = rfr.predict(x_testFinal)\n    RMSE_rfr = np.sqrt(mean_squared_error(y_testFinal, y_preds_rfr))\n    print(\"RMSE for iteration #{} is {}.\".format(i+1, RMSE_rfr))\n    print(\"NE={}, MD={}, MSS={}\".format(n_estimators,\n                                        max_depth,\n                                        min_samples_split))\n    print()\n    initial_preds[RMSE_rfr] = y_preds_rfr\n    testFinal_preds_rfr = rfr.predict(testFinal)\n    final_preds[RMSE_rfr] = [testFinal_preds_rfr]\n    \n    # Create a dataframe with the values above\n    new_row = pd.DataFrame([[RMSE_rfr,\n                             n_estimators, \n                             max_depth, \n                             min_samples_split]],\n                     columns = [\"RMSE\",\n                                \"n_estimators\", \n                                \"max_depth\", \n                                \"min_samples_split\"])\n    \n    # Append the dataframe as a new row in results_df\n    results_rfr = results_rfr.append(new_row, ignore_index=True)", "outputs": [], "cell_type": "code", "execution_count": 195}, {"metadata": {"_uuid": "724b081616fde01cc93d08603ebfcf59ddfcf8a2", "collapsed": false, "trusted": false, "_cell_guid": "901d7318-305b-4542-a0c0-21d290d0dd7b", "scrolled": true}, "source": "# Check the results\nresults_rfr", "outputs": [], "cell_type": "code", "execution_count": 196}, {"metadata": {"_uuid": "a1a54aba6dc6f1b26f478193940159ef821b1303", "collapsed": false, "trusted": false, "_cell_guid": "8e45c157-82f6-484a-95f4-694a99bfd6e3", "scrolled": true}, "source": "# Create an empty dataframe to contain all of the inputs for each iteration of the model\nresults_lgb = pd.DataFrame(columns=[\"RMSE\",\n                                    \"num_leaves\", \n                                    \"max_depth\", \n                                    \"feature_fraction\",\n                                    \"bagging_fraction\",\n                                    \"bagging_freq\",\n                                    \"learning_rate\"])\n\nfor i in range(num_iterations):\n    \n    num_leaves = np.random.randint(100,250)\n    max_depth = np.random.randint(6,12)\n    feature_fraction = np.random.uniform(0.7,1)\n    bagging_fraction = np.random.uniform(0.8,1)\n    bagging_freq = np.random.randint(3,10)\n    learning_rate = np.random.uniform(0.2,1)\n    n_estimators = 100\n    early_stopping_rounds = 5\n\n    gbm = lgb.LGBMRegressor(objective = 'regression',\n                            boosting_type = 'gbdt',\n                            num_leaves = num_leaves,\n                            max_depth = max_depth,\n                            feature_fraction = feature_fraction,\n                            bagging_fraction = bagging_fraction,\n                            bagging_freq = bagging_freq,\n                            learning_rate = learning_rate,\n                            n_estimators = n_estimators)\n    \n    gbm.fit(x_train.values, y_train.values.ravel(),\n            eval_set = [(x_test.values, y_test.values.ravel())],\n            eval_metric = 'rmse',\n            early_stopping_rounds = early_stopping_rounds)\n\n    y_preds_gbm = gbm.predict(x_testFinal, num_iteration = gbm.best_iteration)\n    RMSE_gbm = np.sqrt(mean_squared_error(y_testFinal, y_preds_gbm))\n    print(\"RMSE for iteration #{} is {}.\".format(i+1, RMSE_gbm))\n    print(\"NL={}, MD={}, FF={}, BF={}, BQ={}, LR={}, NE={}, ESR={}\".format(num_leaves,\n                                                                           max_depth,\n                                                                           feature_fraction,\n                                                                           bagging_fraction,\n                                                                           bagging_freq,\n                                                                           learning_rate,\n                                                                           n_estimators,\n                                                                           early_stopping_rounds))\n    print()\n    initial_preds[RMSE_gbm] = y_preds_gbm\n    testFinal_preds_gbm = gbm.predict(testFinal, num_iteration = gbm.best_iteration)\n    final_preds[RMSE_gbm] = [testFinal_preds_gbm]\n    \n    # Create a dataframe with the values above\n    new_row = pd.DataFrame([[RMSE_gbm,\n                             num_leaves, \n                             max_depth, \n                             feature_fraction,\n                             bagging_fraction,\n                             bagging_freq,\n                             learning_rate]],\n                     columns = [\"RMSE\",\n                                \"num_leaves\", \n                                \"max_depth\", \n                                \"feature_fraction\",\n                                \"bagging_fraction\",\n                                \"bagging_freq\",\n                                \"learning_rate\"])\n    \n    # Append the dataframe as a new row in results_df\n    results_lgb = results_lgb.append(new_row, ignore_index=True)", "outputs": [], "cell_type": "code", "execution_count": 197}, {"metadata": {"_uuid": "c8244886599e20ff23fe4e10fc90b8c17143da2a", "collapsed": false, "trusted": false, "_cell_guid": "f1ac4aba-9ba6-4b5e-955f-accc4c5ef574"}, "source": "results_lgb", "outputs": [], "cell_type": "code", "execution_count": 198}, {"metadata": {"_uuid": "feb11d2e920bffd4228f6f98ba9134c9d4ae5fd0", "collapsed": false, "trusted": false, "_cell_guid": "cc67592a-dfed-474f-aba8-2b241338dfa5", "scrolled": true}, "source": "# Create an empty dataframe to contain all of the inputs for each iteration of the model\nresults_cbr = pd.DataFrame(columns=[\"RMSE\",\n                                    \"iterations\", \n                                    \"depth\", \n                                    \"learning_rate\",\n                                    \"rsm\"])\n\nfor i in range(num_iterations):\n\n    iterations = np.random.randint(1,2) # Changed from (50,250) to (1,2) to save time whne uploading to Kaggle\n    depth = np.random.randint(5,12)\n    learning_rate = np.random.uniform(0.5,1)\n    rsm = np.random.uniform(0.8,1)\n\n    cbr = CatBoostRegressor(iterations = iterations, \n                            depth = depth, \n                            learning_rate = learning_rate,  \n                            rsm = rsm,\n                            loss_function='RMSE',\n                            use_best_model=True)\n    \n    cbr.fit(x_train, y_train,\n            eval_set = (x_test, y_test),\n            use_best_model=True)\n\n    y_preds_cbr = cbr.predict(x_testFinal)\n    RMSE_cbr = np.sqrt(mean_squared_error(y_testFinal, y_preds_cbr))\n    print(\"RMSE for iteration #{} is {}.\".format(i+1, RMSE_cbr))\n    print(\"I={}, D={}, LR={}, RSM={}\".format(iterations,\n                                             depth,\n                                             learning_rate,\n                                             rsm))\n    print()\n    initial_preds[RMSE_cbr] = y_preds_cbr\n    testFinal_preds_cbr = cbr.predict(testFinal)\n    final_preds[RMSE_cbr] = [testFinal_preds_cbr]\n    \n    # Create a dataframe with the values above\n    new_row = pd.DataFrame([[RMSE_cbr,\n                             iterations, \n                             depth, \n                             learning_rate,\n                             rsm]],\n                     columns = [\"RMSE\",\n                                \"iterations\", \n                                \"depth\",\n                                \"learning_rate\",\n                                \"rsm\"])\n    \n    # Append the dataframe as a new row in results_df\n    results_cbr = results_cbr.append(new_row, ignore_index=True)", "outputs": [], "cell_type": "code", "execution_count": 199}, {"metadata": {"_uuid": "4ed6318f501c69b64ac2bf1a36460f41257bcde6", "collapsed": false, "trusted": false, "_cell_guid": "71d39202-9a16-48fc-bbfd-309aede25167"}, "source": "results_cbr", "outputs": [], "cell_type": "code", "execution_count": 200}, {"metadata": {"_uuid": "60f0595190f55ad623024f993d0692a6be1dabd5", "collapsed": false, "trusted": false, "_cell_guid": "2b129f4b-7415-4785-b94a-a4969491a34b"}, "source": "sorted_initial_RMSE = sorted(initial_preds)\nprint(sorted_initial_RMSE)", "outputs": [], "cell_type": "code", "execution_count": 201}, {"metadata": {"_uuid": "01e9d3880ce032b2a0ba1c165d9c499213c599d4", "_cell_guid": "7a9d5fa2-93c8-4464-9e53-88c8f6f9db4c"}, "source": "## Making Predictions", "outputs": [], "cell_type": "markdown", "execution_count": null}, {"metadata": {"_uuid": "deb2d79bf22e2e39304c4f5ede8faabc2c576275", "collapsed": false, "trusted": false, "_cell_guid": "55e326eb-2007-4641-9013-cde5127d9105", "scrolled": true}, "source": "best_models = [] # Records teh RMSE of the models to be used for the final predictions\nbest_RMSE = 99999999999 # records the best RMSE\nbest_predictions = np.array([0]*len(x_testFinal)) # records the best predictions for each row\ncurrent_model = 1 # Used to equally weight the predictions from each iteration\n\nfor model in sorted_initial_RMSE:\n    \n    predictions = initial_preds[model]\n    \n    RMSE = np.sqrt(mean_squared_error(y_testFinal, predictions))\n    print(\"RMSE = \", RMSE)\n    \n    # Equally weight each prediction\n    combined_predictions = (best_predictions*(current_model-1) + predictions) / current_model\n    \n    # Find the RMSE with the new predictions\n    new_RMSE = np.sqrt(mean_squared_error(y_testFinal, combined_predictions))\n    print(\"New RMSE = \", new_RMSE)\n    \n    if new_RMSE <= best_RMSE:\n        best_predictions = combined_predictions\n        best_RMSE = new_RMSE\n        best_models.append(model)\n        current_model += 1\n        print(\"Improvement!\")\n        print()\n    else:\n        print(\"No improvement.\")\n        print()", "outputs": [], "cell_type": "code", "execution_count": 208}, {"metadata": {"_uuid": "c18a3c4ea4add36d1105ddb2c5f696c479e664e9", "collapsed": false, "trusted": false, "_cell_guid": "4593d922-1ec0-402a-9a7f-4cf610de522c"}, "source": "best_predictions = pd.DataFrame([0]*len(testFinal)) # Records the predictions to be used for submission to Kaggle\ncurrent_model = 1\n\nfor model in best_models:\n    print(model)\n    predictions = final_preds[model][0]\n    predictions = pd.DataFrame(np.exp(predictions)-1)\n    \n    combined_predictions = (best_predictions*(current_model-1) + predictions) / current_model\n    best_predictions = combined_predictions\n    current_model += 1", "outputs": [], "cell_type": "code", "execution_count": 209}, {"metadata": {"_uuid": "fc8dff71aa0eff64da43170e304b800f10cb481c", "collapsed": true, "trusted": false, "_cell_guid": "85f25b56-1247-4684-98f9-4959ad12d6eb"}, "source": "# Prepare the dataframe for submitting to Kaggle\nbest_predictions['id'] = test.id\nbest_predictions['trip_duration'] = best_predictions[0]\nbest_predictions = best_predictions.drop([0],1)\n\nbest_predictions.to_csv(\"submission_combined.csv\", index=False)", "outputs": [], "cell_type": "code", "execution_count": 210}, {"metadata": {"_uuid": "e687eca956a2becbd78cd10225271233ac9cdfb8", "collapsed": false, "trusted": false, "_cell_guid": "b73b854a-b6a7-44e4-b82c-a44197009752"}, "source": "# Preview the predictions\nbest_predictions.head()", "outputs": [], "cell_type": "code", "execution_count": 211}, {"metadata": {"_uuid": "ef654c820a93ff3508180870326f2fa81056d8e7", "collapsed": false, "trusted": false, "_cell_guid": "2d8bc9c4-d18f-4190-96d2-6960ee98c844"}, "source": "# Compare the predicted values with the training values - the distribution should be similar\nbest_predictions.trip_duration.describe()", "outputs": [], "cell_type": "code", "execution_count": 212}, {"metadata": {"_uuid": "91a2e9aad9d43bdd67b98bb10c158c92be0f9b97", "collapsed": false, "trusted": false, "_cell_guid": "a88c02c5-83e0-4a89-a502-fdc1f314386c", "scrolled": true}, "source": "yFinal.describe()", "outputs": [], "cell_type": "code", "execution_count": 207}, {"metadata": {"_uuid": "c2b4a237d1dd7381f029c1a81f4012b6975cd06f", "_cell_guid": "8251e218-a395-403d-961f-5b730b2b31d5"}, "source": "## Summary", "outputs": [], "cell_type": "markdown", "execution_count": null}, {"metadata": {"_uuid": "1fe28423cc9d62128ae509008b216d8ca1732fc0", "_cell_guid": "a9058261-3c8b-400a-86db-57cc392800cb"}, "source": "This ensemble approach with random search has worked rather well. Currently, I am ranked in the top 13% of this competition. Creating numerous features and fine-tuning the range for the random searches were critical to the success of this work. ", "outputs": [], "cell_type": "markdown", "execution_count": null}, {"metadata": {"_uuid": "3ae3051c0794e1990ef9cb555c1598452ea4f04b", "collapsed": true, "trusted": false, "_cell_guid": "aff9eb73-3070-4b2f-9532-4dcf3adf6041"}, "source": "", "outputs": [], "cell_type": "code", "execution_count": null}]}