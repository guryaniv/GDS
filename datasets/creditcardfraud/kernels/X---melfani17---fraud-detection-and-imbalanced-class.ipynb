{"cells":[{"metadata":{"_uuid":"333f112ff9a92071d4baa002de89c4a2c40730da","_cell_guid":"019580e4-7173-4164-8a42-4363f421f663"},"cell_type":"markdown","source":"# Fraud Detection and Imbalanced Class\n\n**This notebook will illustrate how to solve a skewed data problem using a fraud detection **\n\nOne estimate, from a 2012 report by the Association of Certified Fraud Examiners, puts global fraud losses at $3.5 trillion each year. The reality, though, is that nobody really knows the impact of fraud. Its deceptive nature means much of it passes beneath the radar and is never fully detected.\n\nOver the past decade, the financial services industry has invested heavily in using ‘big data’ and data analytics in their fight against fraudsters. Automated systems scan and score every transaction, claim and policy to detect anomalies and uncover potential cases of fraud.\n\nIn a business point of view, hopefully, there is a lot more real transaction that fraud transaction. Nevertheless, in a data science point of view, we find this unbalance in the different datasets of fraud transactions.\n\n"},{"metadata":{"_uuid":"579917513fdd48e5c64a285e6b24593809f3257c","_cell_guid":"813b9d26-66e4-4609-8544-977a89b13645"},"cell_type":"markdown","source":"## Load Dataset and illustrate the imbalance data"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","scrolled":false,"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\n\nimport os\ndata=pd.read_csv('../input/creditcard.csv')\nprint(type(data))\n\n#print(datadf.columns)\ncount_classes = pd.value_counts(data['Class'], sort = True).sort_index()\nlabels = 'No Fraud', 'Fraud'\nsizes = [count_classes[1]/(count_classes[1]+count_classes[0]), count_classes[0]/(count_classes[1]+count_classes[0])]\nexplode = (0, 0.5,)  # only \"explode\" the 2nd slice (i.e. 'Fraud')\ncolors = ['orange', 'darkblue']\nfig1, ax1 = plt.subplots()\nax1.pie(sizes, explode=explode, colors=colors, labels=labels, autopct='%1.1f%%',\n        shadow=True, startangle=45)\nax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\nplt.title(\"Fraud class repartition\")\nplt.show()","execution_count":1,"outputs":[]},{"metadata":{"_uuid":"0d0585bbba09254a12af36f6bdad81373264cc7f","_cell_guid":"2a3e47a7-84da-43b5-9dbf-40c1cdaaf188"},"cell_type":"markdown","source":"That's it ! As illustrated in the above graph, the fraud transaction account for 0.2% of the total transaction. The risk to implement a model fitting the whole dataset is to underfit the class fraud. In this case,  our model will be not able to identify the fraud among the transactions.\n\nTo illustrate this concept, we will apply a simple SGD classifier. The model will have some difficulties to detect the fraud transactions.\n\nFirstly, we will define the* plot_confusion_matrix* function. The goal of this function is to plot the confusion matrix ( obvious).  We will use this function all along this notebook ."},{"metadata":{"_uuid":"166d18c2f7fde115789c48930a79bd2915fa4470","_cell_guid":"df9ee3b6-bbe0-463b-b3a9-b33d96af22cb","collapsed":true,"trusted":true},"cell_type":"code","source":"import itertools\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n    \n    \n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","execution_count":2,"outputs":[]},{"metadata":{"_uuid":"dfffeb6db6f6b59a9efed611b94426c39df98042","_cell_guid":"765bfc5f-32ab-4b91-95c2-fac4ef5234af"},"cell_type":"markdown","source":"£Now, we can split th"},{"metadata":{"_kg_hide-output":true,"_uuid":"6e66f0ac45abd842a6b6e9a51db6dcf53dffa997","_cell_guid":"da8e4f71-595b-4f5f-8af1-4690ccc93d6b","_kg_hide-input":false,"collapsed":true,"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.preprocessing import StandardScaler\n\n\ndata['normAmount']=StandardScaler().fit_transform(data['Amount'].values.reshape(-1,1))\nY=data['Class']\nX=data.drop(['Time','Amount','Class'],axis=1)\n## Stratify /!\\\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y , test_size=0.3, random_state=42,stratify=Y)\n\n#return \n# print(Y_train.isnull().sum().sum())\n","execution_count":3,"outputs":[]},{"metadata":{"_uuid":"1882c04524af205b6dcdc35b06e80bff513f268d","_cell_guid":"23aa157e-5750-4bd0-a878-b68a97249d6d","trusted":true},"cell_type":"code","source":"\nfrom sklearn import metrics\n\nsgd_clf=SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n       learning_rate='optimal', loss='hinge', max_iter=5, n_iter=None,\n       n_jobs=1, penalty='l2', power_t=0.5, random_state=42, shuffle=True,\n       tol=None, verbose=0, warm_start=False)\n\nsgd_clf.fit(X_train, Y_train) \nY_train_predicted=sgd_clf.predict(X_train)\nY_test_predicted=sgd_clf.predict(X_test)\n\n\nfrom sklearn.model_selection import cross_validate\n\n## Cross Validation\nscoring = ['precision', 'recall']\nscores = cross_validate(sgd_clf, X_train, Y_train, scoring=scoring, cv=5, return_train_score=False,)\n\n\n\n\nfrom sklearn.metrics import precision_score, recall_score\n\nprint(\" ### TRAINING SET ###\")\nprint(\"Recall : \" + str(scores[\"test_recall\"].mean()) +\"  | Precision : \" +str(scores[\"test_precision\"].mean()))\nprint(\" ### TEST SET ###\")\nprint(\"Recall : \" + str(recall_score(Y_test,Y_test_predicted)) +\"  | Precision : \" +str(precision_score(Y_test,Y_test_predicted)))\n\nfrom sklearn.metrics import confusion_matrix\nconfusion=confusion_matrix(Y_test,Y_test_predicted)\n\nclass_names = [0,1]\nplt.figure()\nplot_confusion_matrix(confusion\n                      , classes=class_names\n                      , title='Confusion matrix')\nplt.show()","execution_count":4,"outputs":[]},{"metadata":{"_uuid":"01cc773217601c69bc26b38793664c5bf246a850","_cell_guid":"b173601c-ef8a-4f3a-9e27-db0a851c07a8","collapsed":true},"cell_type":"markdown","source":"## Manage the unbalanced data\n\nNow, we have a baseline to compare different methods to manage unbalanced class to the original dataset.\nThere are two ways to handle the unbalance :\n\n* Decrease the number of the majority by deteling some datas and train the model\n* Increase the number of the minority data by generating (model based) new datas and train the model\n* Increase the number of the minority data by duplicating the minority data\n\n### Decrease the number of the majority data\n\n\n"},{"metadata":{"_uuid":"3db739ea616da0c6a8297ce21e92831501e45ecc","scrolled":false,"_cell_guid":"679fd6a3-dd97-40bc-99ed-21874ca482a1","trusted":true},"cell_type":"code","source":"from sklearn.utils import shuffle\n\nTrain_Data= pd.concat([X_train, Y_train], axis=1)\n## On peut encore faire des trucs cool ici tu dois separer les training et etst data 1  et en dessous tu dois plus prendre de test data vu que tu en as deja tu utiliseras juste de la ceoss validation v\nX_1 =Train_Data[ Train_Data[\"Class\"]==1 ]\nX_0=Train_Data[Train_Data[\"Class\"]==0]\n\nX_0=shuffle(X_0,random_state=42).reset_index(drop=True)\nX_1=shuffle(X_1,random_state=42).reset_index(drop=True)\n\nALPHA=1.4\n\nX_0=X_0.iloc[:round(len(X_1)*ALPHA),:]\ndata_d=pd.concat([X_1, X_0])\n\ncount_classes = pd.value_counts(data_d['Class'], sort = True).sort_index()\nlabels = 'No Fraud', 'Fraud'\nsizes = [count_classes[1]/(count_classes[1]+count_classes[0]), count_classes[0]/(count_classes[1]+count_classes[0])]\nexplode = (0, 0.05,)  # only \"explode\" the 2nd slice (i.e. 'Fraud')\ncolors = ['orange', 'darkblue']\nfig1, ax1 = plt.subplots()\nax1.pie(sizes, explode=explode, colors=colors, labels=labels, autopct='%1.1f%%',\n        shadow=True, startangle=45)\nax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\nplt.title(\"Fraud class repartition\")\nplt.show()","execution_count":5,"outputs":[]},{"metadata":{"_uuid":"485af1f75fcfc91b23275f5e28e6c7d9bdb3a4c6","_cell_guid":"f297d706-89e6-4157-9a4c-aba82ca287bb","trusted":true},"cell_type":"code","source":"\nY_d=Train_Data['Class']\nX_d=Train_Data.drop(['Class'],axis=1)\nTrain_Data.head()\n","execution_count":6,"outputs":[]},{"metadata":{"_uuid":"ccfd6125f3465a93f6d965609cf144bba4957b82","_cell_guid":"77b0b60c-2b3b-4e3d-9792-19fbe199b1e7","trusted":true},"cell_type":"code","source":"sgd_clf_d=SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n       learning_rate='optimal', loss='hinge', max_iter=5, n_iter=None,\n       n_jobs=1, penalty='l2', power_t=0.5, random_state=42, shuffle=True,\n       tol=None, verbose=0, warm_start=False)\n\nsgd_clf_d.fit(X_d, Y_d) \n\n# Cross validation\n\nscoring = ['precision', 'recall']\nscores_d = cross_validate(sgd_clf, X_d, Y_d, scoring=scoring, cv=5, return_train_score=False)\n\nY_test_predicted=sgd_clf_d.predict(X_test)\n\nprint(\" ### TRAINING SET ###\")\nprint(\"Recall : \" + str(scores_d[\"test_recall\"].mean()) +\"  | Precision : \" +str(scores_d[\"test_precision\"].mean()))\nprint(\" ### TEST SET ###\")\nprint(\"Recall : \" + str(recall_score(Y_test,Y_test_predicted)) +\"  | Precision : \" +str(precision_score(Y_test,Y_test_predicted)))\n\nconfusion=confusion_matrix(Y_test,Y_test_predicted)\n\n\nclass_names = [0,1]\nplt.figure()\nplot_confusion_matrix(confusion\n                      , classes=class_names\n                      , title='Confusion matrix')\nplt.show()","execution_count":7,"outputs":[]},{"metadata":{"_uuid":"ffeb05d3d06071698af9a325835db4390f96fe7c","_cell_guid":"f67f3d14-f0f6-43d5-a7ec-5308447ba476","trusted":true},"cell_type":"code","source":"from sklearn.utils import shuffle\n\nX_1 =Train_Data[ Train_Data[\"Class\"]==1 ]\nX_0=Train_Data[Train_Data[\"Class\"]==0]\n\nX_0=shuffle(X_0,random_state=42).reset_index(drop=True)\nX_1=shuffle(X_1,random_state=42).reset_index(drop=True)\n\nscoring = ['precision', 'recall']\n\nALPHA=0\nalpha_array= np.array([])\nprecision_array= np.array([])\nrecall_array= np.array([])\n\nd_alpha_array= np.array([])\nd_precision_array= np.array([])\nd_recall_array= np.array([])\n\n\n                                                       \nfor ALPHA in np.arange(10,200,1):\n    \n    X_0=Train_Data[Train_Data[\"Class\"]==0]\n    X_0=X_0.iloc[:int(len(X_1)*ALPHA/10),:]\n    \n    data_d=pd.concat([X_1, X_0], axis=0)\n    data_d=shuffle(data_d,random_state=42).reset_index(drop=True)\n    Y_d=data_d['Class']\n    X_d=data_d.drop(['Class'],axis=1)\n    \n    \n\n    sgd_clf_d=SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n           eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n           learning_rate='optimal', loss='hinge', max_iter=5, n_iter=None,\n           n_jobs=1, penalty='l2', power_t=0.5, random_state=42, shuffle=True,\n           tol=None, verbose=0, warm_start=False)\n\n    sgd_clf_d.fit(X_d, Y_d) \n\n\n    scores_d = cross_validate(sgd_clf, X_d, Y_d, scoring=scoring, cv=5, return_train_score=False)\n    Y_predicted=sgd_clf_d.predict(X_test)\n    \n                                                           \n    alpha_array=np.append(alpha_array,ALPHA/10)\n    precision_array =np.append(precision_array,scores_d[\"test_precision\"].mean())\n    recall_array=np.append(recall_array,scores_d[\"test_recall\"].mean())\n    \nscoreF1_array=(2*(recall_array*precision_array)/(recall_array+precision_array))\n\n\nplt.plot(alpha_array, recall_array,label=\"Recall\")\nplt.plot(alpha_array,precision_array,label=\"Precision\")\nplt.plot(alpha_array,scoreF1_array,label=\"ScoreF1\")\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\nplt.show()\n\n\n    ","execution_count":8,"outputs":[]},{"metadata":{"_uuid":"638f75dd9c1eb9e921f4fbf7370c65ca9fa66180","_cell_guid":"560d7cd2-d917-41f4-9d9b-6620a8756501","trusted":true},"cell_type":"code","source":"Best_index=np.argmax(scoreF1_array)\nprint(\" Best ALPHA :\",alpha_array[Best_index],\" Recall :\",recall_array[Best_index],\" Precision :\", precision_array[Best_index])\n\nALPHA=alpha_array[Best_index]\n\nX_1 =data[ data[\"Class\"]==1 ]\nX_0=data[data[\"Class\"]==0]\nX_0=shuffle(X_0,random_state=42).reset_index(drop=True)\nX_1=shuffle(X_1,random_state=42).reset_index(drop=True)\n\nX_0=data[data[\"Class\"]==0]\nX_0=X_0.iloc[:int(len(X_1)*ALPHA),:]\ndata_d=pd.concat([X_1, X_0])\n\ndata_d['normAmount']=StandardScaler().fit_transform(data_d['Amount'].values.reshape(-1,1))\nY_d=data_d['Class']\nX_d=data_d.drop(['Time','Amount','Class'],axis=1)\n\nX_d_train, X_d_test, Y_d_train, Y_d_test = train_test_split(X_d, Y_d , test_size=0.3, random_state=42)\n\nsgd_clf_d=SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\neta0=0.0, fit_intercept=True, l1_ratio=0.15,\nlearning_rate='optimal', loss='hinge', max_iter=5, n_iter=None,\nn_jobs=1, penalty='l2', power_t=0.5, random_state=42, shuffle=True,tol=None, verbose=0, warm_start=False)\n\nsgd_clf_d.fit(X_d_train, Y_d_train) \n\nY_test_predicted=sgd_clf_d.predict(X_test)\n\nprint(\"Recall : \" + str(recall_score(Y_test,Y_test_predicted)) +\"  | Precision : \" +str(precision_score(Y_test,Y_test_predicted)))\nconfusion=confusion_matrix(Y_test,Y_test_predicted)\n\nclass_names = [0,1]\nplt.figure()\nplot_confusion_matrix(confusion\n                      , classes=class_names\n                      , title='Confusion matrix')\nplt.show()\n","execution_count":9,"outputs":[]},{"metadata":{"_uuid":"fba98353356acebd728267c44ca9e5ab486a244d","_cell_guid":"f500e85b-a6b2-45f3-b312-c51ca9e5c7c6","trusted":true},"cell_type":"code","source":"\n\nALPHA=15\n\nX_1 =data[ data[\"Class\"]==1 ]\nX_0=data[data[\"Class\"]==0]\nX_0=shuffle(X_0,random_state=42).reset_index(drop=True)\nX_1=shuffle(X_1,random_state=42).reset_index(drop=True)\nlen_X_1=len(X_1)\nX_1=pd.concat([X_1,X_1, X_1])\n\nX_0=data[data[\"Class\"]==0]\nX_0=X_0.loc[:np.round(len_X_1*ALPHA),:]\ndata_d=pd.concat([X_1, X_0])\n\ndata_d['normAmount']=StandardScaler().fit_transform(data_d['Amount'].values.reshape(-1,1))\nY_d=data_d['Class']\nX_d=data_d.drop(['Time','Amount','Class'],axis=1)\n\nX_d_train, X_d_test, Y_d_train, Y_d_test = train_test_split(X_d, Y_d , test_size=0.3, random_state=42)\n\nsgd_clf_d=SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\neta0=0.0, fit_intercept=True, l1_ratio=0.15,\nlearning_rate='optimal', loss='hinge', max_iter=5, n_iter=None,\nn_jobs=1, penalty='l2', power_t=0.5, random_state=42, shuffle=True,tol=None, verbose=0, warm_start=False)\n\nsgd_clf_d.fit(X_d_train, Y_d_train) \n\nY_test_predicted=sgd_clf_d.predict(X_test)\n\nprint(\"Recall : \" + str(recall_score(Y_test,Y_test_predicted)) +\"  | Precision : \" +str(precision_score(Y_test,Y_test_predicted)))\nconfusion=confusion_matrix(Y_test,Y_test_predicted)\n\nclass_names = [0,1]\nplt.figure()\nplot_confusion_matrix(confusion\n                      , classes=class_names\n                      , title='Confusion matrix')\nplt.show()\n","execution_count":10,"outputs":[]},{"metadata":{"_uuid":"c150a28c9a2b39e17e01092bd35df996bc5c74f0","_cell_guid":"8a9bf9ed-357a-43d9-90be-cd46081b112e","collapsed":true,"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a8290dd76d4ea6858f614dfe29e6cca3b856d889","_cell_guid":"67dcb4b5-fb48-411b-9a37-5faba5a1d327","collapsed":true,"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}