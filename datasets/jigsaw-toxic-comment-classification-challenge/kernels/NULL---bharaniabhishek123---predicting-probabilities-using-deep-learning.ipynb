{"nbformat": 4, "cells": [{"cell_type": "code", "execution_count": null, "outputs": [], "source": ["# This Python 3 environment comes with many helpful analytics libraries installed\n", "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n", "# For example, here's several helpful packages to load in \n", "\n", "import numpy as np # linear algebra\n", "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n", "\n", "# Input data files are available in the \"../input/\" directory.\n", "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n", "\n", "from subprocess import check_output\n", "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n", "\n", "# Any results you write to the current directory are saved as output.\n", "def read_data(train):\n", "    data_comments=[]\n", "    data_y = []\n", "    for index,row in train.iterrows():\n", "        comment = row.comment_text\n", "        data_comments.append(comment)\n", "        y_values = np.array([row.toxic,row.severe_toxic,row.obscene,row.threat,row.insult,row.identity_hate])\n", "        data_y.append(y_values)\n", "    return data_comments,data_y\n", "\n", "train = pd.read_csv('../input/train.csv',encoding='utf-8',error_bad_lines=False)\n", "train_comments,train_y = read_data(train)\n", "\n", "from keras.preprocessing.text import Tokenizer\n", "\n", "tokenizer = Tokenizer(num_words=10000)\n", "tokenizer.fit_on_texts(train_comments)\n", "# one_hot_encoding_results = tokenizer.texts_to_matrix(train_comments,mode='binary')\n", "# print(one_hot_encoding_results.shape = [95851,10000])\n", "\n", "x_train = tokenizer.texts_to_matrix(train_comments,mode='binary')\n", "\n", "#vectorize the labels as well\n", "y_train = np.asarray(train_y).astype('float32')\n", "\n", "\n", "from keras import models\n", "from keras import layers\n", "num_classes =6\n", "\n", "model = models.Sequential()\n", "\n", "model.add(layers.Dense(32,activation='relu',input_shape=(10000,)))\n", "model.add(layers.Dense(32,activation='relu'))\n", "model.add(layers.Dense(num_classes,activation='sigmoid'))\n", "model.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['acc'])\n", "\n", "\n", "history = model.fit(x_train,y_train,epochs=8,batch_size=512,\n", "                   validation_split=0.2)\n", "\n", "\n", "\n", "history_dict = history.history\n", "\n", "print(history_dict.keys())\n", "\n", "acc =history_dict['acc']\n", "val_acc =history_dict['val_acc']\n", "\n", "#Plot the training and val loss and accuracy\n", "import matplotlib.pyplot as plt\n", "\n", "loss_values = history_dict['loss']\n", "val_loss_values = history_dict['val_loss']\n", "\n", "epochs = range(1, len(acc) + 1)\n", "\n", "\n", "plt.plot(epochs, loss_values, 'bo', label='Training loss')\n", "plt.plot(epochs, val_loss_values, 'b', label='Validation loss')\n", "plt.title('Training and validation loss')\n", "plt.xlabel('Epochs')\n", "plt.ylabel('Loss')\n", "plt.legend()\n", "\n", "plt.show()\n", "\n", "\n", "plt.clf()\n", "acc_values = history_dict['acc']\n", "val_acc_values = history_dict['val_acc']\n", "\n", "plt.plot(epochs, acc, 'bo', label='Training acc')\n", "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n", "plt.title('Training and validation accuracy')\n", "plt.xlabel('Epochs')\n", "plt.ylabel('Loss')\n", "plt.legend()\n", "\n", "plt.show()\n", "\n", "\n", "\n", "def read_data_test(test_data):\n", "    data_comments = []\n", "    for index, row in test_data.iterrows():\n", "        comment = row.comment_text\n", "        if comment is not np.nan:\n", "            data_comments.append(comment)\n", "    return data_comments\n", "\n", "test = pd.read_csv('../input/test.csv',encoding='utf-8',error_bad_lines=False)\n", "\n", "test_comments = read_data_test(test)\n", "\n", "tokenizer.fit_on_texts(test_comments)\n", "x_test = tokenizer.texts_to_matrix(test_comments,mode='binary')\n", "\n", "# values of probabilities for each id\n", "probabilities =model.predict(x_test)\n", "\n", "#creating dataframe\n", "prob_df= pd.DataFrame(probabilities)\n", "\n", "\n", "#creating dataframe of id's\n", "id_df = test['id']\n", "\n", "\n", "#final results will be\n", "results = pd.concat([id_df,prob_df],axis=1)\n", "\n", "print(results)\n", "\n", "# results.to_csv('final_results.csv',index=False)\n", "\n", "\n", "\n"], "metadata": {"_uuid": "09bd1c190482f814e81f77ae18cc25de1e275510", "_cell_guid": "e6d9f56a-b356-40fd-9ae4-1ffd7e1eec85", "collapsed": true}}], "nbformat_minor": 1, "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.6.3", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python"}}}