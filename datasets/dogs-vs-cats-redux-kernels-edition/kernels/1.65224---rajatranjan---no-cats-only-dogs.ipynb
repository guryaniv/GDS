{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport random\n# Input data files are available in the \"../input/\" directory.\n\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":1,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0d0a8b3ec539c352de5cfc95bd1202cce15e9cee","collapsed":true},"cell_type":"code","source":"# print(os.listdir(\"../input/cat-vs-dogs-arrays\"))","execution_count":2,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"collapsed":true},"cell_type":"code","source":"TRAIN_DIR = '../input/train/'\nTEST_DIR = '../input/test/'\n\nROWS = 64\nCOLS = 64\nCHANNELS = 3\n\n\ntrain_images = [TRAIN_DIR+i for i in os.listdir(TRAIN_DIR)] # use this for full dataset\ntrain_dogs =   [TRAIN_DIR+i for i in os.listdir(TRAIN_DIR) if 'dog' in i]\ntrain_cats =   [TRAIN_DIR+i for i in os.listdir(TRAIN_DIR) if 'cat' in i]\n\ntest_images =  [TEST_DIR+i for i in os.listdir(TEST_DIR)]\n\ntrain_images = train_dogs[:3000] + train_cats[:3000]\nrandom.shuffle(train_images)\n#test_images =  test_images[:1000]","execution_count":3,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"05cb8a7764df934592b2c171dee976c15e0f60a9","collapsed":true},"cell_type":"code","source":"from PIL import ImageFilter\nfrom sklearn import preprocessing\ndef read_image(file_path):\n    img = Image.open(file_path)\n    img=img.resize((ROWS, COLS), Image.ANTIALIAS)\n    #img = img.filter(ImageFilter.BLUR)\n    #img = img.filter(ImageFilter.FIND_EDGES)\n    \n    return np.array(img)\n\n\ndef prep_data(images):\n    count = len(images)\n    data = np.ndarray((count, ROWS, COLS,CHANNELS), dtype=np.uint8)\n\n    for i, image_file in enumerate(images):\n        image = read_image(image_file)\n        data[i] = image\n        if i%1000 == 0:\n            print('Processed {} of {}'.format(i, count))\n            #print(image.shape)\n            #plt.imshow(image)\n            #plt.show()\n    #print(data.shape)\n    return data\n\ntrain = prep_data(train_images)\ntest = prep_data(test_images)\n\n# train=np.load('../input/cat-vs-dogs-arrays/train.npz')\n# train.shape","execution_count":4,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"609db0fb80d31afdb9b6a4a064b263ac4164c8e4","collapsed":true},"cell_type":"code","source":"import seaborn as sns\nfrom matplotlib import ticker\ntrain_labels = []\nfor i in train_images:\n    if 'dog' in i:\n        train_labels.append([1,0])\n    else:\n        train_labels.append([0,1])\ntrain_labels=np.array(train_labels)\n#sns.countplot(train_labels)","execution_count":5,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ada8ac2dc3b7a2f1270df293444ebeb95ecc68a6","collapsed":true},"cell_type":"code","source":"#train_labels=np.array(train_labels)","execution_count":6,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"62a1d9804eac25c8c105be45e33ce77aaca4dfc9","collapsed":true},"cell_type":"code","source":"train_labels.shape","execution_count":7,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f74dc8fd1d77e75843bbad54889d412ef193e9a7","collapsed":true},"cell_type":"code","source":"print(train)\ntrain=train/train.max()\nprint(train)","execution_count":8,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4dd22252255439ce930bc84a7a82aebed7a11013","collapsed":true},"cell_type":"code","source":"test=test/test.max()","execution_count":9,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bba29d5b86c28c3047160a0235dfd9c414f5d937","collapsed":true},"cell_type":"code","source":"for i in range(0,len(train_images),500):\n    print(train_images[i],train_labels[i])","execution_count":10,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f9440e0144230aeee432b401d051f27275fa70a8","collapsed":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ntest_size = 0.25\nX_train, X_test, Y_train, Y_test = train_test_split(train,train_labels, test_size=test_size, random_state=101)\n\nimg_size = 64\nchannel_size = 1\nprint(\"Training Size:\", X_train.shape)\nprint(X_train.shape[0],\"samples - \", X_train.shape[1],\"x\",X_train.shape[2],\"rgb image\")\n\nprint(\"\\n\")\n\nprint(\"Test Size:\",X_test.shape)\nprint(X_test.shape[0],\"samples - \", X_test.shape[1],\"x\",X_test.shape[2],\"rgb image\")","execution_count":11,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"1a8af842986a585ed35f9a321e27539af34affcb"},"cell_type":"code","source":"class SignClass():\n    \n    def __init__(self):\n        self.i = 0\n        \n        self.training_images = X_train\n        self.training_labels = Y_train\n        \n        self.test_images = X_test\n        self.test_labels = Y_test\n        self._epochs_completed = 0\n        self._index_in_epoch = 0\n        \n        self._num_examples = X_train.shape[0]\n    \n\n\n        \n    def next_batch(self, batch_size,fake_data=False, shuffle=True):\n#         x = self.training_images[self.i:self.i+batch_size].reshape(-1,64,64,1)\n#         y = self.training_labels[self.i:self.i+batch_size]\n#         self.i = (self.i + batch_size) % len(self.training_images)\n#         return x, y\n        x = self.training_images[self.i:self.i+batch_size]\n        y = self.training_labels[self.i:self.i+batch_size]\n        self.i = (self.i + batch_size) % len(self.training_images)\n        return x, y\n        \"\"\"if fake_data:\n            fake_image = [1] * 4096\n            if self.one_hot:\n                fake_label = [1] + [0] * 9\n            else:\n                fake_label = 0\n            return [fake_image for _ in xrange(batch_size)], [fake_label for _ in xrange(batch_size)]\n        start = self._index_in_epoch\n        # Shuffle for the first epoch\n        if self._epochs_completed == 0 and start == 0 and shuffle:\n            perm0 = np.arange(self._num_examples)\n            np.random.shuffle(perm0)\n            self.training_images = self.training_images[perm0]\n            self.training_labels = self.training_labels[perm0]\n        # Go to the next epoch\n        if start + batch_size > self._num_examples:\n        # Finished epoch\n            self._epochs_completed += 1\n            # Get the rest examples in this epoch\n            rest_num_examples = self._num_examples - start\n            images_rest_part = self.training_images[start:self._num_examples]\n            labels_rest_part = self.training_labels[start:self._num_examples]\n          # Shuffle the data\n            if shuffle:\n                perm = np.arange(self._num_examples)\n                np.random.shuffle(perm)\n                self.training_images = self.training_images[perm]\n                self.training_labels = self.training_labels[perm]\n            # Start next epoch\n            start = 0\n            self._index_in_epoch = batch_size - rest_num_examples\n            end = self._index_in_epoch\n            images_new_part = self.training_images[start:end]\n            labels_new_part = self.training_labels[start:end]\n            return np.concatenate((images_rest_part, images_new_part), axis=0), np.concatenate((labels_rest_part, labels_new_part), axis=0)\n        else:\n            self._index_in_epoch += batch_size\n            end = self._index_in_epoch\n            return self.training_images[start:end], self.training_labels[start:end]\"\"\"","execution_count":12,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"577e6c44bc1571ef59f4f76259c150a458d1d7a0"},"cell_type":"code","source":"ch = SignClass()","execution_count":13,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"85b987682214405bc978c64781128699e08cb0a3","collapsed":true},"cell_type":"code","source":"import tensorflow as tf","execution_count":14,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"63df78481a11f32b0d98cca289fffc12ffbd9b2a"},"cell_type":"code","source":"x = tf.placeholder(tf.float32,shape=[None,64,64,3])\ny_true = tf.placeholder(tf.float32,shape=[None,2])\nhold_prob = tf.placeholder(tf.float32)","execution_count":15,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"aee0796465d237df89db5b4b7bc5ed8fe1d9c6f9"},"cell_type":"code","source":"def init_weights(shape):\n    init_random_dist = tf.truncated_normal(shape, stddev=0.1)\n    return tf.Variable(init_random_dist)\n\ndef init_bias(shape):\n    init_bias_vals = tf.constant(0.1, shape=shape)\n    return tf.Variable(init_bias_vals)\n\ndef conv2d(x, W):\n    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n\ndef max_pool_2by2(x):\n    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n                          strides=[1, 2, 2, 1], padding='SAME')\n\ndef convolutional_layer(input_x, shape):\n    #with tf.name_scope(name):\n    W = init_weights(shape)\n    b = init_bias([shape[3]])\n    c=conv2d(input_x, W)\n    act=tf.nn.relu(c + b)\n    #tf.summary.histogram(\"weights\",W)\n    #tf.summary.histogram(\"biases\",b)\n    #tf.summary.histogram(\"activations\",act)\n    return act\n\ndef normal_full_layer(input_layer, size):\n    #with tf.name_scope(name):\n    input_size = int(input_layer.get_shape()[1])\n    W = init_weights([input_size, size])\n    b = init_bias([size])\n    return tf.matmul(input_layer, W) + b","execution_count":16,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4dce7c2acb9580d2d2c580e882427e106111d01e","collapsed":true},"cell_type":"code","source":"convo_1 = convolutional_layer(x,shape=[5,5,3,64])\nconvo_1_pooling = max_pool_2by2(convo_1)\nconvo_2 = convolutional_layer(convo_1_pooling,shape=[5,5,64,128])\nconvo_2_pooling = max_pool_2by2(convo_2)\n\nconvo_2_flat = tf.reshape(convo_2_pooling,[-1,16*16*128])\nfull_layer_one = tf.nn.relu(normal_full_layer(convo_2_flat,1024))","execution_count":17,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"67c2b80438aad772fabdf079bd14131dc6152bbf"},"cell_type":"code","source":"full_one_dropout = tf.nn.dropout(full_layer_one,keep_prob=hold_prob)\ny_pred = normal_full_layer(full_one_dropout,2)","execution_count":18,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"6da79a6fb1988abda75e71fd47fc2f45f61a2b64"},"cell_type":"code","source":"# with tf.name_scope(\"crossentropy\"):\ncross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_true,logits=y_pred))\n#     tf.summary.scalar('cross_entropy', cross_entropy)\n# with tf.name_scope(\"optimizer\"):\noptimizer = tf.train.AdamOptimizer(learning_rate=0.001)\ntrainop = optimizer.minimize(cross_entropy)\ninit = tf.global_variables_initializer()","execution_count":19,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f9cca8f1c24739e161d279179cccf8497950ac01","collapsed":true},"cell_type":"code","source":"steps = 1000\nsaver = tf.train.Saver()\nwith tf.Session() as sess:\n    \n    sess.run(init)\n    #merged_Summary=tf.summary.merge_all()\n    #writer=tf.summary.FileWriter('dir3/')\n    #writer.add_graph(sess.graph)\n    for i in range(steps):\n        batch = ch.next_batch(100)\n        \n        sess.run(trainop, feed_dict={x: batch[0], y_true: batch[1], hold_prob: 0.5})\n        \n        # PRINT OUT A MESSAGE EVERY 100 STEPS\n        if i%100 == 0:\n            #s=sess.run(merged_Summary,feed_dict={x: batch[0], y_true: batch[1], hold_prob: 0.5})\n            #writer.add_summary(s,i)\n            print('Currently on step {}'.format(i))\n            print('Accuracy is:')\n            # Test the Train Model\n            #with tf.name_scope(\"accuracy\"):\n            matches = tf.equal(tf.argmax(y_pred,1),tf.argmax(y_true,1))\n\n            acc = tf.reduce_mean(tf.cast(matches,tf.float32))\n            #tf.summary.scalar('accuracy', acc)\n            print(sess.run(acc,feed_dict={x:ch.test_images,y_true:ch.test_labels,hold_prob:1.0}))\n            print('\\n')\n    save_path = saver.save(sess, \"/tmp/model.ckpt\")\n    print(\"Model saved in path: %s\" % save_path)","execution_count":20,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"daf4104076faf03d24b86a5ddc57a4f5eade851f","collapsed":true},"cell_type":"code","source":"#import tensorflow as tf\npredictions=[]\nwith tf.Session() as sess:\n  # Restore variables from disk.\n    saver.restore(sess, \"/tmp/model.ckpt\")\n    print('predicting')\n    #predicta=tf.argmax(y_pred,1)\n    predicts=tf.nn.softmax(y_pred)\n    #print(1)\n#     print(sess.run(tf.Print(y_pred,[y_pred])))\n#     print(sess.run(tf.Print(predicta,[predicta])))\n#     print(sess.run(tf.Print(predicts,[predicts])))\n    \n    predictions1=sess.run(predicts,feed_dict={x:test[0:1500],hold_prob:1.0})\n    predictions2=sess.run(predicts,feed_dict={x:test[1500:3000],hold_prob:1.0})\n    predictions3=sess.run(predicts,feed_dict={x:test[3000:4500],hold_prob:1.0})\n    predictions4=sess.run(predicts,feed_dict={x:test[4500:6000],hold_prob:1.0})\n    predictions5=sess.run(predicts,feed_dict={x:test[6000:7500],hold_prob:1.0})\n    predictions6=sess.run(predicts,feed_dict={x:test[7500:9000],hold_prob:1.0})\n    predictions7=sess.run(predicts,feed_dict={x:test[9000:11500],hold_prob:1.0})\n    predictions8=sess.run(predicts,feed_dict={x:test[11500:],hold_prob:1.0})\n\n# for j in [predictions1,predictions2,predictions3,predictions4,predictions5,predictions6]:\n    \n#     predictions.append(j)\n#predictions=predictions1+predictions2+predictions3+predictions4+predictions5+predictions6\n# for j in predictions1:\n#     predictions.append([format(float(x), '.16f') for x in j])\n# for j in predictions2:\n#     predictions.append([format(float(x), '.16f') for x in j])\n# for j in predictions3:\n#     predictions.append([format(float(x), '.16f') for x in j])\n# for j in predictions4:\n#     predictions.append([format(float(x), '.16f') for x in j])\n# for j in predictions5:\n#     predictions.append([format(float(x), '.16f') for x in j])\n# for j in predictions6:\n#     predictions.append([format(float(x), '.16f') for x in j])\n\nfor j in predictions1:\n    predictions.append(j)\nfor j in predictions2:\n    predictions.append(j)\nfor j in predictions3:\n    predictions.append(j)\nfor j in predictions4:\n    predictions.append(j)\nfor j in predictions5:\n    predictions.append(j)\nfor j in predictions6:\n    predictions.append(j)\nfor j in predictions7:\n    predictions.append(j)\nfor j in predictions8:\n    predictions.append(j)\npredictions=np.array(predictions)\nprint('done >',predictions.shape)","execution_count":21,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c7331f24a40bce9f8372114c548e8cf217214447","collapsed":true},"cell_type":"code","source":"pr=pd.DataFrame(data=predictions,columns=['label','cat'])\n\n# for i in range(1,len(predictions)+1):\n    \nprl=pd.DataFrame(data=pr['label'],columns=['label'])\nprl.head()","execution_count":22,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"796762eb738aff5f998e7373b824866b5c4239e5","collapsed":true},"cell_type":"code","source":"d=np.array(list(range(1,len(predictions)+1)))\nids=pd.DataFrame(data=d,columns=['id'])\nids\nmmm=pd.concat([ids,prl],axis=1)\nmmm.to_csv('s1.csv',index=False)","execution_count":23,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"16dfc34fbc691a64f37f6b5164a4b6a2de807919","collapsed":true},"cell_type":"code","source":"mmm","execution_count":24,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"3c9bae86fe64683479f7e708caad7378896695e6"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}