{"nbformat": 4, "nbformat_minor": 1, "cells": [{"metadata": {"_uuid": "16072695e888fbaf8eded73e43440f6b77541ac7", "_cell_guid": "6cd94d7e-0560-4973-9a18-b253427939b4"}, "source": ["# HapPy Halloween kernel"], "cell_type": "markdown"}, {"metadata": {"_uuid": "b84e72fe5b113d5f7fee7b1849dc41e6588d5ab7", "_cell_guid": "9e8a48eb-bf25-4844-9e5f-5025f811a363"}, "source": ["![](https://media.giphy.com/media/pLWfbn1WVKzMQ/giphy.gif)"], "cell_type": "markdown"}, {"metadata": {"_uuid": "9e823e8a582bf203ad59fd1d07124529cf627f94", "_cell_guid": "42d7ef55-ffe4-4726-8c3d-9f2a55ac53e4"}, "source": ["This kernel is made by a beginner for beginners.\n", "This is my first kernel, so I'm looking for advice !"], "cell_type": "markdown"}, {"metadata": {"_uuid": "ffd772274877dd31953b2cd4655a68f11e347ff3", "_cell_guid": "17dea100-c07a-4c7c-9d17-40bf944e1d47"}, "source": ["## Imports"], "cell_type": "markdown"}, {"outputs": [], "metadata": {"_uuid": "9078bf8d4d2a6ac3d9964ba2e1e9285845a4ed46", "collapsed": true, "_cell_guid": "e5de9f60-44cd-4ea0-b7de-5965c80e5503"}, "execution_count": null, "source": ["import os\n", "import pandas as pd\n", "import numpy as np\n", "import sklearn as sk\n", "import matplotlib.pyplot as plt\n", "import seaborn as sns\n", "from string import punctuation\n", "from nltk import pos_tag\n", "from nltk.stem import PorterStemmer\n", "from nltk.tokenize import word_tokenize, sent_tokenize\n", "from nltk.corpus import stopwords\n", "from nltk import FreqDist\n", "from wordcloud import WordCloud, STOPWORDS\n", "\n", "from sklearn.preprocessing import LabelEncoder\n", "from sklearn.model_selection import train_test_split\n", "from sklearn.model_selection import KFold\n", "from sklearn.base import BaseEstimator, TransformerMixin\n", "from sklearn.pipeline import Pipeline, FeatureUnion\n", "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n", "from xgboost import XGBClassifier\n", "from sklearn.model_selection import GridSearchCV\n", "from sklearn.model_selection import cross_val_score\n", "from sklearn.metrics import confusion_matrix"], "cell_type": "code"}, {"outputs": [], "metadata": {"_uuid": "83c7e7809517a04d1ef93685acddf4941b71a1e5", "collapsed": true, "_cell_guid": "f0e374c3-2037-4a8c-8c02-71fb4e3f17e0"}, "execution_count": null, "source": ["train = pd.read_csv(\"../input/train.csv\")\n", "test = pd.read_csv(\"../input/test.csv\")"], "cell_type": "code"}, {"metadata": {"_uuid": "53f02120a5b0fd845e5d5434890651072017ad59", "_cell_guid": "7324a50d-77bf-4c6b-a04a-5c0bbab030d3"}, "source": ["## Problem Understanding"], "cell_type": "markdown"}, {"metadata": {"_uuid": "733e71e2954738fed55ca1e37beda8460993db42", "_cell_guid": "cc391c97-696f-48f9-be62-ae84e219a3d5"}, "source": ["It is always important to know what we are going for.\n", "The description gives some details:\n", "> The competition dataset contains text from works of fiction written by spooky authors of the public domain: Edgar Allan Poe, HP Lovecraft and Mary Shelley\n", "\n", "The objective is to accurately **identify the author of the sentences in the test set**."], "cell_type": "markdown"}, {"metadata": {"_uuid": "9831d801837125cbd0bdc57ce5b5fde3c7993cf8", "_cell_guid": "7ed26e51-ef4b-4010-acca-8f2917a1d341"}, "source": ["## Data Understanding"], "cell_type": "markdown"}, {"outputs": [], "metadata": {"_uuid": "9e3e600de7c1b94f511b55b8ac718fe710083bca", "_cell_guid": "5c4cc768-d5b3-4c23-ad23-b19c93ba5471"}, "execution_count": null, "source": ["train.head()\n", "print(\"--- Shape ---\")\n", "print(train.shape)\n", "print(\"--- Missing values ---\")\n", "train.isnull().sum() * 100 / len(train)"], "cell_type": "code"}, {"metadata": {"_uuid": "de18a9a978c15414292925d0c6d76d11e1666962", "_cell_guid": "f9cec234-ecce-45ba-b270-84b29fe8a806"}, "source": ["* The dataset goes straight to the point with only 3 columns : ids, texts, and the targets which are authors\n", "* There is no missing value"], "cell_type": "markdown"}, {"outputs": [], "metadata": {"_uuid": "9968f035c1238e0b5f4ef8158f3c511969dece22", "_cell_guid": "5435cc4e-fc30-4eb0-9e57-f0ec2fed726e"}, "execution_count": null, "source": ["sns.countplot(train.author)"], "cell_type": "code"}, {"outputs": [], "metadata": {"_uuid": "0f7a4daf3ee080def0d5896853e2277f9cfe5be6", "collapsed": true, "_cell_guid": "609796b9-5b14-4963-9d07-8e9ace4e9ac4"}, "execution_count": null, "source": ["# Takes a column and concatenate strings\n", "def build_corpus(data):\n", "    data = str(data)\n", "    corpus = \"\"\n", "    for sent in data:\n", "        corpus += str(sent)\n", "    return corpus"], "cell_type": "code"}, {"outputs": [], "metadata": {"_uuid": "5014ff75e8a1289ad872889bea4432fbe372042f", "collapsed": true, "_cell_guid": "3eddf71a-4dc4-4e4d-b0ed-e36a1d3000c2"}, "execution_count": null, "source": ["# Gather text of authors in different dataframes\n", "eap = train[train.author == \"EAP\"]\n", "hpl = train[train.author == \"HPL\"]\n", "mws = train[train.author == \"MWS\"]"], "cell_type": "code"}, {"outputs": [], "metadata": {"_uuid": "bed1d7bd4fad41357aebdea275047c72057f094f", "_cell_guid": "972b943d-14a3-473d-aafd-04cabd927094"}, "execution_count": null, "source": ["plt.figure(figsize=(15,10))\n", "plt.subplot(331)\n", "eap_wc = WordCloud(background_color=\"white\", max_words=100, stopwords=STOPWORDS)\n", "eap_wc.generate(build_corpus(eap.text))\n", "plt.title(\"Edgar Allan Poe\", fontsize=20)\n", "plt.imshow(eap_wc, interpolation='bilinear')\n", "plt.axis(\"off\")\n", "\n", "plt.subplot(332)\n", "hpl_wc = WordCloud(background_color=\"white\", max_words=100, stopwords=STOPWORDS)\n", "hpl_wc.generate(build_corpus(hpl.text))\n", "plt.title(\"HP Lovecraft\", fontsize=20)\n", "plt.imshow(hpl_wc, interpolation='bilinear')\n", "plt.axis(\"off\")\n", "\n", "plt.subplot(333)\n", "mws_wc = WordCloud(background_color=\"white\", max_words=100, stopwords=STOPWORDS)\n", "mws_wc.generate(build_corpus(mws.text))\n", "plt.title(\"Marry Shelley\", fontsize=20)\n", "plt.imshow(mws_wc, interpolation='bilinear')\n", "plt.axis(\"off\")"], "cell_type": "code"}, {"metadata": {"_uuid": "b76f9aad7f53931c6baa45b17e7746c17608f026", "_cell_guid": "2b5133ab-b96d-43b6-b955-aa67afa10d90"}, "source": ["The authors seems to use a different lexical field, that gives us idea about using a Tfidf over texts."], "cell_type": "markdown"}, {"metadata": {"_uuid": "05f39df026500f22f4a12bd4d4079ecbb4390b0d", "_cell_guid": "0809b54b-b580-495c-88cb-7936b17b8e86"}, "source": ["## Data Preparation"], "cell_type": "markdown"}, {"metadata": {"_uuid": "4a69d64ce658fb03005121ea9bdecfaa0da859f4", "_cell_guid": "dfafd151-0680-43ac-b1e2-df647f99cbcf"}, "source": ["Scikit-Learn doesn't always handle strings, so let's encode authors."], "cell_type": "markdown"}, {"outputs": [], "metadata": {"_uuid": "3e19559d2ced6b16f4f8665e782b7eef6c6f7f81", "collapsed": true, "_cell_guid": "16528ce8-988f-4752-8449-773c4fa2cd28"}, "execution_count": null, "source": ["le = LabelEncoder()\n", "author_encoded = le.fit_transform(train.author)"], "cell_type": "code"}, {"metadata": {"_uuid": "15b7688a5869c57a934768c34e948fc60fa30e18", "_cell_guid": "ac3519d8-c7f5-4642-96e6-aa774a702835"}, "source": ["## Modelling"], "cell_type": "markdown"}, {"metadata": {"_uuid": "cfad2f3ee02c1f0b209bedf2e2fa405f30f0e55d", "_cell_guid": "fd2d3465-6c31-461e-ab5b-21f09e9776ae"}, "source": ["Now, this is what you all have been waiting for (or not ! :) )\n", "Let's split a train and a test dataset to evaluate our algorithm.\n", "I decided to use acurracy as the metric as it is easier to interpret.\n", "Finally, I define the cross validation method that will be used."], "cell_type": "markdown"}, {"outputs": [], "metadata": {"_uuid": "a6d72d92ac5d0d5326c0b66771c81f4b79e01abc", "collapsed": true, "_cell_guid": "8674caba-ef7f-428d-b45f-2bc4a1ec03bc"}, "execution_count": null, "source": ["seed = 12\n", "X_train, X_test, y_train, y_test = train_test_split(train.text, author_encoded, \n", "    test_size=0.3, random_state=seed)\n", "metric = 'accuracy'\n", "kfold = KFold(n_splits=10, random_state=seed)"], "cell_type": "code"}, {"metadata": {"_uuid": "9734a2a4d78a81db05c19f4f512fda277b31a2a7", "_cell_guid": "565de6ce-2454-439c-a99d-e7c505e47488"}, "source": ["### Useful classes from [PyData Seattle 2017](https://channel9.msdn.com/Events/PyData/Seattle2017/BRK03)"], "cell_type": "markdown"}, {"outputs": [], "metadata": {"_uuid": "b500c39f0a221770e1305ed8ac4252a1dffa4407", "collapsed": true, "_cell_guid": "fcde8fa4-b609-484c-9385-dd931929c527"}, "execution_count": null, "source": ["# Return called columns of a DataFrame\n", "class ColumnExtractor(TransformerMixin):\n", "    def __init__(self, cols):\n", "        self.cols = cols\n", "    def transform(self, X):\n", "        Xcols = X[self.cols]\n", "        return Xcols\n", "    def fit(self, X, y=None):\n", "        return self\n", "\n", "# Enables to train an estimator within the pipeline\n", "class ModelTransformer(TransformerMixin):\n", "    def __init__(self, model):\n", "        self.model = model\n", "    def fit(self, *args, **kwargs):\n", "        self.model.fit(*args, **kwargs)\n", "        return self\n", "    def transform(self, X, **transform_params):\n", "        return pd.DataFrame(self.model.predict(X))"], "cell_type": "code"}, {"metadata": {"_uuid": "8147d3a4f6be8146173c4480af7665474988924c", "_cell_guid": "4f3831b2-6379-425a-a117-94b8701e24aa"}, "source": ["### Feature engineering"], "cell_type": "markdown"}, {"outputs": [], "metadata": {"_uuid": "9245246b32e5b10fb00f19dcc8b8071d9cce4caa", "collapsed": true, "_cell_guid": "1ea18256-997a-45d5-8975-5a67d39ea568"}, "execution_count": null, "source": ["# Calculate the length of each text\n", "class LengthTransformer(TransformerMixin):\n", "    def transform(self, X, **transform_params):\n", "        return pd.DataFrame(X.apply(lambda x: len(str(x)))) \n", "    def fit(self, X, y=None, **fit_params):\n", "        return self\n", "    \n", "# Count the number of words in each text\n", "class WordCountTransformer(TransformerMixin):\n", "    def transform(self, X, **transform_params):\n", "        return pd.DataFrame(X.apply(lambda x: len(str(x).split()))) \n", "    def fit(self, X, y=None, **fit_params):\n", "        return self\n", "    \n", "# Count the number of unique words in each text\n", "class UniqueWordCountTransformer(TransformerMixin):\n", "    def transform(self, X, **transform_params):\n", "        return pd.DataFrame(X.apply(lambda x: len(set(str(x).split())))) \n", "    def fit(self, X, y=None, **fit_params):\n", "        return self\n", "    \n", "# Calculate the average length of words in each text\n", "class MeanLengthTransformer(TransformerMixin):\n", "    def transform(self, X, **transform_params):\n", "        return pd.DataFrame(X.apply(lambda x: np.mean([len(w) for w in str(x).split()]))) \n", "    def fit(self, X, y=None, **fit_params):\n", "        return self\n", "\n", "# Count the number of punctuation in each sentence\n", "class PunctuationCountTransformer(TransformerMixin):\n", "    def transform(self, X, **transform_params):\n", "        return pd.DataFrame(X.apply(lambda x: len([p for p in str(x) if p in punctuation]))) \n", "    def fit(self, X, y=None, **fit_params):\n", "        return self\n", "\n", "# Count the number of unique words in each text\n", "class StopWordsCountTransformer(TransformerMixin):\n", "    def transform(self, X, **transform_params):\n", "        return pd.DataFrame(X.apply(lambda x: len([sw for sw in str(x).lower().split() if sw in set(stopwords.words(\"english\"))]))) \n", "    def fit(self, X, y=None, **fit_params):\n", "        return self"], "cell_type": "code"}, {"metadata": {"_uuid": "eefa413df8e32a7554ab5c37d441cf60674bd92a", "_cell_guid": "6c0d9e91-a189-4de0-8d62-0d67cb97a456"}, "source": ["The idea is to feed the classifier with the count of commas, the length of sentences, counts of words but also Tf-Idf."], "cell_type": "markdown"}, {"outputs": [], "metadata": {"_uuid": "af3d03c30deaed4d6078823eb9c2afe9582f2293", "collapsed": true, "_cell_guid": "5feee8fb-b6c2-4710-92f1-4d7bbf37ebcb"}, "execution_count": null, "source": ["pipeline = Pipeline([\n", "    ('features', FeatureUnion([\n", "        ('text_length', LengthTransformer()),\n", "        ('word_count', WordCountTransformer()),\n", "        ('mean_length', MeanLengthTransformer()),\n", "        ('punctuation_count', PunctuationCountTransformer()),\n", "        ('stop_words_count', StopWordsCountTransformer()),\n", "        ('count_vect', CountVectorizer(lowercase=False)),\n", "        ('tf_idf', TfidfVectorizer())\n", "    ])),\n", "  ('classifier', XGBClassifier(objective='multi:softprob', random_state = 12, eval_metric='mlogloss'))\n", "])"], "cell_type": "code"}, {"metadata": {"_uuid": "9006bd24ed9fb8a9ce3a8bb10a67f6b54934dcef", "_cell_guid": "325a455e-2a28-4c37-8be0-a52ff0b2b92b"}, "source": ["I really like pipelines, it enables to get rid off useless code lines between steps, and it is really clear to interpret."], "cell_type": "markdown"}, {"metadata": {"_uuid": "1a98d18a70c7e1cd52fdea2022b3498fd5356c28", "_cell_guid": "af7480fe-581b-4aeb-a2a2-a5ad66c6f6b3"}, "source": ["## Evaluation"], "cell_type": "markdown"}, {"outputs": [], "metadata": {"_uuid": "0703482053522b1455ff5e213c7fefe7c4b24587", "_cell_guid": "77d4d58e-03aa-4deb-88e6-4a4a063fd7bf"}, "execution_count": null, "source": ["clf_pipe = pipeline.fit(X_train, y_train)\n", "score_pipe = cross_val_score(clf_pipe, X_train, y_train, cv=kfold, scoring=metric)\n", "print(\"Mean score = %.3f, Std deviation = %.3f\"%(np.mean(score_pipe),np.std(score_pipe)))\n", "score_pipe_test = clf_pipe.score(X_test,y_test)\n", "print(\"Mean score = %.3f, Std deviation = %.3f\"%(np.mean(score_pipe_test),np.std(score_pipe_test)))"], "cell_type": "code"}, {"outputs": [], "metadata": {"_uuid": "ea916c98ed75a8fb0af402b5cd1dcf2e9177ec76", "_cell_guid": "88c4f14f-4fed-4c3f-b57d-3ebc600118ec"}, "execution_count": null, "source": ["conf_mat = confusion_matrix(y_test, clf_pipe.predict(X_test))\n", "sns.heatmap(conf_mat, annot=True)\n", "plt.xticks(range(3), ('EAP', 'HPL', 'MWS'), horizontalalignment='left')\n", "plt.yticks(range(3), ('EAP', 'HPL', 'MWS'), rotation=0)"], "cell_type": "code"}, {"metadata": {"_uuid": "fe6d6133bb3c7bfa4d95311905229d99d05b8c1c", "_cell_guid": "80061a6c-c94b-4bc7-b8c5-6eca01744c19"}, "source": ["We can see that it is quite well classified, but we can see that EAP is chosen too often.\n", "![](https://media.giphy.com/media/azZYZhswISHw4/giphy.gif)"], "cell_type": "markdown"}, {"metadata": {"_uuid": "1d207a6c253f2a33b8a52e5f811712f3e8037c19", "_cell_guid": "714822e1-6c2b-48e4-b3a3-8a07893b79e7"}, "source": ["## Submission"], "cell_type": "markdown"}, {"outputs": [], "metadata": {"_uuid": "a62948761ef2579aa1ff04ac9ede1253c845b62a", "scrolled": false, "_cell_guid": "e1e83b35-7c32-482d-8f31-930856402db4"}, "execution_count": null, "source": ["target_names = ['EAP', 'HPL', 'MWS']\n", "y_pred = pd.DataFrame(clf_pipe.predict(test.text), columns=target_names)\n", "submission = pd.concat([test[\"id\"],y_pred], 1)\n", "submission.to_csv(\"./submission.csv\", index=False)"], "cell_type": "code"}, {"metadata": {"_uuid": "4c74a259da6d661359b4ee7880248dae01bf88ad", "_cell_guid": "9cc8428a-0631-4b08-a225-6263384a68c4"}, "source": ["## Comments"], "cell_type": "markdown"}, {"metadata": {"_uuid": "922cbcd6ecbd9b0cb4ffedab038ff73778ecf0c9", "_cell_guid": "b8d47afc-37bb-4207-ad0e-515c57aeef54"}, "source": ["* I scored 0.47318 as multiclass loss. Despise the fact it is well classified, probabilities need to be refined.\n", "* We could try to study in what extent the use of nouns or adverbs could differenciate the authors\n", "* Open to ideas\n", "* Open to advice\n", "* Hope you enjoyed :)"], "cell_type": "markdown"}, {"metadata": {"_uuid": "01a5e23507d0cd5dfe259484888144a9fc44143c", "_cell_guid": "7b6482b1-1e1a-4506-85cc-d2dbacc877c9"}, "source": ["![](https://media.giphy.com/media/3o7btQsLqXMJAPu6Na/giphy.gif)"], "cell_type": "markdown"}], "metadata": {"kernelspec": {"language": "python", "display_name": "Python 3", "name": "python3"}, "language_info": {"codemirror_mode": {"version": 3, "name": "ipython"}, "nbconvert_exporter": "python", "file_extension": ".py", "pygments_lexer": "ipython3", "version": "3.6.3", "mimetype": "text/x-python", "name": "python"}}}