{"cells":[{"metadata":{"_uuid":"43fb18e001c33257edd80317bf5fe51be3173ea7"},"cell_type":"markdown","source":"## Baseline Random Forest: demonstrating `get_class_bounds()`"},{"metadata":{"_uuid":"03fd35e2eb679fd11a1021e3c429d5e285818720"},"cell_type":"markdown","source":"This kernel uses Kseniia Palin's kernel [baseline random forest](https://www.kaggle.com/beloruk1/baseline-random-forest) to demonstrate a routine, `get_class_bounds()`, that maps real-valued ordinal classes to integer classes based on the known class-distribution of the target y values.\n\nComments have been added to Palin's code but otherwise the actual data preparation and model fitting are left as-is; the purpose of this kernel is to demonstrate `get_class_bounds()` rather than optimize the model. Note that Palin's implementation here generates Test predictions for each of the k-folds and averages those predictions; this is different from fitting the whole training set and using that model to generate a single Test prediction.\n\n`get_class_bounds()` is similar to the OptimizedRounder used in other kernels, but it runs more quickly and it may be less prone to overfitting. It could also be used in OptimizedRounder to set the initial boundary values, e.g., in place of `initial_coef = [0.5, 1.5, 2.5, 3.5]`.\n\n(v7) In the PetFinder data, class 0 seems unique/harder-to-predict: added an option to adjust the fraction that are assigned to class 0; loop over this fraction and select its best value, this is essentially a crude one-bin OptimizedRounder ;-) <br>\n(v8) Added more comments to the code and also show plots of kappa, accuracy, and MSE vs the class0 fraction; these plots show, as expected, that kappa does not vary in the same way as accuracy or MSE.\n"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport json\nfrom pandas.io.json import json_normalize    \nimport os\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import cohen_kappa_score\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import accuracy_score\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"68e12ade7c3cd7ede14aa1db022a7ce0874fa38e"},"cell_type":"markdown","source":"## Read in the Data"},{"metadata":{"trusted":true,"_uuid":"3be8018f52db68a18023ddbf400126cd7997017e"},"cell_type":"code","source":"# Show the contents of the input directory\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"41b2071224e646414db20079a340856ac2c7b809","trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/train/train.csv')\ntest = pd.read_csv('../input/test/test.csv')\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"df359f81989530b68af3efe95b9cd7694bbba828","trusted":true},"cell_type":"code","source":"# Define routines to read in the Training,Test sentiment score and magnitude;\n# 0,0 is returned if there is no file found.\n# Note that when used the argument fn will be one row of a dataframe\n# in shich case fn['PetID'] is the PetId.\n\ndef readFile(fn):\n    file = '../input/train_sentiment/'+fn['PetID']+'.json'\n    if os.path.exists(file):\n        with open(file) as data_file:    \n            data = json.load(data_file)  \n\n        df = json_normalize(data)\n        mag = df['documentSentiment.magnitude'].values[0]\n        score = df['documentSentiment.score'].values[0]\n        return pd.Series([mag,score],index=['mag','score']) \n    else:\n        return pd.Series([0,0],index=['mag','score'])\n    \ndef readTestFile(fn):\n    file = '../input/test_sentiment/'+fn['PetID']+'.json'\n    if os.path.exists(file):\n        with open(file) as data_file:    \n            data = json.load(data_file)  \n\n        df = json_normalize(data)\n        mag = df['documentSentiment.magnitude'].values[0]\n        score = df['documentSentiment.score'].values[0]\n        return pd.Series([mag,score],index=['mag','score']) \n    else:\n        return pd.Series([0,0],index=['mag','score'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7cfbbd598b4f8d548cef0c6fe4c8ec3616419f40","trusted":true},"cell_type":"code","source":"# Here the routines above are applied to each row of the dataframes.\n# This is done using panadas' `apply()` with a small \"anonymous function\" defined with a python `lambda`.\n# Note that just `train` could be used inplace of `train[['PetID']]`,\n# this would make it clearer that x is a row of the dataframe and not the PetID value.\n\ntrain[['SentMagnitude', 'SentScore']] = train[['PetID']].apply(lambda x: readFile(x), axis=1)\ntest[['SentMagnitude', 'SentScore']] = test[['PetID']].apply(lambda x: readTestFile(x), axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cc0e4f06f2192409a1f0b42a7ef7d28ef5353adb"},"cell_type":"markdown","source":"## Do Machine Learning"},{"metadata":{"_uuid":"4dd7fc2487852a7c8bb347a427d420ed0be2c533","trusted":true},"cell_type":"code","source":"# Setup the training X, y, and test X\ntrain_X = train.drop(['Name', 'Description', 'RescuerID', 'PetID', 'AdoptionSpeed'], axis=1)\ntrain_y = train['AdoptionSpeed']\ntest_X = test.drop(['Name', 'Description', 'RescuerID', 'PetID'], axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"97ed9ee76b7feffde769ea256168506e5087fba6","trusted":true},"cell_type":"code","source":"# Define what will be the final predicted train and test values\ntrain_meta = np.zeros(train_y.shape)\ntest_meta = np.zeros(test_X.shape[0])\n\n# Choose and initialize a model.\nclf = RandomForestClassifier(bootstrap=True, criterion = 'gini', max_depth=80,\n                             max_features='auto', min_samples_leaf=5,\n                             min_samples_split=5, n_estimators=200)\n\n# Divide the training data into k-folds, k=4 here.\nsplits = list(StratifiedKFold(n_splits=4, shuffle=True, random_state=1812).split(train_X, train_y))\n\n# Loop over the folds and fit the model to the fold's training data.\n# Then evaluate that model on i) the validation data of that fold, \n# and ii) on all of the test data.\nfor idx, (train_idx, valid_idx) in enumerate(splits):\n        # The training and validation sets for this fold\n        X_train = train_X.iloc[train_idx]\n        y_train = train_y[train_idx]\n        X_val = train_X.iloc[valid_idx]\n        y_val = train_y[valid_idx]\n        \n        # Fit the model\n        clf.fit(X_train, y_train)\n        \n        # Look at the validation kappa and accuracy with classes right from the model\n        y_pred = clf.predict(X_val)\n        print(\"Fold {}: accuracy = {:.1f}%, kappa = {:.4f}  (no boundary adjustment)\".format(idx,\n                                100.0*accuracy_score(y_val, y_pred),     \n                                cohen_kappa_score(y_val, y_pred, weights='quadratic')))\n        #\n        # Assign real-valued classes in addition to the integer classes of y_pred.\n        # Start with the predicted probabilities by class\n        y_probs = clf.predict_proba(X_val)\n        # and get the class values (use a copy incase we change values)\n        class_vals = clf.classes_.copy()\n        # Change the ordinal weight of class 0 to be -1 as suggested by the plot in discussion:\n        # https://www.kaggle.com/c/petfinder-adoption-prediction/discussion/76265\n        # Does mot make much difference, though.\n        class_vals[0] = -1\n        # Create the float class values as the probability-weighted class\n        # Here a python \"list comprehension\" is used rather than a loop.\n        y_floats = [sum(y_probs[ix]*class_vals) for ix in range(len(y_probs[:,0]))]\n        #   \n        # Save these y_float values instead of the y_pred integers;\n        ##train_meta[valid_idx] = y_pred.reshape(-1)\n        train_meta[valid_idx] = y_floats\n        # the predictions for just this validation fold are saved in the train_meta array;\n        # looping over all folds will provide one prediction for each training sample.\n\n        # Now use this fold's same model to generate Test predictions.\n        ##y_test = clf.predict(test_X)\n        # Instead of integer classes, get the predicted probabilites\n        test_probs = clf.predict_proba(test_X)\n        # and turn these into float class values.\n        # Unlike the validation case, we get a test prediction from every fold,\n        # so those float predictions are averaged. python list comprehension is used again.\n        ##test_meta += y_test.reshape(-1) / len(splits)\n        test_meta += np.array([sum(test_probs[ix]*class_vals) for\n                               ix in range(len(test_probs[:,0]))]) / len(splits)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d85073373fc0a8d8519731477cede0f9a1e6e47c"},"cell_type":"markdown","source":"### Adjusting the Class Boundaries"},{"metadata":{"_uuid":"91bc37feed6f0a1ba8cd853ef5e4786ab4bf4eed","trusted":true},"cell_type":"code","source":"# Next two routines are a way to map float regression values to ordinal classes\n# by making use of the known distribution of the training classes.\n\n# In the following, y_pred is a floating value, e.g., the output of a regression to the class.\n# Many sklearn _classifiers_ can also provide probabilities of the classes which\n# can be turned into a floating value as the probability-weighted class, e.g.,:\n#       y_probs = clf.predict_proba(X_val)\n#       # The class values; use a copy incase we want to modify the values\n#       class_vals = clf.classes_.copy()\n#       y_floats = [sum(y_probs[ix]*class_vals) for ix in range(len(y_probs[:,0]))]\n\n\ndef get_class_bounds(y, y_pred, N=5, class0_fraction=-1):\n    \"\"\"\n    Find boundary values for y_pred to match the known y class percentiles.\n    Returns N-1 boundaries in y_pred values that separate y_pred\n    into N classes (0, 1, 2, ..., N-1) with same percentiles as y has.\n    Can adjust the fraction in Class 0 by the given factor (>=0), if desired. \n    \"\"\"\n    ysort = np.sort(y)\n    predsort = np.sort(y_pred)\n    bounds = []\n    for ibound in range(N-1):\n        iy = len(ysort[ysort <= ibound])\n        # adjust the number of class 0 predictions?\n        if (ibound == 0) and (class0_fraction >= 0.0) :\n            iy = int(class0_fraction * iy)\n        bounds.append(predsort[iy])\n    return bounds\n\ndef assign_class(y_pred, boundaries):\n    \"\"\"\n    Given class boundaries in y_pred units, output integer class values\n    \"\"\"\n    y_classes = np.zeros(len(y_pred))\n    for iclass, bound in enumerate(boundaries):\n        y_classes[y_pred >= bound] = iclass + 1\n    return y_classes.astype(int)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"00efbb982d58ec4b0d153e9c6ca7f057177f3589","trusted":true},"cell_type":"code","source":"# Look at the histogram of the predicted float class values.\nplt.hist(train_meta, bins=50)\nplt.title(\"Training: meta float values\")\nplt.xlabel(\"Training y float values\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"aa1b95ba80a6643917adf2d8982c80f9879d831d","trusted":true},"cell_type":"code","source":"# This cell calculates and plots the kappa (and MSE) vs the class0 fraction adjustment.\n# Note that MSE prefers (lower MSE) a class0 fraction near/at 0,\n# whereas kappa prefers (higher kappa) a fraction near 1.\n# Then the class0 fraction that gives best training kappa is selected.\n\n# Save values of kappa, MSE, and accuracy vs the class0 fraction\nkappas = []\nmses = []\naccurs = []\n# fractions to try... (could go larger than 1 if desired.)\ncl0fracs = np.array(np.arange(0.01,1.001,0.01))\nfor cl0frac in cl0fracs:\n    boundaries = get_class_bounds(train_y, train_meta, class0_fraction=cl0frac)\n    train_meta_ints = assign_class(train_meta, boundaries)\n    kappa = cohen_kappa_score(train['AdoptionSpeed'], train_meta_ints, weights='quadratic')\n    kappas.append(kappa)\n    mse = mean_squared_error(train['AdoptionSpeed'], train_meta_ints)\n    mses.append(mse)\n    accur = accuracy_score(train['AdoptionSpeed'], train_meta_ints)\n    accurs.append(accur)\n    \n# Use the class0 fraction that gives the highest training kappa\nifmax = np.array(kappas).argmax()\ncl0frac = cl0fracs[ifmax]\n\nprint(\"Best kappa for class0 fraction = {:.4f}\".format(cl0frac))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"18f546cede272b534be51af7b7a96d80b55a4548"},"cell_type":"code","source":"# Plots to show the kappa, MSE, and Accuracy vs class0 fraction\n\nplt.plot(cl0fracs, kappas)\n# indicate the highest-kappa point\nplt.plot([cl0frac],[kappas[ifmax]],marker=\"o\",color=\"green\")\nplt.title(\"Training: kappa vs class0_fraction\")\nplt.xlabel(\"class0_fraction\")\nplt.ylabel(\"kappa\")\nplt.show()\n\nplt.plot(cl0fracs, mses)\nplt.title(\"Training: MSE vs class0_fraction\")\nplt.xlabel(\"class0_fraction\")\nplt.ylabel(\"MSE\")\nplt.show()\n\nplt.plot(cl0fracs, accurs)\nplt.title(\"Training: Accuracy vs class0_fraction\")\nplt.xlabel(\"class0_fraction\")\nplt.ylabel(\"Accuracy\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"29f6314f570a8a1f33699d67af4d7aa597d7d070","trusted":true},"cell_type":"code","source":"# Can skip the class0_fraction adjustment and plotting cells above;\n# can delete those two cells and just uncomment this line:\n##cl0frac = 1.0\n\nprint(\"Using class0_fraction = {:.4f}, gives boundaries:\".format(cl0frac))\nboundaries = get_class_bounds(train_y, train_meta, class0_fraction=cl0frac)\nprint(boundaries)\n\ntrain_meta_ints = assign_class(train_meta, boundaries)\nkappa = cohen_kappa_score(train_y, train_meta_ints, weights='quadratic')\n\nprint(\"Adjusted boundaries give:\")\nprint(\"kappa = {:.4f}  (with accuracy = {:.1f}%)\".format(kappa,\n                                100.0*accuracy_score(train_y, train_meta_ints)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0383d1129313a839fb1a24813d338dd03658b7d8"},"cell_type":"code","source":"# Confusion Matrix\ncon_mat = confusion_matrix(train_y, train_meta_ints)\n\n# Look at the number that are on the diagonal (exact agreement)\ndiag = 0.0\nfor id in range(5):\n    diag += con_mat[id,id]\nprint(\"\\nConfusion matrix - Columns are prediced 0, predicted 1, etc.\\n\")\nprint(con_mat)\nprint(\"\")\nprint(\"\\n{2:.2f}% = {0}/{1} are on the diagonal (= accuracy)\".format(\n        int(diag), con_mat.sum(), 100.0*diag/con_mat.sum()))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"44d65208341988e9f70d74c98185b86a9a6a01ff","trusted":true},"cell_type":"code","source":"plt.hist(train_meta_ints, bins=40, color='blue')\nplt.hist(train_y, bins=20, bottom=0.0, alpha=0.2)\nplt.title(\"Train: Boundary-based Predictions\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1f357aa7324ae0caaf4b1714c80a7c323e0e801a"},"cell_type":"markdown","source":"## Generate and Output the Test Predictions"},{"metadata":{"_uuid":"ae868bd6da4f1bfdf61939fd2ac5ab545aa4e1ce","trusted":true},"cell_type":"code","source":"plt.hist(test_meta, bins=50)\nplt.title(\"Test: meta float values\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"23937294d0094367dce0807d7a9ebcecc806079c","trusted":true},"cell_type":"code","source":"# Map the test values to integers using the training boundaries\ntest_meta_ints = assign_class(test_meta, boundaries)\nplt.hist(test_meta_ints.astype(int), bins=50)\nplt.title(\"Test: Boundary-based Predictions\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9f865fd1da39bd2ea7a823100c1419ef2deb235a","trusted":true},"cell_type":"code","source":"sub = pd.read_csv('../input/test/sample_submission.csv')\nsub['AdoptionSpeed'] = test_meta_ints\nsub['AdoptionSpeed'] = sub['AdoptionSpeed'].astype(int)\nsub.to_csv(\"submission.csv\", index=False)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2d4bce65ac65dad7aaa14352e86ce2ef26686acb","trusted":true},"cell_type":"code","source":"!head -5 submission.csv\n!echo ...\n!tail -5 submission.csv","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7f5d2d19c10009347fc101a96d49be62b86d613e","collapsed":true,"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"3a713504d155f9a6cbfd6929fa0816e2627abb2b"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":1}