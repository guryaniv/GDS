{"cells":[{"metadata":{"collapsed":true,"_uuid":"e9602b13261afa9897bb74827af2c072d0fc6e87"},"cell_type":"markdown","source":"# Credit Card Fraud Detection Using SMOTE"},{"metadata":{"_uuid":"21e762d9ebec596a92b54cc99e1b02f6a1a9ff1d"},"cell_type":"markdown","source":"This is the 2nd approach I'm sharing for credit card fraud detection. Refer to my earlier kernel @ https://www.kaggle.com/hrao768/gaussian-distrib-for-anomaly-detection-f1-83\n\nWe are going to explore resampling techniques like oversampling in this 2nd approach. Here are the key steps involved in this kernel.\n\n    1) Balance the dataset by oversampling fraud class records using SMOTE\n    \n    2) Train the model using oversampled data by Random Forest\n    \n    3) Evaluate the performance of this model based on predictions on original imbalanced test data\n        \n    4) Add cluster segments to the original train and test data using K-Means algorithm\n    \n    5) Repeat the steps 1, 2 & 3 and see if the performance of Random Forest has improved by adding clusters\n    \n    6) Finally evaluate our model performance and check if it can generalize well on the unseen data using K-fold cross validation on original train data\n    "},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"5d8315bb3719281edce0cb07759197ec7f4e6835"},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport matplotlib.gridspec as gridspec\nimport seaborn as sns\n\nfrom sklearn.base import BaseEstimator,TransformerMixin\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split,cross_val_predict,cross_val_score, GridSearchCV,RandomizedSearchCV\nfrom sklearn.preprocessing import StandardScaler,MinMaxScaler\nfrom sklearn.metrics import confusion_matrix,classification_report,f1_score,recall_score,precision_score,accuracy_score,precision_recall_curve,roc_curve,roc_auc_score\n\nfrom sklearn.linear_model import SGDClassifier, LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, VotingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\n\nfrom sklearn.cluster import KMeans\nfrom imblearn.over_sampling import SMOTE\n\nfrom collections import Counter\n\nplt.rcParams['axes.labelsize'] = 14\nplt.rcParams['xtick.labelsize'] = 12\nplt.rcParams['ytick.labelsize'] = 12\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport random\nrandom.seed(0)","execution_count":69,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"23b809b61e716cb7b45b2529d04da09f1627b9ea"},"cell_type":"code","source":"def data_preparation(data):\n    features = data.iloc[:,0:-1]\n    label = data.iloc[:,-1]\n    x_train,x_test,y_train,y_test = train_test_split(features,label,test_size=0.2,random_state=0)\n\n    #Standarad scaler is not applied since all the features are outcomes of PCA and are already standardized.\n    #sc = StandardScaler()\n    #x_train = sc.fit_transform(x_train)\n    #x_test = sc.transform(x_test)\n    \n    print(\"Length of training data\",len(x_train))\n    print(\"Length of test data\",len(x_test))\n    return x_train,x_test,y_train,y_test\n    ","execution_count":70,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"46b4c608c954cd5d67b6f7537bdbec44bdce3923"},"cell_type":"code","source":"def build_model_train_test(model,x_train,x_test,y_train,y_test):\n    model.fit(x_train,y_train)\n\n    y_pred = model.predict(x_train)\n    \n    print(\"\\n----------Accuracy Scores on Train data------------------------------------\")\n    print(\"F1 Score: \", f1_score(y_train,y_pred))\n    print(\"Precision Score: \", precision_score(y_train,y_pred))\n    print(\"Recall Score: \", recall_score(y_train,y_pred))\n\n\n    print(\"\\n----------Accuracy Scores on Test data------------------------------------\")\n    y_pred_test = model.predict(x_test)\n    \n    print(\"F1 Score: \", f1_score(y_test,y_pred_test))\n    print(\"Precision Score: \", precision_score(y_test,y_pred_test))\n    print(\"Recall Score: \", recall_score(y_test,y_pred_test))\n\n    #Confusion Matrix\n    plt.figure(figsize=(18,6))\n    gs = gridspec.GridSpec(1,2)\n\n    ax1 = plt.subplot(gs[0])\n    cnf_matrix = confusion_matrix(y_train,y_pred)\n    row_sum = cnf_matrix.sum(axis=1,keepdims=True)\n    cnf_matrix_norm =cnf_matrix / row_sum\n    sns.heatmap(cnf_matrix_norm,cmap='YlGnBu',annot=True)\n    plt.title(\"Normalized Confusion Matrix - Train Data\")\n\n    ax2 = plt.subplot(gs[1])\n    cnf_matrix = confusion_matrix(y_test,y_pred_test)\n    row_sum = cnf_matrix.sum(axis=1,keepdims=True)\n    cnf_matrix_norm =cnf_matrix / row_sum\n    sns.heatmap(cnf_matrix_norm,cmap='YlGnBu',annot=True)\n    plt.title(\"Normalized Confusion Matrix - Test Data\")\n","execution_count":71,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"b6d203ac506bbca367e7c406af10c5cb0e858278"},"cell_type":"code","source":"#Loading Dataset\ncc_dataset = pd.read_csv(\"../input/creditcard.csv\")","execution_count":72,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"835a66778f9bc465a7d199e6ebab5086549a0e93"},"cell_type":"code","source":"cc_dataset.shape","execution_count":73,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"9f417db47678c6101d1a5aea04c9925b4c6d6506"},"cell_type":"code","source":"cc_dataset.head()","execution_count":74,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"28535480d819fb96ab0df4ccce94f0695386f901"},"cell_type":"code","source":"cc_dataset.describe()","execution_count":75,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"1452dac38eee5ccc8ccb9049dba9793dda3a8437"},"cell_type":"code","source":"#Code for checking if any feature has null values. Here the output confirms that there are no null values in this data set.\ncc_dataset.isnull().any()","execution_count":76,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"f6e26be2943e368245e4a5467597afcec0b29fbb"},"cell_type":"code","source":"#Counts for each class in the dataset. As you can see, we have only 492 (0.17%) fraud cases out of 284807 records. Remaining 284315 (99.8%) of the records belong to genuine cases.\n#So the dataset is clearly imbalanced!\ncc_dataset['Class'].value_counts()","execution_count":77,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"0b2c8f57074ad69807f8946b0a8c96d73e1174b8"},"cell_type":"code","source":"#Data Visualization for checking the distribution for Genuine cases & Fraud cases for each feature\nv_features = cc_dataset.columns\nplt.figure(figsize=(12,31*4))\ngs = gridspec.GridSpec(31,1)\n\nfor i, col in enumerate(v_features):\n    ax = plt.subplot(gs[i])\n    sns.distplot(cc_dataset[col][cc_dataset['Class']==0],color='g',label='Genuine Class')\n    sns.distplot(cc_dataset[col][cc_dataset['Class']==1],color='r',label='Fraud Class')\n    ax.legend()\nplt.show()","execution_count":78,"outputs":[]},{"metadata":{"_uuid":"4be5883e9d29d48f7eaa21c5b2f47dc5c9dbd05c"},"cell_type":"markdown","source":"Feature selection: \n    1) We can see distribution of anomalous transactions (class = 1) is matching with distribution of genuine transactions (class = 0) for V28','V27','V26','V25','V24','V23','V22','V20','V15','V13','V8' features. It is better to delete these features as they may not be useful in finding anomalous records.\n    2) Time is also not useful variable since it contains the seconds elapsed between the transaction for that record and the first transaction in the dataset. So the data is in increasing order always.\n    \nLet us remove the feature 'Time' for now and build the model."},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"9047752861d928a5afcff2a88c056d1d272febbc"},"cell_type":"code","source":"cc_dataset.drop(labels = ['Time'], axis = 1, inplace=True)","execution_count":79,"outputs":[]},{"metadata":{"_uuid":"567bfbff2ff2703525b7175015cd706e7d2d00fb"},"cell_type":"markdown","source":"The feature 'Amount' has higher standard deviation of 250, which indicate the spread is very high & also we might have outliers in the data. So let us go for feature scaling for Amount variable using StandardScaler()."},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"9c59fe7e0a0d559edb2ea05e4a8dc53e9e87b775"},"cell_type":"code","source":"cc_dataset['Amount'] = StandardScaler().fit_transform(cc_dataset['Amount'].reshape(-1,1))","execution_count":80,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"dbb7ca07a26ca02e91f709ebd7604040717bd5d2"},"cell_type":"code","source":"#Data Preparation\nx_train,x_test,y_train,y_test = data_preparation(cc_dataset)","execution_count":81,"outputs":[]},{"metadata":{"_uuid":"4333494f5bc08e63a8a572268ce1f2a26853e882"},"cell_type":"markdown","source":"imbalanced-learn is a python package offering a number of re-sampling techniques commonly used in datasets showing strong between-class imbalance. It is compatible with scikit-learn and is part of scikit-learn-contrib projects.\n\nimbalanced-learn is currently available on the PyPi's repository and you can install it via pip: pip install -U imbalanced-learn\n\nI'm going to use Synthetic Minority Oversampling Technique (SMOTE) to balance the dataset here."},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"1c8489aa8cc6e4c696898d2f15edd47140a9f1de"},"cell_type":"code","source":"os = SMOTE(random_state=0)","execution_count":82,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"6f3f7c866f2036c65ef2a7f195c34ac0b6e695fe"},"cell_type":"code","source":"#Generate the oversample data\nos_res_x,os_res_y=os.fit_sample(x_train,y_train)\n#Counts of each class in oversampled data\nprint(sorted(Counter(os_res_y).items()))","execution_count":83,"outputs":[]},{"metadata":{"_uuid":"862c942a25edea690d3e59e3f0e860b1f44b67d6"},"cell_type":"markdown","source":"We can see that fraud records are imputed and brought close to genuine records in this oversampled data using SMOTE. Hence both classes are equally distributed now."},{"metadata":{"trusted":false,"_uuid":"1c6878d224574724f3ec4293e3ed8d7fd24ec4f3"},"cell_type":"code","source":"#RandomForest for training over-sampled data set. \nrnd_clf = RandomForestClassifier(n_estimators=100,criterion='gini',n_jobs=-1, random_state=0)\n#Train the model on oversampled data and check the performance on original test data\nbuild_model_train_test(rnd_clf,os_res_x,x_test,os_res_y,y_test)","execution_count":84,"outputs":[]},{"metadata":{"_uuid":"a4faaedec81c4bb269910a9219b5fe997a12fc16"},"cell_type":"markdown","source":"RandomForest has given good results after balancing the training data using synthetic over-sampling approach: F1 score of 85 on orignal test data (without oversmapling)\n\nI'd like to try K-means clustering to identify the clusters in the dataset, which could improve the predictive power in fraud detection."},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"6cc2b1553431954ea9de7b7a265e53932ec34b81"},"cell_type":"code","source":"# #Elbow Curve for identifying the best number of clusters\n# wcss = [] # Within Cluster Sum of Squares\n# for k in range(1, 21):\n#     kmeans = KMeans(n_clusters = k, init = 'k-means++', random_state = 0)\n#     kmeans.fit(x_train)\n#     wcss.append(kmeans.inertia_)\n# plt.plot(range(1, 21), wcss)\n# plt.title('The Elbow Method')\n# plt.xlabel('Number of clusters - k')\n# plt.ylabel('WCSS')\n# plt.show()","execution_count":85,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"2a9dfff259d8df54642d4325884915488610d4fe"},"cell_type":"code","source":"#Clustering with 11 clusters. I used the elbow method to derive on number of clusters. I commented the above code to save the run time\nkmeans_best = KMeans(n_clusters = 11, init = 'k-means++', random_state = 0)\ntrain_clusters = kmeans_best.fit_predict(x_train)","execution_count":86,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"64c46fe750463ea59e315bf909d35448193ce027"},"cell_type":"code","source":"#Merge clusters with other input features on Train Data\nx_train2 = np.c_[(x_train,train_clusters )]\nx_train2.shape","execution_count":87,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"c3f7cd8df2c14e084197f4c4e150a38616f65b5e"},"cell_type":"code","source":"#Predict the cluster for test data & merge it with other features\ntest_clusters = kmeans_best.predict(x_test)\nx_test2 = np.c_[(x_test,test_clusters )]\nx_test2.shape","execution_count":88,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"d2a761d632f81d9c6a8883fe48194619db2cf519"},"cell_type":"code","source":"#Generate the oversample data for training purpose\nos_res_x2,os_res_y2=os.fit_sample(x_train2,y_train)\n#Counts of each class in oversampled data\nprint(sorted(Counter(os_res_y2).items()))\n","execution_count":89,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"a55058a77205e73ba7aef10342f83a9ad4cc47f0"},"cell_type":"code","source":"#RandomForest for training over-sampled data set. \nrnd_clf2 = RandomForestClassifier(n_estimators=100,criterion='gini',n_jobs=-1, random_state=0)\n#Train the model on oversampled data and check the performance on actual test data\nbuild_model_train_test(rnd_clf2,os_res_x2,x_test2,os_res_y2,y_test)\n","execution_count":90,"outputs":[]},{"metadata":{"_uuid":"fe530939ae0191a62557ff613c3ce1c901748ae7"},"cell_type":"markdown","source":"Post adding the clusters to the dataset, the performance of RandomForest model has improved little bit: F1 score of 87 and recall score of 85 on the orignal test data (without oversmapling).\nLet us check the consistency of this model by using cross validation scores based on the original train data."},{"metadata":{"trusted":false,"_uuid":"56380176dac8e4bd5ce1121c4e62afccf2786a37"},"cell_type":"code","source":"#Let us check cross validation scores on the orginal train data\ncv_score = cross_val_score(rnd_clf2,x_train2,y_train,cv=5,scoring='f1')\nprint(\"Average F1 score CV\",cv_score.mean())\n","execution_count":91,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"d0acace2885dbdb6902db7801519db33c3051167"},"cell_type":"code","source":"cv_score = cross_val_score(rnd_clf2,x_train2,y_train,cv=5,scoring='recall')\nprint(\"Average Recall score CV\",cv_score.mean())\n","execution_count":92,"outputs":[]},{"metadata":{"_uuid":"84827791b48786f17ad26de013463ef5ba25fc83"},"cell_type":"markdown","source":"On the cross validation, recall score has gone down little bit. However overall F1-score is still around 85. So we can go ahead with this model."},{"metadata":{"_uuid":"8e46d47eea96ad6cc48b5533cc9c5ce1f33e7695"},"cell_type":"markdown","source":"Conclusion: In general, oversampling techniques like SMOTE should provide better results than normal supervised learning algorithms on imbalanced datasets. We added clustering over the top of SMOTE to identify the patterns better and it has given best results on this dataset."},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"362170767ee935c4c3a254a5a6cb51e2fcc12df0"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"}},"nbformat":4,"nbformat_minor":1}