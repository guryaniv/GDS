{"cells": [{"outputs": [], "cell_type": "code", "metadata": {"collapsed": true}, "execution_count": null, "source": ["import pandas as pd\n", "import numpy as np\n", "from IPython.display import display"]}, {"outputs": [], "cell_type": "code", "metadata": {"collapsed": true}, "execution_count": null, "source": ["# read in data\n", "# train_raw_path = '\\\\\\\\SEAGATE-D4/Documents/My Hoang Nguyen/ML-SDrive/Kaggle/Mercari/train.tsv'\n", "train_raw_path = '../input/mercari-price-suggestion-challenge/train.tsv'\n", "train_raw = pd.read_table(train_raw_path)\n", "\n", "# test_raw_path = '\\\\\\\\SEAGATE-D4/Documents/My Hoang Nguyen/ML-SDrive/Kaggle/Mercari/test.tsv'\n", "test_raw_path = '../input/mercari-price-suggestion-challenge/test.tsv'\n", "test_raw = pd.read_table(test_raw_path)"]}, {"outputs": [], "cell_type": "code", "metadata": {}, "execution_count": null, "source": ["print('train_raw\\n', train_raw.shape)\n", "print(train_raw.dtypes)\n", "display(train_raw.head())\n", "\n", "print('test_raw\\n', test_raw.shape)\n", "print(test_raw.dtypes)\n", "display(test_raw.head())"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# extract features from [item_description] by training a cnn"]}, {"outputs": [], "cell_type": "code", "metadata": {}, "execution_count": null, "source": ["# load Google's pre-trained word2vec\n", "import gensim\n", "# pretrained_word2vec_path = '\\\\\\\\SEAGATE-D4/Documents/My Hoang Nguyen/ML-SDrive/Sentiment Analysis/data/GoogleNews-vectors-negative300.bin'\n", "pretrained_word2vec_path = '../input/word2vecnegative300/GoogleNews-vectors-negative300.bin'\n", "word2vec = gensim.models.KeyedVectors.load_word2vec_format(pretrained_word2vec_path, binary=True)"]}, {"outputs": [], "cell_type": "code", "metadata": {}, "execution_count": null, "source": ["# tokenize text\n", "from keras.preprocessing.text import Tokenizer\n", "\n", "n_words = 20 # top most common words\n", "text = train_raw['item_description'].astype(str).tolist()\n", "tokenizer = Tokenizer(num_words=n_words)\n", "tokenizer.fit_on_texts(text)\n", "\n", "# pad_sequences so they are all of the same length\n", "from keras.preprocessing.sequence import pad_sequences\n", "\n", "sequences = tokenizer.texts_to_sequences(text) # list, same length as data. represent word as rank/index\n", "padded_seq = pad_sequences(sequences)\n", "print('padded_seq.shape', padded_seq.shape)"]}, {"outputs": [], "cell_type": "code", "metadata": {}, "execution_count": null, "source": ["# kfold cv\n", "from sklearn.model_selection import KFold\n", "\n", "x = padded_seq\n", "y = np.asarray(train_raw['price'])\n", "\n", "n_splits = 2\n", "\n", "kf = KFold(n_splits=n_splits)\n", "kf.get_n_splits(x)"]}, {"outputs": [], "cell_type": "code", "metadata": {"collapsed": true}, "execution_count": null, "source": ["# create embedding_matrix to feed in as weights for embedding_layer\n", "word_index = tokenizer.word_index\n", "vocab_size = len(word_index)\n", "EMBEDDING_DIM = 300 # this is from the pretrained vectors\n", "\n", "embedding_matrix = np.zeros((vocab_size + 1, EMBEDDING_DIM))\n", "for word, i in word_index.items():\n", "    if word in word2vec:\n", "        embedding_vector = word2vec[word]\n", "    else:\n", "        embedding_vector = None\n", "    if embedding_vector is not None:\n", "        # words not found in embedding index will be all-zeros.\n", "        embedding_matrix[i] = embedding_vector"]}, {"outputs": [], "cell_type": "code", "metadata": {"collapsed": true}, "execution_count": null, "source": ["# custom loss function\n", "import keras.backend as K\n", "\n", "def rmsle(y_true, y_pred):\n", "    first_log = K.log(K.clip(y_pred, K.epsilon(), None) + 1.)\n", "    second_log = K.log(K.clip(y_true, K.epsilon(), None) + 1.)\n", "    \n", "    return K.sqrt(K.mean(K.square(first_log - second_log), axis=-1))\n", "\n", "import keras\n", "def truncated_normal(seed):\n", "    return keras.initializers.TruncatedNormal(mean=0.0, stddev=0.05, seed=seed)"]}, {"outputs": [], "cell_type": "code", "metadata": {"collapsed": true}, "execution_count": null, "source": ["# create cnn model\n", "from keras.layers import Embedding, Dense, Input, Flatten\n", "from keras.layers import Conv1D, GlobalMaxPooling1D, Concatenate, Dropout\n", "from keras.models import Model\n", "\n", "# parameters\n", "input_length = padded_seq.shape[1] # len (num words) of longest description\n", "seed = 0\n", "filter_sizes = [2,3]\n", "n_filters = 2\n", "dropout_prob = 0.5\n", "\n", "def create_cnn(include_top=True, weights=None):    \n", "\n", "    # input\n", "    sequence_input = Input(shape=(input_length,), dtype='int32', name='input')\n", "    # embedding_layer\n", "    embedding_layer = Embedding(vocab_size + 1, EMBEDDING_DIM, weights=[embedding_matrix], input_length=input_length\n", "                                , name='embedding', trainable=False)(sequence_input)\n", "    # conv layer\n", "    features = []\n", "    i = 0\n", "    for filter_size in filter_sizes:\n", "        i += 1\n", "        # conv layer\n", "        conv = Conv1D(n_filters, filter_size, activation='relu', kernel_initializer=truncated_normal(seed)\n", "                      , name='conv'+str(i))(embedding_layer)\n", "        # global max pooling\n", "        conv = GlobalMaxPooling1D(name='pool'+str(i))(conv)\n", "        # add features together\n", "        features.append(conv)\n", "    # penultimate layer\n", "    nn = Concatenate(name='features')(features)\n", "    if include_top:\n", "        # dropout\n", "        nn = Dropout(dropout_prob, seed=seed, name='dropout')(nn)\n", "        # fully connected layer\n", "        preds = Dense(1, kernel_initializer=truncated_normal(seed), name='output')(nn)\n", "\n", "        model = Model(sequence_input, preds)\n", "    else:\n", "        model = Model(sequence_input, nn)\n", "    \n", "    \n", "    if weights is not None:\n", "        model.set_weights(weights)\n", "        \n", "    return model"]}, {"outputs": [], "cell_type": "code", "metadata": {}, "execution_count": null, "source": ["model = create_cnn()\n", "model.summary()"]}, {"outputs": [], "cell_type": "code", "metadata": {}, "execution_count": null, "source": ["# train cnn\n", "model.compile(loss='msle', optimizer='adadelta', metrics=[rmsle])\n", "\n", "batch_size = 128\n", "epochs = 1\n", "for train_index, val_index in kf.split(x):\n", "    x_train, x_val = x[train_index], x[val_index]\n", "    y_train, y_val = y[train_index], y[val_index]\n", "    model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=epochs, batch_size=batch_size)"]}, {"outputs": [], "cell_type": "code", "metadata": {}, "execution_count": null, "source": ["# predict\n", "# format test data\n", "data = test_raw['item_description'].astype(str).tolist()\n", "sequences = tokenizer.texts_to_sequences(data) # list, same length as data. represent word as rank/index\n", "x_test = pad_sequences(sequences, maxlen=input_length)\n", "print('x_test.shape', x_test.shape)\n", "\n", "# predict\n", "y_test = model.predict(x_test, batch_size=batch_size)\n", "print('y_test.shape', y_test.shape)"]}, {"outputs": [], "cell_type": "code", "metadata": {}, "execution_count": null, "source": ["# submission\n", "submission = pd.concat((test_raw, pd.DataFrame(np.reshape(y_test,(-1,1)), columns=['price'])), axis=1)[['test_id', 'price']]\n", "display(submission.head())\n", "\n", "# export submission\n", "submission.to_csv('first_submission.csv', index=False)"]}, {"outputs": [], "cell_type": "code", "metadata": {"_kg_hide-output": true, "collapsed": true}, "execution_count": null, "source": ["# export submission\n", "submission.to_csv('first_submission.csv', index=False)"]}], "metadata": {"kernelspec": {"language": "python", "name": "python3", "display_name": "Python 3"}, "language_info": {"version": "3.6.2", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "file_extension": ".py", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3"}}, "nbformat_minor": 1, "nbformat": 4}