{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn import preprocessing\n\n\"\"\"\nchanged from https://www.kaggle.com/anycode/simple-nn-baseline\n\nto run this kernel, pip install ultimate first from your custom packages\n\"\"\"\nimport gc, sys\ngc.enable()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"21d781451b4ddadcb4b3a614ed23210d04dff51e"},"cell_type":"code","source":"# Thanks and credited to https://www.kaggle.com/gemartin who created this wonderful mem reducer\ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() \n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() \n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n    \n    return df\n\n#x_train = reduce_mem_usage(x_train)\n#x_test = reduce_mem_usage(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e9dc1b85515fc75712f22cebd8b237075bde9583"},"cell_type":"code","source":"def feature_engineering(is_train=True):\n    if is_train: \n        print(\"processing train.csv\")\n        df = pd.read_csv(\"../input/train_V2.csv\")\n\n        df = df[df['maxPlace'] > 1]\n    else:\n        print(\"processing test.csv\")\n        df = pd.read_csv(\"../input/test_V2.csv\")\n        \n    \n    # df = reduce_mem_usage(df)\n    df['totalDistance'] = df['rideDistance'] + df[\"walkDistance\"] + df[\"swimDistance\"]\n    \n    # df = df[:100]\n    \n    print(\"remove some columns\")\n    target = 'winPlacePerc'\n    features = list(df.columns)\n    features.remove(\"Id\")\n    features.remove(\"matchId\")\n    features.remove(\"groupId\")\n    \n    features.remove(\"matchType\")\n    \n    # matchType = pd.get_dummies(df['matchType'])\n    # df = df.join(matchType)    \n    \n    y = None\n    \n    print(\"get target\")\n    if is_train: \n        y = np.array(df.groupby(['matchId','groupId'])[target].agg('mean'), dtype=np.float64)\n        features.remove(target)\n\n    print(\"get group mean feature\")\n    agg = df.groupby(['matchId','groupId'])[features].agg('mean')\n    agg_rank = agg.groupby('matchId')[features].rank(pct=True).reset_index()\n    \n    if is_train: df_out = agg.reset_index()[['matchId','groupId']]\n    else: df_out = df[['matchId','groupId']]\n\n    df_out = df_out.merge(agg.reset_index(), suffixes=[\"\", \"\"], how='left', on=['matchId', 'groupId'])\n    df_out = df_out.merge(agg_rank, suffixes=[\"_mean\", \"_mean_rank\"], how='left', on=['matchId', 'groupId'])\n    \n    print(\"get group sum feature\")\n    agg = df.groupby(['matchId','groupId'])[features].agg('sum')\n    agg_rank = agg.groupby('matchId')[features].rank(pct=True).reset_index()\n    df_out = df_out.merge(agg.reset_index(), suffixes=[\"\", \"\"], how='left', on=['matchId', 'groupId'])\n    df_out = df_out.merge(agg_rank, suffixes=[\"_sum\", \"_sum_rank\"], how='left', on=['matchId', 'groupId'])\n    \n    df_out=reduce_mem_usage(df_out)\n    \n    #print(\"get group sum feature\")\n    #agg = df.groupby(['matchId','groupId'])[features].agg('sum')\n    #agg_rank = agg.groupby('matchId')[features].agg('sum')\n    #df_out = df_out.merge(agg.reset_index(), suffixes=[\"\", \"\"], how='left', on=['matchId', 'groupId'])\n    #df_out = df_out.merge(agg_rank.reset_index(), suffixes=[\"_sum\", \"_sum_pct\"], how='left', on=['matchId', 'groupId'])\n    \n    print(\"get group max feature\")\n    agg = df.groupby(['matchId','groupId'])[features].agg('max')\n    agg_rank = agg.groupby('matchId')[features].rank(pct=True).reset_index()\n    df_out = df_out.merge(agg.reset_index(), suffixes=[\"\", \"\"], how='left', on=['matchId', 'groupId'])\n    df_out = df_out.merge(agg_rank, suffixes=[\"_max\", \"_max_rank\"], how='left', on=['matchId', 'groupId'])\n    \n    print(\"get group min feature\")\n    agg = df.groupby(['matchId','groupId'])[features].agg('min')\n    agg_rank = agg.groupby('matchId')[features].rank(pct=True).reset_index()\n    df_out = df_out.merge(agg.reset_index(), suffixes=[\"\", \"\"], how='left', on=['matchId', 'groupId'])\n    df_out = df_out.merge(agg_rank, suffixes=[\"_min\", \"_min_rank\"], how='left', on=['matchId', 'groupId'])\n    \n    df_out=reduce_mem_usage(df_out)\n    \n    print(\"get group size feature\")\n    agg = df.groupby(['matchId','groupId']).size().reset_index(name='group_size')\n    df_out = df_out.merge(agg, how='left', on=['matchId', 'groupId'])\n    \n    print(\"get match mean feature\")\n    agg = df.groupby(['matchId'])[features].agg('mean').reset_index()\n    df_out = df_out.merge(agg, suffixes=[\"\", \"_match_mean\"], how='left', on=['matchId'])\n    \n    # print(\"get match type feature\")\n    # agg = df.groupby(['matchId'])[matchType.columns].agg('mean').reset_index()\n    # df_out = df_out.merge(agg, suffixes=[\"\", \"_match_type\"], how='left', on=['matchId'])\n    \n    print(\"get match size feature\")\n    agg = df.groupby(['matchId']).size().reset_index(name='match_size')\n    df_out = df_out.merge(agg, how='left', on=['matchId'])\n    \n    df_out=reduce_mem_usage(df_out)\n    \n    df_out.drop([\"matchId\", \"groupId\"], axis=1, inplace=True)\n    \n    \n\n    X = np.array(df_out)\n    \n    feature_names = list(df_out.columns)\n\n    del df, df_out, agg, agg_rank\n    gc.collect()\n\n    return X, y, feature_names\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"0a5118ddf0aded58e6f39020710b20a3a76f26a1"},"cell_type":"code","source":"x_train, y, feature_names = feature_engineering(True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c72f79fe650a25735855d7c798db5630b81b7dc2"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_val, y_train, y_val = train_test_split (x_train, y, test_size=0.33, random_state=42)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"84a29faa03b0e6446a6185fcdca588e9988e84d6"},"cell_type":"code","source":"#X_train= reduce_mem_usage(X_train)\n#X_val= reduce_mem_usage(X_val)\n#y_train= reduce_mem_usage(y_train)\n#y_val= reduce_mem_usage(y_val)\n\n\nimport os\nimport time\nimport gc\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n# data manipulation\n\nimport numpy as np\nimport pandas as pd\n# plot\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ncolor = sns.color_palette()\n# model\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\nimport lightgbm as lgb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7c089c41e2f9195283f4f9f082451e0087379d1d"},"cell_type":"code","source":"train_data=lgb.Dataset(X_train, label=y_train)\nval_data= lgb.Dataset(X_val, label=y_val)\n\nparams = {\n    'num_leaves': 144,\n    'learning_rate': 0.1,\n    'n_estimators': 1500,\n    'max_depth':12,\n    'max_bin':55,\n    'bagging_fraction':0.8,\n    'bagging_freq':5,\n    'feature_fraction':0.9,\n    'verbose':50, \n    'early_stopping_rounds':100\n    }\n\nparams['metric'] = 'auc'\nlight_reg= lgb.train(params, train_data, valid_sets=val_data, num_boost_round=5000, early_stopping_rounds=100)\ny_pred=light_reg.predict(X_val)\nprint(mean_absolute_error(y_val,y_pred))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"12190c3fa3a8bbeb304ab197bf90914f7434c013"},"cell_type":"code","source":"y_pred=light_reg.predict(X_val)\nprint(mean_absolute_error(y_val,y_pred))\ndel X_val\ndel y_val","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"907bf9841dd05ccde554bab7fcbdd2a1c40cf324"},"cell_type":"code","source":"x_test, y_test, feature_names = feature_engineering(False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3e8af82bebc6332f640d90c9c448471042a66469"},"cell_type":"code","source":"y_test_pred=light_reg.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"38d65dbd03dcedbed7529225db27300fc744f579"},"cell_type":"code","source":"df=pd.read_csv(\"../input/test_V2.csv\")\nvar=pd.DataFrame(columns=['Id','winPlacePerc'])\nvar['Id']= df['Id']\n\nvar['winPlacePerc'] = y_test_pred\n\nsubmission = var[['Id', 'winPlacePerc']]\nsubmission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}