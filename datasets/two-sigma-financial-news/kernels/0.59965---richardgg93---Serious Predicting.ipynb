{
  "cells": [
    {
      "metadata": {
        "_uuid": "2e68af34529cd7175b2839a1e2174d9cd224e039"
      },
      "cell_type": "markdown",
      "source": "# Imports and reading data"
    },
    {
      "metadata": {
        "_uuid": "1d131b1872161a1be46f4fa5dccaef195b5358c3"
      },
      "cell_type": "markdown",
      "source": "##### Import all libraries: pandas, sklearn, matplotlib, seaborn, etc."
    },
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport datetime \nfrom datetime import datetime as dt\n\nimport sklearn # ML\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble import ExtraTreesRegressor\n#from sklearn.svm import SVR\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.datasets import make_regression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import accuracy_score\n\nfrom kaggle.competitions import twosigmanews\n\n# Any results you write to the current directory are saved as output.",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "a1aa1fe2431039b1cce1818133ab9a805cccf0da"
      },
      "cell_type": "markdown",
      "source": "##### Load data from the environment."
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Retreive the environment of the competition\n# You can only call make_env() once, so don't lose it!\nenv = twosigmanews.make_env()\nprint('Data loaded!')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8df93b966bdb40b3a559eadc5d8c0cfd4f4a0ed5"
      },
      "cell_type": "code",
      "source": "# Retrieve all training data\n(market_train_df, news_train_df) = env.get_training_data()\nprint(\"Fetching training data finished... \")\nprint('Data obtained!')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "16554bb88eda199daab8fb6395722c3bccd19889"
      },
      "cell_type": "markdown",
      "source": "# Market data analysis"
    },
    {
      "metadata": {
        "_uuid": "16612000b558f245190d5b1792a7a3b9d5ba080f"
      },
      "cell_type": "markdown",
      "source": "### Types and example"
    },
    {
      "metadata": {
        "_uuid": "24b0d88d23b416f82ac51400ce172b1f43a963a9"
      },
      "cell_type": "markdown",
      "source": "##### We want to get an idea of what is there inside the market data (types, content, etc.)"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "99de5d73c3d49783c4c930c2fb3e7cf901ef0405",
        "scrolled": true
      },
      "cell_type": "code",
      "source": "# Market data analysis\n# Types of the columns\nprint(market_train_df.dtypes)\nmarket_train_df.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "603046b84851be1df5b18a15a8aaed748a4f0b93"
      },
      "cell_type": "markdown",
      "source": "### Analysis of main variables of market"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1ff1bb71f18cef57fe1e41a93599ea264af8ecdd"
      },
      "cell_type": "code",
      "source": "# Lets remove universe as it is useless for predicting\nmarket_train_df.drop(\"universe\", axis=1, inplace=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "46de6973b2235762d0447b113e39f7eef9fe19f4"
      },
      "cell_type": "markdown",
      "source": "##### Correlations (in terms of Pearson coefficient)."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "68e448b00f8e1a0f26ff0ecd196ebf668b48332c"
      },
      "cell_type": "code",
      "source": "# Correlation between the numericals (except universe)\n# Note that this removes the null values from the computation\nmarket_train_df.iloc[:, 3:].corr(method='pearson')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "03d66d1555874cc4bb3dfa52d21713413a4d8cff"
      },
      "cell_type": "code",
      "source": "sns.set(style=\"white\")\n# Compute the correlation matrix\ncorr = market_train_df.iloc[:, 3:].corr(method='pearson').corr()\n# Generate a mask for the upper triangle\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(9, 7))\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=1, vmin=-1, center=0, square=True, linewidths=.5, cbar_kws={\"shrink\": .5})",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "4f5fd3daa73347231d17bd1fa20bb15ed45b49b9"
      },
      "cell_type": "markdown",
      "source": "##### Most relevant assets by volume."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ae614fccfcfd6a5cf266be83c699a794d56c1759"
      },
      "cell_type": "code",
      "source": "# Let's begin understanding what do the market_train_df has. Order assets by volume\nmarket_train_df.sort_values(by = \"volume\", ascending = False)[\"assetName\"].unique()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "c0a0448f56d5dc6b60ee927f1828af5635a8842c"
      },
      "cell_type": "markdown",
      "source": "##### Evolution of market prices at closing time for some assets."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1e9819fe5f0a6ba7b400b72f1c0609fbe1974535"
      },
      "cell_type": "code",
      "source": "apple = market_train_df.loc[market_train_df['assetName'] == 'Apple Inc',:]\nfacebook = market_train_df.loc[market_train_df['assetName'] == 'Facebook Inc',:]\nmicrosoft = market_train_df.loc[market_train_df['assetName'] =='Microsemi Corp',:]\noracle = market_train_df.loc[market_train_df['assetName'] == 'Oracle Corp',:]\nbank_america = market_train_df.loc[market_train_df['assetName'] == 'Bank of America Corp']\n#print(apple.head(), facebook.head(),  microsoft.head(), oracle.head(), bank_america.head())\n#microsoft.head()\nplt.plot(apple['time'], apple['close'], color='blue')\nplt.plot(facebook['time'], facebook['close'], color='red')\nplt.plot(microsoft['time'], microsoft['close'], color='g')\nplt.plot(oracle['time'], oracle['close'], color='magenta')\nplt.plot(bank_america['time'], bank_america['close'], color='yellow')\nplt.legend([\"Apple\", \"Facebook\", \"Microsoft\", \"Oracle\",\"Bank of America\"])\nplt.title(\"Close prices over time\")\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "fa777eefc02761bf34bd2e6162b088ca6583c64f"
      },
      "cell_type": "markdown",
      "source": "##### Evolution of returnsOpenNextMktres10 (target variable) for the same assets."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9d830fc8fe480d5ac4dcebd7cb36c0891610a537"
      },
      "cell_type": "code",
      "source": "apple = market_train_df.loc[market_train_df['assetName'] == 'Apple Inc',:]\nfacebook = market_train_df.loc[market_train_df['assetName'] == 'Facebook Inc',:]\nmicrosoft = market_train_df.loc[market_train_df['assetName'] =='Microsemi Corp',:]\noracle = market_train_df.loc[market_train_df['assetName'] == 'Oracle Corp',:]\nbank_america = market_train_df.loc[market_train_df['assetName'] == 'Bank of America Corp']\n#print(apple.head(), facebook.head(),  microsoft.head(), oracle.head(), bank_america.head())\n#microsoft.head()\nplt.plot(apple['time'], apple['returnsOpenNextMktres10'], color='blue')\nplt.plot(facebook['time'], facebook['returnsOpenNextMktres10'], color='red')\nplt.plot(microsoft['time'], microsoft['returnsOpenNextMktres10'], color='g')\nplt.plot(oracle['time'], oracle['returnsOpenNextMktres10'], color='magenta')\nplt.plot(bank_america['time'], bank_america['returnsOpenNextMktres10'], color='yellow')\nplt.legend([\"Apple\", \"Facebook\", \"Microsoft\", \"Oracle\",\"Bank of America\"])\nplt.title(\"Target variable over time\")\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "940c862d5f5a4cc8030a696026c54122e0c16b54"
      },
      "cell_type": "markdown",
      "source": "##### Distribution of returnsOpenNextMktres10 (target variable).\n##### The variable is actually centered in 0 with only a few outliers higher than 0.25. This makes sense considering that the returns of the market for 10 days are really small. From a business perspective, if the mean had a value much higher than 0, it would imply that everybody would get rich with the stock market, and if very negative that it would be a machine of losing money. Our goal then should be to detect those times in which the wins or loses are really high by making use of the news."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e9fa2c8e40a5368ee4fa1d0dec9134206ac19dcd"
      },
      "cell_type": "code",
      "source": "# Lets analyze further the target variable\n# Very big outliers, lets see their number and distribution\n\nfig, axes = plt.subplots(3,2, figsize=(20, 12)) # create figure and axes\nprint(\"# Rows with |value| > 1 =\", market_train_df[market_train_df[\"returnsOpenNextMktres10\"].abs()>1].shape[0])\nprint(\"# Rows with |value| > 0.5 =\", market_train_df[market_train_df[\"returnsOpenNextMktres10\"].abs()>0.5].shape[0])\nprint(\"# Rows with |value| > 0.25 =\", market_train_df[market_train_df[\"returnsOpenNextMktres10\"].abs()>0.25].shape[0])\nprint(\"# Rows with |value| > 0.1 =\", market_train_df[market_train_df[\"returnsOpenNextMktres10\"].abs()>0.1].shape[0])\n\n# Boxplot with all values\nmarket_train_df.boxplot(column=\"returnsOpenNextMktres10\", ax=axes.flatten()[0])\naxes.flatten()[0].set_xlabel('Boxplot with all values', fontsize=18)\n# Removing rows with outliers (bigger or smaller than 1)\nmarket_train_df[market_train_df[\"returnsOpenNextMktres10\"].abs()<1].boxplot(column=\"returnsOpenNextMktres10\", ax=axes.flatten()[1])\naxes.flatten()[1].set_xlabel('Boxplot with values such that |val| < 1', fontsize=18)\n# Removing rows with outliers (bigger or smaller than 0.5)\nmarket_train_df[market_train_df[\"returnsOpenNextMktres10\"].abs()<0.5].boxplot(column=\"returnsOpenNextMktres10\", ax=axes.flatten()[2])\naxes.flatten()[2].set_xlabel('Boxplot with values such that |val| < 0.5', fontsize=18)\n# Removing rows with outliers (bigger or smaller than 0.25)\nmarket_train_df[market_train_df[\"returnsOpenNextMktres10\"].abs()<0.25].boxplot(column=\"returnsOpenNextMktres10\", ax=axes.flatten()[3])\naxes.flatten()[3].set_xlabel('Boxplot with values such that |val| < 0.25', fontsize=18)\n# Removing rows with outliers (bigger or smaller than 0.1)\nmarket_train_df[market_train_df[\"returnsOpenNextMktres10\"].abs()<0.1].boxplot(column=\"returnsOpenNextMktres10\", ax=axes.flatten()[4])\naxes.flatten()[4].set_xlabel('Boxplot with values such that |val| < 0.1', fontsize=18)\n# Distribution of the target value (not including values bigger or smaller than 1)\nmarket_train_df[market_train_df[\"returnsOpenNextMktres10\"].abs()<0.25].hist(column=\"returnsOpenNextMktres10\", bins=100, ax=axes.flatten()[5])\naxes.flatten()[5].set_xlabel('Histogram for values such that |val| < 0.25', fontsize=18)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "9973d14198f50e2916555a6de57f252a23aa9244"
      },
      "cell_type": "markdown",
      "source": "### Nulls values (replace with the mean of the column)"
    },
    {
      "metadata": {
        "_uuid": "12b519163efbc7d9503969f41ebd4ffaf7b29803"
      },
      "cell_type": "markdown",
      "source": "##### We considered several approaches here; ended up replacing them with the mean of its column"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a5454cd1bffbbb7951c3c42d1cc47bebe6e6c0f4"
      },
      "cell_type": "code",
      "source": "# Number of null values\nmarket_train_df.isna().sum()\n# Where are those null values in terms of dates?\n#rows_with_null=market_train_df[pd.isnull(market_train_df).any(axis=1)]\n#dates_with_null=rows_with_null[\"time\"].unique()\n#nulls_per_date=[rows_with_null[rows_with_null[\"time\"]==d].shape[0] for d in dates_with_null]\n#res=pd.DataFrame({'date': dates_with_null, 'nulls': nulls_per_date })\n#res.head()\n# Where are those null values in terms of assets?\n#rows_with_null=market_train_df[pd.isnull(market_train_df).any(axis=1)]\n#assets_with_null=rows_with_null[\"assetCode\"].unique()\n#nulls_per_asset=[rows_with_null[rows_with_null[\"assetCode\"]==a].shape[0] for a in assets_with_null]\n#res=pd.DataFrame({'asset': assets_with_null, 'nulls': nulls_per_asset})\n#res.sort_values(by=['nulls'], ascending=False, inplace=True)\n#res.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ddeeca72dd3597065a8d5e9f815eae2c4990b601"
      },
      "cell_type": "code",
      "source": "# Lets replace null values with the means without taking into account the possible outliers. \ntol = 0.15\ncols = ['returnsClosePrevMktres1','returnsOpenPrevMktres1','returnsClosePrevMktres10','returnsOpenPrevMktres10']\nreplace_nans_market = {}\nfor c in cols:\n    m = market_train_df[c][market_train_df[c].abs()<= tol].mean()\n    print (c, \"Mean--> \", m)\n    replace_nans_market[c] = m\nmarket_train_df.fillna(value=replace_nans_market, inplace=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "d47c1d7a2a6911215888591f6cf1ae16b2263b41"
      },
      "cell_type": "markdown",
      "source": "# News data analysis"
    },
    {
      "metadata": {
        "_uuid": "38c1cf9dd9bc58ae3350d6a7c5ef2d41916bc16e"
      },
      "cell_type": "markdown",
      "source": "### Types and example"
    },
    {
      "metadata": {
        "_uuid": "3a44fb4f826ff6db17271199c3eb0bbb2770df14"
      },
      "cell_type": "markdown",
      "source": "##### We wanted to know what was there inside the news dataset (types of the data, data included on each column, etc..)."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "97fe2af61adfe66cacc02634fe0cd040f9d783c1"
      },
      "cell_type": "code",
      "source": "# News data analysis\n# Types of the columns\nprint(news_train_df.dtypes)\nnews_train_df.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "067a375191ebb912b2c7ba6212f87fd95049e0e4"
      },
      "cell_type": "markdown",
      "source": "### Nulls values"
    },
    {
      "metadata": {
        "_uuid": "42632045479aef3fbf430455bcd48a4be5fe0b09"
      },
      "cell_type": "markdown",
      "source": "##### There are not nulls on this dataset. Nice!"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3f78fdd031756405e305beccd2e6a76dd820ac98"
      },
      "cell_type": "code",
      "source": "# Number of null values\n# Perfect! There are no nulls\nnews_train_df.isna().sum()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "89a4f2fb916a25498f85e476de25c22d12b975a6"
      },
      "cell_type": "markdown",
      "source": "### Analysis of the main variables "
    },
    {
      "metadata": {
        "_uuid": "82fcf59c7928532b4a1708d7d3277ed9830524e8"
      },
      "cell_type": "markdown",
      "source": "##### Comparing the relevance of all news... 2/3 of the news with relevance 1 (maximum) and most of the others with very close to 0. We will keep all of them in any case for the model"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e130d22ef36cd67b0888ec753578300880092546"
      },
      "cell_type": "code",
      "source": "# Relevance\nplt.hist(news_train_df[\"relevance\"], bins=25)\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c16b5c3a349d8631198538359b2d8067ec75c7ae"
      },
      "cell_type": "code",
      "source": "# remove non relevant\n# news_train_df = news_train_df[news_train_df[\"relevance\"]>0.9]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "97d3b089a69d316484004f6ef7a2848bcd7d045d"
      },
      "cell_type": "markdown",
      "source": "##### Checking how is the sentiment distributed (there are slightly less negative than positive). This could be due to the news being in general positive in this topic or because of a bias when calculating the sentiments the company."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b24e9d38ecb5b9584fc2901713a6a929c79a7b54"
      },
      "cell_type": "code",
      "source": "# Sentiment vars\n#fig, axes = plt.subplots(1,3, figsize=(20, 12)) # create figure and axes\n#news_train_df.boxplot(column=\"sentimentNegative\", ax=axes.flatten()[0])\n#axes.flatten()[0].set_xlabel('Boxplot with all values', fontsize=18)\n#news_train_df.boxplot(column=\"sentimentNeutral\", ax=axes.flatten()[1])\n#axes.flatten()[1].set_xlabel('Boxplot with all values', fontsize=18)\n#news_train_df.boxplot(column=\"sentimentPositive\", ax=axes.flatten()[2])\n#axes.flatten()[2].set_xlabel('Boxplot with all values', fontsize=18)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "36cbb894430029bcc99589859c2c68fa1e8a63d2"
      },
      "cell_type": "code",
      "source": "# Sentiment vars\nvalues = [news_train_df[\"sentimentNegative\"].mean(), news_train_df[\"sentimentNeutral\"].mean(), news_train_df[\"sentimentPositive\"].mean()]\nlabels = 'Negative', 'Neutral', 'Positive'\ncolors = ['lightcoral', 'gold' , 'lightskyblue']\nplt.pie(values, labels=labels, colors=colors, autopct='%1.1f%%', shadow=True)\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "61610b64e0ed6fa3568f33dbb193e7bac3e3ce4e"
      },
      "cell_type": "markdown",
      "source": "# Data Preprocessing"
    },
    {
      "metadata": {
        "_uuid": "de0728701f15854186154cf7fa9cb6470bf5dab8"
      },
      "cell_type": "markdown",
      "source": "### Removing some vars and creating new ones (only news)"
    },
    {
      "metadata": {
        "_uuid": "30deed26932c6f3e151ee7847a621ef36fda7aa7"
      },
      "cell_type": "markdown",
      "source": "##### Since here we are not considering the novelty and volume anymore, and we are creating a new variable to represent at the same time the sentiment and the urgency and relevance of the news together."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ec2bf079be77ffdd2cdf150392cae35826d8ef4f"
      },
      "cell_type": "code",
      "source": "# Let us remove some not important columns\ncols_to_keep_news = [\"time\", \"urgency\", \"wordCount\", \"assetName\", \"relevance\", \"sentimentClass\", \"sentimentNegative\", \"sentimentNeutral\", \"sentimentPositive\"]\nnews_train_df = news_train_df[cols_to_keep_news]\n# And create some others\nnews_train_df[\"sent_rel\"] = news_train_df[\"urgency\"] * news_train_df[\"relevance\"] * (news_train_df[\"sentimentPositive\"]-news_train_df[\"sentimentNegative\"]) \nnews_train_df.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "5dd3f64b9b3f42a9f572a41f2769900fecf88e00"
      },
      "cell_type": "markdown",
      "source": "### Removing hour and minutes from the timestamps"
    },
    {
      "metadata": {
        "_uuid": "81832d05e0bb9b0a3b65b0b0ed77f0e263189e37"
      },
      "cell_type": "markdown",
      "source": "##### We are normalizing the times so we do not consider the hours, minutes, etc. anymore, only the date."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b1537a2685cdfdb67501d28105ed8d931d09c1e3"
      },
      "cell_type": "code",
      "source": "# Minor changes on dates so they match the day\nnews_train_df['time'] = news_train_df['time'].dt.tz_convert(None).dt.normalize()\nmarket_train_df['time'] = market_train_df['time'].dt.tz_convert(None).dt.normalize()\n#print(news_train_df.dtypes)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "ac4250c40fd2f494089acb99a72075093e89ec1e"
      },
      "cell_type": "markdown",
      "source": "### Aggregate news for same day and asset in a single row"
    },
    {
      "metadata": {
        "_uuid": "32bd1a3a98215693bdc7f65a2ca1995db9bdf387"
      },
      "cell_type": "markdown",
      "source": "##### We will merge all info from news of same day and asset into a single row (in genereal we will take the mean, except for urgency)."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "fb39251a5e5b1ff2478586dc648602e149bb0827"
      },
      "cell_type": "code",
      "source": "# Convert news of same day and asset to one row only\n#d = {'sent_rel': 'mean', 'name': 'first', 'amount': 'sum'}\nd_aggs = {'urgency' : 'max', 'wordCount': 'mean', 'relevance' : 'mean', 'sentimentClass' : 'mean', 'sentimentNegative' : 'mean', 'sentimentNeutral' : 'mean', 'sentimentPositive' : 'mean', 'sent_rel': 'mean'}\nreduced_df=news_train_df[[\"time\", \"assetName\"]+list(d_aggs.keys())].groupby(['time', 'assetName'], as_index=False).aggregate(d_aggs)#.reindex(columns=news_train_df.columns)\nreduced_df.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "309b7ce61a3dfd6caf3bed280e374627cf4afc2e"
      },
      "cell_type": "code",
      "source": "print(\"Size before transforming: \", len(news_train_df))\nprint(\"Size after transforming: \", len(reduced_df))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "487a9e849b9d09395fb895c454e4263a874ad4fb"
      },
      "cell_type": "markdown",
      "source": "### Merging datasets of news and market"
    },
    {
      "metadata": {
        "_uuid": "73bc40c6f51c1303e5b4dc0663effd36cbefdca6"
      },
      "cell_type": "markdown",
      "source": "##### After all the previous work we are now able to merge both datasets, assigning the news to the market cases."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "edf97e9975321fe5e0edb2ed06915aaec34b9502"
      },
      "cell_type": "code",
      "source": "# Create the final dataset to train and test\n# Final merge by assetName and time\ndf_final = market_train_df.merge(reduced_df, how = 'left', on = ['time', 'assetName'])\nprint(len(df_final))\ndf_final.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "53ae0d1cd4eb78856f0d17598d1165133cb26969"
      },
      "cell_type": "markdown",
      "source": "# Analysis of the merged data"
    },
    {
      "metadata": {
        "_uuid": "5241b1f409be552224cf9af642d837088cea21d1"
      },
      "cell_type": "markdown",
      "source": "### Sentiment variables with regards to the target variable"
    },
    {
      "metadata": {
        "_uuid": "da2c3bc785b48194ac682916f456eb0e9d5ffa81"
      },
      "cell_type": "markdown",
      "source": "##### We would expect the return to be higher when the news were clearly positive, and lower if clearly negative. Nevertheless following we have the mean return for negative, positive and so so sentiment news, which is not very clear."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "deb6b424bfd3cbe634589c3437f837400859aac4"
      },
      "cell_type": "code",
      "source": "print(\"Returns with negative news:\", df_final[(df_final[\"sentimentClass\"]< -0.4) & (df_final[\"returnsOpenNextMktres10\"].abs()<0.5)][\"returnsOpenNextMktres10\"].mean())\nprint(\"Returns with neutral news\", df_final[(df_final[\"sentimentClass\"].isnull()) & (df_final[\"returnsOpenNextMktres10\"].abs()<0.5)][\"returnsOpenNextMktres10\"].mean())\nprint(\"Returns with positive news:\", df_final[(df_final[\"sentimentClass\"]> 0.4) & (df_final[\"returnsOpenNextMktres10\"].abs()<0.5)][\"returnsOpenNextMktres10\"].mean())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "52e87d4526c401b23a8c29a8ab60ef4c78f7d2d8"
      },
      "cell_type": "markdown",
      "source": "##### And the sentiment mean for high loses and wins:"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "fff8a81a8596209a2f18c30907242b0b16b84195"
      },
      "cell_type": "code",
      "source": "print(\"Mean sentiment for loses:\", df_final[(df_final[\"returnsOpenNextMktres10\"] < -0.2) & (df_final[\"returnsOpenNextMktres10\"] > -0.5)][\"sentimentClass\"].mean())\nprint(\"Mean sentiment for wins\", df_final[(df_final[\"returnsOpenNextMktres10\"] > 0.2) & (df_final[\"returnsOpenNextMktres10\"]< 0.5)][\"sentimentClass\"].mean())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "58acb9a3da4db99ff7ff15698d0bfa0810b18b5f"
      },
      "cell_type": "markdown",
      "source": "##### We can also compare the distributions of all (very positive, very negative and soso). Ignoring the fact that there are more of a type than the other we can observe that the distribution is basicaly the same for all cases, which means that those sentiments may not be really useful to predict..."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d438b11953a0ddd74d538c71f63998377743df59"
      },
      "cell_type": "code",
      "source": "bins = np.linspace(-0.25, 0.25, 200)\n#plt.hist(df_final[(df_final[\"sentimentClass\"].isnull()) & (df_final[\"returnsOpenNextMktres10\"].abs()<0.25)][\"returnsOpenNextMktres10\"], bins = bins, alpha=0.8, label='Non news', color = 'grey')\nplt.hist(df_final[(df_final[\"sentimentClass\"]> 0.4) & (df_final[\"returnsOpenNextMktres10\"].abs()<0.25)][\"returnsOpenNextMktres10\"], bins = bins, alpha=0.8, label='Positive')\nplt.hist(df_final[(df_final[\"sentimentClass\"].abs()< 0.4) & (df_final[\"returnsOpenNextMktres10\"].abs()<0.25)][\"returnsOpenNextMktres10\"], bins = bins, alpha=0.5, label='Neutral', color ='green')\nplt.hist(df_final[(df_final[\"sentimentClass\"]<= -0.4) & (df_final[\"returnsOpenNextMktres10\"].abs()<0.25)][\"returnsOpenNextMktres10\"], bins = bins, alpha=0.8, label='Negative', color = 'orange')\nplt.legend(loc='upper right')\nplt.title(\"returnsOpenNextMktres10\")\nplt.xlabel(\"Value\")\nplt.ylabel(\"Frequency\")\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "9179f937bf05d95e307f0ba834ee19fc2af2beb7"
      },
      "cell_type": "markdown",
      "source": "##### And also adding when there are no news at all..."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "44bb0b1d6d8661178049741dcafed605057b0891"
      },
      "cell_type": "code",
      "source": "bins = np.linspace(-0.25, 0.25, 200)\nplt.hist(df_final[(df_final[\"sentimentClass\"].isnull()) & (df_final[\"returnsOpenNextMktres10\"].abs()<0.25)][\"returnsOpenNextMktres10\"], bins = bins, alpha=0.8, label='No news', color = 'grey')\nplt.hist(df_final[(df_final[\"sentimentClass\"]> 0.4) & (df_final[\"returnsOpenNextMktres10\"].abs()<0.25)][\"returnsOpenNextMktres10\"], bins = bins, alpha=0.8, label='Positive')\nplt.hist(df_final[(df_final[\"sentimentClass\"].abs()< 0.4) & (df_final[\"returnsOpenNextMktres10\"].abs()<0.25)][\"returnsOpenNextMktres10\"], bins = bins, alpha=0.5, label='Neutral', color ='green')\nplt.hist(df_final[(df_final[\"sentimentClass\"]<= -0.4) & (df_final[\"returnsOpenNextMktres10\"].abs()<0.25)][\"returnsOpenNextMktres10\"], bins = bins, alpha=0.8, label='Negative', color = 'orange')\nplt.legend(loc='upper right')\nplt.title(\"returnsOpenNextMktres10\")\nplt.xlabel(\"Value\")\nplt.ylabel(\"Frequency\")\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "8218499de2fdf913e7c0748b05a9c7b9935a476c"
      },
      "cell_type": "markdown",
      "source": " ### Remove nans for asset/days with no news and encode assets names and codes"
    },
    {
      "metadata": {
        "_uuid": "0785540a8fa5956a78e245c7ae9a8f8cd0d74f76"
      },
      "cell_type": "markdown",
      "source": "##### As final steps before modeling we will replace nans appearing after the merge with their means, encode the strings with numbers so sklearn can deal with them and normalize the real numerical data."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ba25de35389531417cb211cdb1712ea1bd1fb49f"
      },
      "cell_type": "code",
      "source": "# After merging we have some nulls, as there are asset/date keys with no news (lets replace them)\ncols = list(d_aggs.keys())\nreplace_nans_global = {}\nfor c in cols:\n    m = df_final[c].mean()\n    replace_nans_global[c] = m\n#df_final.fillna(value=replace_nans_global, inplace=True)\ndf_final.fillna(value=-9999, inplace=True)\n# Lets encode the assetCode and assetName for the ML algorithms\n# Asset codes\nac_encoder = preprocessing.LabelEncoder()\nassetCodes = df_final[\"assetCode\"].unique()\nprint(len(assetCodes))\nac_encoder.fit(assetCodes)\ndf_final[\"assetCodeNum\"]=ac_encoder.transform(df_final[\"assetCode\"]) \n# Asset names\nan_encoder = preprocessing.LabelEncoder()\nassetNames = df_final[\"assetName\"].unique()\nprint(len(assetNames))\nan_encoder.fit(assetNames)\ndf_final[\"assetNameNum\"]=an_encoder.transform(df_final[\"assetName\"])\ndf_final.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6d137f9bc1ac4877919d0431037bda45f0bb1979"
      },
      "cell_type": "code",
      "source": "# Normalize data??\n\"\"\"\ncols_not_normalize = ['time', 'assetCode', 'assetName','assetCodeNum', 'assetNameNum', 'returnsOpenNextMktres10']\ncols_normalize = [x for x in df_final.columns if x not in cols_not_normalize]\ntmp = df_final[cols_normalize].copy()\nscaler = StandardScaler()\n# scaler = MinMaxScaler()\nscaler.fit(tmp)\ntmp =  pd.DataFrame(scaler.transform(tmp), columns=cols_normalize)\ntmp = pd.concat([df_final[cols_not_normalize].copy(), tmp], axis=1, sort=False)\ntmp.head()\"\"\"",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "3836f9e194c338cc1d841a83b45e5b8703e1b2fc"
      },
      "cell_type": "markdown",
      "source": "# Modeling"
    },
    {
      "metadata": {
        "_uuid": "680c68ddea6cfb53e20ec91fbe64eabf8fdae97b"
      },
      "cell_type": "markdown",
      "source": "### Getting data to train and test"
    },
    {
      "metadata": {
        "_uuid": "aa14327c4bbe9925513cf0458f94534f904232f7"
      },
      "cell_type": "markdown",
      "source": "##### We will take here the periods for train (2014 and 2015) and test (2016 and an samll part of 2017) and split both into features and target variables."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f1d8cc864b3b4e01d73571103f6e877aac5daee1"
      },
      "cell_type": "code",
      "source": "# Lets do the splitting for ML !\ntmp = df_final.copy()\n# 2014-2016 to train, 2016-2017 to test\ninit_date=np.datetime64('2014-01-01')\ninit_test=np.datetime64('2016-01-01')\ntmp = tmp[tmp[\"time\"]>=init_date]\n# delete outliers to train\ntmp = tmp[tmp[\"returnsOpenNextMktres10\"].abs()<0.25]\n# split train and test\ntrain = tmp[tmp[\"time\"]<init_test]\ntest = tmp[tmp[\"time\"]>=init_test]\n# remove useless cols\n#tmp.drop([\"time\", \"assetCode\", \"assetName\"]+list(d_aggs.keys()), axis=1, inplace=True)\ntrain.drop([\"time\", \"assetCode\", \"assetName\"], axis=1, inplace=True)\ntest.drop([\"time\", \"assetCode\", \"assetName\"], axis=1, inplace=True)\n# redistribute training dataset\n#c1=train[train[\"returnsOpenNextMktres10\"].abs()>0.08].copy().iloc[:, :]\nc1=train[train[\"returnsOpenNextMktres10\"].abs()>0.03].copy().iloc[:, :]\n#c2=train[(train[\"returnsOpenNextMktres10\"].abs()<=0.1) & (train[\"returnsOpenNextMktres10\"].abs()>0.01)].copy().iloc[-len(c1):, :]\n#train = pd.concat([c1, c2], axis=0, sort=False)\ntrain=c1.copy()\n# split in features and target variable\nX_train = train.loc[:, train.columns != 'returnsOpenNextMktres10']\nX_test = test.loc[:, test.columns != 'returnsOpenNextMktres10']\ny_train = train['returnsOpenNextMktres10'].values\ny_test = test['returnsOpenNextMktres10'].values\n#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, shuffle=False) \n# Save cols order for the final prediction data\ncols_order=X_train.columns\nprint(cols_order)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "fd566309e0cce2372386708c73de52c67e45c862"
      },
      "cell_type": "code",
      "source": "#plt.hist(y_train, bins = 100)\n#plt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "dee67e5c87ca6b88d93bcf7bcc77e29f5b37f9bd"
      },
      "cell_type": "markdown",
      "source": "### Train and predict (with some finetuning of hyperparameters)"
    },
    {
      "metadata": {
        "_uuid": "a98146d7650f0c9e0ffa92a919cfbd18611c2e83"
      },
      "cell_type": "markdown",
      "source": "##### We have tried here LinearRegression, RandomForest, GradientBoosting, ExtraTrees and Neural Networks (all of them with different parameters). Then we take the one with better score in terms of MAE as our final model for predicting. Note that due to the kernel constraints we couldn't carry out a better hyperparameter tuning (with gridsearch for instance) and that we had to avoid temporal crossvalidation."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "37a5bb27ef0edbfb6b333f45c35d653fbf4c93a2",
        "scrolled": true
      },
      "cell_type": "code",
      "source": "# Predict\n#regr1 = LinearRegression(n_jobs=-1)\nregr2 = RandomForestRegressor(n_jobs=-1, max_depth=200, n_estimators=30, min_samples_leaf=10, criterion=\"mse\", random_state=15)\n#regr3 = GradientBoostingRegressor(max_depth=3, n_estimators=10,  criterion=\"mse\")\n#regr4 = ExtraTreesRegressor(n_jobs=-1, max_depth=10, n_estimators=10, criterion=\"mse\")\n#regr5 = MLPRegressor(hidden_layer_sizes=(10,5), activation='relu', alpha=0.0001, solver='adam', learning_rate='adaptive', max_iter=1000)\nestimators = [regr2]\nmaes = []\nmses = []\nfor e in estimators:\n    e.fit(X_train, y_train)\n    y_predicted=e.predict(X_test)\n    maes += [mean_absolute_error(y_test, y_predicted)]\n    mses += [mean_squared_error(y_test, y_predicted)]\n    print(\"MAE:\", mean_absolute_error(y_test, y_predicted))\n    print(\"MSE:\", mean_squared_error(y_test, y_predicted))\nprint(\"Acab√≥\")\nprint(maes)\nprint(mses)\nregr = estimators[np.argmin(maes)]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "5419df4889360289c253ad1419f2edc5730140eb"
      },
      "cell_type": "markdown",
      "source": "### Analysis of results"
    },
    {
      "metadata": {
        "_uuid": "42b1232ae52f8896487f44c55338558ff8d282f4"
      },
      "cell_type": "markdown",
      "source": "##### Not only the target variable but the confidence we have to provide for the final leaderboard of the competition. The goal more specifically is knowing if the returns are positives or negatives so lets see how many times we predicted it properly."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d15313595276808a55b2644f7577622b92f509e8"
      },
      "cell_type": "code",
      "source": "# accuracy\ny_test2 = [0 if y < 0 else 1 for y in y_test]\ny_pred2 = [0 if y < 0 else 1 for y in y_predicted]\nprint(\"Accuracy: \", accuracy_score(y_test2, y_pred2))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "35e8f07498a6a19f6a0d6a9e15e05b64f3bcd304"
      },
      "cell_type": "code",
      "source": "##### Feature relevance for the model (basically returns backwards are the most relevant ones)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3537fac755ee9d6ba0497f3c11e9c00da4df347c"
      },
      "cell_type": "code",
      "source": "#for i in range(X_train.shape[1]):\n#    print(\"%s (%f)\" % (X_train.columns[i], regr.feature_importances_[i]))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "59394765ef5daf8d22426c694dc99fc4b90337c9"
      },
      "cell_type": "markdown",
      "source": "##### Brief comparison between real and predicted variable."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "448adc5dea489a8b2d192953bf3ac5a7f5fd0856",
        "scrolled": true
      },
      "cell_type": "code",
      "source": "df_results = X_test.copy()\ndf_results.insert(loc=df_results.shape[1], column=\"y_real\", value=y_test)\ndf_results.insert(loc=df_results.shape[1], column=\"y_pred\", value=y_predicted)\ndf_results.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "346761fc5ea4946630aefcb3de40ab9891962b07"
      },
      "cell_type": "markdown",
      "source": "##### Evolution of real and predicted variable along the test period (for Oracle Corp only)."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5c4b4dab36046062b29a7d8f5a2411aa51bdb5e0"
      },
      "cell_type": "code",
      "source": "example=df_results[df_results[\"assetNameNum\"]==an_encoder.transform([\"Oracle Corp\"])[0]]\nx=range(len(example))\nplt.plot(x, example[\"y_real\"], color='blue')        # specify color by name\nplt.plot(x, example[\"y_pred\"], color='red')\nplt.legend()\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "0b57f2161607aa858bce8dc8aa028a7c62a5f08d"
      },
      "cell_type": "markdown",
      "source": ""
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "373edfdb3c62b8fa2222087e73e46e2a1b2c8a66"
      },
      "cell_type": "markdown",
      "source": "# Final prediction"
    },
    {
      "metadata": {
        "_uuid": "4044762972ce99fffe2ef05e824924bffce2fb7c"
      },
      "cell_type": "markdown",
      "source": "### Predicting for the validation time of the Kaggle competition"
    },
    {
      "metadata": {
        "_uuid": "2151d7aeb483ad526788c3444f83dfbf10b87e9c"
      },
      "cell_type": "markdown",
      "source": "##### Here we will make use of our resulting model to predict for the final validation set of the competition. Note that we have to perform again all the preprocessing tasks over the new data and that we have to compute a final confidence value instead of the target variable as such."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9e2972aaaa688a24442c6e6bc82ada414c8a3f0d"
      },
      "cell_type": "code",
      "source": "def preprocessing(market, news, replace_nans_market, cols_to_keep_news, d_aggs, replace_nans_global, ac_encoder, an_encoder, cols_order):\n    # replace_nans_market, cols_to_keep_news, d_aggs, replace_nans_global, ac_encoder, an_encoder, cols_order\n    # Remove nans on market\n    market = market.fillna(value=replace_nans_market)\n    # Transform cols\n    news = news[cols_to_keep_news]\n    news[\"sent_rel\"] = news[\"urgency\"] * news[\"relevance\"] * (news[\"sentimentPositive\"]-news[\"sentimentNegative\"]) \n    # Minor changes on dates so they match the day\n    news['time'] = news['time'].dt.tz_convert(None).dt.normalize()\n    market['time'] = market['time'].dt.tz_convert(None).dt.normalize()\n    # Convert news of same day and asset to one row only\n    red = news[[\"time\", \"assetName\"]+list(d_aggs.keys())].groupby(['time', 'assetName'], as_index=False).aggregate(d_aggs)#.reindex(columns=news_train_df.columns)\n    # Final merge by assetName and time\n    df = market.merge(red, how = 'left', on = ['time', 'assetName'])\n    # Now we have some nulls as there are asset/date keys with no news (lets replace them)\n    #df = df.fillna(value=replace_nans_global)\n    df = df.fillna(value=-9999)\n    # Encode the assetCode and assetName for the ML algorithms (note new values would make it crash)\n    le_dict = dict(zip(ac_encoder.classes_, ac_encoder.transform(ac_encoder.classes_)))\n    df[\"assetCodeNum\"]=df[\"assetCode\"].apply(lambda x: le_dict.get(x, -9999))\n    le_dict = dict(zip(an_encoder.classes_, an_encoder.transform(an_encoder.classes_)))\n    df[\"assetNameNum\"]=df[\"assetName\"].apply(lambda x: le_dict.get(x, -9999))\n    # Normalize data\n    \"\"\"cols_not_normalize =  ['time', 'assetCode', 'assetName','assetCodeNum', 'assetNameNum']\n    cols_normalize = [x for x in df.columns if x not in cols_not_normalize]\n    tmp = df[cols_normalize].copy()\n    tmp = pd.DataFrame(scaler.transform(tmp), columns=cols_normalize)\n    df = pd.concat([df[cols_not_normalize].copy(), tmp], axis=1, sort=False)\"\"\"\n    # Remove vars unneeded\n    #df.drop([\"time\", \"assetCode\", \"assetName\"], axis=1, inplace=True)\n    df = df[cols_order]\n    return df",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d54079ef2529705b870aab6c18c55406006c4915"
      },
      "cell_type": "code",
      "source": "def predictions(market, news, predictions_template_df):\n    print(market[\"time\"][0])\n    # Preprocessing\n    tmp = preprocessing(market, news, replace_nans_market, cols_to_keep_news, d_aggs, replace_nans_global, ac_encoder, an_encoder, cols_order)\n    # Predicting\n    y_predicted=regr.predict(tmp)\n    # Final output\n    #mn=min(y_predicted)\n    #mx=max(y_predicted)\n    mn=-0.25\n    mx=0.25\n    # Converting into the confidence value, from -1 to 1\n    predictions_template_df.confidenceValue = [((y-mn)/(mx-mn)*2-1) for y in y_predicted]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9b37747c058292229728ed1fc0d070148330d22e"
      },
      "cell_type": "code",
      "source": "# Retrieve all days to iterate through\n# You can only iterate through a result from `get_prediction_days()` once\n# so be careful not to lose it once you start iterating.\ndays = env.get_prediction_days()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "cb0f516752aca69e3a0d431b954b4aa9fb122caa",
        "scrolled": true
      },
      "cell_type": "code",
      "source": "# Generate the predictions\nfor (market_obs_df, news_obs_df, predictions_template_df) in days:\n    predictions(market_obs_df, news_obs_df, predictions_template_df)\n    env.predict(predictions_template_df)\nprint('Prediction finished!')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "bc677ddfe3b1d83b76970c14713d681af92149a4"
      },
      "cell_type": "code",
      "source": "#env.predict(predictions_template_df)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c2fea5cca4ada4bd1f0fbf8d801fe831c792736f"
      },
      "cell_type": "code",
      "source": "# Write submission file\n# Note that for submitting the results we have to commit and then upload the resulting csv file\nenv.write_submission_file()\nprint([filename for filename in os.listdir('.') if '.csv' in filename])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "edaa719e48320f41d6d471aacb048f693c059cbf"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}