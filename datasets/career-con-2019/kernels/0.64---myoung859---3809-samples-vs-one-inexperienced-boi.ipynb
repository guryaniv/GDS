{"cells":[{"metadata":{},"cell_type":"markdown","source":"So what exactly is this assignment? This is a simple classification task with a minor hiccup: how exactly should the inputs be vectorized? \n\nThis will be a public kernel, so feel free to offer suggestions/yell at me if you're reading this"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"#Getting the basic libraries set up\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n#Machine learning-specific\nimport tensorflow as tf\nfrom tensorflow import keras\n\nfrom collections import Counter\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn import preprocessing\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\n\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\ny_train = pd.read_csv('../input/y_train.csv')\nx_train = pd.read_csv('../input/X_train.csv')\nx_test = pd.read_csv('../input/X_test.csv')\n\n# Any results you write to the current directory are saved as output.","execution_count":22,"outputs":[{"output_type":"stream","text":"['y_train.csv', 'sample_submission.csv', 'X_test.csv', 'X_train.csv']\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"First things first, let's get the inputs and outputs set up.\n\nFor outputs, this is a pretty simple use of label encoding.\n\nFor inputs, this implementation crafts a couple of features from each measurement series: mean, stdev, and range. I suspect that the feature selection is the first thing that needs to be improved to make an accurate model, but I'm not entirely sure what else to add"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"#Function for setting up training & testing data\ndef processor(input,out_name,num_samples):\n    #Input should be an input csv turned into a data frame\n    #out_name is a string ending in .csvorientations = [\"or_x_mean\",\"or_x_std\",\"or_x_range\",\"or_y_mean\",\"or_y_std\",\"or_y_range\",\"or_z_mean\",\"or_z_std\",\"or_z_range\",\"or_w_mean\",\"or_w_std\",\"or_w_range\"]\n    orientations = [\"or_x_mean\",\"or_x_std\",\"or_x_range\",\"or_y_mean\",\"or_y_std\",\"or_y_range\",\"or_z_mean\",\"or_z_std\",\"or_z_range\",\"or_w_mean\",\"or_w_std\",\"or_w_range\"]\n    ang_vels = [\"an_x_mean\",\"an_x_std\",\"an_x_range\",\"an_y_mean\",\"an_y_std\",\"an_y_range\",\"an_z_mean\",\"an_z_std\",\"an_z_range\"]\n    accels = [\"acc_x_mean\",\"acc_x_std\",\"acc_x_range\",\"acc_y_mean\",\"acc_y_std\",\"acc_y_range\",\"acc_z_mean\",\"acc_z_std\",\"acc_z_range\"]\n    columns = orientations + ang_vels+accels\n    processed = pd.DataFrame(index = range(0,num_samples+1), columns = columns)\n    #What you are about to see is an affront to good code, but I'm not entire sure how to make this manageable\n    for i in range(0,num_samples+1):\n        curr = input.loc[input[\"series_id\"]==i]\n        \n        or_x_mean = np.mean(curr[\"orientation_X\"])\n        processed[\"or_x_mean\"][i] = or_x_mean\n        or_x_std = np.std(curr[\"orientation_X\"])\n        processed[\"or_x_std\"][i] = or_x_std\n        or_x_range = max(curr[\"orientation_X\"])-min(curr[\"orientation_X\"])\n        processed[\"or_x_range\"][i] = or_x_range\n        \n        \n        or_y_mean = np.mean(curr[\"orientation_Y\"])\n        processed[\"or_y_mean\"][i] = or_y_mean\n        or_y_std = np.std(curr[\"orientation_Y\"])\n        processed[\"or_y_std\"][i] = or_y_std\n        or_y_range = max(curr[\"orientation_Y\"])-min(curr[\"orientation_Y\"])\n        processed[\"or_y_range\"][i] = or_y_range\n        \n        or_z_mean = np.mean(curr[\"orientation_Z\"])\n        processed[\"or_z_mean\"][i] = or_z_mean\n        or_z_std = np.std(curr[\"orientation_Z\"])\n        processed[\"or_z_std\"][i] = or_z_std\n        or_z_range = max(curr[\"orientation_Z\"])-min(curr[\"orientation_Z\"])\n        processed[\"or_z_range\"][i] = or_z_range\n        \n        or_w_mean = np.mean(curr[\"orientation_W\"])\n        processed[\"or_w_mean\"][i] = or_z_mean\n        or_w_std = np.std(curr[\"orientation_W\"])\n        processed[\"or_w_std\"][i] = or_z_std\n        or_w_range = max(curr[\"orientation_W\"])-min(curr[\"orientation_W\"])\n        processed[\"or_w_range\"][i] = or_z_range\n        \n        an_x_mean = np.mean(curr[\"angular_velocity_X\"])\n        processed[\"an_x_mean\"][i] = an_x_mean\n        an_x_std = np.std(curr[\"angular_velocity_X\"])\n        processed[\"an_x_std\"][i] = an_x_std\n        an_x_range = max(curr[\"angular_velocity_X\"])-min(curr[\"angular_velocity_X\"])\n        processed[\"an_x_range\"][i] = an_x_range\n        \n        \n        an_y_mean = np.mean(curr[\"angular_velocity_Y\"])\n        processed[\"an_y_mean\"][i] = an_y_mean\n        an_y_std = np.std(curr[\"angular_velocity_Y\"])\n        processed[\"an_y_std\"][i] = an_y_std\n        an_y_range = max(curr[\"angular_velocity_Y\"])-min(curr[\"angular_velocity_Y\"])\n        processed[\"an_y_range\"][i] = an_y_range\n        \n        an_z_mean = np.mean(curr[\"angular_velocity_Z\"])\n        processed[\"an_z_mean\"][i] = an_z_mean\n        an_z_std = np.std(curr[\"angular_velocity_Z\"])\n        processed[\"an_z_std\"][i] = an_z_std\n        an_z_range = max(curr[\"angular_velocity_Z\"])-min(curr[\"angular_velocity_Z\"])\n        processed[\"an_z_range\"][i] = an_z_range\n        \n        acc_x_mean = np.mean(curr[\"linear_acceleration_X\"])\n        processed[\"acc_x_mean\"][i] = acc_x_mean\n        acc_x_std = np.std(curr[\"linear_acceleration_X\"])\n        processed[\"acc_x_std\"][i] = acc_x_std\n        acc_x_range = max(curr[\"linear_acceleration_X\"])-min(curr[\"linear_acceleration_X\"])\n        processed[\"acc_x_range\"][i] = acc_x_range\n        \n        acc_y_mean = np.mean(curr[\"linear_acceleration_Y\"])\n        processed[\"acc_y_mean\"][i] = acc_y_mean\n        acc_y_std = np.std(curr[\"linear_acceleration_Y\"])\n        processed[\"acc_y_std\"][i] = acc_y_std\n        acc_y_range = max(curr[\"linear_acceleration_Y\"])-min(curr[\"linear_acceleration_Y\"])\n        processed[\"acc_y_range\"][i] = acc_y_range\n        \n        acc_z_mean = np.mean(curr[\"linear_acceleration_Z\"])\n        processed[\"acc_z_mean\"][i] = acc_z_mean\n        acc_z_std = np.std(curr[\"linear_acceleration_Z\"])\n        processed[\"acc_z_std\"][i] = acc_z_std\n        acc_z_range = max(curr[\"linear_acceleration_Z\"])-min(curr[\"linear_acceleration_Z\"])\n        processed[\"acc_z_range\"][i] = acc_z_range\n        \n    processed.to_csv(out_name)\n    return processed","execution_count":23,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now the first payoff- Building the trainining dataframes"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = processor(x_train,'training_processed.csv',3809)\ntest = processor(x_test,'testing_processed.csv',3815)\n\nle = preprocessing.LabelEncoder()\ny_train = le.fit_transform(y_train['surface'])","execution_count":24,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With this taken care of, we can get to building the actual model\nThis is another place where I think improvements can be made in shaping the model. This is my first idea for doing this"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"X = train.values\n\nmodel = keras.Sequential([\n\nkeras.layers.Dense(25, activation=tf.nn.relu),\nkeras.layers.BatchNormalization(), #Absolutely no idea if this will do anything, throwing data science at the wall and seeing what sticks\nkeras.layers.Dense(25, activation=tf.nn.relu),\nkeras.layers.Dense(9, activation=tf.nn.softmax)\n])\n\nstop = keras.callbacks.EarlyStopping(monitor='loss')\ncallbacks = [stop]\n\nmodel.compile(optimizer='adam', \nloss='sparse_categorical_crossentropy',\nmetrics=['accuracy'])\n\n\nmodel.fit(X, y_train, epochs=1000,batch_size = 32)\nmodel.save_weights('attempt.hd5')"},{"metadata":{},"cell_type":"markdown","source":"...But what if we did this another way? Time for me to find out what random forests are"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"X = train.values\n\nclf = RandomForestClassifier(n_estimators = 200)\n\nclf.fit(X,y_train)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#You know what, lets try this XGBoost thing they've been talking about\nmodel = XGBClassifier(n_estimators = 2000)\nX = train.values\nmodel.fit(X,y_train,early_stopping_rounds=5,eval_set = [(X,y_train)]) #Too laxy to split r/n,just seeing if this works\n\n\npredictions = model.predict(test.values)\npredictions = le.inverse_transform(predictions)\n\nindexes = range(0,3816)\ncolumns = [\"series_id\",\"surface\"]\nsubmission = pd.DataFrame(index = indexes, columns = columns)\nsubmission[\"series_id\"]= range(0,3816)\nsubmission[\"surface\"]=predictions\n\nsubmission.to_csv('predictions.csv', index = False)","execution_count":29,"outputs":[{"output_type":"stream","text":"[0]\tvalidation_0-merror:0.330184\nWill train until validation_0-merror hasn't improved in 5 rounds.\n[1]\tvalidation_0-merror:0.304199\n[2]\tvalidation_0-merror:0.320472\n[3]\tvalidation_0-merror:0.300262\n[4]\tvalidation_0-merror:0.27664\n[5]\tvalidation_0-merror:0.26168\n[6]\tvalidation_0-merror:0.258793\n[7]\tvalidation_0-merror:0.254068\n[8]\tvalidation_0-merror:0.255381\n[9]\tvalidation_0-merror:0.250131\n[10]\tvalidation_0-merror:0.245669\n[11]\tvalidation_0-merror:0.244882\n[12]\tvalidation_0-merror:0.243307\n[13]\tvalidation_0-merror:0.243307\n[14]\tvalidation_0-merror:0.24042\n[15]\tvalidation_0-merror:0.237795\n[16]\tvalidation_0-merror:0.232283\n[17]\tvalidation_0-merror:0.229921\n[18]\tvalidation_0-merror:0.226509\n[19]\tvalidation_0-merror:0.222835\n[20]\tvalidation_0-merror:0.22126\n[21]\tvalidation_0-merror:0.219685\n[22]\tvalidation_0-merror:0.217848\n[23]\tvalidation_0-merror:0.213386\n[24]\tvalidation_0-merror:0.209449\n[25]\tvalidation_0-merror:0.205249\n[26]\tvalidation_0-merror:0.202362\n[27]\tvalidation_0-merror:0.198688\n[28]\tvalidation_0-merror:0.197375\n[29]\tvalidation_0-merror:0.190026\n[30]\tvalidation_0-merror:0.189501\n[31]\tvalidation_0-merror:0.190289\n[32]\tvalidation_0-merror:0.187402\n[33]\tvalidation_0-merror:0.187927\n[34]\tvalidation_0-merror:0.185302\n[35]\tvalidation_0-merror:0.184252\n[36]\tvalidation_0-merror:0.178215\n[37]\tvalidation_0-merror:0.177428\n[38]\tvalidation_0-merror:0.175066\n[39]\tvalidation_0-merror:0.174016\n[40]\tvalidation_0-merror:0.172441\n[41]\tvalidation_0-merror:0.168241\n[42]\tvalidation_0-merror:0.166404\n[43]\tvalidation_0-merror:0.165092\n[44]\tvalidation_0-merror:0.165092\n[45]\tvalidation_0-merror:0.164829\n[46]\tvalidation_0-merror:0.16273\n[47]\tvalidation_0-merror:0.161155\n[48]\tvalidation_0-merror:0.15958\n[49]\tvalidation_0-merror:0.155381\n[50]\tvalidation_0-merror:0.154593\n[51]\tvalidation_0-merror:0.152493\n[52]\tvalidation_0-merror:0.151969\n[53]\tvalidation_0-merror:0.149344\n[54]\tvalidation_0-merror:0.147244\n[55]\tvalidation_0-merror:0.145669\n[56]\tvalidation_0-merror:0.144882\n[57]\tvalidation_0-merror:0.14357\n[58]\tvalidation_0-merror:0.140682\n[59]\tvalidation_0-merror:0.138583\n[60]\tvalidation_0-merror:0.137008\n[61]\tvalidation_0-merror:0.135433\n[62]\tvalidation_0-merror:0.131759\n[63]\tvalidation_0-merror:0.131234\n[64]\tvalidation_0-merror:0.128084\n[65]\tvalidation_0-merror:0.127559\n[66]\tvalidation_0-merror:0.127034\n[67]\tvalidation_0-merror:0.125984\n[68]\tvalidation_0-merror:0.125197\n[69]\tvalidation_0-merror:0.123885\n[70]\tvalidation_0-merror:0.123622\n[71]\tvalidation_0-merror:0.12231\n[72]\tvalidation_0-merror:0.120472\n[73]\tvalidation_0-merror:0.120472\n[74]\tvalidation_0-merror:0.11916\n[75]\tvalidation_0-merror:0.11811\n[76]\tvalidation_0-merror:0.115748\n[77]\tvalidation_0-merror:0.114961\n[78]\tvalidation_0-merror:0.113648\n[79]\tvalidation_0-merror:0.113123\n[80]\tvalidation_0-merror:0.112598\n[81]\tvalidation_0-merror:0.112073\n[82]\tvalidation_0-merror:0.110499\n[83]\tvalidation_0-merror:0.108924\n[84]\tvalidation_0-merror:0.107874\n[85]\tvalidation_0-merror:0.106299\n[86]\tvalidation_0-merror:0.103675\n[87]\tvalidation_0-merror:0.102625\n[88]\tvalidation_0-merror:0.101312\n[89]\tvalidation_0-merror:0.101837\n[90]\tvalidation_0-merror:0.100525\n[91]\tvalidation_0-merror:0.098425\n[92]\tvalidation_0-merror:0.097638\n[93]\tvalidation_0-merror:0.097113\n[94]\tvalidation_0-merror:0.095013\n[95]\tvalidation_0-merror:0.094488\n[96]\tvalidation_0-merror:0.093176\n[97]\tvalidation_0-merror:0.091076\n[98]\tvalidation_0-merror:0.090026\n[99]\tvalidation_0-merror:0.088189\n[100]\tvalidation_0-merror:0.088451\n[101]\tvalidation_0-merror:0.086877\n[102]\tvalidation_0-merror:0.086089\n[103]\tvalidation_0-merror:0.084777\n[104]\tvalidation_0-merror:0.083202\n[105]\tvalidation_0-merror:0.08084\n[106]\tvalidation_0-merror:0.080052\n[107]\tvalidation_0-merror:0.07874\n[108]\tvalidation_0-merror:0.078478\n[109]\tvalidation_0-merror:0.077428\n[110]\tvalidation_0-merror:0.076115\n[111]\tvalidation_0-merror:0.075328\n[112]\tvalidation_0-merror:0.074016\n[113]\tvalidation_0-merror:0.074278\n[114]\tvalidation_0-merror:0.071916\n[115]\tvalidation_0-merror:0.071916\n[116]\tvalidation_0-merror:0.070866\n[117]\tvalidation_0-merror:0.071654\n[118]\tvalidation_0-merror:0.070079\n[119]\tvalidation_0-merror:0.069029\n[120]\tvalidation_0-merror:0.067717\n[121]\tvalidation_0-merror:0.066404\n[122]\tvalidation_0-merror:0.065617\n[123]\tvalidation_0-merror:0.065354\n[124]\tvalidation_0-merror:0.064042\n[125]\tvalidation_0-merror:0.062205\n[126]\tvalidation_0-merror:0.061942\n[127]\tvalidation_0-merror:0.060367\n[128]\tvalidation_0-merror:0.059055\n[129]\tvalidation_0-merror:0.058793\n[130]\tvalidation_0-merror:0.058793\n[131]\tvalidation_0-merror:0.058005\n[132]\tvalidation_0-merror:0.057218\n[133]\tvalidation_0-merror:0.057743\n[134]\tvalidation_0-merror:0.056168\n[135]\tvalidation_0-merror:0.055643\n[136]\tvalidation_0-merror:0.055118\n[137]\tvalidation_0-merror:0.055118\n[138]\tvalidation_0-merror:0.054068\n[139]\tvalidation_0-merror:0.053281\n[140]\tvalidation_0-merror:0.052493\n[141]\tvalidation_0-merror:0.051444\n[142]\tvalidation_0-merror:0.050919\n[143]\tvalidation_0-merror:0.050394\n[144]\tvalidation_0-merror:0.050656\n[145]\tvalidation_0-merror:0.050656\n[146]\tvalidation_0-merror:0.049344\n[147]\tvalidation_0-merror:0.049869\n[148]\tvalidation_0-merror:0.048819\n[149]\tvalidation_0-merror:0.048819\n[150]\tvalidation_0-merror:0.048556\n[151]\tvalidation_0-merror:0.048031\n[152]\tvalidation_0-merror:0.047244\n[153]\tvalidation_0-merror:0.045144\n[154]\tvalidation_0-merror:0.044357\n[155]\tvalidation_0-merror:0.044619\n[156]\tvalidation_0-merror:0.044094\n[157]\tvalidation_0-merror:0.042782\n[158]\tvalidation_0-merror:0.043307\n[159]\tvalidation_0-merror:0.042782\n[160]\tvalidation_0-merror:0.04252\n[161]\tvalidation_0-merror:0.041995\n[162]\tvalidation_0-merror:0.040945\n[163]\tvalidation_0-merror:0.040945\n[164]\tvalidation_0-merror:0.040945\n[165]\tvalidation_0-merror:0.040157\n[166]\tvalidation_0-merror:0.039633\n[167]\tvalidation_0-merror:0.039633\n[168]\tvalidation_0-merror:0.03937\n[169]\tvalidation_0-merror:0.039108\n[170]\tvalidation_0-merror:0.038845\n[171]\tvalidation_0-merror:0.03832\n[172]\tvalidation_0-merror:0.037533\n[173]\tvalidation_0-merror:0.035958\n[174]\tvalidation_0-merror:0.036745\n[175]\tvalidation_0-merror:0.036745\n[176]\tvalidation_0-merror:0.035433\n[177]\tvalidation_0-merror:0.034908\n[178]\tvalidation_0-merror:0.033858\n[179]\tvalidation_0-merror:0.033858\n[180]\tvalidation_0-merror:0.033858\n[181]\tvalidation_0-merror:0.033858\n[182]\tvalidation_0-merror:0.033071\n[183]\tvalidation_0-merror:0.032808\n[184]\tvalidation_0-merror:0.032283\n[185]\tvalidation_0-merror:0.032283\n[186]\tvalidation_0-merror:0.032546\n[187]\tvalidation_0-merror:0.031759\n[188]\tvalidation_0-merror:0.031234\n[189]\tvalidation_0-merror:0.030446\n[190]\tvalidation_0-merror:0.030184\n[191]\tvalidation_0-merror:0.030184\n[192]\tvalidation_0-merror:0.030184\n[193]\tvalidation_0-merror:0.029659\n[194]\tvalidation_0-merror:0.029134\n[195]\tvalidation_0-merror:0.028871\n[196]\tvalidation_0-merror:0.028609\n[197]\tvalidation_0-merror:0.028084\n[198]\tvalidation_0-merror:0.028084\n[199]\tvalidation_0-merror:0.027822\n[200]\tvalidation_0-merror:0.027034\n[201]\tvalidation_0-merror:0.026772\n[202]\tvalidation_0-merror:0.026247\n[203]\tvalidation_0-merror:0.026247\n[204]\tvalidation_0-merror:0.025984\n[205]\tvalidation_0-merror:0.024934\n[206]\tvalidation_0-merror:0.024672\n[207]\tvalidation_0-merror:0.024409\n[208]\tvalidation_0-merror:0.024147\n[209]\tvalidation_0-merror:0.022835\n[210]\tvalidation_0-merror:0.02231\n[211]\tvalidation_0-merror:0.022047\n[212]\tvalidation_0-merror:0.02231\n[213]\tvalidation_0-merror:0.021785\n[214]\tvalidation_0-merror:0.022047\n[215]\tvalidation_0-merror:0.022047\n[216]\tvalidation_0-merror:0.02126\n[217]\tvalidation_0-merror:0.020735\n[218]\tvalidation_0-merror:0.020735\n[219]\tvalidation_0-merror:0.02021\n[220]\tvalidation_0-merror:0.019685\n[221]\tvalidation_0-merror:0.01916\n[222]\tvalidation_0-merror:0.018898\n[223]\tvalidation_0-merror:0.018635\n[224]\tvalidation_0-merror:0.018635\n[225]\tvalidation_0-merror:0.01811\n[226]\tvalidation_0-merror:0.017585\n[227]\tvalidation_0-merror:0.017585\n[228]\tvalidation_0-merror:0.016535\n[229]\tvalidation_0-merror:0.016273\n[230]\tvalidation_0-merror:0.01601\n[231]\tvalidation_0-merror:0.015748\n[232]\tvalidation_0-merror:0.015748\n[233]\tvalidation_0-merror:0.015748\n[234]\tvalidation_0-merror:0.015748\n[235]\tvalidation_0-merror:0.014173\n","name":"stdout"},{"output_type":"stream","text":"[236]\tvalidation_0-merror:0.013911\n[237]\tvalidation_0-merror:0.014173\n[238]\tvalidation_0-merror:0.014173\n[239]\tvalidation_0-merror:0.013911\n[240]\tvalidation_0-merror:0.013911\n[241]\tvalidation_0-merror:0.013386\n[242]\tvalidation_0-merror:0.013123\n[243]\tvalidation_0-merror:0.012598\n[244]\tvalidation_0-merror:0.012598\n[245]\tvalidation_0-merror:0.012336\n[246]\tvalidation_0-merror:0.012073\n[247]\tvalidation_0-merror:0.012073\n[248]\tvalidation_0-merror:0.012336\n[249]\tvalidation_0-merror:0.011811\n[250]\tvalidation_0-merror:0.011811\n[251]\tvalidation_0-merror:0.011811\n[252]\tvalidation_0-merror:0.011811\n[253]\tvalidation_0-merror:0.011286\n[254]\tvalidation_0-merror:0.011024\n[255]\tvalidation_0-merror:0.011024\n[256]\tvalidation_0-merror:0.011286\n[257]\tvalidation_0-merror:0.011286\n[258]\tvalidation_0-merror:0.010761\n[259]\tvalidation_0-merror:0.010761\n[260]\tvalidation_0-merror:0.010761\n[261]\tvalidation_0-merror:0.010761\n[262]\tvalidation_0-merror:0.010761\n[263]\tvalidation_0-merror:0.010761\nStopping. Best iteration:\n[258]\tvalidation_0-merror:0.010761\n\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Now we apply the model to the test set"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"#This is the predictions using the NN implementation\npredictions = model.predict(test)\nindexes = range(0,3816)\ntrue_predictions = np.zeros((3816,)) #Ask Mike about this one\n\n#convert the number arrays to actual surfaces\nfor i in range(len(predictions)):\n    true_predictions[i]= np.argmax(predictions[i])\n\ntrue_predictions = true_predictions.astype(int)\ntrue_predictions = le.inverse_transform(true_predictions)  \n    \ncolumns = [\"series_id\",\"surface\"]\nsubmission = pd.DataFrame(index = indexes, columns = columns)\n\nsubmission[\"series_id\"]= range(0,3816)\nsubmission[\"surface\"]=true_predictions\n\nsubmission.to_csv('predictions.csv', index = False)"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"#Random forest implementation\n\npredictions=clf.predict(test.values)\n\npredictions = le.inverse_transform(predictions)\n\nindexes = range(0,3816)\ncolumns = [\"series_id\",\"surface\"]\nsubmission = pd.DataFrame(index = indexes, columns = columns)\nsubmission[\"series_id\"]= range(0,3816)\nsubmission[\"surface\"]=predictions\n\nsubmission.to_csv('predictions.csv', index = False)"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"#Random forest implementation\n\npredictions=clf.predict(test.values)\n\npredictions = le.inverse_transform(predictions)\n\nindexes = range(0,3816)\ncolumns = [\"series_id\",\"surface\"]\nsubmission = pd.DataFrame(index = indexes, columns = columns)\nsubmission[\"series_id\"]= range(0,3816)\nsubmission[\"surface\"]=predictions\n\nsubmission.to_csv('predictions.csv', index = False)"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}