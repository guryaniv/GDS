{"cells": [{"metadata": {"collapsed": true, "_cell_guid": "59f6c310-712e-4828-afde-f71f0a1f6c47", "_uuid": "f5345ce5641a69b6c32a6d998bc71f46d796ffb0"}, "source": ["from sklearn.pipeline import Pipeline\n", "import lightgbm as lgb \n", "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n", "from sklearn.feature_extraction.text import TfidfTransformer\n", "from nltk.corpus import stopwords\n", "from nltk.tokenize import word_tokenize"], "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"collapsed": true, "_cell_guid": "30f410bb-6307-4c43-9163-ed64a69043ef", "_uuid": "1d78e8270556f3d5c583a9dac7c18d12cc069746"}, "source": ["train_merge = train_var.merge(train_text,left_on=\"ID\",right_on=\"ID\")\n", "train_merge.head(5)\n", "\n", "test_merge = test_var.merge(test_text,left_on=\"ID\",right_on=\"ID\")\n", "test_merge.head(5)\n"], "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"collapsed": true, "_cell_guid": "10742081-0769-416a-8938-3c3bcfece1ce", "_uuid": "db0678e2f60653c4e7a313003cdedc20d0ec117c"}, "source": ["import missingno as msno\n", "%matplotlib inline\n", "msno.bar(train_merge)"], "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"collapsed": true, "_cell_guid": "1b39a655-f253-4eb4-bd81-a772b9b199bd", "_uuid": "303a77690ffd0c75cc4bbf2f8257e1d7b42aa539"}, "source": ["import missingno as msno\n", "%matplotlib inline\n", "msno.bar(test_merge)"], "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"collapsed": true, "_cell_guid": "4358f16e-4fdc-400b-8823-5bba4d0525eb", "_uuid": "045207976f005559a8e358f20d8e950b00bf9f94"}, "source": ["import regex as re\n", "def textClean(text):\n", "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n", "    text = text.lower().split()\n", "    stops = {'so', 'his', 't', 'y', 'ours', 'herself', \n", "             'your', 'all', 'some', 'they', 'i', 'of', 'didn', \n", "             'them', 'when', 'will', 'that', 'its', 'because', \n", "             'while', 'those', 'my', 'don', 'again', 'her', 'if',\n", "             'further', 'now', 'does', 'against', 'won', 'same', \n", "             'a', 'during', 'who', 'here', 'have', 'in', 'being', \n", "             'it', 'other', 'once', 'itself', 'hers', 'after', 're',\n", "             'just', 'their', 'himself', 'theirs', 'whom', 'then', 'd', \n", "             'out', 'm', 'mustn', 'where', 'below', 'about', 'isn',\n", "             'shouldn', 'wouldn', 'these', 'me', 'to', 'doesn', 'into',\n", "             'the', 'until', 'she', 'am', 'under', 'how', 'yourself',\n", "             'couldn', 'ma', 'up', 'than', 'from', 'themselves', 'yourselves',\n", "             'off', 'above', 'yours', 'having', 'mightn', 'needn', 'on', \n", "             'too', 'there', 'an', 'and', 'down', 'ourselves', 'each',\n", "             'hadn', 'ain', 'such', 've', 'did', 'be', 'or', 'aren', 'he', \n", "             'should', 'for', 'both', 'doing', 'this', 'through', 'do', 'had',\n", "             'own', 'but', 'were', 'over', 'not', 'are', 'few', 'by', \n", "             'been', 'most', 'no', 'as', 'was', 'what', 's', 'is', 'you', \n", "             'shan', 'between', 'wasn', 'has', 'more', 'him', 'nor',\n", "             'can', 'why', 'any', 'at', 'myself', 'very', 'with', 'we', \n", "             'which', 'hasn', 'weren', 'haven', 'our', 'll', 'only',\n", "             'o', 'before'}\n", "    ## I ketp getting errors on importing the stopwords and I have no clue why\n", "    #stops = set(stopwords.words(\"English\"))\n", "    text = [w for w in text if not w in stops]    \n", "    text = \" \".join(text)\n", "    text = text.replace(\".\",\" \").replace(\",\",\" \")\n", "    return(text)"], "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"collapsed": true, "_cell_guid": "90b0f3c3-1655-4cf5-b848-fb0f23ad6e74", "_uuid": "20b344d2fe16af80286db816169e0afa6d810b43"}, "source": ["trainText = []\n", "for it in train_merge['Text']:\n", "    newT = textClean(it)\n", "    trainText.append(newT)\n", "testText = []\n", "for it in test_merge['Text']:\n", "    newT = textClean(it)\n", "    testText.append(newT)"], "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"collapsed": true, "_cell_guid": "378b78a6-dd89-4ccd-a00d-556ae21dbd5d", "_uuid": "214796ca24fd056908f44116a9cc4e3c5a83fb26"}, "source": ["train_merge['Clean_text']=trainText\n", "test_merge['Clean_text']=testText\n", "\n", "train_merge=train_merge.drop('ID',axis=1)\n", "train_merge=train_merge.drop('Text',axis=1)"], "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"collapsed": true, "_cell_guid": "d7090c6e-6f63-4eae-a040-3e68350110e4", "_uuid": "245f6820aa54f7006845109a7ee44e558681b1cb"}, "source": ["test_merge=test_merge.drop(['ID','Text'],axis=1)\n", "test_merge.head(5)"], "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"collapsed": true, "_cell_guid": "f86bd84d-7a6b-44ad-a7ef-e29993cd2532", "_uuid": "a19a3ad84fc8138ab38a3dbbf458ef6838827425"}, "source": ["from sklearn.model_selection import train_test_split\n", "train ,test = train_test_split(train_merge,test_size=0.2) \n", "np.random.seed(0)\n", "train.head(5)"], "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"collapsed": true, "_cell_guid": "56d52dee-4bee-46cc-a156-a8784ac05112", "_uuid": "ec261955a1f0f98725e1105748fb3a8b22cff6dd"}, "source": ["x_train = train['Clean_text'].values\n", "x_test = test['Clean_text'].values\n", "y_train = train['Class'].values\n", "y_test = test['Class'].values"], "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"collapsed": true, "_cell_guid": "1fa6722d-c4d5-436d-a140-cefb15af8068", "_uuid": "51e9536834c8f3d3ac7a87fd18053d7d92b74bf8"}, "source": ["def my_tokenizer(X):\n", "    newlist = []\n", "    for alist in X:\n", "        newlist.append(alist[0].split(' '))\n", "    return newlist\n", "\n", "maxFeats=500 \n", "\n", "cvec = CountVectorizer(min_df=5, ngram_range=(1,3), max_features=maxFeats, \n", "                       strip_accents='unicode',\n", "                       lowercase =True, analyzer='word', token_pattern=r'\\w+',\n", "                       stop_words = 'english',tokenizer=my_tokenizer)\n", "tfidf = TfidfVectorizer(min_df=5, max_features=maxFeats, ngram_range=(1,3),\n", "                        strip_accents='unicode',\n", "                        lowercase =True, analyzer='word', token_pattern=r'\\w+',\n", "                        use_idf=True, smooth_idf=True, sublinear_tf=True, \n", "                        stop_words = 'english')"], "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"collapsed": true, "_cell_guid": "293ba359-4240-4f6d-9e55-b8887349aed7", "_uuid": "9ccdcde5bc45d209c82c5c03df5264c7f9252440"}, "source": ["y_test=y_test-1\n", "y_train=y_train-1\n", "\n", "train_tran=tfidf.fit_transform(x_train)\n", "test_tran=tfidf.fit_transform(x_test)"], "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"collapsed": true, "_cell_guid": "9e9debbe-5453-4515-861b-e1d143032db4", "_uuid": "bcd3cdf20b9710e81c80f71d4910031004579f7a"}, "source": ["d_train = lgb.Dataset(train_tran, label=y_train)\n", "d_val = lgb.Dataset(test_tran, label=y_test)"], "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"collapsed": true, "_cell_guid": "6b33fc21-3279-41dc-97a1-4a0eca0fb355", "_uuid": "f7e1b5ca5f7f6af1825bad9c900cb17fd1292fb6"}, "source": ["parms = {'task': 'train',\n", "    'boosting_type': 'gbdt',\n", "    'objective': 'multiclass',\n", "    'num_class': 9,\n", "    'metric': {'multi_logloss'},\n", "    'learning_rate': 0.05, \n", "    'max_depth': 5,\n", "    'num_iterations': 400, \n", "    'num_leaves': 95, \n", "    'min_data_in_leaf': 60, \n", "    'lambda_l1': 1.0,\n", "    'feature_fraction': 0.8, \n", "    'bagging_fraction': 0.8, \n", "    'bagging_freq': 5}\n", "\n", "rnds = 500\n", "mod = lgb.train(parms, train_set=d_train, num_boost_round=rnds,\n", "               valid_sets=[d_val], valid_names=['dval'], verbose_eval=20,early_stopping_rounds=20)"], "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"collapsed": true, "_cell_guid": "a459a0af-b10e-4c57-9014-6f044ad615df", "_uuid": "8be4b51172c492f273cf684a7b7f8dcfe77688bf"}, "source": ["import matplotlib.pyplot as plt\n", "%matplotlib inline\n", "lgb.plot_importance(mod, max_num_features=30, figsize=(14,10))"], "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"collapsed": true, "_cell_guid": "6c0e8f10-068a-423c-b1cf-d14ee8f80ffc", "_uuid": "8d8f3997207e5026a9fe4d0fa82491ebfac23635"}, "source": ["test_data=test_merge['Clean_text']\n", "test_data=tfidf.fit_transform(test_data)"], "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"collapsed": true, "_cell_guid": "06f66f9e-ca59-4ed9-afdd-882e7d4b7059", "_uuid": "d942b3ceccbd18738678c6f7c5b859658b7d745b"}, "source": ["pred = mod.predict(test_data)"], "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"collapsed": true, "_cell_guid": "c5223024-d310-41fc-9418-2d704329dc4e", "_uuid": "31deeb350a6f0a2a1fff902622fdbca2437b2eb1"}, "source": ["pred1=pred\n", "pred1=(pred1 == pred1.max(axis=1)[:,None]).astype(int)"], "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"collapsed": true, "_cell_guid": "9381f289-105f-4c51-b151-680d02940b17", "_uuid": "e101b8236a92e10f7084c8ce5ae0b9d80059f55e"}, "source": ["submission=pd.DataFrame(pred1)\n", "submission['ID']=test_var['ID']\n", "submission.columns=[\"Class1\",\"Class2\",\"Class3\",\"Class4\",\"Class5\",\"Class6\",\"Class7\",\"Class8\",\"Class9\",\"ID\"]\n", "submission.head(5)"], "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"collapsed": true, "_cell_guid": "b1468149-ec1f-4cbc-998b-9a2ffe929cff", "_kg_hide-output": true, "_uuid": "e2d7a0b40fd668a4e0da4cbf6f94f07c105f92e3"}, "source": ["submission.to_csv('submission1.csv', index=False)"], "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"collapsed": true, "_cell_guid": "8f74a113-b61c-413d-a640-91890bb7802d", "_uuid": "b0da05484caab02d5934761886974756a2026be6"}, "source": ["import pandas as pd\n", "temp=pd.read_csv(\"../input/submissionFile\")"], "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "e7a9d102-08cf-40b3-9f88-c479857ee91f", "_uuid": "a3c6ee2990885372a4fee117afb6a708885446c2"}, "source": ["temp.to_csv(\"../output/result.csv\",index=False)"], "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"collapsed": true, "_cell_guid": "01ee99cd-6a93-4114-8b70-fac21f40150e", "_uuid": "eeae391e634639de8e25cf656df87949afcf9635"}, "source": [], "execution_count": null, "cell_type": "code", "outputs": []}], "nbformat": 4, "metadata": {"language_info": {"pygments_lexer": "ipython3", "nbconvert_exporter": "python", "name": "python", "codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "version": "3.6.1"}, "kernelspec": {"language": "python", "name": "python3", "display_name": "Python 3"}}, "nbformat_minor": 1}