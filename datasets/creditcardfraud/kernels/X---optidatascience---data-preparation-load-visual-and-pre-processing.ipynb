{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "12758b4d-ee50-43a2-71eb-cd4ba1e0a010"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "This notebook aims to introduce a standard way of \n",
        "\n",
        " 1. loading the data into python notebook \n",
        " 2. Visual and identify issues\n",
        " 3. Pre-process the data e.g., variable transformation, normalization and etc. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "6485f976-74b7-b991-04fe-1914beb62915"
      },
      "outputs": [],
      "source": [
        "## Load packages\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from scipy.stats import norm\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import normalize\n",
        "from scipy import stats\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "%matplotlib inline\n",
        "from ggplot import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "9249a1c7-0dbf-bd1f-a71f-7be8023584f1"
      },
      "outputs": [],
      "source": [
        "## Load data into Python\n",
        "\n",
        "alldata = pd.read_csv('../input/creditcard.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "bab69388-e719-788a-bdab-58ac705bd443"
      },
      "source": [
        "## Step 1: Take a peak\n",
        "\n",
        "Now take a peak at what's in the data. This including how many rows and columns and what are they. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "8885e760-a03c-7113-05dc-bf677206ae2c"
      },
      "outputs": [],
      "source": [
        "## What is in the data\n",
        "print(alldata.columns)\n",
        "print(alldata.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "64111417-1bb3-59dd-6c58-69a08ee21785"
      },
      "outputs": [],
      "source": [
        "alldata.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "cf3c64ba-1a2b-0fad-d569-eefe9de5d75a"
      },
      "source": [
        "### Take necessary transformations\n",
        "\n",
        " 1. Class is our dependent variable. It is coded as 0 and 1. Sometimes, we want it to be coded as Y/N. And other times, we want it to be either numeric or factor data types. \n",
        " 2. Usually when dollar is one of the fields, it is skewed as a lognormal distribution. We can take a log transformation. But when 0 is in the data, we can add 0.5 so that the transformation can be completed. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "bc3e12e5-312a-2451-26a0-16c9a80ddc4c"
      },
      "outputs": [],
      "source": [
        "## Add dummy variable Source \n",
        "## And convert known class type variables \n",
        "\n",
        "alldata['Source']='Train'\n",
        "alldata['Class']=alldata['Class'].astype(object)\n",
        "alldata['ClassInt']=alldata['Class'].astype(int)\n",
        "alldata['ClassYN']=['Y' if x == 1 else 'N' for x in alldata.Class]\n",
        "alldata['LogAmt']=np.log(alldata.Amount+0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "789785f6-131a-61f5-0dcb-7cfcc3ae24f3"
      },
      "outputs": [],
      "source": [
        "## Numerical and Categorical data types\n",
        "alldata_dtype=alldata.dtypes\n",
        "display_nvar = len(alldata.columns)\n",
        "alldata_dtype_dict = alldata_dtype.to_dict()\n",
        "alldata.dtypes.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "9ab56a58-bd77-adfa-add1-f58526414b27"
      },
      "source": [
        "## Step 2: Data Visualization\n",
        "\n",
        "### Variable Description\n",
        "\n",
        "I wrote this function with intension to compare train/test data and check if some variable is illy behaved. It is modified a little to fit this dataset to compared between normal/fraud subset. \n",
        "\n",
        "It can be applied to both numeric and object data types:\n",
        "\n",
        " 1. When the data type is object, it will output the value count of each categories\n",
        " 2. When the data type is numeric, it will output the quantiles\n",
        " 3. It also seeks any missing values in the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c92e4e94-55a3-b3a5-7f2d-e9f793f6f0f5"
      },
      "outputs": [],
      "source": [
        "def var_desc(dt):\n",
        "    print('--------------------------------------------')\n",
        "    for c in alldata.columns:\n",
        "        if alldata[c].dtype==dt:\n",
        "            t1 = alldata[alldata.Class==0][c]\n",
        "            t2 = alldata[alldata.Class==1][c]\n",
        "            if dt==\"object\":\n",
        "                f1 = t1[pd.isnull(t1)==False].value_counts()\n",
        "                f2 = t2[pd.isnull(t2)==False].value_counts()\n",
        "            else:\n",
        "                f1 = t1[pd.isnull(t1)==False].describe()\n",
        "                f2 = t2[pd.isnull(t2)==False].describe()\n",
        "            m1 = t1.isnull().value_counts()\n",
        "            m2 = t2.isnull().value_counts()\n",
        "            f = pd.concat([f1, f2], axis=1)\n",
        "            m = pd.concat([m1, m2], axis=1)\n",
        "            f.columns=['NoFraud','Fraud']\n",
        "            m.columns=['NoFraud','Fraud']\n",
        "            print(dt+' - '+c)\n",
        "            print('UniqValue - ',len(t1.value_counts()),len(t2.value_counts()))\n",
        "            print(f.sort_values(by='NoFraud',ascending=False))\n",
        "            print()\n",
        "\n",
        "            m_print=m[m.index==True]\n",
        "            if len(m_print)>0:\n",
        "                print('missing - '+c)\n",
        "                print(m_print)\n",
        "            else:\n",
        "                print('NO Missing values - '+c)\n",
        "            if dt!=\"object\":\n",
        "                if len(t1.value_counts())<=10:\n",
        "                    c1 = t1.value_counts()\n",
        "                    c2 = t2.value_counts()\n",
        "                    c = pd.concat([c1, c2], axis=1)\n",
        "                    f.columns=['NoFraud','Fraud']\n",
        "                    print(c)\n",
        "            print('--------------------------------------------')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "9fa04729-1c62-2143-3da5-b2d177472229"
      },
      "outputs": [],
      "source": [
        "var_desc('int64')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "78c437ec-a4e4-5433-60ad-f06ba74a0e3c"
      },
      "outputs": [],
      "source": [
        "var_desc('float64')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "0b5152c9-00a4-a733-fded-620565f10352"
      },
      "source": [
        "### Correlation\n",
        "\n",
        "Correlation is useful to find peers of input field so we are aware when building models, either to transform them (principal component) or remove one of the two. \n",
        "\n",
        "In this particular data, it is told that all the V-variables are already principal components. So correlation is not useful to inspect the data. It can be verified below to see correlation coefficient of 0 for those V-variables. \n",
        "\n",
        "But one thing we can do is to find some correlation between V-variables and Class. Recall now that coding Class into a integer data type would be useful. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "27ec926f-f347-f70d-9b64-cb925a12f383"
      },
      "outputs": [],
      "source": [
        "# Top 10 correlated variables\n",
        "corrmat = alldata.corr()\n",
        "k = 8 #number of variables for heatmap\n",
        "cols = corrmat.nlargest(k, 'ClassInt')['ClassInt'].index\n",
        "cm = np.corrcoef(alldata[cols].values.T)\n",
        "f, ax = plt.subplots(figsize=(8, 8))\n",
        "sns.set(font_scale=1.25)\n",
        "hm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "a3d640ae-e242-ca60-ad0f-ad64b04c1b09"
      },
      "source": [
        "### Distribution\n",
        "\n",
        "This is my favorite plot to inspect the data and it always gives hints on what are the important variables in your model. \n",
        "\n",
        "Violin plot with split between Fraud and Normal data will clear show some distinctions of V-variable distributions, and these V-variables are the keys to classify Fraud records. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "15fbd652-057a-b5f6-57e4-68c2ccda14b9"
      },
      "outputs": [],
      "source": [
        "## Violin plot of the same numeric variables\n",
        "\n",
        "dt = 'float64'\n",
        "sel_col = alldata.columns[alldata.dtypes==dt]\n",
        "\n",
        "plt.figure(figsize=(12,len(sel_col)*4))\n",
        "gs = gridspec.GridSpec(len(sel_col),1)\n",
        "for i, cn in enumerate(alldata[sel_col]):\n",
        "    ax = plt.subplot(gs[i])\n",
        "    data_1  = pd.concat([alldata[cn], alldata.ClassYN], axis=1)\n",
        "    data_2  = pd.melt(data_1,id_vars=cn)\n",
        "    sns.violinplot( x=cn, y='variable', hue=\"value\"\n",
        "                   ,data=data_2, palette=\"Set2\", split=True\n",
        "                   ,inner=\"quartile\")\n",
        "    ax.set_xlabel('')\n",
        "    ax.set_title('Violin plot of : ' + str(cn))\n",
        "plt.show()\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "dabb7d09-03d7-8b34-8516-daae3e2c5f5a"
      },
      "source": [
        "## Step 3: Feature Engineering\n",
        "\n",
        "Now with all the inspection we've done. We can help the *machine* to create some variables to predict the outcome. Feature engineering is to make additional variables as input features. I usually take the following steps, \n",
        "\n",
        " 1. Covert numeric variable to factors when there are only a few levels for the numeric values;\n",
        " 2. Transform skewed variable, for example, we already did for Amount \n",
        " 3. Treat extremely small categories: usually we can merge them, or re-code them.. We don't have to worry it in this problem as there are no categorical variables\n",
        " 4. Treat missing data: this can be a big issue for some problems. We can randomly assign a value, impute, assign an average, or create a dummy variable for those missing records. In this problem, we don't have to worry as there are no missing values. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "eb92ab84-8bd8-877c-1d95-8e1b1a466297"
      },
      "outputs": [],
      "source": [
        "## Treating skewed continuous data - transformation\n",
        "## already did it"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "24dbcf5c-3dad-df33-4d49-6bbf0f8b8597"
      },
      "source": [
        "## Step 4: Prepare train, validation, test dataset\n",
        "\n",
        " 1. Remove unnecessary fields\n",
        " 2. Normalize numeric fields\n",
        " 3. Think about splitting, how to split between train/test, is ensemble/stacking needed? How to split between stage 1, 2, and 3 stacking?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "f7a641f4-edeb-82c1-b989-fe6ad95c22aa"
      },
      "outputs": [],
      "source": [
        "## Remove unnessary columns\n",
        "list_col_rm = ['Amount','Class','Source','ClassYN']\n",
        "list_col_keep = alldata.columns.difference(list_col_rm)\n",
        "print(list_col_keep)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "decc3452-71cf-613f-4469-a5cf06c83b4b"
      },
      "outputs": [],
      "source": [
        "## normalize numeric variables\n",
        "##\n",
        "excl_cols = ['ClassInt']\n",
        "alldata_dtype_dict = alldata.dtypes.to_dict()\n",
        "for c in alldata.columns:\n",
        "    if c in list_col_keep and c not in excl_cols and alldata_dtype_dict[c]!='object':\n",
        "        print('----------------------')\n",
        "        print(c , alldata_dtype_dict[c])\n",
        "        alldata[c] = (alldata[c]-alldata[c].mean())/(alldata[c].std())\n",
        "print()\n",
        "print(alldata.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "55e4698d-32b9-cde0-8eb9-fda7c36fcc37"
      },
      "outputs": [],
      "source": [
        "## Data Sampling, we do a 80/20 split on train/test.. \n",
        "## will talk about stacking later on. \n",
        "trainY = alldata[alldata.Class==1].sample(frac=0.8)\n",
        "trainN = alldata[alldata.Class==0].sample(frac=0.8)\n",
        "train = pd.concat([trainY, trainN], axis = 0)\n",
        "test  = alldata.loc[~alldata.index.isin(train.index)]\n",
        "print(train.shape)\n",
        "print(test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "71766e59-63bf-c479-4fe6-f9974e499287"
      },
      "outputs": [],
      "source": [
        "## Save processed data\n",
        "## so I can download into a new notebook to build models\n",
        "import pickle\n",
        "file_obj = open('./data.p', 'wb') \n",
        "pickle.dump([train, test, list_col_keep], file_obj) \n",
        "file_obj.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "5b2a96ee-f17c-3b7c-b359-592d14f631b2"
      },
      "source": [
        "### I will start a new notebook to build models "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "df353c18-d66b-31e9-0b60-4565ee58333e"
      },
      "outputs": [],
      "source": ""
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}