{"cells":[{"metadata":{"_uuid":"a864b0fece8600cd96a3b21d3fbc4603f19b2d29"},"cell_type":"markdown","source":"As you have probably noticed (or calculated), there is no way how to get all the dataset into the RAM given the 16 GB of Kaggle Kernels (for float32 representation of inputs, you would need around 128 GB of RAM just for training dataset itself, if my calculations are correct)."},{"metadata":{"trusted":true,"_uuid":"4d08e4c00fa45478ce86ce8168fafc87c393d9fe"},"cell_type":"code","source":"import sys\nimport numpy as np\n\noneImage8 = np.zeros((1, 512, 512, 4), dtype=np.int8)\noneImage16 = np.zeros((1, 512, 512, 4), dtype=np.float16)\noneImage32 = np.zeros((1, 512, 512, 4), dtype=np.float32)\nprint('Size of all training images, if encoded as int8: ', sys.getsizeof(oneImage8) * 31072 / 1024 / 1024 / 1024, ' GB')\nprint('Size of all training images, if encoded as float16: ', sys.getsizeof(oneImage16) * 31072 / 1024 / 1024 / 1024, ' GB')\nprint('Size of all training images, if encoded as float32: ', sys.getsizeof(oneImage32) * 31072 / 1024 / 1024 / 1024, ' GB')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"281db121defcb1b9ae7d22aa8251ba90a1ef59af"},"cell_type":"markdown","source":"This kernel tries to play around with custom data generator for fast on-fly data loading. It's written as a descendant of **keras.utils.Sequence**, which has the nice property to be compatible with the **use_multiprocessing=True** setting in *model.fit_generator()* function.\n\nNow with added caching: If you have enough RAM (e.g. running this script on Google Cloud highmem VC), you can let the Data Generator save all the data into RAM and then use these in the following epochs. Do not try this on Kaggle Kernels with full training dataset, though.\nCurrently, using cache cannot be combined with **use_multiprocessing=True** because this parameter leads to creating multiple DataGenerator objects that do not have any shared variables. Solutions to this are welcome."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import keras\nfrom keras.utils import Sequence\nfrom PIL import Image\nfrom matplotlib import pyplot as plt\nimport pandas as pd\nfrom tqdm import tqdm\nimport os","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b3fd4b30584e8fdf544f23d1a56028fa5fa05d1f"},"cell_type":"code","source":"BATCH_SIZE = 16\nSEED = 777\nSHAPE = (512, 512, 4)\nDIR = '../input'\nVAL_RATIO = 0.1 # 10 % as validation\nDEBUG = True\nTHRESHOLD = 0.05 # due to different cost of True Positive vs False Positive, this is the probability threshold to predict the class as 'yes'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fd1045b817e77ff540e789ea3b0eea22bcdb9a6f"},"cell_type":"code","source":"def getTrainDataset():\n    \n    path_to_train = DIR + '/train/'\n    data = pd.read_csv(DIR + '/train.csv')\n\n    paths = []\n    labels = []\n    \n    for name, lbl in zip(data['Id'], data['Target'].str.split(' ')):\n        y = np.zeros(28)\n        for key in lbl:\n            y[int(key)] = 1\n        paths.append(os.path.join(path_to_train, name))\n        labels.append(y)\n\n    return np.array(paths), np.array(labels)\n\ndef getTestDataset():\n    \n    path_to_test = DIR + '/test/'\n    data = pd.read_csv(DIR + '/sample_submission.csv')\n\n    paths = []\n    labels = []\n    \n    for name in data['Id']:\n        y = np.ones(28)\n        paths.append(os.path.join(path_to_test, name))\n        labels.append(y)\n\n    return np.array(paths), np.array(labels)\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# credits: https://github.com/keras-team/keras/blob/master/keras/utils/data_utils.py#L302\n# credits: https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly\n\nclass ProteinDataGenerator(keras.utils.Sequence):\n            \n    def __init__(self, paths, labels, batch_size, shape, shuffle = False, use_cache = False):\n        self.paths, self.labels = paths, labels\n        self.batch_size = batch_size\n        self.shape = shape\n        self.shuffle = shuffle\n        self.use_cache = use_cache\n        if use_cache == True:\n            self.cache = np.zeros((paths.shape[0], shape[0], shape[1], shape[2]))\n            self.is_cached = np.zeros((paths.shape[0]))\n        self.on_epoch_end()\n    \n    def __len__(self):\n        return int(np.ceil(len(self.paths) / float(self.batch_size)))\n    \n    def __getitem__(self, idx):\n        indexes = self.indexes[idx * self.batch_size : (idx+1) * self.batch_size]\n\n        paths = self.paths[indexes]\n        X = np.zeros((paths.shape[0], self.shape[0], self.shape[1], self.shape[2]))\n        # Generate data\n        if self.use_cache == True:\n            X = self.cache[indexes]\n            for i, path in enumerate(paths[np.where(self.is_cached[indexes] == 0)]):\n                image = self.__load_image(path)\n                self.is_cached[indexes[i]] = 1\n                self.cache[indexes[i]] = image\n                X[i] = image\n        else:\n            for i, path in enumerate(paths):\n                X[i] = self.__load_image(path)\n\n        y = self.labels[indexes]\n        \n        return X, y\n    \n    def on_epoch_end(self):\n        \n        # Updates indexes after each epoch\n        self.indexes = np.arange(len(self.paths))\n        if self.shuffle == True:\n            np.random.shuffle(self.indexes)\n\n    def __iter__(self):\n        \"\"\"Create a generator that iterate over the Sequence.\"\"\"\n        for item in (self[i] for i in range(len(self))):\n            yield item\n            \n    def __load_image(self, path):\n        R = Image.open(path + '_red.png')\n        G = Image.open(path + '_green.png')\n        B = Image.open(path + '_blue.png')\n        Y = Image.open(path + '_yellow.png')\n\n        im = np.stack((\n            np.array(R), \n            np.array(G), \n            np.array(B),\n            np.array(Y)), -1)\n        \n        im = np.divide(im, 255)\n        return im","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0e9180e931f97c2b508ad671f3b6849f3ec999ba"},"cell_type":"code","source":"paths, labels = getTrainDataset()\ntg = ProteinDataGenerator(paths, labels, BATCH_SIZE, SHAPE)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c3e0458299b13fed6a20ec583fd3c0202a551061"},"cell_type":"markdown","source":"Let's measure the time to get 128 images. Standard methods are around ~4 seconds on Kaggle Kernels."},{"metadata":{"trusted":true,"_uuid":"9e7765ce0212b8529270455fb6bdcec9501b2047"},"cell_type":"code","source":"%%time\nfor i in range(8):\n    im, lbl = tg[i]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b88e906d58cc26be222973f86747b88547e34fec"},"cell_type":"markdown","source":"Let's test the RAM caching functionality (and measure the loading time):"},{"metadata":{"trusted":true,"_uuid":"cfdcd4ac899716100bf2f15cb3af7884f018a1f4"},"cell_type":"code","source":"tg_cache = ProteinDataGenerator(paths[0:200], labels[0:200], BATCH_SIZE, SHAPE, use_cache=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e0dec02d26dc91b3e59f2a0305c4e24603e88696"},"cell_type":"code","source":"%%time\n#first reading of 128 images should take same time as before\nfor i in range(8):\n    im, lbl = tg_cache[i]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6831661622e604ff1ab19f979b68d891de5efe47"},"cell_type":"code","source":"%%time\n#second reading from RAM should be MUCH faster\nfor i in range(8):\n    im, lbl = tg_cache[i]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"42ff803ff2a936773842b1a230083a8885068a5b"},"cell_type":"code","source":"# read data from DB\nim, lbl = tg[0]\n\nfig, ax = plt.subplots(4, 4, figsize=(50,50))\n\nfor row in range(4):\n    for col in range(4):\n        ax[row, col].imshow(im[row*4+col, :, :, 0:3])\n        #plt.imshow(image[row*4+col, :, :, 0:3]) # first three channels are RGB, fourth is yellow\n\nplt.show()\ndel(tg, tg_cache)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"97a72f6757c7c2a0e6c8116eb604d185dc6c64ed"},"cell_type":"markdown","source":"# Using in Keras\nLet's try to test the multi_processing."},{"metadata":{"trusted":true,"_uuid":"31aff62537d634ee34cd7752025d98615b1fc8e9"},"cell_type":"code","source":"from keras.preprocessing.image import ImageDataGenerator\nfrom keras.models import Sequential, load_model\nfrom keras.layers import Activation, Dropout, Flatten, Dense, Input, Conv2D, MaxPooling2D, BatchNormalization\nfrom keras import metrics\nfrom keras.optimizers import Adam\nfrom keras.callbacks import ModelCheckpoint\nfrom keras import backend as K\nimport keras\nimport tensorflow as tf\n\nfrom tensorflow import set_random_seed\nset_random_seed(SEED)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5748b96367ce78ea9bce4a34a40a66dd5a49e945"},"cell_type":"code","source":"# credits: https://www.kaggle.com/guglielmocamporese/macro-f1-score-keras\n\ndef f1(y_true, y_pred):\n    #y_pred = K.round(y_pred)\n    y_pred = K.cast(K.greater(K.clip(y_pred, 0, 1), THRESHOLD), K.floatx())\n    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n\n    p = tp / (tp + fp + K.epsilon())\n    r = tp / (tp + fn + K.epsilon())\n\n    f1 = 2*p*r / (p+r+K.epsilon())\n    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n    return K.mean(f1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2d9bbc05fdf72242f8f98422ef9c480a353f4fe3"},"cell_type":"code","source":"# some basic useless model\ndef create_model(input_shape):\n    \n    model = Sequential()\n    model.add(Conv2D(8, (3, 3), activation='relu', input_shape=input_shape))\n    model.add(BatchNormalization(axis=-1))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Dropout(0.25))\n    model.add(Conv2D(16, (3, 3), activation='relu'))\n    model.add(BatchNormalization(axis=-1))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Dropout(0.25))\n    model.add(Conv2D(32, (3, 3), activation='relu'))\n    model.add(BatchNormalization(axis=-1))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Dropout(0.25))\n    model.add(Conv2D(64, (3, 3), activation='relu'))\n    model.add(BatchNormalization(axis=-1))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Dropout(0.25))\n    model.add(Conv2D(128, (3, 3), activation='relu'))\n    model.add(BatchNormalization(axis=-1))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Dropout(0.25))\n    model.add(Conv2D(256, (3, 3), activation='relu'))\n    model.add(BatchNormalization(axis=-1))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Dropout(0.25))\n    model.add(Flatten())\n    model.add(Dropout(0.5))\n    #model.add(Dense(28))\n    #model.add(Activation('relu'))\n    #model.add(Dropout(0.1))\n    model.add(Dense(28))\n    model.add(Activation('sigmoid'))\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a4982e06ba3bb236e2bb10b0b08683c93149a61c"},"cell_type":"code","source":"model = create_model((512,512,4))\nmodel.compile(\n    loss='binary_crossentropy', \n    optimizer=Adam(0.0001),\n    metrics=['acc',f1])\n\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b8831e442b8f5461dfc37da4292f66e1684c84c8"},"cell_type":"code","source":"paths, labels = getTrainDataset()\n\n# divide to \nkeys = np.arange(paths.shape[0], dtype=np.int)  \nnp.random.seed(SEED)\nnp.random.shuffle(keys)\nlastTrainIndex = int((1-VAL_RATIO) * paths.shape[0])\n\nif DEBUG == True:  # use only small subset for debugging, Kaggle's RAM is limited\n    pathsTrain = paths[0:256]\n    labelsTrain = labels[0:256]\n    pathsVal = paths[lastTrainIndex:lastTrainIndex+256]\n    labelsVal = labels[lastTrainIndex:lastTrainIndex+256]\n    use_cache = True\nelse:\n    pathsTrain = paths[0:lastTrainIndex]\n    labelsTrain = labels[0:lastTrainIndex]\n    pathsVal = paths[lastTrainIndex:]\n    labelsVal = labels[lastTrainIndex:]\n    use_cache = False\n\nprint(paths.shape, labels.shape)\nprint(pathsTrain.shape, labelsTrain.shape, pathsVal.shape, labelsVal.shape)\n\ntg = ProteinDataGenerator(pathsTrain, labelsTrain, BATCH_SIZE, SHAPE, use_cache=use_cache)\nvg = ProteinDataGenerator(pathsVal, labelsVal, BATCH_SIZE, SHAPE, use_cache=use_cache)\n\n# https://keras.io/callbacks/#modelcheckpoint\ncheckpoint = ModelCheckpoint('./base.model', monitor='val_f1', verbose=1, save_best_only=True, save_weights_only=False, mode='max', period=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"78b72de7c88f7845f92a0cc93eeea7a5498cdb72"},"cell_type":"code","source":"epochs = 50\n\nif DEBUG == True:\n    use_multiprocessing = False # DO NOT COMBINE WITH CACHE! \n    workers = 1 # DO NOT COMBINE WITH CACHE! \nelse:\n    use_multiprocessing = True\n    workers = 2\n\nhist = model.fit_generator(\n    tg,\n    steps_per_epoch=len(tg),\n    validation_data=vg,\n    validation_steps=8,\n    epochs=epochs,\n    use_multiprocessing=use_multiprocessing, # you have to train the model on GPU in order to this to be benefitial\n    workers=workers, # you have to train the model on GPU in order to this to be benefitial\n    verbose=1,\n    callbacks=[checkpoint])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9237d3586a1bba31a3a5ef7f8e81a91827d94743"},"cell_type":"code","source":"fig, ax = plt.subplots(1, 2, figsize=(15,5))\nax[0].set_title('loss')\nax[0].plot(hist.epoch, hist.history[\"loss\"], label=\"Train loss\")\nax[0].plot(hist.epoch, hist.history[\"val_loss\"], label=\"Validation loss\")\nax[1].set_title('acc')\nax[1].plot(hist.epoch, hist.history[\"f1\"], label=\"Train F1\")\nax[1].plot(hist.epoch, hist.history[\"val_f1\"], label=\"Validation F1\")\nax[0].legend()\nax[1].legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f76abc9bb2cae0d85fc281c7582b3d642d2f1ff8"},"cell_type":"code","source":"#fullValGen = ProteinDataGenerator(paths[lastTrainIndex:], labels[lastTrainIndex:], BATCH_SIZE, SHAPE)\n#fullValPred = np.zeros((paths[lastTrainIndex:].shape[0], 28))\n#for i in tqdm(range(len(fullValGen))):\nbestModel = load_model('./base.model', custom_objects={'f1': f1})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"982c438bc6d8fa9208ba6050312abc9200f84799"},"cell_type":"code","source":"pathsTest, labelsTest = getTestDataset()\n\ntestg = ProteinDataGenerator(pathsTest, labelsTest, BATCH_SIZE, SHAPE)\nsubmit = pd.read_csv(DIR + '/sample_submission.csv')\nP = np.zeros((pathsTest.shape[0], 28))\nfor i in tqdm(range(len(testg))):\n    images, labels = testg[i]\n    score = bestModel.predict(images)\n    P[i*BATCH_SIZE:i*BATCH_SIZE+score.shape[0]] = score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f52ecf0f38c5b61e97c31698d2f0db69a6a48184"},"cell_type":"code","source":"PP = np.array(P)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"668fd597b69767521b17a4a23320d3455d70bc73"},"cell_type":"code","source":"prediction = []\n\nfor row in tqdm(range(submit.shape[0])):\n    \n    str_label = ''\n    \n    for col in range(PP.shape[1]):\n        if(PP[row, col] < THRESHOLD):   # to account for losing TP is more costly than decreasing FP\n            #print(PP[row])\n            str_label += ''\n        else:\n            str_label += str(col) + ' '\n    prediction.append(str_label.strip())\n    \nsubmit['Predicted'] = np.array(prediction)\nsubmit.to_csv('datagenerator_model_v1.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0315dd474abf610cffee8c15388bfec32ad35e43"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}