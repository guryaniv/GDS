{"cells":[{"metadata":{"_cell_guid":"2386d98e-728c-44d4-9158-9f51d910d61a","_uuid":"d22e617bfd98e8df727eed405fd73cfb8b76a56b"},"cell_type":"markdown","source":"# Loading Required Libraries"},{"metadata":{"_cell_guid":"3e2702ef-cdb1-4fc1-8a8d-043bf290454c","scrolled":false,"id":"VO6TORg-yl3l","colab":{"autoexec":{"wait_interval":0,"startup":false}},"colab_type":"code","_uuid":"b4f953c1e3e65c804d84bab1bbb8096284eba9bb","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport re\n\n# Importing required libraries\nfrom sklearn.model_selection import train_test_split\nfrom nltk.corpus import stopwords\n\n# keras\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils import to_categorical\n\nfrom keras.layers import Embedding\n\nfrom keras.layers import Dense, Input, GlobalMaxPooling1D\nfrom keras.layers import Conv1D, MaxPooling1D, Embedding\nfrom keras.models import Model\nfrom keras.layers import Input, Dense, Embedding, Conv2D, MaxPooling2D, Dropout,concatenate\nfrom keras.layers.core import Reshape, Flatten\nfrom keras.callbacks import EarlyStopping\nfrom keras.optimizers import SGD, RMSprop, Adagrad, Adadelta, Adam, Adamax, Nadam\nfrom keras.models import Model\nfrom keras import regularizers\n\n# gensim\nimport gensim\nfrom gensim.models import Word2Vec\nfrom gensim.utils import simple_preprocess\n\nfrom gensim.models.keyedvectors import KeyedVectors","execution_count":1,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"e50b8e65-c196-40a6-9e17-3f82b2ee00a9","_uuid":"59166afb907bdaec7f6bc502283b1576c886cfc0","id":"POSiAyaZy5Lm","colab":{"autoexec":{"wait_interval":0,"startup":false}},"colab_type":"code","trusted":true},"cell_type":"code","source":"# defining function to clean text and retrive closs-validation datasets\ndef cleantxt(txt):\n    \"\"\"\n    Cleans the string passed. Cleaning Includes-\n    1. remove special characters/symbols\n    2. convert text to lower-case\n    3. retain only alphabets\n    4. remove words less than 3 characters\n    5. remove stop-words\n    \"\"\"  \n    # collecting english stop words from nltk-library\n    stpw = stopwords.words('english')\n    \n    # Adding custom stop-words\n    stpw.extend(['www','http','utc'])\n    stpw = set(stpw)\n    \n    # using regex to clean the text\n    txt = re.sub(r\"\\n\", \" \", txt)\n    txt = re.sub(\"[\\<\\[].*?[\\>\\]]\", \" \", txt)\n    txt = txt.lower()\n    txt = re.sub(r\"[^a-z ]\", \" \", txt)\n    txt = re.sub(r\"\\b\\w{1,3}\\b\", \" \",txt)\n    txt = \" \".join([x for x in txt.split() if x not in stpw])\n    return txt\n\n\ndef load_data():\n    \"\"\"\n    Loads data and returns train, val, and test splits\n    \"\"\"\n    # Load the train dataset\n    df = pd.read_csv(\"../input/jigsaw-toxic-comment-classification-challenge/train.csv\")\n    \n    # Clean the text\n    df['comment_text'] = df.comment_text.apply(lambda x : cleantxt(x))\n    \n    # separate explanatory and dependent variables\n    X = df.iloc[:,1]\n    y = df.iloc[:,2:]\n\n    # split for cross-validation (train-60%, validation 20% and test 20%)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=123)\n    X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.5, random_state=123)\n\n    return X_train, X_val, X_test, y_train, y_val, y_test","execution_count":2,"outputs":[]},{"metadata":{"_cell_guid":"3a95424d-bd70-4e0d-8d9d-ace05da076a3","colab_type":"text","_uuid":"fedc9b5a07c8c77381f2a3d2ead7a447b176d22c","id":"B2XrS6MByhLp"},"cell_type":"markdown","source":"# Implementation of CNN on word embeddings using word2vec\n"},{"metadata":{"_cell_guid":"7cdfbb35-22ef-4c14-8b21-0f32f9a36eb1","colab_type":"text","_uuid":"44b9f25456cd348d1f2ebc66ee7550b793ebb17d","id":"CZAqRc-8yhLt"},"cell_type":"markdown","source":"## 1. Loading Data"},{"metadata":{"collapsed":true,"_cell_guid":"a66755fd-26a8-4956-ade4-54f7c5dce812","scrolled":true,"id":"pgUlZs87yhL_","colab":{"autoexec":{"wait_interval":0,"startup":false}},"colab_type":"code","_uuid":"51e82dd01d9e21749cd222c2209153628d01eccb","trusted":true},"cell_type":"code","source":"# Load the data\nX_train, X_val, X_test, y_train, y_val, y_test = load_data()","execution_count":3,"outputs":[]},{"metadata":{"_cell_guid":"daf4d78b-f06c-4795-a450-f4d52f56017b","colab_type":"text","_uuid":"5fc9d863868ee3fd4bab805929dbd66056d07f21","id":"xrRmlaX-yhMJ"},"cell_type":"markdown","source":"### 2. Tokenize text of the training data with keras text preprocessing functions ###"},{"metadata":{"collapsed":true,"_cell_guid":"50f3d6c6-978f-4432-93ac-8ebca6229c41","_uuid":"c7869a47bdb2bac61cd9ee294b420225d5702b3b","scrolled":true,"trusted":true},"cell_type":"code","source":"# Adding list of Bad words to tokanizer\nbad_words = pd.read_csv(\"../input/bad-words/bad_words.csv\")\nbad_words =  list(bad_words.bad_words.values)","execution_count":4,"outputs":[]},{"metadata":{"_cell_guid":"51767870-5140-42f8-be67-3fa12779f6ed","_uuid":"0aedc385ea0a454a8a5bfa956ea2addc06c9d569","trusted":true},"cell_type":"code","source":"# Set Maximum number of words to be embedded\nNUM_WORDS = 20000\n\n# Define/Load Tokenize text function\ntokenizer = Tokenizer(num_words=NUM_WORDS,filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\\'',\n                      lower=True)\n\n# Fit the function on the text\ntokenizer.fit_on_texts(X_train)\n\n# Count number of unique tokens\nword_index = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word_index))","execution_count":5,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"28c252a2-8f9e-4d16-b02b-7f007071bc60","_uuid":"f6832b80483e4f151be6ae822cdfea206b57fa99","trusted":true},"cell_type":"code","source":"# Find top words/tokens by frequency\n# [pair[0] for pair in sorted(tokenizer.word_counts.items(), key=lambda item: item[1])]","execution_count":6,"outputs":[]},{"metadata":{"_cell_guid":"72e8c46f-090f-4aba-aa2a-2a95eac58932","_uuid":"cbaa5b2e987cf1fb35f8430783e09d7ac1284dd7","trusted":true},"cell_type":"code","source":"# size of bad words\nnum_badwords = len(bad_words)\nnum_badwords","execution_count":7,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"8a63c2e0-424e-4f34-8d6b-3a37cf379ce4","_uuid":"a649db269c1a6ab5f3640bbb684e48f9a2fded24","trusted":true},"cell_type":"code","source":"# Adding list of bad words to tokanizer\nn = 0\ntemp_bw = bad_words\nfor word, i in word_index.items():\n    if word in bad_words:\n        temp_bw.remove(word)\n        n = n+1\n    if i > (NUM_WORDS-num_badwords+n):\n        for bw in temp_bw:\n            tokenizer.word_index[bw] = i\n            i=i+1\n        break           ","execution_count":8,"outputs":[]},{"metadata":{"collapsed":true,"outputId":"9424bff9-4cab-430e-b6b3-58d3006b2550","_uuid":"8e08b7965c4d034d92c7b8bc7cb1ffa3da77934a","id":"IEAt7RF4yhMm","_cell_guid":"e503736e-05b7-46c5-a50b-aca174879f9d","colab":{"base_uri":"https://localhost:8080/","autoexec":{"wait_interval":0,"startup":false},"height":34},"executionInfo":{"status":"ok","user_tz":-60,"user":{"userId":"103216699183948428762","displayName":"Ashish Poigal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128"},"timestamp":1523837964999,"elapsed":8649},"colab_type":"code","scrolled":true,"trusted":true},"cell_type":"code","source":"# Convert train and val to sequence\nsequences_train = tokenizer.texts_to_sequences(X_train)\nsequences_valid=tokenizer.texts_to_sequences(X_val)","execution_count":9,"outputs":[]},{"metadata":{"outputId":"7d8878d3-a9f6-46e4-ddda-443b5da6caed","_uuid":"b7c6251b25843a9fab0db68cb59e70cf2f6bcd34","id":"T2O3DOboyhMz","_cell_guid":"af164756-4e7b-4f0c-acb3-816305ca7b9f","colab":{"base_uri":"https://localhost:8080/","autoexec":{"wait_interval":0,"startup":false},"height":52},"executionInfo":{"status":"ok","user_tz":-60,"user":{"userId":"103216699183948428762","displayName":"Ashish Poigal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128"},"timestamp":1523838021410,"elapsed":1605},"colab_type":"code","trusted":true},"cell_type":"code","source":"# Limit size of train/val to 50 and pad the sequence\nX_train = pad_sequences(sequences_train,maxlen=50)\nX_val = pad_sequences(sequences_valid,maxlen=X_train.shape[1])\n\n# Convert target to array\ny_train = np.asarray(y_train)\ny_val = np.asarray(y_val)\n\n# Printing shape\nprint('Shape of X train and X validation tensor:', X_train.shape,X_val.shape)\nprint('Shape of label train and validation tensor:', y_train.shape,y_val.shape)","execution_count":10,"outputs":[]},{"metadata":{"_cell_guid":"e5a87e44-2606-45b4-8b22-f14729d2b5ed","colab_type":"text","_uuid":"bb8383980af1ad67d68ae496a2c85f21eed06db2","id":"dPz4NP2nyhM_"},"cell_type":"markdown","source":"# word embedding"},{"metadata":{"_cell_guid":"9b0475c6-51df-4f1b-a9f5-3b1bd2b3332e","colab_type":"text","_uuid":"fbb4ece091062fcfb8d56e94cc0340da01e37dbe","id":"uvkDd18IyhNC"},"cell_type":"markdown","source":"## Use pretrain Word2Vec model from Google https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit\n## Contains 300-dimensional vectors for 3 million words and phrases "},{"metadata":{"collapsed":true,"_cell_guid":"8c248bea-ea6a-4c76-86ca-54f09dfbc72d","_uuid":"6def2e44dae247b64d31f3af4bf1323d4ce4c68b","trusted":true},"cell_type":"code","source":"word_vectors = KeyedVectors.load_word2vec_format('../input/word2vecnegative300/GoogleNews-vectors-negative300.bin', binary=True)","execution_count":11,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"dce4daa6-7f8e-469e-bde8-4a2dec050a5d","_uuid":"d3ed46933cda6ba54cdcd9f8a40e73fb6141025b","trusted":true},"cell_type":"code","source":"EMBEDDING_DIM=300\nvocabulary_size=min(len(word_index)+1,(NUM_WORDS))\n\nembedding_matrix = np.zeros((vocabulary_size, EMBEDDING_DIM))\n\n\nfor word, i in word_index.items():\n    if i>=NUM_WORDS:\n        continue\n    try:\n        embedding_vector = word_vectors[word]\n        embedding_matrix[i] = embedding_vector\n    except KeyError:\n        vec = np.zeros(EMBEDDING_DIM)\n        if word in bad_words:\n            vec = word_vectors['fuck']\n        embedding_matrix[i]=vec","execution_count":12,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"5f996987-08cb-465c-926b-cc9499fd5cac","_uuid":"e1cea3677d0eceef76740302709db8862301c552","trusted":true},"cell_type":"code","source":"del(word_vectors)","execution_count":13,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"fefc67fc-ac22-454e-81e6-d687abddc706","_uuid":"c31683138042d4faad80896b8494436943064260","id":"ZXcgKAZtyhNT","colab":{"autoexec":{"wait_interval":0,"startup":false}},"colab_type":"code","trusted":true},"cell_type":"code","source":"# Dfine Embedding function using the embedding_matrix\nembedding_layer = Embedding(vocabulary_size,\n                            EMBEDDING_DIM,\n                            weights=[embedding_matrix],\n                            trainable=True)","execution_count":14,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"68d66102-a020-4922-b753-b37b07de6d53","_uuid":"172eedbae7fe3d624ff17dbcd29bddd60bba3120","trusted":true},"cell_type":"code","source":"del(embedding_matrix)","execution_count":15,"outputs":[]},{"metadata":{"_cell_guid":"ea1bb063-cab1-468c-bd36-4dd81c2c591c","colab_type":"text","_uuid":"dc59dbf5861e99ce827e7a578bbd8f0a9634012b","id":"8hIDotl3yhNc"},"cell_type":"markdown","source":"# Build network and train it untill validation loss reduces (EarlyStopping)"},{"metadata":{"collapsed":true,"_cell_guid":"949be9f8-1983-410a-944e-f1cc56df66a5","scrolled":true,"id":"4c2jgSmGyhNf","colab":{"autoexec":{"wait_interval":0,"startup":false}},"colab_type":"code","_uuid":"b648d26622154d789479c67724a8e22a37b6b532","trusted":true},"cell_type":"code","source":"sequence_length = X_train.shape[1]\nfilter_sizes = [3,4]\nnum_filters = 100\ndrop = 0.4\n\ninputs = Input(shape=(sequence_length,))\nembedding = embedding_layer(inputs)\nreshape = Reshape((sequence_length,EMBEDDING_DIM,1))(embedding)\n\nconv_0 = Conv2D(num_filters, (filter_sizes[0], EMBEDDING_DIM),activation='relu',kernel_regularizer=regularizers.l2(0.01))(reshape)\nconv_1 = Conv2D(num_filters, (filter_sizes[1], EMBEDDING_DIM),activation='relu',kernel_regularizer=regularizers.l2(0.01))(reshape)\n\nmaxpool_0 = MaxPooling2D((sequence_length - filter_sizes[0] + 1, 1), strides=(1,1))(conv_0)\nmaxpool_1 = MaxPooling2D((sequence_length - filter_sizes[1] + 1, 1), strides=(1,1))(conv_1)\n\nmerged_tensor = concatenate([maxpool_0, maxpool_1], axis=1)\nflatten = Flatten()(merged_tensor)\nreshape = Reshape((2*num_filters,))(flatten)\ndropout = Dropout(drop)(flatten)\nconc = Dense(40)(dropout)\noutput = Dense(units=6, activation='sigmoid',kernel_regularizer=regularizers.l2(0.01))(conc)\n\n# this creates a model that includes\nmodel = Model(inputs, output)","execution_count":16,"outputs":[]},{"metadata":{"outputId":"f4e2826d-b933-4e3d-a98d-9ca5962c5933","_uuid":"dd0374ebc2a71a58b66ecc12f90a0d17d17c7c00","id":"xdAOHA2dyhNn","_cell_guid":"465040ac-0a16-4ca3-8d80-71d121bbc9f3","colab":{"autoexec":{"wait_interval":0,"startup":false}},"colab_type":"code","scrolled":false,"trusted":true},"cell_type":"code","source":"# Compiling Model using optimizer\nopt = Adam(lr=1e-3)\nmodel.compile(loss='binary_crossentropy',optimizer=opt)\n\n# Fitting Model to the data\ncallbacks = [EarlyStopping(monitor='val_loss')]\nhist_adam = model.fit(X_train, y_train, batch_size=400, epochs=20, verbose=2, validation_data=(X_val, y_val),\n         callbacks=callbacks)  # starts training","execution_count":17,"outputs":[]},{"metadata":{"_cell_guid":"04ca7f41-48dd-4615-b825-549769232f15","_uuid":"7dd0a36d71a55f9ec45d2174c1cf987f56e419ae","scrolled":true,"trusted":true},"cell_type":"code","source":"#plotting Loss\nplt.suptitle('Optimizer : Adam', fontsize=10)\nplt.ylabel('Loss', fontsize=16)\nplt.xlabel('Epoch', fontsize=14)\nplt.plot(hist_adam.history['loss'], color='b', label='Training Loss')\nplt.plot(hist_adam.history['val_loss'], color='r', label='Validation Loss')\nplt.legend(loc='upper right')","execution_count":18,"outputs":[]},{"metadata":{"collapsed":true,"outputId":"f4e2826d-b933-4e3d-a98d-9ca5962c5933","_uuid":"dd0374ebc2a71a58b66ecc12f90a0d17d17c7c00","id":"xdAOHA2dyhNn","_cell_guid":"465040ac-0a16-4ca3-8d80-71d121bbc9f3","colab":{"autoexec":{"wait_interval":0,"startup":false}},"colab_type":"code","scrolled":false,"trusted":false},"cell_type":"code","source":"# Compiling Model using optimizer\nopt = SGD(lr=0.01, momentum=0.9, decay=0.0001)\nmodel.compile(loss='binary_crossentropy',optimizer=opt)\n\n# Fitting Model to the data\ncallbacks = [EarlyStopping(monitor='val_loss')]\nhist_sgd = model.fit(X_train, y_train, batch_size=400, epochs=20, verbose=2, validation_data=(X_val, y_val),\n         callbacks=callbacks)  # starts training","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"04ca7f41-48dd-4615-b825-549769232f15","_uuid":"7dd0a36d71a55f9ec45d2174c1cf987f56e419ae","scrolled":true,"trusted":false},"cell_type":"code","source":"#plotting Loss\nplt.suptitle('Optimizer : SGD', fontsize=10)\nplt.ylabel('Loss', fontsize=16)\nplt.xlabel('Epoch', fontsize=14)\nplt.plot(hist_sgd.history['loss'], color='b', label='Training Loss')\nplt.plot(hist_sgd.history['val_loss'], color='r', label='Validation Loss')\nplt.legend(loc='upper right')","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"outputId":"f4e2826d-b933-4e3d-a98d-9ca5962c5933","_uuid":"dd0374ebc2a71a58b66ecc12f90a0d17d17c7c00","id":"xdAOHA2dyhNn","_cell_guid":"465040ac-0a16-4ca3-8d80-71d121bbc9f3","colab":{"autoexec":{"wait_interval":0,"startup":false}},"colab_type":"code","scrolled":false,"trusted":false},"cell_type":"code","source":"# Compiling Model using optimizer\nopt = Adadelta()\nmodel.compile(loss='binary_crossentropy',optimizer=opt)\n# Fitting Model to the data\ncallbacks = [EarlyStopping(monitor='val_loss')]\nhist_adad = model.fit(X_train, y_train, batch_size=400, epochs=20, verbose=2, validation_data=(X_val, y_val),\n         callbacks=callbacks)  # starts training","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"04ca7f41-48dd-4615-b825-549769232f15","_uuid":"7dd0a36d71a55f9ec45d2174c1cf987f56e419ae","scrolled":true,"trusted":false},"cell_type":"code","source":"#plotting Loss\nplt.suptitle('Optimizer : Adadelta', fontsize=10)\nplt.ylabel('Loss', fontsize=16)\nplt.xlabel('Epoch', fontsize=14)\nplt.plot(hist_adad.history['loss'], color='b', label='Training Loss')\nplt.plot(hist_adad.history['val_loss'], color='r', label='Validation Loss')\nplt.legend(loc='upper right')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ad0ebf12-eb02-4473-a049-f5fb1e2f9fec","_uuid":"84042bcdb2bae11171ab14814d830fa8ee05f0e6"},"cell_type":"markdown","source":"# Best Model"},{"metadata":{"collapsed":true,"_cell_guid":"5269ca78-bb76-4bdd-b07e-2f357af66f14","_uuid":"588e5512021a94ecd6f29a1a92719e4823edb1d4","trusted":false},"cell_type":"code","source":"# Compiling Model using optimizer\nopt = Adam(lr=1e-3)\nmodel.compile(loss='binary_crossentropy',optimizer=opt)\n\n# Fitting Model to the data\ncallbacks = [EarlyStopping(monitor='val_loss')]\nmodel.fit(X_train, y_train, batch_size=400, epochs=5, verbose=2, validation_data=(X_val, y_val),\n         callbacks=callbacks)  # starts training","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"78074769-9cd2-4cba-a763-86722a6b837c","colab_type":"text","_uuid":"532a1ec3af6233e091d08b87d525d49a2f3a538f","id":"6DCC0q6fyhN3"},"cell_type":"markdown","source":"# Predict on test data"},{"metadata":{"collapsed":true,"_cell_guid":"1d9710fb-d034-4fe7-b6d6-60a8a06f1715","_uuid":"2c071e7ed7efefa58b88251ed6abf51484ed4a9d","id":"MMxTx2ZyyhN6","colab":{"autoexec":{"wait_interval":0,"startup":false}},"colab_type":"code","trusted":false},"cell_type":"code","source":"# convert test to sequence and padding the sequence\nsequences_test=tokenizer.texts_to_sequences(X_test)\nX_test2 = pad_sequences(sequences_test,maxlen=X_train.shape[1])","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"5bd2f085-45d6-4274-8e16-c5247fe63dc9","_uuid":"46aadb7d1a8c53af7d928d9eeecd3863b93e6c4f","id":"gErAMYukyhOa","colab":{"autoexec":{"wait_interval":0,"startup":false}},"colab_type":"code","trusted":false},"cell_type":"code","source":"# Creating empty prediction array\ncol = ['toxic', 'severe_toxic', 'obscene', 'threat','insult', 'identity_hate']\n\n# Predict on train, val and test datasets\npred_train = model.predict(X_train)\npred_test = model.predict(X_test2)\npred_val = model.predict(X_val)\n\n# Emply array to collect AUC scores\nAUC = np.zeros((3,6))\nAUC","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"2513e888-1522-436a-9785-18c653f7b5a6","_uuid":"4ed66eb12ae4abf8eb1f34611f8f31a71d7504fc","trusted":false},"cell_type":"code","source":"from sklearn import metrics\nfor i,x in enumerate(col):\n    auc = np.array([metrics.roc_auc_score(y_train[:,i], pred_train[:,i]),\n                    metrics.roc_auc_score(y_val[:,i], pred_val[:,i]),\n                    metrics.roc_auc_score(y_test[x], pred_test[:,i])])\n    print(x,\"Train AUC:\",auc[0],\", Val AUC:\",auc[1],\", Test AUC:\",auc[2])\n    AUC[:,i] = auc\n    \navg_auc = AUC.mean(axis=1)\nprint(\"Average Train AUC:\",avg_auc[0],\", Average Val AUC:\",avg_auc[1],\", Average Test AUC:\",avg_auc[2])","execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"name":"CNN_word2vec_.ipynb","default_view":{},"toc_visible":true,"views":{},"version":"0.3.2"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}