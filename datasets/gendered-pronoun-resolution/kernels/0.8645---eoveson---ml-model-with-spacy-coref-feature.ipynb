{"cells":[{"metadata":{"trusted":true,"_uuid":"ccd419f81880f56c85d6b191441e1e2380657a87"},"cell_type":"code","source":"# Tried adding spaCy coref as a feature. I have no expertise with this library.\n# It seems to work in many cases, but for some cases the coref resolves to just \n# he/she/etc rather than a noun. Not sure if it is because the coref model is \n# not confident, I'm navigating the object model incorrectly, or just a limitation\n# of the model.  But I do see some gain.\n\n# forked from: https://www.kaggle.com/shujian/ml-model-example-with-train-test\n# loading spaCy coref extension like: https://www.kaggle.com/ryches/applying-spacy-coreference-but-nothing-goes-right","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b7999faf2e420f7d306014e7cccbe7efc8f61f6f"},"cell_type":"code","source":"# per https://www.kaggle.com/ryches/applying-spacy-coreference-but-nothing-goes-right \n!pip install https://github.com/huggingface/neuralcoref-models/releases/download/en_coref_md-3.0.0/en_coref_md-3.0.0.tar.gz\n!pip install cymem==1.31.2 spacy==2.0.12","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\n#from spacy import displacy\nimport en_coref_md\nfrom spacy.tokens import Doc\nnlp = en_coref_md.load()\n\nimport nltk\nfrom sklearn import *\nfrom sklearn.ensemble import RandomForestClassifier\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\n\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3582a50ab09f03d8f63e2ad9fdb4e88b06084d3f"},"cell_type":"code","source":"test = pd.read_csv('../input/test_stage_1.tsv', delimiter='\\t').rename(columns={'A': 'A_Noun', 'B': 'B_Noun'})\nsub = pd.read_csv('../input/sample_submission_stage_1.csv')\ntest.shape, sub.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bfd7cc6a7b02d4155074c8f9111315a582e46a40"},"cell_type":"code","source":"# True test here:\n#gh_train = pd.read_csv(\"https://raw.githubusercontent.com/google-research-datasets/gap-coreference/master/gap-development.tsv\", delimiter='\\t')\n\ngh_test = pd.read_csv(\"https://raw.githubusercontent.com/google-research-datasets/gap-coreference/master/gap-test.tsv\", delimiter='\\t')\ngh_valid = pd.read_csv(\"https://raw.githubusercontent.com/google-research-datasets/gap-coreference/master/gap-validation.tsv\", delimiter='\\t')\ntrain = pd.concat((gh_test, gh_valid)).rename(columns={'A': 'A_Noun', 'B': 'B_Noun'}).reset_index(drop=True)\ntrain.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"aceac5adecb47fa9c200a033da5bf21e33019453"},"cell_type":"code","source":"def get_coref(row):\n    coref = None\n    \n    nlpr = nlp(row['Text'])\n    \n    # dunno if more direct way to get token from text offset\n    for tok in nlpr.doc:\n        if tok.idx == row['Pronoun-offset']:\n            # model limitation that sometimes there are no coref clusters for the token?\n            # also, sometimes the coref clusters will just be something like:\n            # He: his, him, his\n            # So there is no proper name to map back to?\n            try:\n                if len(tok._.coref_clusters) > 0:\n                    coref = tok._.coref_clusters[0][0].text\n            except:\n                # for some, get the following exception just checking len(tok._.coref_clusters)\n                # *** TypeError: 'NoneType' object is not iterable\n                pass\n            break\n    \n    if coref:\n        coref = coref.lower()\n        # sometimes the coref is I think meant to be the same as A or B, but\n        # it is either a substring or superstring of A or B\n        A_Noun = row['A_Noun'].lower()\n        B_Noun = row['B_Noun'].lower()\n        if coref in A_Noun or A_Noun in coref:\n            coref = A_Noun\n        elif coref in B_Noun or B_Noun in coref:\n            coref = B_Noun\n        \n    return coref","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c8c778e65d1b0a7558f8c06cbb055f3a03bee2ce"},"cell_type":"code","source":"def get_coref_features(df):\n    df['Coref'] = df.apply(get_coref, axis=1)\n    df['Spacy-Coref-A'] = df['Coref'] == df['A_Noun'].str.lower()\n    df['Spacy-Coref-B'] = df['Coref'] == df['B_Noun'].str.lower()\n    return df\ntrain = get_coref_features(train)\ntest = get_coref_features(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"67f6933d5524239e2954db2706a6392f2aa92a78"},"cell_type":"code","source":"def name_replace(s, r1, r2):\n    s = str(s).replace(r1,r2)\n    for r3 in r1.split(' '):\n        s = str(s).replace(r3,r2)\n    return s\n\ndef get_features(df):\n    df['section_min'] = df[['Pronoun-offset', 'A-offset', 'B-offset']].min(axis=1)\n    df['Pronoun-offset2'] = df['Pronoun-offset'] + df['Pronoun'].map(len)\n    df['A-offset2'] = df['A-offset'] + df['A_Noun'].map(len)\n    df['B-offset2'] = df['B-offset'] + df['B_Noun'].map(len)                               \n    df['section_max'] = df[['Pronoun-offset2', 'A-offset2', 'B-offset2']].max(axis=1)\n    #df['Text'] = df.apply(lambda r: r['Text'][: r['Pronoun-offset']] + 'pronountarget' + r['Text'][r['Pronoun-offset'] + len(str(r['Pronoun'])): ], axis=1)\n    df['Text'] = df.apply(lambda r: name_replace(r['Text'], r['A_Noun'], 'subjectone'), axis=1)\n    df['Text'] = df.apply(lambda r: name_replace(r['Text'], r['B_Noun'], 'subjecttwo'), axis=1)\n    \n    \n    df['A-dist'] = (df['Pronoun-offset'] - df['A-offset']).abs()\n    df['B-dist'] = (df['Pronoun-offset'] - df['B-offset']).abs()\n    return(df)\n\ntrain = get_features(train)\ntest = get_features(test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cfa12b1a81855f4a88347cb7f489c071b7f355dc","trusted":true},"cell_type":"code","source":"def get_nlp_features(s, w):\n    doc = nlp(str(s))\n    tokens = pd.DataFrame([[token.text, token.dep_] for token in doc], columns=['text', 'dep'])\n    return len(tokens[((tokens['text']==w) & (tokens['dep']=='poss'))])\n\ntrain['A-poss'] = train['Text'].map(lambda x: get_nlp_features(x, 'subjectone'))\ntrain['B-poss'] = train['Text'].map(lambda x: get_nlp_features(x, 'subjecttwo'))\ntest['A-poss'] = test['Text'].map(lambda x: get_nlp_features(x, 'subjectone'))\ntest['B-poss'] = test['Text'].map(lambda x: get_nlp_features(x, 'subjecttwo'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b113656f16ed8c6b34d4a0f3916e0e0d85277c1b"},"cell_type":"code","source":"train = train.rename(columns={'A-coref':'A', 'B-coref':'B'})\ntrain['A'] = train['A'].astype(int)\ntrain['B'] = train['B'].astype(int)\ntrain['NEITHER'] = 1.0 - (train['A'] + train['B'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c5f42bc5c2ad912aa9b69c538ac3b171d4856be7"},"cell_type":"code","source":"col = ['Pronoun-offset', 'A-offset', 'B-offset', 'section_min', 'Pronoun-offset2', 'A-offset2', 'B-offset2', 'section_max', 'A-poss', 'B-poss', 'A-dist', 'B-dist', 'Spacy-Coref-A', 'Spacy-Coref-B']\nx1, x2, y1, y2 = model_selection.train_test_split(train[col].fillna(-1), train[['A', 'B', 'NEITHER']], test_size=0.2, random_state=1)\nx1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"714def76447082fbf1d595685d54e014033ddc97"},"cell_type":"code","source":"model = multiclass.OneVsRestClassifier(ensemble.RandomForestClassifier(max_depth = 7, n_estimators=1000, random_state=33))\n# model = multiclass.OneVsRestClassifier(ensemble.ExtraTreesClassifier(n_jobs=-1, n_estimators=100, random_state=33))\n\n# param_dist = {'objective': 'binary:logistic', 'max_depth': 1, 'n_estimators':1000, 'num_round':1000, 'eval_metric': 'logloss'}\n# model = multiclass.OneVsRestClassifier(xgb.XGBClassifier(**param_dist))\n\nmodel.fit(x1, y1)\nprint('log_loss', metrics.log_loss(y2, model.predict_proba(x2)))\nmodel.fit(train[col].fillna(-1), train[['A', 'B', 'NEITHER']])\nresults = model.predict_proba(test[col])\ntest['A'] = results[:,0]\ntest['B'] = results[:,1]\ntest['NEITHER'] = results[:,2]\ntest[['ID', 'A', 'B', 'NEITHER']].to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}