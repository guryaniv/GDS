{"cells": [{"outputs": [], "execution_count": null, "metadata": {"_cell_guid": "52308f52-f7d1-47ef-899f-11cc8a4f2256", "_uuid": "c4a005af2a942f6e85f9bb2070ae2790c74b4d70", "collapsed": true}, "source": ["import numpy as np\n", "np.random.seed(666)\n", "import pandas as pd\n", "from sklearn.cross_validation import train_test_split\n", "from subprocess import check_output\n", "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n", "\n", "\n", "#LOAD DATA\n", "train = pd.read_csv(\"../input/train.csv\")\n", "test = pd.read_csv(\"../input/test.csv\")\n", "sample = pd.read_csv(\"../input/sample_submission.csv\")\n", "\n", "#CREATE TARGET VARIABLE\n", "train[\"EAP\"] = (train.author==\"EAP\")*1\n", "train[\"HPL\"] = (train.author==\"HPL\")*1\n", "train[\"MWS\"] = (train.author==\"MWS\")*1\n", "train.drop(\"author\", 1, inplace=True)\n", "target_vars = [\"EAP\", \"HPL\", \"MWS\"]\n", "train.head(2)"], "cell_type": "code"}, {"outputs": [], "execution_count": null, "metadata": {"_cell_guid": "9505edbb-956a-4e25-9291-b63b3ac5fbfa", "_uuid": "b170d6dc306e3e8b8f72ff2d520685befa96d648", "collapsed": true}, "source": ["from nltk.corpus import stopwords\n", "eng_stopwords = set(stopwords.words(\"english\"))\n", "import string\n", "## Number of words in the text ##\n", "train[\"num_words\"] = train[\"text\"].apply(lambda x: len(str(x).split()))\n", "test[\"num_words\"] = test[\"text\"].apply(lambda x: len(str(x).split()))\n", "\n", "## Number of unique words in the text ##\n", "train[\"num_unique_words\"] = train[\"text\"].apply(lambda x: len(set(str(x).split())))\n", "test[\"num_unique_words\"] = test[\"text\"].apply(lambda x: len(set(str(x).split())))\n", "\n", "## Number of characters in the text ##\n", "train[\"num_chars\"] = train[\"text\"].apply(lambda x: len(str(x)))\n", "test[\"num_chars\"] = test[\"text\"].apply(lambda x: len(str(x)))\n", "\n", "## Number of stopwords in the text ##\n", "train[\"num_stopwords\"] = train[\"text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n", "test[\"num_stopwords\"] = test[\"text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n", "\n", "## Number of punctuations in the text ##\n", "train[\"num_punctuations\"] =train['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n", "test[\"num_punctuations\"] =test['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n", "\n", "## Number of title case words in the text ##\n", "train[\"num_words_upper\"] = train[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n", "test[\"num_words_upper\"] = test[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n", "\n", "## Number of title case words in the text ##\n", "train[\"num_words_title\"] = train[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n", "test[\"num_words_title\"] = test[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n", "\n", "## Average length of the words in the text ##\n", "train[\"mean_word_len\"] = train[\"text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n", "test[\"mean_word_len\"] = test[\"text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n", "\n", "num_vars = [\"mean_word_len\", \"num_words_title\", \"num_punctuations\", \"num_chars\"\n", "            , \"num_stopwords\", \"num_chars\", \"num_unique_words\", \"num_words\"]"], "cell_type": "code"}, {"outputs": [], "execution_count": null, "metadata": {"_cell_guid": "c4ed6f7e-187d-4552-8e53-f534ab849c46", "_uuid": "49b0149267c17e41bddbaf489c83c21c14928933", "collapsed": true}, "source": ["#STEMMING WORDS\n", "import nltk.stem as stm\n", "import re\n", "stemmer = stm.SnowballStemmer(\"english\")\n", "train[\"stem_text\"] = train.text.apply(lambda x: (\" \").join([stemmer.stem(z) for z in re.sub(\"[^a-zA-Z0-9]\",\" \", x).split(\" \")]))\n", "test[\"stem_text\"] = test.text.apply(lambda x: (\" \").join([stemmer.stem(z) for z in re.sub(\"[^a-zA-Z0-9]\",\" \", x).split(\" \")]))\n", "\n", "#PROCESS TEXT: RAW\n", "from keras.preprocessing.text import Tokenizer\n", "tok_raw = Tokenizer()\n", "tok_raw.fit_on_texts(train.text.str.lower())\n", "tok_stem = Tokenizer()\n", "tok_stem.fit_on_texts(train.stem_text)\n", "train[\"seq_text_stem\"] = tok_stem.texts_to_sequences(train.stem_text)\n", "test[\"seq_text_stem\"] = tok_stem.texts_to_sequences(test.stem_text)\n", "\n"], "cell_type": "code"}, {"outputs": [], "execution_count": null, "metadata": {"_cell_guid": "a4bca788-12ea-4870-8c9a-1b602db94546", "_uuid": "4930dcfbc90520bb5bc05331b25f8a3ddc27fe73", "collapsed": true}, "source": ["#EXTRACT DATA FOR KERAS MODEL\n", "from keras.preprocessing.sequence import pad_sequences\n", "from sklearn.preprocessing import StandardScaler\n", "from sklearn.feature_extraction.text import TfidfVectorizer\n", "from sklearn.decomposition import TruncatedSVD\n", "from sklearn.pipeline import Pipeline\n", "\n", "def get_keras_data(dataset, maxlen=20, scaler=None, tdfidf=None):\n", "    if scaler==None:\n", "        scaler = StandardScaler()\n", "        scaler.fit(dataset[num_vars])\n", "    if tdfidf==None:\n", "        tdfidf = Pipeline(steps=[('tdfidf', TfidfVectorizer(analyzer='word', binary=False\n", "                                , ngram_range=(1, 4), stop_words=\"english\"))\n", "                            , ('svd', TruncatedSVD(algorithm='randomized', n_components=20, n_iter=10,\n", "                                       random_state=None, tol=0.0)\n", "                                )])\n", "        tdfidf.fit(dataset.text)\n", "    X = {\n", "        \"stem_input\": pad_sequences(dataset.seq_text_stem, maxlen=maxlen),\n", "        \"num_input\": scaler.transform(dataset[num_vars]),\n", "        \"svd_vect\": tdfidf.transform(dataset.text)\n", "    }\n", "    return X, scaler, tdfidf\n", "\n", "\n", "maxlen = 60\n", "dtrain, dvalid = train_test_split(train, random_state=123, train_size=0.9)\n", "print(\"processing train...\")\n", "X_train, scaler, tdfidf = get_keras_data(dtrain, maxlen)\n", "y_train = np.array(dtrain[target_vars])\n", "print(\"processing valid...\")\n", "X_valid, _, _ = get_keras_data(dvalid, maxlen, scaler, tdfidf)\n", "y_valid = np.array(dvalid[target_vars])\n", "print(\"processing test...\")\n", "X_test, _, _ = get_keras_data(test, maxlen, scaler, tdfidf)\n", "\n", "n_stem_seq = np.max( [np.max(X_valid[\"stem_input\"]), np.max(X_train[\"stem_input\"])])+1"], "cell_type": "code"}, {"outputs": [], "execution_count": null, "metadata": {"_cell_guid": "e107482a-53f8-42da-a3ab-6dbaab951756", "_uuid": "1ea4dbac31590e0ccc5d98402c1932f00bf1c872", "collapsed": true}, "source": ["#KERAS MODEL DEFINITION\n", "from keras.layers import Dense, Dropout, Embedding\n", "from keras.layers import Flatten, Input, SpatialDropout1D, Concatenate\n", "from keras.models import Model\n", "from keras.optimizers import Adam \n", "from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping\n", "\n", "def get_callbacks(filepath, patience=2):\n", "    es = EarlyStopping('val_loss', patience=patience, mode=\"min\")\n", "    msave = ModelCheckpoint(filepath, save_best_only=True)\n", "    return [es, msave]\n", "\n", "def get_model():\n", "    embed_dim = 30\n", "    dropout_rate = 0.9\n", "    emb_dropout_rate = 0.9\n", "   \n", "    input_text = Input(shape=[maxlen], name=\"stem_input\")\n", "    \n", "    input_num = Input(shape=[X_train[\"num_input\"].shape[1]], name=\"num_input\")\n", "    \n", "    input_svd = Input(shape=[X_train[\"svd_vect\"].shape[1]], name=\"svd_vect\")\n", "    \n", "    emb_lstm = SpatialDropout1D(emb_dropout_rate) (Embedding(n_stem_seq, embed_dim\n", "                                                ,input_length = maxlen\n", "                                                               ) (input_text))\n", "    concatenate = Concatenate()([(Flatten() (emb_lstm)), input_num, input_svd])\n", "    dense = Dropout(dropout_rate) (Dense(256) (concatenate))\n", "    \n", "    output = Dense(3, activation=\"softmax\")(dense)\n", "\n", "    model = Model([input_text, input_num, input_svd], output)\n", "\n", "    optimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n", "    model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n", "    return model\n", "\n", "model = get_model()\n", "model.summary()"], "cell_type": "code"}, {"outputs": [], "execution_count": null, "metadata": {"_cell_guid": "fa3bca6c-13d4-4e1b-8e48-68c0afe7f5b3", "scrolled": true, "_uuid": "3fc1de3e53843b081df15a4418595b8784cb32e2", "collapsed": true}, "source": ["#TRAIN KERAS MODEL\n", "file_path = \".model_weights.hdf5\"\n", "callbacks = get_callbacks(filepath=file_path, patience=5)\n", "\n", "model = get_model()\n", "model.fit(X_train, y_train, epochs=150\n", "          , validation_data=[X_valid, y_valid]\n", "         , batch_size=512\n", "         , callbacks = callbacks)"], "cell_type": "code"}, {"outputs": [], "execution_count": null, "metadata": {"_cell_guid": "f0d09781-e25a-4a89-a037-e2d0f4d6ced3", "_uuid": "0a36663461f74bb0e1e1df4defbdd7e78b92c9c6", "collapsed": true}, "source": ["#MODEL EVALUATION\n", "from sklearn.metrics import log_loss\n", "\n", "model = get_model()\n", "model.load_weights(file_path)\n", "\n", "preds_train = model.predict(X_train)\n", "preds_valid = model.predict(X_valid)\n", "\n", "print(log_loss(y_train, preds_train))\n", "print(log_loss(y_valid, preds_valid))"], "cell_type": "code"}, {"outputs": [], "execution_count": null, "metadata": {"_cell_guid": "9d823373-aec8-4db7-a9de-8763feca62f2", "_uuid": "4ec1a7c7f6824b6d929d612aa923749ddd7136bd", "collapsed": true}, "source": ["#PREDICTION\n", "preds = pd.DataFrame(model.predict(X_test), columns=target_vars)\n", "submission = pd.concat([test[\"id\"],preds], 1)\n", "submission.to_csv(\"./submission.csv\", index=False)\n", "submission.head()"], "cell_type": "code"}, {"metadata": {"_cell_guid": "8b647350-9f06-4bb4-b5c4-1bd0aa675672", "_uuid": "3099d9906ea396643a9c6cdb5506a9dfe60f11f5", "collapsed": true}, "source": ["Thanks to [SRK](https://www.kaggle.com/sudalairajkumar) for these good features of  his [kernel](https://www.kaggle.com/sudalairajkumar/simple-feature-engg-notebook-spooky-author?scriptVersionId=1667963) !!!"], "cell_type": "markdown"}], "nbformat": 4, "nbformat_minor": 1, "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "name": "python", "file_extension": ".py", "version": "3.6.3", "mimetype": "text/x-python"}}}