{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0"},"cell_type":"markdown","source":"<h1>Who Will Survive the Titanic??</h1>"},{"metadata":{"_uuid":"cb44d610ed5129db88a457a865b1b87f3253ed5a"},"cell_type":"markdown","source":"This notebook builds on the work of Faron, Sina, and Anisotropic"},{"metadata":{"_uuid":"3a944c33e52d20098d3939e316cab4adb281e5a6"},"cell_type":"markdown","source":"### The goal of this notebook is to build a simple 2-layer ensembled model that will outperform my previous works attacking this problem with a neural net and XGBoost without stacking. Let's begin!"},{"metadata":{"trusted":true,"_uuid":"467761de64659e4b6d6ebdc571c6d0165bb9d577"},"cell_type":"code","source":"# Load libraries\nimport pandas as pd\nimport numpy as np\nimport re\nimport sklearn\nimport xgboost as xgb\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# core models for stacking\nfrom sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier,\n                             GradientBoostingClassifier, ExtraTreesClassifier)\nfrom sklearn.svm import SVC\nfrom sklearn.cross_validation import KFold","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b7ebe9cbbf4137b0eb3186ae081bc1d3310a26bb"},"cell_type":"markdown","source":"<h1>Data Exploration, Feature Engineering & Cleaning</h1>"},{"metadata":{"trusted":true,"_uuid":"d04f7786eafe25fe4423d462277c640bcb40a539"},"cell_type":"code","source":"train = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\n\nPassengerId = test['PassengerId']\n\ntrain.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"69e641c8071fa963ae8f02831fc1bb296287ae49"},"cell_type":"code","source":"# Any null values to worry about? \ntrain.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"af781c97dc8e061c198db29a79745b55d2bf3caf","collapsed":true},"cell_type":"code","source":"full_data = [train, test]\n\n# Encode Cabin column to track is passenger has cabin or not\nfor dataset in full_data:\n    dataset['Has_Cabin'] = dataset['Cabin'].apply(lambda x: 0 if type(x) == float else 1)\n    \n    # combine SibSp and Parch to gather FamilySize\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n    \n    # if FamilySize = 0 mark that passenger as alone\n    dataset['IsAlone'] = 0\n    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\n    \n    # Remove null values from Embarked\n    dataset['Embarked'] = dataset['Embarked'].fillna('S')\n    \n# Bucket fare into categories, assign at CategoricalFare \ntrain['CategoricalFare'] = pd.qcut(train['Fare'], 4, labels=[1,2,3,4])\n\nfor dataset in full_data:\n    age_avg = dataset['Age'].mean()\n    age_std = dataset['Age'].std()\n    age_null_count = dataset['Age'].isnull().sum()\n    # fill null age values with range of probable ages (+- 1 std) \n    age_null_random_list = np.random.randint(age_avg - age_std,\n                                            age_avg + age_std, \n                                             size=age_null_count)\n    dataset['Age'][np.isnan(dataset['Age'])] = age_null_random_list\n    dataset['Age'] = dataset['Age'].astype(int)\ntrain['CategoricalAge'] = pd.cut(train['Age'], 5)\n\n# Pull titles from passenger names and make feature\ndef get_title(name):\n    title_search = re.search(r'([A-Za-z]+)\\.', name)\n    if title_search:\n        return title_search.group(1)\n    return ''\n\nfor dataset in full_data:\n    dataset['Title'] = dataset['Name'].apply(get_title)\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess', 'Capt', 'Col',\n                                                'Don', 'Dr', 'Major', 'Master', 'Rev', 'Sir', 'Jonkheer', 'Dona'],\n                                               'Rare')\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f7d868e30412b6c599833901fdc5bfc48b6586bc","collapsed":true},"cell_type":"code","source":"# Map features\nfor dataset in full_data:\n    dataset['Sex'] = dataset['Sex'].map({'female': 0, 'male': 1}) #.astype(int)\n    dataset['Title'] = dataset['Title'].map({'Mr': 1, 'Miss': 2, 'Mrs': 3, 'Rare': 4})\n    dataset['Title'] = dataset['Title'].fillna(0)\n    dataset['Embarked'] = dataset['Embarked'].map({'S': 0, 'C': 1, 'Q': 2}) #.astype(int)\n    dataset.loc[dataset['Fare'] <= 7.91, 'Fare'] = 0\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare'] = 2\n    dataset.loc[dataset['Fare'] > 31, 'Fare'] = 3\n    dataset['Fare'] = dataset['Fare'] #astype(int)\n    dataset.loc[dataset['Age'] <=16, 'Age'] = 0\n    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n    dataset.loc[dataset['Age'] > 64, 'Age'] = 4","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2a4b57c9fcacd0747803cd028629192627752a34"},"cell_type":"code","source":"drop_elements = ['PassengerId', 'Name', 'Ticket', 'Cabin', 'SibSp']\ntrain = train.drop(drop_elements, axis =1)\ntrain = train.drop(['CategoricalAge', 'CategoricalFare'], axis = 1)\ntest = test.drop(drop_elements, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e429b9a577c568d4b2c392e3f66d0a371d5af1cf"},"cell_type":"code","source":"train.dtypes.sample(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4cd3c96b84aa7107ac04e68bad7ad5c12c724bee"},"cell_type":"code","source":"test.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"a01b21db21d73bfd9a7287403776df524491c217"},"cell_type":"code","source":"test['Fare'] = test['Fare'].fillna(test['Fare'].median())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c24b19fec7678485cfc32730248351c8d36365ae"},"cell_type":"code","source":"test.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fff78da9de0f3c47c2cf042021f782142750cc14"},"cell_type":"markdown","source":"<h1>Visualizations</h1>"},{"metadata":{"trusted":true,"_uuid":"0b3c5b7ed0521cacb5bdfbe82de331428a22e667"},"cell_type":"code","source":"# Let's make a heatmap to find correlations in features\ncolormap = plt.cm.RdBu\nplt.figure(figsize=(14,12))\nplt.title('Pearson Correlation of Features', y=1.05, size=15)\nsns.heatmap(train.astype(float).corr(), linewidths=0.1,vmax=1.0,\n           square=True, cmap=colormap, linecolor='white', annot=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"42e792004633409828223f1fc633fe9793ea2a5c"},"cell_type":"code","source":"# more correlation mining with pairplots\ng = sns.pairplot(train[[u'Survived', u'Pclass', u'Sex',\n                       u'Parch', u'Fare', u'Embarked', \n                       u'FamilySize', u'Title']], hue='Survived',\n                palette = 'seismic', size=1.2, diag_kind = 'kde', diag_kws=dict(shade=True), plot_kws=dict(s=10))\ng.set(xtickLabels=[])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2566626dfe963ada7d0fec082cb7337b2f6abda1"},"cell_type":"code","source":"loner_plot = sns.jointplot('IsAlone', 'Survived', data=train, kind='reg')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bfe2ca3fca994e92d72d29ee3978e5655207d312"},"cell_type":"code","source":"g = sns.violinplot(x= 'Survived', y='FamilySize', data=train, hue='Survived')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b0ccd3821a532760f33ef9277791e36a6bfa4cad"},"cell_type":"code","source":"g = sns.violinplot(x= 'Survived', y='IsAlone', data=train, hue='Survived')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"540a6aa921b3690735b5cb1b4e13be92b871c876"},"cell_type":"markdown","source":"<h1>Ensembling & Stacking</h1>"},{"metadata":{"trusted":true,"_uuid":"e73f28b7478e01aaefac7c795baa1add89b5a7e6","collapsed":true},"cell_type":"code","source":"# build helper class for deploying sklearn classifier\n# will help with ensembling multiple classifiers\n\n# global parameters\nntrain = train.shape[0]\nntest = test.shape[0]\nSEED = 0 # for reproducing data when using random\nNFOLDS = 5 # number of classifiers we're ensembling\nkf = KFold(ntrain, n_folds= NFOLDS, random_state=SEED)\n\nclass SklearnHelper(object):\n    def __init__(self, clf, seed=0, params=None):\n        params['random_state'] = seed\n        self.clf = clf(**params)\n        \n    def train(self, x_train, y_train):\n        self.clf.fit(x_train, y_train)\n        \n    def predict(self, x):\n        return self.clf.predict(x)\n    \n    def fit(self,x,y):\n        return self.clf.fit(x,y)\n    \n    def feature_importances(self,x,y):\n        return(self.clf.fit(x,y).feature_importances_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"0d3f1bb6de9ce840846b37921a5dd0ce3431708d"},"cell_type":"code","source":"def get_oof(clf, x_train, y_train, x_test):\n    oof_train = np.zeros((ntrain,))\n    oof_test = np.zeros((ntest,))\n    oof_test_skf = np.empty((NFOLDS, ntest))\n    \n    for i, (train_index, test_index) in enumerate(kf):\n        x_tr = x_train[train_index]\n        y_tr = y_train[train_index]\n        x_te = x_train[test_index]\n        \n        clf.train(x_tr, y_tr)\n        \n        oof_train[test_index] = clf.predict(x_te)\n        oof_test_skf[i, :] = clf.predict(x_test)\n        \n    oof_test[:] = oof_test_skf.mean(axis=0)\n    return oof_train.reshape(-1,1), oof_test.reshape(-1,1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f06f06939d6f5774621015df40be1f2d8af6634c","collapsed":true},"cell_type":"code","source":"# parameters for classifiers\n# Random Forrest\nrf_params = {\n    'n_jobs': -1,\n    'n_estimators': 500,\n    'warm_start': True,\n    'max_depth': 6,\n    'min_samples_leaf': 2, \n    'max_features': 'sqrt',\n    'verbose': 0\n}\n\n# Extra Trees\net_params = {\n    'n_jobs': -1,\n    'n_estimators': 500,\n    'max_depth': 8, \n    'min_samples_leaf': 2, \n    'verbose': 0\n}\n\n# AdaBoost \nada_params = {\n    'n_estimators': 500,\n    'learning_rate': 0.75\n}\n\n# Gradient Boosting\ngb_params = {\n    'n_estimators': 500,\n    'max_depth': 5, \n    'min_samples_leaf': 2, \n    'verbose': 0\n}\n\n# Support Vector Classifier\nsvc_params = {\n    'kernel':'linear',\n    'C': 0.025\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"05cdafdf7a7d82b6eba32a82a5cb9a8784ecb341","collapsed":true},"cell_type":"code","source":"# 5 objects representing our models\nrf = SklearnHelper(clf=RandomForestClassifier, seed=SEED, params=rf_params)\net = SklearnHelper(clf=ExtraTreesClassifier, seed=SEED, params=et_params)\nada = SklearnHelper(clf=AdaBoostClassifier, seed=SEED, params=ada_params)\ngb = SklearnHelper(clf=GradientBoostingClassifier, seed=SEED, params=gb_params)\nsvc = SklearnHelper(clf=SVC, seed=SEED, params=svc_params)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f9454df2f5323feaaeb826146d00f083663b30f6"},"cell_type":"code","source":"y_train = train['Survived']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0b8bbaf4d48198aff62e5116edd916bd4de75cb5"},"cell_type":"code","source":"# numpy array of dataset to feed our models\ntrain = train.drop(['Survived'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9356e00b4adf4fc08f008fa59148879580f82338"},"cell_type":"code","source":"x_train = train.values\nx_test = test.values\nprint(len(train.values[0]))\nprint(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"349b133fcb6f32cd73c7097ee170ad85691084d6"},"cell_type":"code","source":"# generate base results to use as new features\nrf_oof_train, rf_oof_test = get_oof(rf, x_train, y_train, x_test)\net_oof_train, et_oof_test = get_oof(et, x_train, y_train, x_test)\nada_oof_train, ada_oof_test = get_oof(ada, x_train, y_train, x_test)\ngb_oof_train, gb_oof_test = get_oof(gb, x_train, y_train, x_test)\nsvc_oof_train, svc_oof_test = get_oof(svc, x_train, y_train, x_test)\nprint('Training complete')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8eb6126116b1059702442b51eb41d05e7bc4c93c"},"cell_type":"code","source":"# quantify importance of features in the models\nrf_features = rf.feature_importances(x_train, y_train)\net_features = et.feature_importances(x_train, y_train)\nada_features = ada.feature_importances(x_train, y_train)\ngb_features = gb.feature_importances(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4a34a4cca06d11620ab5fb0b44908def25b6d56c"},"cell_type":"code","source":"# test to make sure array worked\nrf_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f66d0330cc8200feba806609e09ce178b1487509"},"cell_type":"code","source":"cols = train.columns.values\nfeature_dataframe = pd.DataFrame( {\n    'features': cols,\n    'Random Forest feature importances': rf_features,\n    'Extra Trees feature importances': et_features,\n    'AdaBoost feature importances': ada_features,\n    'Gradient Boost feature importances': gb_features\n})\nfeature_dataframe.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e1b76758f899a30b0bbba70b87e418b8682a4019"},"cell_type":"code","source":"# now we'll visualize our feature importance dataframe\n# to gather intuition about what features matter\n\n# Random Forest scatter plot \ntrace = go.Scatter(\n    x = feature_dataframe['features'].values,\n    y = feature_dataframe['Random Forest feature importances'].values,\n    mode='markers',\n    marker=dict(\n        sizemode = 'diameter',\n        sizeref = 1,\n        size = 25,\n        color = feature_dataframe['Random Forest feature importances'].values,\n        colorscale='Portland',\n        showscale=True),\n    text = feature_dataframe['features'].values\n)\ndata = [trace]\n\nlayout = go.Layout(\n    autosize=True,\n    title='Random Forest Feature Importance',\n    hovermode='closest',\n    yaxis=dict(\n        title='Feature Importance',\n        ticklen=5,\n        gridwidth=2),\n    showlegend=False\n)\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig,filename='scatter2010')\n\n# Extra Trees scatter plot\ntrace = go.Scatter(\n    x = feature_dataframe['features'].values,\n    y = feature_dataframe['Extra Trees feature importances'].values,\n    mode = 'markers',\n    marker = dict(\n        sizemode = 'diameter',\n        sizeref = 1, \n        size = 25, \n        color = feature_dataframe['Extra Trees feature importances'].values,\n        colorscale = 'Portland',\n        showscale = True),\n    text = feature_dataframe['features'].values\n)\ndata = [trace]\n\nlayout = go.Layout(\n    autosize = True,\n    title = 'Extra Trees Feature Importance',\n    hovermode = 'closest',\n    yaxis = dict(\n        title = 'Feature Importance',\n        ticklen = 5, \n        gridwidth = 2),\n    showlegend = False\n)\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig,filename='scatter2010')\n\n# AdaBoost scatter plot\ntrace = go.Scatter(\n    x = feature_dataframe['features'].values,\n    y = feature_dataframe['AdaBoost feature importances'].values,\n    mode = 'markers',\n    marker = dict(\n        sizemode = 'diameter',\n        sizeref = 1, \n        size = 25,\n        color = feature_dataframe['AdaBoost feature importances'].values,\n        colorscale = 'Portland',\n        showscale = True),\n    text = feature_dataframe['features'].values\n)\ndata = [trace]\n\nlayout = go.Layout(\n    autosize = True,\n    title = 'AdaBoost Feature Importance',\n    hovermode = 'closest',\n    yaxis = dict(\n        title = 'Feature Importance',\n        ticklen = 5, \n        gridwidth = 2),\n)\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig,filename='scatter20101')\n\n# Gradient Boost scatter plot\ntrace = go.Scatter(\n    x = feature_dataframe['features'].values,\n    y = feature_dataframe['Gradient Boost feature importances'].values,\n    mode = 'markers',\n    marker = dict(\n        sizemode = 'diameter',\n        sizeref = 1,\n        size = 25,\n        color = feature_dataframe['Gradient Boost feature importances'].values,\n        colorscale= 'Portland',\n        showscale = True),\n    text = feature_dataframe['features'].values\n)\ndata = [trace]\n\nlayout = go.Layout(\n    autosize = True,\n    title = 'Gradient Boosting Feature Importance',\n    hovermode = 'closest',\n    yaxis = dict(\n        title = 'Feature Importance',\n        ticklen = 5, \n        gridwidth = 2),\n    showlegend = False\n)\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename='scatter2010')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7efd6f54ad4614ec33b49bb0dabbe46e0aab5c88"},"cell_type":"code","source":"# create new column storing the average of values\n\nfeature_dataframe['mean'] = feature_dataframe.mean(axis=1)\nfeature_dataframe.sample(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0a90bd98a05f728767b6b59c70274516e96eecbd"},"cell_type":"code","source":"# plot the average values\nx = feature_dataframe['features'].values\ny = feature_dataframe['mean'].values\ndata = [go.Bar(\n            x = x,\n            y = y,\n            width = 0.5,\n            marker = dict(\n                color = feature_dataframe['mean'].values,\n            colorscale = 'Portland',\n            showscale = True,\n            reversescale = False),\n        opacity = 0.6\n)]\n\nlayout = go.Layout(\n    autosize = True,\n    title = 'Barplots of Mean Feature Importance',\n    hovermode = 'closest',\n    yaxis = dict(\n        title = 'Feature Importance',\n        ticklen = 5,\n        gridwidth = 2),\n    showlegend = False\n)\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename='bar-direct-labels')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"790d8be17a5c1a00db9ea7b945104df3da62360c"},"cell_type":"markdown","source":"<h1>Second-Level Predictions</h1>"},{"metadata":{"trusted":true,"_uuid":"acffbf87098617955ddaf5f676678295c9bb24a6"},"cell_type":"code","source":"# Now we'll be building a new classifier that takes in our initial predictions as a pre-train model\nbase_predictions_train = pd.DataFrame( {'RandomForest': rf_oof_train.ravel(),\n                                       'ExtraTrees': et_oof_train.ravel(),\n                                       'AdaBoost': ada_oof_train.ravel(),\n                                       'GradientBoost': gb_oof_train.ravel()})\nbase_predictions_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"0138bbe9f565b42aecdff97193651ce1c7fad754"},"cell_type":"code","source":"# correlation heatmap of second-level training\ndata = [\n    go.Heatmap(\n        x = base_predictions_train.columns.values,\n        y = base_predictions_train.columns.values,\n            colorscale = 'Viridis',\n            showscale = True,\n            reversescale = True,\n        z = base_predictions_train.astype(float).corr().values\n    )\n]\npy.iplot(data, filename='labeled-headmap')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"532e6c7e8a83280f8a8a2437000764eb5c2b3c90"},"cell_type":"code","source":"x_train = np.concatenate((et_oof_train, rf_oof_train, \n                          ada_oof_train, gb_oof_train,\n                         svc_oof_train), axis=1)\nx_test = np.concatenate(( et_oof_test, rf_oof_test,\n                        ada_oof_test, gb_oof_test,\n                        svc_oof_test), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8415a7636b1e845104e182f4125ee65897488576"},"cell_type":"code","source":"# build and fit to XGBoost\nparams = ()\ngbm = xgb.XGBClassifier(n_estimators = 2000,\n                        max_depth = 4,\n                        min_child_weight = 2,\n                        gamma = 0.9, \n                        subsample = 0.8, \n                        colsample_bytree = 0.8,\n                        objective = 'binary:logistic', \n                        nthread = -1,\n                        scale_pos_weight = 1).fit(x_train, y_train)\npredictions = gbm.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fd42aa4c8f5ca63587fc811564694766d44cf970"},"cell_type":"code","source":"StackingSubmission = pd.DataFrame({'PassengerId': PassengerId,\n                                  'Survived': predictions})\nStackingSubmission.to_csv('StackingSubmission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"afaa61481f3ad5948ff06d957250d7536d9a4aaa"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}