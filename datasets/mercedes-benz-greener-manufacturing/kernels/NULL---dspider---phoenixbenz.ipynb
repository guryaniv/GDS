{"nbformat": 4, "nbformat_minor": 0, "cells": [{"execution_count": null, "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder, minmax_scale\n\nimport matplotlib.pyplot as plt\n\nfrom sklearn.metrics import mean_squared_error, r2_score\n\nfrom sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\nfrom sklearn.model_selection import train_test_split, GridSearchCV, KFold, cross_val_score\nfrom sklearn.linear_model import LassoLarsCV, SGDRegressor\n\nfrom sklearn.svm import SVR, LinearSVC\n\nfrom sklearn.decomposition import PCA, FastICA\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.random_projection import GaussianRandomProjection\nfrom sklearn.random_projection import SparseRandomProjection\n\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\n\nimport xgboost as xgb\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.", "metadata": {"_execution_state": "idle", "_cell_guid": "aa64880f-8e35-4e00-a7dc-df0a828a7767", "trusted": false, "_uuid": "ea8d6ac98b4194c132ca0658ce4342b2ceb8f13b"}, "cell_type": "code", "outputs": []}, {"execution_count": null, "source": "train_df = pd.read_csv(\"../input/train.csv\")\ntest_df = pd.read_csv(\"../input/test.csv\")\n\ntrain_y = train_df['y']\ntrain_id = train_df['ID']\ntrain_df = train_df.drop(\"y\", 1)\ntrain_df = train_df.drop(\"ID\", 1)\n\ntest_id = test_df['ID']\ntest_df = test_df.drop(\"ID\", 1)\n\nnum_train = len(train_df)\n\ndf_all = pd.concat([train_df, test_df])\ndf_all = pd.get_dummies(df_all, drop_first=True)\n\ntrain_df = df_all[:num_train]\ntest_df = df_all[num_train:]\n\n#############################\n\nn_comp = 12\n\n# tSVD\ntsvd = TruncatedSVD(n_components=n_comp, random_state=420)\ntsvd_results_train = tsvd.fit_transform(train_df)\ntsvd_results_test = tsvd.transform(test_df)\n\n# PCA\npca = PCA(n_components=n_comp, random_state=420)\npca2_results_train = pca.fit_transform(train_df)\npca2_results_test = pca.transform(test_df)\n\n# ICA\nica = FastICA(n_components=n_comp, random_state=420)\nica2_results_train = ica.fit_transform(train_df)\nica2_results_test = ica.transform(test_df)\n\n# GRP\ngrp = GaussianRandomProjection(n_components=n_comp, eps=0.1, random_state=420)\ngrp_results_train = grp.fit_transform(train_df)\ngrp_results_test = grp.transform(test_df)\n\n# SRP\nsrp = SparseRandomProjection(n_components=n_comp, dense_output=True, random_state=420)\nsrp_results_train = srp.fit_transform(train_df)\nsrp_results_test = srp.transform(test_df)\n\n# Append decomposition components to datasets\nfor i in range(1, n_comp+1):\n    train_df['pca_' + str(i)] = pca2_results_train[:,i-1]\n    test_df['pca_' + str(i)] = pca2_results_test[:, i-1]\n    \n    train_df['ica_' + str(i)] = ica2_results_train[:,i-1]\n    test_df['ica_' + str(i)] = ica2_results_test[:, i-1]\n\n    train_df['tsvd_' + str(i)] = tsvd_results_train[:,i-1]\n    test_df['tsvd_' + str(i)] = tsvd_results_test[:, i-1]\n    \n    train_df['grp_' + str(i)] = grp_results_train[:,i-1]\n    test_df['grp_' + str(i)] = grp_results_test[:, i-1]\n    \n    train_df['srp_' + str(i)] = srp_results_train[:,i-1]\n    test_df['srp_' + str(i)] = srp_results_test[:, i-1]\n\nX_dtrain, X_test, y_dtrain, y_test = train_test_split(train_df, train_y, random_state=7, test_size=0.3)", "metadata": {"_uuid": "656eb37d1365551eff6402c59ddcbf4289e4a18c", "collapsed": false, "_cell_guid": "f2df1ba0-ad18-4984-80d6-db4af31d3652", "trusted": false, "_execution_state": "idle"}, "cell_type": "code", "outputs": []}, {"execution_count": null, "source": "model_rfr = RandomForestRegressor(n_estimators=600, max_depth=3, min_samples_split=4, min_samples_leaf=60)\n\n# Let's see the feature importance for this model\nimportances = model_rfr.fit(train_df, train_y).feature_importances_\nfeatures = pd.DataFrame()\nfeatures['feature'] = train_df.columns\nfeatures['importance'] = importances\n\ntodrop = features.loc[features['importance'] == 0].feature\nnew_train_df = train_df.drop(todrop, 1)", "metadata": {"_uuid": "6867a13e3abae59d7def50272bc36b5060a60554", "collapsed": false, "_cell_guid": "f9e42b01-9339-44a1-bba0-2340dfcfd8c1", "trusted": false, "_execution_state": "idle"}, "cell_type": "code", "outputs": []}, {"execution_count": null, "source": "model_ls = KNeighborsRegressor(n_neighbors=15, weights='distance', algorithm='auto', p=1)\n\n\nparam_test1 = {\n \"p\": (1, 2)\n}\n\ngsearch1 = GridSearchCV(estimator = model_ls, \n param_grid = param_test1, scoring='r2',iid=False, cv=5)", "metadata": {"_uuid": "f05ac56139864300565b2b6e4fc3d28546f56171", "collapsed": false, "_cell_guid": "defbe0e6-474c-49bb-98b5-8fe1ff18921a", "trusted": false, "_execution_state": "idle"}, "cell_type": "code", "outputs": []}, {"execution_count": null, "source": "\ngsearch1.fit(new_train_df,train_y)\ngsearch1.grid_scores_, gsearch1.best_params_, gsearch1.best_score_", "metadata": {"_uuid": "759bd575ed4f02d01200b84b60db0c8de43bb616", "collapsed": false, "_cell_guid": "3f2e2cfd-e39e-47e9-be24-77851b2af012", "trusted": false, "_execution_state": "idle"}, "cell_type": "code", "outputs": []}], "metadata": {"language_info": {"file_extension": ".py", "mimetype": "text/x-python", "nbconvert_exporter": "python", "version": "3.6.1", "pygments_lexer": "ipython3", "name": "python", "codemirror_mode": {"name": "ipython", "version": 3}}, "kernelspec": {"language": "python", "display_name": "Python 3", "name": "python3"}}}