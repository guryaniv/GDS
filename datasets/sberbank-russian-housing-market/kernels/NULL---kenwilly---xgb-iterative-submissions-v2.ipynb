{"nbformat": 4, "nbformat_minor": 0, "metadata": {"language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "name": "python", "pygments_lexer": "ipython3", "file_extension": ".py", "version": "3.6.1", "codemirror_mode": {"name": "ipython", "version": 3}}, "kernelspec": {"language": "python", "display_name": "Python 3", "name": "python3"}}, "cells": [{"source": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport xgboost as xgb\nfrom sklearn import model_selection, preprocessing\n\ntry:\n    df_train = pd.read_csv(\"../input/train.csv\", parse_dates=['timestamp'])\n    df_test = pd.read_csv(\"../input/test.csv\", parse_dates=['timestamp'])\n    df_macro = pd.read_csv(\"../input/macro.csv\", parse_dates=['timestamp'])\n    print (\"Training Data Loaded with {} samples and {} features\".format(*df_train.shape)) \n    print (\"Testing Data Loaded with {} samples and {} features\".format(*df_test.shape)) \n    print (\"Macro Data Loaded with {} samples and {} features\".format(*df_macro.shape))\nexcept:\n    print (\"Oh snap!\")", "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"_execution_state": "idle", "trusted": false, "_cell_guid": "3bd438d9-e513-48d9-ab20-a8ea90201657", "_uuid": "17ed3d760e85f27039a905b731971b840be23700"}}, {"source": "# Round 1) Running raw data through XGB to generate baseline score\n### Score: 0.32305", "execution_count": null, "outputs": [], "cell_type": "markdown", "metadata": {"collapsed": false, "_execution_state": "idle", "_cell_guid": "20fe4cc4-c163-495b-9637-3161126fa459", "_uuid": "ec7a5943099e1f8a7d0fecceaf3252596366cf74"}}, {"source": "# Deal with categorical features\nfor f in df_train.columns:\n    if df_train[f].dtype=='object':\n        lbl = preprocessing.LabelEncoder()\n        lbl.fit(list(df_train[f].values)) \n        df_train[f] = lbl.transform(list(df_train[f].values))       \n        \n# Set data as DMatrix\ntrain_y = df_train.price_doc.values\ntrain_X = df_train.drop([\"id\", \"timestamp\", \"price_doc\"], axis=1)\n\nXGB_Train = xgb.DMatrix(train_X,label=train_y,feature_names=train_X.columns.values)\n\n# Set parameters\nxgb_params = {\n    'eta': 0.05,\n    'max_depth': 5,\n    'subsample': 1.0,\n    'colsample_bytree': 0.7,\n    'objective': 'reg:linear',\n    'eval_metric': 'rmse',\n    'silent': 1\n}\n\n# Create model\nmodel = xgb.train(xgb_params, XGB_Train, num_boost_round=100)\n\n# plot the important features #\nfig, ax = plt.subplots(figsize=(12,18))\nxgb.plot_importance(model, max_num_features=50, height=0.8, ax=ax)\nplt.show()", "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"_execution_state": "idle", "trusted": false, "_cell_guid": "493814f2-556d-4076-b1a6-adab035809d1", "_uuid": "d384047119ff537329f3e34d692193db962aa70c"}}, {"source": "# Deal with categorical features in test data\nfor f in df_test.columns:\n    if df_test[f].dtype=='object':\n        lbl = preprocessing.LabelEncoder()\n        lbl.fit(list(df_test[f].values)) \n        df_test[f] = lbl.transform(list(df_test[f].values))  \n\n# Run raw test data through model to generate baseline predictions\ntest = df_test.drop([\"id\", \"timestamp\"], axis=1)\nXGB_Test = xgb.DMatrix(test)\n\n# Generate baseline predictions\nypred = model.predict(XGB_Test)\n\n# Merge predictions with ID's to create submission\nidCol = df_test[\"id\"].values\nif len(ypred) == len(idCol):\n    sub = np.column_stack((idCol,ypred))\n    df_sub = pd.DataFrame(data=sub,columns=[\"id\",\"price_doc\"])\n    df_sub = df_sub.astype(int)\n    print(\"Final output:\")\n    print(df_sub.head())\nelse:\n    print(\"Oh snap!\")\n\n# Generating first submission and submitting for baseline score\ndf_sub.to_csv('submission1.csv', index=False)\n\n# Score = 0.32305", "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"collapsed": false, "_execution_state": "idle", "trusted": false, "_cell_guid": "b73b4315-aad6-4785-bc6e-5cf48e261365", "_uuid": "9999f958c800d333970b42c8854ed4fa28f35eed"}}, {"source": "# Round 2) Joining macro data and doing some basic engineering\n### Score: 0.34993", "execution_count": null, "outputs": [], "cell_type": "markdown", "metadata": {"collapsed": false, "_execution_state": "idle", "_cell_guid": "0c1162a4-b4df-4467-913c-54ec117c81ee", "_uuid": "3c4fb10ad4b9ba37949575aae4b5603a4f45f1d0"}}, {"source": "# Deal with categorical features in macro data\nfor f in df_macro.columns:\n    if df_macro[f].dtype=='object':\n        lbl = preprocessing.LabelEncoder()\n        lbl.fit(list(df_macro[f].values)) \n        df_macro[f] = lbl.transform(list(df_macro[f].values)) \n\n# Joining macro data and combining test/train into single dataframe\nnum_train = len(df_train)\ntrain_labels = df_train[['id','price_doc']]\ndf_trainingFeatures = df_train.drop(['price_doc'], axis=1)\ndf_all = pd.concat([df_trainingFeatures, df_test])\n\ndf_all = pd.merge_ordered(df_all, df_macro, on='timestamp', how='left')", "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"collapsed": false, "_execution_state": "idle", "trusted": false, "_cell_guid": "f157c756-a48e-49da-ab6c-c1c7241b3ebc", "_uuid": "26e96de0f65d1999cdc16ac87ae864b5c93a34aa"}}, {"source": "# Deal with categorical features in macro data\nfor f in df_macro.columns:\n    if df_macro[f].dtype=='object':\n        lbl = preprocessing.LabelEncoder()\n        lbl.fit(list(df_macro[f].values)) \n        df_macro[f] = lbl.transform(list(df_macro[f].values)) \n\n# Joining macro data and combining test/train into single dataframe\nnum_train = len(df_train)\ntrain_labels = df_train[['id','price_doc']]\ndf_trainingFeatures = df_train.drop(['price_doc'], axis=1)\ndf_all = pd.concat([df_trainingFeatures, df_test])\n\ndf_all = pd.merge_ordered(df_all, df_macro, on='timestamp', how='left')\n\n# ------\n# Feature engineering the timestamp a bit\n\n# Add month-year\nmonth_year = (df_all.timestamp.dt.month + df_all.timestamp.dt.year * 100)\nmonth_year_cnt_map = month_year.value_counts().to_dict()\ndf_all['month_year_cnt'] = month_year.map(month_year_cnt_map)\n\n# Add week-year count\nweek_year = (df_all.timestamp.dt.weekofyear + df_all.timestamp.dt.year * 100)\nweek_year_cnt_map = week_year.value_counts().to_dict()\ndf_all['week_year_cnt'] = week_year.map(week_year_cnt_map)\n\n# Add month and day-of-week\ndf_all['month'] = df_all.timestamp.dt.month\ndf_all['dow'] = df_all.timestamp.dt.dayofweek\n\n# Splitting back in to test & training sets\ndf_train = df_all[:num_train]\ndf_test = df_all[num_train:]\n\n# Adding price_doc back to training set to remove outliers\ndf_train = pd.merge_ordered(df_train, train_labels, on='id', how='left')\n\nstartingRows = df_train.shape[0]\nstartingColumns = df_train.shape[1]\n\n# removing outlier rows from training set\ntop = df_train[\"price_doc\"].quantile(0.97)\nbottom = df_train[\"price_doc\"].quantile(0.03)\ndf_train = df_train[df_train[\"price_doc\"] < top]\ndf_train = df_train[df_train[\"price_doc\"] > bottom]\n\n# ------\n# Creating new model\n\n# Set data as DMatrix\ntrain_y = df_train.price_doc.values\ntrain_X = df_train.drop([\"id\", \"timestamp\", \"price_doc\"], axis=1)\n\nXGB_Train = xgb.DMatrix(train_X,label=train_y,feature_names=train_X.columns.values)\n\n# Set parameters\nxgb_params = {\n    'eta': 0.05,\n    'max_depth': 5,\n    'subsample': 1.0,\n    'colsample_bytree': 0.7,\n    'objective': 'reg:linear',\n    'eval_metric': 'rmse',\n    'silent': 1\n}\n\n# Create model\nmodel = xgb.train(xgb_params, XGB_Train, num_boost_round=100)\n\n# plot the important features #\nfig, ax = plt.subplots(figsize=(12,18))\nxgb.plot_importance(model, max_num_features=50, height=0.8, ax=ax)\nplt.show()\n", "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"collapsed": false, "_execution_state": "idle", "trusted": false, "_cell_guid": "8626877a-90a4-404a-b848-b093d9931e8a", "_uuid": "f66ed2658353fde737657fdd43a1d5c7f795f87f"}}, {"source": "# Run test data through model to generate a second round of predictions\ntest = df_test.drop([\"id\", \"timestamp\"], axis=1)\nXGB_Test = xgb.DMatrix(test)\n\n# Generate baseline predictions\nypred = model.predict(XGB_Test)\n\n# Merge predictions with ID's to create submission\nidCol = df_test[\"id\"].values\nif len(ypred) == len(idCol):\n    sub = np.column_stack((idCol,ypred))\n    df_sub = pd.DataFrame(data=sub,columns=[\"id\",\"price_doc\"])\n    df_sub = df_sub.astype(int)\n    print(\"Final output:\")\n    print(df_sub.head())\nelse:\n    print(\"Oh snap!\")\n\n# Generating second submission and submitting for score\ndf_sub.to_csv('submission2.csv', index=False)\n\n# Score = 0.34993\n# This is actually worse... Probably need to clean up the data a bit.", "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"collapsed": false, "_execution_state": "idle", "trusted": false, "_cell_guid": "e7ec8c5f-ca87-40a4-9189-8de984d93055", "_uuid": "7a8fc14c7bee93f04815f2567a5a3e002349ae5b"}}]}