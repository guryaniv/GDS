{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport sys\nimport datetime\nimport numpy as np\nimport pandas as pd \nimport tensorflow as tf\nimport zipfile\nfrom matplotlib import pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# Loading Dataset\ntrain_df = pd.read_csv(\"../input/train.csv\")\ntest_df = pd.read_csv(\"../input/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e9a909b164a96aac0e5a52da6bba022eedf32c7d"},"cell_type":"code","source":"print(train_df.author.value_counts())\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ae7c64a6194e09596fb672c763e04f7dbdd700ce"},"cell_type":"code","source":"train_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cabbf6ea1c0e6b1d48d5bc8b90d6aa3127ebae15"},"cell_type":"code","source":"index_to_drop = train_df[train_df['author'] == \"HPL\"].index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"575ae30af443105b6228b3951162e0c96b584d9f"},"cell_type":"code","source":"train_df.drop(index_to_drop, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c33befe814498bb506052edf89122be688d79861"},"cell_type":"code","source":"print(train_df.author.value_counts())\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b8a10aced124774d3ecb33f9a789ed422dcacc6d"},"cell_type":"markdown","source":"## Downloading BERT Addon From Official GitHub"},{"metadata":{"trusted":true,"_uuid":"31c75ba3cc987f4f6a7306ca464022f08f847a73"},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0957e54b2640cdfc1ce6c1583a9691cff6d66c2f"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fb6874a0cbba7020158f97a2c13e91e75fdebeaf"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b9e7030a1c4925b44259ae2ef1b24871b5b1f47c"},"cell_type":"code","source":"# Python Files,Model And Configurations from -- https://github.com/google-research/bert\n!wget https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\n!wget https://raw.githubusercontent.com/google-research/bert/master/modeling.py \n!wget https://raw.githubusercontent.com/google-research/bert/master/optimization.py \n!wget https://raw.githubusercontent.com/google-research/bert/master/run_classifier.py \n!wget https://raw.githubusercontent.com/google-research/bert/master/tokenization.py ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b9e9a8d9833074951e6f6c7326c6334127759296"},"cell_type":"code","source":"import modeling\nimport optimization\nimport run_classifier\nimport tokenization","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"aa11efc50969e43c072f0f5475314e0b7c8edad5"},"cell_type":"code","source":"folder = 'model_folder'\nwith zipfile.ZipFile(\"uncased_L-12_H-768_A-12.zip\",\"r\") as zip_ref:\n    zip_ref.extractall(folder)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"139bdfec3b168c3cc61091047cfb7071be4e4cc9"},"cell_type":"code","source":"BERT_MODEL = 'uncased_L-12_H-768_A-12'\nBERT_PRETRAINED_DIR = f'{folder}/uncased_L-12_H-768_A-12'\nOUTPUT_DIR = f'{folder}/outputs'\nprint(f'>> Model output directory: {OUTPUT_DIR}')\nprint(f'>>  BERT pretrained directory: {BERT_PRETRAINED_DIR}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c65fc21a60a12c5bdfdb404c2b26c8fd91dd736d"},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d927947a9619c53a0fac2dc3d39ae098179168ad"},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ntrain_df.author = le.fit_transform(train_df.author)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e7b7c08b9009bca9ac77d74a3e4763ee892e1693"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"28c96ac02770802bf79a39f21d81de90e009c9fe"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ntrain, test = train_test_split(train_df, test_size = 0.2, random_state=1326)\ntrain_lines, train_labels = train.text.values, train.author.values\ntest_lines, test_labels = test.text.values, test.author.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9aa082dee0184f242553a2fb13b64df5edc5a05d"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d84404cffe96d50b326948e7e7b6605c996a3f74"},"cell_type":"code","source":"def create_examples(lines, set_type, labels=None):\n#Generate data for the BERT model\n    guid = f'{set_type}'\n    examples = []\n    if guid == 'train':\n        for line, label in zip(lines, labels):\n            text_a = line\n            label = str(label)\n            examples.append(\n              run_classifier.InputExample(guid=guid, text_a=text_a, text_b=None, label=label))\n    else:\n        for line in lines:\n            text_a = line\n            label = '0'\n            examples.append(\n              run_classifier.InputExample(guid=guid, text_a=text_a, text_b=None, label=label))\n    return examples\n\n# Model Hyper Parameters\nTRAIN_BATCH_SIZE = 32\nEVAL_BATCH_SIZE = 8\nLEARNING_RATE = 1e-5\nNUM_TRAIN_EPOCHS = 3.0\nWARMUP_PROPORTION = 0.1\nMAX_SEQ_LENGTH = 128\n# Model configs\nSAVE_CHECKPOINTS_STEPS = 1000 #if you wish to finetune a model on a larger dataset, use larger interval\n# each checpoint weights about 1,5gb\nITERATIONS_PER_LOOP = 1000\nNUM_TPU_CORES = 8\nVOCAB_FILE = os.path.join(BERT_PRETRAINED_DIR, 'vocab.txt')\nCONFIG_FILE = os.path.join(BERT_PRETRAINED_DIR, 'bert_config.json')\nINIT_CHECKPOINT = os.path.join(BERT_PRETRAINED_DIR, 'bert_model.ckpt')\nDO_LOWER_CASE = BERT_MODEL.startswith('uncased')\nlabel_list = ['0', '1','2']\ntokenizer = tokenization.FullTokenizer(vocab_file=VOCAB_FILE, do_lower_case=DO_LOWER_CASE)\ntrain_examples = create_examples(train_lines, 'train', labels=train_labels)\ntpu_cluster_resolver = None #Since training will happen on GPU, we won't need a cluster resolver\n#TPUEstimator also supports training on CPU and GPU. You don't need to define a separate tf.estimator.Estimator.\nrun_config = tf.contrib.tpu.RunConfig(\n    cluster=tpu_cluster_resolver,\n    model_dir=OUTPUT_DIR,\n    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS,\n    tpu_config=tf.contrib.tpu.TPUConfig(\n        iterations_per_loop=ITERATIONS_PER_LOOP,\n        num_shards=NUM_TPU_CORES,\n        per_host_input_for_training=tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2))\n\nnum_train_steps = int(\n    len(train_examples) / TRAIN_BATCH_SIZE * NUM_TRAIN_EPOCHS)\nnum_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)\n\nmodel_fn = run_classifier.model_fn_builder(\n    bert_config=modeling.BertConfig.from_json_file(CONFIG_FILE),\n    num_labels=len(label_list),\n    init_checkpoint=INIT_CHECKPOINT,\n    learning_rate=LEARNING_RATE,\n    num_train_steps=num_train_steps,\n    num_warmup_steps=num_warmup_steps,\n    use_tpu=False, #If False training will fall on CPU or GPU, depending on what is available  \n    use_one_hot_embeddings=True)\n\nestimator = tf.contrib.tpu.TPUEstimator(\n    use_tpu=False, #If False training will fall on CPU or GPU, depending on what is available \n    model_fn=model_fn,\n    config=run_config,\n    train_batch_size=TRAIN_BATCH_SIZE,\n    eval_batch_size=EVAL_BATCH_SIZE)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dc0ae2d482eb581fa962b458b126e61529df72fe"},"cell_type":"code","source":"print('Please wait...')\ntrain_features = run_classifier.convert_examples_to_features(\n    train_examples, label_list, MAX_SEQ_LENGTH, tokenizer)\nprint('>> Started training at {} '.format(datetime.datetime.now()))\nprint('  Num examples = {}'.format(len(train_examples)))\nprint('  Batch size = {}'.format(TRAIN_BATCH_SIZE))\ntf.logging.info(\"  Num steps = %d\", num_train_steps)\ntrain_input_fn = run_classifier.input_fn_builder(\n    features=train_features,\n    seq_length=MAX_SEQ_LENGTH,\n    is_training=True,\n    drop_remainder=True)\nestimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\nprint('>> Finished training at {}'.format(datetime.datetime.now()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7a4497a205089fd6229795f0187aa89bf9fffd83"},"cell_type":"code","source":"\"\"\"\nThere is a weird bug in original code.\nWhen predicting, estimator returns an empty dict {}, without batch_size.\nI redefine input_fn_builder and hardcode batch_size, irnoring 'params' for now.\n\"\"\"\n\ndef input_fn_builder(features, seq_length, is_training, drop_remainder):\n  \"\"\"Creates an `input_fn` closure to be passed to TPUEstimator.\"\"\"\n\n  all_input_ids = []\n  all_input_mask = []\n  all_segment_ids = []\n  all_label_ids = []\n\n  for feature in features:\n    all_input_ids.append(feature.input_ids)\n    all_input_mask.append(feature.input_mask)\n    all_segment_ids.append(feature.segment_ids)\n    all_label_ids.append(feature.label_id)\n\n  def input_fn(params):\n    \"\"\"The actual input function.\"\"\"\n    print(params)\n    batch_size = 500\n\n    num_examples = len(features)\n\n    d = tf.data.Dataset.from_tensor_slices({\n        \"input_ids\":\n            tf.constant(\n                all_input_ids, shape=[num_examples, seq_length],\n                dtype=tf.int32),\n        \"input_mask\":\n            tf.constant(\n                all_input_mask,\n                shape=[num_examples, seq_length],\n                dtype=tf.int32),\n        \"segment_ids\":\n            tf.constant(\n                all_segment_ids,\n                shape=[num_examples, seq_length],\n                dtype=tf.int32),\n        \"label_ids\":\n            tf.constant(all_label_ids, shape=[num_examples], dtype=tf.int32),\n    })\n\n    if is_training:\n      d = d.repeat()\n      d = d.shuffle(buffer_size=100)\n\n    d = d.batch(batch_size=batch_size, drop_remainder=drop_remainder)\n    return d\n\n  return input_fn","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c17ba84dd6df92f25c969a0d754f24dcf8652240"},"cell_type":"code","source":"len(test_lines)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f7ba1162c1e80c0b87c968e6c2135cfed35723ac"},"cell_type":"code","source":"predict_examples = create_examples(test_lines, 'test')\n\npredict_features = run_classifier.convert_examples_to_features(\n    predict_examples, label_list, MAX_SEQ_LENGTH, tokenizer)\n\npredict_input_fn = input_fn_builder(\n    features=predict_features,\n    seq_length=MAX_SEQ_LENGTH,\n    is_training=False,\n    drop_remainder=False)\n\nresult = estimator.predict(input_fn=predict_input_fn)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"83dac0bce89741b01ffb39929ceb265ebc946ee4"},"cell_type":"code","source":"from tqdm import tqdm\npreds = []\nfor prediction in tqdm(result):\n      preds.append((prediction['probabilities']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"67502c1727746bd018a354da0124d9d19b7471c5"},"cell_type":"code","source":"PREDICTIONS = np.array(preds).argmax(axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b0a85ad81738ce436f27217d47a85ffb5c5164e6"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"56b3a994abaa95e19d2f70fd0e10e2bbd724b0c0"},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score, roc_auc_score\nprint(\">> Spooky Author Experiment (2 Classes Only)\")\nprint(\">> F1 Score on 4500 trained samples: \", f1_score(test_labels, PREDICTIONS))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"565db8e09dc68d6f490389fb01cd5edcf9f64bb6"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}