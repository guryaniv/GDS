{"nbformat_minor": 1, "cells": [{"cell_type": "code", "execution_count": null, "metadata": {"_uuid": "879762d86b27fa2b6fc0fed43fe35e5760fabf01", "_cell_guid": "c4f516db-88b9-40f5-9ab4-a58add0f6b32"}, "source": ["import numpy as np\n", "import pandas as pd\n", "import xgboost as xgb\n", "import lightgbm as lgb\n", "from sklearn import neighbors\n", "from sklearn.neighbors import KNeighborsRegressor\n", "from sklearn.preprocessing import OneHotEncoder\n", "from sklearn.preprocessing import StandardScaler\n", "from sklearn.preprocessing import LabelEncoder\n", "from sklearn.linear_model import LinearRegression\n", "import random\n", "import datetime as dt\n", "import gc\n", "\n", "#Viz\n", "import seaborn as sns\n", "import matplotlib.pyplot as plt\n", "import seaborn as sns\n", "from numpy import arange\n", "%matplotlib inline\n", "\n", "pd.options.mode.chained_assignment = None\n", "pd.options.display.max_columns = 999\n", "\n", "#Explore training data\n", "train_start= pd.read_csv('../input/train_2016_v2.csv', parse_dates=[\"transactiondate\"])\n", "train_start.head()\n", "\n", "from subprocess import check_output\n", "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))"], "outputs": []}, {"cell_type": "code", "execution_count": null, "metadata": {"_uuid": "31fff4e669f284acbdecbec560379c8260c97c0a", "_cell_guid": "3df2cd10-7464-4646-aae6-04fed5413034"}, "source": ["#Explore training data\n", "train_start= pd.read_csv('../input/train_2016_v2.csv', parse_dates=[\"transactiondate\"])\n", "train_start.head()"], "outputs": []}, {"cell_type": "code", "execution_count": null, "metadata": {"_uuid": "4e3e8549443019043de2918fc6c1c06928f38a03", "_cell_guid": "b14323c5-d6f9-422b-89e1-34a7e1b2ffd6"}, "source": ["#Explore LogError to check for outliers because improving residual error is a component of the competition\n", "plt.figure(figsize=(8,6))\n", "plt.scatter(range(train_start.shape[0]), np.sort(train_start.logerror.values))\n", "plt.xlabel('index', fontsize=14)\n", "plt.ylabel('logerror', fontsize=14)\n", "plt.show()"], "outputs": []}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true, "_uuid": "0c0214c7ec2d3f40d34b5da275d9a5476037e31b", "_cell_guid": "36ea34a3-abee-422f-957b-f4aa503230b9"}, "source": ["#Remove outliers\n", "#upper_per = np.percentile(train_start.logerror.values, 99)\n", "#lower_per = np.percentile(train_start.logerror.values, 1)\n", "#train_start['logerror'].ix[train_start['logerror']>upper_per] = upper_per\n", "#train_start['logerror'].ix[train_start['logerror']<lower_per] = lower_per\n", "\n", "#Histogram of resulting data\n", "#plt.figure(figsize=(12,8))\n", "#sns.distplot(train_start.logerror.values, bins=50, kde=False)\n", "#plt.xlabel('logerror', fontsize=14)\n", "#plt.show()"], "outputs": []}, {"cell_type": "code", "execution_count": null, "metadata": {"_uuid": "b3faae365e030505a5f4845484e6cfdbc566eb8f", "_cell_guid": "bedca074-36f4-4c5d-a293-9b8ede2b34b0"}, "source": ["#Explore the data sales dates i.e. number of transactions in a month\n", "train_start['transaction_month']=train_start['transactiondate'].dt.month\n", "counts=train_start['transaction_month'].value_counts()\n", "plt.figure(figsize=(14,8))\n", "sns.barplot(counts.index, counts.values, alpha=0.8)\n", "plt.xticks(rotation='vertical')\n", "plt.xlabel('Transactions (month)', fontsize=14)\n", "plt.ylabel('Occurences',fontsize=14)\n", "plt.show()"], "outputs": []}, {"cell_type": "code", "execution_count": null, "metadata": {"_uuid": "634ebd3643d7e0e6b43bdf246d8c6d549fceaf09", "_cell_guid": "66b8b49b-b716-481c-a37c-c9525e0769a5"}, "source": ["#Datasets\n", "prop_start = pd.read_csv('../input/properties_2016.csv')\n", "test2 = pd.read_csv('../input/sample_submission.csv')\n", "#Rename test data field ParcelID to match training data\n", "test = test2.rename(columns={'ParcelId':'parcelid'})\n", "#print (test.dtypes)\n", "#print (train.dtypes)\n", "#print (props.dtypes)\n", "\n", "#Explore Propery data fields\n", "prop_start.head()"], "outputs": []}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true, "_uuid": "696563e7bad0502f06c09fcd36a92c0a087ba567", "_cell_guid": "c2bffb03-dfef-43f3-98d4-77032a8424af"}, "source": ["##Change the data types to improve memory usage\n", "for column in prop_start.columns:\n", "    if prop_start[column].dtype==int:\n", "        prop_start[column]=prop_start[column].astype(np.int32)\n", "    if prop_start[column].dtype==float:\n", "        prop_start[column]=prop_start[column].astype(np.float32)\n", "        \n", "for column in test.columns:\n", "       if test[column].dtype==int:\n", "           test[column]=test[column].astype(np.int32)\n", "       if test[column].dtype==float:\n", "           test[column]=test[column].astype(np.float32)"], "outputs": []}, {"cell_type": "code", "execution_count": null, "metadata": {"_uuid": "dede59823aa611fcf9d5a47055c4881fd36ad478", "_cell_guid": "13a2ff9d-493f-4547-bd47-0f6221372dba"}, "source": ["#Visualize the missing data NaN\n", "missing_dv=prop_start.isnull().sum(axis=0).reset_index()\n", "missing_dv.columns=['column_name', 'missing']\n", "missing_dv = missing_dv.ix[missing_dv['missing']>0]\n", "missing_dv = missing_dv.sort_values(by='missing')\n", "wth=0.9\n", "index=np.arange(missing_dv.shape[0])\n", "fig, ax = plt.subplots(figsize=(14,20))\n", "rects = ax. barh(index, missing_dv.missing.values, color='blue')\n", "ax.set_yticks(index)\n", "ax.set_yticklabels(missing_dv.column_name.values, rotation='horizontal')\n", "ax.set_xlabel(\"Missing Values (count)\")\n", "plt.show()"], "outputs": []}, {"cell_type": "code", "execution_count": null, "metadata": {"_uuid": "b06ba3ae00ad9dca86e245abd2b121f22353f10c", "_cell_guid": "dde64b77-bd6d-42b7-97a3-f49a6e6aa05f"}, "source": ["#Explore spatial data files\n", "plt.figure(figsize=(14,14))\n", "sns.jointplot(x=prop_start.latitude.values, y=prop_start.longitude.values, size=12)\n", "plt.ylabel('Longitude',fontsize=8)\n", "plt.xlabel('Latitude', fontsize=8)\n", "plt.show()"], "outputs": []}, {"cell_type": "code", "execution_count": null, "metadata": {"_uuid": "e29527ba2826b48a892786455505c29103702860", "_cell_guid": "bfed691e-2a3c-416e-b7aa-4a0367ff2eca"}, "source": ["###Calculate some properties for estimating home value\n", "\n", "#Living area of properties\n", "prop_start['living_area'] = prop_start['calculatedfinishedsquarefeet']/prop_start['lotsizesquarefeet']\n", "prop_start['EstRoomSize']= prop_start['calculatedfinishedsquarefeet']/prop_start['roomcnt']\n", "\n", "#Ratio of tax per worth of property\n", "prop_start['tax_ratio']=prop_start['taxvaluedollarcnt']/prop_start['taxamount']\n", "\n", "#Proportion of structure value to land\n", "prop_start['tax_proportion']=prop_start['structuretaxvaluedollarcnt']/prop_start['landtaxvaluedollarcnt']\n", "\n", "#Time of unpaid taxes\n", "prop_start['DeliqCount']= 2018- prop_start['taxdelinquencyyear']\n", "\n", "#Simplify Land Uses from 25 to 4 categories: Mixed, Home, Other, Not Built\n", "prop_start['LandUse'] = prop_start.propertylandusetypeid.replace({31 : \"Mixed\", 46 : \"Other\", \n", "                                                                  47 : \"Mixed\", 246 : \"Mixed\", \n", "                                                                  247 : \"Mixed\", 248 : \"Mixed\", \n", "                                                                  260 : \"Home\", 261 : \"Home\", 262 : \"Home\", \n", "                                                                  263 : \"Home\", 264 : \"Home\", 265 : \"Home\", \n", "                                                                  266 : \"Home\", 267 : \"Home\", 268 : \"Home\", \n", "                                                                  269 : \"Not Built\", 270 : \"Home\", 271 : \"Home\", \n", "                                                                  273 : \"Home\", 274 : \"Other\", 275 : \"Home\", \n", "                                                                  276 : \"Home\", 279 : \"Home\", 290 : \"Not Built\", \n", "                                                                  291 : \"Not Built\" })\n", "print('Done')"], "outputs": []}, {"cell_type": "code", "execution_count": null, "metadata": {"_uuid": "60bc8c7bf5a23b8fad3a2a61c6bdbac1c974b5e0", "_cell_guid": "f4f15a36-7462-4f4b-a862-117d9364238b"}, "source": ["#Add propertiy data to training and testing datasets\n", "m_train= train_start.merge(prop_start, how='left', on='parcelid')\n", "m_test= test.merge(prop_start, how='left', on='parcelid')\n", "\n", "#Look at heatmap to see if variables are correlated\n", "var= ['airconditioningtypeid','architecturalstyletypeid','buildingqualitytypeid',\n", "            'buildingclasstypeid','decktypeid','fips','hashottuborspa','heatingorsystemtypeid','living_area',\n", "            'pooltypeid10','pooltypeid2','pooltypeid7','propertycountylandusecode',\n", "            'propertylandusetypeid','propertyzoningdesc','rawcensustractandblock','regionidcity',\n", "            'regionidcounty','regionidneighborhood','regionidzip','storytypeid','tax_ratio', 'tax_proportion',\n", "            'typeconstructiontypeid','yearbuilt','taxdelinquencyflag']\n", "call=[i for i in m_train.columns if i not in var]\n", "plt.figure(figsize=(14,14))\n", "cmap=sns.diverging_palette(220,20, sep=20, as_cmap=True)\n", "sns.heatmap(data=m_train[call].corr(),cmap=cmap)\n", "plt.show()\n", "plt.gcf().clear()"], "outputs": []}, {"cell_type": "code", "execution_count": null, "metadata": {"_uuid": "0bacc774d1074781b19178c9fd84aa914a630689", "_cell_guid": "c5166933-b99d-4eb8-a270-287e3c8992d4"}, "source": ["###More data clean-up, identify and fill-in missing values with label encoder\n", "#Label Encoding: Encode labels with value between 0 and n_classes-1.\n", "from sklearn.preprocessing import LabelEncoder\n", "LE=LabelEncoder()\n", "for l in m_train.columns:\n", "    m_train[l]=m_train[l].fillna(0)\n", "    if m_train[l].dtype=='object':\n", "        LE.fit(list(m_train[l].values)) #normalize labels\n", "        m_train[l]=LE.transform(list(m_train[l].values))  #transform non-numerical labels to numerical\n", "        \n", "for l in m_test.columns:\n", "    m_test[l]=m_test[l].fillna(0)\n", "    if m_test[l].dtype=='object':\n", "        LE.fit(list(m_test[l].values)) #normalize labels\n", "        m_test[l]=LE.transform(list(m_test[l].values)) #transform non-numerical labels to numerical\n", "        \n", "#Drop properties we don't need anymore for training: parcelid, log error, sell date, propert zoning description and land use code\n", "sec_train=m_train.drop(['transaction_month','parcelid', 'logerror', 'transactiondate', 'propertyzoningdesc', 'propertycountylandusecode'], axis=1)\n", "#Drop values we don't need for validation\n", "sec_test= m_test.drop(['parcelid','propertyzoningdesc','propertycountylandusecode','201610','201611','201612', '201710', '201711', '201712'], axis=1)\n", "print('Done')"], "outputs": []}, {"cell_type": "code", "execution_count": null, "metadata": {"_uuid": "3056a5f068b35e9b068c3a80c5ab175d5bddd8a4", "_cell_guid": "9803f543-e4d9-4d27-86de-236d346753a6", "scrolled": true}, "source": ["#Use XGBoost to look more into the variable importance\n", "X=sec_train.values\n", "Y=m_train['logerror'].values\n", "xgb_params = {'eta': 0.05,'max_depth': 8,'subsample': 0.7,'colsample_bytree': 0.7, 'objective': 'reg:linear',\n", "    'silent': 1,'seed' : 0}\n", "dtrain = xgb.DMatrix(sec_train, Y, feature_names=sec_train.columns.values)\n", "model = xgb.train(dict(xgb_params, silent=0), dtrain, num_boost_round=150)\n", "# plot the important features #\n", "fig, ax = plt.subplots(figsize=(18,18))\n", "xgb.plot_importance(model, max_num_features=50, height=0.8, ax=ax)\n", "plt.show()"], "outputs": []}, {"cell_type": "code", "execution_count": null, "metadata": {"_uuid": "f4f7a58dc43d58f3af546330356942bd4f9de0a4", "_cell_guid": "4e59b160-35cb-4bd8-8d76-1966348d0d80"}, "source": ["train_columns = sec_train.columns\n", "d_train = lgb.Dataset(sec_train, label=Y)\n", "params = {'max_bin': 10,'learning_rate': 0.0021,'boosting_type': 'gbdt','objective': 'regression',\n", "          'metric': 'l1','sub_feature': 0.345,'bagging_fraction': 0.85,'bagging_freq': 40,\n", "          'num_leaves':512, 'min_data': 500,'min_hessian': 0.05,'verbose': 0,'feature_fraction_seed':2,\n", "          'bagging_seed': 3}\n", "#Light Gradient Boosting Model\n", "print('Fitting LightGBM model ...')\n", "clf = lgb.train(params, d_train, 200)\n", "print ('completed')\n"], "outputs": []}, {"cell_type": "code", "execution_count": null, "metadata": {"_uuid": "d97d7d74e278a567bc7074283521732e6692888e", "_cell_guid": "6ec46cbe-0563-4332-bf76-eb0cbaf04a4a"}, "source": ["Xtest = sec_test[train_columns]\n", "p_test = clf.predict(Xtest)\n", "#print('1')\n", "Ymean = np.mean(Y)\n", "#Extreme Gradient Boosting Model 1\n", "print ('prepping paramaters for Xboost')\n", "paramsXGB = {'eta': 0.037,'max_depth': 5,'subsample': 0.80,'objective': 'reg:linear','eval_metric': 'mae',\n", "    'lambda': 0.8,'alpha': 0.4, 'base_score': Ymean,'silent': 1}\n", "dtrain = xgb.DMatrix(sec_train, Y)\n", "dtest = xgb.DMatrix(Xtest)\n", "num_boost = 250\n", "print( 'Training XGBoost')\n", "model = xgb.train(dict(paramsXGB, silent=1), dtrain, num_boost_round=num_boost)\n", "print( 'Predicting with XGBoost')\n", "xgb_pred1 = model.predict(dtest)\n", "print( 'First XGBoost predictions:')\n", "print( pd.DataFrame(xgb_pred1).head())"], "outputs": []}, {"cell_type": "code", "execution_count": null, "metadata": {"_uuid": "536227c666da8d4a58037be1f320f3a2d68b098d", "_cell_guid": "2794a12d-d1ce-4dde-95df-399b2086560c"}, "source": ["#Extreme Gradient Boosting Model 2\n", "paramsXGB2 = {'eta': 0.033,'max_depth': 6,'subsample': 0.80,'objective': 'reg:linear',\n", "    'eval_metric': 'mae','base_score': Ymean,'silent': 1}\n", "num_boost2 = 150\n", "print( 'Training XGBoost for second model ...')\n", "model = xgb.train(dict(paramsXGB2, silent=1), dtrain, num_boost_round=num_boost2)\n", "#print( 'Predicting with XGBoost second time')\n", "xgb_pred2 = model.predict(dtest)\n", "print( 'Second XGBoost predictions:' )\n", "print( pd.DataFrame(xgb_pred2).head() )"], "outputs": []}, {"cell_type": "code", "execution_count": null, "metadata": {"_uuid": "3ac1c93dfecbc99a3f602747448265d540b47700", "_cell_guid": "4170ddf3-2b39-43f9-bdc8-5bdf02ff9baa"}, "source": ["XGB1_WEIGHT = 0.8083 # Weight of first in combination of two XGB models\n", "xgb_pred = XGB1_WEIGHT*xgb_pred1 + (1-XGB1_WEIGHT)*xgb_pred2\n", "print( 'Combined XGBoost predictions:' )\n", "print( pd.DataFrame(xgb_pred).head() )"], "outputs": []}, {"cell_type": "code", "execution_count": null, "metadata": {"_uuid": "d1398c165cb7f4748a704e5f79fb4f02e3863ea6", "_cell_guid": "271854ee-8ea7-4ee2-8975-e61ef79d7760", "scrolled": false}, "source": ["#Combine LGB and Xboost\n", "XGB_WEIGHT = 0.700 #based on https://www.kaggle.com/hsperr/finding-ensamble-weights and various Kernel results\n", "BASELINE_WEIGHT = 0.0056 #based on\n", "OLS_WEIGHT = 0.0620 # based on https://www.kaggle.com/the1owl/primer-for-the-zillow-pred-approach\n", "XGB1_WEIGHT = 0.8083 # Weight of first in combination of two XGB models\n", "BASELINE_PRED = 0.0115 # Baseline based on mean of training data\n", "gc.collect()\n", "np.random.seed(17)\n", "random.seed(17)\n", "train_F = pd.read_csv(\"../input/train_2016_v2.csv\", parse_dates=[\"transactiondate\"])\n", "properties2 = pd.read_csv(\"../input/properties_2016.csv\")\n", "submission = pd.read_csv(\"../input/sample_submission.csv\")\n", "#print(len(train_F),len(properties2),len(submission))\n", "##OLS based on https://www.kaggle.com/the1owl/primer-for-the-zillow-pred-approach\n", "def get_features(df):\n", "    df[\"transactiondate\"] = pd.to_datetime(df[\"transactiondate\"])\n", "    df[\"transactiondate_year\"] = df[\"transactiondate\"].dt.year\n", "    df[\"transactiondate_month\"] = df[\"transactiondate\"].dt.month\n", "    df['transactiondate'] = df['transactiondate'].dt.quarter\n", "    df = df.fillna(-1.0)\n", "    return df\n", "#Submission Calculation: logerror=log(Zestimate)\u2212log(SalePrice)\n", "def MAE(y, ypred):\n", "    return np.sum([abs(y[i]-ypred[i]) for i in range(len(y))]) / len(y)\n", "train_F = pd.merge(train_F, properties2, how='left', on='parcelid')\n", "Y = train_F['logerror'].values\n", "test = pd.merge(submission, properties2, how='left', left_on='ParcelId', right_on='parcelid')\n", "properties = [] #memory\n", "exc = [train_F.columns[c] for c in range(len(train_F.columns)) if train_F.dtypes[c] == 'O'] + ['logerror','parcelid']\n", "col = [c for c in train_F.columns if c not in exc]\n", "train_F = get_features(train_F[col])\n", "test['transactiondate'] = '2016-01-01' \n", "test = get_features(test[col])\n", "reg = LinearRegression(n_jobs=-1)\n", "reg.fit(train_F, Y); #print('fit...')\n", "#print(MAE(Y, reg.predict(train_F)))\n", "train_F = [];  Y = [] \n", "test_dates = ['2016-10-01','2016-11-01','2016-12-01','2017-10-01','2017-11-01','2017-12-01']\n", "test_columns = ['201610','201611','201612','201710','201711','201712']\n", "print( 'Combining XGBoost, LightGBM, and baseline predicitons' )\n", "lgb_weight = (1 - XGB_WEIGHT - BASELINE_WEIGHT) / (1 - OLS_WEIGHT)\n", "xgb_weight0 = XGB_WEIGHT / (1 - OLS_WEIGHT)\n", "baseline_weight0 =  BASELINE_WEIGHT / (1 - OLS_WEIGHT)\n", "pred0 = xgb_weight0*xgb_pred + baseline_weight0*BASELINE_PRED + lgb_weight*p_test\n", "print( 'Combined XGB/LGB/baseline predictions:' )\n", "print( pd.DataFrame(pred0).head() )\n", "print( 'Predicting with OLS and combining with XGB/LGB/baseline predicitons:' )\n", "for i in range(len(test_dates)):\n", "    test['transactiondate'] = test_dates[i]\n", "    pred = OLS_WEIGHT*reg.predict(get_features(test)) + (1-OLS_WEIGHT)*pred0\n", "    submission[test_columns[i]] = [float(format(x, '.4f')) for x in pred]\n", "    print('predict...', i)\n", "print( 'Final Prediction Log Errors (XGB/LGB/baseline):' )\n", "from datetime import datetime\n", "print( 'Writing results to disk')\n", "submission.to_csv('sub{}.csv'.format(datetime.now().strftime('%Y%m%d_%H%M%S')), index=False)\n", "print( 'Finished')\n", "LogError=submission['201610'].values\n", "Avg=np.mean(LogError)\n", "print ('Avgerage Log Error: ')\n", "print (Avg)\n", "submission"], "outputs": []}], "metadata": {"language_info": {"mimetype": "text/x-python", "pygments_lexer": "ipython3", "codemirror_mode": {"version": 3, "name": "ipython"}, "version": "3.6.3", "name": "python", "file_extension": ".py", "nbconvert_exporter": "python"}, "kernelspec": {"display_name": "Python 3", "name": "python3", "language": "python"}}, "nbformat": 4}