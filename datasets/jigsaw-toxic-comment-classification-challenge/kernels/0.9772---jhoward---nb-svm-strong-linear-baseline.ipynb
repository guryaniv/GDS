{"cells": [{"metadata": {"_cell_guid": "d3b04218-0413-4e6c-8751-5d8a404d73a9", "_uuid": "0bca9739b82d5d51e1229243e03ea1b6db35c17e"}, "source": ["## Introduction\n", "\n", "This kernel shows how to use NBSVM (Naive Bayes - Support Vector Machine) to create a strong baseline for the [Toxic Comment Classification Challenge](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge) competition. NBSVM was introduced by Sida Wang and Chris Manning in the paper [Baselines and Bigrams: Simple, Good Sentiment and Topic Classi\ufb01cation](https://nlp.stanford.edu/pubs/sidaw12_simple_sentiment.pdf). In this kernel, we use sklearn's logistic regression, rather than SVM, although in practice the two are nearly identical (sklearn uses the liblinear library behind the scenes).\n", "\n", "If you're not familiar with naive bayes and bag of words matrices, I've made a preview available of one of fast.ai's upcoming *Practical Machine Learning* course videos, which introduces this topic. Here is a link to the section of the video which discusses this: [Naive Bayes video](https://youtu.be/37sFIak42Sc?t=3745)."], "cell_type": "markdown"}, {"metadata": {"_cell_guid": "ef06cd19-66b6-46bc-bf45-184e12d3f7d4", "_uuid": "cca038ca9424a3f66e10262fc9129de807b5f855", "collapsed": true}, "outputs": [], "source": ["import pandas as pd, numpy as np\n", "from sklearn.linear_model import LogisticRegression\n", "from sklearn.feature_extraction.text import CountVectorizer"], "execution_count": null, "cell_type": "code"}, {"metadata": {"_cell_guid": "a494f561-0c2f-4a38-8973-6b60c22da357", "_uuid": "f70ebe669fcf6b434c595cf6fb7a76120bf7809c", "collapsed": true}, "outputs": [], "source": ["train = pd.read_csv('../input/train.csv')\n", "test = pd.read_csv('../input/test.csv')\n", "subm = pd.read_csv('../input/sample_submission.csv')"], "execution_count": null, "cell_type": "code"}, {"metadata": {"_cell_guid": "3996a226-e1ca-4aa8-b39f-6524d4dadb07", "_uuid": "2c18461316f17d1d323b1959c8eb4e5448e8a44e"}, "source": ["## Looking at the data\n", "\n", "The training data contains a row per comment, with an id, the text of the comment, and 6 different labels that we'll try to predict."], "cell_type": "markdown"}, {"metadata": {"_cell_guid": "5ddb337b-c9b2-4fec-9652-cb26769dc3c6", "_uuid": "5f5269c56ea6ded273881b0d4dcdb6af83a3e089", "scrolled": true}, "outputs": [], "source": ["train.head()"], "execution_count": null, "cell_type": "code"}, {"metadata": {"_cell_guid": "b3b071fb-7a2c-4195-9817-b01983d11c0e", "_uuid": "004d2e823056e98afc5adaac433b7afbfe93b82d"}, "source": ["Here's a couple of examples of comments, one toxic, and one with no labels."], "cell_type": "markdown"}, {"metadata": {"_cell_guid": "d57f0b31-c09b-4305-a0b0-0b864e944fd1", "_uuid": "1ba9522a65227881a3a55aefaee9de93c4cfd792"}, "outputs": [], "source": ["train['comment_text'][0]"], "execution_count": null, "cell_type": "code"}, {"metadata": {"_cell_guid": "9caf5da3-33bb-422d-81c4-fef20fbda1a8", "_uuid": "b0d70e9d745411ea6228c95c5f19bd3a2ca6dd55", "scrolled": true}, "outputs": [], "source": ["train['comment_text'][2]"], "execution_count": null, "cell_type": "code"}, {"metadata": {"_cell_guid": "2ea37597-02f7-43cf-ad16-a3d50aac1aba", "_uuid": "5c4c716de98a4b1c2ecc0e516e67813b4fc1473e"}, "source": ["The length of the comments varies a lot."], "cell_type": "markdown"}, {"metadata": {"_cell_guid": "fd3fe158-4d7f-4b30-ac15-42605240ea4f", "_uuid": "9c1a3f81397199fa250a2b642edc7fbc5f9f504e"}, "outputs": [], "source": ["lens = train.comment_text.str.len()\n", "lens.mean(), lens.std(), lens.max()"], "execution_count": null, "cell_type": "code"}, {"metadata": {"_cell_guid": "d2e55012-4736-425f-84f3-c148ac1f4852", "_uuid": "eb68f1c83a5ad11e652ca5f2150993a06d43edb4"}, "outputs": [], "source": ["lens.hist();"], "execution_count": null, "cell_type": "code"}, {"metadata": {"_cell_guid": "b8515824-b2dd-4c95-bbf9-dc74c80355db", "_uuid": "0151ab55887071aed82d297acb2c6545ed964c2b"}, "source": ["We'll create a list of all the labels to predict, and we'll also create a 'none' label so we can see how many comments have no labels. We can then summarize the dataset."], "cell_type": "markdown"}, {"metadata": {"_cell_guid": "c66f79d1-1d9f-4d94-82c1-8026af198f2a", "_uuid": "4ba6ef86c82f073bf411785d971a694348c3efa9"}, "outputs": [], "source": ["label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n", "train['none'] = 1-train[label_cols].max(axis=1)\n", "train.describe()"], "execution_count": null, "cell_type": "code"}, {"metadata": {"_cell_guid": "9f6316e3-7e29-431b-abef-73acf4a08637", "_uuid": "b7b0d391248f929a026b16fc38936b7fc0176351"}, "outputs": [], "source": ["len(train),len(test)"], "execution_count": null, "cell_type": "code"}, {"metadata": {"_cell_guid": "1b221e62-e23f-422a-939d-6747edf2d613", "_uuid": "bfdcf59624717b37ca4ffc0c99d2c28a2d419b06"}, "source": ["There are a few empty comments that we need to get rid of, otherwise sklearn will complain."], "cell_type": "markdown"}, {"metadata": {"_cell_guid": "fdba531c-7ef2-4967-88e2-fc2b04f6f2ef", "_uuid": "1e1229f403225f1889c7a7b4fc9be90fda818af5", "collapsed": true}, "outputs": [], "source": ["COMMENT = 'comment_text'\n", "train[COMMENT].fillna(\"unknown\", inplace=True)\n", "test[COMMENT].fillna(\"unknown\", inplace=True)"], "execution_count": null, "cell_type": "code"}, {"metadata": {"_cell_guid": "480780f1-00c0-4f9a-81e5-fc1932516a80", "_uuid": "f2e77e8e6df5e29b620c7a2a0add1438c35af932"}, "source": ["## Building the model\n", "\n", "We'll start by creating a *bag of words* representation, as a *term document matrix*. We'll use ngrams, as suggested in the NBSVM paper."], "cell_type": "markdown"}, {"metadata": {}, "outputs": [], "source": ["import re, string\n", "re_tok = re.compile(f'([{string.punctuation}\u201c\u201d\u00a8\u00ab\u00bb\u00ae\u00b4\u00b7\u00ba\u00bd\u00be\u00bf\u00a1\u00a7\u00a3\u20a4\u2018\u2019])')\n", "def tokenize(s): return re_tok.sub(r' \\1 ', s).split()"], "execution_count": null, "cell_type": "code"}, {"metadata": {"_cell_guid": "31ad6c98-d054-426c-b3bd-b3b18f52eb6f", "_uuid": "75f3f27d56fb2d7d539e65c292d9e77c92ceead3", "collapsed": true}, "outputs": [], "source": ["n = train.shape[0]\n", "vec = CountVectorizer(ngram_range=(1,2), tokenizer=tokenize)\n", "trn_term_doc = vec.fit_transform(train[COMMENT])\n", "test_term_doc = vec.transform(test[COMMENT])"], "execution_count": null, "cell_type": "code"}, {"metadata": {"_cell_guid": "4cf3ec26-8237-452b-90c9-831cb0297955", "_uuid": "6d215bc460e64d88b08f501d5c5a67c290e40635"}, "source": ["This creates a *sparse matrix* with only a small number of non-zero elements (*stored elements* in the representation  below)."], "cell_type": "markdown"}, {"metadata": {"_cell_guid": "4c7bdbcc-4451-4477-944c-772e99bac777", "_uuid": "8816cc35f66b9fed9c12978fbdef5bb68fae10f4"}, "outputs": [], "source": ["trn_term_doc, test_term_doc"], "execution_count": null, "cell_type": "code"}, {"metadata": {"_cell_guid": "59131479-a861-4f46-add9-b2af09a51976", "_uuid": "5fc487461f4c6fdaea25f2cd471fc801856c6689"}, "source": ["Here's the basic naive bayes feature equation:"], "cell_type": "markdown"}, {"metadata": {"_cell_guid": "45fc6070-ba13-455b-9274-5c2611e2809c", "_uuid": "8b277f01cecd575ed4fcae2e630c0dd8ce979793", "collapsed": true}, "outputs": [], "source": ["def pr(y_i, y):\n", "    p = x[y==y_i].sum(0)\n", "    return (p+1) / ((y==y_i).sum()+1)"], "execution_count": null, "cell_type": "code"}, {"metadata": {"_cell_guid": "de2a19e8-776b-45fa-ae08-ca58ccf625f0", "_uuid": "c2686ec77a47956870bd484d96f8feab230cbcdb"}, "source": ["We *binarize* the features as discussed in the NBSVM paper."], "cell_type": "markdown"}, {"metadata": {"_cell_guid": "2299d24b-5515-4d37-92d9-e7f6b16a290a", "_uuid": "926eaa2e40e588f4ef2b86e0a28f8e575c9ed5f4", "collapsed": true}, "outputs": [], "source": ["x=trn_term_doc.sign()\n", "test_x = test_term_doc.sign()"], "execution_count": null, "cell_type": "code"}, {"metadata": {"_cell_guid": "c0b494ac-0dfc-4faa-a909-0a6d7696d1fc", "_uuid": "dc5cafeab86d17ac4f036d58658437636a885a87"}, "source": ["Fit a model for one dependent at a time:"], "cell_type": "markdown"}, {"metadata": {"_cell_guid": "b756c889-a383-4952-9ee9-eca79fd3454f", "_uuid": "8652ab2f5f84e77fa395252be9b60be1e44fd583", "collapsed": true}, "outputs": [], "source": ["def get_mdl(y):\n", "    y = y.values\n", "    r = np.log(pr(1,y) / pr(0,y))\n", "    m = LogisticRegression(C=0.1, dual=True)\n", "    x_nb = x.multiply(r)\n", "    return m.fit(x_nb, y), r"], "execution_count": null, "cell_type": "code"}, {"metadata": {"_cell_guid": "33fd5f8c-adfc-45a1-9fde-1769a0993e76", "_uuid": "0fa103b5406aabdc36ea9ef21612d343e4982fc4"}, "outputs": [], "source": ["preds = np.zeros((len(test), len(label_cols)))\n", "\n", "for i, j in enumerate(label_cols):\n", "    print('fit', j)\n", "    m,r = get_mdl(train[j])\n", "    preds[:,i] = m.predict_proba(test_x.multiply(r))[:,1]"], "execution_count": null, "cell_type": "code"}, {"metadata": {"_cell_guid": "1a99c4d9-916f-4189-9a25-fedcb7700336", "_uuid": "5525045116474e6d12b6edc890250d30c0790f06"}, "source": ["And finally, create the submission file."], "cell_type": "markdown"}, {"metadata": {"_cell_guid": "bc6a4575-fbbb-47ea-81ac-91fa702dc194", "_uuid": "5dd033a93e6cf32cdbdaa0a8b05cd8d27de2b21d", "collapsed": true}, "outputs": [], "source": ["submid = pd.DataFrame({'id': subm[\"id\"]})\n", "submission = pd.concat([submid, pd.DataFrame(preds, columns = label_cols)], axis=1)\n", "submission.to_csv('submission.csv', index=False)"], "execution_count": null, "cell_type": "code"}, {"metadata": {"collapsed": true}, "outputs": [], "source": [], "execution_count": null, "cell_type": "code"}], "metadata": {"language_info": {"name": "python", "codemirror_mode": {"version": 3, "name": "ipython"}, "file_extension": ".py", "version": "3.6.3", "mimetype": "text/x-python", "pygments_lexer": "ipython3", "nbconvert_exporter": "python"}, "kernelspec": {"language": "python", "display_name": "Python 3", "name": "python3"}}, "nbformat_minor": 1, "nbformat": 4}