{"cells":[{"metadata":{"_uuid":"6ee2b1e0e7ab192d0056628229b4aef75cd70051"},"cell_type":"markdown","source":"\n# <center> Getting started in ML with Titanic dataset! <br>\n![RMS Titanic](https://upload.wikimedia.org/wikipedia/commons/thumb/f/fd/RMS_Titanic_3.jpg/300px-RMS_Titanic_3.jpg \"Titanic photo\")\n<br>\nHello there and welcome to my kernel. Here i will use titanic dataset from titanic competition.<br>\nBefore we start i must say that i'm no data scientist or ML Engineer (yet ;-) ), but a student interested in DS and ML.<br>\nHere i'll try do cover such themes as:<br>\n* basic exploratory data analysis\n* some simple ML models such as LogisticRegression and DecisionTree\n* handling missing values, scaling, feature selection and feature engineering\n* ensembling with RandomForest etc.\n* hyperparameters tuning with cross_val_score and GridSearchCV"},{"metadata":{"_kg_hide-output":true,"trusted":true,"_uuid":"99d94d23d4c9274992ae817a80aaf446555b6ca9"},"cell_type":"code","source":"%env JOBLIBTEMPFOLDER=/tmp","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# From now all i will try to do all the imports\n# before the section where i use them\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\n# you can comment the following 2 lines if you'd like to see warnings\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# If you will use this notebook on your computer you would like to change these\nINPUT_DATA_DIR = '../input'\nTRAIN_FILE_NAME = 'train.csv'\nTEST_FILE_NAME = 'test.csv'\nTRAIN_DATA_PATH = os.path.join(INPUT_DATA_DIR, TRAIN_FILE_NAME)\nTEST_DATA_PATH = os.path.join(INPUT_DATA_DIR, TEST_FILE_NAME)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0b78e04f695d33efb8a9388a9afbd2908aa85563"},"cell_type":"code","source":"# helper function to write submissions\ndef write_submission_file(prediction, filename,\n    path_to_sample=os.path.join(INPUT_DATA_DIR, 'gender_submission.csv')):\n    submission = pd.read_csv(path_to_sample, index_col='PassengerId')\n    submission['Survived'] = prediction\n    submission.to_csv(filename)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f9b33bd5ab19738550ee3170cd633a55c3099130"},"cell_type":"markdown","source":"## 1 - The very beginnings\n### Let's see what we got\nIt's always a good idea to look at data before you start doing anything."},{"metadata":{"trusted":true,"_uuid":"be575dbb55c9144d61854b18c57013149b033f03"},"cell_type":"code","source":"train_data = pd.read_csv(TRAIN_DATA_PATH)\ntest_data = pd.read_csv(TEST_DATA_PATH)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ec5f8e7835ab8fe401e0195ff2a8c1d60de4078b"},"cell_type":"code","source":"train_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"91a0f00f1a3469b5b9740ffae965e426f7f9c952"},"cell_type":"code","source":"print(f'So we got {train_data.columns.values} columns.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d0992fd55940bf55ebf719fc15d0c556ec4a9b9b"},"cell_type":"code","source":"train_data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"803d00f0bddff54d26599910923d83575c8b622f"},"cell_type":"code","source":"test_data.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"149569289ba6cc9645df3209bd4725d7630a42e6"},"cell_type":"markdown","source":"As you can see it's a fairly small dataset (11 features + target and only 891 rows).<br>\nAlso we have a lot of values missing in the `Cabin` column and some part of `Age`, `Fare` and `Embarked` features are missing as well in both datasets.<br>\nNow let's see what pandas can say use about the features."},{"metadata":{"trusted":true,"_uuid":"f96ff47cf54c0812b948835d1f58372144a2791e"},"cell_type":"code","source":"train_data.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"33cab67cef7ac1aadec4fe793aa2a6baa6091be6"},"cell_type":"markdown","source":"So as we can see only around **38%** of ppl from train dataset actually survived."},{"metadata":{"trusted":true,"_uuid":"241a1a01c03dc046cb1c32aa7517273426a96426"},"cell_type":"code","source":"train_data.drop(['Name', 'Ticket'], axis=1).describe(include=['object', 'bool'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4e30a08fc37123a00d1490fa6b8e5c124f71fabf"},"cell_type":"markdown","source":"### New let's do some initial preprocessing <br>\nIt would be a good idea to exclude `PassengerId` and `Ticket`.<br>\nTransform `Sex` to integer (1 - for men, 0 - for women) will help our classifier as well.<br>\nFill the NaN values for `Age`, `Embarked` and `Fare`."},{"metadata":{"trusted":true,"_uuid":"a66673e83555e42009326c44a98e0763c3659971"},"cell_type":"code","source":"train_test = [train_data, test_data]\np_s_age_means = train_data.groupby(['Pclass', 'Sex']).agg({'Age': pd.Series.mean}).values\np_s_age = lambda x: p_s_age_means[x[0]-1+x[1]]\n\nfor dataset in train_test:\n    # excluding some features\n    dataset.drop(['PassengerId', 'Ticket'], axis=1, inplace=True)\n    dataset['Sex'] = dataset['Sex'].map({'male': 1, 'female': 0})\n    dataset['Age'].fillna(train_data[['Pclass', 'Sex']].apply(p_s_age, axis=1),\n                          inplace=True)\n#     #fill missing age with median\n#     dataset['Age'].fillna(train_data['Age'].median(), inplace = True)\n\n    #fill missing embarked with mode\n    dataset['Embarked'].fillna(train_data['Embarked'].mode()[0], inplace = True)\n\n    #fill missing fare with median\n    dataset['Fare'].fillna(train_data['Fare'].median(), inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5e66da23ce95ea19f78bfdd8c158aa14e79b4522"},"cell_type":"markdown","source":"## 2 - A little bit of visualization and feature engineering\nLet's look into our data with the help of visualition. For this purpose we will use `matplotlib` and `seaborn`."},{"metadata":{"trusted":true,"_uuid":"58ea1d7fdc771ed1112b936edc242fed4ec7db48"},"cell_type":"code","source":"# import necessary modules\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nsns.set()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4e53b11f580205905bd9064ad0ae06e02d56ab3d"},"cell_type":"markdown","source":"Let's see how `Sex` affects the survival odds."},{"metadata":{"trusted":true,"_uuid":"9ca0bd908bab0f15fdb871d1752a0032241f9df8"},"cell_type":"code","source":"sns.countplot(x='Sex', hue='Survived', data=train_data)\nplt.ylabel('Survived count')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e7f83f39867d5e2c06f6bad12ec3c6e845eec238"},"cell_type":"markdown","source":"It is cleare, that the survival ration for women is much higher. <br>\nSo `Sex` is very valuable feature for us."},{"metadata":{"trusted":true,"_uuid":"c5308d923a5485decf4db83fb42e99d041323b27"},"cell_type":"code","source":"is_male = (train_data['Sex'] == 1)\nmale_survived_perc = train_data[is_male]['Survived'].mean()\nfemale_survived_perc = train_data[~is_male]['Survived'].mean()\nprint(f'% of men survived: {male_survived_perc*100:.2f}')\nprint(f'% for women survived: {female_survived_perc*100:.2f}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dd530bb1c7c5a02a5fcf48d0f1ff9739426ee482"},"cell_type":"code","source":"sns.countplot(x='Pclass', hue='Survived', data=train_data)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"51686ed5461cb59db8717953aa3ae4f7c5ff29df"},"cell_type":"markdown","source":"We see quite the same situation for `Pclass` - the survival ratio for 3rd class is quite low. <br>\nAnd it's quite logical, since the break in the ship was on the lowest lewels."},{"metadata":{"trusted":true,"_uuid":"81e5ca0de92f0d33a8354f8562127b1860e715f5"},"cell_type":"code","source":"sns.catplot(x='Sex', hue='Survived', col='Pclass',\n            data=train_data, saturation=1,\n            kind='count', ci=None, aspect=0.5, height=5)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"32900fd556062c27e7b75f7f4ff013da25ba5412"},"cell_type":"markdown","source":"Well, almost all women from 1st class survived, ~90% of women from 2nd class as well, but 50% for the third class is quite low.<br>\nFor men the situation was tragical for all `Pclass`'s, but a little bit better for the 1st class."},{"metadata":{"_uuid":"5ff6f3ab1af454de41a658fd961ab597247e1c90"},"cell_type":"markdown","source":"At this moment we have seen that `Pclass` and `Sex` give us a lot of information. <br>\nAnd i am curious to make a baseline model just for this two features so that we could see how adding new features affects the model. <br>\nSo let's make a pause and try to fit the basic `LogisticRegression` with the default parameters and see what we got with `cross_val_score`."},{"metadata":{"trusted":true,"_uuid":"a57bd136157247930802f44dd639bd4ab420ea56"},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\nfrom sklearn.tree import DecisionTreeClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"77810d815cf865f9f23a9a8a447b7990ee6ef032"},"cell_type":"code","source":"train_target = train_data['Survived'] # saving target feature\nbase_feats = ['Sex', 'Pclass']\nX_train_base = train_data[base_feats]\nX_test_base = test_data[base_feats]\nscores = cross_val_score(estimator=DecisionTreeClassifier(random_state=17),\n                         X=X_train_base, y=train_target,\n                         # i know that it's a big cv value,\n                         # but the data is too small\n                         # so we can afford it\n                         cv=10, scoring='accuracy', n_jobs=-1)\nprint(f'The mean accuracy of our baseline is {np.mean(scores)*100} %')\nbase_est = DecisionTreeClassifier(random_state=17)\nbase_est.fit(X_train_base, train_target)\npreds = base_est.predict(X_test_base)\nwrite_submission_file(preds, 'baseline_preds.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8aa0b7101928cb363f89e54701929060e491c42c"},"cell_type":"markdown","source":"78% and around 70% for test data is quite good for a baseline with only two features. <br>\nLet's look further into our data!"},{"metadata":{"trusted":true,"_uuid":"ea750db805410b0f24e0d634cde7145719cf76a1"},"cell_type":"code","source":"sns.catplot(x='Pclass', y='Survived', hue='Sex', col='Embarked',\n            data=train_data, kind='point', ci=None, aspect=0.6)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"42c0dd8a837e1caf1670ab2296c53c0745d17d59"},"cell_type":"markdown","source":"Nothing special here. Women have much higher survival probability than men, but we see that survival rate differs from port to port."},{"metadata":{"_uuid":"b98cb92a5f62829854fe706f25b72968c6658d87"},"cell_type":"markdown","source":"From now on we can try to create new features. <br>\nLet's try and see what we can get."},{"metadata":{"trusted":true,"_uuid":"50f60b20f27db0bc6796c03aa6d0fd1a5896e493"},"cell_type":"code","source":"for dataset in train_test:\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"64640355f06318dee64514333057db3fe4a93fd1"},"cell_type":"code","source":"sns.pointplot(x='FamilySize',y='Survived', hue='Sex',\n              data=train_data, ci=None, scale=0.8)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0a656a30d16dcece7a2eb4921b6af1d1e8788dbd"},"cell_type":"markdown","source":"Looks like there is a relation between `FamilySize` and `Survived` for both men and women."},{"metadata":{"trusted":true,"_uuid":"248035e0dbcb13c5ce22d171c9a536a56afb9aff"},"cell_type":"code","source":"for dataset in train_test:\n    dataset['IsAlone'] = (dataset['FamilySize'] == 1).astype('int')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"add22f7d63bb28481c11b441c9d5f060d2e8e0e9"},"cell_type":"code","source":"sns.catplot(x='IsAlone', hue='Survived', col='Sex',\n            data=train_data, saturation=1,\n            kind='count', ci=None, aspect=.7)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"25d54e5c209356433cd31201bf532732e99f97c0"},"cell_type":"markdown","source":"As we can see alone women survived a little more, but for men the situation is opposite. "},{"metadata":{"trusted":true,"_uuid":"aaf885c3e527539945aede26e58a956bcee211e5"},"cell_type":"code","source":"for dataset in train_test:\n    dataset['Title'] = dataset['Name']\\\n                       .str.split(\",\", expand=True)[1]\\\n                       .str.split(\".\", expand=True)[0]\\\n                       .str.strip()\n    \n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n    \n    rare_count = 10\n    title_names = (dataset['Title'].value_counts() < rare_count)\n    dataset['Title'] = dataset['Title']\\\n                       .apply(lambda x: 'Rare' if title_names.loc[x] == True else x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"786f1625bb1ba6be89696aa12c703b26802fb56a"},"cell_type":"code","source":"sns.countplot(x='Title', hue='Survived',\n              data=train_data, saturation=1)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e72d75e136785505694ec923575a85430cba8e84"},"cell_type":"code","source":"for dataset in train_test:\n    dataset['FareBin'] = pd.qcut(dataset['Fare'], 4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"71591d7f8e30be4995b51b9ceabd041d51dff8da"},"cell_type":"code","source":"sns.catplot(x='FareBin', row='Sex', hue='Survived',\n            data=train_data, saturation=1, kind='count',\n            ci=None, aspect=1.5, height=4)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"af909e6438dc894a22e8dc02a0d6283871a742e2"},"cell_type":"code","source":"for dataset in train_test:\n    dataset['AgeBin'] = pd.cut(dataset['Age'].astype(int), 4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8e0531ac104f6b13f2616621b14d13dcba926b70"},"cell_type":"code","source":"sns.catplot(x='AgeBin', hue='Survived', row='Sex',\n            data=train_data, kind='count', saturation=1,\n            ci=None, aspect=1.5, height=4)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"764ded31a788605ae2ddf2e8b2e71d4b4729c6ff"},"cell_type":"markdown","source":"* The higher the `AgeBin` the lower are chances for men\n* The higher the `AgeBin` the higher are chances for women"},{"metadata":{"trusted":true,"_uuid":"7d2591e7c995aca2ec01fb60643a6e3c212616fa"},"cell_type":"code","source":"# get_deck = lambda x: (ord(x[0]) - ord('A') +1) if x[0] != 'T' else 1\n# train_data['Deck'] = train_data['Cabin'].map(get_deck, na_action='ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1998d5b0634869f37ea4b520c7f73065b8d17b37"},"cell_type":"code","source":"# # now let's fill missing with modes\n# modes = train_data.groupby(by='Pclass').agg({'Deck': pd.Series.mode}).values\n# pclass_deck_modes = dict(enumerate(modes[:, 0], 1))\n# train_data['Deck'].fillna(train_data['Pclass'].map(pclass_deck_modes, na_action=None),\n#                           inplace=True)\n# # # or you can fill all NaN with special value\n# # train_data['Deck'].fillna(0, inplace=True) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eb87448d64a431c6e62539d56186b116aa5769a8"},"cell_type":"code","source":"# sns.catplot(x='Sex', y='Survived', col='Deck',\n#             data=train_data.sort_values(by='Deck'), kind='bar',\n#             ci=None, aspect=.35)\n# plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1909ea07eac4f04d5e25ce2e0aeaee844af95b85"},"cell_type":"code","source":"# test_data['Deck'] = test_data['Cabin'].map(get_deck, na_action='ignore')\n# test_data['Deck'].fillna(test_data['Pclass'].map(pclass_deck_modes, na_action=None),\n#                           inplace=True)\n# # test_data['Deck'].fillna(0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a0bbbc69c82527e1a0eae5cbe1d38c108d9beeca"},"cell_type":"code","source":"for dataset in (train_test):\n    dataset['Name_Length'] = dataset['Name'].apply(len)\n    dataset['Has_Cabin'] = (~dataset['Cabin'].isnull()).astype('int')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"df22ac27a2ac1a65dda519221a328e62fa922ce5"},"cell_type":"code","source":"sns.catplot(x='Has_Cabin', y='Survived', hue='Sex',\n            data=train_data, saturation=1, kind='bar',\n            ci=None, aspect=1.5, height=4)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cf73b2d154decc9706701f4aadd474723c422f70"},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\nlabel = LabelEncoder()\nfor dataset in train_test:\n    dataset['AgeBin_Code'] = label.fit_transform(dataset['AgeBin'])\n    dataset['FareBin_Code'] = label.fit_transform(dataset['FareBin'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1b9ab8fec52abe660216a177e6d52c5f14b82b4b"},"cell_type":"code","source":"feats_to_exclude = ['Name', 'Age', 'Fare', 'Cabin', #'IsAlone', 'SibSp', 'Parch',\n                    'FareBin', 'AgeBin']\nX_train = train_data.drop(['Survived']+feats_to_exclude, axis=1)\nX_test = test_data.drop(feats_to_exclude, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"96f1778d6e1d50546cd8d365cea1b5b8bd5bdc4d"},"cell_type":"code","source":"X_train = pd.get_dummies(X_train)\nX_test = pd.get_dummies(X_test)\nX_train.head(4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"04cdc7ca747f8f9e1a1e118356a03e3ffad0edc6"},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\n\nfeatures_to_scale = ['Name_Length',#'Pclass', 'FamilySize', 'Parch', 'SibSp',\n                     'AgeBin_Code', 'FareBin_Code']\nfeats_scaler = MinMaxScaler()\nX_train[features_to_scale] = feats_scaler.fit_transform(X_train[features_to_scale])\nX_test[features_to_scale] = feats_scaler.transform(X_test[features_to_scale])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0e67ce034ab81ec51bf8f37efa94b1e464f6e3e1"},"cell_type":"code","source":"X_train.head(4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"97a827ccbc8ec797dfd4f868a1ac8d6c52b2c0f8"},"cell_type":"markdown","source":"## 3 - Selecting and tuning our classifier\nNow we can try different Classifiers. <br>"},{"metadata":{"trusted":true,"_uuid":"0a5c0d1a47415b71698a94d1f3e8e02c83f70b19"},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import (RandomForestClassifier, \n                              AdaBoostClassifier,\n                              ExtraTreesClassifier)\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"46e1254909749e1e25514d3072ef8e1aea5c4732"},"cell_type":"code","source":"def get_score(estimator, X, y):\n    scores = cross_val_score(estimator, X, y,\n                             cv=10, n_jobs=-1, scoring='accuracy')\n    return scores.mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"00c42bb57fb45bb2a4326c7f31829768cbbd956c"},"cell_type":"code","source":"clfs = [RandomForestClassifier(random_state=17), \n        ExtraTreesClassifier(random_state=17),\n#         DecisionTreeClassifier(random_state=17),\n        AdaBoostClassifier(random_state=17),\n        KNeighborsClassifier(),\n        SVC(random_state=17),\n        LogisticRegression(random_state=17),\n        GaussianNB()]\nclfs_names = ['RandomForestClassifier', \n              'ExtraTreesClassifier'\n#               'DecisionTreeClassifier',\n              'AdaBoostClassifier',\n              'KNeighborsClassifier',\n              'SVC',\n              'LogisticRegression',\n              'GaussianNB']\nclf_scores = dict(zip(clfs_names,(get_score(clf,\n                                            X_train,\n                                            train_target) for clf in clfs)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d7531eed44cbfb935646823d31cbaf6931dabecf"},"cell_type":"code","source":"pd.DataFrame.from_dict(clf_scores,\n                       orient='index',\n                       columns=['Score']).sort_values(by='Score', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"415c743dbe2735860ed91d1aea6cf5cc7f2b51eb"},"cell_type":"code","source":"nb = GaussianNB()\nnb.fit(X_train, train_target)\nnb_preds = nb.predict(X_test)\nwrite_submission_file(nb_preds, 'nb_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"70bd2651ad110c2d6faa83a06e356a812f670660"},"cell_type":"markdown","source":"Also i would like to use RandomForest for a while just to show the feature importances."},{"metadata":{"trusted":true,"_uuid":"fbb094d228700be8fb8d3f1da48d77ebadf4d225"},"cell_type":"code","source":"rf = RandomForestClassifier(random_state=17).fit(X_train, train_target)\nimportances = rf.feature_importances_\nsorted_idx = np.argsort(importances)\nplt.barh(X_train.columns[sorted_idx], importances[sorted_idx])\nplt.title('Feature importances: RandomForest')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"23055c9b26078118553832ffdcd2a677276f88e0"},"cell_type":"markdown","source":"Looks like some features are almost useless for RandomForest, but i will now drop them right now."},{"metadata":{"_uuid":"3015127eb17d0406fae8ff1fa5ef356ff47e80ea"},"cell_type":"markdown","source":"Why dont we just try to stack some models. For example let's get our tuned svc, random forest and logistic regression.<br>\nAnd then add their predictions to the data and train new classifier on this data.<br>"},{"metadata":{"trusted":true,"_uuid":"3275644e65b42517c2d2d1deb813e8b3e0e6f5b8"},"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold, GridSearchCV","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d3b1b8fed8c92b881641699082fe15168c76e4a5"},"cell_type":"code","source":"def get_stacking_preds(estimator, X_train, train_target,\n                       X_test, n_splits=10, random_state=17):\n    folds = StratifiedKFold(n_splits=n_splits, random_state=random_state)\n    train_pred = np.empty((0,1),float)\n    # for each fold\n    for train_indices,val_indices in folds.split(X_train, train_target):\n        # splitting the train set\n        x_train, x_val = X_train.iloc[train_indices], X_train.iloc[val_indices]\n        y_train, y_val = train_target.iloc[train_indices], train_target.iloc[val_indices]\n        # training the model on the training part\n        # yeah, like training part of the train set\n        estimator.fit(X=x_train, y=y_train)\n        # predict the validation set to avoid data leakage\n        train_pred = np.append(train_pred, estimator.predict(x_val))\n    # fit full train data\n    estimator.fit(X_train, train_target)\n    # and predict the test data\n    test_pred = estimator.predict(X_test)\n    return train_pred, test_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4d7b15cd9193266f7ad2f333f291453db6cd1cda"},"cell_type":"code","source":"%%time\nsvc = SVC(random_state=17)\nCs = np.linspace(0.01, 10, 15)\ngamma = ['auto', 'scale']+ list(np.logspace(-5, 1, num=15))\nkernel = ['linear', 'rbf', 'sigmoid']\nsvc_params = {'C': Cs, 'gamma' : gamma, 'kernel': kernel}\nsvc_grid = GridSearchCV(estimator=svc, param_grid=svc_params, cv=10,\n                        n_jobs=-1, scoring='accuracy', verbose=False)\nsvc_grid.fit(X_train, train_target)\n\nprint(svc_grid.best_score_, svc_grid.best_params_)\nbest_svc = svc_grid.best_estimator_\n\nbest_svc.fit(X_train, train_target)\nbest_svc_preds = best_svc.predict(X_test)\nwrite_submission_file(best_svc_preds, 'best_svc_preds_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"39402c494a50f324297b4eea75c7829cf730e2df"},"cell_type":"code","source":"%%time\nrf = RandomForestClassifier(random_state=17)\nrf_params = {'n_estimators': [100, 250, 500, 750], \n             'max_depth': [None, 3, 5, 8],\n             'min_samples_leaf': [1, 2, 5],\n             'min_samples_split': [2, 4, 10],\n             'max_features': [None, 'sqrt', 'log2', 0.6]}\nrf_grid = GridSearchCV(estimator=rf, param_grid=rf_params,\n                       cv=5, n_jobs=-1, verbose=False)\nrf_grid.fit(X_train, train_target)\n\nprint(rf_grid.best_score_, rf_grid.best_params_)\nbest_rf = rf_grid.best_estimator_\n\nbest_rf.fit(X_train, train_target)\nbest_rf_preds = best_rf.predict(X_test)\nwrite_submission_file(best_rf_preds, 'best_rf_preds_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0fb9969d9879111d0027d2536659d3e69eff65ca"},"cell_type":"code","source":"%%time\net = ExtraTreesClassifier(random_state=17)\net_params = rf_params\net_grid = GridSearchCV(estimator=et, param_grid=et_params,\n                       cv=5, n_jobs=-1, verbose=False)\net_grid.fit(X_train, train_target)\n\nprint(et_grid.best_score_, et_grid.best_params_)\nbest_et = et_grid.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5bb46d907c4cc07c54e1603e73721ab65ddfef4d"},"cell_type":"code","source":"%%time\nada = AdaBoostClassifier(random_state=17)\nada_params = {'n_estimators': [100, 200, 300, 400, 500, 600],\n              'learning_rate': [0.01, 0.025, 0.05, 0.075, 0.1, 0.5]}\nada_grid = GridSearchCV(estimator=ada, param_grid=ada_params,\n                        cv=5, n_jobs=-1, verbose=False)\nada_grid.fit(X_train, train_target)\n\nprint(ada_grid.best_score_, ada_grid.best_params_)\nbest_ada = ada_grid.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2cbdcd1e812db538423cfdb4364e1293d4dad489"},"cell_type":"code","source":"%%time\nknn = KNeighborsClassifier()\nparams = {'n_neighbors': range(1, 10),\n          'weights': ['uniform', 'distance'],\n          'p': [1, 2, 3]}\nknn_grid = GridSearchCV(estimator=knn,\n                        param_grid=params,\n                        cv=10, scoring='accuracy',\n                        verbose=False, n_jobs=-1)\nknn_grid.fit(X_train, train_target)\n\nprint(knn_grid.best_score_, knn_grid.best_params_)\nbest_knn = knn_grid.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"40cfd897f66a27846e10a8529bc263d62279495a"},"cell_type":"code","source":"%%time\nlr = LogisticRegression(random_state=17)\nlr_params = {'C': np.linspace(0.001, 10, 40),\n            'class_weight': [None, 'balanced'],\n            'penalty': ['l1', 'l2']}\nlr_grid = GridSearchCV(estimator=lr,\n                        param_grid=lr_params,\n                        cv=10, scoring='accuracy',\n                        verbose=False, n_jobs=-1)\nlr_grid.fit(X_train, train_target)\n\nprint(lr_grid.best_score_, lr_grid.best_params_)\nbest_lr = lr_grid.best_estimator_\nbest_lr.fit(X_train, train_target)\nbest_lr_preds = best_lr.predict(X_test)\nwrite_submission_file(best_lr_preds, 'best_lr_preds_submission.csv')\n\nbest_lr.fit(X_train, train_target)\nbest_lr_preds = best_lr.predict(X_test)\nwrite_submission_file(best_lr_preds, 'best_logistic_regression_preds_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8bb36c3dacdbfb4ebdb64ec631f8f1c73983f75e"},"cell_type":"code","source":"%%time\nclfs = [\n    best_svc,\n    best_rf, \n    best_et,\n    best_ada,\n    best_knn,\n    best_lr,\n    GaussianNB()\n]\n# seed = 17\n# clfs = [\n#     SVC(random_state=seed),\n#     RandomForestClassifier(random_state=seed), \n#     ExtraTreesClassifier(random_state=seed),\n#     AdaBoostClassifier(random_state=seed),\n#     KNeighborsClassifier(),\n#     LogisticRegression(random_state=seed),\n#     GaussianNB()\n# ]\nclfs_names = [\n    'SVC', \n    'RandomForest',\n    'ExtraTrees',\n    'AdaBoost',\n    'KNN',\n    'LogisticRegression',\n    'GaussianNB'\n    ]\nbase_models_train_preds = pd.DataFrame()\nbase_models_test_preds = pd.DataFrame()\nfor clf, name in zip(clfs, clfs_names):\n    clf_train_preds, clf_test_preds = get_stacking_preds(clf, X_train,\n                                                         train_target, X_test)\n    base_models_train_preds[name+'_pred'] = clf_train_preds\n    base_models_test_preds[name+'_pred'] = clf_test_preds    \n# X_train_stack = pd.concat([X_train, base_models_train_preds], axis=1)\n# X_test_stack = pd.concat([X_train, base_models_test_preds], axis=1)\nX_train_stack = base_models_train_preds\nX_test_stack = base_models_test_preds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"20da943dbfa1be0a89b4cffab88842e105300ee0"},"cell_type":"code","source":"# import xgboost as xgb\n# aggregator = xgb.XGBClassifier(\n#     learning_rate = 0.02,\n#     n_estimators= 1000,\n#     max_depth= 3,\n#     min_child_weight= 2,\n#     gamma=0.8,\n#     subsample=1,\n#     colsample_bytree=0.7,\n#     objective= 'binary:logistic',\n#     n_jobs=-1,\n#     scale_pos_weight=1\n# )\n# aggregator = SVC()\n# aggregator = RandomForestClassifier()\naggregator = LogisticRegression()\nprint(f'Score: {get_score(aggregator, X_train_stack, train_target)*100:.2f}%')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"07114c034ad6bd4ca2331f99f73358e4976b95ea"},"cell_type":"code","source":"aggregator.fit(X_train_stack, train_target)\nstacked_preds = aggregator.predict(X_test_stack)\nwrite_submission_file(stacked_preds, 'stacked_untuned_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ed4cf1c7841d1b525a7d37f83944feccf50fcc81"},"cell_type":"code","source":"%%time\nlr_agg_grid = GridSearchCV(estimator=aggregator,\n                        param_grid=lr_params,\n                        cv=10, scoring='accuracy',\n                        verbose=False, n_jobs=-1)\nlr_agg_grid.fit(X_train_stack, train_target)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"598329ff2dc2ddfafa8578465d39085324fbe9f9"},"cell_type":"code","source":"# %%time\n# rf_agg_grid = GridSearchCV(estimator=aggregator,\n#                         param_grid=rf_params,\n#                         cv=10, scoring='accuracy',\n#                         verbose=False, n_jobs=-1)\n# rf_agg_grid.fit(X_train_stack, train_target)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9d8a69d4435885cbe6844f4251aeaef761999ff2"},"cell_type":"code","source":"# %%time\n# svc_agg_grid = GridSearchCV(estimator=aggregator,\n#                         param_grid=svc_params,\n#                         cv=10, scoring='accuracy',\n#                         verbose=False, n_jobs=-1)\n# svc_agg_grid.fit(X_train_stack, train_target)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a173ba6a72b07d91ce116c5e8c38a0291cfc3b59"},"cell_type":"code","source":"# aggregator = svc_agg_grid.best_estimator_\n# aggregator = rf_agg_grid.best_estimator_\naggregator = lr_agg_grid.best_estimator_\nprint(lr_agg_grid.best_params_)\nprint(f'Score: {get_score(aggregator, X_train_stack, train_target)*100:.2f}%')\naggregator.fit(X_train_stack, train_target)\nstacked_preds = aggregator.predict(X_test_stack)\nwrite_submission_file(stacked_preds, 'stacked_tuned_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8beb259cb105aa952b357513f773d84ace553d32"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}