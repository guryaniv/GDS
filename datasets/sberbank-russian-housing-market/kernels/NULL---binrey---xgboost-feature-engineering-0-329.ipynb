{"cells":[{"metadata":{"_uuid":"05c6575725630edfd58e410423363bda5296eb93"},"cell_type":"markdown","source":"# SBERBANK HOUSING MARKET"},{"metadata":{"_uuid":"b7c37e63aa811b59ca79c65fc401c9aee6820ec6"},"cell_type":"markdown","source":"## 1. Prepair DATA"},{"metadata":{"trusted":true,"_uuid":"425dd0005844061519e0f4d70a8c58d893fbc4f0"},"cell_type":"code","source":"# function for finding categorical features\n\ndef categ_props(data):\n    columns_to_del = []\n    for c in data.columns:\n        try:\n            float(data[c].values[1])\n        except:\n            columns_to_del.append(c)\n    return columns_to_del","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4f1b6594df017a2ef084ae75d8775e73b62c9ec4"},"cell_type":"code","source":"from feature_selector import FeatureSelector\nfrom matplotlib import pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n%pylab inline","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"2e598bdc4c992bdf05b18d0acc9fb262f0f7f7ae"},"cell_type":"code","source":"#import DATA\n\nx = pd.read_csv('../input/train.csv')\nx.timestamp = pd.to_datetime(x.timestamp)\n\nx_test = pd.read_csv('../input/test.csv')\nx_test.timestamp = pd.to_datetime(x_test.timestamp)\n\nx_macro = pd.read_csv('../input/macro.csv')\nx_macro.timestamp = pd.to_datetime(x_macro.timestamp)\n\nprint('x:{0} x_test:{1} x_macro:{2}'.format(x.shape, x_test.shape, x_macro.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b809c48ae6103ddb90db972a852f8380c706c4b0"},"cell_type":"code","source":"# Concatenate train + test + macro\n\ntest_ID = x_test.id\ny_all = np.log1p(x[\"price_doc\"])\nx.price_doc = y_all\n\nx_length = x.shape[0]\nx_all = pd.concat([x, x_test])\nx_all = pd.merge_ordered(x_all, x_macro, on='timestamp', how='left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8878614d6bbd894f7dbd65e3248d67656121bfc4"},"cell_type":"code","source":"# Check for features with too math missing data\n\nfs = FeatureSelector(x_all, x_all.columns)\nfs.identify_missing(0.7)\nfs.missing_stats.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e45f0d7eed153c6b9644672243d7ad3048bd6407"},"cell_type":"code","source":"# Check for features with only one value\n\nfs.identify_single_unique()\nfs.unique_stats.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"42646a5fd9f8cae9f7f9e280f54d1bb0f860b580"},"cell_type":"code","source":"# Remove bad features\n\nx_all.drop(['id',                                 # unnesesary\n            'provision_retail_space_modern_sqm',  # too match missing values\n            'provision_retail_space_sqm',         # too match missing values\n            'child_on_acc_pre_school',            # bad values \n            'modern_education_share',             # bad values\n            'old_education_build_share'           # bad values\n           ], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"380e5ff812134f896c141b1d83837f024682a50e"},"cell_type":"code","source":"# Process categorical features\n\ncat_props = categ_props(x_all)\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\n\nx_all.product_type.fillna('noType', inplace=True)\n\nfor cat_col in cat_props:\n    print(cat_col)\n    x_all[cat_col]=le.fit_transform(x_all[cat_col]) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"069b6584ef4fc155e026e527965c0aaab4fdb105"},"cell_type":"code","source":"# Create several time-based features\n\nx_all['d'] = x.timestamp.dt.year+x.timestamp.dt.dayofyear/365\nx_all['w'] = x.timestamp.dt.year+x.timestamp.dt.weekofyear/48\nx_all['m'] = x.timestamp.dt.year+x.timestamp.dt.month/12\n\nx_all['wofy'] = x.timestamp.dt.weekofyear\nx_all['mofy'] = x.timestamp.dt.month\nx_all['dofw'] = x.timestamp.dt.dayofweek\n\n# Create relative features\nx_all['rel_kitch_sq'] = x_all['kitch_sq'] / x_all['full_sq'].astype(float)\nx_all['rel_life_sq'] = x_all['life_sq'] / x_all['full_sq'].astype(float)\nx_all.rel_kitch_sq[x_all.rel_kitch_sq>1]=1\nx_all.rel_life_sq[x_all.rel_life_sq>1]=1\n\nx_all['rel_floor'] = x_all['floor'] / x_all['max_floor'].astype(float)\nx_all.rel_floor[x_all.rel_floor>1]=1\n\nx_all['brent_rub'] = x_all['brent'] * x_all['usdrub'].astype(float)\n\nx_all.drop(['timestamp'], axis=1, inplace=True)    \n    \nprint(x_all.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bfe91e51b5a58130e8f4d43ed2abf0c943ae854c"},"cell_type":"code","source":"# Create a validation set, with last 20% of data\n\nx_all.replace([np.inf, -np.inf], np.nan)\nx_all.fillna(0, inplace=True)\n\nnum_val = int(x_length * 0.2)\n\nx_train_all = x_all[:x_length]\nx_train = x_all[:x_length-num_val]\nx_val = x_all[x_length-num_val:x_length]\ny_train = y_all[:x_length-num_val]\ny_val = y_all[x_length-num_val:x_length]\n\nx_test = x_all[x_length:]\n\nprint('x_train:{0} x_val:{1} x_test:{2}'.format(x_train.shape, x_val.shape, x_test.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ecc1b363c20cabe12fb0b2232753b165e74f67ef"},"cell_type":"code","source":"# Time averaged features\n\npmy = x_train.groupby('mofy')['price_doc'].aggregate(np.mean)\nx_train['p_mofy'] = x_train.mofy.map(pmy)\nx_val['p_mofy'] = x_val.mofy.map(pmy)\nx_test['p_mofy'] = x_test.mofy.map(pmy)\n\npwy = x_train.groupby('wofy')['price_doc'].aggregate(np.mean)\nx_train['p_wofy'] = x_train.wofy.map(pwy)\nx_val['p_wofy'] = x_val.wofy.map(pwy)\nx_test['p_wofy'] = x_test.wofy.map(pwy)\n\npbw = x_train.groupby('dofw')['price_doc'].aggregate(np.mean)\nx_train['p_pdw'] = x_train.dofw.map(pbw)\nx_val['p_pdw'] = x_val.dofw.map(pbw)\nx_test['p_pdw'] = x_test.dofw.map(pbw)\n\nfig, ax = plt.subplots(1, 1, figsize=(17, 5), sharey=True)\nplt.subplot(131)\nplt.xlabel('month')\nplt.ylabel('average price')\nplt.plot(pmy, '.-');\n\nplt.subplot(132)\nplt.xlabel('week of year')\nplt.plot(pwy, '.-');\n\nplt.subplot(133)\nplt.xlabel('day of week')\nplt.plot(pbw, '.-');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3ad35a35fad5bc765565e224c7fc3cd930a9ffb0"},"cell_type":"code","source":"# Distribution of prices\n# There are two points with unrealistic buil year and significant amount of zero values\n\ni_del = x_train[x_train.build_year>2018].index\nfig, ax = plt.subplots(1, 3, figsize=(17, 3))\n\nplt.subplot(131)\nplt.title('train data')\nplt.xlabel('build year')\nplt.ylabel('amount of points')\nplt.hist(x_train.drop(i_del).build_year, bins=100);\nplt.subplot(132)\nplt.title('validate data')\nplt.xlabel('build year')\nplt.hist(x_val.build_year, bins=100);\nplt.subplot(133)\nplt.title('test data')\nplt.xlabel('build year')\nplt.hist(x_test.build_year, bins=100);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8ef8cf71bf634ed7cbd7fd8f743726b316bf093f"},"cell_type":"code","source":"# Distribution of prices (for build year > 1900)\n\nfig, ax = plt.subplots(1, 3, figsize=(17, 3))\nplt.subplot(131)\nplt.title('train data')\nplt.xlabel('build year')\nplt.ylabel('amount of points')\nplt.hist(x_train.drop(i_del).build_year[x_train.build_year>1900], bins=100);\nplt.subplot(132)\nplt.title('validate data')\nplt.xlabel('build year')\nplt.hist(x_val.build_year[x_val.build_year>1900], bins=100);\nplt.subplot(133)\nplt.title('test data')\nplt.xlabel('build year')\nplt.hist(x_test.build_year[x_test.build_year>1900], bins=100);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eaf4d6178fa7550e6a8d38c35b8bb3c45bf4a119"},"cell_type":"code","source":"buid_y_means = x_train.drop(i_del).groupby('build_year')['price_doc'].aggregate(np.mean)\nfig, ax = plt.subplots(1, 1, figsize=(17, 6))\nplt.xlabel('build year')\nplt.ylabel('average price')\nplt.xlim([1850,2018])\nplt.bar(x = buid_y_means.index, height = buid_y_means-13, bottom = 13);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"04e466564040c5f070b5c1c3d469758383ffb37d"},"cell_type":"code","source":"# Average price - time dependence. There is notisable linear correlation\n# Unfortunately this approximation can't improve regression task significantly\n\npm_train = x_train.groupby('m')['price_doc'].aggregate(np.mean)\npm_val = x_val.groupby('m')['price_doc'].aggregate(np.mean)\np = np.polyfit(pm_train.index, pm_train.values, 1)\n\nfig, ax = plt.subplots(1, 1, figsize=(17, 6))\nplt.xlabel('time')\nplt.ylabel('average price')\nplt.plot(x_train.groupby('m')['price_doc'].aggregate(np.mean));\nplt.plot(x_val.groupby('m')['price_doc'].aggregate(np.mean));\nplt.plot(pm_train.index, np.polyval(p, pm_train.index));\nplt.plot(pm_val.index, np.polyval(p, pm_val.index),'--');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a049154af81918be44466b4895b5e34b19ed80ac"},"cell_type":"code","source":"# Delete target\nx_train.drop(\"price_doc\", axis = 1, inplace = True)\nx_val.drop(\"price_doc\", axis = 1, inplace = True)\nx_test.drop(\"price_doc\", axis = 1, inplace = True)\nprint('x_train:{0} x_val:{1} x_test:{2}'.format(x_train.shape, x_val.shape, x_test.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"e4afbe428ff6e2f0b54e137bccaaeec78a10cb78"},"cell_type":"code","source":"# Find collinear features (correlation > 0.99).\n# There are many collinear features in particular dataset such metro_km_avto and metro_min_avto. \nfs = FeatureSelector(x_train, y_train)\nfs.identify_collinear(0.99)\nprint(fs.ops['collinear'][:20])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"850b27c9800e4c74f3cd911e3ec724ca4ec795b9"},"cell_type":"code","source":"# Identify features with zero importance\nfs.identify_zero_importance(task='regression', eval_metric='rmse', n_iterations=5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"406b681d5f944c156d76b8d0e1cf76f0bbfb54d7"},"cell_type":"code","source":"# Identify features with low importance\n\nfs.identify_low_importance(cumulative_importance=0.99)\nfs.plot_feature_importances(plot_n=20, threshold=0.99)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bef5b070ccf7c87b07f45053e6d43c1612c02815"},"cell_type":"code","source":"# Drop selected features\n\nx_train = fs.remove('all')\nx_val.drop(fs.removed_features, axis=1, inplace=True)\nx_test.drop(fs.removed_features, axis=1, inplace=True)\n\nprint('x_train:{0} x_val:{1} x_test:{2}'.format(x_train.shape, x_val.shape, x_test.shape))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"12964d7577cddaf4e89d96d41adf30f70e6b62c8"},"cell_type":"markdown","source":"## 2. Train the model"},{"metadata":{"trusted":true,"_uuid":"ec26cf78688b9634595ea30065230b6bd2e6bb53"},"cell_type":"code","source":"# Train XGBoost model and validate results\n\nimport xgboost as xgb\nfrom sklearn import metrics\nclf = xgb.XGBRegressor(n_estimators=150, max_depth=5, learning_rate=0.1, min_child_weight=20)\nclf.fit(x_train, y_train)\n\nprint(metrics.mean_squared_error(y_val, clf.predict(x_val))**0.5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"de819acd01041446d2fb13c9d149346b965dfe83"},"cell_type":"code","source":"# Plot importances of XGBoost model\n# Some of created features can be noticed in top 50 important features!\nfig, ax = plt.subplots(1, 1, figsize=(8, 16))\nxgb.plot_importance(clf, max_num_features=50, height=0.5, ax=ax);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"48e5c2abcba3ac3ee32ca3544cfb41857e8f18be"},"cell_type":"code","source":"# Plot true values vs preicted ones\n\nplt.scatter(y_train, clf.predict(x_train), alpha=0.3, c='red')\nplt.scatter(y_val, clf.predict(x_val), alpha=0.3, c='blue');\nplt.xlabel('true values')\nplt.ylabel('predicted values')\nplt.axis([13,19,13,19])\nplt.plot([13,19],[13,19]);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ae93b0def756a1558f9a2df9fc396285fa6f839d"},"cell_type":"markdown","source":"## 3. Create submission"},{"metadata":{"trusted":true,"_uuid":"6e5a3532f202edd65e2e39e09e68ca777e168bf7"},"cell_type":"code","source":"# Train model on all data\nclf.fit(pd.concat([x_train, x_val]), y_all)\ny_sub = clf.predict(x_test)\nsubmit_df = pd.DataFrame({'id':test_ID, 'price_doc':np.expm1(y_sub)})\nsubmit_df.to_csv('submit_data.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bb05d3ebd9646fd7a45a6d568c448145b3a1eeae"},"cell_type":"markdown","source":"### Validation score 0.329"},{"metadata":{"_uuid":"55a0d0078c28ef00c35947a71cb055c402957592"},"cell_type":"markdown","source":"## 4. Final conclusions"},{"metadata":{"_uuid":"aabd1c91d0e8dac53e06aa1b5a90707ec3767d09"},"cell_type":"markdown","source":"This score is not my best result on this data set. With almost not feature engeneering (even without macro.csv) the result was 0.322! So the conclusion is that data preparation described above is not efficient."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"}},"nbformat":4,"nbformat_minor":1}