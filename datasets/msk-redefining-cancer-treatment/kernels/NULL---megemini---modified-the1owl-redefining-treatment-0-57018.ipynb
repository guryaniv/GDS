{"metadata": {"language_info": {"version": "3.6.1", "nbconvert_exporter": "python", "name": "python", "file_extension": ".py", "mimetype": "text/x-python", "pygments_lexer": "ipython3", "codemirror_mode": {"version": 3, "name": "ipython"}}, "kernelspec": {"language": "python", "display_name": "Python 3", "name": "python3"}}, "cells": [{"cell_type": "markdown", "metadata": {"_cell_guid": "a3076f31-7052-4e2c-ac9e-f6fd93d18a86", "_uuid": "0d77c2144b6b5523aeb22d0ea999b216ebbe56fa"}, "source": "# Donated to Cancer Treatment Too", "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"_cell_guid": "217a645a-29bc-4f2d-8795-24fb4f55d94d", "_uuid": "8abb336a1fb5b8c4e23b41efdaa9207c95b5974e"}, "source": "* Note: As this is my first public Kernel and I'm still in learning NLP, please feel free to comment if you have any questions.\n* This Kernel is modified from [the1owl: Redefining Treatment](https://www.kaggle.com/the1owl/redefining-treatment-0-57456), really thanks!\n* I will highlight the main differences from the original.\n* Finnally, donated to cancer treatment too.", "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"_cell_guid": "56f93ab8-fa0f-4bf4-9b30-625c2eda8d15", "_uuid": "8c73ab9999e9d01e83d3a986e7bd1a13f2b800c2", "trusted": false}, "source": "from sklearn import preprocessing, pipeline, feature_extraction, decomposition, model_selection, metrics, cross_validation, svm\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, BaggingClassifier\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.feature_selection import SelectKBest, chi2\nfrom sklearn.preprocessing import normalize, Imputer\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.naive_bayes import MultinomialNB\n\nimport sklearn\nimport pandas as pd\nimport numpy as np\nimport xgboost as xgb\n\nimport datetime", "outputs": [], "execution_count": 1}, {"cell_type": "code", "metadata": {"_cell_guid": "c2340896-53c5-467b-8087-1c1136cf87a8", "_uuid": "6c717a3ad79b2144092286757adeff5dd6f6945e", "scrolled": false, "trusted": false}, "source": "train = pd.read_csv('../input/training_variants')\ntest = pd.read_csv('../input/test_variants')\ntrainx = pd.read_csv('../input/training_text', sep=\"\\|\\|\", engine='python', header=None, skiprows=1, names=[\"ID\",\"Text\"])\ntestx = pd.read_csv('../input/test_text', sep=\"\\|\\|\", engine='python', header=None, skiprows=1, names=[\"ID\",\"Text\"])\n\ntrain = pd.merge(train, trainx, how='left', on='ID').fillna('')\ny = train['Class'].values\ntrain = train.drop(['Class'], axis=1)\n\ntest = pd.merge(test, testx, how='left', on='ID').fillna('')\npid = test['ID'].values", "outputs": [], "execution_count": 2}, {"cell_type": "code", "metadata": {"_cell_guid": "a57c12f5-73c2-415a-9cd0-3adaed0ddd2c", "_uuid": "22e5d25aa11c98d6f913d975cd51d436f8b64351", "scrolled": true, "trusted": false}, "source": "train.head()", "outputs": [], "execution_count": 3}, {"cell_type": "code", "metadata": {"_cell_guid": "fd7f7d96-f093-4321-8b60-e734c9d1794c", "_uuid": "145564b8cf0753323450a93fd5202c458c49d1b6", "trusted": false}, "source": "y", "outputs": [], "execution_count": 4}, {"cell_type": "code", "metadata": {"_cell_guid": "e24024d6-c577-4099-8c4b-ed728bb3da82", "_uuid": "42a876f902870ca8f1de3d0863dbc33921fac24b", "trusted": false}, "source": "test.head()", "outputs": [], "execution_count": 5}, {"cell_type": "code", "metadata": {"_cell_guid": "391d7e9a-96dd-454b-924f-0876bf6cefee", "_uuid": "17c2a7ed868ae06ac1e15f8c90e19e4b17d321ad", "trusted": false}, "source": "pid", "outputs": [], "execution_count": 6}, {"cell_type": "markdown", "metadata": {"_cell_guid": "1edd1f68-0353-4f92-a739-db42d2adf265", "_uuid": "4a26635cd88839c392aa1019e60b42b369ab3bd2"}, "source": "### 1. Not use the codes below.\n***\nNot used in this Kernel.\n\n```python\n# commented for Kaggle Limits\nfor i in range(56):\n    df_all['Gene_'+str(i)] = df_all['Gene'].map(lambda x: str(x[i]) if len(x)>i else '')\n    df_all['Variation'+str(i)] = df_all['Variation'].map(lambda x: str(x[i]) if len(x)>i else '')\n```", "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"_cell_guid": "90833673-de06-422c-8df2-13ffe5911a5c", "_uuid": "86a1fbd09654d814e68622b05a327843290c7304", "collapsed": true, "scrolled": true, "trusted": false}, "source": "df_all = pd.concat((train, test), axis=0, ignore_index=True)\ndf_all['Gene_Share'] = df_all.apply(lambda r: sum([1 for w in r['Gene'].split(' ') if w in r['Text'].split(' ')]), axis=1)\ndf_all['Variation_Share'] = df_all.apply(lambda r: sum([1 for w in r['Variation'].split(' ') if w in r['Text'].split(' ')]), axis=1)", "outputs": [], "execution_count": 7}, {"cell_type": "code", "metadata": {"_cell_guid": "9527f162-c93d-433d-a834-9c0215c45069", "_uuid": "4182af290004b8736d345f0e1a64f3e8969daf5d", "scrolled": false, "trusted": false}, "source": "df_all.head()", "outputs": [], "execution_count": 8}, {"cell_type": "code", "metadata": {"_cell_guid": "b923b200-2d21-4166-afc9-0f39f167d86e", "_uuid": "158f5e7bfe484e1f006aefdfe681e4f2ac8db6cf", "trusted": false}, "source": "gen_var_lst = sorted(list(train.Gene.unique()) + list(train.Variation.unique()))\nprint(len(gen_var_lst))", "outputs": [], "execution_count": 9}, {"cell_type": "code", "metadata": {"_cell_guid": "0dc59167-7e16-43ac-bdc4-d0a48dcbd296", "_uuid": "1a42b0fc3a8b361064c8f73fdfe5ed6525f85eba", "scrolled": true, "trusted": false}, "source": "gen_var_lst = [x for x in gen_var_lst if len(x.split(' '))==1]\nprint(len(gen_var_lst))\ni_ = 0\n\n#commented for Kaggle Limits\n# for gen_var_lst_itm in gen_var_lst:\n#     if i_ % 100 == 0: print(i_)\n#     df_all['GV_'+str(gen_var_lst_itm)] = df_all['Text'].map(lambda x: str(x).count(str(gen_var_lst_itm)))\n#     i_ += 1", "outputs": [], "execution_count": 10}, {"cell_type": "code", "metadata": {"_cell_guid": "38cca869-3d4d-44f6-8c55-b78bb2840609", "_uuid": "199df6688fc7654251228cd85bb5dcd87fd5ebda", "collapsed": true, "trusted": false}, "source": "for c in df_all.columns:\n    if df_all[c].dtype == 'object':\n        if c in ['Gene','Variation']:\n            lbl = preprocessing.LabelEncoder()\n            df_all[c+'_lbl_enc'] = lbl.fit_transform(df_all[c].values)  \n            df_all[c+'_len'] = df_all[c].map(lambda x: len(str(x)))\n            df_all[c+'_words'] = df_all[c].map(lambda x: len(str(x).split(' ')))\n        elif c != 'Text':\n            lbl = preprocessing.LabelEncoder()\n            df_all[c] = lbl.fit_transform(df_all[c].values)\n        if c=='Text': \n            df_all[c+'_len'] = df_all[c].map(lambda x: len(str(x)))\n            df_all[c+'_words'] = df_all[c].map(lambda x: len(str(x).split(' '))) \n\ntrain = df_all.iloc[:len(train)]\ntest = df_all.iloc[len(train):]", "outputs": [], "execution_count": 11}, {"cell_type": "code", "metadata": {"_cell_guid": "ac511901-464b-4d08-aeba-1be18edb9661", "_uuid": "570ce7b3ab8b9f46c51843ea9d5c103c24dbb06e", "scrolled": true, "trusted": false}, "source": "train.head()", "outputs": [], "execution_count": 12}, {"cell_type": "code", "metadata": {"_cell_guid": "3b870664-6bda-49f9-8f1d-9fc85072ef3d", "_uuid": "3050f029fe7406337d4946b834c05124eaff2b76", "scrolled": true, "trusted": false}, "source": "test.head()", "outputs": [], "execution_count": 13}, {"cell_type": "code", "metadata": {"_cell_guid": "03a1bc89-a8dd-4e0b-9fb2-e4ddc8286fa6", "_uuid": "a6406d11c0298511b4107abbbe0fa947de522791", "trusted": false}, "source": "train.shape", "outputs": [], "execution_count": 14}, {"cell_type": "code", "metadata": {"_cell_guid": "bba97217-b7ee-4747-b1bc-71e68d2d57fa", "_uuid": "2d13c3b056df19eef7fac1416a9b27c01f2723fe", "trusted": false}, "source": "test.shape", "outputs": [], "execution_count": 15}, {"cell_type": "code", "metadata": {"_cell_guid": "ae2a7760-2706-4342-a0f5-03212584b9ae", "_uuid": "7e2a2ec9fd7908f36000470699c19b4425fc6181", "collapsed": true, "trusted": false}, "source": "class cust_regression_vals(sklearn.base.BaseEstimator, sklearn.base.TransformerMixin):\n    def fit(self, x, y=None):\n        return self\n    def transform(self, x):\n        x = x.drop(['Gene', 'Variation','ID','Text'],axis=1).values\n        return x\n\nclass cust_txt_col(sklearn.base.BaseEstimator, sklearn.base.TransformerMixin):\n    def __init__(self, key):\n        self.key = key\n    def fit(self, x, y=None):\n        return self\n    def transform(self, x):\n        return x[self.key].apply(str)", "outputs": [], "execution_count": null}, {"cell_type": "markdown", "metadata": {"_cell_guid": "36663a09-da14-4df0-87ec-9fe8771abf6c", "_uuid": "9b62f6d90b60d79c974b2f61a5d374e3412de7f2"}, "source": "### 2. Main difference\n***\n#### 1. Pipeline Changed\n\nThe original Kernel uses the pipeline with these codes below for 'Text' feature extraction:\n```python\n#commented for Kaggle Limits\n('pi3', pipeline.Pipeline([('Text', cust_txt_col('Text')), \n                           ('tfidf_Text', feature_extraction.text.TfidfVectorizer(ngram_range=(1, 2))), \n                           ('tsvd3', decomposition.TruncatedSVD(n_components=50, n_iter=25, random_state=12))]))\n```\nUnfortunately, it can not fit my memory of 8GB + 2GB(swap). And without these features, I can only get nearly 0.7xxx on PL.\n\nSo, I try to use **HashingVectorizer + TfidfTransformer** instead of **TfidfVectorizer**. [Reference](http://scikit-learn.org/stable/modules/feature_extraction.html#vectorizing-a-large-text-corpus-with-the-hashing-trick)\n\n#### 2. Parameter Tuning\n\nFor HashingVectorizer saved my memory, I try to use **ngram_range=(1, 3)** with HashingVectorizer. \n\nAnd **n_components=300** with **TruncatedSVD**.\n\n#### 3. Batch Transform\n\nWith these codes, I can fit_transform the **train**, but still out of memory if transform **test**.\n\nSo, I try to use batch transform of test data step by step, and vstack all. \n\nAnd, it works!", "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"_cell_guid": "6f895147-c745-4792-9a43-f6bd6ff7822c", "_uuid": "5fcca871cc477a821fa964d87f46a314b168dfa5", "scrolled": true, "trusted": false}, "source": "print('Pipeline...')\nfp = pipeline.Pipeline([\n    ('union', pipeline.FeatureUnion(\n        n_jobs = -1,\n        transformer_list = [\n            ('standard', cust_regression_vals()),\n            ('pi1', pipeline.Pipeline([('Gene', cust_txt_col('Gene')), \n                                       ('count_Gene', feature_extraction.text.CountVectorizer(analyzer=u'char', ngram_range=(1, 8))), \n                                       ('tsvd1', decomposition.TruncatedSVD(n_components=20, n_iter=25, random_state=12))])),\n            ('pi2', pipeline.Pipeline([('Variation', cust_txt_col('Variation')), \n                                       ('count_Variation', feature_extraction.text.CountVectorizer(analyzer=u'char', ngram_range=(1, 8))), \n                                       ('tsvd2', decomposition.TruncatedSVD(n_components=20, n_iter=25, random_state=12))])),\n            #commented for Kaggle Limits\n#             ('pi3', pipeline.Pipeline([('Text', cust_txt_col('Text')), \n#                                        ('hv', feature_extraction.text.HashingVectorizer(decode_error='ignore', n_features=2 ** 16, non_negative=True, ngram_range=(1, 3))),\n#                                        ('tfidf_Text', feature_extraction.text.TfidfTransformer()), \n#                                        ('tsvd3', decomposition.TruncatedSVD(n_components=300, n_iter=25, random_state=12))]))\n\n        \n        ])\n    )])\n\n\ntrain = fp.fit_transform(train)\nprint (train.shape)\n\ntest_t = np.empty([0, train.shape[1]])\nstep = 200\nfor i in range(0, len(test), step):\n    step_end = i+step\n    step_end = step_end if step_end < len(test) else len(test)\n    _test = fp.transform(test.iloc[i:step_end])\n    test_t = np.vstack((test_t, _test))\ntest = test_t\nprint (test.shape)", "outputs": [], "execution_count": null}, {"cell_type": "markdown", "metadata": {"_cell_guid": "d373d666-f0a9-4f84-aa34-26c5674f4736", "_uuid": "c85df64546e595beb036aa357b6135a063fc3d0b"}, "source": "### 3. Xgboost Parameter Tuning\n***\n#### 1. eta 0.03333 -> 0.02 \n\nI like small learning rate.\n\n#### 2. max_depth 4 -> 6 \n\nBigger means higher risk of overfitting. But, since we got more features, maybe it could be better for this big data?! I'm not sure yet.\n\n#### 3. test_size 0.18 -> 0.15\n\nWith slightly more data to train.", "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"_cell_guid": "7af95c64-27c7-4c86-9a72-775af3c3a9a2", "_uuid": "2d1c0b506fb71abf8b6ac60f5f51d91504db209f", "collapsed": true, "trusted": false}, "source": "y = y - 1 #fix for zero bound array", "outputs": [], "execution_count": null}, {"cell_type": "code", "metadata": {"_cell_guid": "2ba02c19-f3f5-4502-b976-3768453a2c1a", "_uuid": "4e5ae29c6ac788f0d4d157d8ffde170e58023a82", "scrolled": true, "trusted": false}, "source": "file_pre = datetime.datetime.now().strftime('%m_%d_%H_%M_%S')\n\ndenom = 0\nfold = 1 #Change to 5, 1 for Kaggle Limits\nfor i in range(fold):\n    params = {\n#         'eta': 0.03333,\n        'eta': 0.02,\n#         'max_depth': 4,\n        'max_depth': 6,\n        'objective': 'multi:softprob',\n        'eval_metric': 'mlogloss',\n        'num_class': 9,\n        'seed': i,\n        'silent': True\n    }\n    x1, x2, y1, y2 = model_selection.train_test_split(train, y, test_size=0.15, random_state=i)\n    watchlist = [(xgb.DMatrix(x1, y1), 'train'), (xgb.DMatrix(x2, y2), 'valid')]\n    model = xgb.train(params, xgb.DMatrix(x1, y1), 1000,  watchlist, verbose_eval=50, early_stopping_rounds=100)\n    score1 = metrics.log_loss(y2, model.predict(xgb.DMatrix(x2), ntree_limit=model.best_ntree_limit), labels = list(range(9)))\n    print(score1)\n    #if score < 0.9:\n    if denom != 0:\n        pred = model.predict(xgb.DMatrix(test), ntree_limit=model.best_ntree_limit+80)\n        preds += pred\n    else:\n        pred = model.predict(xgb.DMatrix(test), ntree_limit=model.best_ntree_limit+80)\n        preds = pred.copy()\n    denom += 1\n#     submission = pd.DataFrame(pred, columns=['class'+str(c+1) for c in range(9)])\n#     submission['ID'] = pid\n#     submission.to_csv('./result/submission_xgb_fold_'  + str(i) + '_' + file_pre + '.csv', index=False)\npreds /= denom\nsubmission = pd.DataFrame(preds, columns=['class'+str(c+1) for c in range(9)])\nsubmission['ID'] = pid\nsubmission.to_csv('./result/submission_xgb_' + file_pre + '.csv', index=False)\n", "outputs": [], "execution_count": null}, {"cell_type": "code", "metadata": {"_cell_guid": "e68d420c-618b-4936-9528-e21d9d993472", "_uuid": "6c53a32d9150a8b3e531f978860eb59c54e3ea44", "trusted": false}, "source": "import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nplt.rcParams['figure.figsize'] = (7.0, 7.0)\nxgb.plot_importance(booster=model,); plt.show()", "outputs": [], "execution_count": null}, {"cell_type": "markdown", "metadata": {"_cell_guid": "8c9d124c-2c43-433a-9876-9e9d4bd593b6", "_uuid": "44fb8218b1775155c09a92ec9fbd3dd9e4fd751d"}, "source": "### References\n* [Redefining Treatment](https://www.kaggle.com/the1owl/redefining-treatment-0-57456)\n* [Vectorizing a large text corpus with the hashing trick](http://scikit-learn.org/stable/modules/feature_extraction.html#vectorizing-a-large-text-corpus-with-the-hashing-trick)\n* [HashingVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.HashingVectorizer.html)", "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"_cell_guid": "df9a7b7c-0f33-41c1-b188-92ec405ef626", "_uuid": "0f62da96ffd443abab31de7de5c14fdb2844236b", "collapsed": true, "trusted": false}, "source": "", "outputs": [], "execution_count": null}], "nbformat_minor": 1, "nbformat": 4}