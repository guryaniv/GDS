{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "510e42b6-9a2f-cb0b-e799-15b21672917b"
      },
      "source": [
        "## Introduction ##\n",
        "The Kaggle Leaderboard is a special kind of place. It does not resemble a real-life situation, but you are based on only one metric: the Log Loss. This means that where we would normally care about for example speed or the size of our models, we don't now. If you have the right models, the tricks in this notebook will help you improve your score a little bit more (hopefully)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "9dd6280c-6dcf-88a6-12c2-e910f8562e4d"
      },
      "source": [
        "## Log Loss ##\n",
        "There is a really good notebook going more into detail on log loss. You can find it [here][1]. The thing we have to take away from this notebook is that when we're more certain about a class and we're wrong, we're punished harder than linear. Just like the Dutch tax laws.\n",
        "\n",
        "\n",
        "  [1]: https://www.kaggle.com/grfiv4/log-loss-depicted-1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "097da9c7-4076-bcb9-f539-2a5d3666a363"
      },
      "source": [
        "## Trick 1: Clipping ##\n",
        "Clipping is a simple operation on our predictions where we set a maximum and a minimum certainty. This avoids really hard punishment in case we're wrong. This means while your model gets better, the less clipping will help you improve your score. For example in the earlier stages of this competition, a clip of 0.90 improved our score from 0.94380 to 0.91815."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "2128ad76-ea49-2d91-585c-2294a43cefca"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "clip = 0.90\n",
        "classes = 8\n",
        "\n",
        "def clip_csv(csv_file, clip, classes):\n",
        "    # Read the submission file\n",
        "    df = pd.read_csv(csv_file, index_col=0)\n",
        "\n",
        "    # Clip the values\n",
        "    df = df.clip(lower=(1.0 - clip)/float(classes - 1), upper=clip)\n",
        "    \n",
        "    # Normalize the values to 1\n",
        "    df = df.div(df.sum(axis=1), axis=0)\n",
        "\n",
        "    # Save the new clipped values\n",
        "    df.to_csv('clip.csv')\n",
        "    print(df.head(10))\n",
        "    \n",
        "# Of course you are going to use your own submission here\n",
        "clip_csv('../input/sample_submission_stg1.csv', clip, classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "299add50-809c-b8cf-14fc-2659358a50a9"
      },
      "source": [
        "## Trick 2: Blending ##\n",
        "When you have different models or different model configurations, then it could be that some models are experts at recognizing all kinds of tuna, while others are better at distinguishing fish vs no fish. Good specialist models are only very certain in their own area. In this case it helps to let them work together to a solution. A way of combining the outputs of multiple models or model settings is blending. It's a very simple procedure where all predictions are added to each other for each image, class pair and then divided by the number of models.\n",
        "\n",
        "Blending can for example be used for test augmentation: all test image are augmented with several operations (flipping, rotation, zooming etc.). When you augment each image a couple of times, you can use them as separate submission files, which can be combined using blending afterwards. This approach improved our score from 0.89893 to 0.86401 for this competition.\n",
        "\n",
        "An other simple alternative for blending is majority voting, where every model is allowed to make one prediction per image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b49baff3-af16-0a72-c731-59c3a991506b"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def blend_csv(csv_paths):\n",
        "    if len(csv_paths) < 2:\n",
        "        print(\"Blending takes two or more csv files!\")\n",
        "        return\n",
        "    \n",
        "    # Read the first file\n",
        "    df_blend = pd.read_csv(csv_paths[0], index_col=0)\n",
        "    \n",
        "    # Loop over all files and add them\n",
        "    for csv_file in csv_paths[1:]:\n",
        "        df = pd.read_csv(csv_file, index_col=0)\n",
        "        df_blend = df_blend.add(df)\n",
        "        \n",
        "    # Divide by the number of files\n",
        "    df_blend = df_blend.div(len(csv_paths))\n",
        "\n",
        "    # Save the blend file\n",
        "    df_blend.to_csv('blend.csv')\n",
        "    print(df_blend.head(10))\n",
        "\n",
        "# Obviously replace this with two or more of your files\n",
        "blend_csv(['../input/sample_submission_stg1.csv', '../input/sample_submission_stg1.csv'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "461279ff-3945-c2f6-aa63-98e4e071dd4c"
      },
      "source": [
        "# Trick 3: Pseudo-Labeling\n",
        "This is an technique where test images are used during training. The labels are provide by the predictions of the model. One of the first thoughts that come to mind is: *\"WhaAT the HECK!?? How can this even work!?\"*.  There is a theorem why this could work. Until we know if it's true, applying Pseudo-Labeling is it's at least worth trying.\n",
        "\n",
        "Quick summary of the explanation given in the paper below: In semi-supervised learning the goal is to make a clear separation between the classes. The decision boundary should be in low-density regions. This way the model generalizes better. The network should use similar activations for the same class.\n",
        "\n",
        "When you want to read more, [here][1] is a paper on the subject.\n",
        "\n",
        "  [1]: http://deeplearning.net/wp-content/uploads/2013/03/pseudo_label_final.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "ca24877a-40c6-01ff-dcea-9282f83df5b4"
      },
      "source": [
        "## Keras example\n",
        "Below is an example of Pseudo-Labling using Keras. The code is cherry-picked from the code from a lesson from fast.ai. The source files are [this][1] and [this][2] one.\n",
        "\n",
        "You can play with the amount of test data you add to the batches. In the example below we choose to take about one third of non-training data, being 1/16th validation data and 1/4th test data. \n",
        "\n",
        "  [1]: https://github.com/fastai/courses/blob/master/deeplearning1/nbs/utils.py\n",
        "  [2]: https://github.com/fastai/courses/blob/master/deeplearning1/nbs/lesson7.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "1b4741c6-e568-a7cf-b5cc-9b13f3634830"
      },
      "outputs": [],
      "source": [
        "# Create a MixIterator object\n",
        "# This class is a simple method to create batches from several other batch generators\n",
        "class MixIterator(object):\n",
        "    def __init__(self, iters):\n",
        "        self.iters = iters\n",
        "        self.multi = type(iters) is list\n",
        "        if self.multi:\n",
        "            self.N = sum([it[0].N for it in self.iters])\n",
        "        else:\n",
        "            self.N = sum([it.N for it in self.iters])\n",
        "\n",
        "    def reset(self):\n",
        "        for it in self.iters: it.reset()\n",
        "\n",
        "    def __iter__(self):\n",
        "        return self\n",
        "\n",
        "    def next(self, *args, **kwargs):\n",
        "        if self.multi:\n",
        "            nexts = [[next(it) for it in o] for o in self.iters]\n",
        "            n0 = np.concatenate([n[0] for n in nexts])\n",
        "            n1 = np.concatenate([n[1] for n in nexts])\n",
        "            return (n0, n1)\n",
        "        else:\n",
        "            nexts = [next(it) for it in self.iters]\n",
        "            n0 = np.concatenate([n[0] for n in nexts])\n",
        "            n1 = np.concatenate([n[1] for n in nexts])\n",
        "        return (n0, n1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "703bf7dc-cd0c-4523-2c6f-bec2941a403d"
      },
      "outputs": [],
      "source": [
        "# Example usage in Keras\n",
        "# [replace by your own code]\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, kernel_size=(3, 3),\n",
        "                 activation='relu',\n",
        "                 input_shape=input_shape))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adam(lr=0.001), metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          verbose=1,\n",
        "validation_data=(x_test, y_test))\n",
        "\n",
        "batch_size = 8\n",
        "# [/replace by your own code]\n",
        "\n",
        "predictions = model.predict(x_test, batch_size=batch_size)\n",
        "\n",
        "gen = ImageDataGenerator()\n",
        "\n",
        "train_batches = gen.flow(x_train, y_train, batch_size=44)\n",
        "val_batches = gen.flow(x_val, y_val, batch_size=4)\n",
        "test_batches = gen.flow(x_test, predictions, batch_size=16)\n",
        "\n",
        "mi = MixIterator([train_batches, test_batches, val_batches])\n",
        "model.fit_generator(mi, mi.N, nb_epoch=8, validation_data=(x_val, y_val))\n"
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}