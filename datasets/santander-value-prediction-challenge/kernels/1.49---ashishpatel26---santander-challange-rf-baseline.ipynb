{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.ensemble import ExtraTreesRegressor\n# from sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.decomposition import PCA, TruncatedSVD, FastICA\nfrom sklearn.random_projection import GaussianRandomProjection, SparseRandomProjection\nfrom sklearn.model_selection import train_test_split\n\nimport xgboost\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"collapsed":true},"cell_type":"code","source":"train_df = pd.read_csv('../input/train.csv')\ntest_df = pd.read_csv('../input/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3d2b3f4e0ff1e89521f0d4ea8a1dfb3ee106affa","collapsed":true},"cell_type":"code","source":"X_train = train_df.drop([\"ID\", \"target\"], axis=1)\ny_train = np.log1p(train_df[\"target\"].values)\n\nX_test = test_df.drop([\"ID\"], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e2e8bb2614516d1b00f24004a37f4b94cf0327c0","collapsed":true},"cell_type":"code","source":"print(\"Total Train Features with NaN Values = \" + str(train_df.columns[train_df.isnull().sum() != 0].size))\nif (train_df.columns[train_df.isnull().sum() != 0].size):\n    print(\"Features with NaN => {}\".format(list(train_df.columns[train_df.isnull().sum() != 0])))\n    train_df[train_df.columns[train_df.isnull().sum() != 0]].isnull().sum().sort_values(ascending = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4840f7743c9344d3a0668beb29cf2d0c955fbb7f","collapsed":true},"cell_type":"code","source":"zero_count = []\nfor col in X_train.columns[2:]:\n    zero_count.append([i[1] for i in list(X_train[col].value_counts().items()) if i[0] == 0][0])\n    \nprint('{0} features of 4491 have zeroes in 99% or more samples.'.format(len([i for i in zero_count if i >= 4459 * 0.99])))\nprint('{0} features of 4491 have zeroes in 98% or more samples.'.format(len([i for i in zero_count if i >= 4459 * 0.98])))\nprint('{0} features of 4491 have zeroes in 97% or more samples.'.format(len([i for i in zero_count if i >= 4459 * 0.97])))\nprint('{0} features of 4491 have zeroes in 96% or more samples.'.format(len([i for i in zero_count if i >= 4459 * 0.96])))\nprint('{0} features of 4491 have zeroes in 95% or more samples.'.format(len([i for i in zero_count if i >= 4459 * 0.95])))\n\ncols_to_drop = [col for col in X_train.columns[2:] if [i[1] for i in list(X_train[col].value_counts().items()) if i[0] == 0][0] >= 4459 * 0.98]\n\nX_train.drop(cols_to_drop, axis=1, inplace=True)\nX_test.drop(cols_to_drop, axis=1, inplace=True)\n\nprint('\\nTrain shape: {}\\nTest shape: {}'.format(X_train.shape, X_test.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bf6f1837e8c912cb53faf29821851528644ed197","collapsed":true},"cell_type":"code","source":"colsToRemove = []\nfor col in X_train.columns:\n    if X_train[col].std() == 0: \n        colsToRemove.append(col)\n        \n# remove constant columns in the training set\ntrain_df.drop(colsToRemove, axis=1, inplace=True)\n\n# remove constant columns in the test set\ntest_df.drop(colsToRemove, axis=1, inplace=True) \n\nprint(\"Removed `{}` Constant Columns\\n\".format(len(colsToRemove)))\nprint(colsToRemove)\nprint('\\nTrain shape: {}\\nTest shape: {}'.format(X_train.shape, X_test.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5882e4a3aeeb6ba408ec7d5be2d2a40f9bf6ed5b","collapsed":true},"cell_type":"code","source":"colsToRemove = []\ncolsScaned = []\ndupList = {}\n\ncolumns = X_train.columns\n\nfor i in range(len(columns)-1):\n    v = X_train[columns[i]].values\n    dupCols = []\n    for j in range(i+1,len(columns)):\n        if np.array_equal(v, X_train[columns[j]].values):\n            colsToRemove.append(columns[j])\n            if columns[j] not in colsScaned:\n                dupCols.append(columns[j]) \n                colsScaned.append(columns[j])\n                dupList[columns[i]] = dupCols\n                \n# remove duplicate columns in the training set\nX_train.drop(colsToRemove, axis=1, inplace=True) \n\n# remove duplicate columns in the testing set\nX_test.drop(colsToRemove, axis=1, inplace=True)\n\nprint(\"Removed `{}` Duplicate Columns\\n\".format(len(dupList)))\nprint(dupList)\n\nprint('\\nTrain shape: {}\\nTest shape: {}'.format(X_train.shape, X_test.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"0955643b29fb47dadd378e7e8deec28e021bdb4e"},"cell_type":"code","source":"def drop_sparse(train, test):\n    flist = [x for x in train.columns if not x in ['ID','target']]\n    for f in flist:\n        if len(np.unique(train[f]))<2:\n            train.drop(f, axis=1, inplace=True)\n            test.drop(f, axis=1, inplace=True)\n    return train, test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8b2d0468f2df752aa76bd5ae419d5fa141917eb8","collapsed":true},"cell_type":"code","source":"print(\"x Shape:\", X_train.shape)\nprint(\"y Shape:\", y_train.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1fbbb37e27d6c66b0ecdab29ce9037ce92f4f485","collapsed":true},"cell_type":"code","source":"def add_SumZeros(train, test, features):\n    flist = [x for x in train.columns if not x in ['ID','target']]\n    if 'SumZeros' in features:\n        train.insert(1, 'SumZeros', (train[flist] == 0).astype(int).sum(axis=1))\n        test.insert(1, 'SumZeros', (test[flist] == 0).astype(int).sum(axis=1))\n    flist = [x for x in train.columns if not x in ['ID','target']]\n\n    return train, test\n\nX_train, X_test = add_SumZeros(X_train, X_test, ['SumZeros'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"49d14e4b7f8ca88724a17de79b3be0851e0a03a6"},"cell_type":"code","source":"def add_SumValues(train, test, features):\n    flist = [x for x in train.columns if not x in ['ID','target']]\n    if 'SumValues' in features:\n        train.insert(1, 'SumValues', (train[flist] != 0).astype(int).sum(axis=1))\n        test.insert(1, 'SumValues', (test[flist] != 0).astype(int).sum(axis=1))\n    flist = [x for x in train.columns if not x in ['ID','target']]\n\n    return train, test\n\nX_train, X_test = add_SumValues(X_train, X_test, ['SumValues'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"7c8729962e5d2366716607bc2b19b00c3c9883ae"},"cell_type":"code","source":"def add_OtherAgg(train, test, features):\n    flist = [x for x in train.columns if not x in ['ID','target','SumZeros','SumValues']]\n    if 'OtherAgg' in features:\n        train['Mean'] = train.mean(axis=1)\n        train['Median'] = train.median(axis=1)\n        train['Mode'] = train.mode(axis=1)\n        train['Max'] = train.max(axis=1)\n        train['Var'] = train.var(axis=1)\n        train['Std'] = train.std(axis=1)\n        \n        test['Mean'] = test.mean(axis=1)\n        test['Median'] = test.median(axis=1)\n        test['Mode'] = test.mode(axis=1)\n        test['Max'] = test.max(axis=1)\n        test['Var'] = test.var(axis=1)\n        test['Std'] = test.std(axis=1)\n    flist = [x for x in train.columns if not x in ['ID','target','SumZeros','SumValues']]\n\n    return train, test\n\ndef kmeans(X_Tr,Xte):\n    flist = [x for x in X_Tr.columns if not x in ['ID','target']]\n    flist_kmeans = []\n    for ncl in range(2,11):\n        cls = KMeans(n_clusters=ncl)\n        cls.fit_predict(X_train[flist].values)\n        X_Tr['kmeans_cluster_'+str(ncl)] = cls.predict(X_Tr[flist].values)\n        Xte['kmeans_cluster_'+str(ncl)] = cls.predict(Xte[flist].values)\n        flist_kmeans.append('kmeans_cluster_'+str(ncl))\n    print(flist_kmeans)\n    \n    return X_Tr,Xte\n\ndef pca(X_Tr,Xte):\n    flist = [x for x in X_Tr.columns if not x in ['ID','target']]\n    n_components = 20\n    flist_pca = []\n    pca = PCA(n_components=n_components)\n    x_train_projected = pca.fit_transform(StandardScaler(X_Tr[flist], axis=0))\n    x_test_projected = pca.transform(StandardScaler(X_test[flist], axis=0))\n    for npca in range(0, n_components):\n        X_Tr.insert(1, 'PCA_'+str(npca+1), x_train_projected[:, npca])\n        Xte.insert(1, 'PCA_'+str(npca+1), x_test_projected[:, npca])\n        flist_pca.append('PCA_'+str(npca+1))\n    print(flist_pca)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"703fc4453d5af4a30fd1cdb82e3d343f83045a2c","collapsed":true},"cell_type":"code","source":"print('\\nTrain shape: {}\\nTest shape: {}'.format(X_train.shape, X_test.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bdc0ebbe0423e5d4e4a8036532ea289c0076e33b","collapsed":true},"cell_type":"code","source":"PERC_TRESHOLD = 0.97   ### Percentage of zeros in each feature ###\nN_COMP = 97            ### Number of decomposition components ###\n\nprint(\"\\nStart decomposition process...\")\nprint(\"PCA\")\npca = PCA(n_components=N_COMP, random_state=17)\npca_results_train = pca.fit_transform(X_train)\npca_results_test = pca.transform(X_test)\nprint(pca.explained_variance_ratio_)\n\nprint(\"tSVD\")\ntsvd = TruncatedSVD(n_components=N_COMP, random_state=17)\ntsvd_results_train = tsvd.fit_transform(X_train)\ntsvd_results_test = tsvd.transform(X_test)\n\nprint(\"ICA\")\nica = FastICA(n_components=N_COMP, random_state=17)\nica_results_train = ica.fit_transform(X_train)\nica_results_test = ica.transform(X_test)\n\nprint(\"GRP\")\ngrp = GaussianRandomProjection(n_components=N_COMP, eps=0.1, random_state=17)\ngrp_results_train = grp.fit_transform(X_train)\ngrp_results_test = grp.transform(X_test)\n\nprint(\"SRP\")\nsrp = SparseRandomProjection(n_components=N_COMP, dense_output=True, random_state=17)\nsrp_results_train = srp.fit_transform(X_train)\nsrp_results_test = srp.transform(X_test)\n\nprint(\"Append decomposition components to datasets...\")\nfor i in range(1, N_COMP + 1):\n    X_train['pca_' + str(i)] = pca_results_train[:, i - 1]\n    X_test['pca_' + str(i)] = pca_results_test[:, i - 1]\n    \n    X_train['ica_' + str(i)] = ica_results_train[:, i - 1]\n    X_test['ica_' + str(i)] = ica_results_test[:, i - 1]\n\n    X_train['tsvd_' + str(i)] = tsvd_results_train[:, i - 1]\n    X_test['tsvd_' + str(i)] = tsvd_results_test[:, i - 1]\n\n    X_train['grp_' + str(i)] = grp_results_train[:, i - 1]\n    X_test['grp_' + str(i)] = grp_results_test[:, i - 1]\n\n    X_train['srp_' + str(i)] = srp_results_train[:, i - 1]\n    X_test['srp_' + str(i)] = srp_results_test[:, i - 1]\nprint('\\nTrain shape: {}\\nTest shape: {}'.format(X_train.shape, X_test.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c6764454b86093299de2fd42d218010920694b61","collapsed":true},"cell_type":"code","source":"print(tsvd.explained_variance_ratio_)\nsum1 = 0\nfor i in range(len(tsvd.explained_variance_ratio_)):\n    sum1 = sum1 + tsvd.explained_variance_ratio_[i]\n\nprint(sum1)\n\nsum = 0\nfor i in range(len(pca.explained_variance_ratio_)):\n    sum = sum + pca.explained_variance_ratio_[i]\n\nprint(sum)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b4f52eaab20a2769eb1e0059805e49e55a302f34","collapsed":true},"cell_type":"code","source":"# # from sklearn.model_selection import RandomizedSearchCV\n# rf_model = RandomForestRegressor()\n# # n_iter_search = 20\n# # random_search = RandomizedSearchCV(clf, param_distributions=param_dist,n_iter=n_iter_search)\n# # clf.fit(X_train,y_train)\n# rf_model.fit(X_train, y_train)\n\n# # n_estimators=400, n_jobs=-1,oob_score = True,random_state =1, max_depth=8, max_features = \"auto\", verbose=1, bootstrap=True, max_leaf_nodes=31\n# # n_estimators=400, n_jobs=-1,oob_score = True,bootstrap=True,max_depth=70,max_features='auto',min_samples_leaf=4,min_samples_split=10,criterion='rmse',random_state=42","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a7d2d8d7a55c575777e6c347fbfd5a0e640e3d3b","collapsed":true},"cell_type":"code","source":"# rf_model.score(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7717009bd40522a47c3e2ec173d550d991055948","collapsed":true},"cell_type":"code","source":"# y_pre = rf_model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8ee1b0f4df903589cca3390cc732789dfd370b06","collapsed":true},"cell_type":"code","source":"# #submit\n# sub = pd.read_csv('../input/sample_submission.csv')\n# sub['target'] = np.around(np.expm1(y_pre), 0)\n# sub.to_csv('sub_rf.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"e8b42061e76a940ae76909afdd7cbec98feb3a40"},"cell_type":"code","source":"tree = ExtraTreesRegressor()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0f3c3def2c34b4e79f7c741281abe74feb72d44b","collapsed":true},"cell_type":"code","source":"tree.fit(X_train, y_train)\ntree.score(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"e5737abc35b844e27b3f8d505664a4a83989657d"},"cell_type":"code","source":"y_pred = tree.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b76cb321871d6f6b82e4c86f0c26817dc8076422","collapsed":true},"cell_type":"code","source":"sub = pd.read_csv('../input/sample_submission.csv')\nsub['target'] = np.around(np.expm1(y_pred), 0)\nsub.to_csv('sub_rf.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c2c264595d0032fe02270e87728cbb3c6f539b74","collapsed":true},"cell_type":"code","source":"sub.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"27e6998a2dc44f73c12e2b79bf93fcb1d5199b8b"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}