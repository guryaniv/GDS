{"cells":[{"metadata":{"_uuid":"dd3f67fff557521ce3f02e8b14211622a0fb9b55"},"cell_type":"markdown","source":"# Introduction"},{"metadata":{"_uuid":"8ef3b292d87f5412fcfbf935e7b1ccf32feae41f"},"cell_type":"markdown","source":"- In this kernel, I suggest the simple strategy to fill null data.\n- (1) Make new dataframe which contains features without null data. This is new train\n- (2) Make new dataframe which contains features with null data. This is new test. Each feature will be target.\n- (3) We do just training and prediction. Choose feature from new test. And Set the feature as target. Train with samples which have target and predict for the samples which don't have target."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\n\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import KFold\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport gc\n\nfrom sklearn.preprocessing import MinMaxScaler, LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.linear_model import LinearRegression\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6eba8606ff8244d042377a6260e9408b72e8b3eb"},"cell_type":"code","source":"print('Importing data...')\n\ndata = pd.read_csv('../input/application_train.csv')\ntest = pd.read_csv('../input/application_test.csv')\nprev = pd.read_csv('../input/previous_application.csv')\nburo = pd.read_csv('../input/bureau.csv')\nburo_balance = pd.read_csv('../input/bureau_balance.csv')\ncredit_card  = pd.read_csv('../input/credit_card_balance.csv')\nPOS_CASH  = pd.read_csv('../input/POS_CASH_balance.csv')\npayments = pd.read_csv('../input/installments_payments.csv')\nlgbm_submission = pd.read_csv('../input/sample_submission.csv')\n\ny = data['TARGET']\n\ndel data['TARGET']\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"023874599ac1d54bcdd90bf2ef8bc2797ab818e6"},"cell_type":"markdown","source":"# Preparation"},{"metadata":{"_uuid":"ac31e4a27e5154e32b5f169f3e9ccc65df5b22ef"},"cell_type":"markdown","source":"- This preparation of dataset  is from https://www.kaggle.com/zahedi/first-xgb.\n- Thanks for sharing!"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"data['loan_to_income'] = data.AMT_ANNUITY/data.AMT_INCOME_TOTAL\ntest['loan_to_income'] = test.AMT_ANNUITY/test.AMT_INCOME_TOTAL\n\n#One-hot encoding of categorical features in data and test sets\n\ncategorical_features = [col for col in data.columns if data[col].dtype == 'object']\n\n\none_hot_df = pd.concat([data,test])\none_hot_df = pd.get_dummies(one_hot_df, columns=categorical_features)\n\n\ndata = one_hot_df.iloc[:data.shape[0],:]\ntest = one_hot_df.iloc[data.shape[0]:,]\n\n#Pre-processing buro_balance\n\nprint('Pre-processing buro_balance...')\n\nburo_grouped_size = buro_balance.groupby('SK_ID_BUREAU')['MONTHS_BALANCE'].size()\nburo_grouped_max = buro_balance.groupby('SK_ID_BUREAU')['MONTHS_BALANCE'].max()\nburo_grouped_min = buro_balance.groupby('SK_ID_BUREAU')['MONTHS_BALANCE'].min()\n\nburo_counts = buro_balance.groupby('SK_ID_BUREAU')['STATUS'].value_counts(normalize = False)\nburo_counts_unstacked = buro_counts.unstack('STATUS')\nburo_counts_unstacked.columns = ['STATUS_0', 'STATUS_1','STATUS_2','STATUS_3','STATUS_4','STATUS_5','STATUS_C','STATUS_X',]\nburo_counts_unstacked['MONTHS_COUNT'] = buro_grouped_size\nburo_counts_unstacked['MONTHS_MIN'] = buro_grouped_min\nburo_counts_unstacked['MONTHS_MAX'] = buro_grouped_max\n\nburo = buro.join(buro_counts_unstacked, how='left', on='SK_ID_BUREAU')\n\n#Pre-processing previous_application\n\nprint('Pre-processing previous_application...')\n#One-hot encoding of categorical features in previous application data set\nprev_cat_features = [pcol for pcol in prev.columns if prev[pcol].dtype == 'object']\nprev = pd.get_dummies(prev, columns=prev_cat_features)\navg_prev = prev.groupby('SK_ID_CURR').mean()\ncnt_prev = prev[['SK_ID_CURR', 'SK_ID_PREV']].groupby('SK_ID_CURR').count()\navg_prev['nb_app'] = cnt_prev['SK_ID_PREV']\n\ndel avg_prev['SK_ID_PREV']\n\nprint('Pre-processing buro...')\n\n#One-hot encoding of categorical features in buro data set\n\nburo_cat_features = [bcol for bcol in buro.columns if buro[bcol].dtype == 'object']\nburo = pd.get_dummies(buro, columns=buro_cat_features)\navg_buro = buro.groupby('SK_ID_CURR').mean()\n\navg_buro['buro_count'] = buro[['SK_ID_BUREAU', 'SK_ID_CURR']].groupby('SK_ID_CURR').count()['SK_ID_BUREAU']\n\ndel avg_buro['SK_ID_BUREAU']\n\n#Pre-processing POS_CASH\n\nprint('Pre-processing POS_CASH...')\n\nle = LabelEncoder()\nPOS_CASH['NAME_CONTRACT_STATUS'] = le.fit_transform(POS_CASH['NAME_CONTRACT_STATUS'].astype(str))\nnunique_status = POS_CASH[['SK_ID_CURR', 'NAME_CONTRACT_STATUS']].groupby('SK_ID_CURR').nunique()\nnunique_status2 = POS_CASH[['SK_ID_CURR', 'NAME_CONTRACT_STATUS']].groupby('SK_ID_CURR').max()\nPOS_CASH['NUNIQUE_STATUS'] = nunique_status['NAME_CONTRACT_STATUS']\nPOS_CASH['NUNIQUE_STATUS2'] = nunique_status2['NAME_CONTRACT_STATUS']\nPOS_CASH.drop(['SK_ID_PREV', 'NAME_CONTRACT_STATUS'], axis=1, inplace=True)\n\nprint('Pre-processing credit_card...')\ncredit_card['NAME_CONTRACT_STATUS'] = le.fit_transform(credit_card['NAME_CONTRACT_STATUS'].astype(str))\nnunique_status = credit_card[['SK_ID_CURR', 'NAME_CONTRACT_STATUS']].groupby('SK_ID_CURR').nunique()\nnunique_status2 = credit_card[['SK_ID_CURR', 'NAME_CONTRACT_STATUS']].groupby('SK_ID_CURR').max()\ncredit_card['NUNIQUE_STATUS'] = nunique_status['NAME_CONTRACT_STATUS']\ncredit_card['NUNIQUE_STATUS2'] = nunique_status2['NAME_CONTRACT_STATUS']\ncredit_card.drop(['SK_ID_PREV', 'NAME_CONTRACT_STATUS'], axis=1, inplace=True)\n\n#Pre-processing payments\n\nprint('Pre-processing payments...')\n\navg_payments = payments.groupby('SK_ID_CURR').mean()\navg_payments2 = payments.groupby('SK_ID_CURR').max()\navg_payments3 = payments.groupby('SK_ID_CURR').min()\n\ndel avg_payments['SK_ID_PREV']\n\ndata = data.merge(right=avg_prev.reset_index(), how='left', on='SK_ID_CURR')\ntest = test.merge(right=avg_prev.reset_index(), how='left', on='SK_ID_CURR')\n\ndata = data.merge(right=avg_buro.reset_index(), how='left', on='SK_ID_CURR')\ntest = test.merge(right=avg_buro.reset_index(), how='left', on='SK_ID_CURR')\n\ndata = data.merge(POS_CASH.groupby('SK_ID_CURR').mean().reset_index(), how='left', on='SK_ID_CURR')\ntest = test.merge(POS_CASH.groupby('SK_ID_CURR').mean().reset_index(), how='left', on='SK_ID_CURR')\n\ndata = data.merge(credit_card.groupby('SK_ID_CURR').mean().reset_index(), how='left', on='SK_ID_CURR')\ntest = test.merge(credit_card.groupby('SK_ID_CURR').mean().reset_index(), how='left', on='SK_ID_CURR')\n\ndata = data.merge(right=avg_payments.reset_index(), how='left', on='SK_ID_CURR')\ntest = test.merge(right=avg_payments.reset_index(), how='left', on='SK_ID_CURR')\n\ndata = data.merge(right=avg_payments2.reset_index(), how='left', on='SK_ID_CURR')\ntest = test.merge(right=avg_payments2.reset_index(), how='left', on='SK_ID_CURR')\n\n\ndata = data.merge(right=avg_payments3.reset_index(), how='left', on='SK_ID_CURR')\ntest = test.merge(right=avg_payments3.reset_index(), how='left', on='SK_ID_CURR')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"10c2bdaec4bbb6db63ed2fb064ca809455dc18c1"},"cell_type":"code","source":"print('data: ', data.shape, 'test: ', test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f09423935b548f0693c9d714068b513b763448aa"},"cell_type":"code","source":"print('Removing features with more than 80% missing...')\n\ntest = test[test.columns[data.isnull().mean() < 0.85]]\ndata = data[data.columns[data.isnull().mean() < 0.85]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"83e266cfec30952ab4d5fb11cde593c104ab6932"},"cell_type":"code","source":"print('data: ', data.shape, 'test: ', test.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"97c3d8052a4bb0371ab51b81ce9219c0b9b3453e"},"cell_type":"markdown","source":"# Filling with linear regression\n- You can choose other regression algorithm.\n- And You can add gridsearch part for simple parameter tunning."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"ba7e58a20e1ede51201116a921b24890bd08f6d8"},"cell_type":"code","source":"train_length = data.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"5b958fa838258a9fa54b8cd226631177b2b7b4a3"},"cell_type":"code","source":"df_train_ID = data['SK_ID_CURR']\ndf_test_ID = test['SK_ID_CURR']\n\ndata.drop(['SK_ID_CURR'], axis=1, inplace=True)\ntest.drop('SK_ID_CURR', axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"5c5853810f515482a71e0ea55a79951b1288c4f2"},"cell_type":"code","source":"df_all = pd.concat([data, test]).reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"a2ddc78ec768f9e0124c9cfc11c289a30e1b151f"},"cell_type":"code","source":"all_with_null = df_all.loc[:, df_all.isnull().any()]\nall_without_null = df_all.loc[:, df_all.notnull().all()]\nfeatures_with_null = all_with_null.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"95f9d7ec8bae3e993f1e05ec0583ba3ed1b9898c"},"cell_type":"code","source":"for i, temp_feature in enumerate(features_with_null):\n    print('For now, {} features have null data'.format(df_all.isnull().any().sum()))\n    print('{} have {} null data'.format(temp_feature, df_all[temp_feature].isnull().sum()))\n    temp_train = all_without_null.copy()\n    temp_train[temp_feature] = all_with_null[temp_feature]\n\n    new_train = temp_train.loc[temp_train[temp_feature].notnull(), :]\n    new_test = temp_train.loc[temp_train[temp_feature].isnull(), :]\n\n    temp_target = new_train[temp_feature].values\n\n    new_train.drop([temp_feature], axis=1, inplace=True)\n    new_test.drop([temp_feature], axis=1, inplace=True)\n    \n    # you can add gridsearch or randomsearch for parameter tunning of linear regression model\n#     x_tr, x_vld, y_tr, y_vld = train_test_split(new_train, temp_target, test_size=0.2, random_state=1989)\n    print('-'*30,  '{} : Start Linear regression'.format(i), '-'*30)\n    lr = LinearRegression()\n    lr.fit(new_train, temp_target)\n\n    temp_pred = lr.predict(new_test)\n\n    new_train[temp_feature] = temp_target\n    new_test[temp_feature] = temp_pred\n    print('Prediction and concat')\n    foo = pd.concat([new_train, new_test]).sort_index()\n    \n    df_all[temp_feature] = foo[temp_feature]\n    del foo","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9e3cc1878cf7f0886ef6f282a612de7292de86f8"},"cell_type":"markdown","source":"- This is our result which doesn't!!! have null data."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"87564826014a23f534e356834cd31587c6ff18d9"},"cell_type":"code","source":"df_train_filled = df_all[:train_length]\ndf_test_filled = df_all[train_length:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"7261e3cd289b3dbce5a570ace98894497135deb3"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}