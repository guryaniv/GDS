{"cells": [{"cell_type": "markdown", "source": ["# Introduction\n", "\n", "In this notebook we will look at the credit risk fraud data and try to use an Auto encoder to identify fraud transactions. The data analysis is allready been described in another notebook, so that part will be skipped.\n", "\n", "I choose the following approach:\n", "1. Split the fraud cases in a train and testset\n", "2. Use the same ratio for the normal cases\n", "3. Oversample the fraud training set to match the number of cases in the normal training set\n", "4. Define an auto encoder with two hidden tensors, reducing the dimensionality to 2 dimensions\n", "5. Train an auto encoder on the oversampled set\n", "5. Evaluate the model on the test set with  the trained hidden layer.\n", "\n", "By reducing the data to two dimensions, while the model has learned how frauds look like, we hope to see a clear separation in the 2 dimensional space.\n", "\n", "The choice for oversampling is based on the wish to include as much normal transactions as posible. In this way the model has the best opportunity to learn how a normal transaction looks like. The fraud transactions are copies, a large group with little variantion."], "metadata": {"_uuid": "2eba680526e7a24a9ccfb3a5c584b975835adcfb", "_cell_guid": "eec8ab34-764c-45af-89be-e0b78d3562e7"}}, {"cell_type": "code", "execution_count": null, "metadata": {"_uuid": "70775862827223e03685b9645e60866fcacc8a8f", "collapsed": true, "_cell_guid": "65670ccf-44a2-4cba-939b-27f15b621868"}, "outputs": [], "source": ["# This Python 3 environment comes with many helpful analytics libraries installed\n", "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n", "# For example, here's several helpful packages to load in \n", "\n", "import numpy as np # linear algebra\n", "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n", "\n", "# Input data files are available in the \"../input/\" directory.\n", "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n", "\n", "import seaborn as sns\n", "import matplotlib.gridspec as gridspec\n", "import matplotlib.pyplot as plt\n", "%matplotlib inline"]}, {"cell_type": "markdown", "source": ["# Getting the data\n", "\n", "First let's fetch the data and describe it."], "metadata": {"_uuid": "929409a8b238800fd7f820c8cd5d09bff8becbb4", "_cell_guid": "e8a66d77-1070-488d-be78-5d803f261a8d"}}, {"cell_type": "code", "execution_count": null, "metadata": {"_uuid": "c790cb538240ae7291df3e3776b061fe6fdfd91a", "_cell_guid": "ae3554f3-2336-438c-a30f-af51d0a880f7"}, "outputs": [], "source": ["df = pd.read_csv(\"../input/creditcard.csv\")\n", "df.describe()"]}, {"cell_type": "code", "execution_count": null, "metadata": {"_uuid": "b94b2cecf60a43937f471dcfdb6a9db5d30d7736", "collapsed": true, "_cell_guid": "2b5efdb5-f5bd-44c8-acae-450a67b39b7e"}, "outputs": [], "source": ["# reshuffle the data\n", "df=df.sample(frac=1).reset_index(drop=True)"]}, {"cell_type": "markdown", "source": ["## Sampling\n", "\n", "We select a training set of frauds - leaving 30% as test set. We oversample the training fraud set to match the training set of normals. "], "metadata": {"_uuid": "f73f4ed02c929f7bc579ca68a2c67e9ab4ba7698", "_cell_guid": "07314768-da24-408b-a8ed-4e0088649788"}}, {"cell_type": "code", "execution_count": null, "metadata": {"_uuid": "7987ca3603448885e91ecba850d369b8768308c3", "_cell_guid": "eecb7aa4-e87e-4431-bd42-2327fe3be15e"}, "outputs": [], "source": ["fraud_indices = np.array(df[df.Class == 1].index)\n", "number_records_fraud = len(fraud_indices)\n", "\n", "# Picking the indices of the normal classes\n", "normal_indices = np.array(df[df.Class == 0].index)\n", "number_records_normal = len(normal_indices)\n", "\n", "trainingratio = 0.7\n", "training_n_normal = round(number_records_normal*trainingratio)\n", "training_n_fraude = round(number_records_fraud*trainingratio)\n", "\n", "# Select the fraud cases trainingset\n", "random_fraud_indices = np.random.choice(fraud_indices, training_n_fraude, replace = False)\n", "random_fraud_indices = np.array(random_fraud_indices)\n", "\n", "# Out of the fraud indices pick training_n_normal cases with replacement to oversample\n", "duplicated_fraud_indices = np.random.choice(random_fraud_indices, training_n_normal, replace = True)\n", "duplicated_fraud_indices = np.array(duplicated_fraud_indices)\n", "\n", "# Select random the training normal cases without replacement\n", "random_normal_indices = np.random.choice(normal_indices, training_n_normal, replace = False)\n", "random_normal_indices = np.array(random_normal_indices)\n", "\n", "# Appending the 2 indices\n", "sample_indices = np.concatenate([random_normal_indices,duplicated_fraud_indices])\n", "\n", "# Sample dataset\n", "sample_data = df.iloc[sample_indices,:]\n", "test_data = df.drop(sample_indices,axis=0)\n", "\n", "# sort on Class for the scatter plots at the end, to make sure that Frauds are drawn last\n", "test_data=test_data.sort_values(['Class'], ascending=[True])\n", "\n", "#shuffle the data, because the frauds where added to the tail\n", "sample_data=sample_data.sample(frac=1).reset_index(drop=True)\n", "\n", "print(\"Normal transactions:                     \", number_records_normal)\n", "print(\"Fraud  transactions:                     \", number_records_fraud)\n", "print(\"Fraud  transactions for training:        \", len(random_fraud_indices))\n", "\n", "print(\"Selected normal transactions:            \", len(random_normal_indices))\n", "print(\"Selected oversampled fraud transactions: \",  len(duplicated_fraud_indices))\n", "\n", "print(\"Fraud  transactions selected for test:   \", len(test_data[test_data.Class == 1]))\n", "\n", "print(\"Normal transactions selected for test:   \", len(test_data[test_data.Class == 0]))"]}, {"cell_type": "markdown", "source": ["The fraud data is oversampled. \n", "\n", "Now the data can be scaled. The scaler is fitted on the whole set, applying the transformation to the train and test set.\n", "\n", "\n", "\n"], "metadata": {"_uuid": "269666ddf9a54721e55d2948743878431a6acd61", "_cell_guid": "f7fd634f-2e93-4822-8b59-d986d7e2e5ca"}}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.preprocessing import MinMaxScaler\n", "scaler = MinMaxScaler()\n", "scaler.fit(df.drop(['Class','Time'],axis=1))\n", "\n", "scaled_data = scaler.transform(sample_data.drop(['Class','Time'],axis=1))\n", "scaled_test_data = scaler.transform(test_data.drop(['Class','Time'],axis=1))\n", "print(\"Size training data: \", len(scaled_data))\n", "print(\"Size test data:     \", len(scaled_test_data))"]}, {"cell_type": "markdown", "source": ["# Encoder\n", "We use *Tensorflow* to define and generate an encoder. There are 3 layers with in the middle a hidden layer with 2 tensors. We train the network to represent the training data with this hidden layer of 2 nodes. Hoping that the fraud and non-frauds can be distinguish from each other."], "metadata": {}}, {"cell_type": "code", "execution_count": null, "metadata": {"_uuid": "20bc7151beba4fee7b2df52a43a132c2c3460bd5", "collapsed": true, "_cell_guid": "793cec68-f755-4b24-9344-7a1c934ec0c8"}, "outputs": [], "source": ["import tensorflow as tf\n", "\n", "num_inputs = len(scaled_data[1])\n", "num_hidden = 2  \n", "num_outputs = num_inputs \n", "\n", "learning_rate = 0.001\n", "keep_prob = 0.5\n", "tf.reset_default_graph() "]}, {"cell_type": "markdown", "source": ["## Placeholders and layers\n", "We define the placeholders and the layers. We use a simple model. I tried some other activations but the tanh gives the best results. Also a dropout layer is defined to force the network to generalize."], "metadata": {"_uuid": "2acb3ce5a50d48fd4d9de9b2940750e42e65ccef", "_cell_guid": "fea2032c-5d6a-449a-b165-0b4c5757068a"}}, {"cell_type": "code", "execution_count": null, "metadata": {"_uuid": "8f5cb8b942f6c68635b114733b69c726b3c4f8c9", "collapsed": true, "_cell_guid": "8a549a09-d340-49d5-992a-010ef80c7f5e"}, "outputs": [], "source": ["# placeholder X\n", "X = tf.placeholder(tf.float32, shape=[None, num_inputs])\n", "\n", "# weights\n", "initializer = tf.variance_scaling_initializer()\n", "w = tf.Variable(initializer([num_inputs, num_hidden]), dtype=tf.float32)\n", "w_out = tf.Variable(initializer([num_hidden, num_outputs]), dtype=tf.float32)\n", "\n", "# bias\n", "b = tf.Variable(tf.zeros(num_hidden))\n", "b_out = tf.Variable(tf.zeros(num_outputs))\n", "\n", "#activation\n", "act_func = tf.nn.tanh\n", "\n", "# layers\n", "hidden_layer = act_func(tf.matmul(X, w) + b)\n", "dropout_layer= tf.nn.dropout(hidden_layer,keep_prob=keep_prob)\n", "output_layer = tf.matmul(dropout_layer, w_out) + b_out"]}, {"cell_type": "markdown", "source": ["## Functions \n", "The loss and the optimizer have to be defined. For the loss we want the output (output_layer) to be as close to the input (X) as possible. The maximum value of X is 1. \n", "\n", "We specifiy a function to create a new batch with random data."], "metadata": {"_uuid": "b68ac74d8f0e6a3aa052f805f38d49de55fc83c5", "_cell_guid": "2e7db2d3-b46b-445c-b32a-5eb5b1fb71dd"}}, {"cell_type": "code", "execution_count": null, "metadata": {"_uuid": "6422a3248c5f3eb5fd3ab339f9104ebd5efd84b0", "collapsed": true, "_cell_guid": "6512f3ef-c4ab-4f30-ad62-1ecb592a778c"}, "outputs": [], "source": ["loss = tf.reduce_mean(tf.abs(output_layer - X))\n", "optimizer = tf.train.AdamOptimizer(learning_rate)\n", "train  = optimizer.minimize( loss)\n", "init = tf.global_variables_initializer()\n", "\n", "def next_batch(x_data,batch_size):\n", "    \n", "    rindx = np.random.choice(x_data.shape[0], batch_size, replace=False)\n", "    x_batch = x_data[rindx,:]\n", "    return x_batch"]}, {"cell_type": "markdown", "source": ["## Training\n", "Let's define the training and evaluate the loss from each epoch."], "metadata": {"_uuid": "a532a2e9ff175632ce54fefc344c1516926a225c", "_cell_guid": "ba87a2a9-5355-4072-bd75-7ebfd41abb62"}}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["num_steps = 10\n", "batch_size = 150\n", "num_batches = len(scaled_data) // batch_size\n", "\n", "with tf.Session() as sess:\n", "    sess.run(init)\n", "    for step in range(num_steps):        \n", "        for iteration in range(num_batches):\n", "            X_batch = next_batch(scaled_data,batch_size)\n", "            sess.run(train,feed_dict={X: X_batch})\n", "        \n", "        if step % 1 == 0:\n", "            err = loss.eval(feed_dict={X: scaled_data})\n", "            print(step, \"\\tLoss:\", err)\n", "            output_2d = hidden_layer.eval(feed_dict={X: scaled_data})\n", "    \n", "    output_2d_test = hidden_layer.eval(feed_dict={X: scaled_test_data})"]}, {"cell_type": "markdown", "source": ["### Training results\n", "\n", "The hidden layer is trained, let's see where the frauds (yellow)  and non-frauds are located in the 2 dimensional space.\n", "\n"], "metadata": {"_uuid": "d6e9526b234a942511f2b56ad55cea3d7b5121d6", "_cell_guid": "168eea4c-776c-4981-9d5e-54cc92a46a83"}}, {"cell_type": "code", "execution_count": null, "metadata": {"_uuid": "51717fa85242a460c833d18f2b7b43b9c6963605", "_cell_guid": "b6e08ef5-41cc-447a-9faf-006a2f618d95"}, "outputs": [], "source": ["plt.figure(figsize=(20,8))\n", "plt.scatter(output_2d[:,0],output_2d[:,1],c=sample_data['Class'],alpha=0.7)"]}, {"cell_type": "markdown", "source": ["That looks promising. The most frauds (yellow) are seperated from the purple (normal) cloud. However some of the normal transactions at the far end share more with the frauds than with the other normal transactions.\n", "\n", "### Test results\n", "At the end of the training we can evaluate the test data, let's see what the scatter looks like."], "metadata": {"_uuid": "6665e6c384c2e063a50ffe8f5f15f1ee1e3cdfdc", "_cell_guid": "8f1858d8-4096-466c-bb77-82946054526c"}}, {"cell_type": "code", "execution_count": null, "metadata": {"_uuid": "1bc8c7593f5182b38af3c44d2751884c9fb837b6", "_cell_guid": "a2def8d7-2990-4742-9feb-95a54275b84c"}, "outputs": [], "source": ["plt.figure(figsize=(20,8))\n", "plt.scatter(output_2d_test[:,0],output_2d_test[:,1],c=test_data['Class'],alpha=1)"]}, {"cell_type": "markdown", "source": ["There is a seperation between the normal and fraud transactions. Based on the values in the hidden layer a fraud can be predicted, with a slight chance of false positives. "], "metadata": {}}, {"cell_type": "markdown", "source": ["## Things left to do\n", "A few things would be interesting to look into:\n", "1. Define a formula based on the hidden layer to predict whether a transaction is a fraud\n", "2. Redesign the layers to establish beter seperaion in the hidden layer"], "metadata": {}}], "nbformat_minor": 1, "metadata": {"language_info": {"file_extension": ".py", "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "mimetype": "text/x-python", "codemirror_mode": {"version": 3, "name": "ipython"}, "name": "python", "version": "3.6.4"}, "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}}, "nbformat": 4}