{"cells":[{"metadata":{"_uuid":"e96a59ab490ee9f694c36b06a7f0406b411fb0d3"},"cell_type":"markdown","source":"# **Predicting Fraud - Anomaly Detection**\n\nWell, I am guessing you guys have probably had a look at the other kernels before coming to this one.\nSo here's a few things I am assuming you might already know:\n- Data set is the result of PCA (Principal Component Analysis) transformation of the original credit card transactions. And features V1 to V28 are the resultant features of that transformation.\n- This dataset is highly skewed with only 492 frauds in 284807 transactions i.e. only 0.172 % of the dataset is contaminated.\n- There are no missing values.\n\nLets get on with the necessities:\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"#Loading necessary libraries\nimport pandas as pd\nimport numpy as np\nimport seaborn as sb\nimport matplotlib.pyplot as plt\nfrom sklearn.utils import shuffle\nfrom sklearn.metrics import precision_recall_fscore_support as prfs\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.manifold import TSNE\nfrom scipy.stats import multivariate_normal\n\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"#Loading the dataset\ndf = pd.read_csv(\"../input/creditcard.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9a5a9034cad9b384093e40cf06529dff1828524b","scrolled":false},"cell_type":"code","source":"#Exploring the dataset\nprint(\"Dataset is of shape: {}\".format(df.shape))\nprint(\"Fraud cases: {}\".format(len(df[df.Class==1])))\nprint(\"Normal cases: {}\".format(len(df[df.Class==0])))\nprint(\"Contamination: {}\".format((float(len(df[df.Class==1]))/len(df))*100))\ndf.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"74c2a5c95e842c36ddd9e7a7c636be4573e5d53e"},"cell_type":"markdown","source":"## **t-SNE Visualization**\nt-SNE (t-distributed stochastic neighbor embedding) is a visualization technique used to visualize high dimensional data. It starts by calculating the probability of similarity of points in high-dimensional space and probability of similarity of points in the corresponding low-dimensional space. It then uses a gradient descent method to minimize the error (sum of difference of proabilities of high and corresponding low dimension).\n\nIf you want to know more, then here's a pretty nice explanation of what t-SNE does: http://www.youtube.com/watch?v=NEaUSP4YerM\n\nWe can not infer much from t-SNE, it is just used here for better visualization of our dataset:"},{"metadata":{"trusted":true,"_uuid":"3278273b390b952bb775831dd01537bd7a3f9c73"},"cell_type":"code","source":"#Sampling the dataset for tsne algorithm\ntsne_data_fraud = df[df.Class==1]\ntsne_data_normal = df[df.Class==0].sample(frac=0.05, random_state=1)\nprint(tsne_data_fraud.shape)\nprint(tsne_data_normal.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0d315d60ac7b6af029b579acf1f86d4f58ff4d3a"},"cell_type":"markdown","source":"t-SNE is very time consuming and it directly relates to the amount of the data given to it. Performing it on 284807 transactions is not practical given the gig i have, hence, I am only taking a fraction of it, which corresponds to around 15000 examples."},{"metadata":{"trusted":true,"_uuid":"bb8ec23deaf1668beccaa4478f2beb014c418434"},"cell_type":"code","source":"#Data engineering for tsne\ntsne_data = tsne_data_fraud.append(tsne_data_normal, ignore_index=True)\ntsne_data = shuffle(tsne_data)\nlabel = tsne_data.iloc[:, -1]\ntsne_data = tsne_data.iloc[:, :30]\ntsne_data = tsne_data.astype(np.float64)\n\nstandard_scaler = StandardScaler()\ntsne_data = standard_scaler.fit_transform(tsne_data)\n\nprint(tsne_data.shape)\nprint(label.shape) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4e7e8035de26cb62b7c13b4f6dbb7ee9df6bc94f"},"cell_type":"code","source":"#Performing dimension reduction (tsne)\ntsne = TSNE(n_components=2, random_state=0)\ntsne_data = tsne.fit_transform(tsne_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2e1605c8ffdc5b77ec1846c4b1855a9ba65cbcbd"},"cell_type":"code","source":"#Making final changes to the resulted data from tsne\nprint(tsne_data.shape)\ntsne_plot = np.vstack((tsne_data.T, label))\ntsne_plot = tsne_plot.T\nprint(tsne_plot.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c25c93ddd326befcb58022dac61ae98277277e80"},"cell_type":"code","source":"#Plotting the tsne results\ntsne_plot = pd.DataFrame(data=tsne_plot, columns=(\"V1\", \"V2\", \"Class\"))\nsb.FacetGrid(tsne_plot, size=6, hue=\"Class\").map(plt.scatter, \"V1\", \"V2\").add_legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dc23decb3ef80f390546218c96b20406732b9aab"},"cell_type":"markdown","source":"You can see how most of the anomilies are clustered together in groups. So we need an efficient anomaly detection technique and train it to identify those points as outliers and hence brand them as anomalies.\n\n## **Multivariate Gaussian Anomaly Detection:**\nI will be using multivariate gaussian anomaly detection. For those who don't know what it is, kindly follow the \"Machine Learning by Andrew Ng\" course on coursera, you will find anomaly detection in  week 9 of the course (its a pretty nice course :).\n\nHere's a summary of what is to be done in this anomaly detection algorithm:\n* Divide the dataset into 3 parts: training set(only normal cases), cross-validation set(normal + fraud) and test set(normal + fraud).\n* Fit the model p(x) by finding the mean and co-variance matrix using the training set.\n* Use cross-validation set to find an optimal epsilon.\n* Apply the model on test set. For any new data, its p(x) will be calculated and it will be marked as an anomaly if it is less than epsilon.\n"},{"metadata":{"trusted":true,"_uuid":"43fd72b1248f477311ab97b3c218151dde04b106"},"cell_type":"code","source":"#Visualizing each feature separately\ndf.hist(figsize=(20,20), bins=50, color=\"green\", alpha=0.5)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6c3d4b615d7387b90fbb31dc1a56a88951c4b9cd"},"cell_type":"markdown","source":"As the number of features is quite less, there is no specific need to select and drop the irrelevent features.\nAnd as for creating new features from the existing ones, Multivariate Gaussian itlself detects the correlation among features so no need for that too !"},{"metadata":{"trusted":true,"_uuid":"7ba9f25435794a05ab222c677776a62dd944df38"},"cell_type":"code","source":"#Creating train, cross-validation and test set\ndf_fraud = shuffle(df[df.Class==1])\ndf_normal = shuffle(df[df.Class==0].sample(n=280000))\nprint(df_fraud.shape)\nprint(df_normal.shape)\ndf_train = df_normal.iloc[:240000, :].drop(labels = [\"Class\", \"Time\"], axis = 1)\ndf_cross = shuffle(df_normal.iloc[240000:260000, :].append(df_fraud.iloc[:246, :]))\nY_cross = df_cross.loc[:, \"Class\"]\ndf_cross = df_cross.drop(labels = [\"Class\", \"Time\"], axis = 1)\ndf_test = shuffle(df_normal.iloc[260000:, :].append(df_fraud.iloc[246:, :]))\nY_test = df_test.loc[:, \"Class\"]\ndf_test = df_test.drop(labels = [\"Class\", \"Time\"], axis = 1)\nprint(df_train.shape)\nprint(df_cross.shape)\nprint(Y_cross.shape)\nprint(df_test.shape)\nprint(Y_test.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5b557c6113fee54b82167a7162437c6b5ca2e590"},"cell_type":"code","source":"#Defining fuctions to calculate mean, cov and gaussian probablities\ndef mean_variance(data):\n    mean = np.mean(data, axis=0)\n    cov = np.cov(data.T)\n    return mean, cov\n\ndef gaussian_dist(data, mean, cov):\n    prob = multivariate_normal.pdf(data, mean=mean, cov=cov)\n    return prob","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"94fdaea755cbffcbefc9b34215069b8d0325a4d0"},"cell_type":"code","source":"#Fitting the model for train, cross and test set using mean and cov from train_set\nmean, cov = mean_variance(df_train)\nprint(mean.shape)\nprint(cov.shape)\nprob_train = gaussian_dist(df_train, mean, cov)\nprob_cross = gaussian_dist(df_cross, mean, cov)\nprob_test = gaussian_dist(df_test, mean, cov)\n\nprint(prob_train.shape)\nprint(prob_cross.shape)\nprint(prob_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4cf0b39de14655ac039f82f6524788c989cdd6a1"},"cell_type":"markdown","source":"Below is the optimization function for epsilon, normally you would start by setting *max_e = prob_train.max()*\nand then* keep decreasing max_e* or *keep increasing min_e* depending on the results untill you reach to an optimal balance between recall and precision or simply a better f1 score.\nBut it isn't practical to show all the steps here so I am just skipping to the end:"},{"metadata":{"trusted":true,"_uuid":"5ec88a1f66d35104ab2d8a2cc3cc30fde9a616f4","_kg_hide-output":true,"scrolled":true},"cell_type":"code","source":"#Using cross-validation set to find the optimum epsilon\ndef optimize_for_epsilon(prob_train, prob_cross, Y_cross):\n    best_f1 = 0\n    max_e = 2.062044871798754e-79\n    min_e = prob_train.min()\n    step = (max_e - min_e) / 1000\n    \n    for e in np.arange(prob_cross.min(), max_e, step):\n        Y_cross_pred = prob_cross < e\n        precision, recall, f1_score, support = prfs(Y_cross, Y_cross_pred, average=\"binary\")\n        print(\"for epsilon: {}\".format(e))\n        print(\"f1_score: {}\".format(f1_score))\n        print(\"recall: {}\".format(recall))\n        print(\"precision: {}\".format(precision))\n        print(\"support: {}\".format(support))\n        print()\n        \n        if f1_score > best_f1:\n            best_f1 = f1_score\n            best_epsilon = e\n            recall = recall\n        \n    return best_f1, best_epsilon, recall\n\nbest_f1, best_epsilon, recall = optimize_for_epsilon(prob_train, prob_cross, Y_cross)\nprint(best_f1, best_epsilon, recall)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e8cbed51730f9bc6ef79e854987a85f61d4dc1ce"},"cell_type":"code","source":"#Predicting the anomalies on test_set using the optimal epsilon from above results\nY_test_pred = prob_test < best_epsilon\nprecision, recall, f1_score, ignore = prfs(Y_test, Y_test_pred, average=\"binary\")\nprint(\"epsilon: {}\".format(best_epsilon))\nprint(\"f1_score: {}\".format(f1_score))\nprint(\"recall: {}\".format(recall))\nprint(\"precision: {}\".format(precision))\n# print(\"support: {}\".format(support))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2b81a9f95f252252ba0c4a80e6307ed2a49b497e"},"cell_type":"markdown","source":"*Note: Values mentioned below might vary a bit with the above results as the model gives a bit different results in different epochs.*\n\nWe were able to achieve f1 score of 0.71 and were able to catch almost 80% of the fraud cases (0.7926 recall) with around 65% precision. We can increase f1 score even more but then recall will reduce as there will be a trade off between recall and precision and i personally think catching more of the frauds even if we get some false positives along with is more important rather than improving precision and missing lots of fraud cases."},{"metadata":{"trusted":true,"_uuid":"02f8f75f08896b01ed8ce2a5a693a8a6bf85f37b"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}