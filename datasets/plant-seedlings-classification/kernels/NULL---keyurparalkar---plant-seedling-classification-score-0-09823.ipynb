{"cells":[{"metadata":{"trusted":true,"collapsed":true,"_uuid":"55c7168b774243f982bfd44bde96048d807824cb"},"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision\nimport torchvision.transforms as transforms\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch.nn.functional as F\nimport torchvision.models as models\n\nimport os\nfrom PIL import Image\nplt.ion()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"6e6bac7bb667bf656d93cb5a13df3eff2b5153d9"},"cell_type":"code","source":"PATH = '../input'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"4564b843fa806f5b285d55f2f09c28f77eb9f3d3"},"cell_type":"code","source":"#loadingg test set data:\ndef load_test_data(data_path,transform):\n    temp = []\n    \n    allTestImages = os.listdir(data_path)\n    for x in allTestImages:\n        img = Image.open(data_path+'/'+allTestImages[1])\n        temp.append(transform(np.array(img)))\n        \n    return temp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3c8ec43706303b201f116c3e9cfd8ec5bbfefc2c"},"cell_type":"code","source":"#Loading train dataset\ntransform = {'train': transforms.Compose([\n    transforms.CenterCrop(224),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n]),\n  'test':transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.CenterCrop(224),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n}\ntrainData = torchvision.datasets.ImageFolder(root=PATH+'/train',transform=transform['train'])\ntrainLen = len(trainData)\ntrainData1, valData = torch.utils.data.dataset.random_split(trainData,[int((trainLen*4)/5),int(trainLen/5)])\n\ntrainData1Loader = torch.utils.data.DataLoader(dataset=trainData1, shuffle=False, batch_size=4)\nvalDataLoader = torch.utils.data.DataLoader(dataset=valData, shuffle=False, batch_size=4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"095d4cf819506bf3c4b7ab5f23e24b2e6b7ad514"},"cell_type":"code","source":"len(trainData1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"da627a184fd92a01f8ee781134cc56576634cbbc"},"cell_type":"code","source":"#Loading test dataset\ntestData = torch.stack(load_test_data(PATH+'/test',transform=transform['test'])) #For converting list to tensor\n# testData = transform(testData)\ntestDataLoader = torch.utils.data.DataLoader(dataset=testData, batch_size=4)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"326dce05d74d6674094f4e15e70f142fc2cd5a36"},"cell_type":"code","source":"print(\"TRAIN DATASET === \")\nprint(\"No. of examples = \",len(trainData1Loader.dataset))\nprint(\"VAL SET ==== \")\nprint(\"No. of examples =\",len(valDataLoader.dataset))\nprint(\"\\nTEST DATASET ===\")\nprint(\"No. of exmaples = \",testDataLoader.dataset.size()[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"75d03e73f78bb460c7d6345dcab25737f8c12170"},"cell_type":"code","source":"#Visualizing Train dataset\n'''\nIn trainDataLoader Dimensions are given as\ndim. index              0    1    2\nActual Dims.           [3   128  128]\n\nThese dimension are not suitable for plt.imshow() it needs dimensions in the format HxWxC but we have CxHxW\nSo to change this we need our this dim. sequence = 0,1,2 in this format i.e. new dim. sequence 1,2,0 i.e. HxWxC\nSo thats what np.transpose(img,(1,2,0)) is doing its changing the dims to suitable format.\n'''\n\ndef imageShow(img):\n    img = [0.229, 0.224, 0.225]*np.transpose(img.numpy(),(1,2,0)) + [0.485, 0.456, 0.406]\n    plt.imshow(img)\n    plt.xlabel('Train images batch = 4')\n    \niterator = iter(trainData1Loader)\nimage, label =  iterator.next()\n\nimageShow(torchvision.utils.make_grid(image))\nprint('Ground Truth = \\n',' '.join('%10s' % trainData1Loader.dataset.classes[x] for x in label.numpy()))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"78e76bed96eb7576ac34f90407b4d48f33a2c7c2"},"cell_type":"code","source":"# Forward => loss => backward => update_weights\ndef train_model(model,criterion,optimizer,scheduler,dictionary,num_epochs=12):\n    correct = 0\n    total = 0\n    totalLoss = []\n    prediction = []\n    temp = []\n    \n    for epoch in range(num_epochs):\n        print('Epoch {}/{}\\n'.format(epoch,num_epochs-1))\n        scheduler.step()   #to step or to update weights\n        model.train()\n            \n        for batch_id,(image,label) in enumerate(trainData1Loader):\n            optimizer.zero_grad()\n                \n            image = image.to(device)\n            label = label.to(device)\n                \n            outputs = model(image)\n            _, predictionIndex = torch.max(outputs,1)\n            loss = criterion(outputs,label)\n            prediction.append(predictionIndex)\n            \n            #printing loss =\n            print(\"Loss = {0:.5f}\".format(loss.item()),end=\"\\r\")\n            correct += (predictionIndex == label).sum().item()\n            total +=label.size(0)\n            \n            loss.backward()\n            optimizer.step()        \n            \n            del image, label    #important\n            \n        totalLoss.append(loss)\n#         prediction.append(temp)\n        torch.cuda.empty_cache()      #important\n        \n    dictionary['totalLoss'] = totalLoss\n    dictionary['correct'] = correct\n    dictionary['totalSize'] = total\n    dictionary['prediction'] = prediction\n    \n    #ALWAYS return the model object\n    return model","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"53a1c324dc391ae3451a926615854d7b3a2837fb"},"cell_type":"code","source":"model_ft = models.vgg16(pretrained=True)\n\nfor child in model_ft.features.children():\n    for param in child.parameters():\n        param.requires_grad = False\n        \n    \n    \nmodel_ft.classifier[6].out_features = 12\nprint(model_ft)\n\nmodel_ft = model_ft.to(device)\n\ncriterion = nn.CrossEntropyLoss().cuda()\n\noptimizer_ft = torch.optim.SGD(params=model_ft.classifier.parameters(), lr=0.001, momentum=0.9)\n\nexp_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"5d0d058f02c4f4df6affc8bb0521fa940b72384c"},"cell_type":"code","source":"dictModel = {}\nmodel_ft = train_model(model_ft,criterion,optimizer_ft,exp_lr_scheduler,dictionary=dictModel)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ad1d4747e71754ba4cdf25aec252b6e7ff5a6423"},"cell_type":"code","source":"dictModel","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bb5b46d83509a0673c4db39054240aceaaa5f44b"},"cell_type":"code","source":"#loss vs iteration graph:\nplt.plot(dictModel['totalLoss'])\nplt.xlabel('epochs')\nplt.ylabel('loss')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"492c22bcf26fe2ed539652cd43d3dc084f08464c"},"cell_type":"code","source":"print(\"Train Accuracy = \",100*(dictModel['correct']/dictModel['totalSize']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9ae185fb2ca7e3455865c52426d1487de6361668"},"cell_type":"code","source":"#Validation set:\nmodel_ft_val = train_model(model_ft,criterion,optimizer_ft,exp_lr_scheduler,dictionary=dictModel)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9c9451da445f7a6ef12765d8ed0cf114ee723354"},"cell_type":"code","source":"print(\"Val Accuracy = \",100*(dictModel['correct']/dictModel['totalSize']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"41b21adcb783a9142cfe4678fb7a22d3266e4f3e"},"cell_type":"code","source":"# Validation set\nplt.plot(dictModel['totalLoss'])\nplt.xlabel('epochs')\nplt.ylabel('loss')","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"0d27c6e57c8853028dbae6d23a14590be31e51d0"},"cell_type":"code","source":"model_ft_val = model_ft_val.to(device)\nmodel_ft_val.eval()\n\nresult = []\n\nfor batch_id,image in enumerate(testDataLoader):\n    img = image.to(device)\n    ip = torch.autograd.Variable(img)\n    testOutput = model_ft(ip)\n    _, testPredictionIndex = torch.max(testOutput,1)\n    result.append(testPredictionIndex)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"96f0cc43d139ef1a02f05319fdd84e8696d5a7c8"},"cell_type":"code","source":"result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fc1cc57003ff740af3ae7f15bd0a266544e47634"},"cell_type":"code","source":"temp = []\nfor x in result:\n    for y in x.cpu().numpy():\n        temp.append(y)\nlen(temp)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"795fa91b5aa0b88a1ba07e806048279ddcd01b17"},"cell_type":"code","source":"dfDict = {\n    'file':os.listdir(PATH+'test'),\n    'species':[trainData.classes[m] for m in temp]\n}\ndf = pd.DataFrame(dfDict)\ndf.to_csv(path_or_buf='submission.csv',index=False)\nprint(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"c750e21580bd7a83948a72ce9c0a73141c553910"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"}},"nbformat":4,"nbformat_minor":1}