{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4485e2c739c73bce4f3f44f607bbcb768dfadceb"},"cell_type":"markdown","source":"This is my try to do a benchmark prediction :) "},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"df_train = pd.read_csv('../input/train.csv')\ndf_test = pd.read_csv('../input/test.csv')\nsample = pd.read_csv('../input/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dd7195785dddf7ed2834a7072bfc051fe82de19d"},"cell_type":"code","source":"sample.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6a1a181a1d1a7a8daf044f6ee23bc21c64704ef2"},"cell_type":"code","source":"import nltk\nfrom nltk.corpus import stopwords\nSTOPWORDS = set(stopwords.words('english'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"acdb04772da4452a1abfe9a9a1c048ae7be7cc3f"},"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB \nimport matplotlib.pyplot as plt\nimport string","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"452cf6876808fc6f75e9137c8beb44b7f78ee1c1"},"cell_type":"code","source":"#FROM https://www.kaggle.com/sudalairajkumar/simple-exploration-notebook-qiqc\n\n## Number of unique words in the text ##\ndf_train[\"num_words\"] = df_train[\"question_text\"].apply(lambda x: len(str(x).split()))\ndf_test[\"num_words\"] = df_test[\"question_text\"].apply(lambda x: len(str(x).split()))\n                                                                    \n## Number of characters in the text ##\ndf_train[\"num_chars\"] = df_train[\"question_text\"].apply(lambda x: len(str(x)))\ndf_test[\"num_chars\"] = df_test[\"question_text\"].apply(lambda x: len(str(x)))\n                                                                    \n## Number of stopwords in the text ##\ndf_train[\"num_stopwords\"] = df_train[\"question_text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in STOPWORDS]))\ndf_test[\"num_stopwords\"] = df_test[\"question_text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in STOPWORDS]))\n                                                                    \n## Number of punctuations in the text ##\ndf_train[\"num_punctuations\"] =df_train['question_text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\ndf_test[\"num_punctuations\"] =df_test['question_text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n                  \n## Number of title case words in the text ##\ndf_train[\"num_words_title\"] = df_train[\"question_text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\ndf_test[\"num_words_title\"] = df_test[\"question_text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n          \n## Average length of the words in the text ##\ndf_train[\"mean_word_len\"] = df_train[\"question_text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\ndf_test[\"mean_word_len\"] = df_test[\"question_text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))         ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a781f14520917140eb61085aafe18f6109d311bf"},"cell_type":"code","source":"# countVector\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport scipy as sp\ncount_vectorizer = CountVectorizer(decode_error='ignore')\nY = df_train['target'].values\n\nX = sp.sparse.hstack((count_vectorizer.fit_transform(df_train.question_text),df_train[['num_words', 'num_chars', 'num_stopwords', 'num_punctuations', 'num_words_title', 'mean_word_len']].values),format='csr')\nX_columns=count_vectorizer.get_feature_names()+df_train[['num_words', 'num_chars', 'num_stopwords', 'num_punctuations', 'num_words_title', 'mean_word_len']].columns.tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"374ea981fad140a4f202689e951a0411ab8be475"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nXtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"99fde83d48a049be28bb65fec21707dae46100d6"},"cell_type":"code","source":"model = MultinomialNB()\nmodel.fit(Xtrain, Ytrain)\nprint('Classification rate for NB: ', model.score(Xtrain, Ytrain))\nprint('Classification rate for NB: ', model.score(Xtest, Ytest))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9c58b40a2dae83e15afefdd1a0ed1eb688cb163a"},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\ntest_prediction = model.predict(Xtest)\nprint(confusion_matrix(Ytest, test_prediction))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a79f4d707dab71a864e0c1f291ec67ffdac6009a"},"cell_type":"code","source":"# To understand overall, how well the model predicts. \nfrom sklearn.metrics import classification_report\nprint(classification_report(Ytest, test_prediction))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a67d2a888183dcac8d2c945b9635f62572b4dc79"},"cell_type":"markdown","source":"Obviously, it's really bad. F-score is super low for insincere questions *sadface*"},{"metadata":{"trusted":true,"_uuid":"5390a0984c507ea8a886943408ad2618b88c2a41"},"cell_type":"code","source":"# https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-classification-in-python/\n# roc curve and auc\nfrom sklearn.datasets import make_classification\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\nfrom matplotlib import pyplot\n\n# predict probabilities\nprobs = model.predict_proba(Xtest)\n# keep probabilities for the positive outcome only\nprobs = probs[:, 1]\n# calculate AUC\nauc = roc_auc_score(Ytest, probs)\nprint('AUC: %.3f' % auc)\n# calculate roc curve\nfpr, tpr, thresholds = roc_curve(Ytest, probs)\n# plot no skill\npyplot.plot([0, 1], [0, 1], linestyle='--')\n# plot the roc curve for the model\npyplot.plot(fpr, tpr, marker='.')\n# show the plot\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d1a595968d4396fd35e59f3cafa31b229cdce036"},"cell_type":"code","source":"# https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-classification-in-python/\n# precision-recall curve and f1\nfrom sklearn.datasets import make_classification\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import auc\nfrom sklearn.metrics import average_precision_score\nfrom matplotlib import pyplot\n\nprobs = model.predict_proba(Xtest)\n# keep probabilities for the positive outcome only\nprobs = probs[:, 1]\n# predict class values\nyhat = model.predict(Xtest)\n# calculate precision-recall curve\nprecision, recall, thresholds = precision_recall_curve(Ytest, probs)\n# calculate F1 score\nf1 = f1_score(Ytest, yhat)\n# calculate precision-recall AUC\nauc = auc(recall, precision)\n# calculate average precision score\nap = average_precision_score(Ytest, probs)\nprint('f1=%.3f auc=%.3f ap=%.3f' % (f1, auc, ap))\n# plot no skill\npyplot.plot([0, 1], [0.5, 0.5], linestyle='--')\n# plot the roc curve for the model\npyplot.plot(recall, precision, marker='.')\n# show the plot\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"aad171318fd381ae4d6e0dc1f075f41d01f6c382"},"cell_type":"code","source":"final = sp.sparse.hstack((count_vectorizer.transform(df_test.question_text),df_test[['num_words', 'num_chars', 'num_stopwords', 'num_punctuations', 'num_words_title', 'mean_word_len']].values),format='csr')\ndf_test['prediction'] = model.predict(final)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4dd142125391ea35032a92735369a12b25785a19"},"cell_type":"code","source":"print(df_test.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a5f1a9f64b4981d1d07ebcb45e7cb97cc7eeb284"},"cell_type":"code","source":"df_test = df_test[['qid', 'prediction']]\ndf_test.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}