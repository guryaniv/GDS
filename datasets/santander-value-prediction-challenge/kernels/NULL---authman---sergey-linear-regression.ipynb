{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"markdown","source":"@sggpls had mentioned he was able to get 1.5x CV using [LR traned row-wise](https://www.kaggle.com/c/santander-value-prediction-challenge/discussion/61189#357214). That seemed worth investigating, perhaps we can featurize it and throw it into our model. Currently, our models only use the train values and stats. Sergey's LR used the indices of the non-zero values. Let's give it a shot."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"45c45d61de0aceb36031fea0e3669efd2ec6b3c1"},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\n\nimport matplotlib.pyplot as plt \nfrom multiprocessing import Pool","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"8d1f71a18416afbb29d6379c159fd28fb17c0130"},"cell_type":"code","source":"def get_data():\n    print('Reading data')\n    data = pd.read_csv('../input/train.csv')\n    test = pd.read_csv('../input/test.csv')\n    \n    print('Train shape ', data.shape, ' Test shape ', test.shape)\n    return data, test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"10f0bd5b28cee292e9f64aa9f950b201fd2dd40d","collapsed":true},"cell_type":"code","source":"data, test = get_data()\n\ndel data['ID']\n\n# Log space all, including target. LR won't work in the skewed raw space\ndata = np.log1p(data)\ntarget = data.target.values\ndel data['target']\n\nsub = test[['ID']].copy()\ndel test['ID']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"c016988fccae25815989cf0cec31059dbfd8272c"},"cell_type":"code","source":"# True. Target is nonzero values. Data is indices of nonzero features...\ntv = data.values\n\ndef min_it(lindex, return_data=False):\n    lr = LinearRegression(n_jobs=1)\n    \n    preds  = np.zeros(tv.shape[0])\n    errors = np.ones(tv.shape[0])\n    for i in range(tv.shape[0]):\n        nzv = tv[i]\n        indices = np.array(np.nonzero(nzv > 0) [0]).reshape(-1,1)\n        nzv     = nzv[nzv>0]\n\n        # predict next value in time series?\n        # what should be index? len(nzv)? len(dset)? something else?\n        # last_index = np.array([nzv.shape[0]]*2).reshape(-1,1)\n        # last_index = np.array([data.shape[1]//2 + nzv.shape[0]*4 ]).reshape(-1,1)\n        last_index = np.array([lindex]).reshape(-1,1)\n\n        if nzv.shape[0] == 0:\n            #contigency -- use mean or something else. just put 0\n            preds[i] = np.mean(nzv)\n            print('WTX')\n            continue\n\n        lr.fit(indices,nzv)\n        pred      = lr.predict(last_index)\n        preds[i]  = pred[0]\n        errors[i] = mean_squared_error([target[i]], pred) ** .5\n\n    score = mean_squared_error(target, preds) ** .5\n    return score, preds, errors","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"a820b4b91bfddaadb153c07096d3226897d762ac"},"cell_type":"code","source":"# # If you want to run the full thing... don't try in kaggle kernel:\n# pool    = Pool(processes=11) #12 core\n# results = pool.map(min_it, range(1,data.shape[1]*2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"19aa0c747a4e0c428ccf12bcd90767db6289c7da","collapsed":true},"cell_type":"code","source":"score, preds, errors = min_it(data.shape[1]//2, return_data=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a724111f9288c129e08f605b990545b2fe9bf05e","collapsed":true},"cell_type":"code","source":"'RMSLE', score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"113efebc7ab924ecc5383a58df17ecdd088b6c8f","collapsed":true},"cell_type":"code","source":"errors[:100]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f3732218bceb8285e57188687b66f643b6f25926"},"cell_type":"markdown","source":"The min result after running the optimization was **1.6595739169729133**. That can't possible be the best we can perform. For sure, the number of non-zeros must play a role. I know because I scored 1.6586905090389363 CV by using the row dependent formula `data.shape[1]//2 + nzv.shape[0]*4`.\n\nBy changing the formula in the method to: `last_index = np.array([lindex + nzv.shape[0]*4 ]).reshape(-1,1)`\n\nAnd re-running the optimization again for `results = pool.map(min_it, range(2000,3000))`,\n\nThe following chart is produced:\n\n![Opt](https://image.ibb.co/d9ojmy/download.png)\n\n514+2000 being passed in produces the minimal value of 1.658632 RMSLE."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"**TODO**\n\n* Brute force the opimal avg index to use for prediction\n* Investigate train[:100] independently, and find the optimal index. Is there a way to predict the optimal index to use for target prediction using the non-zero indices, row stats, or non-zero values directly?\n* Investigate if dropping std()==0 columns improves scores, as this would affect the column indices.\n* Add predicted LR score, and LR-Confidence, trained using BLL or Regression, to current best scoring model (1.37) and evaluate\n* Explore introducing, for example, [:100] non-zero indices as inputs into GBDT models..."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"2358a9410beff959c1c95776f19034eb212a4414"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}