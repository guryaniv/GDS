{"metadata": {"kernelspec": {"display_name": "Python 3", "name": "python3", "language": "python"}, "language_info": {"version": "3.6.3", "nbconvert_exporter": "python", "file_extension": ".py", "pygments_lexer": "ipython3", "mimetype": "text/x-python", "name": "python", "codemirror_mode": {"version": 3, "name": "ipython"}}}, "nbformat_minor": 1, "cells": [{"cell_type": "code", "metadata": {"_uuid": "fd2c925daba048dca542a68c9520cd2642b33775", "_cell_guid": "66bbc270-79ff-4156-bdfe-f3e69ce7eefc"}, "execution_count": null, "source": ["# IDEAS FROM\n", "# https://www.kaggle.com/valentinw/simple-data-exploration-and-visualization\n", "# https://www.kaggle.com/iamprateek/submission-to-mercari-price-suggestion-challenge\n", "# https://www.kaggle.com/huguera/mercari-data-analysis\n", "# https://www.kaggle.com/rimunoz/titanic/\n", "\n", "\n", "import numpy as np # linear algebra\n", "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n", "from wordcloud import WordCloud, STOPWORDS\n", "import squarify \n", "import matplotlib.pyplot as plt\n", "\n", "# Input data files are available in the \"../input/\" directory.\n", "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n", "\n", "from subprocess import check_output\n", "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n", "\n", "def transform_category_name(category_name):\n", "    try:\n", "        main, sub1, sub2= category_name.split('/')\n", "        return main, sub1, sub2\n", "    except:\n", "        return np.nan, np.nan, np.nan\n", "print('OK')\n", "\n", "# load dataset\n", "train = pd.read_csv('../input/train.tsv', sep = \"\\t\")\n", "test = pd.read_csv('../input/test.tsv', sep = \"\\t\")\n", "\n", "train = train.sample(frac=0.10, replace=False)\n", "#test  = test.sample(frac=0.25, replace=False)\n", "\n", "# Store our ID for easy access\n", "testId = test['test_id']\n", "\n", "#train.head(1)\n", "print(train.shape)\n", "print(test.shape)"], "outputs": []}, {"cell_type": "code", "metadata": {"_uuid": "c6d60d3b096de628b550f34c3f3e62dec7e9939d", "_cell_guid": "d58586de-3464-4f0e-916d-17531d5e0905"}, "execution_count": null, "source": ["from sklearn.feature_extraction.text import CountVectorizer\n", "\n", "nrow_train = train.shape[0]\n", "nrow_test  = test.shape[0]\n", "print(str(nrow_train) + \"-\" + str(nrow_test))\n", "################################# ITEM DESCRIPTION #############\n", "text_a = train['item_description'].fillna('NA')\n", "text_b =  test['item_description'].fillna('NA')\n", "text = text_a.append(text_b)\n", "\n", "vect = CountVectorizer(max_features = 20,stop_words='english')\n", "dtm = vect.fit_transform(text)\n", "item_dtm = pd.DataFrame(dtm.toarray(), columns=vect.get_feature_names()) \n", "\n", "print('item')\n", "################################# NAME #############\n", "text_a = train['name'].fillna('NA')\n", "text_b =  test['name'].fillna('NA')\n", "text = text_a.append(text_b)\n", "\n", "vect = CountVectorizer(max_features = 10,stop_words='english')\n", "dtm = vect.fit_transform(text)\n", "name_dtm = pd.DataFrame(dtm.toarray(), columns=vect.get_feature_names()) \n", "\n", "print('name')\n", "################################# CATEGORY NAME #############\n", "text_a = train['category_name'].fillna('NA')\n", "text_b =  test['category_name'].fillna('NA')\n", "text = text_a.append(text_b)\n", "\n", "vect = CountVectorizer(max_features = 5,stop_words='english')\n", "dtm = vect.fit_transform(text)\n", "cat_dtm = pd.DataFrame(dtm.toarray(), columns=vect.get_feature_names()) \n", "\n", "print('cat')\n", "################################# BRAND NAME #############\n", "text_a = train['brand_name'].fillna('NA')\n", "text_b =  test['brand_name'].fillna('NA')\n", "text = text_a.append(text_b)\n", "\n", "vect = CountVectorizer(max_features = 5,stop_words='english')\n", "dtm = vect.fit_transform(text)\n", "brand_dtm = pd.DataFrame(dtm.toarray(), columns=vect.get_feature_names()) \n", "\n", "print('brand')\n", "\n", "text_dtm = item_dtm\n", "print(text_dtm.shape)\n", "text_dtm = pd.concat([text_dtm.reset_index(drop=True), name_dtm.reset_index(drop=True)], axis=1)\n", "print(text_dtm.shape)\n", "text_dtm = pd.concat([text_dtm.reset_index(drop=True), cat_dtm.reset_index(drop=True)], axis=1)\n", "print(text_dtm.shape)\n", "text_dtm = pd.concat([text_dtm.reset_index(drop=True), brand_dtm.reset_index(drop=True)], axis=1)\n", "print(text_dtm.shape)\n", "\n", "train_text_dtm = text_dtm.iloc[:nrow_train,].reset_index(drop=True)\n", "test_text_dtm  = text_dtm.iloc[nrow_train:,].reset_index(drop=True)\n", "\n", "nrow_train1 = train_text_dtm.shape[0]\n", "nrow_test1  = test_text_dtm.shape[0]\n", "print(str(nrow_train1) + \"-\" + str(nrow_test1))\n", "print(train_text_dtm.info())\n", "print(test_text_dtm.info())"], "outputs": []}, {"cell_type": "code", "metadata": {"_uuid": "b7c30aa2caad63d6b72cea123174772ff710afc4", "_cell_guid": "49d50f66-f4de-4734-9ff1-0ce1ae3ea321"}, "execution_count": null, "source": ["np.sum(train_text_dtm)"], "outputs": []}, {"cell_type": "code", "metadata": {"_uuid": "013d86e18c2f7270f8f5d48e2052add907e18fa8", "_cell_guid": "4d8f9010-aa1f-4cef-bb23-3ad9a69ceb66"}, "execution_count": null, "source": ["def if_null(row):\n", "    if row == row:\n", "        return 1\n", "    else:\n", "        return 0\n", "print('OK')"], "outputs": []}, {"cell_type": "markdown", "metadata": {"_uuid": "62bbf8a90f2507c60156c7127afa3cf32535ad18", "_cell_guid": "83b233c4-4283-4f9d-8509-8d383b44bdc7"}, "source": ["df = train.groupby(['brand_name']).count()['train_id']\n", "\n", "import pandas as pd\n", "df=pd.DataFrame(df)\n", "df.sort_values('train_id', ascending= False).head(20)"]}, {"cell_type": "code", "metadata": {"_uuid": "6cfd0c4dd1898c7de5cedec2aeebb78429b5a89f", "_cell_guid": "b60f6d48-7a02-4489-bc30-122c30e3100c"}, "execution_count": null, "source": ["full_data = [train, test]\n", "for dataset in full_data:\n", "    dataset['category_main'], dataset['category_sub1'], dataset['category_sub2'] = zip(*dataset['category_name'].apply(transform_category_name))\n", "    dataset['item_description_len'] = dataset['item_description'].str.len()\n", "    dataset['name_len'] = dataset['name'].str.len()\n", "    \n", "    dataset['has_descrip'] = 1\n", "    dataset.loc[dataset.item_description=='No description yet', 'has_descrip'] = 0\n", "    dataset['has_brand'] = dataset.brand_name.apply(lambda row : if_null(row))\n", "    \n", "    dataset['contains_brand_new'] = dataset['item_description'].str.contains(\"Brand New\")\n", "    dataset['contains_brand_new'] = dataset['contains_brand_new'].map( {True: 1, False: 0} ).astype(float)\n", "    \n", "    dataset['contains_free_shipping'] = dataset['item_description'].str.contains(\"free shipping\")\n", "    dataset['contains_free_shipping'] = dataset['contains_free_shipping'].map( {True: 1, False: 0} ).astype(float)\n", "    \n", "    dataset['contains_price_firm'] = dataset['item_description'].str.contains(\"Price firm\")\n", "    dataset['contains_price_firm'] = dataset['contains_price_firm'].map( {True: 1, False: 0} ).astype(float)\n", "    \n", "    dataset['contains_rm'] = dataset['item_description'].str.contains(\"[rm]\")\n", "    dataset['contains_rm'] = dataset['contains_rm'].map( {True: 1, False: 0} ).astype(float)\n", "    \n", "    dataset['is_Adidas'] = dataset['brand_name'].str.contains(\"Adidas\")\n", "    dataset['is_Adidas'] = dataset['is_Adidas'].map( {True: 1, False: 0} ).astype(float)\n", "    dataset['is_American_Eagle'] = dataset['brand_name'].str.contains(\"American Eagle\")\n", "    dataset['is_American_Eagle'] = dataset['is_American_Eagle'].map( {True: 1, False: 0} ).astype(float)\n", "    dataset['is_Apple'] = dataset['brand_name'].str.contains(\"Apple\")\n", "    dataset['is_Apple'] = dataset['is_Apple'].map( {True: 1, False: 0} ).astype(float)\n", "    dataset['is_Bath__Body_Works'] = dataset['brand_name'].str.contains(\"Bath & Body Works\")\n", "    dataset['is_Bath__Body_Works'] = dataset['is_Bath__Body_Works'].map( {True: 1, False: 0} ).astype(float)\n", "    dataset['is_Coach'] = dataset['brand_name'].str.contains(\"Coach\")\n", "    dataset['is_Coach'] = dataset['is_Coach'].map( {True: 1, False: 0} ).astype(float)\n", "    dataset['is_Disney'] = dataset['brand_name'].str.contains(\"Disney\")\n", "    dataset['is_Disney'] = dataset['is_Disney'].map( {True: 1, False: 0} ).astype(float)\n", "    dataset['is_FOREVER_21'] = dataset['brand_name'].str.contains(\"FOREVER 21\")\n", "    dataset['is_FOREVER_21'] = dataset['is_FOREVER_21'].map( {True: 1, False: 0} ).astype(float)\n", "    dataset['is_Funko'] = dataset['brand_name'].str.contains(\"Funko\")\n", "    dataset['is_Funko'] = dataset['is_Funko'].map( {True: 1, False: 0} ).astype(float)\n", "    dataset['is_LuLaRoe'] = dataset['brand_name'].str.contains(\"LuLaRoe\")\n", "    dataset['is_LuLaRoe'] = dataset['is_LuLaRoe'].map( {True: 1, False: 0} ).astype(float)\n", "    dataset['is_Lululemon'] = dataset['brand_name'].str.contains(\"Lululemon\")\n", "    dataset['is_Lululemon'] = dataset['is_Lululemon'].map( {True: 1, False: 0} ).astype(float)\n", "    dataset['is_Michael_Kors'] = dataset['brand_name'].str.contains(\"Michael Kors\")\n", "    dataset['is_Michael_Kors'] = dataset['is_Michael_Kors'].map( {True: 1, False: 0} ).astype(float)\n", "    dataset['is_Nike'] = dataset['brand_name'].str.contains(\"Nike\")\n", "    dataset['is_Nike'] = dataset['is_Nike'].map( {True: 1, False: 0} ).astype(float)\n", "    dataset['is_Nintendo'] = dataset['brand_name'].str.contains(\"Nintendo\")\n", "    dataset['is_Nintendo'] = dataset['is_Nintendo'].map( {True: 1, False: 0} ).astype(float)\n", "    dataset['is_Old_Navy'] = dataset['brand_name'].str.contains(\"Old Navy\")\n", "    dataset['is_Old_Navy'] = dataset['is_Old_Navy'].map( {True: 1, False: 0} ).astype(float)\n", "    dataset['is_PINK'] = dataset['brand_name'].str.contains(\"PINK\")\n", "    dataset['is_PINK'] = dataset['is_PINK'].map( {True: 1, False: 0} ).astype(float)\n", "    dataset['is_Rae_Dunn'] = dataset['brand_name'].str.contains(\"Rae Dunn\")\n", "    dataset['is_Rae_Dunn'] = dataset['is_Rae_Dunn'].map( {True: 1, False: 0} ).astype(float)\n", "    dataset['is_Sephora'] = dataset['brand_name'].str.contains(\"Sephora\")\n", "    dataset['is_Sephora'] = dataset['is_Sephora'].map( {True: 1, False: 0} ).astype(float)\n", "    dataset['is_Sony'] = dataset['brand_name'].str.contains(\"Sony\")\n", "    dataset['is_Sony'] = dataset['is_Sony'].map( {True: 1, False: 0} ).astype(float)\n", "    dataset['is_Under_Armour'] = dataset['brand_name'].str.contains(\"Under Armour\")\n", "    dataset['is_Under_Armour'] = dataset['is_Under_Armour'].map( {True: 1, False: 0} ).astype(float)\n", "    dataset['is_Victoria_Secret'] = dataset['brand_name'].str.contains(\"Victoria's Secret\")\n", "    dataset['is_Victoria_Secret'] = dataset['is_Victoria_Secret'].map( {True: 1, False: 0} ).astype(float)\n", "    \n", "    dataset['item_description'] = dataset['item_description'].str.lower()\n", "    \n", "    \n", "    ### VAR IDEAS FROM https://www.kaggle.com/lopuhin/eli5-for-mercari\n", "    dataset['contains_gb'] = dataset['item_description'].str.contains(\"gb \")\n", "    dataset['contains_gb'] = dataset['contains_gb'].map( {True: 1, False: 0} ).astype(float)\n", "    \n", "    dataset['contains_14k'] = dataset['item_description'].str.contains(\"14k \")\n", "    dataset['contains_14k'] = dataset['contains_14k'].map( {True: 1, False: 0} ).astype(float)\n", "    \n", "    dataset['contains_unlocked'] = dataset['item_description'].str.contains(\"unlocked\")\n", "    dataset['contains_unlocked'] = dataset['contains_unlocked'].map( {True: 1, False: 0} ).astype(float)\n", "    \n", "    dataset['contains_carat'] = dataset['item_description'].str.contains(\"carat\")\n", "    dataset['contains_carat'] = dataset['contains_carat'].map( {True: 1, False: 0} ).astype(float)\n", "    \n", "    dataset['is_vitamix'] = dataset['brand_name'].str.contains(\"vitamix\")\n", "    dataset['is_vitamix'] = dataset['is_vitamix'].map( {True: 1, False: 0} ).astype(float)\n", "    \n", "    dataset['is_david_yurman'] = dataset['brand_name'].str.contains(\"david yurman\")\n", "    dataset['is_david_yurman'] = dataset['is_david_yurman'].map( {True: 1, False: 0} ).astype(float)\n", "    \n", "    dataset['is_hatchimal'] = dataset['name'].str.contains(\"hatchimal\")\n", "    dataset['is_hatchimal'] = dataset['is_hatchimal'].map( {True: 1, False: 0} ).astype(float)\n", "    \n", "    dataset['is_dockatot'] = dataset['name'].str.contains(\"dockatot\")\n", "    dataset['is_dockatot'] = dataset['is_dockatot'].map( {True: 1, False: 0} ).astype(float)   \n", "    \n", "\n", "    #####################################################################\n", "    dataset['category_main'] = dataset['category_main'].str.replace(' ','_')\n", "    dataset['category_sub1'] = dataset['category_sub1'].str.replace(' ','_')\n", "    dataset['category_sub2'] = dataset['category_sub2'].str.replace(' ','_')\n", "    \n", "    dataset['category_main'] = dataset['category_main'].str.replace('&','')\n", "    dataset['category_sub1'] = dataset['category_sub1'].str.replace('&','')\n", "    dataset['category_sub2'] = dataset['category_sub2'].str.replace('&','')\n", "    \n", "   \n", " \n", "\n", "train = pd.concat([train, pd.get_dummies(train.category_main, prefix_sep='', prefix='M_')], axis=1)\n", "test  = pd.concat([test , pd.get_dummies(test.category_main , prefix_sep='', prefix='M_')], axis=1)\n", "train = pd.concat([train, pd.get_dummies(train.item_condition_id, prefix_sep='', prefix='M_')], axis=1)\n", "test  = pd.concat([test , pd.get_dummies(test.item_condition_id , prefix_sep='', prefix='M_')], axis=1)\n", "\n", "#dataset = pd.concat([dataset, pd.get_dummies(dataset.category_sub1, prefix_sep='', prefix='S1_')], axis=1)\n", "#dataset = pd.concat([dataset, pd.get_dummies(dataset.category_sub2, prefix_sep='', prefix='S2_')], axis=1)\n", "    \n", "train.head(3)"], "outputs": []}, {"cell_type": "markdown", "metadata": {"collapsed": true, "_uuid": "98b535192c002d3c3bc6a9f03667516bc9a81c96", "_cell_guid": "bae295fd-b4fb-42f7-8989-19b699d0ce8a"}, "source": ["wc = WordCloud(background_color=\"white\", max_words=5000, \n", "               stopwords=STOPWORDS, max_font_size= 50)\n", "\n", "wc.generate(\" \".join(str(s) for s in train.item_description.values))\n", "\n", "plt.figure(figsize=(20,12))\n", "plt.axis('off')\n", "plt.imshow(wc, interpolation='bilinear')"]}, {"cell_type": "markdown", "metadata": {"collapsed": true, "_uuid": "32e70428c11cd7efee48b9741c9342f88cbc030c", "_cell_guid": "f38fff75-5d67-46e1-87ca-af6aa5ae2b5a"}, "source": ["train.groupby(['brand_name'])['price'].mean()"]}, {"cell_type": "code", "metadata": {"_uuid": "de3e786c8829cc7c0a5865ba84d17a8f6a7cbd89", "_cell_guid": "faaa3d24-fc56-4300-947e-690f79a36b7f"}, "execution_count": null, "source": ["# Feature Selection\n", "drop_elements = ['train_id', 'name', 'category_name', 'brand_name', 'item_description', 'category_main', 'category_sub1', 'category_sub2', 'item_condition_id']\n", "train = train.drop(drop_elements, axis = 1)\n", "drop_elements = ['test_id', 'name', 'category_name', 'brand_name', 'item_description', 'category_main', 'category_sub1', 'category_sub2', 'item_condition_id']\n", "test  = test.drop(drop_elements, axis = 1)\n", "\n", "train.head(3)"], "outputs": []}, {"cell_type": "code", "metadata": {"_uuid": "2febfdeeb69ff5044b8c284f374d9793a653a054", "_cell_guid": "b2383d70-4455-48ab-a4dd-35cdf4bce098"}, "execution_count": null, "source": ["print (train.info())"], "outputs": []}, {"cell_type": "code", "metadata": {"_uuid": "dae544a415091ef2f34863ac2d0e5ab0d6712352", "_cell_guid": "301b6dfe-800c-45a9-9d3e-8315645588ea"}, "execution_count": null, "source": ["full_data = [train, test]\n", "for dataset in full_data:   \n", "    dataset['item_description_len']   = dataset['item_description_len'].fillna(0)\n", "    dataset['contains_brand_new']     = dataset['contains_brand_new'].fillna(0).astype(int)\n", "    dataset['contains_free_shipping'] = dataset['contains_free_shipping'].fillna(0).astype(int)\n", "    dataset['contains_price_firm']    = dataset['contains_price_firm'].fillna(0).astype(int)\n", "    dataset['contains_rm']    = dataset['contains_rm'].fillna(0).astype(int)\n", "    dataset['has_brand']    = dataset['has_brand'].fillna(0).astype(int)\n", "    dataset['is_Adidas'] = dataset['is_Adidas'].fillna(0).astype(int)\n", "    dataset['is_American_Eagle'] = dataset['is_American_Eagle'].fillna(0).astype(int)\n", "    dataset['is_Apple'] = dataset['is_Apple'].fillna(0).astype(int)\n", "    dataset['is_Bath__Body_Works'] = dataset['is_Bath__Body_Works'].fillna(0).astype(int)\n", "    dataset['is_Coach'] = dataset['is_Coach'].fillna(0).astype(int)\n", "    dataset['is_Disney'] = dataset['is_Disney'].fillna(0).astype(int)\n", "    dataset['is_FOREVER_21'] = dataset['is_FOREVER_21'].fillna(0).astype(int)\n", "    dataset['is_Funko'] = dataset['is_Funko'].fillna(0).astype(int)\n", "    dataset['is_LuLaRoe'] = dataset['is_LuLaRoe'].fillna(0).astype(int)\n", "    dataset['is_Lululemon'] = dataset['is_Lululemon'].fillna(0).astype(int)\n", "    dataset['is_Michael_Kors'] = dataset['is_Michael_Kors'].fillna(0).astype(int)\n", "    dataset['is_Nike'] = dataset['is_Nike'].fillna(0).astype(int)\n", "    dataset['is_Nintendo'] = dataset['is_Nintendo'].fillna(0).astype(int)\n", "    dataset['is_Old_Navy'] = dataset['is_Old_Navy'].fillna(0).astype(int)\n", "    dataset['is_PINK'] = dataset['is_PINK'].fillna(0).astype(int)\n", "    dataset['is_Rae_Dunn'] = dataset['is_Rae_Dunn'].fillna(0).astype(int)\n", "    dataset['is_Sephora'] = dataset['is_Sephora'].fillna(0).astype(int)\n", "    dataset['is_Sony'] = dataset['is_Sony'].fillna(0).astype(int)\n", "    dataset['is_Under_Armour'] = dataset['is_Under_Armour'].fillna(0).astype(int)\n", "    dataset['is_Victoria_Secret'] = dataset['is_Victoria_Secret'].fillna(0).astype(int)\n", "    \n", "    dataset['contains_gb'] = dataset['contains_gb'].fillna(0).astype(int)\n", "    dataset['contains_14k'] = dataset['contains_14k'].fillna(0).astype(int)    \n", "    dataset['contains_unlocked'] = dataset['contains_unlocked'].fillna(0).astype(int) \n", "    dataset['contains_carat'] = dataset['contains_carat'].fillna(0).astype(int)\n", "    dataset['is_vitamix'] = dataset['is_vitamix'].fillna(0).astype(int)\n", "    dataset['is_david_yurman'] = dataset['is_david_yurman'].fillna(0).astype(int)\n", "    dataset['is_hatchimal'] = dataset['is_hatchimal'].fillna(0).astype(int)\n", "    dataset['is_dockatot'] = dataset['is_dockatot'].fillna(0).astype(int)\n", "\n", "\n", "print (train.info())\n", "#print(train.describe())\n"], "outputs": []}, {"cell_type": "code", "metadata": {"_uuid": "5af1467a83f445847c6e8c96e2e6b0274d4db890", "_cell_guid": "0a6eb3ed-d06b-4be0-9e3a-bdc8a7d60004"}, "execution_count": null, "source": ["print (test.info())\n", "#print(test.describe())"], "outputs": []}, {"cell_type": "code", "metadata": {"_uuid": "dc179f3b237d8f5be4e656f1b9c426ff58d92297", "_cell_guid": "22e49eb7-2802-4d62-b50f-91a6eb6fb64c"}, "execution_count": null, "source": ["np.sum(train)"], "outputs": []}, {"cell_type": "code", "metadata": {"_uuid": "0d64d404e8a44c5cb8a2dee5e495a0627b6679c2", "_cell_guid": "f52faeeb-4ac2-411a-90bd-1d8ff68b61a0"}, "execution_count": null, "source": ["# Feature Selection 2\n", "drop_elements = ['is_vitamix' ,'is_david_yurman','is_hatchimal','is_dockatot']\n", "train = train.drop(drop_elements, axis = 1)\n", "test  = test.drop(drop_elements, axis = 1)\n", "\n", "train.head(3)"], "outputs": []}, {"cell_type": "code", "metadata": {}, "execution_count": null, "source": ["full_data = [train_text_dtm, test_text_dtm]\n", "for dataset in full_data:  \n", "    for column in dataset:\n", "        #print(column)\n", "        dataset[column]   = dataset[column].fillna(0).astype(int)\n", "print(train_text_dtm.info())\n", "print(test_text_dtm.info())"], "outputs": []}, {"cell_type": "code", "metadata": {"_uuid": "f499a8b0f69a82d4fe961a05aef9a52b3afd65e3", "_cell_guid": "9a9ccb4f-22d8-4717-a1d1-f1df15934f5b"}, "execution_count": null, "source": ["from sklearn import preprocessing\n", "\n", "#yy_train = (np.log(train['price']+1))\n", "yy_train = train['price']\n", "x_train = (train.drop('price',axis=1))\n", "x_train = x_train.iloc[:, 0:].values\n", "\n", "x_train = pd.concat([pd.DataFrame(x_train).reset_index(drop=True), train_text_dtm.reset_index(drop=True)], axis=1)\n", "test = pd.concat([pd.DataFrame(test).reset_index(drop=True), test_text_dtm.reset_index(drop=True)], axis=1)\n", "\n", "print(x_train.shape)\n", "print(test.shape)\n", "print(x_train.info())\n", "print(test.info())\n", "\n", "\n", "# Feature Scaling\n", "from sklearn.preprocessing import StandardScaler\n", "sc = StandardScaler()\n", "x_train = sc.fit_transform(x_train)\n", "data_test = sc.transform(test)\n", "\n", "import numpy as np\n", "#x_train = np.delete(x_train, 4, axis=1)\n", "#data_test = np.delete(data_test, 4, axis=1)\n", "#x_train   = x_train.astype(int)\n", "#data_test = data_test.astype(int)\n", "\n", "data_train = x_train\n", "\n", "print(data_train.__class__.__name__)\n", "print(data_test.__class__.__name__)\n", "#print(pd.DataFrame(yy_train).describe())\n", "#print(pd.DataFrame(data_train).describe())\n", "#print(pd.DataFrame(data_test).describe())\n", "\n"], "outputs": []}, {"cell_type": "code", "metadata": {"collapsed": true, "_uuid": "0ce94ba596ed656fe23b20451a8b4b8344e44095", "_cell_guid": "1b1c5c6c-83e2-4777-9670-521ab209a282"}, "execution_count": null, "source": ["data_test"], "outputs": []}, {"cell_type": "code", "metadata": {"_uuid": "4db52fd8ef882486fcfbee85132a915ab077c5a3", "_cell_guid": "2e0800e0-821d-4034-a77c-051544fdc2e5"}, "execution_count": null, "source": ["import numpy as np\n", "\n", "def rmsle(h, y): \n", "    \"\"\"\n", "    Compute the Root Mean Squared Log Error for hypthesis h and targets y\n", "\n", "    Args:\n", "        h - numpy array containing predictions with shape (n_samples, n_targets)\n", "        y - numpy array containing targets with shape (n_samples, n_targets)\n", "    \"\"\"\n", "    return np.sqrt(np.square(np.log(h + 1) - np.log(y + 1)).mean())\n", "print('ok')"], "outputs": []}, {"cell_type": "code", "metadata": {"_uuid": "fa6c93c2d424f7eed79f85a0cbc60307fef1ef20", "_cell_guid": "cd460aa0-950d-4bfc-bc0a-dd50d80601f3"}, "execution_count": null, "source": ["# Splitting the dataset into the Training set and Test set\n", "from sklearn.model_selection import train_test_split\n", "X_train, X_test, y_train, y_test = train_test_split(data_train, yy_train, test_size = 0.2, random_state = 0)\n", "print('ok')"], "outputs": []}, {"cell_type": "code", "metadata": {"_uuid": "be341ec499225ea4c456d9a40c1b7cd8fe4d31e4", "_cell_guid": "df7e31a0-5cf1-490f-9ac6-4c7744420b07"}, "execution_count": null, "source": ["from sklearn.linear_model import LinearRegression, SGDRegressor, RidgeCV, Lasso, LassoCV, ElasticNetCV\n", "from sklearn.linear_model import Ridge\n", "from sklearn.svm import SVR\n", "from sklearn.neural_network import MLPRegressor\n", "from sklearn.ensemble import RandomForestRegressor\n", "from sklearn.ensemble import ExtraTreesRegressor\n", "from sklearn.ensemble import GradientBoostingRegressor\n", "from sklearn.gaussian_process import GaussianProcessRegressor\n", "from sklearn.gaussian_process.kernels import RBF, Matern, RationalQuadratic, ExpSineSquared, DotProduct,ConstantKernel                                            \n", "                                              \n", "\n", "import tensorflow as tf\n", "import tensorflow.contrib.learn as learn\n", "#Some useful packages\n", "from sklearn.model_selection import KFold\n", "from sklearn.model_selection import cross_val_score\n", "from sklearn.model_selection import cross_val_predict\n", "from sklearn   import metrics\n", "from sklearn.model_selection import train_test_split\n", "\n", "def trainModels(model, X_train, y_train, X_test):\n", "    if(model == 'linear'):\n", "        cl = LinearRegression()   \n", "    if(model == 'SGD'):\n", "        cl= SGDRegressor()\n", "    if(model == 'Ridge'):\n", "        cl = Ridge(solver='auto',\n", "        fit_intercept=True,\n", "        alpha=0.5,\n", "        max_iter=100,\n", "        normalize=False,\n", "        tol=0.05)\n", "    if(model == 'RidgeCV'):\n", "        cl = RidgeCV()        \n", "    if(model == 'Lasso'):\n", "        cl = LassoCV()\n", "    if(model == 'ElasticNet'):\n", "        cl = ElasticNetCV()\n", "    if(model == 'SVR'):\n", "        cl = SVR(kernel='linear', C=1e4) \n", "    if(model == 'NeuralNet'):\n", "        cl = MLPRegressor(solver='lbfgs', \n", "                                       alpha=1e-5, \n", "                                       hidden_layer_sizes=(80,50,20), \n", "                                       random_state=1)\n", "    if(model == 'RandomForest'):\n", "        cl = RandomForestRegressor(n_estimators=500,oob_score=True, max_features = 5, max_depth = 3)\n", "    if(model == 'ExtraTrees'):\n", "        cl = ExtraTreesRegressor(n_estimators=500)\n", "        \n", "    if(model == 'GradientBoosting'):\n", "        cl = GradientBoostingRegressor(n_estimators=1000, \n", "                                                  learning_rate=0.05, \n", "                                                  min_samples_leaf=50, \n", "                                                  min_samples_split=20, \n", "                                                  loss='huber')\n", "\n", "    cl.fit(X_train, y_train)\n", "    pred = cl.predict(X_test)\n", "    #score = np.sqrt(metrics.mean_squared_error(y_test,pred))\n", "    score = rmsle(pred,y_test)\n", "    return score, cl\n", "\n", "print('OK')"], "outputs": []}, {"cell_type": "code", "metadata": {"scrolled": true, "_uuid": "5cace31e427f453c60962dad92ed779fdb56f682", "_cell_guid": "fc517a36-8c69-4805-948e-ad2009080064"}, "execution_count": null, "source": ["a = train.drop('price',axis=1).columns.tolist()\n", "b = train_text_dtm.columns.tolist()\n", "lista = a + b\n", "print(len(a))\n", "print(len(b))\n", "print(len(lista))\n"], "outputs": []}, {"cell_type": "code", "metadata": {"_uuid": "450d4c1ea53dd9017a01a344d21dc541441c1c61", "_cell_guid": "f268e89d-f79f-43b0-80f8-674bfd262ac6"}, "execution_count": null, "source": ["clf = ExtraTreesRegressor()\n", "clf = clf.fit(X_train, y_train)\n", "clf.feature_importances_  \n", "#model = SelectFromModel(clf, prefit=True)\n", "importances = clf.feature_importances_\n", "std = np.std([f.feature_importances_ for f in clf.estimators_], axis=0)\n", "indices = np.argsort(importances)[::-1]\n", "features = lista"], "outputs": []}, {"cell_type": "code", "metadata": {"_uuid": "f570a20782d205ecb12d209f215818950dd2d1aa", "_cell_guid": "eb592f71-e1ee-48a1-b1ed-4c6089555bb8"}, "execution_count": null, "source": ["print(len(indices))\n", "print(len(features))\n", "print(len(importances))"], "outputs": []}, {"cell_type": "code", "metadata": {"_uuid": "a5fcb8231f365f782b664df2973b74f71c8f1aa4", "_cell_guid": "f16f2b4d-8118-4398-828e-1909e1c1cfea"}, "execution_count": null, "source": ["# Print the feature ranking\n", "print(\"Feature ranking:\")\n", "\n", "cum_imp = 0\n", "sel_features = list()\n", "sel_features_idx = list()\n", "for f in range(X_train.shape[1]):\n", "    cum_imp = cum_imp + importances[indices[f]]\n", "    print('rank '+(str(f)) + \" var: \"+ (str(indices[f])) + \" \"+ features[indices[f]] + \"--\\t\\t\" + str(importances[indices[f]]) + \"--\\t\" + str(cum_imp))\n", "    if(cum_imp <= 0.8):\n", "        sel_features.append(features[indices[f]])\n", "        sel_features_idx.append(indices[f])\n", "\n", "plt.figure()\n", "plt.title(\"Feature importances\")\n", "plt.bar(range(X_train.shape[1]), importances[indices],\n", "       color=\"r\", yerr=std[indices], align=\"center\")\n", "plt.xticks(range(X_train.shape[1]), indices)\n", "plt.xlim([-1, X_train.shape[1]])\n", "plt.show()\n", "\n", "print(sel_features)\n", "print(sel_features_idx)"], "outputs": []}, {"cell_type": "code", "metadata": {"_uuid": "3a3874d2cdcb9b6c2105e8d9407b8d8c6971b5f4", "_cell_guid": "485f37fa-c695-439d-a564-af549465df84"}, "execution_count": null, "source": ["# Feature Selection 3\n", "X_train    = X_train[:,sel_features_idx]\n", "X_test     = X_test[:,sel_features_idx]\n", "data_test = data_test[:,sel_features_idx]\n", "\n", "X_train"], "outputs": []}, {"cell_type": "code", "metadata": {"scrolled": false, "_uuid": "bada13dafe3389629e5cf737da66b66c9b3ca028", "_cell_guid": "a4fab774-e852-4b43-8e0d-d93416803680"}, "execution_count": null, "source": ["s, cl_linear = trainModels('linear', X_train, y_train, X_test)\n", "print('linear:\\t' + str(s))\n", "s, cl_SGD = trainModels('SGD', X_train, y_train, X_test)\n", "print('SGD:\\t' + str(s))\n", "s, cl_Ridge = trainModels('Ridge', X_train, y_train, X_test)\n", "print('Ridge:\\t' + str(s))\n", "s, cl_RidgeCV = trainModels('RidgeCV', X_train, y_train, X_test)\n", "print('RidgeCV:\\t' + str(s))\n", "s, cl_Lasso = trainModels('Lasso', X_train, y_train, X_test)\n", "print('Lasso:\\t' + str(s))\n", "s, cl_ElasticNet = trainModels('ElasticNet', X_train, y_train, X_test)\n", "print('ElasticNet:\\t' + str(s))\n", "#s, cl_SVR = trainModels('SVR', X_train, y_train, X_test)\n", "#print('SVR:\\t' + str(s))\n", "s, cl_NeuralNet = trainModels('NeuralNet', X_train, y_train, X_test)\n", "print('NeuralNet:\\t' + str(s))\n", "s, cl_RandomForest = trainModels('RandomForest', X_train, y_train, X_test)\n", "print('RandomForest:\\t' + str(s))\n", "s, cl_ExtraTrees = trainModels('ExtraTrees', X_train, y_train, X_test)\n", "print('ExtraTrees:\\t' + str(s))\n", "s, cl_GradientBoosting = trainModels('GradientBoosting', X_train, y_train, X_test)\n", "print('GradientBoosting:\\t' + str(s))\n", "\n", "\n"], "outputs": []}, {"cell_type": "code", "metadata": {"_uuid": "8af22ec60b1982e336129f4d66fc070fef89b47a", "_cell_guid": "fd6b91e1-f13f-429d-8a57-44a7e1a6146f"}, "execution_count": null, "source": ["candidate_regressor = cl_GradientBoosting\n", "candidate_regressor.fit(X_train, y_train)\n", "pred = candidate_regressor.predict(X_test)\n", "#y_test = np.exp(y_test)-1\n", "#pred = np.exp(pred)-1\n", "score = rmsle(pred,y_test)\n", "print(score)\n", "result = candidate_regressor.predict(data_test)\n", "\n", "\n", "# Generate Submission File \n", "Submission = pd.DataFrame({ 'test_id': testId, 'price': result })\n", "\n", "print(Submission['price'].mean())\n", "print(Submission.head(3))\n"], "outputs": []}, {"cell_type": "code", "metadata": {"_uuid": "8c9fb2e426f70ac05747667f5449002183068ac7", "_cell_guid": "872acdec-f5b7-4cf4-be50-7b07be1e3f80"}, "execution_count": null, "source": ["Submission.to_csv(\"Submission.csv\", index=False)\n", "print('OK')"], "outputs": []}, {"cell_type": "code", "metadata": {"collapsed": true, "_uuid": "5bfcea7191905398ea304f99d784a8950d3f0539", "_cell_guid": "7bb6ed3e-c9ca-4796-b974-6847c130341e"}, "execution_count": null, "source": ["print(X_train.shape[0])\n", "print(X_train.shape[1])"], "outputs": []}, {"cell_type": "code", "metadata": {"scrolled": false, "collapsed": true, "_uuid": "5005059c80bf1e1d1e42458816ac6a0435248398", "_cell_guid": "40513f28-1643-41b9-8b77-c6c9fbc1f2bc"}, "execution_count": null, "source": ["from numpy import array\n", "from matplotlib import pyplot\n", "\n", "from keras.models import Sequential\n", "from keras.layers import Dense\n", "from keras import backend\n", "from keras.layers import Dropout\n", " \n", "def rmse_k(y_true, y_pred):\n", "    return backend.sqrt(backend.mean(backend.square(y_pred - y_true), axis=-1))\n", "\n", "def rmsle_k(y_true, y_pred):\n", "    return backend.sqrt(backend.mean(backend.square(backend.log(y_pred + 1) - backend.log(y_true + 1)), axis=-1))\n", "\n", "# create model\n", "drop = 0.3\n", "model = Sequential()\n", "model.add(Dense(units = 12, input_dim=12, kernel_initializer = 'uniform', activation='relu'))\n", "model.add(Dropout(drop))\n", "model.add(Dense(units = 10, kernel_initializer = 'uniform', activation='relu'))\n", "model.add(Dropout(drop))\n", "model.add(Dense(units = 8, kernel_initializer = 'uniform', activation = 'relu'))\n", "model.add(Dropout(drop))\n", "model.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu'))\n", "model.add(Dropout(drop))\n", "model.add(Dense(units = 1,  kernel_initializer = 'uniform'))\n", "model.compile(loss='mse', optimizer='adam', metrics=[rmsle_k])\n", "# train model\n", "history = model.fit(X_train, y_train, epochs=30, batch_size=1000, verbose=0)\n", "# plot metrics\n", "pyplot.plot(history.history['rmsle_k'])\n", "pyplot.show()"], "outputs": []}, {"cell_type": "code", "metadata": {"scrolled": true, "collapsed": true, "_uuid": "b8bf03681b9d2f985a3dd80c340054407524e8fa", "_cell_guid": "a1a3d12c-7f3c-4e03-9cf8-893b2dadef1c"}, "execution_count": null, "source": ["pred = model.predict(X_test)\n", "pred = backend.cast_to_floatx(pred)\n", "price_test = backend.cast_to_floatx(y_test)\n", "#pred = np.exp(pred) -1\n", "#price_test = np.exp(price_test) -1\n", "\n", "score = rmsle(pred,price_test)\n", "print(score)\n", "\n", "#result = model.predict(data_test)\n", "#result = result[:,0]\n", "\n", "# Generate Submission File \n", "#Submission = pd.DataFrame({ 'test_id': testId, 'price': result })\n", "\n", "#print(Submission['price'].mean())\n", "#print(Submission.head(3))"], "outputs": []}, {"cell_type": "code", "metadata": {"collapsed": true, "_uuid": "47aada433e90ae14ed0847e348cec482ce3b8bce", "_cell_guid": "90fe323c-97d6-4159-a2b3-af8c6968e771"}, "execution_count": null, "source": ["#GradientBoostingRegressor OPTIMIZATION\n", "from sklearn.ensemble import GradientBoostingRegressor\n", "#from sklearn.model_selection import cross_validation, metrics   #Additional scklearn functions\n", "from sklearn.model_selection  import GridSearchCV   #Perforing grid search\n", "\n", "def rmsle_gd(y_true, y_pred): return np.sqrt(np.square(np.log(y_pred + 1) - np.log(y_true + 1)).mean()) \n", "\n", "parametros = {'n_estimators':      [1000],\n", "              'min_samples_leaf':  [50],\n", "              'min_samples_split': [20,50]\n", "              #'learning_rate':     [0.05, 0.1]\n", "             }\n", "print(parametros)\n", "\n", "custom_score = 'neg_mean_squared_error'\n", "\n", "gsearch1 = GridSearchCV(estimator = \n", "                        GradientBoostingRegressor(loss='huber'), \n", "                            param_grid = parametros,\n", "                            n_jobs= 1,\n", "                            iid=False,\n", "                            scoring=custom_score,\n", "                            cv=3)\n", "#gsearch1.fit(X_train, y_train)\n", "\n", "\n"], "outputs": []}, {"cell_type": "code", "metadata": {"collapsed": true, "_uuid": "e43d01b9a5b44573bd04e62f7882d8140acdcdb9", "_cell_guid": "e58a31ff-c777-4a44-bba0-f07b929db380"}, "execution_count": null, "source": ["#best_parameters = gsearch1.best_params_\n", "#best_accuracy = gsearch1.best_score_\n", "\n", "#print(best_parameters)\n", "#print(best_accuracy)\n", "#gsearch1.grid_scores_"], "outputs": []}, {"cell_type": "markdown", "metadata": {"_uuid": "6ed57aebd845089926b3c4a65cb6a3408eb43f2a", "_cell_guid": "80feae8b-5e8f-4e65-8b07-77b2c030f9fb"}, "source": ["[mean: -1296.47879, std: 80.36403, params: {'n_estimators': 100},\n", " mean: -1282.70548, std: 78.57870, params: {'n_estimators': 200},\n", " mean: -1273.14318, std: 77.92016, params: {'n_estimators': 1000}]\n", " [mean: -1274.88992, std: 79.04721, params: {'min_samples_leaf': 25, 'n_estimators': 1000},\n", " mean: -1271.36757, std: 78.19887, params: {'min_samples_leaf': 50, 'n_estimators': 1000}]\n", " \n", " "]}, {"cell_type": "code", "metadata": {"collapsed": true, "_uuid": "8fde4395b064a4a051289c71e9de9a3e7d517605", "_cell_guid": "21ab0e67-927e-4757-8cac-7c9f3659f8f3"}, "execution_count": null, "source": ["from numpy import array\n", "from matplotlib import pyplot\n", "\n", "from keras.models import Sequential\n", "from keras.layers import Dense\n", "from keras import backend\n", "from keras.layers import Dropout\n", "from keras.wrappers.scikit_learn import KerasClassifier\n", "from sklearn.model_selection  import GridSearchCV \n", "# Importing the Keras libraries and packages\n", "from keras.layers import Conv2D\n", "from keras.layers import MaxPooling2D\n", "from keras.layers import Flatten\n", "from keras.layers import Conv1D, MaxPooling1D\n", "    \n", "def rmse_k(y_true, y_pred):\n", "    return backend.sqrt(backend.mean(backend.square(y_pred - y_true), axis=-1))\n", "\n", "def rmsle_k(y_true, y_pred):\n", "    return backend.sqrt(backend.mean(backend.square(backend.log(y_pred + 1) - backend.log(y_true + 1)), axis=-1))\n", "\n", "# create model\n", "def build_classifier(drop, optimizer):\n", "    model = Sequential()\n", "    model.add(Dense(units = 40, input_dim=44, kernel_initializer = 'uniform', activation='relu'))\n", "    model.add(Dropout(drop))\n", "    model.add(Dense(units = 20, kernel_initializer = 'uniform', activation='relu'))\n", "    model.add(Dropout(drop))\n", "    model.add(Dense(units = 15, kernel_initializer = 'uniform', activation = 'relu'))\n", "    model.add(Dropout(drop))\n", "    model.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu'))\n", "    model.add(Dropout(drop))\n", "    model.add(Dense(units = 1,  kernel_initializer = 'uniform'))\n", "    model.compile(loss='mse', optimizer=optimizer, metrics=[rmsle_k])\n", "    return model\n", "\n", "# 118603\n", "# 44\n", "X_train1 = np.expand_dims(X_train, axis=2) # reshape (569, 30) to (569, 30, 1) \n", "X_test1  = np.expand_dims(X_test, axis=2) # reshape (569, 30) to (569, 30, 1) \n", "def build_classifier_CNN(drop, optimizer):\n", "    classifier = Sequential()\n", "    classifier.add(Conv1D(44, (5), input_shape = (44,1), activation = 'relu'))\n", "    classifier.add(Dropout(drop))\n", "    classifier.add(MaxPooling1D(pool_size = (3)))\n", "    classifier.add(Conv1D(20, (5), activation = 'relu'))\n", "    classifier.add(MaxPooling1D(pool_size = (3)))\n", "    classifier.add(Flatten())\n", "    classifier.add(Dense(units = 15, activation = 'relu'))\n", "    classifier.add(Dropout(drop))\n", "    classifier.add(Dense(units = 10, activation = 'relu'))\n", "    classifier.add(Dense(units = 1,  kernel_initializer = 'uniform'))\n", "    classifier.compile(optimizer = optimizer, loss = 'msle', metrics = [rmsle_k])\n", "    return classifier\n", "\n", "\n", "classifier = KerasClassifier(build_fn = build_classifier)\n", "classifier_CNN = KerasClassifier(build_fn = build_classifier_CNN)\n", "\n", "parameters = {'batch_size': [5000], #, 10000],\n", "              'epochs': [30],#, 40],\n", "              'optimizer': ['adam'],#, 'rmsprop'],\n", "              'drop': [0.2]#,0.3]\n", "             }\n", "#'rmsprop'\n", "custom_score = 'neg_mean_squared_log_error'\n", "\n", "grid_search = GridSearchCV(estimator = classifier_CNN,\n", "                           param_grid = parameters,\n", "                           scoring = custom_score,\n", "                           cv = 3)\n", "#grid_search = grid_search.fit(X_train1, y_train)\n", "best_parameters = grid_search.best_params_\n", "best_accuracy = grid_search.best_score_\n", "best_parameters\n", "best_accuracy"], "outputs": []}, {"cell_type": "code", "metadata": {"collapsed": true, "_uuid": "c2374beb7e5ccd145fb7124fec73b5c4bbb4a083", "_cell_guid": "1fc6ae02-66e2-4acc-a81a-f2056bb8168e"}, "execution_count": null, "source": ["best_parameters = grid_search.best_params_\n", "best_accuracy = grid_search.best_score_\n", "\n", "print(best_parameters)\n", "print(best_accuracy)\n", "grid_search.grid_scores_"], "outputs": []}, {"cell_type": "code", "metadata": {"scrolled": true, "collapsed": true, "_uuid": "7ae973d3b92eea93a28ebe3350817190805fee8d", "_cell_guid": "6a469be2-8cae-44b4-9585-6e131f5b9e23"}, "execution_count": null, "source": ["build_fn = build_classifier_CNN(drop = 0.2,optimizer = 'adam')\n", "cl_CNN = KerasClassifier(build_fn)\n", "history = cl_CNN.fit(X_train1, y_train, epochs=30, batch_size=2000, verbose=2,  validation_data = (X_test1, y_test))\n", "pyplot.plot(history.history['rmsle_k'])\n", "pyplot.show()"], "outputs": []}, {"cell_type": "code", "metadata": {"collapsed": true, "_uuid": "2de6662c1c1e38e477664c85392d46916baa7d6a", "_cell_guid": "959a3d44-7883-412f-bd30-36b9a7a4a126"}, "execution_count": null, "source": ["pred = cl_CNN.predict(X_train1)\n", "print(pd.DataFrame(pred).describe())\n", "pred = backend.cast_to_floatx(pred)\n", "price_test = backend.cast_to_floatx(y_train)\n", "print(pred)\n", "score = rmsle(pred,price_test)\n", "print(score)\n", "\n", "#data_test1  = np.expand_dims(data_test, axis=2) # reshape (569, 30) to (569, 30, 1) \n", "#result = classifier_CNN.predict(data_test1)\n", "#result = result[:,0]\n", "\n", "# Generate Submission File \n", "#Submission = pd.DataFrame({ 'test_id': testId, 'price': result })\n", "\n", "#print(Submission['price'].mean())\n", "#print(Submission.head(3))"], "outputs": []}], "nbformat": 4}