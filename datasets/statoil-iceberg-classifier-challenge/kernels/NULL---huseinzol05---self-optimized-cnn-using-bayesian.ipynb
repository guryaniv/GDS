{"metadata": {"language_info": {"codemirror_mode": {"version": 3, "name": "ipython"}, "name": "python", "file_extension": ".py", "version": "3.6.3", "mimetype": "text/x-python", "pygments_lexer": "ipython3", "nbconvert_exporter": "python"}, "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}}, "nbformat_minor": 1, "cells": [{"metadata": {"_uuid": "0d99a3cf087b8bf9c32b3cf40829ea14e4174cf6", "_cell_guid": "6cf9684f-771d-464c-9cf5-30bf73ea8f64"}, "execution_count": null, "cell_type": "code", "source": ["import pandas as pd\n", "import tensorflow as tf\n", "from bayes_opt import BayesianOptimization\n", "from sklearn.cross_validation import train_test_split\n", "import numpy as np"], "outputs": []}, {"metadata": {}, "execution_count": null, "cell_type": "code", "source": ["df = pd.read_json('../input/train.json')\n", "df.inc_angle = df.inc_angle.replace('na', 0)\n", "df.inc_angle = df.inc_angle.astype(float).fillna(0.0)"], "outputs": []}, {"metadata": {}, "execution_count": null, "cell_type": "code", "source": ["x_band1 = np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in df[\"band_1\"]])\n", "x_band2 = np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in df[\"band_2\"]])\n", "X_train = np.concatenate([x_band1[:, :, :, np.newaxis]\n", "                          , x_band2[:, :, :, np.newaxis]\n", "                         , ((x_band1+x_band1)/2)[:, :, :, np.newaxis]], axis=-1)\n", "X_angle_train = np.array(df.inc_angle)\n", "y_train = np.array(df[\"is_iceberg\"])"], "outputs": []}, {"metadata": {}, "execution_count": null, "cell_type": "code", "source": ["# just take 100 dataset to do optimization\n", "# we assume this 100 able to generalize the whole dataset\n", "# if not enough, increase the number\n", "X_train = X_train[:100]\n", "y_train = y_train[:100]\n", "X_angle_train = X_angle_train[:100].reshape((-1, 1))\n", "y_onehot = np.zeros((y_train.shape[0], np.unique(y_train).shape[0]))\n", "for i in range(y_train.shape[0]):\n", "    y_onehot[i, y_train[i]] = 1.0\n", "    \n", "x_train, x_test, y_train, y_test, x_train_angle, x_test_angle = train_test_split(X_train, y_onehot, X_angle_train, test_size = 0.20)"], "outputs": []}, {"metadata": {}, "execution_count": null, "cell_type": "code", "source": ["def neural_network(fully_conn_size, # wide size of fully connected layer\n", "                   len_layer_conv, # each conv layer included max pooling\n", "                   kernel_size, # kernel size for conv process\n", "                   learning_rate, # learning rate\n", "                   pooling_size, # kernel and stride size for pooling\n", "                   multiply, # constant to multiply for conv output\n", "                   dropout_rate, # dropout\n", "                   beta, # l2 norm discount\n", "                   activation,\n", "                   batch_normalization, \n", "                   batch_size = 20):\n", "    \n", "    tf.reset_default_graph()\n", "    if activation == 0:\n", "        activation = tf.nn.sigmoid\n", "    elif activation == 1:\n", "        activation = tf.nn.tanh\n", "    else:\n", "        activation = tf.nn.relu\n", "    \n", "    def conv_layer(x, conv, out_shape):\n", "        w = tf.Variable(tf.truncated_normal([conv, conv, int(x.shape[3]), out_shape]))\n", "        b = tf.Variable(tf.truncated_normal([out_shape], stddev = 0.01))\n", "        return tf.nn.conv2d(x, w, [1, 1, 1, 1], padding = 'SAME') + b\n", "    \n", "    def fully_connected(x, out_shape):\n", "        w = tf.Variable(tf.truncated_normal([int(x.shape[1]), out_shape]))\n", "        b = tf.Variable(tf.truncated_normal([out_shape], stddev = 0.01))\n", "        return tf.matmul(x, w) + b\n", "    \n", "    def pooling(x, k = 2, stride = 2):\n", "        return tf.nn.max_pool(x, ksize = [1, k, k, 1], \n", "                              strides = [1, stride, stride, 1], \n", "                              padding = 'SAME')\n", "    \n", "    X_img = tf.placeholder(tf.float32, (None, 75, 75, 3))\n", "    X_angle = tf.placeholder(tf.float32, (None, 1))\n", "    Y = tf.placeholder(tf.float32, (None, y_onehot.shape[1]))\n", "    # for batch normalization\n", "    train = tf.placeholder(tf.bool)\n", "    \n", "    for i in range(len_layer_conv):\n", "        if i == 0:\n", "            conv = activation(conv_layer(X_img, kernel_size, \n", "                                         int(np.around(int(X_img.shape[3]) * multiply))))\n", "        else:\n", "            conv = activation(conv_layer(conv, kernel_size, \n", "                                         int(np.around(int(conv.shape[3]) * multiply))))\n", "        conv = pooling(conv, k = pooling_size, stride = pooling_size)\n", "        if batch_normalization:\n", "            conv = tf.layers.batch_normalization(conv, training = train)\n", "        conv = tf.nn.dropout(conv, dropout_rate)\n", "    print(conv.shape)\n", "    output_shape = int(conv.shape[1]) * int(conv.shape[2]) * int(conv.shape[3])\n", "    conv = tf.reshape(conv, [-1, output_shape])\n", "    conv = tf.concat([conv, X_angle], axis = 1)\n", "    \n", "    # our fully connected got 1 layer\n", "    # you can increase it if you want\n", "    for i in range(1):\n", "        if i == 0:\n", "            fc = activation(fully_connected(conv, fully_conn_size))\n", "        else:\n", "            fc = activation(fully_connected(fc, fully_conn_size))\n", "        fc = tf.nn.dropout(fc, dropout_rate)\n", "        if batch_normalization:\n", "            fc = tf.layers.batch_normalization(fc, training = train)\n", "            \n", "    logits = fully_connected(fc, y_onehot.shape[1])\n", "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = Y, \n", "                                                                  logits = logits))\n", "    cost += sum(beta * tf.nn.l2_loss(tf_var) for tf_var in tf.trainable_variables())\n", "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n", "    correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(Y, 1))\n", "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n", "    \n", "    sess = tf.InteractiveSession()\n", "    sess.run(tf.global_variables_initializer())\n", "    COST, TEST_COST, ACC, TEST_ACC = [], [], [], []\n", "    \n", "    for i in range(20):\n", "        train_acc, train_loss = 0, 0\n", "        for n in range(0, (X_train.shape[0] // batch_size) * batch_size, batch_size):\n", "            _, loss = sess.run([optimizer, cost], \n", "                               feed_dict = {X_img: x_train[n: n + batch_size, :, :, :],\n", "                                            X_angle: x_train_angle[n: n + batch_size],\n", "                                            Y: y_train[n: n + batch_size, :], \n", "                                            train: True})\n", "            train_acc += sess.run(accuracy, \n", "                                  feed_dict = {X_img: x_train[n: n + batch_size, :, :, :],\n", "                                               X_angle: x_train_angle[n: n + batch_size],\n", "                                               Y: y_train[n: n + batch_size, :], \n", "                                               train: False})\n", "            train_loss += loss\n", "        results = sess.run([cost, accuracy], \n", "                           feed_dict = {X_img: x_test,\n", "                                        X_angle: x_test_angle,\n", "                                        Y: y_test, \n", "                                        train: False})\n", "        TEST_COST.append(results[0])\n", "        TEST_ACC.append(results[1])\n", "        train_loss /= (x_train.shape[0] // batch_size)\n", "        train_acc /= (x_train.shape[0] // batch_size)\n", "        ACC.append(train_acc)\n", "        COST.append(train_loss)\n", "    COST = np.array(COST).mean()\n", "    TEST_COST = np.array(TEST_COST).mean()\n", "    ACC = np.array(ACC).mean()\n", "    TEST_ACC = np.array(TEST_ACC).mean()\n", "    return COST, TEST_COST, ACC, TEST_ACC"], "outputs": []}, {"metadata": {"collapsed": true}, "execution_count": null, "cell_type": "code", "source": ["def generate_nn(fully_conn_size, len_layer_conv, kernel_size, learning_rate, pooling_size, multiply,\n", "                dropout_rate, beta, activation, batch_normalization):\n", "    global accbest\n", "    param = {\n", "        'fully_conn_size' : int(np.around(fully_conn_size)),\n", "        'len_layer_conv' : int(np.around(len_layer_conv)),\n", "        'kernel_size': int(np.around(kernel_size)),\n", "        'learning_rate' : max(min(learning_rate, 1), 0.0001),\n", "        'pooling_size': int(np.around(pooling_size)),\n", "        'multiply': multiply,\n", "        'dropout_rate' : max(min(dropout_rate, 0.99), 0),\n", "        'beta' : max(min(beta, 0.5), 0.000001),\n", "        'activation': int(np.around(activation)),\n", "        'batch_normalization' : int(np.around(batch_normalization))\n", "    }\n", "    learning_cost, valid_cost, learning_acc, valid_acc = neural_network(**param)\n", "    print(\"stop after 20 iteration with train cost %f, valid cost %f, train acc %f, valid acc %f\" % (learning_cost, valid_cost, learning_acc, valid_acc))\n", "    # a very simple benchmark, just use correct accuracy\n", "    # if you want to change to f1 score or anything else, can\n", "    if (valid_acc > accbest):\n", "        costbest = valid_acc\n", "    return valid_acc"], "outputs": []}, {"metadata": {}, "execution_count": null, "cell_type": "code", "source": ["accbest = 0.0\n", "NN_BAYESIAN = BayesianOptimization(generate_nn, \n", "                              {'fully_conn_size': (16, 128),\n", "                               'len_layer_conv': (3, 5),\n", "                               'kernel_size': (2, 7),\n", "                               'learning_rate': (0.0001, 1),\n", "                               'pooling_size': (2, 4),\n", "                               'multiply': (1, 3),\n", "                               'dropout_rate': (0.1, 0.99),\n", "                               'beta': (0.000001, 0.49),\n", "                               'activation': (0, 2),\n", "                               'batch_normalization': (0, 1)\n", "                              })\n", "NN_BAYESIAN.maximize(init_points = 10, n_iter = 10, acq = 'ei', xi = 0.0)"], "outputs": []}, {"metadata": {"collapsed": true}, "execution_count": null, "cell_type": "code", "source": ["print('Maximum NN accuracy value: %f' % NN_BAYESIAN.res['max']['max_val'])\n", "print('Best NN parameters: ', NN_BAYESIAN.res['max']['max_params'])"], "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": ["The accuracy is low because our cnn model is very complex. the purpose is, bayesian still able to find the best maxima in non convex hyper-parameters function without do any derivation"]}, {"metadata": {"collapsed": true}, "execution_count": null, "cell_type": "code", "source": [], "outputs": []}], "nbformat": 4}