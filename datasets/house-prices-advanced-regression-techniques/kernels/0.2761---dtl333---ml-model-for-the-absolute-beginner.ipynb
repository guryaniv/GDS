{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"markdown","source":"A bit of a precursor before we begin. This shall be my first attempt at writing a very basic and simple walkthrough of a Machine Learning (ML) model on Kaggle. I hope for this kernel to be a well written and clear approach to ML, so much so that an absolute beginner may read through and understand what is going on. As such, this kernel will include little to no exploratory data analysis (EDA) nor feature engineering, but will focus more so on the absolute basics of training and running an ML model. Enjoy!!"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"To begin we will first load in our training and testing datasets that are conveniently given to us from Kaggle. We shall do this using Pandas, which is a very useful Python library that can be thought of as a more robust form of Microsoft Excel (Pandas dataframes (df) are very similar to spreadsheets in Excel). After loading in our train and test data, we will use pd.concat to combine our train and test datasets so that we can perform some data cleanup on both datasets at the same time. We will also drop 'SalePrice' from our training data as well as creating an housing_label dataframe with just SalePrice to use as our feature target (label) in our ML model to test how well our model performs. "},{"metadata":{"trusted":true,"_uuid":"d10ac44436ceb6838d018e4adfeec1a998be046a","collapsed":true},"cell_type":"code","source":"import pandas as pd \n\ntrain = pd.read_csv('../input/train.csv')\ntrain_df = train.drop('SalePrice', axis=1)\ntest_df = pd.read_csv('../input/test.csv')\nhousing_labels = train['SalePrice']\ndata_full = pd.concat([train_df, test_df])\n\ndata_full.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2a908cb7ed858f15b4b5fd5b4ea20a36db5a147d"},"cell_type":"markdown","source":"Now let us do some very basic data cleanup. First let us view which columns in our dataset have NaN values."},{"metadata":{"trusted":true,"_uuid":"a08621828917939925d7dc0d989178cd7c4cb09e","collapsed":true},"cell_type":"code","source":"data_NaN = data_full.isnull().sum()\ndata_NaN  = data_NaN[data_NaN>0]\ndata_NaN.sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"540013e2e20bf8c1f12a159145cc9cba5f26ea18"},"cell_type":"markdown","source":"We will now fill our columns that contain NaN values with 0 inplace of the NaN. This is very rudimentary for taking care of missing values, and in the future it is recommended to fill NaN values with something along the line of median or mean, as well as encoding any categorical data. But as this is a very basic walkthrough, I will be using a very basic method of filling in NaN values."},{"metadata":{"trusted":true,"_uuid":"5736458105362ba2127bf89c0d038eba9cbbcfae","collapsed":true},"cell_type":"code","source":"data_full['PoolQC'].fillna(0, inplace=True)        \ndata_full['MiscFeature'].fillna(0, inplace=True)\ndata_full['Alley'].fillna(0, inplace=True)   \ndata_full['Fence'].fillna(0, inplace=True)         \ndata_full['FireplaceQu'].fillna(0, inplace=True)     \ndata_full['LotFrontage'].fillna(0, inplace=True)      \ndata_full['GarageFinish'].fillna(0, inplace=True)     \ndata_full['GarageYrBlt'].fillna(0, inplace=True)     \ndata_full['GarageQual'].fillna(0, inplace=True)       \ndata_full['GarageCond'].fillna(0, inplace=True)      \ndata_full['GarageType'].fillna(0, inplace=True)       \ndata_full['BsmtExposure'].fillna(0, inplace=True)     \ndata_full['BsmtCond'].fillna(0, inplace=True)         \ndata_full['BsmtQual'].fillna(0, inplace=True)          \ndata_full['BsmtFinType2'].fillna(0, inplace=True)     \ndata_full['BsmtFinType1'].fillna(0, inplace=True)      \ndata_full['MasVnrType'].fillna(0, inplace=True)        \ndata_full['MasVnrArea'].fillna(0, inplace=True)        \ndata_full['MSZoning'].fillna(0, inplace=True)           \ndata_full['BsmtFullBath'].fillna(0, inplace=True)       \ndata_full['BsmtHalfBath'].fillna(0, inplace=True)       \ndata_full['Utilities'].fillna(0, inplace=True)          \ndata_full['Functional'].fillna(0, inplace=True)         \ndata_full['Exterior2nd'].fillna(0, inplace=True)        \ndata_full['Exterior1st'].fillna(0, inplace=True)        \ndata_full['SaleType'].fillna(0, inplace=True)           \ndata_full['BsmtFinSF1'].fillna(0, inplace=True)         \ndata_full['BsmtFinSF2'].fillna(0, inplace=True)         \ndata_full['BsmtUnfSF'].fillna(0, inplace=True)          \ndata_full['Electrical'].fillna(0, inplace=True)         \ndata_full['KitchenQual'].fillna(0, inplace=True)        \ndata_full['GarageCars'].fillna(0, inplace=True)         \ndata_full['GarageArea'].fillna(0, inplace=True)         \ndata_full['TotalBsmtSF'].fillna(0, inplace=True)        \n\ndata_full.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cd618541dae647c8d95ec1c0494d7be84d75eab4"},"cell_type":"markdown","source":"An extremely basic but useful data analysis tool to use is something know as correlation. A correlation value relates how likely one variable is to follow another variable, ranging from values -1 to 1. A value of -1 means that if our variable we are correlating against moves up, the variable with a -1 correlation(r) value will move in the opposite direction which is in this case move down. A 1 value would mean that both variables move in the same direction.  A correlation table is very useful to see relationships between different variables in our dataset. \n(We will be using our training dataset to view the correlation of features against our target, SalePrice)"},{"metadata":{"trusted":true,"_uuid":"b9d39c8269e282684fb36d99344e3b052360a164","collapsed":true},"cell_type":"code","source":"corr_matrix = train.corr()\ncorr_matrix['SalePrice'].sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2a997d56791e8b7a04b768afd412a1ce9afda47c"},"cell_type":"markdown","source":"Now lets create a Pipeline to transform our data so that we may use it with our ML model. A Pipeline simply uses our data as input and scales it and convert it to a numpy array as Scikit-Learn cannot use Pandas dataframes as inputs. We also must scale our data so that our numerical values are present on the same scale, which will help boost the performance of our ML model.  We will also create a DataSelector class which we will use to choose the relevant features we would like to use with our model. The code for this may seem overwhelming at first, especially for those new to coding, but please bare with me.  We will then use our pipeline to fit our data and are now ready for our ML model testing!!\n(As we have done no categorical encoding, you must choose features that are either int64 or float64, any columns with datatype=object will not work)\nFor our example I have chosen 3 basic features to use: LotArea, 1stFlrSF, 2ndFlrSF."},{"metadata":{"trusted":true,"_uuid":"ecbd416eb01a9b3b5b445863c52e4152d0b25a3f","collapsed":true},"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom warnings import filterwarnings\nfilterwarnings('ignore') #Use this to get rid of the DataConversion warning concerning converting int64 data to float64 data.\n\nclass DataSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, attribute_names):\n        self.attribute_names = attribute_names\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        return X[self.attribute_names].values\n\nfeatures = data_full[['LotArea', '1stFlrSF', '2ndFlrSF']]\nfeatures_selected = list(features)\n\npipeline = Pipeline([\n    ('selected_features', DataSelector(features_selected)),\n    ('scaler', StandardScaler())\n])\n\nhousing_prepared = pipeline.fit_transform(train_df)\nhousing_prepared","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dc5fe379523b529c3ea0c347ef4c8a55b619c4ef"},"cell_type":"markdown","source":"We are now ready to test some models! Since this is a regression task, we must use a regression ML model (instead of a classifier). The difference between regression and classification is that regression is used to predict a NUMBER(in this case SalePrice), while classification is used to predict a CLASS(e.g. \"Yes or No\", \"Alive or Dead\", etc.). \n\nFirst we must split our housing_prepared data into a training set(housing_prepared_train) and a testing testing set(housing_prepared_test). We do this so that we can test how well our models do on predicting sale price. The test_size parameter refers to our testing set being 20% of our full training set, letting us see which model performs best. \n\nTo use our ML models(Linear Regression, Decision Tree Regression, Random Forest Regression) we must fit our training data to our model. We then use this model to predict our SalePrice using our testing data. We will then use a performance metric to see how well our model predicts SalePrice.Our performance metric we will use is Root Mean Squared Error(RMSE). The model with the lowest RMSE will be the model we shall use."},{"metadata":{"trusted":true,"_uuid":"bccb0b9b0091a728370ec8c88305613b2aeb2460","collapsed":true},"cell_type":"code","source":"import numpy as np\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\n\nhousing_prepared_train, housing_prepared_test, housing_labels_train, housing_labels_test = train_test_split(housing_prepared, housing_labels, test_size=0.2, random_state=33)\n\n#LinearRegression\nlin_reg = LinearRegression()\nlin_reg.fit(housing_prepared_train, housing_labels_train)\nlin_housing_predictions = lin_reg.predict(housing_prepared_test)\nlin_mse = mean_squared_error(housing_labels_test, lin_housing_predictions)\nlin_rmse = np.sqrt(lin_mse)\n\n#DecisionTreeRegression\ntree_reg = DecisionTreeRegressor()\ntree_reg.fit(housing_prepared_train, housing_labels_train)\ntree_reg_predictions = tree_reg.predict(housing_prepared_test)\ntree_mse = mean_squared_error(housing_labels_test, tree_reg_predictions)\ntree_rmse = np.sqrt(tree_mse)\n\n#RandomForestRegression\nforest_reg = RandomForestRegressor()\nforest_reg.fit(housing_prepared_train, housing_labels_train)\nforest_reg_predictions = forest_reg.predict(housing_prepared_test)\nforest_mse = mean_squared_error(housing_labels_test, forest_reg_predictions)\nforest_rmse = np.sqrt(forest_mse)\n\nprint(lin_rmse)\nprint(tree_rmse)\nprint(forest_rmse)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"40118df534b0112e5e093bac8ff551e9d5682244"},"cell_type":"markdown","source":"Our Random Forest Regression model had the lowest RMSE score so that is the model we will use. We will now use our pipeline to prepare our test data and make our final predictions. After this we will create a new pandas dataframe that is ready for submission!!"},{"metadata":{"trusted":true,"_uuid":"e487e95df8f8b9ee683bd34a3aced47dd6be7ccc","collapsed":true},"cell_type":"code","source":"test_prepared = pipeline.fit_transform(test_df)\n\nfinal_model = RandomForestRegressor()\nfinal_model.fit(housing_prepared, housing_labels)\nfinal_predictions = final_model.predict(test_prepared)\n\nmy_submission = pd.DataFrame({'Id': test_df.Id, 'SalePrice': final_predictions})\nmy_submission.head()\n\nmy_submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"baff07cc25634515f760f2975607ce0c1796c039"},"cell_type":"markdown","source":"As this is my first kernel on Kaggle I would very much appreciate any feedback! I have only begun learning python and data science around a month ago and kaggle has been and extremely valuable learning resource. I know this kernel is very basic and lacking things such as GridSearchCV and CrossValidation, and in the future I will most likely create an updated kernel for these things, but for my first kernel I felt the need to create a very basic and simplistic model for beginners to utilize and hopefully learn from. Thank you and good luck Kaggling!"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}