{"cells":[{"metadata":{"_uuid":"52a347c52effb67ed05878443c805f13a26a9bba"},"cell_type":"markdown","source":"## Overview\n\nThe purpose of this kernel is to take a look at the data, come up with some insights, and attempt to create a predictive model or two. This notebook is still **very** raw - the first few EDA sections have been decently explored/formatted, but the latter predictive sections are completely uncommented. I hope to work on those as my, very limited, time permits.\n\n## Packages\n\nFirst, let's load a few useful Python packages. This section will keep growing in subsequent versions of this EDA."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport operator\n#import gc\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import describe\n%matplotlib inline\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom sklearn.metrics import mean_squared_error\nimport lightgbm as lgb\nimport xgboost as xgb","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"Now let us look at the input folder. Here we find all the relevant files for this competition."},{"metadata":{"trusted":true,"_uuid":"1ab7d66eb649513fb142f6aa3824e1d4929a5e04","collapsed":true},"cell_type":"code","source":"print(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"209b77aae6250801dd22b533d38cf742690a6da2"},"cell_type":"markdown","source":"We see that the input folder only contains three files ```train.csv```, ```test.csv```, and ```sample_submission.csv```. It seems that for this competition we don't have to do any complicated combination and mergers of files.\n\nNow let's import and take a glimpse at these files."},{"metadata":{"trusted":true,"_uuid":"3d8e44dd3752cbac8b2fb84c9b708e503d4edd34","collapsed":true},"cell_type":"code","source":"train_df = pd.read_csv('../input/train.csv')\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"31ca1bf89d2a41bec45105c80fb2af5fd13dcf67","collapsed":true},"cell_type":"code","source":"train_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1972cac45aef601a2a8c5bdf6c064d62b7e028bf","collapsed":true},"cell_type":"code","source":"train_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"033d47dce5196dc097f7424dc421b6608bc7086e","collapsed":true},"cell_type":"code","source":"train_df.isnull().values.sum(axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"198ff084f2b5849a1f995105f96e3ea00c37214a","collapsed":true},"cell_type":"code","source":"train_df_describe = train_df.describe()\ntrain_df_describe","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5d300fe9c66f5eece1d66afb45176867e9fb39d9"},"cell_type":"markdown","source":"We see that the train dataset is fairly small - just under 10,000 rows. It has 143 features, including the Id and Target. Total number of features that can be used for training is 141, which includes 8 float-valued, 130 integer-valued, and 4 object valued, which need to be converted to numerical values. We also see that there are 5 features with missing data, including 3 that are dominated by missing values."},{"metadata":{"trusted":true,"_uuid":"fc74cd7b754b05be7aa21ca2f8b2d3fcef2873d7","collapsed":true},"cell_type":"code","source":"test_df = pd.read_csv('../input/test.csv')\ntest_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bc37e86cb308d281c186ee8b4f19754cfed09890","collapsed":true},"cell_type":"code","source":"test_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9f185193e8627a21e0ecf804a00af1fe97db442c","collapsed":true},"cell_type":"code","source":"test_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9e263af4746de8e69fd1c825577a801fd1326ecb","collapsed":true},"cell_type":"code","source":"test_df.isnull().values.sum(axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e16aec7d61811ebd0c2adf7baf36a47cca271655","collapsed":true},"cell_type":"code","source":"test_df_describe = test_df.describe()\ntest_df_describe","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"02eff8ed09320ea4a511ea03ed7e7635d1075857"},"cell_type":"markdown","source":"The test set is almost 2.5 times larger than the train set. It also has 5 features with missing data.\n\nNow, let's take a look at the target. We want to see the distribution of target values in the train set."},{"metadata":{"trusted":true,"_uuid":"3d7d4430d2ea0b181e88ed179c44d28cf5131e4c","collapsed":true},"cell_type":"code","source":"plt.figure(figsize=(12, 5))\nplt.hist(train_df.Target.values, bins=4)\nplt.title('Histogram target counts')\nplt.xlabel('Count')\nplt.ylabel('Target')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"24714ef1ee315f0c4f9ff263664feb6b4bfd0d90","collapsed":true},"cell_type":"code","source":"np.unique(train_df.Target.values)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4ef2553c47c6d5824fb3bd62b0c1fdbd54d4d3c6"},"cell_type":"markdown","source":"We see that tehre are only 4 numerical values in for the target. Value 4 seems to dominate, with about 60% of all values. "},{"metadata":{"trusted":true,"_uuid":"ef85e77bcd154b950a14605c4ab2c180e022aaf9","collapsed":true},"cell_type":"code","source":"pd.value_counts(train_df.Target)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d944f9eac2e4debbe04820b9a0a9790253ef5696"},"cell_type":"markdown","source":"Now we want to subset the features, so that they don't include Id and target."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"78e31d1e5b29b4d41b398c661c93126ba930b319"},"cell_type":"code","source":"columns_to_use = train_df.columns[1:-1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9991744c21460213bb9b85665548ddff36bbad60","collapsed":true},"cell_type":"code","source":"columns_to_use","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a65daf804c2fd336b96e9c3057cce0d33e5c3271"},"cell_type":"markdown","source":"We'll set up the new variable ```y``` that will be our target variable for training."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"4831039baa583ad54584e6b2bc562e89f8f37144"},"cell_type":"code","source":"y = train_df['Target'].values-1","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e653a1aa620ffb5881adea6977c0cf883b9f3337"},"cell_type":"markdown","source":"Next, we'll combine the train and test sets, so we can consistenly label encode all the categorical features."},{"metadata":{"trusted":true,"_uuid":"39cd42dec88e2ea4dc46d48892da6ee897c2d577","collapsed":true},"cell_type":"code","source":"train_test_df = pd.concat([train_df[columns_to_use], test_df[columns_to_use]], axis=0)\ncols = [f_ for f_ in train_test_df.columns if train_test_df[f_].dtype == 'object']\n\nfor col in cols:\n    le = LabelEncoder()\n    le.fit(train_test_df[col].astype(str))\n    train_df[col] = le.transform(train_df[col].astype(str))\n    test_df[col] = le.transform(test_df[col].astype(str))\ndel le","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"18bf20ea3e399cb1d12b32296d172f09c15c5883"},"cell_type":"markdown","source":"Now is the time to build our first model. We'll use make a simple LGBM model."},{"metadata":{"trusted":true,"_uuid":"4a3e535d0befc10a1b89dfd01ad75bf585e75bc6","collapsed":true},"cell_type":"code","source":"train = lgb.Dataset(train_df[columns_to_use].astype('float'),y ,feature_name = \"auto\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"4adadeac5e0554eb775733ec672159f706df1f44"},"cell_type":"code","source":"params = {\n    'boosting_type': 'gbdt',\n    'objective': 'multiclass',\n    'metric': 'multi_logloss',\n    'max_depth': 5,\n    'num_leaves': 31,\n    'learning_rate': 0.05,\n    'feature_fraction': 0.9,\n    'bagging_fraction': 0.6,\n    'bagging_freq': 5,\n    'verbose': -1,\n    'num_threads': 6,\n    'lambda_l2': 1.0,\n    'min_gain_to_split': 0,\n    'num_class': len(np.unique(y)),\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bf25cffa52a56f35a85d5e2a6642421f379a978f","collapsed":true},"cell_type":"code","source":"clf = lgb.train(params,\n        train,\n        num_boost_round = 500,\n        verbose_eval=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bbe8d4ebc070e022f2b03536a1782a66d32c9fdf","collapsed":true},"cell_type":"code","source":"preds1 = clf.predict(test_df[columns_to_use])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"c54ee997b201da71ea62baa73b75120a928572d8"},"cell_type":"code","source":"xgb_params = {\n        'learning_rate': 0.1,\n        'n_estimators': 1000,\n        'max_depth': 5,\n        'min_child_weight': 1,\n        'gamma': 0,\n        'subsample': 0.9,\n        'colsample_bytree': 0.84,\n        'objective': 'multi:softprob',\n        'scale_pos_weight': 1,\n        'eval_metric': 'merror',\n        'silent': 1,\n        'verbose': False,\n        'num_class': 4,\n        'seed': 44}\n    \nd_train = xgb.DMatrix(train_df[columns_to_use].values.astype('float'), y)\nd_test = xgb.DMatrix(test_df[columns_to_use].values.astype('float'))\n    \nmodel = xgb.train(xgb_params, d_train, num_boost_round = 500, verbose_eval=100)\n                        \nxgb_pred = model.predict(d_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0d9353358eef5048cd064e46df3c7747f4640b03","collapsed":true},"cell_type":"code","source":"xgb_pred.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4be01edba411b1a74101a3ad4bc556ad0c32ceaf","collapsed":true},"cell_type":"code","source":"preds = 0.5*preds1 + 0.5*xgb_pred\n\npreds = np.argmax(preds, axis = 1) +1\npreds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6071a8cab9e56e511d228f58ee92ce73a3f42ab1","collapsed":true},"cell_type":"code","source":"sample_submission = pd.read_csv(\"../input/sample_submission.csv\")\nsample_submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"68703252f2f447bf25e373c291a97b0feb5874f1","collapsed":true},"cell_type":"code","source":"sample_submission['Target'] = preds\nsample_submission.to_csv('simple_lgbm_xgb_1.csv', index=False)\nsample_submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f6512383ce30d432823d7eadc5ede61944b16ae5","collapsed":true},"cell_type":"code","source":"np.mean(preds)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"69d9a2fffde412e87e27c6a474cf940934bdb8d9"},"cell_type":"markdown","source":"To be continued ..."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"26de9eb65a4146ca76ab3418c6df460752abc90f"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}