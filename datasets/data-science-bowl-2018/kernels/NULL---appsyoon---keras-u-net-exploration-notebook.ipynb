{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5"},"cell_type":"markdown","source":"# Introduction"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true},"cell_type":"markdown","source":"This is a notebook exploring [U-Net: Convolutional Networks for Biomedical Image Segmentation](https://arxiv.org/pdf/1505.04597.pdf) for the 2018 Data Science Bowl.\n\nThanks to [Kjetil Åmdal-Sævik's Keras U-Net starter - LB 0.277](https://www.kaggle.com/keegil/keras-u-net-starter-lb-0-277?scriptVersionId=2164855) for the inspiration.\n"},{"metadata":{"_cell_guid":"43e4460f-9c64-4234-a08d-f29f85da017d","_uuid":"ec37207bd2938a2fdd7cbd3a3ff94748788a2077","trusted":true,"collapsed":true},"cell_type":"code","source":"import os\nimport random\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\n\nfrom skimage.io import imread \nfrom skimage.transform import resize\nfrom skimage.morphology import label\n\nfrom keras.layers import Input, Conv2D, Lambda, MaxPooling2D, Conv2DTranspose, concatenate\nfrom keras.models import Model, load_model\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom keras.optimizers import Adam\nfrom keras.preprocessing.image import ImageDataGenerator","execution_count":26,"outputs":[]},{"metadata":{"_cell_guid":"b01c47b2-7219-4c09-be63-4d844995103d","_uuid":"072d83a16bcacdb21f9ffef70032d4b76f42061e","collapsed":true,"trusted":true},"cell_type":"code","source":"TRAIN_PATH = '../input/stage1_train/'\nTEST_PATH = '../input/stage1_test/'\n\nIMG_WIDTH = IMG_HEIGHT = 256\nIMG_CHANNELS = 3","execution_count":27,"outputs":[]},{"metadata":{"_uuid":"f1c610a5179f26d01756c2ec55ae6347f3c76708"},"cell_type":"markdown","source":"Stated in [U-Net: Convolutional Networks for Biomedical Image Segmentation](https://arxiv.org/pdf/1505.04597.pdf):\n\n> To minimize the overhead and make maximum use\nof the GPU memory, we favor large input tiles over a large batch size and hence\nreduce the batch to a single image.\n\nSo we'll adjust the epochs but stick to the batch size of 1."},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"72c11d822fe0a08c19882716105a482e38ad0c42"},"cell_type":"code","source":"NUM_EPOCHS = 30\nSTEPS_PER_EPOCH = 600 # there are 670 images\nBATCH_SIZE = 1","execution_count":28,"outputs":[]},{"metadata":{"_uuid":"a457fa0904126be41d55185a6702dce4bba13051"},"cell_type":"markdown","source":"# Preprocessing"},{"metadata":{"_cell_guid":"4a335735-ec1d-4a2c-a268-af1a5af90e0c","_uuid":"7cd8a6e248d047adb4962623a5e2488564197070","trusted":true},"cell_type":"code","source":"train_ids = next(os.walk(TRAIN_PATH))\n\nmask_count = 0\nfor train_id in train_ids[1]:\n    masks = next(os.walk(TRAIN_PATH + train_id + '/masks/'))[2]\n    mask_count += len(masks)\n\nprint('There are {} images.'.format(len(train_ids[1])))\nprint('There are {} masks.'.format(mask_count))\nprint('Average {} masks per image.'.format(mask_count // len(train_ids[1])))","execution_count":29,"outputs":[]},{"metadata":{"_cell_guid":"2ecc5d56-b650-4a00-815c-05e7eefa5895","_uuid":"52076bd3934ca7d54db07af11678b5bb2e738ef4","collapsed":true,"trusted":true},"cell_type":"code","source":"#input/\n#    stage1_train/\n#         image_id/\n#             images/\n#             masks/\n#         image_id/\n# ...\ntrain_ids = next(os.walk(TRAIN_PATH))[1]\ntest_ids = next(os.walk(TEST_PATH))[1]","execution_count":30,"outputs":[]},{"metadata":{"_cell_guid":"e852004c-7081-400c-b4cc-ea05ac982318","_uuid":"138d8c57625b5ade99fdb8daf311082375661ff2","trusted":true},"cell_type":"code","source":"print(train_ids[:5])","execution_count":31,"outputs":[]},{"metadata":{"_cell_guid":"b9fb4dfe-55c0-43a5-bc40-a5f2c99e7088","_uuid":"7c6f05e89ec7edf672cd0dafc2116ca7d8719065","trusted":true},"cell_type":"code","source":"train_X = np.zeros((len(train_ids), IMG_WIDTH, IMG_HEIGHT, IMG_CHANNELS), dtype=np.uint8)\ntrain_Y = np.zeros((len(train_ids), IMG_WIDTH, IMG_HEIGHT, 1), dtype=np.bool)\n\nprint('Preparing the training data...')\n\nfor index_, image_id in tqdm(enumerate(train_ids), total=len(train_ids)):\n    # base path\n    path = TRAIN_PATH + image_id\n    # getting the images\n    image_path = path + '/images/' + image_id + '.png'\n    image = imread(image_path)\n    resized = resize(image, (IMG_WIDTH, IMG_HEIGHT, IMG_CHANNELS), mode='constant', preserve_range=True)\n    train_X[index_] = resized\n    # getting the masks\n    complete_mask = np.zeros((IMG_WIDTH, IMG_HEIGHT, 1), dtype=np.bool)\n    for mask_id in next(os.walk(path + '/masks/'))[2]:\n        mask_path = path + '/masks/' + mask_id\n        mask = imread(mask_path)\n        resized_mask = resize(mask, (IMG_WIDTH, IMG_HEIGHT, 1), mode='constant', preserve_range=True)\n        # creating one mask for all the masks for this image\n        complete_mask = np.maximum(resized_mask, complete_mask)\n    train_Y[index_] = complete_mask\n    \nprint(train_X[:5])","execution_count":32,"outputs":[]},{"metadata":{"_uuid":"f239c24ddafea745eb950b898b1fb80637ac92ac"},"cell_type":"markdown","source":"> we primarily need shift and rotation invariance as well as\nrobustness to deformations and gray value variations"},{"metadata":{"trusted":true,"_uuid":"82c78eb3441226804045cb9145fa614ee8efdce1"},"cell_type":"code","source":"datagen = ImageDataGenerator(rotation_range=45, \n                             width_shift_range=0.25, \n                             height_shift_range=0.25, \n                             horizontal_flip=True, \n                             vertical_flip=True)\ndatagen.fit(train_X)","execution_count":33,"outputs":[]},{"metadata":{"_cell_guid":"2a7ed359-77ee-4baa-93f8-4e2e04bf66cb","_uuid":"b937f70933a7c65525fed46e0c63929ca31cf676","trusted":true},"cell_type":"code","source":"random_i = random.randint(0, len(train_ids)) \n\nplt.imshow(train_X[random_i])\nplt.show()\nplt.imshow(np.squeeze(train_Y[random_i]))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"369c05b1-0f53-4af0-b239-c9e99b9fa8b6","_uuid":"df1c7995e3e768990373a69059b45040effecdeb","trusted":true},"cell_type":"code","source":"test_ids = next(os.walk(TEST_PATH))[1]\nprint(test_ids[1][0])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"44aa3580-5eea-4d61-b54f-04dadffb5e85","_uuid":"a2bb53ebcfab0c9904bba2b9a3946a06175bdca1","trusted":true},"cell_type":"code","source":"test_X = np.zeros((len(test_ids), IMG_WIDTH, IMG_HEIGHT, IMG_CHANNELS), dtype=np.uint8)\n\ntest_image_sizes = [] # we are going to resize the predicted test images back to original size\n\nfor index_, test_id in tqdm(enumerate(test_ids), total=len(test_ids)):\n    image_path = TEST_PATH + test_id + '/images/' + test_id + '.png'\n    image = imread(image_path)\n    test_image_sizes.append((image.shape[0], image.shape[1]))\n    resized = resize(image, (IMG_WIDTH, IMG_HEIGHT, IMG_CHANNELS), mode='constant', preserve_range=True)\n    test_X[index_] = resized","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3bded08c-c5c8-4044-bead-177a64735974","_uuid":"dfbeb5cfdaac7ce312c595ec3611cfa5dd9231a7","trusted":true},"cell_type":"code","source":"random_i = random.randint(0, len(test_X))\n\nplt.imshow(test_X[random_i])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"1e19b26e-1271-471d-a65b-b4a918d34b23","_uuid":"47f4982d50c3fc66152dcfa7bf302cc044ea26f1"},"cell_type":"markdown","source":"# Training"},{"metadata":{"_cell_guid":"3bb86543-827f-47db-966c-8c32275b4104","_uuid":"d95cae9993a29e7c602a9195224b63fb5acbd7ac","collapsed":true,"trusted":true},"cell_type":"code","source":"inputs = Input((IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS))\ns = Lambda(lambda x: x / 255) (inputs)\n\nconv_1 = Conv2D(16, (3, 3), activation='relu', padding='same')(inputs)\nconv_1 = Conv2D(16, (3, 3), activation='relu', padding='same')(conv_1)\npool_1 = MaxPooling2D((2, 2))(conv_1)\n\nconv_2 = Conv2D(32, (3, 3), activation='relu', padding='same')(pool_1)\nconv_2 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv_2)\npool_2 = MaxPooling2D((2, 2))(conv_2)\n\nconv_3 = Conv2D(64, (3, 3), activation='relu', padding='same')(pool_2)\nconv_3 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv_3)\npool_3 = MaxPooling2D((2, 2))(conv_3)\n\nconv_4 = Conv2D(128, (3, 3), activation='relu', padding='same')(pool_3)\nconv_4 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv_4)\npool_4 = MaxPooling2D((2, 2))(conv_4)\n\nconv_5 = Conv2D(256, (3, 3), activation='relu', padding='same')(pool_4)\nconv_5 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv_5)\n\nup_6 = Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(conv_5)\nup_6 = concatenate([up_6, conv_4], axis=3)\nconv_6 = Conv2D(128, (3, 3), activation='relu', padding='same')(up_6)\nconv_6 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv_6)\n\nup_7 = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(conv_6)\nup_7 = concatenate([up_7, conv_3], axis=3)\nconv_7 = Conv2D(64, (3, 3), activation='relu', padding='same')(up_7)\nconv_7 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv_7)\n\nup_8 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(conv_7)\nup_8 = concatenate([up_8, conv_2], axis=3)\nconv_8 = Conv2D(32, (3, 3), activation='relu', padding='same')(up_8)\nconv_8 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv_8)\n\nup_9 = Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(conv_8)\nup_9 = concatenate([up_9, conv_1], axis=3)\nconv_9 = Conv2D(16, (3, 3), activation='relu', padding='same')(up_9)\nconv_9 = Conv2D(16, (3, 3), activation='relu', padding='same')(conv_9)\n\n# TODO: Drop-out layers at the end of the contracting path perform further implicit data augmentation.\n\nconv_10 = Conv2D(1, (1, 1), activation='sigmoid')(conv_9)\n\nmodel = Model(inputs=[inputs], outputs=[conv_10])\n\noptimizer = Adam(lr=1e-5)\nmodel.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=[])\n\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"d720e653-eeab-4867-bb56-57c5d9970165","_uuid":"42102e27216d33d8663c47452381230374153957","collapsed":true,"trusted":true},"cell_type":"code","source":"early = EarlyStopping(patience=3, verbose=1)\ncheckpoint = ModelCheckpoint('keras_unet_01.h', verbose=1, save_best_only=True)\n\nresult = model.fit_generator(datagen.flow(train_X, train_Y), \n                             validation_split=0.2,\n                             batch_size=BATCH_SIZE, \n                             epochs=NUM_EPOCHS, \n                             steps_per_epoch=STEPS_PER_EPOCH, \n                             callbacks=[early, checkpoint])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"db119c3a-5560-4c0d-a810-2efc3c928086","_uuid":"567452506c8134d819f2e93ddd984520b1a59a54"},"cell_type":"markdown","source":"# Prediction"},{"metadata":{"_cell_guid":"f52e16d5-b5a8-4cd2-afd9-3b44c70fd76e","_uuid":"2f99961b6d387b81cf7d3d65c98cf4bc0766827b","collapsed":true,"trusted":true},"cell_type":"code","source":"model = load_model('keras_unet_01.h')\npredictions = model.predict(test_X, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e7ea897f-8123-4cb1-95c9-1c003b7376f8","_uuid":"7adf78cc11be2eb77e70238e4654eccd39c77043","collapsed":true,"trusted":true},"cell_type":"code","source":"print(predictions[0])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"2e5cc5d8-0e2b-45d6-a0bc-78ff292728bb","_uuid":"1a74a7d208c14c519fcc9726a8a21a40b2e6f5a2","collapsed":true,"trusted":true},"cell_type":"code","source":"preds = np.squeeze(predictions)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"efed29e6-efb5-4153-9bd9-6313e21436a5","_uuid":"bc3ad4981fb6d1c2f18040ab7c3173a8228249d7","collapsed":true,"trusted":true},"cell_type":"code","source":"print(preds[0])\nprint(preds.shape)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"7a0487c1-9bc0-41ee-bbc5-1af9a516b6fb","_uuid":"b1f4317126ba07b44b59b3c507a88e9768fe2f62","collapsed":true,"trusted":true},"cell_type":"code","source":"index_ = random.randint(0, len(test_X))\ntest_image = test_X[index_]\n\nplt.imshow(test_image)\nplt.show()\n\nprediction = preds[index_]\n\nplt.imshow(prediction)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"fdbf80e5-251b-48bb-9dcc-e54ac9dff733","_uuid":"cfef2abf0ec586874d7a85c7be352c14102b64de"},"cell_type":"markdown","source":"# Submission"},{"metadata":{"_cell_guid":"cfaa5dcc-c4cc-422f-aa68-2bad9dfb5475","_uuid":"26ed5136f8881d9e44c7c55f093b8cdd87bd0677"},"cell_type":"markdown","source":"[Using rakhlin's Fast Run-Length Encoding (Python)](https://www.kaggle.com/rakhlin/fast-run-length-encoding-python)"},{"metadata":{"_cell_guid":"edbd8baf-1d58-4f01-99a5-a987e3b08543","_uuid":"363d01174ca606d0df75bf743e8f3ca081ebd94e","collapsed":true,"trusted":true},"cell_type":"code","source":"def rle_encoding(x):\n    '''\n    x: numpy array of shape (height, width), 1 - mask, 0 - background\n    Returns run length as list\n    '''\n    dots = np.where(x.T.flatten() == 1)[0] # .T sets Fortran order down-then-right\n    run_lengths = []\n    prev = -2\n    for b in dots:\n        if (b > prev + 1): run_lengths.extend((b + 1, 0))\n        run_lengths[-1] += 1\n        prev = b\n        \n    return run_lengths\n\ndef prob_to_rles(x, cutoff=0.5):\n    lab_img = label(x > cutoff)\n    for i in range(1, lab_img.max() + 1):\n        yield rle_encoding(lab_img == i)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"bb87f1b6-6e7d-4dd3-9623-52b2fd54e486","_uuid":"bffeda52c584b06573c43b83241e6ebcc16200ee","collapsed":true,"trusted":true},"cell_type":"code","source":"# resizing the predictions to original sizea\npreds_resized = []\nfor index_, pred in enumerate(preds):\n    image = resize(pred, test_image_sizes[index_], mode='constant', preserve_range=True)\n    preds_resized.append(image)\n\nnew_test_ids = []\nrles = []\nfor n, id_ in enumerate(test_ids):\n    rle = list(prob_to_rles(preds_resized[n]))\n    rles.extend(rle)\n    new_test_ids.extend([id_] * len(rle))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"812f15b6-97f2-4c4b-ab8c-49c90588e129","_uuid":"91b459cc809de0033f49c2f3827f658b22686da8","collapsed":true,"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame()\nsubmission['ImageId'] = new_test_ids\nsubmission['EncodedPixels'] = pd.Series(rles).apply(lambda x: ' '.join(str(y) for y in x))\nsubmission.to_csv('keras_unet_01.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"56d9a6411495273c1a08591f3eafd0f15f1d66f5"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}