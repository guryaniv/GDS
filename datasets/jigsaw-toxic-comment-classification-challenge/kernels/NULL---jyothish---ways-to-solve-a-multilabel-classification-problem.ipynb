{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","collapsed":true,"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nfrom sklearn.pipeline import Pipeline\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.feature_extraction.text import TfidfVectorizer","execution_count":1,"outputs":[]},{"metadata":{"_cell_guid":"abb3e786-45d9-45c6-a060-ce31b666631d","collapsed":true,"_uuid":"03f5ffa543e599adb3f0ed9024e89c67271f2ce3","trusted":true},"cell_type":"code","source":"train_data=pd.read_csv(\"../input/train.csv\")\ntest_data=pd.read_csv(\"../input/test.csv\")\nfinal_out=pd.read_csv(\"../input/sample_submission.csv\")","execution_count":2,"outputs":[]},{"metadata":{"_cell_guid":"973c1cfb-4926-473e-bb1d-162d60a29b2b","_uuid":"ba56fc178854933ddaecbce5c13c6061a76af1a9","trusted":true},"cell_type":"code","source":"train_data.head()","execution_count":3,"outputs":[]},{"metadata":{"_cell_guid":"66c295fa-44c3-424a-a09b-d4c4683088d5","_uuid":"3f0de528a06b9fb8f60b3068344bd1eb61b8a069"},"cell_type":"markdown","source":"The problem is a multi label problem:\n\n**What is a multilabel problem?**\n\nA multilabel problem is a classificatoin problem in which the input can be mapped into different classes. This can be better considered with the movie genres. We would have seen many labels for the movies in theatres. A single movie will be classified as 'Romance\", \"Comedy\" genres at the same time.\n\n**How to Solve the Problem?**\nWe can approach this in 3 ways:\n- one v/s All\n- one v/s one\n- Error Correcting Output Codes"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"},"cell_type":"markdown","source":"**One v/s All**\n\nThis strategy, also known as one-vs-all, is implemented in OneVsRestClassifier. The strategy consists in fitting one classifier per class. For each classifier, the class is fitted against all the other classes. Advantage of this approach is its interpretability.\nSince each class is represented by one and only one classifier, it is possible to gain knowledge about the class by inspecting its corresponding classifier. This is the most commonly used strategy and is a fair default choice.\nSo we shall go by this method\n\nThe classifier we are using here is a *Linear Support Vector Machines:*\n\nwe can try other SVM variants like: [\"c-svm\",\"nu-svm\",\"Bound Constraint c-svm\",\"weston watkins multiclass svm\"] and more\n\n"},{"metadata":{"_cell_guid":"d902de61-30ed-4ac1-ae9d-61edff8e6410","_uuid":"f93dd51489802e13bd07657e5845e99d1c6de637"},"cell_type":"markdown","source":"**Embeddings**\n\nLIke any other machine learning algorithms these classifiers cannot work on the texts. so we need to find a way to convert thetexts into numerical equivalents\nWe have two ways to do that :\n- TFIDF\n- Word2Vec\n\nI think these two terminologies are much familar to every1. hence we can see a way to implement them\n"},{"metadata":{"_cell_guid":"55dc4028-3339-495d-9b40-70b058810382","_uuid":"581e30b2ad178f66ec0489ffd554b7b6039a2257"},"cell_type":"markdown","source":"**Model 1: Word2Vec with OneV/s All**\n![](http://)"},{"metadata":{"_cell_guid":"9025d0f1-f300-40a2-8a12-ed2df8d6b4e9","_uuid":"ccc7d473bf7d2bb9229a40beb0450d71ab3f6da3"},"cell_type":"markdown","source":"*Step 1 :Generate the word embeddings*\n\n we can generate word embeddings for all the words in the text and the training and the test set"},{"metadata":{"_cell_guid":"62f0acb5-1f05-47bf-8d81-433d71953d4d","collapsed":true,"_uuid":"537672f67496dd00738f30dcc114f6ed8d3bea58","trusted":true},"cell_type":"code","source":"combined_df=train_data['comment_text'].append(test_data['comment_text'])","execution_count":4,"outputs":[]},{"metadata":{"_cell_guid":"972d158e-0be3-4aca-899b-c6fba32d1a4e","collapsed":true,"_uuid":"cdf71ed7e6c4df6e7b3c076d4f31e46316c954d2","trusted":true},"cell_type":"code","source":"from gensim.models import Word2Vec\nVocab_list=(combined_df.apply(lambda x:str(x).strip().split())\n                         )\nmodels=Word2Vec(Vocab_list,size=100)\n\nWordVectorz=dict(zip(models.wv.index2word,models.wv.vectors))","execution_count":5,"outputs":[]},{"metadata":{"_cell_guid":"c5ea8c6f-d684-46e7-9574-a722a4782f15","_uuid":"589454ad45fd15e1e4908c509758cfe342fca606"},"cell_type":"markdown","source":"**Fit these things into a pipeline**\n\nWe need to make a class which implements the fit and the transform method(as per scikit learns Pipeline class documentation)\nSo how should we transform a sentence representation to fit the pipeline. \n\nThe easiest transformation is to move from a word embedding level to a sentence level embedding:\n\n**Sentence level embedding:**\nCalculate average of all the word vectors that make up the sentence and  this average vector will be a repressentation for the sentence."},{"metadata":{"_cell_guid":"08e11029-fc2e-4226-8d92-8768827b286d","collapsed":true,"_uuid":"8f0bf21a88c7622c6bc6d48bcd62ade7b953c442","trusted":true},"cell_type":"code","source":"class AverageEmbeddingVectorizer(object):\n    def __init__(self, word2vec):\n        self.word2vec = word2vec\n        # if a text is empty we should return a vector of zeros\n        # with the same dimensionality as all the other vectors\n        self.dim =100 # because we use 100 embedding points \n\n    def fit(self, X, y):\n        return self\n\n    def transform(self, X):\n        return np.array([\n            np.mean([self.word2vec[w] for w in words if w in self.word2vec]\n                    or [np.zeros(self.dim)], axis=0)\n            for words in X\n        ])\n","execution_count":6,"outputs":[]},{"metadata":{"_cell_guid":"fdcc7a12-cbb0-48ae-a51a-1c0b74424b14","collapsed":true,"_uuid":"9cd0b3e796581728c41d72308303a2c2a9a6231b","trusted":false},"cell_type":"code","source":"pipe1=Pipeline([(\"wordVectz\",AverageEmbeddingVectorizer(WordVectorz)),(\"multilabel\",OneVsRestClassifier(LinearSVC(random_state=0)))])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b950cf8e-3b30-4fd2-8e21-4109cb727adc","collapsed":true,"_uuid":"fc14332487437c2089f28872f2036f8fb7dfb183","trusted":true},"cell_type":"code","source":"y_train=train_data[[i for i in train_data.columns if i not in [\"comment_text\",\"id\"]]]","execution_count":10,"outputs":[]},{"metadata":{"_cell_guid":"1f226ce0-c654-4bfb-a569-c29530dd1e72","collapsed":true,"_uuid":"1f6ffdf209507b51d7f33a8f011c756364cbd2af","trusted":false},"cell_type":"code","source":"pipe1.fit(train_data['comment_text'],y_train)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"953738e0-3969-4311-90b8-8ff24aea308d","_uuid":"9b831112663a5c4d4333dc16db48a598f671a5d0"},"cell_type":"markdown","source":"**Prediction and Submission File Generation Model 1**\n"},{"metadata":{"_cell_guid":"ba51be5e-7d1c-47eb-937f-e6bce5fcca24","collapsed":true,"_uuid":"e58c929e47135282a161da041018056a9a9fbb89","trusted":false},"cell_type":"code","source":"predicted1=pipe1.predict(test_data['comment_text'])\nlabel_cols=train_data.columns[2:]\nsubmid = pd.DataFrame({'id': final_out[\"id\"]})\nsubmission = pd.concat([submid, pd.DataFrame(predicted1, columns = label_cols)], axis=1)\nsubmission.to_csv('submission_W2v_m1.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"373e4def-d273-470e-8e76-45c50f9d2c3b","_uuid":"0f98aec5984cff2bccb552da69177130abe35039"},"cell_type":"markdown","source":"**Model -2:TFiDf with OneV/s All**"},{"metadata":{"_cell_guid":"9864d03d-6312-4f15-adf2-7dd4288bc89f","_uuid":"2cfcd5fe01db58043d30fe0ba20d8de574367e15","trusted":true},"cell_type":"code","source":"pipe2=Pipeline([('TFidf',TfidfVectorizer()),(\"multilabel\",OneVsRestClassifier(LinearSVC(random_state=0)))])","execution_count":16,"outputs":[]},{"metadata":{"_cell_guid":"6b8bbc27-3187-4dd3-aea0-7d060739971e","_uuid":"52bbbdbc5d97cf13e085dab30a9b3502b4cdc012","trusted":true},"cell_type":"code","source":"pipe2.fit(train_data['comment_text'],y_train)","execution_count":17,"outputs":[]},{"metadata":{"_cell_guid":"709f30fb-758e-47e4-af33-c82910863b73","_uuid":"266af9524fe0381b3142facca626dcd567dae73a","trusted":true},"cell_type":"code","source":"predicted2=pipe2.predict(test_data['comment_text'])\nlabel_cols=train_data.columns[2:]\nsubmid = pd.DataFrame({'id': final_out[\"id\"]})\nsubmission = pd.concat([submid, pd.DataFrame(predicted2, columns = label_cols)], axis=1)\nsubmission.to_csv('submission_TfIdf_m1.csv', index=False)","execution_count":20,"outputs":[]},{"metadata":{"_uuid":"9c03c1d7daab7482b0f257bb3417a071f2079dd8"},"cell_type":"markdown","source":"**Conculsion  from Model1 and Model2:**\n\nModel 1 LB_score: 0.563\nModel2 LB_score:0.754\n\nThese approaches provide a decent begining to  a multilabel classification problems.\n"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"0d729a29c5455dd61360059b354cf3d416db7922"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}