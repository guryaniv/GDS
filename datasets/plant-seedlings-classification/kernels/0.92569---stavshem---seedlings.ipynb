{"cells":[{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"%matplotlib inline\nimport matplotlib.pyplot as plt\nfrom os.path import join, isdir\nfrom os import listdir, makedirs, getcwd, remove\nfrom PIL import Image\nfrom mpl_toolkits.axes_grid1 import ImageGrid\nimport pandas as pd\nimport numpy as np\nimport torch\nfrom torch.autograd import Variable\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision\nfrom torchvision import transforms, datasets, models\nimport random\nfrom collections import deque\n\nuse_cuda = torch.cuda.is_available()\nuse_cuda","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f568900c68954e196409ab6ba3dfdec7bd64c139"},"cell_type":"code","source":"manualSeed = 999\ndef fixSeed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if use_cuda:\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\nfixSeed(manualSeed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4a4fe18339d747b3bee29cda7b1564a7633da5c8"},"cell_type":"code","source":"import os\n\ndef find_classes(fullDir):\n    classes = [d for d in os.listdir(fullDir) if os.path.isdir(os.path.join(fullDir, d))]\n    classes.sort()\n    class_to_idx = {classes[i]: i for i in range(len(classes))}\n    num_to_class = dict(zip(range(len(classes)), classes))\n    \n    train = []\n    for index, label in enumerate(classes):\n      path = fullDir + label + '/'\n      for file in listdir(path):\n        train.append(['{}/{}'.format(label, file), label, index])\n    \n    df = pd.DataFrame(train, columns=['file', 'category', 'category_id', ])\n    \n    return classes, class_to_idx, num_to_class, df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"122ae046604f9cc65c6a401d9c6a69e91a5ba5c7"},"cell_type":"code","source":"classes, class_to_idx, num_to_class, df = find_classes(\"../input/train/\")\n\nX, y = df.drop('category_id', axis=1), df['category_id']\n\nprint(f'X shape: {X.shape}, y shape: {y.shape}')\nX.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"975a6144be76931f826b6117393f2cf0ac604fbf"},"cell_type":"code","source":"class SeedlingDataset(Dataset):\n    def __init__(self, filenames, labels, root_dir, subset=False, transform=None):\n        self.filenames = filenames\n        self.labels = labels\n        self.root_dir = root_dir\n        self.transform = transform\n    \n    def __len__(self):\n        return len(self.labels)\n    \n    def __getitem__(self, idx):\n        fullname = join(self.root_dir, self.filenames.iloc[idx])\n        image = Image.open(fullname).convert('RGB')\n        if self.transform:\n            image = self.transform(image)\n        return image, self.labels.iloc[idx]\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c89ffddb4bd6f997b361e636ce7e5497539e554d"},"cell_type":"markdown","source":"# Train Validation Test Split"},{"metadata":{"trusted":true,"_uuid":"7bff9e2dc180ddce0c5b6710734b25ea285d3629"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a2e130c1333f4e360801c1c8e585fc4190f8c9f4"},"cell_type":"code","source":"X, y = df.drop(['category_id', 'category'], axis=1), df['category_id']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9b08e2eb0d974333f88f0ff01894d9457979cd71"},"cell_type":"code","source":" X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n\n X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a1261505191f77f353f5e850080ae853a15c2a1c"},"cell_type":"code","source":"print(f'train size: {X_train.shape}, validation size: {X_val.shape}, test size: {X_test.shape}')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d3befbe67ab9dd8a0b68efde888153325349cbda"},"cell_type":"markdown","source":"# Train and Test Methods"},{"metadata":{"trusted":true,"_uuid":"32d5c8ed00e4bc46361ffd088e0198fe31f1f0cd"},"cell_type":"code","source":"def train(train_loader, model, optimizer, criterion):\n    model.train()\n    \n    losses, accuracies = [], []\n    for batch_idx, (data, target) in (enumerate(train_loader)):\n        correct = 0\n        \n        if use_cuda:\n            data, target = data.cuda(), target.cuda()\n        \n        optimizer.zero_grad()\n        output = model(data) # forward\n        loss = criterion(output, target)\n        loss.backward()\n        # Calculate Accuracy\n        pred = output.data.max(1)[1] # max probability\n        correct += pred.eq(target.data).cpu().sum() \n        accuracy = 100. * correct / len(data)\n        optimizer.step()\n        \n        losses.append(loss.data.item())\n        accuracies.append(accuracy)\n        \n    return np.mean(losses), np.mean(accuracies)\n\n\ndef test(test_loader, criterion, model):\n    test_loss = 0\n    correct = 0\n    for data, target in test_loader:\n        if use_cuda:\n            data, target = data.cuda(), target.cuda()\n        \n        data, target = Variable(data, volatile=True), Variable(target)\n        output = model(data)\n        test_loss += criterion(output, target).data.item()\n        pred = output.data.max(1)[1] # get the index of the max log-probability\n        correct += pred.eq(target.data).cpu().sum().item()\n    \n    test_loss /= len(test_loader) # loss function already averages over batch size\n    accuracy = 100. * correct / len(test_loader.dataset)\n    return test_loss, accuracy\n\ndef _should_early_stop(recent_validation_losses, validation_loss, early_stopping_rounds):\n    recent_validation_losses.append(validation_loss)\n    if early_stopping_rounds < len(recent_validation_losses):\n        recent_validation_losses.popleft()\n        return all(np.diff(recent_validation_losses) > 0)\n    return False\n    \ndef run_train_process(epochs, loaders, model, optimizer, criterion, early_stopping_rounds=10):\n    epoch_to_results = {}\n    recent_validation_losses = deque()\n    \n    for epoch in tqdm(range(epochs)):\n        train_loss, train_accuracy = train(loaders['train'], model, optimizer, criterion)\n        validation_loss, validation_accuracy = test(loaders['validation'], criterion, model)\n        epoch_to_results[epoch] = {\n            'train_loss': train_loss,\n            'train_accuracy': train_accuracy,\n            'validation_loss': validation_loss,\n            'validation_accuracy': validation_accuracy,\n        }\n\n        # Debug\n        if epoch % 5 == 0:\n            print('[%s] Train Accuracy: %.5f , Validation Accuracy: %.5f' % (epoch, train_accuracy, validation_accuracy))\n        \n        # Early Stop\n        should_early_stop = _should_early_stop(recent_validation_losses, validation_loss, early_stopping_rounds)\n        if should_early_stop:\n            print(f'Train epoch {epoch} EARLY STOPPING - validation loss has not improved in the last {early_stopping_rounds} rounds')\n            break\n    \n    df_epoch_to_results = pd.DataFrame.from_dict(epoch_to_results).T\n    _, test_accuracy = test(loaders['test'], criterion, model)\n    \n    return df_epoch_to_results, test_accuracy","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"eceb7049714ff03eb9c019057d3d32d7da7a1280"},"cell_type":"markdown","source":"# Lenet"},{"metadata":{"trusted":true,"_uuid":"92c26ff9dc44555b9ae2a503c016abc568b3240c"},"cell_type":"code","source":"import torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom tqdm import tqdm, tqdm_notebook","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0d1b88ab9ba4ae39e0dda8b6fa984a6475011d7c"},"cell_type":"code","source":"class LeNet(nn.Module):\n    def __init__(self, num_classes=12, num_rgb=3):\n        super(LeNet, self).__init__()\n        self.conv1 = nn.Conv2d(num_rgb, 6, 5)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        self.fc1 = nn.Linear(44944, 120)\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, num_classes)\n\n    def forward(self, x):\n        out = F.relu(self.conv1(x))\n        out = F.max_pool2d(out, 2)\n        out = F.relu(self.conv2(out))\n        out = F.max_pool2d(out, 2)\n        out = out.view(out.size(0), -1)\n        out = F.relu(self.fc1(out))\n        out = F.relu(self.fc2(out))\n        out = out.view(out.size(0), -1)\n        out = self.fc3(out)\n        return out\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a808b6576a977ddcc881d707ed95e056f2a40162"},"cell_type":"markdown","source":"# Experiment 2"},{"metadata":{"trusted":true,"_uuid":"19935272a8fb65a2113acadffa1da3e8037246cf","scrolled":false},"cell_type":"code","source":"learning_rate = 1e-3\nlenet = LeNet().cuda()\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(lenet.parameters(), lr=learning_rate, momentum=0.9)\n\nimage_size = (224, 224)\ntrain_trans = transforms.Compose([\n    transforms.Resize(size=image_size),\n    transforms.ToTensor()\n])\nvalidation_trans = transforms.Compose([\n    transforms.Resize(size=image_size),\n    transforms.ToTensor()\n])\n\ntrain_set = SeedlingDataset(X_train.file, y_train, '../input/train/', transform=train_trans)\nvalid_set = SeedlingDataset(X_val.file, y_val, '../input/train/', transform=validation_trans)\ntest_set = SeedlingDataset(X_test.file, y_test, '../input/train/', transform=validation_trans)\n\nbatch_size = 8\nloaders = {\n    'train': DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0),\n    'validation': DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=0),\n    'test': DataLoader(test_set, batch_size=batch_size, shuffle=True, num_workers=0)\n}\nepochs = 3\n\ndf_experiment2_result, test_accuracy_experiment2 = run_train_process(epochs, loaders, lenet, optimizer, criterion)\ndf_experiment2_result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"36d7d30b67ca69e36ee0f40ff4a55abda713b6cd"},"cell_type":"code","source":"ax = df_experiment2_result[['train_accuracy', 'validation_accuracy']].plot(figsize=(16,9),)\ndf_experiment2_result[['train_loss', 'validation_loss']].plot(ax=ax.twinx())\nax.set_title(f'[Experiment 2] Test Accuracy: {test_accuracy_experiment2:.2f}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ffa37ad7a1f2931b7c9aa1c886c9582ca5c00322"},"cell_type":"markdown","source":"# Experiment 3"},{"metadata":{"trusted":true,"_uuid":"aa07f0596bc9d5e8f0e50cfdb8577f5898c391de"},"cell_type":"markdown","source":"## Normalize Images"},{"metadata":{"trusted":true,"_uuid":"5189a6c4b8512b0e182c927bb04dc50290f25d36"},"cell_type":"code","source":"image_size = (224, 224)\ntrain_set = SeedlingDataset(\n    X_train.file, y_train, '../input/train/',\n    transform=transforms.Compose([\n        transforms.Resize(size=image_size),\n        transforms.ToTensor()]))\n\nmeans = []\nmeans_sq = []\n\nfor img, _ in tqdm_notebook(train_set):\n    means.append(np.asarray(img, dtype='float32').mean(axis=(1,2)))\n    means_sq.append((np.asarray(img, dtype='float32') ** 2).mean(axis=(1,2)))\n\nmean_img = np.mean(means, axis=0)\nstd_img = np.sqrt(np.mean(means_sq, axis=0) - (mean_img ** 2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bb122d8edede7541b0fbbb90cd30244c15a05e27"},"cell_type":"code","source":"mean_img, std_img","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"847344159f83668f80571bfa07f3c51649d48b9a"},"cell_type":"code","source":"learning_rate = 1e-3\nlenet = LeNet().cuda()\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(lenet.parameters(), lr=learning_rate, momentum=0.9)\n\nimage_size = (224, 224)\ntrain_trans = transforms.Compose([\n    transforms.Resize(size=image_size),\n    transforms.ToTensor(),\n    transforms.Normalize(mean_img, std_img)\n])\nvalidation_trans = transforms.Compose([\n    transforms.Resize(size=image_size),\n    transforms.ToTensor(),\n    transforms.Normalize(mean_img, std_img)\n])\n\ntrain_set = SeedlingDataset(X_train.file, y_train, '../input/train/', transform=train_trans)\nvalid_set = SeedlingDataset(X_val.file, y_val, '../input/train/', transform=validation_trans)\ntest_set = SeedlingDataset(X_test.file, y_test, '../input/train/', transform=validation_trans)\n\nbatch_size = 8\nloaders = {\n    'train': DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0),\n    'validation': DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=0),\n    'test': DataLoader(test_set, batch_size=batch_size, shuffle=True, num_workers=0)\n}\nepochs = 3\n\ndf_experiment3_part1_result, test_accuracy_experiment3_part1 = run_train_process(epochs, loaders, lenet, optimizer, criterion)\ndf_experiment3_part1_result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9c45e1507f986e50d8439bae7ef6f117684d28e8"},"cell_type":"code","source":"ax = df_experiment3_part1_result[['train_accuracy', 'validation_accuracy']].plot(figsize=(16,9),)\ndf_experiment3_part1_result[['train_loss', 'validation_loss']].plot(ax=ax.twinx())\nax.set_title(f'[Experiment 3 Part 1] Test Accuracy: {test_accuracy_experiment3_part1:.2f}')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"68ac054ddc4d72a12be27c00ebc5a61f344babc1"},"cell_type":"markdown","source":"## Augmentations"},{"metadata":{"trusted":true,"_uuid":"4c7c93e3014d744233ee1ad248375e6e9579c586"},"cell_type":"code","source":"image_size = (224, 224)\nbatch_size = 8\n\ntrain_trans = transforms.Compose([\n    transforms.transforms.RandomHorizontalFlip(),\n    transforms.transforms.RandomRotation(180),\n    transforms.transforms.RandomVerticalFlip(),\n    transforms.Resize(size=image_size),\n    transforms.ToTensor(),\n    transforms.Normalize(mean_img, std_img),\n])\nvalidation_trans = transforms.Compose([\n    transforms.Resize(size=image_size),\n    transforms.ToTensor(),\n    transforms.Normalize(mean_img, std_img)\n])\n\nlearning_rate = 1e-3\nlenet = LeNet().cuda()\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(lenet.parameters(), lr=learning_rate, momentum=0.9)\n\ntrain_set = SeedlingDataset(X_train.file, y_train, '../input/train/', transform=train_trans)\nvalid_set = SeedlingDataset(X_val.file, y_val, '../input/train/', transform=validation_trans)\ntest_set = SeedlingDataset(X_test.file, y_test, '../input/train/', transform=validation_trans)\n\nloaders = {\n    'train': DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0),\n    'validation': DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=0),\n    'test': DataLoader(test_set, batch_size=batch_size, shuffle=True, num_workers=0)\n}\nepochs = 3\n\ndf_experiment3_part2_result, test_accuracy_experiment3_part2 = run_train_process(epochs, loaders, lenet, optimizer, criterion)\ndf_experiment3_part2_result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6678ff1ea6c34753d0c1336987e8dfd326faca3c"},"cell_type":"code","source":"ax = df_experiment3_part2_result[['train_accuracy', 'validation_accuracy']].plot(figsize=(16,9),)\ndf_experiment3_part2_result[['train_loss', 'validation_loss']].plot(ax=ax.twinx())\nax.set_title(f'[Experiment 3 Part 2] Test Accuracy: {test_accuracy_experiment3_part2:.2f}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0bcc43c35b7e2b076808a390b872b724524c88a0"},"cell_type":"markdown","source":"# Experiment 4"},{"metadata":{"_uuid":"a0998c53cb8bb607248ca4037cb0069a7c8cb24e"},"cell_type":"markdown","source":"## Change Kernel Size to 3"},{"metadata":{"trusted":true,"_uuid":"2edb7eecea3a51647b12bfdcb9d75d3e05915e73"},"cell_type":"code","source":"class LeNetWithKernelSize3(LeNet):\n    def __init__(self, num_classes=12, num_rgb=3):\n        super(LeNetWithKernelSize3, self).__init__(num_classes, num_rgb)\n        self.conv1 = nn.Conv2d(num_rgb, 6, 3)\n        self.conv2 = nn.Conv2d(6, 16, 3)\n        self.fc1 = nn.Linear(46656, 120)\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, num_classes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b6726b011ebf7b435fcfe61ff82868d79e6dbb1c"},"cell_type":"code","source":"image_size = (224, 224)\nbatch_size = 8\n\ntrain_trans = transforms.Compose([\n    transforms.transforms.RandomHorizontalFlip(),\n    transforms.transforms.RandomRotation(180),\n    transforms.transforms.RandomVerticalFlip(),\n    transforms.Resize(size=image_size),\n    transforms.ToTensor(),\n    transforms.Normalize(mean_img, std_img),\n])\nvalidation_trans = transforms.Compose([\n    transforms.Resize(size=image_size),\n    transforms.ToTensor(),\n    transforms.Normalize(mean_img, std_img)\n])\n\ntrain_set = SeedlingDataset(X_train.file, y_train, '../input/train/', transform=train_trans)\nvalid_set = SeedlingDataset(X_val.file, y_val, '../input/train/', transform=validation_trans)\ntest_set = SeedlingDataset(X_test.file, y_test, '../input/train/', transform=validation_trans)\n\ntrain_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0)\nvalidation_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=0)\n\nlearning_rate = 1e-3\nlenet = LeNetWithKernelSize3().cuda()\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(lenet.parameters(), lr=learning_rate, momentum=0.9)\n\ntrain_set = SeedlingDataset(X_train.file, y_train, '../input/train/', transform=train_trans)\nvalid_set = SeedlingDataset(X_val.file, y_val, '../input/train/', transform=validation_trans)\ntest_set = SeedlingDataset(X_test.file, y_test, '../input/train/', transform=validation_trans)\n\nbatch_size = 8\nloaders = {\n    'train': DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0),\n    'validation': DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=0),\n    'test': DataLoader(test_set, batch_size=batch_size, shuffle=True, num_workers=0)\n}\nepochs = 3\n\ndf_experiment4_part1_result, test_accuracy_experiment4_part1 = run_train_process(epochs, loaders, lenet, optimizer, criterion)\ndf_experiment4_part1_result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"54ba15c1c17c25553529f8d6ce109ddd9a235635"},"cell_type":"code","source":"ax = df_experiment4_part1_result[['train_accuracy', 'validation_accuracy']].plot(figsize=(16,9),)\ndf_experiment4_part1_result[['train_loss', 'validation_loss']].plot(ax=ax.twinx())\nax.set_title(f'[Experiment 4 Part 1] Test Accuracy: {test_accuracy_experiment4_part1:.2f}')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"32ddfe9c3624f1ebb98e15f7115e142f2b086c45"},"cell_type":"markdown","source":"## Change Batch size to 32"},{"metadata":{"trusted":true,"_uuid":"ee71b26344d5755528011919a7c5bb1328cb04eb"},"cell_type":"code","source":"image_size = (224, 224)\nbatch_size = 32\n\ntrain_trans = transforms.Compose([\n    transforms.transforms.RandomHorizontalFlip(),\n    transforms.transforms.RandomRotation(180),\n    transforms.transforms.RandomVerticalFlip(),\n    transforms.Resize(size=image_size),\n    transforms.ToTensor(),\n    transforms.Normalize(mean_img, std_img),\n])\nvalidation_trans = transforms.Compose([\n    transforms.Resize(size=image_size),\n    transforms.ToTensor(),\n    transforms.Normalize(mean_img, std_img)\n])\n\ntrain_set = SeedlingDataset(X_train.file, y_train, '../input/train/', transform=train_trans)\nvalid_set = SeedlingDataset(X_val.file, y_val, '../input/train/', transform=validation_trans)\ntest_set = SeedlingDataset(X_test.file, y_test, '../input/train/', transform=validation_trans)\n\ntrain_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0)\nvalidation_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=0)\n\nlearning_rate = 1e-3\nlenet = LeNetWithKernelSize3().cuda()\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(lenet.parameters(), lr=learning_rate, momentum=0.9)\n\ntrain_set = SeedlingDataset(X_train.file, y_train, '../input/train/', transform=train_trans)\nvalid_set = SeedlingDataset(X_val.file, y_val, '../input/train/', transform=validation_trans)\ntest_set = SeedlingDataset(X_test.file, y_test, '../input/train/', transform=validation_trans)\n\nloaders = {\n    'train': DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0),\n    'validation': DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=0),\n    'test': DataLoader(test_set, batch_size=batch_size, shuffle=True, num_workers=0)\n}\nepochs = 100\n\ndf_experiment4_part2_result, test_accuracy_experiment4_part2 = run_train_process(epochs, loaders, lenet, optimizer, criterion)\ndf_experiment4_part2_result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5d893f3c9a7adc243711ed95ce0043aadd967029"},"cell_type":"code","source":"ax = df_experiment4_part2_result[['train_accuracy', 'validation_accuracy']].plot(figsize=(16,9),)\ndf_experiment4_part2_result[['train_loss', 'validation_loss']].plot(ax=ax.twinx())\nax.set_title(f'[Experiment 4 Part 2] Test Accuracy: {test_accuracy_experiment4_part2:.2f}')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3bae8a483fe964d03e43a12708bb078f201ecd4f"},"cell_type":"markdown","source":"# Experiment 5"},{"metadata":{"_uuid":"83db159115920a3b436903c0975a6034e55553f1"},"cell_type":"markdown","source":"## Change Activation Function to Sigmoid"},{"metadata":{"trusted":true,"_uuid":"b4b609e44f427be47f597bbbfcfccdb372547a56"},"cell_type":"code","source":"class LeNetWithSigmoidActivation(LeNet):\n    def forward(self, x):\n        out = F.sigmoid(self.conv1(x))\n        out = F.max_pool2d(out, 2)\n        out = F.sigmoid(self.conv2(out))\n        out = F.max_pool2d(out, 2)\n        out = out.view(out.size(0), -1)\n        out = F.sigmoid(self.fc1(out))\n        out = F.sigmoid(self.fc2(out))\n        out = out.view(out.size(0), -1)\n        out = self.fc3(out)\n        return out","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8c03e26086044832794829500498e915d1c5eeef"},"cell_type":"code","source":"image_size = (224, 224)\nbatch_size = 32\n\ntrain_trans = transforms.Compose([\n    transforms.transforms.RandomHorizontalFlip(),\n    transforms.transforms.RandomRotation(180),\n    transforms.transforms.RandomVerticalFlip(),\n    transforms.Resize(size=image_size),\n    transforms.ToTensor(),\n    transforms.Normalize(mean_img, std_img),\n])\nvalidation_trans = transforms.Compose([\n    transforms.Resize(size=image_size),\n    transforms.ToTensor(),\n    transforms.Normalize(mean_img, std_img)\n])\n\ntrain_set = SeedlingDataset(X_train.file, y_train, '../input/train/', transform=train_trans)\nvalid_set = SeedlingDataset(X_val.file, y_val, '../input/train/', transform=validation_trans)\ntest_set = SeedlingDataset(X_test.file, y_test, '../input/train/', transform=validation_trans)\n\ntrain_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0)\nvalidation_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=0)\n\nlearning_rate = 1e-3\nlenet = LeNetWithSigmoidActivation().cuda()\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(lenet.parameters(), lr=learning_rate, momentum=0.9)\n\ntrain_set = SeedlingDataset(X_train.file, y_train, '../input/train/', transform=train_trans)\nvalid_set = SeedlingDataset(X_val.file, y_val, '../input/train/', transform=validation_trans)\ntest_set = SeedlingDataset(X_test.file, y_test, '../input/train/', transform=validation_trans)\n\nloaders = {\n    'train': DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0),\n    'validation': DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=0),\n    'test': DataLoader(test_set, batch_size=batch_size, shuffle=True, num_workers=0)\n}\nepochs = 100\n\ndf_experiment5_part1_result, test_accuracy_experiment5_part1 = run_train_process(epochs, loaders, lenet, optimizer, criterion)\ndf_experiment5_part1_result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d9344d234ecb6a91ca3274cd1fe2f1f7807aac84"},"cell_type":"code","source":"ax = df_experiment5_part1_result[['train_accuracy', 'validation_accuracy']].plot(figsize=(16,9),)\ndf_experiment5_part1_result[['train_loss', 'validation_loss']].plot(ax=ax.twinx())\nax.set_title(f'[Experiment 5 Part 1] Test Accuracy: {test_accuracy_experiment5_part1:.2f}')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"435337d09c0b681e62523e903c3efc20274b1403"},"cell_type":"markdown","source":"## Change Optimization Function to Adam"},{"metadata":{"trusted":true,"_uuid":"506bd056bc078c9194a2a1c5913a04b039b3402a"},"cell_type":"code","source":"image_size = (224, 224)\nbatch_size = 32\n\ntrain_trans = transforms.Compose([\n    transforms.transforms.RandomHorizontalFlip(),\n    transforms.transforms.RandomRotation(180),\n    transforms.transforms.RandomVerticalFlip(),\n    transforms.Resize(size=image_size),\n    transforms.ToTensor(),\n    transforms.Normalize(mean_img, std_img),\n])\nvalidation_trans = transforms.Compose([\n    transforms.Resize(size=image_size),\n    transforms.ToTensor(),\n    transforms.Normalize(mean_img, std_img)\n])\n\ntrain_set = SeedlingDataset(X_train.file, y_train, '../input/train/', transform=train_trans)\nvalid_set = SeedlingDataset(X_val.file, y_val, '../input/train/', transform=validation_trans)\ntest_set = SeedlingDataset(X_test.file, y_test, '../input/train/', transform=validation_trans)\n\ntrain_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0)\nvalidation_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=0)\n\nlearning_rate = 1e-3\nlenet = LeNet().cuda()\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(lenet.parameters(), lr=learning_rate)\n\ntrain_set = SeedlingDataset(X_train.file, y_train, '../input/train/', transform=train_trans)\nvalid_set = SeedlingDataset(X_val.file, y_val, '../input/train/', transform=validation_trans)\ntest_set = SeedlingDataset(X_test.file, y_test, '../input/train/', transform=validation_trans)\n\nloaders = {\n    'train': DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0),\n    'validation': DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=0),\n    'test': DataLoader(test_set, batch_size=batch_size, shuffle=True, num_workers=0)\n}\nepochs = 100\n\ndf_experiment5_part2_result, test_accuracy_experiment5_part2 = run_train_process(epochs, loaders, lenet, optimizer, criterion)\ndf_experiment5_part2_result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c59f34ce45aa8f0667d92c55c54de7e6e38c3e5e"},"cell_type":"code","source":"ax = df_experiment5_part2_result[['train_accuracy', 'validation_accuracy']].plot(figsize=(16,9),)\ndf_experiment5_part2_result[['train_loss', 'validation_loss']].plot(ax=ax.twinx())\nax.set_title(f'[Experiment 5 Part 2] Test Accuracy: {test_accuracy_experiment5_part2:.2f}')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c05fe84a0a3b6641997dab0ca2f0c4c1038c7503"},"cell_type":"markdown","source":"# Experiment 8 - Transfer Learning"},{"metadata":{"trusted":true,"_uuid":"7ce097eb6ee442901d656678ae98165c1fbaa01f"},"cell_type":"code","source":"# Based on: https://www.kaggle.com/carloalbertobarbano/vgg16-transfer-learning-pytorch\n\n# Load the pretrained model from pytorch\nvgg16 = models.vgg16_bn(pretrained=True)\nprint(f'Number of output fearurs of pretrained vgg is: {vgg16.classifier[6].out_features}') # 1000\n\n# Freeze training for all layers\nfor param in vgg16.features.parameters():\n    param.require_grad = False\n\n# Newly created modules have require_grad=True by default\nnum_features = vgg16.classifier[6].in_features\nprint(f'Last layer of vgg16 has {num_features} input features.')\nprint(f'Removing layer: {list(vgg16.classifier.children())[-1]}')\nclassifier_layers = list(vgg16.classifier.children())[:-1] # Remove last layer\nlayer_to_add = nn.Linear(num_features, len(np.unique(y)))\nprint(f'Adding layer: {layer_to_add}')\nclassifier_layers.extend([layer_to_add]) # Add our layer with 4 outputs\nvgg16.classifier = nn.Sequential(*classifier_layers) # Replace the model classifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9bdb7ac11e94c134cb3d8145acee84d6f713d444"},"cell_type":"code","source":"image_size = (224, 224)\nbatch_size = 32\n\ntrain_trans = transforms.Compose([\n    transforms.transforms.RandomHorizontalFlip(),\n    transforms.transforms.RandomRotation(180),\n    transforms.transforms.RandomVerticalFlip(),\n    transforms.Resize(size=image_size),\n    transforms.ToTensor(),\n    transforms.Normalize(mean_img, std_img),\n])\nvalidation_trans = transforms.Compose([\n    transforms.Resize(size=image_size),\n    transforms.ToTensor(),\n    transforms.Normalize(mean_img, std_img)\n])\n\ntrain_set = SeedlingDataset(X_train.file, y_train, '../input/train/', transform=train_trans)\nvalid_set = SeedlingDataset(X_val.file, y_val, '../input/train/', transform=validation_trans)\ntest_set = SeedlingDataset(X_test.file, y_test, '../input/train/', transform=validation_trans)\n\ntrain_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0)\nvalidation_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=0)\n\nlearning_rate = 1e-3\nvgg = vgg16.cuda()\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(vgg.parameters(), lr=0.001, momentum=0.9)\n#optimizer = optim.Adam(vgg.parameters(), lr=learning_rate)\n\ntrain_set = SeedlingDataset(X_train.file, y_train, '../input/train/', transform=train_trans)\nvalid_set = SeedlingDataset(X_val.file, y_val, '../input/train/', transform=validation_trans)\ntest_set = SeedlingDataset(X_test.file, y_test, '../input/train/', transform=validation_trans)\n\nloaders = {\n    'train': DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0),\n    'validation': DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=0),\n    'test': DataLoader(test_set, batch_size=batch_size, shuffle=True, num_workers=0)\n}\nepochs = 30\n\ndf_experiment8_result, test_accuracy_experiment8 = run_train_process(epochs, loaders, vgg, optimizer, criterion)\ndf_experiment8_result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"808756c36afa829152750d4b358bc57565ef48ff"},"cell_type":"code","source":"ax = df_experiment8_result[['train_accuracy', 'validation_accuracy']].plot(figsize=(16,9),)\ndf_experiment8_result[['train_loss', 'validation_loss']].plot(ax=ax.twinx())\nax.set_title(f'[Experiment 8] Test Accuracy: {test_accuracy_experiment8:.2f}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4115b0f33ab5f8b7a9db2e23c3d01acc277dd025"},"cell_type":"markdown","source":"# Combine Results"},{"metadata":{"trusted":true,"_uuid":"35b16f1fe175f1e829d47e979b0a8a3342913ead"},"cell_type":"code","source":"df_experiment2_result['experiment']       = 'experiment2'\ndf_experiment3_part1_result['experiment'] = 'experiment3_part1'\ndf_experiment3_part2_result['experiment'] = 'experiment3_part2'\ndf_experiment4_part1_result['experiment'] = 'experiment4_part1'\ndf_experiment4_part2_result['experiment'] = 'experiment4_part2'\ndf_experiment5_part1_result['experiment'] = 'experiment5_part1'\ndf_experiment5_part2_result['experiment'] = 'experiment5_part2'\ndf_experiment8_result['experiment']       = 'experiment8'\n\ndf_all_expirement_results = pd.concat([\n    df_experiment2_result,\n    df_experiment3_part1_result,\n    df_experiment3_part2_result,\n    df_experiment4_part1_result,\n    df_experiment4_part2_result,\n    df_experiment5_part1_result,\n    df_experiment5_part2_result,\n    df_experiment8_result\n]).rename_axis('epoch').reset_index()\n\ndf_all_expirement_results","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4104cfe21d2315e1468193befbd2e9e7d71c4283"},"cell_type":"code","source":"df_all_expirement_results.to_csv('df_all_expirement_results.csv', index=False, header=True)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true,"_uuid":"6abeea9b245edfdd9dc212094df6efaf573034b9"},"cell_type":"markdown","source":"# Kaggle Sumbission"},{"metadata":{"trusted":true,"_uuid":"c9cefb0de16422d55b0fa36363a2c4231fdc15e4"},"cell_type":"code","source":"data_dir = '../input/'\nsample_submission = pd.read_csv(os.path.join(data_dir, 'sample_submission.csv'))\n\nimage_size = (224, 224)\ntest_trans = transforms.Compose([\n    transforms.Resize(size=image_size),\n    transforms.ToTensor(),\n    transforms.Normalize(mean_img, std_img)\n])\n\nkaggle_train_set = SeedlingDataset(\n    sample_submission.file,\n    sample_submission.species,\n    '../input/test/',\n    transform=test_trans)\n\nkaggle_test_loader = DataLoader(kaggle_train_set, batch_size=8, shuffle=False, num_workers=0)\n\ndef predict(sample_submission, kaggle_test_loader, model):\n    predictions = []\n    for data, _ in kaggle_test_loader:\n        if use_cuda:\n            data = data.cuda() \n            data = Variable(data)\n            output = model(data)\n            pred = output.data.max(1)[1] # get the index of the max log-probability\n            predictions.extend(pred.tolist())\n\n    df_predictions = pd.DataFrame.from_dict({\n        'file': sample_submission.file,\n        'species': [num_to_class[p] for p in predictions]\n    })\n    return df_predictions","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true,"_uuid":"b03260f8b0b432b0fcae545de1dfbc0c1d7f9d8d"},"cell_type":"code","source":"df_predictions = predict(sample_submission, kaggle_test_loader, lenet)\ndf_predictions.to_csv('predictions_lenet.csv', index=False, header=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f820d5de9d04fb4d68bef6431509461e6059e2db"},"cell_type":"code","source":"df_predictions = predict(sample_submission, kaggle_test_loader, vgg)\ndf_predictions.to_csv('predictions_vgg.csv', index=False, header=True)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}