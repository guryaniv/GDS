{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"scrolled":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.\n\nimport tensorflow as tf\nimport keras.preprocessing.image\nimport sklearn.preprocessing\nimport sklearn.model_selection\nimport sklearn.metrics\nimport sklearn.linear_model\nimport sklearn.naive_bayes\nimport sklearn.tree\nimport sklearn.ensemble\nimport os;\nimport datetime  \nimport cv2 \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm  \n\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\n\ntrain = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\n","execution_count":1,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","scrolled":true,"trusted":false},"cell_type":"code","source":"train['label'].value_counts().sort_index()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"2c457eeb8041f97ad99b0f4bee23b4bd2be4cc3f","_cell_guid":"3067caa0-d2de-4ef7-b2be-1edd6deb6c0b","trusted":true},"cell_type":"code","source":"width = height = 28\nchannel = 1\ndata_train = train.iloc[:,1:].values.reshape(-1,width,height,channel).astype(np.float)\ndata_train_labels = train.iloc[:,0].values\ndata_train=data_train/data_train.max()","execution_count":2,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"4782f2da285228b75ed65c8aa66843e36f3b5ea8","_cell_guid":"baee3e1f-6c8e-4004-a182-238183f47c28","trusted":false},"cell_type":"code","source":"plt.figure(figsize=(15,9))\nfor i in range(50):\n    plt.subplot(5,10,1+i)\n    plt.title(data_train_labels[i])\n    plt.imshow(data_train[i].reshape(28,28), cmap=cm.binary)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"d26cc51640cffc22a9d164679e0c6f75d79d1ab0","_cell_guid":"1ab19165-ce0a-4f20-86eb-3a7c9d5ce2fd","trusted":true},"cell_type":"code","source":"nlabels = data_train_labels.shape[0]\nnclasses = np.unique(data_train_labels).shape[0]\ndata_train_labels_one_hot = np.zeros((nlabels,nclasses))\ndata_train_labels_one_hot.flat[np.arange(nlabels)*nclasses+data_train_labels.ravel()]=1","execution_count":3,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"77c6de4d09c80032ac52d88c74e07bb7fca0461c","_cell_guid":"83795d8f-819a-4c66-9a52-c839cbef6b9a","trusted":true},"cell_type":"code","source":"image_generator = keras.preprocessing.image.ImageDataGenerator(rotation_range = 10, \n                                                               width_shift_range = 0.1 , \n                                                               height_shift_range = 0.1, \n                                                               zoom_range = 0.1)","execution_count":4,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"a553f38d9c451f66440700455b622a7a7d75eb42","_cell_guid":"250be12f-35af-498a-8d12-3cc08507b25a","trusted":true},"cell_type":"code","source":"class cnn:\n    def __init__(self, args):\n        #hyperparameters\n        self.conv1_filter_size = args['conv1_filter'][1]\n        self.conv1_filter_num = args['conv1_filter'][0]\n        self.conv2_filter_size = args['conv2_filter'][1]\n        self.conv2_filter_num = args['conv2_filter'][0]\n        self.conv3_filter_size = args['conv3_filter'][1]\n        self.conv3_filter_num = args['conv3_filter'][0]\n        self.fullconn1_filter_num = args['fullconn1_filter_num']\n    \n        self.batch_size = args['batch_size']\n        self.keep_prob_ = args['keep_prob']\n        self.learn_rate_array = args['learn_rate_array']\n        self.learn_rate_change_every = args['learn_rate_change_every'] #learn_rate_step_size\n        \n        #parameters\n        self.learn_rate_ = self.learn_rate_array[0]\n        self.current_learn_rate_index = 0 #learn_rate_pos\n        self.current_index_in_epoch = 0 #index_in_epoch\n        self.current_progress_in_epoch = 0 #current_epoch\n        self.current_batch = 0 #n_log_step\n        self.log_every_progress = args['log_every_progress'] #log_step\n        self.use_tb_summary = args['use_tb_summary'] \n        self.use_tf_saver = args['use_tf_saver'] \n        self.name = args['name']\n        \n        # permutation array\n        self.perm_array = np.array([])\n    \n        self.image_generator = keras.preprocessing.image.ImageDataGenerator(\n                                rotation_range = 10, width_shift_range = 0.1 , height_shift_range = 0.1,\n                                zoom_range = 0.1)\n        \n    def next_batch(self):\n        start = self.current_index_in_epoch\n        self.current_index_in_epoch += self.batch_size\n        self.current_progress_in_epoch += self.batch_size/len(self.x_train)\n        \n        if len(self.perm_array) != len(self.x_train):\n            self.perm_array = np.arange(len(self.x_train))\n        \n        if start == 0:\n            np.random.shuffle(self.perm_array)\n        \n        if self.current_index_in_epoch > self.x_train.shape[0]:\n            np.random.shuffle(self.perm_array)\n            start = 0\n            self.current_index_in_epoch = self.batch_size\n            \n            if self.augmented:\n                self.x_train_aug = self.augment(self.x_train)\n                self.x_train_aug = self.x_train_aug/self.x_train_aug.max()\n                self.y_train_aug = self.y_train\n            \n        end = self.current_index_in_epoch\n        \n        if self.augmented:\n            x_train_ = self.x_train_aug[self.perm_array[start:end]]\n            y_train_ = self.y_train_aug[self.perm_array[start:end]]\n        else:\n            x_train_ = self.x_train[self.perm_array[start:end]]\n            y_train_ = self.y_train[self.perm_array[start:end]]\n        \n        return x_train_, y_train_\n    \n    def augment(self,imgs):\n        print('Augmenting images')\n        imgs_ = self.image_generator.flow(imgs.copy(), np.zeros(len(imgs)),\n                                    batch_size=len(imgs), shuffle = False).next()\n        return imgs_[0]\n    \n    def init_weight(self, shape, name = None):\n        initial = tf.truncated_normal(shape, stddev=0.1)\n        return tf.Variable(initial, name = name)\n    \n    def init_bias(self, shape, name = None):\n        initial = tf.constant(0.1, shape=shape)\n        return tf.Variable(initial, name = name)\n    \n    def conv2d(self, x, W, name = None):\n        return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME', name = name)\n    \n    def max_pool(self, x, name = None):\n        return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1],\n                              padding='SAME', name = name) \n    \n    def summary_variable(self, var, var_name):\n        with tf.name_scope(var_name):\n            mean = tf.reduce_mean(var)\n            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n            tf.summary.scalar('mean', mean)\n            tf.summary.scalar('stddev', stddev)\n            tf.summary.scalar('max', tf.reduce_max(var))\n            tf.summary.scalar('min', tf.reduce_min(var))\n            tf.summary.histogram('histogram', var)\n    \n    def create_graph(self):\n        tf.reset_default_graph()\n        \n        self.x_data = tf.placeholder(dtype=tf.float32, shape=[None,28,28,1], name = 'x_data')\n        self.y_data = tf.placeholder(dtype=tf.float32, shape=[None,10], name = 'y_data')\n        \n        #layer 1: conv -> max pool\n        self.conv1_W = self.init_weight([self.conv1_filter_size, self.conv1_filter_size, 1, self.conv1_filter_num], name = 'conv1_W')\n        self.conv1_b = self.init_bias([self.conv1_filter_num], name = 'conv1_b')\n        self.conv1_h = tf.nn.relu(self.conv2d(self.x_data, self.conv1_W)+self.conv1_b, name = 'conv1_h')\n        self.pool1_h = self.max_pool(self.conv1_h, name = 'pool1_h')\n        \n        #layer 2: conv -> max pool\n        self.conv2_W = self.init_weight([self.conv2_filter_size, self.conv2_filter_size, self.conv1_filter_num, self.conv2_filter_num], name = 'conv2_W')\n        self.conv2_b = self.init_bias([self.conv2_filter_num], name = 'conv2_b')\n        self.conv2_h = tf.nn.relu(self.conv2d(self.pool1_h, self.conv2_W)+self.conv2_b, name = 'conv2_h')\n        self.pool2_h = self.max_pool(self.conv2_h, name = 'pool2_h')\n        \n        #layer 3: conv -> max pool\n        self.conv3_W = self.init_weight([self.conv3_filter_size, self.conv3_filter_size, self.conv2_filter_num, self.conv3_filter_num], name = 'conv3_W')\n        self.conv3_b = self.init_bias([self.conv3_filter_num], name = 'conv3_b')\n        self.conv3_h = tf.nn.relu(self.conv2d(self.pool2_h, self.conv3_W)+self.conv3_b, name = 'conv3_h')\n        self.pool3_h = self.max_pool(self.conv3_h, name = 'pool3_h')\n        \n        #layer 4: fully connected\n        self.fullconn1_W = self.init_weight([4*4*self.conv3_filter_num,self.fullconn1_filter_num],name='fullconn1_W')\n        self.fullconn1_b = self.init_bias([self.fullconn1_filter_num], name='fullconn1_b')\n        self.pool3_h_flat = tf.reshape(self.pool3_h, [-1, 4*4*self.conv3_filter_num], name = 'pool3_h_flat')\n        self.fullconn1_h = tf.nn.relu(tf.matmul(self.pool3_h_flat,self.fullconn1_W)+self.fullconn1_b, name = 'fullconn1_h')\n\n        #dropout\n        self.keep_prob = tf.placeholder(dtype=tf.float32, name = 'keep_prob')\n        self.fullconn1_h_dropout = tf.nn.dropout(self.fullconn1_h, self.keep_prob, name = 'fullconn1_h_dropout')\n\n        #layer 5: fully connected\n        self.fullconn2_W = self.init_weight([self.fullconn1_filter_num,10], name = 'fullconn2_W')\n        self.fullconn2_b = self.init_bias([10], name = 'fullconn2_b')\n        self.pred = tf.add(tf.matmul(self.fullconn1_h_dropout, self.fullconn2_W),self.fullconn2_b, name = 'pred')\n        \n        #cost function\n        self.cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=self.y_data, logits=self.pred), name = 'cross_entropy')\n        \n        #training\n        self.learn_rate = tf.placeholder(dtype=tf.float32,name = 'learn_rate')\n        self.train_step = tf.train.AdamOptimizer(self.learn_rate).minimize(self.cross_entropy, name = 'train_step')\n        \n        self.y_pred_prob = tf.nn.softmax(self.pred, name = 'y_pred_prob')\n        \n        self.y_pred_correct = tf.equal(tf.argmax(self.y_pred_prob,1), tf.argmax(self.y_data,1), name = 'y_pred_correct')\n        \n        self.accuracy = tf.reduce_mean(tf.cast(self.y_pred_correct, dtype=tf.float32), name = 'accuracy')\n        \n        self.train_loss = tf.Variable(np.array([]),dtype=tf.float32, validate_shape=False, name = 'train_loss')\n        self.valid_loss = tf.Variable(np.array([]),dtype=tf.float32, validate_shape=False, name = 'valid_loss' )\n        self.train_acc = tf.Variable(np.array([]),dtype=tf.float32, validate_shape=False, name = 'train_acc')\n        self.valid_acc = tf.Variable(np.array([]),dtype=tf.float32, validate_shape=False, name = 'valid_acc' )\n        \n        return None\n    \n    def attach_summary(self, session):\n        self.use_tb_summary = True\n        self.summary_variable(self.conv1_W, 'conv1_W')\n        self.summary_variable(self.conv1_b, 'conv1_b')\n        self.summary_variable(self.conv2_W, 'conv2_W')\n        self.summary_variable(self.conv2_b, 'conv2_b')\n        self.summary_variable(self.conv3_W, 'conv3_W')     \n        self.summary_variable(self.conv3_b, 'conv3_b')\n        self.summary_variable(self.fullconn1_W, 'fullconn1_W')\n        self.summary_variable(self.fullconn1_b, 'fullconn1_b')\n        self.summary_variable(self.fullconn2_W, 'fullconn2_W')\n        self.summary_variable(self.fullconn2_b, 'fullconn2_b')\n        \n        tf.summary.scalar('cross_entropy', self.cross_entropy)\n        tf.summary.scalar('accuracy', self.accuracy)\n        \n        self.merged_summary = tf.summary.merge_all()\n        \n        timestamp = datetime.datetime.now().strftime('%d-%m-%Y_%H-%M-%S')\n        filepath = os.path.join(os.getcwd(), 'logs', (self.name+'_'+timestamp))\n        self.train_writer = tf.summary.FileWriter(os.path.join(filepath,'train'), session.graph)\n        self.valid_writer = tf.summary.FileWriter(os.path.join(filepath,'valid'), session.graph)\n        \n    def attach_saver(self):\n        self.use_tf_saver=True\n        self.saver=tf.train.Saver()\n    \n    def train_graph(self, session, x_train, y_train, x_valid, y_valid, n_epoch=1, augmented=False):\n        \n        self.augmented = augmented\n        \n        self.x_train=x_train\n        self.y_train=y_train\n        self.x_valid=x_valid\n        self.y_valid=y_valid\n        \n        if self.augmented:\n            print('Augmenting images')\n            self.x_train_aug=self.augment(self.x_train)\n            self.x_train_aug=self.x_train_aug/self.x_train_aug.max()\n            self.y_train_aug=self.y_train\n        \n        batch_per_epoch=self.x_train.shape[0]/self.batch_size\n        train_loss, train_acc, valid_loss, valid_acc = [],[],[],[]\n        \n        start = datetime.datetime.now();\n        print(datetime.datetime.now().strftime('%d-%m-%Y %H:%M:%S'),': start training')\n        print('learnrate = ',self.learn_rate_,', n_epoch = ', n_epoch, ', batch_size = ', self.batch_size)\n        \n        for i in range(int(n_epoch*batch_per_epoch)+1):\n            \n            self.current_learn_rate_index=int(self.current_progress_in_epoch//self.learn_rate_change_every)\n            if self.learn_rate_ != self.learn_rate_array[self.current_learn_rate_index]:\n                self.learn_rate_ = self.learn_rate_array[self.current_learn_rate_index]\n                print(datetime.datetime.now()-start,': set learn rate to %.6f'%self.learn_rate_)\n            \n            x_batch, y_batch = self.next_batch()\n            \n            session.run(self.train_step, feed_dict={self.x_data: x_batch, self.y_data: y_batch, \n                                                    self.keep_prob: self.keep_prob_, self.learn_rate: self.learn_rate_})\n        \n            if i%int(self.log_every_progress*batch_per_epoch) == 0 or i == int(n_epoch*batch_per_epoch):\n                self.current_batch +=1\n                \n                feed_dict_train = {\n                    self.x_data: self.x_train[self.perm_array[:len(self.x_valid)]],\n                    self.y_data: self.y_train[self.perm_array[:len(self.y_valid)]],\n                    self.keep_prob: 1.0\n                }\n                \n                feed_dict_valid = {\n                    self.x_data: self.x_valid,\n                    self.y_data: self.y_valid,\n                    self.keep_prob: 1.0\n                }\n                \n                if self.use_tb_summary:\n                    train_summary = session.run(self.merged_summary, feed_dict=feed_dict_train)\n                    valid_summary = session.run(self.merged_summary, feed_dict=feed_dict_valid)\n                    self.train_writer.add_summary(train_summary, self.current_batch)\n                    self.valid_writer.add_summary(valid_summary, self.current_batch)\n                    \n                train_loss.append(session.run(self.cross_entropy, feed_dict=feed_dict_train))\n                train_acc.append(self.accuracy.eval(session=session, feed_dict=feed_dict_train))  \n                \n                valid_loss.append(session.run(self.cross_entropy, feed_dict=feed_dict_valid))\n                valid_acc.append(self.accuracy.eval(session=session, feed_dict=feed_dict_valid))\n                \n                print('%.2f epoch: train/val loss = %.4f/%.4f, train/val acc = %.4f/%.4f'%(\n                    self.current_progress_in_epoch, train_loss[-1], valid_loss[-1], train_acc[-1], valid_acc[-1]))\n        \n        tl_c = np.concatenate([self.train_loss.eval(session=session), train_loss], axis = 0)\n        vl_c = np.concatenate([self.valid_loss.eval(session=session), valid_loss], axis = 0)\n        ta_c = np.concatenate([self.train_acc.eval(session=session), train_acc], axis = 0)\n        va_c = np.concatenate([self.valid_acc.eval(session=session), valid_acc], axis = 0)\n   \n        session.run(tf.assign(self.train_loss, tl_c, validate_shape = False))\n        session.run(tf.assign(self.valid_loss, vl_c , validate_shape = False))\n        session.run(tf.assign(self.train_acc, ta_c , validate_shape = False))\n        session.run(tf.assign(self.valid_acc, va_c , validate_shape = False))\n        \n        print('running time for training: ', datetime.datetime.now() - start)\n        return None\n  \n    def save_model(self, session):\n        if self.use_tf_saver:\n            filepath = os.path.join(os.getcwd(), self.name)\n            self.saver.save(session,filepath)\n        \n        if self.use_tb_summary:\n            self.train_writer.close()\n            self.valid_writer.close()\n        return None\n    \n    def predict(self, session, x_data):\n        return self.y_pred_prob.eval(session=session, feed_dict={self.x_data: x_data, self.keep_prob:1.0})\n    \n    def load_tensors(self, graph):\n        \n        self.x_data = graph.get_tensor_by_name(\"x_data:0\")\n        self.y_data = graph.get_tensor_by_name(\"y_data:0\")\n        \n        self.conv1_W = graph.get_tensor_by_name(\"conv1_W:0\")\n        self.conv1_b = graph.get_tensor_by_name(\"conv1_b:0\")\n        self.conv2_W = graph.get_tensor_by_name(\"conv2_W:0\")\n        self.conv2_b = graph.get_tensor_by_name(\"conv2_b:0\")\n        self.conv3_W = graph.get_tensor_by_name(\"conv3_W:0\")\n        self.conv3_b = graph.get_tensor_by_name(\"conv3_b:0\")\n        self.fullconn1_W = graph.get_tensor_by_name(\"fullconn1_W:0\")\n        self.fullconn1_b = graph.get_tensor_by_name(\"fullconn1_b:0\")\n        self.fullconn2_W = graph.get_tensor_by_name(\"fullconn2_W:0\")\n        self.fullconn2_b = graph.get_tensor_by_name(\"fullconn2_b:0\")\n        \n        # activation tensors\n        self.conv1_h = graph.get_tensor_by_name('conv1_h:0')  \n        self.pool1_h = graph.get_tensor_by_name('pool1_h:0')\n        self.conv2_h = graph.get_tensor_by_name('conv2_h:0')  \n        self.pool2_h = graph.get_tensor_by_name('pool2_h:0')\n        self.conv3_h = graph.get_tensor_by_name('conv3_h:0')  \n        self.pool3_h = graph.get_tensor_by_name('pool3_h:0')\n        \n        self.fullconn1_h = graph.get_tensor_by_name('fullconn1_h:0')\n        self.pred = graph.get_tensor_by_name('pred:0')\n        \n        # training and prediction tensors\n        self.learn_rate = graph.get_tensor_by_name(\"learn_rate:0\")\n        self.keep_prob = graph.get_tensor_by_name(\"keep_prob:0\")\n        self.cross_entropy = graph.get_tensor_by_name('cross_entropy:0')\n        self.train_step = graph.get_operation_by_name('train_step')\n        self.pred = graph.get_tensor_by_name('pred:0')\n        self.y_pred_prob = graph.get_tensor_by_name(\"y_pred_prob:0\")\n        self.y_pred_correct = graph.get_tensor_by_name('y_pred_correct:0')\n        self.accuracy = graph.get_tensor_by_name('accuracy:0')\n        \n        # tensor of stored losses and accuricies during training\n        self.train_loss = graph.get_tensor_by_name(\"train_loss:0\")\n        self.train_acc = graph.get_tensor_by_name(\"train_acc:0\")\n        self.valid_loss = graph.get_tensor_by_name(\"valid_loss:0\")\n        self.valid_acc = graph.get_tensor_by_name(\"valid_acc:0\")\n  \n        return None\n    \n    def get_loss(self, session):\n        train_loss = self.train_loss.eval(session = session)\n        valid_loss = self.valid_loss.eval(session = session)\n        return train_loss, valid_loss \n        \n    def get_accuracy(self, session):\n        train_acc = self.train_acc.eval(session = session)\n        valid_acc = self.valid_acc.eval(session = session)\n        return train_acc, valid_acc \n    \n    def get_weights(self, session):\n        conv1_W = self.conv1_W.eval(session = session)\n        conv2_W = self.conv2_W.eval(session = session)\n        conv3_W = self.conv3_W.eval(session = session)\n        fullconn1_W = self.fullconn1_W.eval(session = session)\n        fullconn2_W = self.fullconn2_W.eval(session = session)\n        return conv1_W, conv2_W, conv3_W, fullconn1_W, fullconn2_W\n    \n    def get_biases(self, session):\n        conv1_b = self.conv1_b.eval(session = session)\n        conv2_b = self.conv2_b.eval(session = session)\n        conv3_b = self.conv3_b.eval(session = session)\n        fullconn1_b = self.fullconn1_b.eval(session = session)\n        fullconn2_b = self.fullconn2_b.eval(session = session)\n        return conv1_b, conv2_b, conv3_b, fullconn1_b, fullconn2_b\n    \n    def load_session_from_file(self, filename):\n        tf.reset_default_graph()\n        filepath = os.path.join(os.getcwd(), filename + '.meta')\n        saver = tf.train.import_meta_graph(filepath)\n        print(filepath)\n        session = tf.Session()\n        saver.restore(session, mn)\n        graph = tf.get_default_graph()\n        self.load_tensors(graph)\n        return session\n    \n    def get_activations(self, session, x_data):\n        feed_dict = {self.x_data: x_data, self.keep_prob: 1.0}\n        conv1_h = self.conv1_h.eval(session = session, feed_dict = feed_dict)\n        pool1_h = self.pool1_h.eval(session = session, feed_dict = feed_dict)\n        conv2_h = self.conv2_h.eval(session = session, feed_dict = feed_dict)\n        pool2_h = self.pool2_h.eval(session = session, feed_dict = feed_dict)\n        conv3_h = self.conv3_h.eval(session = session, feed_dict = feed_dict)\n        pool3_h = self.pool3_h.eval(session = session, feed_dict = feed_dict)\n        fullconn1_h = self.fullconn1_h.eval(session = session, feed_dict = feed_dict)\n        fullconn2_h = self.pred.eval(session = session, feed_dict = feed_dict)\n        return conv1_h,pool1_h,conv2_h,pool2_h,conv3_h,pool3_h,fullconn1_h,fullconn2_h        \n                    ","execution_count":5,"outputs":[]},{"metadata":{"scrolled":true,"_uuid":"d6a2ae5dc80131e03240d7bf5c8209065b617ba9","_cell_guid":"c790b78d-cec8-4dd5-8553-1de86734b914","trusted":true},"cell_type":"code","source":"names=['nn0']\n\ncv_num=10\nkfold=sklearn.model_selection.KFold(cv_num,shuffle=True,random_state=42)\n\nfor i,(train_index,valid_index) in enumerate(kfold.split(data_train)):\n    start=datetime.datetime.now()\n    \n    x_train = data_train[train_index]\n    y_train = data_train_labels_one_hot[train_index]\n    x_valid = data_train[valid_index]\n    y_valid = data_train_labels_one_hot[valid_index]\n    \n    args={\n        'name': names[i],\n        'conv1_filter': [36, 3],\n        'conv2_filter': [36, 3],\n        'conv3_filter': [36, 3],\n        'fullconn1_filter_num': 576,\n        'batch_size': 50,\n        'keep_prob': 0.33,\n        'learn_rate_array': [1e-3,7.5e-4,5e-4,2.5e-4,1e-4,1e-4,1e-4,7.5e-5,\n                             0.5e-4,0.25e-4,1e-5,1e-5,7.5e-6,5e-6,2.5e-6,1e-6,\n                             7.5e-7,5e-7,2.5e-7,1e-7],\n        'learn_rate_change_every': 3,\n        'log_every_progress': 0.2,\n        'use_tb_summary': False,\n        'use_tf_saver': False\n    }\n    \n    nn_graph = cnn(args=args)\n    nn_graph.create_graph()\n    nn_graph.attach_saver()\n    \n    with tf.Session() as session:\n        nn_graph.attach_summary(session)\n        session.run(tf.global_variables_initializer())\n        \n        nn_graph.train_graph(session, x_train, y_train, x_valid, y_valid, n_epoch=1.0)\n        nn_graph.train_graph(session, x_train, y_train, x_valid, y_valid, n_epoch=14.0, augmented=True)\n        \n        nn_graph.save_model(session)\n    \n    if True:\n        break;\n\nprint('total running time for training: ', datetime.datetime.now() - start)\n","execution_count":7,"outputs":[]},{"metadata":{"_uuid":"c259cbb020a89faac0542d9596087f50638a736b","_cell_guid":"d4e349f9-3d1b-4ffa-a849-c2523a6d2430","trusted":true},"cell_type":"code","source":"y_valid_pred={}\nmn = names[0]\nnn_graph = cnn(args=args)\nsession = nn_graph.load_session_from_file(mn)\ny_valid_pred[mn] = nn_graph.predict(session, x_valid)\nsession.close()\n\ncnf_matrix = sklearn.metrics.confusion_matrix(\n    np.argmax(y_valid_pred[mn],1), np.argmax(y_valid,1)).astype(np.float32)\n\nlabels_array = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\nfig, ax = plt.subplots(1,figsize=(10,10))\nax = sns.heatmap(cnf_matrix, ax=ax, cmap=plt.cm.Greens, annot=True)\nax.set_xticklabels(labels_array)\nax.set_yticklabels(labels_array)\nplt.title('Confusion matrix of validation set')\nplt.ylabel('True digit')\nplt.xlabel('Predicted digit')\nplt.show();","execution_count":11,"outputs":[]},{"metadata":{"_uuid":"dbd0fae027a4a0b0259c0e33d7a045ab9e118148","_cell_guid":"2709bb4d-ac77-436d-bca1-cec107520a26","trusted":true},"cell_type":"code","source":"train_loss={}\nvalid_loss={}\ntrain_acc={}\nvalid_acc={}\nmn = names[0]\nnn_graph = cnn(args=args)\nsession = nn_graph.load_session_from_file(mn)\ntrain_loss[mn], valid_loss[mn] = nn_graph.get_loss(session)\ntrain_acc[mn], valid_acc[mn] = nn_graph.get_accuracy(session)\nsession.close()\n\nprint('final train/valid loss = %.4f/%.4f, train/valid accuracy = %.4f/%.4f'%(\n    train_loss[mn][-1], valid_loss[mn][-1], train_acc[mn][-1], valid_acc[mn][-1]))\n\nplt.figure(figsize=(10, 5));\nplt.subplot(1,2,1);\nplt.plot(np.arange(0,len(train_acc[mn])), train_acc[mn],'-b', label='Training')\nplt.plot(np.arange(0,len(valid_acc[mn])), valid_acc[mn],'-g', label='Validation')\nplt.legend(loc='lower right', frameon=False)\nplt.ylim(ymax = 1.1, ymin = 0.0)\nplt.ylabel('accuracy')\nplt.xlabel('log steps');\n\nplt.subplot(1,2,2)\nplt.plot(np.arange(0,len(train_loss[mn])), train_loss[mn],'-b', label='Training')\nplt.plot(np.arange(0,len(valid_loss[mn])), valid_loss[mn],'-g', label='Validation')\nplt.legend(loc='lower right', frameon=False)\nplt.ylim(ymax = 3.0, ymin = 0.0)\nplt.ylabel('loss')\nplt.xlabel('log steps');","execution_count":12,"outputs":[]},{"metadata":{"_uuid":"b22bf16c06cf8e7c93402484f698c689c55ef5e0","_cell_guid":"ca88a211-f84b-47ee-89c1-481aad7dee08","trusted":true},"cell_type":"code","source":"img_no = 5;\nmn = names[0]\nnn_graph = cnn(args=args)\nsession = nn_graph.load_session_from_file(mn)\n(conv1_h, pool1_h, conv2_h, pool2_h,conv3_h, pool3_h, fullconn1_h,\n fullconn2_h) = nn_graph.get_activations(session, data_train[img_no:img_no+1])\nsession.close()\n    \n# original image\nplt.figure(figsize=(15,9))\nplt.subplot(2,4,1)\nplt.imshow(data_train[img_no].reshape(28,28),cmap=cm.binary);\n\n# 1. convolution\nplt.subplot(2,4,2)\nplt.title('conv1_h ' + str(conv1_h.shape))\nconv1_h = np.reshape(conv1_h,(-1,28,28,6,6))\nconv1_h = np.transpose(conv1_h,(0,3,1,4,2))\nconv1_h = np.reshape(conv1_h,(-1,6*28,6*28))\nplt.imshow(conv1_h[0], cmap=cm.binary);\n\n# 1. max pooling\nplt.subplot(2,4,3)\nplt.title('pool1_h ' + str(pool1_h.shape))\npool1_h = np.reshape(pool1_h,(-1,14,14,6,6))\npool1_h = np.transpose(pool1_h,(0,3,1,4,2))\npool1_h = np.reshape(pool1_h,(-1,6*14,6*14))\nplt.imshow(pool1_h[0], cmap=cm.binary);\n\n# 2. convolution\nplt.subplot(2,4,4)\nplt.title('conv2_h ' + str(conv2_h.shape))\nconv2_h = np.reshape(conv2_h,(-1,14,14,6,6))\nconv2_h = np.transpose(conv2_h,(0,3,1,4,2))\nconv2_h = np.reshape(conv2_h,(-1,6*14,6*14))\nplt.imshow(conv2_h[0], cmap=cm.binary);\n\n# 2. max pooling\nplt.subplot(2,4,5)\nplt.title('pool2_h ' + str(pool2_h.shape))\npool2_h = np.reshape(pool2_h,(-1,7,7,6,6))\npool2_h = np.transpose(pool2_h,(0,3,1,4,2))\npool2_h = np.reshape(pool2_h,(-1,6*7,6*7))\nplt.imshow(pool2_h[0], cmap=cm.binary);\n\n# 3. convolution\nplt.subplot(2,4,6)\nplt.title('conv3_h ' + str(conv3_h.shape))\nconv3_h = np.reshape(conv3_h,(-1,7,7,6,6))\nconv3_h = np.transpose(conv3_h,(0,3,1,4,2))\nconv3_h = np.reshape(conv3_h,(-1,6*7,6*7))\nplt.imshow(conv3_h[0], cmap=cm.binary);\n\n# 3. max pooling\nplt.subplot(2,4,7)\nplt.title('pool3_h ' + str(pool3_h.shape))\npool3_h = np.reshape(pool3_h,(-1,4,4,6,6))\npool3_h = np.transpose(pool3_h,(0,3,1,4,2))\npool3_h = np.reshape(pool3_h,(-1,6*4,6*4))\nplt.imshow(pool3_h[0], cmap=cm.binary);\n\n# 4. FC layer\nplt.subplot(2,4,8)\nplt.title('fullconn1_h ' + str(fullconn1_h.shape))\nfullconn1_h = np.reshape(fullconn1_h,(-1,24,24))\nplt.imshow(fullconn1_h[0], cmap=cm.binary);\n\n# 5. FC layer\nnp.set_printoptions(precision=2)\nprint('fullconn2_h = ', fullconn2_h)","execution_count":13,"outputs":[]},{"metadata":{"_uuid":"0f2f1a712e1d8f11300e7a1fcac7df579c8abf29","_cell_guid":"d9104366-3098-41bd-8203-ae6755a16687","trusted":true},"cell_type":"code","source":"mn = names[0]\nnn_graph = cnn(args=args)\nsession = nn_graph.load_session_from_file(mn)\ny_valid_pred[mn] = nn_graph.predict(session, x_valid)\nsession.close()\n\ny_valid_pred_label = np.argmax(y_valid_pred[mn],1)\ny_valid_label = np.argmax(y_valid,1)\ny_val_false_index = []\n\nfor i in range(y_valid_label.shape[0]):\n    if y_valid_pred_label[i] != y_valid_label[i]:\n        y_val_false_index.append(i)\n\nprint('# false predictions: ', len(y_val_false_index),'out of', len(y_valid))\n\nplt.figure(figsize=(10,15))\nfor j in range(0,5):\n    for i in range(0,10):\n        if j*10+i<len(y_val_false_index):\n            plt.subplot(10,10,j*10+i+1)\n            plt.title('%d/%d'%(y_valid_label[y_val_false_index[j*10+i]],\n                               y_valid_pred_label[y_val_false_index[j*10+i]]))\n            plt.imshow(x_valid[y_val_false_index[j*10+i]].reshape(28,28),cmap=cm.binary)  ","execution_count":14,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6090a27a40b03965f5de78efa550e17255c0aaff"},"cell_type":"code","source":"data_test = test.iloc[:,0:].values.reshape(-1,width,height,1) # (28000,28,28,1) array\ndata_test = data_test.astype(np.float)\ndata_test = data_test/data_test.max()\n\ny_test_pred={}\ny_test_pred_labels={}\n\nmn = names[0]\nnn_graph = cnn(args=args)\nsession = nn_graph.load_session_from_file(mn)\ny_test_pred[mn] = nn_graph.predict(session, data_test )\nsession.close()\n\ny_test_pred_labels[mn] = np.argmax(y_test_pred[mn],1)\n\nprint(mn+': y_test_pred_labels[mn].shape = ', y_test_pred_labels[mn].shape)\nunique, counts = np.unique(y_test_pred_labels[mn], return_counts=True)\nprint(dict(zip(unique, counts)))\n\n# save predictions\nnp.savetxt('submission.csv', \n           np.c_[range(1,len(data_test)+1), y_test_pred_labels[mn]], \n           delimiter=',', \n           header = 'ImageId,Label', \n           comments = '', \n           fmt='%d')\n\nprint('submission.csv completed')","execution_count":10,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2bd45d6b53bb3c853db82954996a7306b65475d8"},"cell_type":"code","source":"plt.figure(figsize=(10,15))\nfor j in range(0,5):\n    for i in range(0,10):\n        plt.subplot(10,10,j*10+i+1)\n        plt.title('%d'%y_test_pred_labels[mn][j*10+i])\n        plt.imshow(data_test[j*10+i].reshape(28,28), cmap=cm.binary)","execution_count":16,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}