{"cells": [{"metadata": {"_cell_guid": "e3d9097b-325f-45a0-94f8-a7dc3b451047", "_uuid": "9f0e0f9451028fd7013eb9a57114dfefc2d4c907"}, "cell_type": "markdown", "source": ["# Characterizing Horror Stories\n", "\n", "This is one of my first explorations into NLP, something that I find quite fascinating. The real challenge for this exercise isn't necessarily the model selection and parameter (although this is a key decision), but more about the features that build the model. I decided to start simple an build more complex features as I go, so for now I will start with the ratio of words per punctuation and the most common types of punctuation used."]}, {"outputs": [], "cell_type": "code", "execution_count": null, "metadata": {"_cell_guid": "da775980-9555-4f10-817c-f1d31d65e816", "_uuid": "08677108ddd6946fc8e7f4bef73c870dbf1b2430"}, "source": ["import numpy as np # linear algebra\n", "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n", "import nltk\n", "from collections import Counter\n", "import string\n", "from subprocess import check_output\n", "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))"]}, {"outputs": [], "cell_type": "code", "execution_count": null, "metadata": {"_cell_guid": "b71b4a96-d4a1-4f98-a801-de3c5b86be13", "_uuid": "f9931aba9ed74ad51b5501af0466aa93717ddf6c"}, "source": ["train = pd.read_csv('../input/train.csv')\n", "train.head()"]}, {"metadata": {"_cell_guid": "f5feff67-d6a4-4f21-8aa2-8c1b407289b0", "_uuid": "63c3ff95d2eeac603d312a86c3f379eb78a5c4fc"}, "cell_type": "markdown", "source": ["The first step is to simple tokenize the text using the nltk package. This takes the string from the text feature and splits the words and puncuation into individual strings and places them in a list."]}, {"outputs": [], "cell_type": "code", "execution_count": null, "metadata": {"_cell_guid": "6381ecf5-fd5d-4644-8a46-a716d33ac5f4", "_uuid": "6f211a7f6d8a73597f9068e69f36cd7030714b47"}, "source": ["train['tokens'] = [nltk.word_tokenize(i) for i in train['text']]\n", "train.head()"]}, {"metadata": {"_cell_guid": "fc1ce768-3b8c-42e9-a02d-e0473be03502", "_uuid": "1d15509b268ddb979fc387b365aeea1184ca420e"}, "cell_type": "markdown", "source": ["To get to the words to puncutation ratio, I first need to ge the count of puncutations and the count of words for each record. I created a field for count of words, count of puncuation, and the ratio between them. I also created a token list without punctuation, which will be useful later. "]}, {"outputs": [], "cell_type": "code", "execution_count": null, "metadata": {"collapsed": true, "_cell_guid": "68679488-c551-4601-9d83-e6819346671a", "_uuid": "b4388ef57f35bb7356938fac62a9791ef8d40de5"}, "source": ["#Count of Punc.\n", "punc_list = [string.punctuation]\n", "def count_punc_in_tokens(series):\n", "    counts = []\n", "    for s in series:\n", "        counter = 0\n", "        for i in s:\n", "            if i in punc_list[0]:\n", "                counter += 1\n", "        counts.append(counter)\n", "    return counts\n", "train['count of punc'] = count_punc_in_tokens(train['tokens'])"]}, {"outputs": [], "cell_type": "code", "execution_count": null, "metadata": {"collapsed": true, "_cell_guid": "efa592a4-f50a-41e1-9aaa-f5802edd7708", "_uuid": "c44a7bdab0f5f903ba40bf0f2846efed295d0c96"}, "source": ["# Count of Non-Punc\n", "punc_list = [string.punctuation]\n", "def count_words_in_tokens(series):\n", "    counts = []\n", "    for s in series:\n", "        counter = 0\n", "        for i in s:\n", "            if i not in punc_list[0]:\n", "                counter += 1\n", "        counts.append(counter)\n", "    return counts\n", "train['count of not punc'] = count_words_in_tokens(train['tokens'])\n", "train['words_to_punc_ratio'] = train['count of not punc']/train['count of punc']\n", "def tokens_without_punc(series):\n", "    row = []\n", "    for s in series:\n", "        tokens = []\n", "        for i in s:\n", "            if i not in punc_list[0]:\n", "                tokens.append(i)\n", "        row.append(tokens)\n", "    return row\n", "\n", "train['token_without_punc'] = [i for i in tokens_without_punc(train['tokens'])]"]}, {"metadata": {"_cell_guid": "3b57602c-dce7-45a9-9510-86183d9cf1cb", "_uuid": "58b9f91b2da2573ef3e5d36cb18928928bb6f9b2"}, "cell_type": "markdown", "source": ["Next, I wanted to see if the authors had distinct parts of speech (POS) they liked to use. I used nltk for this as well. The nltk module pos_tag creates a list of tuples which contain a POS and the token from the tokens without punc list. I then used the Counter() class from collections to generate a list of dicts where the key is the POS and the value is the count of that POS in the list of tokens. From these dicts, I joined them into a single df with each unique POS acting as the index and each column representing the counts for each POS by record in the training data. This allows me to easily transpose the df and join it by the index to the rest of the training date, making each POS count a new feature. We probably do not want to use all of these features in our model, so I droped the POS where the sum of the counts was less than the median sum of the counts. "]}, {"outputs": [], "cell_type": "code", "execution_count": null, "metadata": {"_cell_guid": "df8f45cd-0a82-4885-9719-a91aa54ed850", "_uuid": "a31875c1f5bb88d8bee5d0d312f48881c5c3ee27"}, "source": ["def pos_used(series):\n", "    counts = []\n", "    itter = 0\n", "    for s in series:\n", "        cnt = Counter()\n", "        tags = nltk.pos_tag(s)\n", "        print(tags)\n", "        pos_list = []\n", "        for i in tags:\n", "            pos_list.append(i[1])\n", "        for i in pos_list:\n", "            cnt[i] += 1\n", "        count = list(cnt.items())\n", "        df = pd.DataFrame(count,  columns=['POS', str(itter)])\n", "        df.set_index('POS', inplace = True)\n", "        counts.append(df)\n", "        itter += 1\n", "    #[i.set_index('POS', inplace = True) for i in counts]\n", "    pos_df = pd.concat(counts, axis=1)\n", "    return pos_df\n", "\n", "pos_used = pos_used(train['token_without_punc'])"]}, {"outputs": [], "cell_type": "code", "execution_count": null, "metadata": {"_cell_guid": "4b27c4bc-9a23-4cee-b854-be2a9a1e7510", "_uuid": "11abeb5b3341ddfaed2e74ad087d6539f1074ddd"}, "source": ["pos_used.fillna(0,inplace=True)\n", "pos_used['SUM'] = pos_used.sum(axis=1)\n", "pos_used = pos_used[pos_used['SUM'] >= pos_used['SUM'].quantile(.5)]\n", "pos_used.drop(['SUM'], axis=1, inplace=True)"]}, {"outputs": [], "cell_type": "code", "execution_count": null, "metadata": {"_cell_guid": "adeaf630-88e0-43e1-8084-34529eb7b364", "_uuid": "104bac3aa7a78c88ff0b6144b74bf293bac45d37"}, "source": ["#print(pos_used.T.shape) # 19579, 38\n", "#print(train.shape) # 19579, 8\n", "pos_df = pos_used.transpose()\n", "pos_df.reset_index(inplace = True)\n", "pos_df.drop(['index'], inplace = True, axis= 1)"]}, {"outputs": [], "cell_type": "code", "execution_count": null, "metadata": {"_cell_guid": "480af1d1-da95-4fbe-a442-72156e95951c", "_uuid": "af5145e9a2bf1fe0230f6c5acaf5e27c63ad8489"}, "source": ["train2 = pd.concat([train,pos_df], axis = 1)"]}, {"outputs": [], "cell_type": "code", "execution_count": null, "metadata": {"collapsed": true, "_cell_guid": "cc5d6356-cedf-42f0-928d-9aea58570f6b", "_uuid": "c6ff380b95bff9323a9131a74346f8f9cd8d76a9"}, "source": ["train = train2.drop(['id', 'text', 'tokens', 'token_without_punc','count of punc', 'count of not punc'], axis = 1)"]}, {"outputs": [], "cell_type": "code", "execution_count": null, "metadata": {"_cell_guid": "e6419aa5-9d98-4092-af96-96333b80b17f", "_uuid": "e201a16d33b9bcc6ed4e2674e7f995d315977425"}, "source": ["train.head()"]}, {"metadata": {"_cell_guid": "1f2ecdb8-40b6-4298-961a-c0749c92bd1c", "_uuid": "6ff9f0a690df223a63b3bd3f843d5c2ac8c48d4f"}, "cell_type": "markdown", "source": ["Finally, on to the model. I decided to start with a one-vs-all SVM model and random forest classifier. I standardized the features first then randomly split the rows 60:40 into training and validation sets.  I did not do any hyperparameter tweaking to these models. "]}, {"outputs": [], "cell_type": "code", "execution_count": null, "metadata": {"_cell_guid": "b7d604dc-88f7-48c5-ac60-aa596f564e74", "_uuid": "c89e0e2aae1c9133597ba070b392b866f8075f9c"}, "source": ["from sklearn.preprocessing import StandardScaler\n", "from sklearn.svm import SVC\n", "from sklearn.model_selection import train_test_split\n", "from sklearn.model_selection import cross_val_score\n", "from sklearn.multiclass import OneVsRestClassifier\n", "from sklearn.ensemble import RandomForestClassifier\n", "y = train.iloc[:,0]\n", "X = train.iloc[:,1:]\n", "X = StandardScaler().fit_transform(X)\n", "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = .4, random_state = 42)"]}, {"outputs": [], "cell_type": "code", "execution_count": null, "metadata": {"_cell_guid": "aeb861fe-65a6-4362-a068-e76e5b97c0ad", "_uuid": "9ea49cb0898c3ac070cc7f38f0f8845c924e0968"}, "source": ["clf = OneVsRestClassifier(SVC(kernel = 'linear'))\n", "clf.fit(X_train,y_train)\n", "cross_val_score(clf, X_train, y_train, cv=3, scoring = 'accuracy')"]}, {"outputs": [], "cell_type": "code", "execution_count": null, "metadata": {"_cell_guid": "cee6c7f1-f89d-461e-9afe-e6806013aa7f", "_uuid": "31658f4d8c4c81fbac1c4daa08c91386891b418f"}, "source": ["clf = RandomForestClassifier()\n", "clf.fit(X_train,y_train)\n", "cross_val_score(clf, X_train, y_train, cv=3, scoring = 'accuracy')"]}, {"metadata": {"collapsed": true, "_cell_guid": "bd5aec45-dec6-4c8a-9846-aa4b3aa41336", "_uuid": "4a7926fb566e46666fa2d3bb1e317d2dd9df7d05"}, "cell_type": "markdown", "source": ["The accuracy score for the One-vs-rest SVM classifier showed the best results, but a low score in general. Both models produced a close score, however, which points to a lack of feature seperation over model choice. So I will brain storm some more features to experiment with and "]}], "metadata": {"language_info": {"file_extension": ".py", "mimetype": "text/x-python", "name": "python", "version": "3.6.3", "nbconvert_exporter": "python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3"}, "kernelspec": {"language": "python", "name": "python3", "display_name": "Python 3"}}, "nbformat_minor": 1, "nbformat": 4}