{"cells":[{"metadata":{"_uuid":"a574ff4248fd94eff334596839f228a1e20bb5fc"},"cell_type":"markdown","source":"This kernel gets 0.854 on Public LB and takes 16666 to run (both training and prediction). The model's performance can be definitely be improved by using some other tricks, one obvious way is to use KFold Cross Validation. You can train with lovasz loss to improve it further. I wanted to keep the kernel simple and run it within time limit, so it is a no frills models.\nI am really thankful to the kaggle community for sharing their insights. I am thankful to kaggle for providing\nGPUs and allowing multiple GPU Kernels in parallel. I dont' have access tp a good GPU so  I wouldn't have been able to experiment without these kernels.\n\nThe major highlights of the kernel are:\n1.  The Encoder and Decoder Architecture\nIn the competition, everyone seems to be using ResNet34 encoder.\nKeras doesn't provide pre-trained model weights for ResNet34. And kaggle didn't \nsupport pytorch v0.4 until recently. So, i had to look for other ways..\nI experimented with all the pretrained models with different decoder architectures.\nAfter lots of interesting experiments, I found that pretrained Xception model with ResNet decoder works best.\n2. Use of Pseudo-Labelling.\nAfter reaching 0.83+, my models started overfitting on the training set.\nSo, i took multiple models using different encoder architecture and found \npredictions in the test set that are common for every model.\nFor example, if three different models predict nearly the same mask on the \ntest set, its highly likely the predicted mask is correct. \nIn this way i generated two types of masks no-salt masks and some-salt\nmasks.I used these masks while training.\nIn the train set nearly 0.39% images don't have mask. I maintained this ratio\nwhile using these masks for training (0.39*6000=2340)\n3. Using normalized Gradient optimizer\nThe basic idea is to normalize each layer of the mini-batch stochastic gradient.\nIt has been shown that the normalized gradient methods having constant step size with occasionally decay, such as SGD with momentum,\nhave better performance in the deep convolution neural networks, than optimizers with adaptive step sizes likt Adam.\nThis optimizer was very useful for training my network. Normal SGD takes a very long time to converge.\n4. Stochastic Weight Averaging (SWA)\nIt has been shown in recent paper that SWA finds much broader optima than SGD. I got a boost of around 0.003 with SWA.\nIt is extremely easy to implement and has very little computational overhead !!\n"},{"metadata":{"_uuid":"f28a30a0101733475a5ad3ad3a191960222d7ef2"},"cell_type":"markdown","source":"# Some Useful Links"},{"metadata":{"_uuid":"346f2e494bff00d78960b8f7fddb58dce4ab7fa5"},"cell_type":"markdown","source":"Useful Keras Implementations\n\nTitu1994 github repo - https://github.com/titu1994\n\nKaggle Kernels \n\nJack (Jiaxin) Shao: https://www.kaggle.com/shaojiaxin/u-net-with-simple-resnet-blocks-v2-new-loss\n\nBruno G. do Amaral: https://www.kaggle.com/bguberfain/elastic-transform-for-data-augmentation\n\nPeter Hönigschmid: https://www.kaggle.com/phoenigs/u-net-dropout-augmentation-stratification\n\nJuan C EsquivelTGS: https://www.kaggle.com/jcesquiveld/tgs-vanilla-u-net-with-simple-augmentation\n\nNPHard: https://www.kaggle.com/meaninglesslives/using-resnet50-pretrained-model-in-keras\n\nKaggle Discussions\n\nhttps://www.kaggle.com/c/tgs-salt-identification-challenge/discussion/64875\n\nhttps://www.kaggle.com/c/tgs-salt-identification-challenge/discussion/66568\n\nhttps://www.kaggle.com/c/tgs-salt-identification-challenge/discussion/64093"},{"metadata":{"_uuid":"1c5ef627f3327353ea3df4223e85d9eebd1e598c"},"cell_type":"markdown","source":"# Loading Libraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport gc\nimport keras\n\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn-white')\nimport seaborn as sns\nsns.set_style(\"white\")\n\nfrom sklearn.model_selection import train_test_split\n\nfrom skimage.transform import resize\nimport tensorflow as tf\nimport keras.backend as K\nfrom keras.losses import binary_crossentropy\n\nfrom keras.preprocessing.image import load_img\nfrom keras import Model\nfrom keras.callbacks import  ModelCheckpoint\nfrom keras.layers import Input, Conv2D, Conv2DTranspose, MaxPooling2D, concatenate, Dropout,BatchNormalization\nfrom keras.layers import Conv2D, Concatenate, MaxPooling2D\nfrom keras.layers import UpSampling2D, Dropout, BatchNormalization\nfrom tqdm import tqdm_notebook\nfrom keras import initializers\nfrom keras import regularizers\nfrom keras import constraints\nfrom keras.utils import conv_utils\nfrom keras.utils.data_utils import get_file\nfrom keras.engine.topology import get_source_inputs\nfrom keras.engine import InputSpec\nfrom keras import backend as K\nfrom keras.layers import LeakyReLU\nfrom keras.layers import ZeroPadding2D\nfrom keras.losses import binary_crossentropy\nimport keras.callbacks as callbacks\nfrom keras.callbacks import Callback\nfrom keras.applications.xception import Xception\nfrom keras.layers import multiply\n\n\nfrom keras import optimizers\nfrom keras.legacy import interfaces\nfrom keras.utils.generic_utils import get_custom_objects\n\nfrom keras.engine.topology import Input\nfrom keras.engine.training import Model\nfrom keras.layers.convolutional import Conv2D, UpSampling2D, Conv2DTranspose\nfrom keras.layers.core import Activation, SpatialDropout2D\nfrom keras.layers.merge import concatenate\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.layers.pooling import MaxPooling2D\nfrom keras.layers import Input,Dropout,BatchNormalization,Activation,Add\nfrom keras.regularizers import l2\nfrom keras.layers.core import Dense, Lambda\nfrom keras.layers.merge import concatenate, add\nfrom keras.layers import GlobalAveragePooling2D, Reshape, Dense, multiply, Permute\nfrom keras.optimizers import SGD","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"962c2c6775b5fcf605df8e7c59cbcabe6ba9ceaa"},"cell_type":"markdown","source":"# Params and helpers"},{"metadata":{"trusted":true,"_uuid":"e54e151245d665e42bb95d9cf2e1a33cb9440e48"},"cell_type":"code","source":"img_size_ori = 101\nimg_size_target = 128\n\ndef upsample(img):\n    if img_size_ori == img_size_target:\n        return img\n    return resize(img, (img_size_target, img_size_target), mode='constant', preserve_range=True)\n    \ndef downsample(img):\n    if img_size_ori == img_size_target:\n        return img\n    return resize(img, (img_size_ori, img_size_ori), mode='constant', preserve_range=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"530c358f2868a444e8233936996463a66c2cc4f3"},"cell_type":"markdown","source":"# Loading of training/testing ids and depths"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(\"../input/tgs-salt-identification-challenge/train.csv\", index_col=\"id\", usecols=[0])\nval_ids = pd.read_csv('../input/validation-ids/validation_ids.csv')\ntrain_df = train_df.drop(val_ids.iloc[:,0].values)\n\nno_salt_ids =  pd.read_csv(\"../input/pseudolabel-gen/no_salt_ids.csv\", index_col=\"id\", usecols=[0])\nno_salt_ids = no_salt_ids.sample(2340)\nsome_salt_ids =  pd.read_csv(\"../input/pseudolabel-gen/some_salt_ids.csv\", index_col=\"id\")\nsome_salt_ids = some_salt_ids.sample(3660)\n\ndepths_df = pd.read_csv(\"../input/tgs-salt-identification-challenge/depths.csv\", index_col=\"id\")\ntrain_df = train_df.join(depths_df)\ntest_df = depths_df[~depths_df.index.isin(train_df.index)]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"24d7f3d982bfa582b222f012129acdda55282b6d"},"cell_type":"markdown","source":"# Read images and masks\nLoad the images and masks into the DataFrame and divide the pixel values by 255."},{"metadata":{"trusted":true,"_uuid":"0ef43aefbd9434f36c25e2b1323990dc7e2001f7"},"cell_type":"code","source":"train_df[\"images\"] = [np.array(load_img(\"../input/tgs-salt-identification-challenge/train/images/{}.png\".format(idx), grayscale=True)) / 255 for idx in tqdm_notebook(train_df.index)]\ntrain_df[\"masks\"] = [np.array(load_img(\"../input/tgs-salt-identification-challenge/train/masks/{}.png\".format(idx), grayscale=True)) / 255 for idx in tqdm_notebook(train_df.index)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d22070481e4ee16e064f921c8dec53db1821b234"},"cell_type":"code","source":"def rle_decode(rle_mask):\n    '''\n    rle_mask: run-length as string formated (start length)\n    shape: (height,width) of array to return \n    Returns numpy array, 1 - mask, 0 - background\n\n    '''\n    if str(rle_mask)==str(np.nan):\n        return np.zeros((101,101))\n    s = rle_mask.split()\n    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n    starts -= 1\n    ends = starts + lengths\n    img = np.zeros(101*101, dtype=np.uint8)\n    for lo, hi in zip(starts, ends):\n        img[lo:hi] = 1\n    return img.reshape(101,101)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6515e94826715d7f062deed602cf59abb1e43b75"},"cell_type":"markdown","source":"# Generating Pseudo Labels"},{"metadata":{"_uuid":"2b998eaeee905e5ac8c912fe98bc47d8394a6a91"},"cell_type":"markdown","source":"No-Salt images"},{"metadata":{"trusted":true,"_uuid":"e398db269243f8b02088c8199d2cd81a0dc95722"},"cell_type":"code","source":"train_df_temp = pd.DataFrame()\ntrain_df_temp['id'] = no_salt_ids.index\ntrain_df_temp = train_df_temp.set_index('id')\ntrain_df_temp = train_df_temp.join(depths_df)\ntrain_df_temp[\"images\"] = [np.array(load_img(\"../input/tgs-salt-identification-challenge/test/images/{}.png\".format(idx), grayscale=True)) / 255 for idx in tqdm_notebook(no_salt_ids.index)]\ntrain_df_temp[\"masks\"] = [np.zeros((img_size_ori,img_size_ori)) for idx in tqdm_notebook(no_salt_ids.index)]\ntrain_df = train_df.append(train_df_temp)\ndel train_df_temp","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0d07cea5070929b618dccb9b7b6a7e8a449e40f3"},"cell_type":"markdown","source":"Some Salt images"},{"metadata":{"trusted":true,"_uuid":"b39943b4df5a7b742832caa5cad8373130fce5f4"},"cell_type":"code","source":"train_df_temp = pd.DataFrame()\ntrain_df_temp['id'] = some_salt_ids.index\ntrain_df_temp = train_df_temp.set_index('id')\ntrain_df_temp = train_df_temp.join(depths_df)\ntrain_df_temp[\"images\"] = [np.array(load_img(\"../input/tgs-salt-identification-challenge/test/images/{}.png\".format(idx), grayscale=True)) / 255 for idx in tqdm_notebook(some_salt_ids.index)]\ntrain_df_temp[\"masks\"] = [np.fliplr(np.rot90(rle_decode(some_salt_ids.loc[idx,'rle_mask']),-1)) for idx in tqdm_notebook(some_salt_ids.index)]\ntrain_df = train_df.append(train_df_temp)\ndel train_df_temp\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1f80df360502bda9cc0ec4fee68deee6d041b0b7"},"cell_type":"markdown","source":"# Visualizing the class coverage"},{"metadata":{"trusted":true,"_uuid":"18d2aa182a44c65a87c75f41047c653a79bc1c3f"},"cell_type":"code","source":"train_df[\"coverage\"] = train_df.masks.map(np.sum) / pow(img_size_ori, 2)\ndef cov_to_class(val):    \n    for i in range(0, 11):\n        if val * 10 <= i :\n            return i\n        \ntrain_df[\"coverage_class\"] = train_df.coverage.map(cov_to_class)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0f4ccfb9d213e933323e13d26a0e4fc9f1f5d9d0"},"cell_type":"code","source":"fig, axs = plt.subplots(1, 2, figsize=(15,5))\nsns.distplot(train_df.coverage, kde=False, ax=axs[0])\nsns.distplot(train_df.coverage_class, bins=10, kde=False, ax=axs[1])\nplt.suptitle(\"Salt coverage\")\naxs[0].set_xlabel(\"Coverage\")\naxs[1].set_xlabel(\"Coverage class\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e5de09a328f9679a464a6ad73dcbd066aef1415d"},"cell_type":"markdown","source":"# Loading Validation Set"},{"metadata":{"_uuid":"03647664c1ab7c745ef6db377c3ebed67dc5b124"},"cell_type":"markdown","source":"I have fixed the validation set for all my experiments. This allows me to easily compare different model performance."},{"metadata":{"trusted":true,"_uuid":"35fe0f07472453386d1ecbf79327e7d044546461"},"cell_type":"code","source":"ids_valid = val_ids.iloc[:,0].values\ntemp_df = pd.DataFrame()\ntemp_df[\"images\"] = [np.array(load_img(\"../input/tgs-salt-identification-challenge/train/images/{}.png\".format(idx), grayscale=True)) / 255 for idx in tqdm_notebook(ids_valid)]\ntemp_df[\"masks\"] = [np.array(load_img(\"../input/tgs-salt-identification-challenge/train/masks/{}.png\".format(idx), grayscale=True)) / 255 for idx in tqdm_notebook(ids_valid)]\nx_valid = np.array(temp_df.images.map(upsample).tolist()).reshape(-1, img_size_target, img_size_target, 1)\ny_valid = np.array(temp_df.masks.map(upsample).tolist()).reshape(-1, img_size_target, img_size_target, 1)\ndel temp_df\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2d3c3157512d11e71ac74ce51a937b85bedfe1d1"},"cell_type":"code","source":"ids_train,x_train,y_train = train_df.index.values,\\\n    np.array(train_df.images.map(upsample).tolist()).reshape(-1, img_size_target, img_size_target, 1), \\\n    np.array(train_df.masks.map(upsample).tolist()).reshape(-1, img_size_target, img_size_target, 1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7e07a9adca04095f690733343ab30cdc8f3f95a2"},"cell_type":"markdown","source":"# Calculating IOU"},{"metadata":{"trusted":true,"_uuid":"45410fdbd533d546a578d9dc29982a1657b8dfa9"},"cell_type":"code","source":"# https://www.kaggle.com/cpmpml/fast-iou-metric-in-numpy-and-tensorflow\ndef get_iou_vector(A, B):\n    # Numpy version    \n    batch_size = A.shape[0]\n    metric = 0.0\n    for batch in range(batch_size):\n        t, p = A[batch], B[batch]\n        true = np.sum(t)\n        pred = np.sum(p)\n        \n        # deal with empty mask first\n        if true == 0:\n            metric += (pred == 0)\n            continue\n        \n        # non empty mask case.  Union is never empty \n        # hence it is safe to divide by its number of pixels\n        intersection = np.sum(t * p)\n        union = true + pred - intersection\n        iou = intersection / union\n        \n        # iou metrric is a stepwise approximation of the real iou over 0.5\n        iou = np.floor(max(0, (iou - 0.45)*20)) / 10\n        \n        metric += iou\n        \n    # teake the average over all images in batch\n    metric /= batch_size\n    return metric\n\n\ndef my_iou_metric(label, pred):\n    # Tensorflow version\n    return tf.py_func(get_iou_vector, [label, pred > 0.5], tf.float64)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d88d952ca7c27fd0e2411eb63ec16fd25cb8ebec"},"cell_type":"code","source":"def dice_coef(y_true, y_pred):\n    y_true_f = K.flatten(y_true)\n    y_pred = K.cast(y_pred, 'float32')\n    y_pred_f = K.cast(K.greater(K.flatten(y_pred), 0.5), 'float32')\n    intersection = y_true_f * y_pred_f\n    score = 2. * K.sum(intersection) / (K.sum(y_true_f) + K.sum(y_pred_f))\n    return score\n\ndef dice_loss(y_true, y_pred):\n    smooth = 1.\n    y_true_f = K.flatten(y_true)\n    y_pred_f = K.flatten(y_pred)\n    intersection = y_true_f * y_pred_f\n    score = (2. * K.sum(intersection) + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n    return 1. - score\n\ndef bce_dice_loss(y_true, y_pred):\n    return binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)\n\ndef bce_logdice_loss(y_true, y_pred):\n    return binary_crossentropy(y_true, y_pred) - K.log(1. - dice_loss(y_true, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"98c8b0aff0989293ce43592a82eb3c5dcf8bbee9"},"cell_type":"code","source":"class SnapshotCallbackBuilder:\n    def __init__(self, nb_epochs, nb_snapshots, init_lr=0.1):\n        self.T = nb_epochs\n        self.M = nb_snapshots\n        self.alpha_zero = init_lr\n\n    def get_callbacks(self, model_prefix='Model'):\n\n        callback_list = [\n            callbacks.ModelCheckpoint(\"./keras.model\",monitor='val_my_iou_metric', \n                                   mode = 'max', save_best_only=True, verbose=1),\n            swa,\n            callbacks.LearningRateScheduler(schedule=self._cosine_anneal_schedule)\n        ]\n\n        return callback_list\n\n    def _cosine_anneal_schedule(self, t):\n        cos_inner = np.pi * (t % (self.T // self.M))  # t - 1 is used when t has 1-based indexing.\n        cos_inner /= self.T // self.M\n        cos_out = np.cos(cos_inner) + 1\n        return float(self.alpha_zero / 2 * cos_out)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dc9e6a9ab02ceeb1cb2491568870ab1bd881b5f2"},"cell_type":"markdown","source":"Useful Model Blocks"},{"metadata":{"trusted":true,"_uuid":"a8e87341f77e48a6c54de11f8dc72f50f43b8637"},"cell_type":"code","source":"def convolution_block(x, filters, size, strides=(1,1), padding='same', activation=True):\n    x = Conv2D(filters, size, strides=strides, padding=padding)(x)\n    x = BatchNormalization()(x)\n    if activation == True:\n        x = LeakyReLU(alpha=0.1)(x)\n    return x\n\ndef residual_block(blockInput, num_filters=16):\n    x = LeakyReLU(alpha=0.1)(blockInput)\n    x = BatchNormalization()(x)\n    blockInput = BatchNormalization()(blockInput)\n    x = convolution_block(x, num_filters, (3,3) )\n    x = convolution_block(x, num_filters, (3,3), activation=False)\n    x = Add()([x, blockInput])\n    return x","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f464edae7c4c308bc70dbc50140ee1c6dee6470"},"cell_type":"markdown","source":"# Defining UXception Model"},{"metadata":{"_uuid":"dd410e24ea4348c42c272e9bcbda0ffee316092c"},"cell_type":"markdown","source":"As mentioned above, this model uses pretrained Xception model as encoder. It uses Residual blocks in the decoder part,"},{"metadata":{"trusted":true,"_uuid":"95fee2ea26eb7d84ebbf325a2de0128814675e7c"},"cell_type":"code","source":"def UXception(input_shape=(None, None, 3)):\n\n    backbone = Xception(input_shape=input_shape,weights='imagenet',include_top=False)\n    input = backbone.input\n    start_neurons = 16\n\n    conv4 = backbone.layers[121].output\n    conv4 = LeakyReLU(alpha=0.1)(conv4)\n    pool4 = MaxPooling2D((2, 2))(conv4)\n    pool4 = Dropout(0.1)(pool4)\n    \n     # Middle\n    convm = Conv2D(start_neurons * 32, (3, 3), activation=None, padding=\"same\")(pool4)\n    convm = residual_block(convm,start_neurons * 32)\n    convm = residual_block(convm,start_neurons * 32)\n    convm = LeakyReLU(alpha=0.1)(convm)\n    \n    # 10 -> 20\n    deconv4 = Conv2DTranspose(start_neurons * 16, (3, 3), strides=(2, 2), padding=\"same\")(convm)\n    uconv4 = concatenate([deconv4, conv4])\n    uconv4 = Dropout(0.1)(uconv4)\n    \n    uconv4 = Conv2D(start_neurons * 16, (3, 3), activation=None, padding=\"same\")(uconv4)\n    uconv4 = residual_block(uconv4,start_neurons * 16)\n    uconv4 = residual_block(uconv4,start_neurons * 16)\n    uconv4 = LeakyReLU(alpha=0.1)(uconv4)\n    \n    # 10 -> 20\n    deconv3 = Conv2DTranspose(start_neurons * 8, (3, 3), strides=(2, 2), padding=\"same\")(uconv4)\n    conv3 = backbone.layers[31].output\n    uconv3 = concatenate([deconv3, conv3])    \n    uconv3 = Dropout(0.1)(uconv3)\n    \n    uconv3 = Conv2D(start_neurons * 8, (3, 3), activation=None, padding=\"same\")(uconv3)\n    uconv3 = residual_block(uconv3,start_neurons * 8)\n    uconv3 = residual_block(uconv3,start_neurons * 8)\n    uconv3 = LeakyReLU(alpha=0.1)(uconv3)\n\n    # 20 -> 40\n    deconv2 = Conv2DTranspose(start_neurons * 4, (3, 3), strides=(2, 2), padding=\"same\")(uconv3)\n    conv2 = backbone.layers[21].output\n    conv2 = ZeroPadding2D(((1,0),(1,0)))(conv2)\n    uconv2 = concatenate([deconv2, conv2])\n        \n    uconv2 = Dropout(0.1)(uconv2)\n    uconv2 = Conv2D(start_neurons * 4, (3, 3), activation=None, padding=\"same\")(uconv2)\n    uconv2 = residual_block(uconv2,start_neurons * 4)\n    uconv2 = residual_block(uconv2,start_neurons * 4)\n    uconv2 = LeakyReLU(alpha=0.1)(uconv2)\n    \n    # 40 -> 80\n    deconv1 = Conv2DTranspose(start_neurons * 2, (3, 3), strides=(2, 2), padding=\"same\")(uconv2)\n    conv1 = backbone.layers[11].output\n    conv1 = ZeroPadding2D(((3,0),(3,0)))(conv1)\n    uconv1 = concatenate([deconv1, conv1])\n    \n    uconv1 = Dropout(0.1)(uconv1)\n    uconv1 = Conv2D(start_neurons * 2, (3, 3), activation=None, padding=\"same\")(uconv1)\n    uconv1 = residual_block(uconv1,start_neurons * 2)\n    uconv1 = residual_block(uconv1,start_neurons * 2)\n    uconv1 = LeakyReLU(alpha=0.1)(uconv1)\n    \n    \n    # 80 -> 160\n    uconv0 = Conv2DTranspose(start_neurons * 1, (3, 3), strides=(2, 2), padding=\"same\")(uconv1)   \n    uconv0 = Dropout(0.1)(uconv0)\n    uconv0 = Conv2D(start_neurons * 1, (3, 3), activation=None, padding=\"same\")(uconv0)\n    uconv0 = residual_block(uconv0,start_neurons * 1)\n    uconv0 = residual_block(uconv0,start_neurons * 1)\n    uconv0 = LeakyReLU(alpha=0.1)(uconv0)\n    \n    uconv0 = Dropout(0.1/2)(uconv0)\n    output_layer = Conv2D(1, (1,1), padding=\"same\", activation=\"sigmoid\")(uconv0)    \n    \n    model = Model(input, output_layer)\n    model.name = 'u-xception'\n\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2382b088c8a6be16490354ebd386120a9ced414d"},"cell_type":"code","source":"K.clear_session()\nmodel = UXception(input_shape=(img_size_target,img_size_target,3))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"13fbee7b260569837800e96e4eb0e40f57c1d887"},"cell_type":"markdown","source":"# Defining the Normalized Gradient SGD Optimizer"},{"metadata":{"_uuid":"014a07a4b6402ca164e28cb1a510ca91131c1e9f"},"cell_type":"markdown","source":"This leads to much faster converagnce as compare to normal SGD."},{"metadata":{"trusted":true,"_uuid":"4ad4dfe6f27148ada1de4e93778245d91ee2d829"},"cell_type":"code","source":"# https://github.com/titu1994/keras-normalized-optimizers\n# Computes the L-2 norm of the gradient.\ndef l2_norm(grad):\n    norm = K.sqrt(K.sum(K.square(grad))) + K.epsilon()\n    return norm\n\nclass OptimizerWrapper(optimizers.Optimizer):\n\n    def __init__(self, optimizer):     \n        \n        self.optimizer = optimizers.get(optimizer)\n\n        # patch the `get_gradients` call\n        self._optimizer_get_gradients = self.optimizer.get_gradients\n\n    def get_gradients(self, loss, params):      \n        grads = self._optimizer_get_gradients(loss, params)\n        return grads\n\n    @interfaces.legacy_get_updates_support\n    def get_updates(self, loss, params):\n        # monkey patch `get_gradients`\n        self.optimizer.get_gradients = self.get_gradients\n\n        # get the updates\n        self.optimizer.get_updates(loss, params)\n\n        # undo monkey patch\n        self.optimizer.get_gradients = self._optimizer_get_gradients\n\n        return self.updates\n\n    def set_weights(self, weights):       \n        self.optimizer.set_weights(weights)\n\n    def get_weights(self):        \n        return self.optimizer.get_weights()\n\n    def get_config(self):       \n        # properties of NormalizedOptimizer\n        config = {'optimizer_name': self.optimizer.__class__.__name__.lower()}\n\n        # optimizer config\n        optimizer_config = {'optimizer_config': self.optimizer.get_config()}\n        return dict(list(optimizer_config.items()) + list(config.items()))\n\n    @property\n    def weights(self):\n        return self.optimizer.weights\n\n    @property\n    def updates(self):\n        return self.optimizer.updates\n\n    @classmethod\n    def from_config(cls, config):\n        raise NotImplementedError\n\n    @classmethod\n    def set_normalization_function(cls, name, func):\n        global _NORMS\n        _NORMS[name] = func\n\n    @classmethod\n    def get_normalization_functions(cls):        \n        global _NORMS\n        return sorted(list(_NORMS.keys()))\n\n\nclass NormalizedOptimizer(OptimizerWrapper):\n\n    def __init__(self, optimizer, normalization='l2'):       \n        super(NormalizedOptimizer, self).__init__(optimizer)\n\n        if normalization not in _NORMS:\n            raise ValueError('`normalization` must be one of %s.\\n' \n                             'Provided was \"%s\".' % (str(sorted(list(_NORMS.keys()))), normalization))\n\n        self.normalization = normalization\n        self.normalization_fn = _NORMS[normalization]\n        self.lr = K.variable(1e-3, name='lr')\n\n    def get_gradients(self, loss, params):       \n        grads = super(NormalizedOptimizer, self).get_gradients(loss, params)\n        grads = [grad / self.normalization_fn(grad) for grad in grads]\n        return grads\n\n    def get_config(self):        \n        # properties of NormalizedOptimizer\n        config = {'normalization': self.normalization}\n\n        # optimizer config\n        base_config = super(NormalizedOptimizer, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n    @classmethod\n    def from_config(cls, config):       \n        optimizer_config = {'class_name': config['optimizer_name'],\n                            'config': config['optimizer_config']}\n\n        optimizer = optimizers.get(optimizer_config)\n        normalization = config['normalization']\n\n        return cls(optimizer, normalization=normalization)\n\n\n_NORMS = {\n    'l2': l2_norm,\n}\n\n# register this optimizer to the global custom objects when it is imported\nget_custom_objects().update({'NormalizedOptimizer': NormalizedOptimizer})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3d7a02b2dca5074cbd7025947d031b134a17ff54"},"cell_type":"code","source":"class SWA(keras.callbacks.Callback):\n    \n    def __init__(self, filepath, swa_epoch):\n        super(SWA, self).__init__()\n        self.filepath = filepath\n        self.swa_epoch = swa_epoch \n    \n    def on_train_begin(self, logs=None):\n        self.nb_epoch = self.params['epochs']\n        print('Stochastic weight averaging selected for last {} epochs.'\n              .format(self.nb_epoch - self.swa_epoch))\n        \n    def on_epoch_end(self, epoch, logs=None):\n        \n        if epoch == self.swa_epoch:\n            self.swa_weights = self.model.get_weights()\n            \n        elif epoch > self.swa_epoch:    \n            for i in range(len(self.swa_weights)):\n                self.swa_weights[i] = (self.swa_weights[i] * \n                    (epoch - self.swa_epoch) + self.model.get_weights()[i])/((epoch - self.swa_epoch)  + 1)  \n\n        else:\n            pass\n        \n    def on_train_end(self, logs=None):\n        self.model.set_weights(self.swa_weights)\n        print('Final model parameters set to stochastic weight average.')\n        self.model.save_weights(self.filepath)\n        print('Final stochastic averaged weights saved to file.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c9c5111f1e7b51fbdbf2f5d6f2a787dfd283ec0f"},"cell_type":"code","source":"sgd = SGD(0.01, momentum=0.9, nesterov=True)\nsgd = NormalizedOptimizer(sgd, normalization='l2')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3399029adb039b049e3d6ca01fef30ed8653482b"},"cell_type":"code","source":"model.compile(loss=bce_dice_loss, optimizer=sgd, metrics=[my_iou_metric])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"10da07478b82620181b94c13f53784a58b2b7968"},"cell_type":"markdown","source":"# Augmentation using fliplr"},{"metadata":{"_uuid":"46041598a44ed40c1325c1da4d89d97c82f5e7b3"},"cell_type":"markdown","source":"I only augment the images in training set.  I repeat the image in to get 3 channels which is required for using pretrained imagenet models."},{"metadata":{"trusted":true,"_uuid":"88b3f57eac3ec3719b401730dc6d8d2d89d09ccc"},"cell_type":"code","source":"x_train = np.append(x_train, [np.fliplr(x) for x in x_train[0:3200]], axis=0)\ny_train = np.append(y_train, [np.fliplr(x) for x in y_train[0:3200]], axis=0)\nx_train = np.repeat(x_train,3,axis=3)\nx_valid = np.repeat(x_valid,3,axis=3)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f5a6b1abaa4681cba3b608bc5f33cf260370d82a"},"cell_type":"markdown","source":"# Training the Model"},{"metadata":{"trusted":true,"_uuid":"f1773642758da7b4480e0e48c045bd01ea3684ae","scrolled":true},"cell_type":"code","source":"epochs = 40\nsnapshot = SnapshotCallbackBuilder(nb_epochs=epochs,nb_snapshots=1,init_lr=1e-3)\nbatch_size = 32\nswa = SWA('./keras_swa.model',35)\nhistory = model.fit(x_train, y_train,\n                    validation_data=[x_valid, y_valid], \n                    epochs=epochs,\n                    batch_size=batch_size,\n                    callbacks=snapshot.get_callbacks(),shuffle=True,verbose=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"42e9ef3c4e0a2bb2539e5e51740ba6bfc092d37c"},"cell_type":"code","source":"plt.plot(history.history['my_iou_metric'][1:])\nplt.plot(history.history['val_my_iou_metric'][1:])\nplt.title('model iou metric')\nplt.ylabel('val_my_iou_metric')\nplt.xlabel('epoch')\nplt.legend(['train','Validation'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ebf26143b898ae345b517e0c2ec8519a6e0502ba"},"cell_type":"code","source":"del ids_train, x_train, y_train,some_salt_ids,depths_df\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c824f6bca47f051500966c433ce7fb5a9528f6d7"},"cell_type":"code","source":"# Load best model\ntry:\n    print('using swa weight model')\n    model.load_weights('./keras_swa.model')\nexcept:\n    model.load_weights('./keras.model')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0f168318eadb324daa8c020f0e3e0a24d82a464f"},"cell_type":"markdown","source":"# Predict the validation set to do a sanity check\nAgain plot some sample images including the predictions."},{"metadata":{"trusted":true,"_uuid":"fc4d63ca6c7e6c13e4cfb988a554199712486af2"},"cell_type":"code","source":"def predict_result(model,x_test,img_size_target,batch_size): # predict both orginal and reflect x\n    x_test_reflect =  np.array([np.fliplr(x) for x in x_test])\n    preds_test1 = model.predict([x_test],batch_size=batch_size).reshape(-1, img_size_target, img_size_target)\n    preds_test2_refect = model.predict([x_test_reflect],batch_size=batch_size).reshape(-1, img_size_target, img_size_target)\n    preds_test2 = np.array([ np.fliplr(x) for x in preds_test2_refect] )\n    preds_avg = (preds_test1 +preds_test2)/2\n    return preds_avg","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"16cbfe2fee11a8b13b96ce78161ce19b5e5a0c46"},"cell_type":"code","source":"preds_valid = predict_result(model,x_valid,img_size_target,batch_size)\npreds_valid = np.array([downsample(x) for x in preds_valid])\ny_valid_ori = np.array([downsample(x) for x in y_valid])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3b1198b6fb7369c3cfb70e68cd1b78d36aa188bc"},"cell_type":"code","source":"max_images = 60\ngrid_width = 15\ngrid_height = int(max_images / grid_width)\nfig, axs = plt.subplots(grid_height, grid_width, figsize=(grid_width, grid_height))\nfor i, idx in enumerate(ids_valid[:max_images]):\n    img = x_valid[i]\n    mask = y_valid[i].squeeze()\n    pred = preds_valid[i]\n    ax = axs[int(i / grid_width), i % grid_width]\n    ax.imshow(img, cmap=\"Greys\")\n    ax.imshow(mask, alpha=0.3, cmap=\"Greens\")\n    ax.imshow(pred, alpha=0.3, cmap=\"OrRd\")\nplt.suptitle(\"Green: salt, Red: prediction. Top-left: coverage class, top-right: salt coverage, bottom-left: depth\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"fd973023204ebf921fe1f23748856e6a6f692aa4"},"cell_type":"markdown","source":"# Scoring\nScore the model and do a threshold optimization by the best IoU."},{"metadata":{"trusted":true,"_uuid":"d261beec66b6867ac0d5c94684f12aa08b70d638"},"cell_type":"code","source":"# src: https://www.kaggle.com/aglotero/another-iou-metric\ndef iou_metric(y_true_in, y_pred_in, print_table=False):\n    labels = y_true_in\n    y_pred = y_pred_in\n    \n    true_objects = 2\n    pred_objects = 2\n\n    intersection = np.histogram2d(labels.flatten(), y_pred.flatten(), bins=(true_objects, pred_objects))[0]\n\n    # Compute areas (needed for finding the union between all objects)\n    area_true = np.histogram(labels, bins = true_objects)[0]\n    area_pred = np.histogram(y_pred, bins = pred_objects)[0]\n    area_true = np.expand_dims(area_true, -1)\n    area_pred = np.expand_dims(area_pred, 0)\n\n    # Compute union\n    union = area_true + area_pred - intersection\n\n    # Exclude background from the analysis\n    intersection = intersection[1:,1:]\n    union = union[1:,1:]\n    union[union == 0] = 1e-9\n\n    # Compute the intersection over union\n    iou = intersection / union\n\n    # Precision helper function\n    def precision_at(threshold, iou):\n        matches = iou > threshold\n        true_positives = np.sum(matches, axis=1) == 1   # Correct objects\n        false_positives = np.sum(matches, axis=0) == 0  # Missed objects\n        false_negatives = np.sum(matches, axis=1) == 0  # Extra objects\n        tp, fp, fn = np.sum(true_positives), np.sum(false_positives), np.sum(false_negatives)\n        return tp, fp, fn\n\n    # Loop over IoU thresholds\n    prec = []\n    if print_table:\n        print(\"Thresh\\tTP\\tFP\\tFN\\tPrec.\")\n    for t in np.arange(0.5, 1.0, 0.05):\n        tp, fp, fn = precision_at(t, iou)\n        if (tp + fp + fn) > 0:\n            p = tp / (tp + fp + fn)\n        else:\n            p = 0\n        if print_table:\n            print(\"{:1.3f}\\t{}\\t{}\\t{}\\t{:1.3f}\".format(t, tp, fp, fn, p))\n        prec.append(p)\n    \n    if print_table:\n        print(\"AP\\t-\\t-\\t-\\t{:1.3f}\".format(np.mean(prec)))\n    return np.mean(prec)\n\ndef iou_metric_batch(y_true_in, y_pred_in):\n    batch_size = y_true_in.shape[0]\n    metric = []\n    for batch in range(batch_size):\n        value = iou_metric(y_true_in[batch], y_pred_in[batch])\n        metric.append(value)\n    return np.mean(metric)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"85f6d9567cec0ef8976730a6834b6569b6e108a0"},"cell_type":"code","source":"## Scoring for last model\nthresholds = np.linspace(0.3, 0.7, 31)\nious = np.array([iou_metric_batch(y_valid_ori, np.int32(preds_valid > threshold)) for threshold in tqdm_notebook(thresholds)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"183d37ad32bc2f1f0d17a9538702c45a826ccefc"},"cell_type":"code","source":"threshold_best_index = np.argmax(ious) \niou_best = ious[threshold_best_index]\nthreshold_best = thresholds[threshold_best_index]\n\nplt.plot(thresholds, ious)\nplt.plot(threshold_best, iou_best, \"xr\", label=\"Best threshold\")\nplt.xlabel(\"Threshold\")\nplt.ylabel(\"IoU\")\nplt.title(\"Threshold vs IoU ({}, {})\".format(threshold_best, iou_best))\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"423b3268c580dc1eae84f54deeeb0f691eff6028"},"cell_type":"markdown","source":"# Another sanity check with adjusted threshold\nAgain some sample images with the adjusted threshold."},{"metadata":{"trusted":true,"_uuid":"40c263765ac6d53a8c0c1361ff1e6f061eecf825"},"cell_type":"code","source":"max_images = 60\ngrid_width = 15\ngrid_height = int(max_images / grid_width)\nfig, axs = plt.subplots(grid_height, grid_width, figsize=(grid_width, grid_height))\nfor i, idx in enumerate(ids_valid[:max_images]):\n    img = x_valid[i]\n    mask = y_valid[i].squeeze()\n    pred = preds_valid[i]\n    ax = axs[int(i / grid_width), i % grid_width]\n    ax.imshow(img, cmap=\"Greys\")\n    ax.imshow(mask, alpha=0.3, cmap=\"Greens\")\n    ax.imshow(np.array(np.round(pred > threshold_best), dtype=np.float32), alpha=0.3, cmap=\"OrRd\")\nplt.suptitle(\"Green: salt, Red: prediction. Top-left: coverage class, top-right: salt coverage, bottom-left: depth\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5b43a50c7febc9347d31e4e8150b8bd13bb4e3f1"},"cell_type":"code","source":"del x_valid, y_valid,preds_valid,y_valid_ori,train_df\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4a0123d4cbd90c49c822cf5dc545a30f4a9eb456"},"cell_type":"markdown","source":"# Test Set Prediction"},{"metadata":{"trusted":true,"_uuid":"fa37d44c1d4a9b5d936c3c994e7c92c056636e3b"},"cell_type":"code","source":"train_df = pd.read_csv(\"../input/tgs-salt-identification-challenge/train.csv\", index_col=\"id\", usecols=[0])\ndepths_df = pd.read_csv(\"../input/tgs-salt-identification-challenge/depths.csv\", index_col=\"id\")\ntrain_df = train_df.join(depths_df)\ntest_df = depths_df[~depths_df.index.isin(train_df.index)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bd6ce9b4d5fc80a2502a43e80299d628fb5ffc42"},"cell_type":"code","source":"batch_size = 500\npreds_test = []\ni = 0\nwhile i < test_df.shape[0]:\n    index_val = test_df.index[i:i+batch_size]\n#     depth_val = test_df.z[i:i+batch_size]\n    x_test = np.array([upsample(np.array(load_img(\"../input/tgs-salt-identification-challenge/test/images/{}.png\".format(idx), grayscale=True))) / 255 for idx in (index_val)]).reshape(-1, img_size_target, img_size_target, 1)\n    x_test = np.repeat(x_test,3,axis=3)\n    preds_test_temp = predict_result(model,x_test,img_size_target,32)\n    if i==0:\n        preds_test = preds_test_temp\n    else:\n        preds_test = np.concatenate([preds_test,preds_test_temp],axis=0)\n    if i%2000==0:\n        print('Images Processed:',i)\n    i += batch_size    \nprint('Done!')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"834f8f03bccb46931ce8e8afb0f26af1658a3985"},"cell_type":"markdown","source":"# Some Test Set Predictions"},{"metadata":{"trusted":true,"_uuid":"b63b8f23f51fb7030cbe6f10d38b186044dc7d4e"},"cell_type":"code","source":"max_images = 60\ngrid_width = 15\ngrid_height = int(max_images / grid_width)\nfig, axs = plt.subplots(grid_height, grid_width, figsize=(grid_width, grid_height))\nfor i, idx in enumerate(index_val[:max_images]):\n    img = x_test[i]\n    pred = preds_test[i]\n    ax = axs[int(i / grid_width), i % grid_width]\n    ax.imshow(img, cmap=\"Greys\")\n    ax.imshow(np.array(np.round(pred > threshold_best), dtype=np.float32), alpha=0.3, cmap=\"OrRd\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4c8a8426176fbc0911b60a3bcab7333850bfbbf4"},"cell_type":"code","source":"\"\"\"\nused for converting the decoded image to rle mask\nFast compared to previous one\n\"\"\"\ndef rle_encode(im):\n    '''\n    im: numpy array, 1 - mask, 0 - background\n    Returns run length as string formated\n    '''\n    pixels = im.flatten(order = 'F')\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n    return ' '.join(str(x) for x in runs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"388d2d738bd15cc4b7259d1ec41df6e4eede94e7"},"cell_type":"code","source":"import time\nt1 = time.time()\npred_dict = {idx: rle_encode(np.round(downsample(preds_test[i]) > threshold_best)) for i, idx in enumerate(tqdm_notebook(test_df.index.values))}\nt2 = time.time()\n\nprint(f\"Usedtime = {t2-t1} s\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3d85641bbb14e796c7f47a6122f2b9ed2c97a46f"},"cell_type":"code","source":"sub = pd.DataFrame.from_dict(pred_dict,orient='index')\nsub.index.names = ['id']\nsub.columns = ['rle_mask']\nsub.to_csv('swa_xce_submission.csv')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}