{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nimport seaborn as sns\nimport math\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\n\n#firstly we will play with the data to understand better the data dimensions\ndata = pd.read_csv(\"../input/creditcard.csv\")\nm = data.shape[0]\nn = data.shape[1]\n#print(data.shape)\n#print(data)\n\n#check if NaN exist in our datasets\nprint(np.any(data.isnull()) == True)\n\n#we don't have any Nan, so continue\n\ndata_class = pd.value_counts(data['Class'])\nprint(data_class)\ndata_class_len = len(data_class)\n#there are 284315 genuine data\nprint(data_class[0])\n#there are 492 fraud data\nprint(data_class[1])\n\n#we can see from the data visualization , the data is clearly skewed\ndata_class.plot(kind = 'bar')\nplt.title(\"Fraud class histogram\")\nplt.xlabel(\"Class\")\nplt.ylabel(\"Frequency\")\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"print(type(data))\n#transform dataframe to numpy matrix\ndata_matrix = data.values\n\n'''\nNext we need to plot every feature to check their distribution for genuine and fraud class.\nbecause we will use gaussian distribution to detect the anormaly , so it's better to make sure every features has gaussian distribution,\notherwise, we need to transform the data to gaussian distribution using log or some other method.\n'''\nv_features = data.columns\nplt.figure(figsize=(12,31*4))\ngs = gridspec.GridSpec(31,1)\n\nfor i, col in enumerate(v_features):\n    ax = plt.subplot(gs[i])\n    sns.distplot(data[col][data['Class']==0],color='g',label='Genuine Class')\n    sns.distplot(data[col][data['Class']==1],color='r',label='Fraud Class')\n    ax.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bef7164147ff6cbabd3ee823a2b2b4fbee48c0d7"},"cell_type":"code","source":"'''\nwe can see from the features' data visualization , these features :'V1', 'V5', 'V6','V7','V8','V13','V15','V20','V21','V22','V23','V24','V25','V26','V27','V28','Amount'\nwill not help us to find the anormaly, why ? \n   1. The genuine class has the same distribution compared to the fraud class\n   2. For example : These features 'V1','V2','V5','V6','V7','V21','Amount', they have a large part of the intersection for genuine and fraud class\n'''\ndata = data.drop(['Time','V1', 'V5', 'V6','V7','V8','V13','V15','V20','V21','V22','V23','V24','V25','V26','V27','V28','Amount'],axis=1)\nprint(data.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6a725d4b15dd157815ee0c6a986e3ad3ddce5942"},"cell_type":"code","source":"'''\ndata_class_0 = data[data.Class==0]\ndata_class_1 = data[data.Class==1]\n\ndata_class_0_gaussian_v2 = np.power(data_class_0.V2,-3)\ndata_class_1_gaussian_v2 = np.power(data_class_1.V2,-3)\ndata_class_0_gaussian_v3 = np.power(data_class_0.V3,1)\ndata_class_0_gaussian_v4 = np.power(data_class_0.V19,1)\n#data_class_0_log = data_class_0_log.dropna(\n#    axis=0,     # 0: row, 1: column\n#    how='any'   # 'any': dorp if any Nan exist; 'all': drop if all row are Nan\n#    )\n#sns.set_style('darkgrid')\n#sns.distplot(data_class_0_gaussian_v2,color='r')\n#sns.distplot(data_class_0_gaussian_v3,color='r')\n#sns.distplot(data_class_0_gaussian_v4,color='r')\n'''\n#we can see our features has gaussian distribution \nv_features = data.columns\nfig, axList = plt.subplots(13,2,figsize=(12,36))\nfor i, col in enumerate(v_features):\n  data.query(\"Class==1\").hist(column=col,bins=np.linspace(-10,10,20),ax=axList[i][0],label='Fraud')\n  #ax1.legend()\n  data.query(\"Class==0\").hist(column=col,bins=np.linspace(-10,10,20),ax=axList[i][1],label='Genuine')\n  #plt.legend()\nplt.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"46bde359557325207211045604e4d2b2bf500ae8","scrolled":true},"cell_type":"code","source":"'''\nNext we will create our gaussian distribution with the dataset:\nImportant point:\n1. Because the data is skewed, classification accuracy would not be a good evaluation metrics, so what's the good evaluation metric to use,\npossible evaluation metrics : \n      - True positive, false positive , false negative, true negative\n      - Precision/Recall\n      - F1-score\n      \n2. we need to use the cross validation dataset to choose our parameter epsilon for gaussian distribution\n\nif p(x) < eplion: y = 1 , Anomaly \n\nif p(x) > eplion: y = 0 , Normal \n\nFrom above , we can see that there are 284315 genuine data and 492 fraud data\n  Traning set :276315 genuine\n  CV : 4000 genuine, 242 fraud\n  Test set :4000 genuine, 250 fraud\n'''\n#calculate mu and sigma using Training set\n\n#(284807, 13)\n#print(data.shape)\n#m = data.shape[0]\n#number of features\nn = data.shape[1]\ndata_class_normal = data[data.Class==0]\ndata_class_anomaly = data[data.Class==1]\n#delete Class which will not be used for calculate the p(x)\n#data_class_normal = data_class_normal.drop(['Class'],axis=1)\n#data_class_anomaly = data_class_anomaly.drop(['Class'],axis=1)\nm_train = 276315\nm_cv = 4000\nm_cv_anomaly = 242\nm_test = 4000\nm_test_anomaly = 250\ndata_train = data_class_normal[0:m_train][:]\n\ndata_cv = data_class_normal[m_train:m_train+m_cv][:]\ndata_cv_anomaly = data_class_anomaly[0:m_cv_anomaly][:]\ndata_cv_combined = np.vstack((data_cv,data_cv_anomaly))\n#get the last column 'Class'\ndata_cv_combined_y = data_cv_combined[:, n-1:]\n#get the first 12 rows for calculating the normal distribution p(x)\ndata_cv_combined = np.delete(data_cv_combined, -1, axis=1)\n\ndata_test = data_class_normal[m_train+m_cv:m_train+m_cv+m_test][:]\ndata_test_anomaly = data_class_anomaly[m_cv_anomaly:m_cv_anomaly+m_test_anomaly][:]\ndata_test_combined = np.vstack((data_test,data_test_anomaly))\n#get the last column 'Class'\ndata_test_combined_y = data_test_combined[:, n-1:]\n#get the first 12 rows for calculating the normal distribution p(x)\ndata_test_combined = np.delete(data_test_combined, -1, axis=1)\n\n\n#(276315, 13)\nprint('data_train',data_train.shape)\n#(4000, 13)\nprint('data_cv',data_cv.shape)\n#(242, 13)\nprint('data_cv_anomaly',data_cv_anomaly.shape)\n#(4242, 13)\nprint('data_cv_combined',data_cv_combined.shape)\n#(4000, 13)\nprint('data_test',data_test.shape)\n#(250, 13)\nprint('data_test_anomaly',data_test_anomaly.shape)\n#(4250, 13)\nprint('data_test_combined',data_test_combined.shape)\n\n#caculate mu and sigma using data_train\nmu = np.mean(data_train,axis=0)\n#sigma = np.sqrt(np.sum(((data_train - mu) ** 2),axis=0) / data_train.shape[0])\nsigma = np.std(data_train,ddof=0,axis=0)\nprint('mu shape : ',mu.shape)\nprint('sigma shape:',sigma.shape)\nmu=mu.values.reshape(1,n)\nmu = np.delete(mu, -1, axis=1)\nsigma = sigma.values.reshape(1,n)\nsigma = np.delete(sigma, -1, axis=1)\nprint('mu shape : ',mu.shape)\nprint('sigma shape:',sigma.shape)\n\ndef selectThreshold(yval,pval):\n  bestEpsilon = 0.\n  bestF1 = 0.\n  F1 = 0.\n  step = (np.max(pval)-np.min(pval))/1000\n  #print('step:',step)\n  #print('minpval : ',np.min(pval))\n  #print('maxpval : ',np.max(pval))\n  for epsilon in np.arange(np.min(pval),np.max(pval),step):\n    cvPrecision = (pval < epsilon)\n    # sum return int, so need to transform to float\n    tp = np.sum((cvPrecision == 1) & (yval == 1)).astype(float)\n    fp = np.sum((cvPrecision == 1) & (yval == 0)).astype(float)\n    fn = np.sum((cvPrecision == 1) & (yval == 0)).astype(float)\n\n    precision = tp/(tp+fp)\n    recision = tp/(tp+fn)\n    #calculate F1\n    F1 = (2*precision*recision)/(precision+recision)\n    #print('precision,recision,F1',precision,',',recision,',',F1)\n    if F1 > bestF1:\n      bestF1 = F1\n      bestEpsilon = epsilon\n\n  return bestEpsilon,bestF1\n#print(pval)\n\n#calculate using CV datasets\npval = np.exp(-((data_cv_combined-mu)**2)/2*(sigma**2))/(np.sqrt(2*math.pi)*sigma)\n\nprint(pval.shape)\nbestEpsilon,bestF1 = selectThreshold(data_cv_combined_y,pval)\nprint('bestEpsilon: ',bestEpsilon)\nprint('bestF1:',bestF1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bf92184f70e6c580cee3b7b09563299a2f3a2199"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"72809974f5ec6be5ef2bb1b84df75c2af67ee16b"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}