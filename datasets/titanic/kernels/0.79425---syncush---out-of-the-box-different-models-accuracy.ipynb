{"cells":[{"metadata":{"_uuid":"8592523278ca3e7450f32a4a921c20e0e59e18a2"},"cell_type":"markdown","source":"# Table of Content\n\n1.  [Packages Import ](#1 )\n2. [Outliers and Features ](#2 )\n3. [Preparing the Data for the Models ](#3 ) \n4. Models:\n    1. [Decision Tree ](#4)\n    2. [Neural Network ](#5 )\n    3. [K-NN](#6 ) \n    4. [ Naive Bayes](# 7 )\n    5.  [Voting ](#8 )\n    6. [XGB ](#9 ) \n    \n\n## TL;DR\n\n### Validation Set Accuracy : \n* Decision Tree 93-94 % (Decision Tree overfits, don't let the number fool you)\n* Neural Network 78-80.5 %\n* K-NN 79-81 %\n* Naive Bayes 80-84.6 %\n* Voting 81-83 %\n* XGB 79-81 %"},{"metadata":{"_uuid":"9834b232a1cbaeaad961eb26f8f55e2fa930928a"},"cell_type":"markdown","source":"# Package Imports <a id=\"1\"></a>"},{"metadata":{"trusted":true,"_uuid":"2643d4b278850ead72b0bdea030fd7b0be92ff63"},"cell_type":"code","source":"import seaborn as sns\nimport numpy as np\nimport matplotlib.pyplot as pyplot\nimport pandas as pd\nimport os\nimport plotly.offline as py\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\nimport plotly.figure_factory as ff\nimport plotly.graph_objs as go\nimport torch\nfrom torch.utils.data.dataset import Dataset\nfrom torch.autograd import Variable\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nimport torch.nn.functional as F\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nfrom sklearn.svm import LinearSVC\nimport xgboost as xgb\nfrom collections import Counter\ninit_notebook_mode()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c3c4a0ec24597bf121ac5d02034bc3b07e1487d5"},"cell_type":"markdown","source":"# Outliers and Features <a id=\"2\"></a>\nI used [yassine ghouzam's kernel](https://www.kaggle.com/yassineghouzam/titanic-top-4-with-ensemble-modeling) for detecting the outliers and extracting more features for each person"},{"metadata":{"trusted":true,"_uuid":"4bc985cc75153a939884813af607f489fe000272"},"cell_type":"code","source":"def print_acc(acc,model_name):\n    print(\"{} validation accuracy is {:.4f}%\".format(model_name, acc))\n\n# Outlier detection \ndef detect_outliers(df,n,features):\n    \"\"\"\n    Takes a dataframe df of features and returns a list of the indices\n    corresponding to the observations containing more than n outliers according\n    to the Tukey method.\n    \"\"\"\n    outlier_indices = []\n    # iterate over features(columns)\n    for col in features:\n        # 1st quartile (25%)\n        Q1 = np.percentile(df[col], 25)\n        # 3rd quartile (75%)\n        Q3 = np.percentile(df[col],75)\n        # Interquartile range (IQR)\n        IQR = Q3 - Q1\n        \n        # outlier step\n        outlier_step = 1.5 * IQR\n        \n        # Determine a list of indices of outliers for feature col\n        outlier_list_col = df[(df[col] < Q1 - outlier_step) | (df[col] > Q3 + outlier_step )].index\n        \n        # append the found outlier indices for col to the list of outlier indices \n        outlier_indices.extend(outlier_list_col)\n        \n    # select observations containing more than 2 outliers\n    outlier_indices = Counter(outlier_indices)        \n    multiple_outliers = list( k for k, v in outlier_indices.items() if v > n )\n    # detect outliers from Age, SibSp , Parch and Fare\n    # Drop outliers\n    df = df.drop(multiple_outliers, axis = 0).reset_index(drop=True)\n    return multiple_outliers\n\ndef prepare_data(path, is_test=False):\n    data = pd.read_csv(path)\n    data[\"Pclass\"].astype('int32')\n    if not is_test:\n        data[\"Survived\"].astype('int32')\n    data[\"Parch\"].astype('int32')\n    data[\"SibSp\"].astype('int32')\n    data[\"Fare\"].astype('float')\n    data[\"Age\"].astype('float')\n    index_NaN_age = list(data[\"Age\"][data[\"Age\"].isnull()].index)\n    for i in index_NaN_age :\n        age_med = data[\"Age\"].median()\n        age_pred = data[\"Age\"][((data['SibSp'] == data.iloc[i][\"SibSp\"]) & (data['Parch'] == data.iloc[i][\"Parch\"]) & (data['Pclass'] == data.iloc[i][\"Pclass\"]))].median()\n        if not np.isnan(age_pred) :\n            data['Age'].iloc[i] = age_pred\n        else :\n            data['Age'].iloc[i] = age_med\n    data[\"Embarked\"].fillna('S')\n    data[\"f_size\"] = data[\"SibSp\"] + data[\"Parch\"] + 1\n    data['Single'] = data['f_size'].map(lambda x: 1 if x == 1 else 0)\n    data['SmallF'] = data['f_size'].map(lambda x: 1 if  x == 2  else 0)\n    data['MedF'] = data['f_size'].map(lambda x: 1 if 3 <= x <= 4 else 0)\n    data['LargeF'] = data['f_size'].map(lambda x: 1 if x >= 5 else 0)\n    data['is_male'] = data['Sex'].map(lambda x: 1 if x == \"male\" else 0)\n    data['is_female'] = data['Sex'].map(lambda x: 1 if x == \"female\" else 0)\n    dataset_title = [i.split(\",\")[1].split(\".\")[0].strip() for i in data[\"Name\"]]\n    data[\"Title\"] = pd.Series(dataset_title)\n    data[\"Title\"] = data[\"Title\"].replace(['Lady', 'the Countess','Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n    data[\"Title\"] = data[\"Title\"].map({\"Master\":0, \"Miss\":1, \"Ms\" : 1 , \"Mme\":1, \"Mlle\":1, \"Mrs\":1, \"Mr\":2, \"Rare\":3})\n    data[\"Embarked\"] = data[\"Embarked\"].fillna('S')\n    data[\"Embarked\"] = data[\"Embarked\"].map({\"S\":1, \"C\":2, \"Q\":3})\n    data[\"Embarked\"] = data[\"Embarked\"].astype(int)\n    data[\"Title\"] = data[\"Title\"].astype(int)\n    data.drop(columns=[\"Name\", \"Cabin\", \"Ticket\"], inplace=True)\n    if not is_test:\n        detect_outliers(data, 2, [\"Age\",\"SibSp\",\"Parch\",\"Fare\"])\n    return data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1e074811b8a13f673590932b69c32cc24aa7c45e"},"cell_type":"markdown","source":"## Preparing the Data for the Models <a id=\"3\"></a>"},{"metadata":{"trusted":true,"_uuid":"2500ca34acdf5a223caf6091d22cb3caa6aa53aa"},"cell_type":"code","source":"data = prepare_data('../input/train.csv')\ndata.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c8aac71f6be550b24e369563d7978634a3e4cad4"},"cell_type":"code","source":"features1 = [\"Age\",\"SibSp\",\"Parch\", \"Pclass\", \"is_male\", \"is_female\", \"f_size\", \"Single\", \"SmallF\", \"MedF\", \"LargeF\", \"Title\", \"Embarked\"]\ntarget = [\"Survived\"]\nprint(\"Total number of features is {}\".format(len(features1)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2c4b07834390b3ba8a1f99d0bf966aa7fae2cb63"},"cell_type":"code","source":"test_csv = prepare_data('../input/test.csv', True)\ntest_data = test_csv[features1]\ntest_csv.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4f547dba15a787ddaf3616e4b6b03b64724a55bc"},"cell_type":"code","source":"train, valid = train_test_split(data, test_size=0.2)\nprint(\"The Train Set Size is {} \\nThe Validation Set Size is {}\".format(len(train), len(valid)))\nprint(\"Test Set Size is {}\".format(len(test_data)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f3c7c2ec690d060603d7ad57656b26f3b13e7b46"},"cell_type":"markdown","source":"# Decision Tree  <a id=\"4\"></a>"},{"metadata":{"trusted":true,"_uuid":"3358fe23b826d40ff8ccb079722e20701d5fe710"},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\nfrom sklearn.tree import DecisionTreeClassifier\n\ntrain_x , train_y = data[features1].as_matrix(), data[target].as_matrix()\n\nclf = DecisionTreeClassifier()\nclf.fit(train_x, train_y)\n\nprint((np.array(clf.predict(valid[features1].as_matrix()) == valid[target].as_matrix().flatten(), dtype=np.int).sum() * 100.) / len(valid))\n\nresult = pd.DataFrame(data={'PassengerId': test_csv['PassengerId'], 'Survived': clf.predict(test_data.as_matrix())})\nresult.to_csv(path_or_buf='decision_tree_submittion.csv', index = False, header = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"42a0e55a08d9f4bdc52c1c994692b1f72c5caea9"},"cell_type":"markdown","source":"# Neural Network  <a id=\"5\"></a>"},{"metadata":{"trusted":true,"_uuid":"b875321d87ee622e0ebce88c12c6fd0f06825866"},"cell_type":"code","source":"class TitanicLoader(Dataset):\n    def __init__(self,train,transforms=None):\n        self.X = train.as_matrix(columns=features1)\n        self.Y = train.as_matrix(columns=target).flatten()\n        self.count = len(self.X)\n        # get iterator\n        self.transforms = transforms\n\n    def __getitem__(self, index):\n        nextItem = Variable(torch.tensor(self.X[index]).type(torch.FloatTensor))\n\n        if self.transforms is not None:\n            nextItem = self.transforms(nextItem[0])\n\n        # return tuple but with no label\n        return (nextItem, self.Y[index])\n\n    def __len__(self):\n        return self.count","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bf47b21891a991ee4ad845b8ecab6bed95b30ea2"},"cell_type":"code","source":"class DNN(nn.Module):\n    def __init__(self, input_size, first_hidden_size, second_hidden_size, num_classes):\n        super(DNN, self).__init__()\n        self.z1 = nn.Linear(input_size, first_hidden_size)\n        self.relu = nn.ReLU()\n        self.z2 = nn.Linear(first_hidden_size, second_hidden_size)\n        self.z3 = nn.Linear(second_hidden_size, num_classes)\n        self.bn1 = nn.BatchNorm1d(first_hidden_size)\n        self.bn2 = nn.BatchNorm1d(second_hidden_size)\n        self.dropout = nn.Dropout(p=0.5)\n        self.dropout2 = nn.Dropout(p=0.1)\n        self.log_softmax = nn.LogSoftmax(dim=1)\n\n    def forward(self, x):\n        out = self.z1(x) # input\n        out = self.relu(out)\n        out = self.dropout(out)\n        out = self.bn1(out)\n        out = self.z2(out) # first hidden layer\n        out = self.relu(out)\n        out = self.dropout2(out)\n        out = self.bn2(out)\n        out = self.z3(out) # second hidden layer\n        out = self.log_softmax(out) # output\n        return out\n\n    def name(self):\n        return \"DNN\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1769d5d9185566bb8614386bc31d37979042afca"},"cell_type":"code","source":"def train_dnn(net, trainL, validL):\n    count = 0\n    accuList = []\n    lossList = []\n    optimizer = torch.optim.Adam(net.parameters(),lr=0.001)\n    for epc in range(1,epochs + 1):\n        print(\"Epoch # {}\".format(epc))\n        vcount = 0\n        total_loss = 0\n        net.train()\n        for data,target in trainL:\n            optimizer.zero_grad()\n            out = net(data)\n            loss = F.nll_loss(out, target, size_average=False)\n            pred = out.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n            count += pred.eq(target.data.view_as(pred)).sum()\n            # Backward and optimize\n            loss.backward()\n            # update parameters\n            optimizer.step()\n        net.eval()\n        for data, target in validL:\n            out = net(data)\n            loss = F.nll_loss(out, target, size_average=False)\n            total_loss += loss.item()\n            pred = out.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n            vcount += pred.eq(target.data.view_as(pred)).sum().item()\n        \n        accuList.append(100. * (vcount / len(validL)))\n        lossList.append(total_loss / len(validL))\n    \n    return accuList, lossList\ndef test(net, loader):\n    net.eval()\n    vcount = 0\n    count = 0\n    total_loss = 0.0\n    for data, target in loader:\n        out = net(data)\n        loss = F.nll_loss(out, target, size_average=False)\n        total_loss += loss.item()\n        pred = out.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n        vcount += pred.eq(target.data.view_as(pred)).sum().item()\n    return 100. * (vcount / len(loader)), total_loss / len(loader)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"da8663f555fb35fd17794b3eb6e07242679129ae"},"cell_type":"code","source":"epochs = 12","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7908d97536ed3c6e44c13594f8946e0a63593181"},"cell_type":"code","source":"titanic_train_DS = TitanicLoader(train)\ntitanic_valid_DS = TitanicLoader(valid)\n\ntrain_loader = torch.utils.data.DataLoader(titanic_train_DS,\n            batch_size=6, shuffle=False)\nvalid_loader = torch.utils.data.DataLoader(titanic_valid_DS,\n            batch_size=1, shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"70344a78f9bfd54a19e0218d663c5a396d6b2f3c"},"cell_type":"code","source":"myNet = DNN(len(features1), 23, 4, 2)\naccuList, lossList = train_dnn(myNet, train_loader, valid_loader)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"23cc78d7b2230e8399ac577fb1239754d9325351","scrolled":true},"cell_type":"code","source":"pyplot.figure()\npyplot.plot(range(1, epochs + 1), accuList, \"b--\", marker=\"o\", label='Validation Accuracy')\npyplot.legend()\npyplot.show()\npyplot.figure()\npyplot.plot(range(1, epochs + 1), lossList, \"r\", marker=\".\", label='Validation Loss')\npyplot.legend()\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f1ad057697642c0e21c7556d175a03c14693479d"},"cell_type":"code","source":"def get_preds(test, net):\n    net.eval()\n    preds = []\n    for data, target in test:\n        out = net(data)\n        pred = out.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n        preds.append(pred.item())\n    return preds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8b874270fc386ce3f2fbf7b15c535ad121a87119"},"cell_type":"code","source":"test_data['Survived'] = -1\ntitanic_test_DS = TitanicLoader(test_data)\ntest_loader = torch.utils.data.DataLoader(titanic_test_DS,\n            batch_size=1, shuffle=False)\nresult = pd.DataFrame(data={'PassengerId': test_csv['PassengerId'], 'Survived': get_preds(test_loader, myNet)})\nresult.to_csv(path_or_buf='neural_network_submittion.csv', index = False, header = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a476895599414d34bd856e25c0d867d5cd3b379b"},"cell_type":"markdown","source":"# K-NN <a id=\"6\"></a>\n"},{"metadata":{"trusted":true,"_uuid":"bdff36092a147287a83a296e62aa2bd38b989683"},"cell_type":"code","source":"neigh = KNeighborsClassifier(n_neighbors=5, weights='distance', p=1)\nneigh.fit(train[features1].as_matrix(), train[target].as_matrix().flatten())\nprint_acc((np.array(neigh.predict(valid[features1].as_matrix()) == valid[target].as_matrix().flatten(), dtype=np.int).sum() * 100.) / len(valid), \"K-NN\")\nresult = pd.DataFrame(data={'PassengerId': test_csv['PassengerId'], 'Survived': neigh.predict(test_data[features1].as_matrix())})\nresult.to_csv(path_or_buf='knn_submittion.csv', index = False, header = True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"921fb905e702f1515426d6b00ca15a828a128722"},"cell_type":"markdown","source":"# Naive Bayes <a id=\"7\"></a>"},{"metadata":{"trusted":true,"_uuid":"cd24fe9621d3779a1c7460dc60ad7dbad64f2119"},"cell_type":"code","source":"gnb = GaussianNB()\ny_pred = gnb.fit(train[features1].as_matrix(), train[target].as_matrix().flatten()).predict(valid[features1].as_matrix())\nprint_acc(float(np.array(y_pred == valid[target].as_matrix().flatten(), dtype=np.int).sum() * 100) / len(valid), \"Naive Bayes\")\nresult = pd.DataFrame(data={'PassengerId': test_csv['PassengerId'], 'Survived': gnb.predict(test_data[features1].as_matrix())})\nresult.to_csv(path_or_buf='naive_bayes_submittion.csv', index = False, header = True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"10093d95080bf6d10c820d0226888ade062fa90e"},"cell_type":"markdown","source":"# Voting <a id=\"8\"></a>"},{"metadata":{"trusted":true,"_uuid":"15205e37e6a408e81f103cb431583035c2605397"},"cell_type":"code","source":"clf1 = LogisticRegression(random_state=1)\nclf2 = RandomForestClassifier(n_estimators=25,random_state=1)\nclf3 = GaussianNB(var_smoothing=True)\nclf4 = LinearSVC(random_state=5)\ngbm =  xgb.XGBClassifier(max_depth=5, n_estimators=300, learning_rate=0.05)\neclf1 = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3), ('xgb', gbm)], voting='soft')\neclf1 = eclf1.fit(train[features1].as_matrix(), train[target].as_matrix().flatten())\nprint_acc(float(np.array(eclf1.predict(valid[features1].as_matrix()) == valid[target].as_matrix().flatten(), dtype=np.int).sum() * 100) / len(valid), \"Voting\")\nresult = pd.DataFrame(data={'PassengerId': test_csv['PassengerId'], 'Survived': eclf1.predict(test_data[features1].as_matrix())})\nresult.to_csv(path_or_buf='soft_voting_submittion.csv', index = False, header = True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b5da2da71926a937be9ced7e2be61fbd62aac1e8"},"cell_type":"markdown","source":"# XGB <a id=\"9\"></a>\n"},{"metadata":{"trusted":true,"_uuid":"c0f2960efbe93350b2eb9c266edc73fb96f54f13"},"cell_type":"code","source":"gbm = xgb.XGBClassifier(max_depth=3, n_estimators=600, learning_rate=0.05)\ny_pred = gbm.fit(train[features1].as_matrix(), train[target].as_matrix().flatten()).predict(valid[features1].as_matrix())\nprint_acc(float(np.array(y_pred == valid[target].as_matrix().flatten(), dtype=np.int).sum() * 100) / len(valid), \"XGB\")\nresult = pd.DataFrame(data={'PassengerId': test_csv['PassengerId'], 'Survived': gbm.predict(test_data[features1].as_matrix())})\nresult.to_csv(path_or_buf='gbm_submittion.csv', index = False, header = True)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}