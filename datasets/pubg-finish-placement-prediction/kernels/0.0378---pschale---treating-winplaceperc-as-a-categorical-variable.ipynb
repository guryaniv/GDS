{"cells":[{"metadata":{"_uuid":"21084dff80ecba16442ef37b0713279f19a68325"},"cell_type":"markdown","source":"# Overview\nThe target variable here -- winPlacePerc -- is a float. The usual assumption is that floats are continuous, but that's not the case here. If we multiply by the variable maxPlace (minus 1) we'll see that we get an integer."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nfrom matplotlib import pyplot as plt\n\nimport tensorflow as tf\n\nfrom keras.models import Model, load_model\nfrom keras.layers import Input, Cropping2D, BatchNormalization, Dropout\nfrom keras.layers.core import Lambda, Dense\nfrom keras.layers.merge import concatenate\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom keras import backend as K\nfrom keras.optimizers import Adam\nfrom keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\nfrom keras.losses import binary_crossentropy\n\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn import svm\n\nimport xgboost as xgb\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nimport gc\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9c584c770dd1d54d37aeb9671905341055e6239c","_kg_hide-input":true},"cell_type":"code","source":"#options\nsmall_train = True\nonly_imp_cols = False","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"try:\n    train = pd.read_csv('../input/train_V2.csv')\nexcept:\n    train = pd.read_csv('~/.kaggle/competitions/pubg-finish-placement-prediction/train_V2.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9f48ea170713a67c825feef7e6224f9fa79df0c3"},"cell_type":"code","source":"((train['maxPlace'] - 1) * train['winPlacePerc'])[:10]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8325f35d870e6daa51da1720525b579bbb4887e9"},"cell_type":"markdown","source":"Now normally discrete variables make for simple classification problems. But here we don't have a fixed number of categories. In this notebook, I'll deal with that by making it 3 different classification problems. To see why this helps, let's see what goes wrong when we simply run a normal classification. First we do some feature engineering, then set up a simple NN in keras.\n\nNext we have some details - we'll take only 40% of the training set to make it run faster, do some feature engineering, and do some memory reduction. Input cells are hidden because these aren't important to the main point of this notebook."},{"metadata":{"trusted":true,"_uuid":"b4fa0b2556510a2ad85bbd34ce0df62c61a79f49","_kg_hide-input":true},"cell_type":"code","source":"if small_train:\n    train = train[train['matchId'].isin(train['matchId'].unique()[np.random.rand(len(train['matchId'].unique())) < 0.4])].reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"026e718a222b455a5688b05966124bc34b75ce09","_kg_hide-input":false},"cell_type":"code","source":"def feature_engineering(df, train=True):\n    \n    drop_labels = ['Id', 'groupId', 'matchId', 'maxPlace', 'numGroups', 'matchDuration', 'matchType']\n    if train:\n        df['wppBin'] = np.uint16(np.round(df['winPlacePerc']*(df['maxPlace']-1)))\n        df['wppBin2'] = df['wppBin'] + (100 - df['maxPlace'])\n        df['wppBin3'] = df['wppBin'] + np.uint16(np.round((100 - df['maxPlace'])*.5))\n        drop_labels += ['winPlacePerc', 'wppBin', 'wppBin2', 'wppBin3']\n    df['killrate'] = df['kills']/df['matchDuration']\n    df['totalDistance'] = df['walkDistance'] + df['swimDistance'] + df['rideDistance']\n    df['avgSpeed'] = df['totalDistance'] / df['matchDuration']\n    df['healsPlusBoosts'] = df['boosts'] + df['heals']\n    df['kar'] = df['kills'] + df['assists'] + df['revives']\n    \n    le = LabelEncoder()\n    le.fit(['squad-fpp', 'duo', 'solo-fpp', 'squad', 'duo-fpp', 'solo',\n       'normal-squad-fpp', 'crashfpp', 'flaretpp', 'normal-solo-fpp',\n       'flarefpp', 'normal-duo-fpp', 'normal-duo', 'crashtpp',\n       'normal-squad', 'normal-solo'])\n    \n    df['matchType'] = le.transform(df['matchType'])\n    \n    agglabels = [ele for ele in df.columns if ele not in drop_labels]\n    \n    groupByMatch = df.groupby(['matchId','groupId'])[agglabels]\n    groupByGroup = df.groupby(['groupId', 'Id'])[agglabels]\n    df = df.merge(groupByMatch.agg('min').reset_index(), suffixes=['','match_min'], on=['matchId','groupId'], how='left')\n    print('Checkpoint 1/7')\n    df = df.merge(groupByMatch.agg('max').reset_index(), suffixes=['','match_max'], on=['matchId','groupId'], how='left')\n    print('Checkpoint 2/7')\n    df = df.merge(groupByMatch.agg('mean').reset_index(), suffixes=['','match_mean'], on=['matchId','groupId'], how='left')\n    print('Checkpoint 3/7')\n    df = df.merge(groupByMatch.agg('mean').groupby('matchId').rank(pct=True).reset_index(), suffixes=['','match_rank'], on=['matchId', 'groupId'], how='left')\n\n    print('Checkpoint 4/7')\n    df = df.merge(groupByGroup.agg('min').reset_index(), suffixes=['','group_min'], on=['Id', 'groupId'], how='left')\n    print('Checkpoint 5/7')\n    df = df.merge(groupByGroup.agg('max').reset_index(), suffixes=['','group_max'], on=['Id', 'groupId'], how='left')\n    print('Checkpoint 6/7')\n    df = df.merge(groupByGroup.agg('mean').reset_index(), suffixes=['','group_mean'], on=['Id', 'groupId'], how='left')\n    print('Checkpoint 7/7')\n    \n    df = df.merge(groupByGroup.agg('mean').groupby('groupId').rank(pct=True).reset_index(), suffixes=['','group_rank'], on=['Id', 'groupId'], how='left')\n    df = df.merge(groupByGroup.size().reset_index(name='group_size'), how='left', on=['Id', 'groupId'])\n    df = df.merge(groupByMatch.size().reset_index(name='match_size'), how='left', on=['matchId','groupId'])\n    \n\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ab9dfa5aab7af7942a22e1153dca951f65dbf9f0","_kg_hide-input":true},"cell_type":"code","source":"def reduce_mem_usage(props, verbose=False):\n    \n    # slightly adapted (but mostly just copied) from here: \n    # https://www.kaggle.com/arjanso/reducing-dataframe-memory-size-by-65\n    \n    start_mem_usg = props.memory_usage().sum() / 1024**2 \n    print(\"Memory usage of properties dataframe is :\",start_mem_usg,\" MB\")\n    NAlist = [] # Keeps track of columns that have missing values filled in. \n    for col in props.columns:\n        if props[col].dtype != object:  # Exclude strings\n            \n            # Print current column type\n            if verbose:\n                print(\"******************************\")\n                print(\"Column: \",col)\n                print(\"dtype before: \",props[col].dtype)\n            \n            # make variables for Int, max and min\n            IsInt = False\n            mx = props[col].max()\n            mn = props[col].min()\n            \n            # Integer does not support NA, therefore, NA needs to be filled\n            if not np.isfinite(props[col]).all(): \n                NAlist.append(col)\n                props[col].fillna(mn-1,inplace=True)  \n                   \n            # test if column can be converted to an integer\n            asint = props[col].fillna(0).astype(np.int64)\n            result = (props[col] - asint)\n            result = result.sum()\n            if result > -0.01 and result < 0.01:\n                IsInt = True\n\n            \n            # Make Integer/unsigned Integer datatypes\n            if IsInt:\n                if mn >= 0:\n                    if mx < 255:\n                        props[col] = props[col].astype(np.uint8)\n                    elif mx < 65535:\n                        props[col] = props[col].astype(np.uint16)\n                    elif mx < 4294967295:\n                        props[col] = props[col].astype(np.uint32)\n                    else:\n                        props[col] = props[col].astype(np.uint64)\n                else:\n                    if mn > np.iinfo(np.int8).min and mx < np.iinfo(np.int8).max:\n                        props[col] = props[col].astype(np.int8)\n                    elif mn > np.iinfo(np.int16).min and mx < np.iinfo(np.int16).max:\n                        props[col] = props[col].astype(np.int16)\n                    elif mn > np.iinfo(np.int32).min and mx < np.iinfo(np.int32).max:\n                        props[col] = props[col].astype(np.int32)\n                    elif mn > np.iinfo(np.int64).min and mx < np.iinfo(np.int64).max:\n                        props[col] = props[col].astype(np.int64)    \n            \n            # Make float datatypes 32 bit\n            else:\n                props[col] = props[col].astype(np.float32)\n            \n            # Print new column type\n            if verbose:\n                print(\"dtype after: \",props[col].dtype)\n                print(\"******************************\")\n    \n    # Print final result\n    print(\"___MEMORY USAGE AFTER COMPLETION:___\")\n    mem_usg = props.memory_usage().sum() / 1024**2 \n    print(\"Memory usage is: \",mem_usg,\" MB\")\n    print(\"This is \",100*mem_usg/start_mem_usg,\"% of the initial size\")\n    return props, NAlist","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6a89dd0f9fd084a5f834faabbb274ea71ad5eff6","_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"train, _ = reduce_mem_usage(train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"49a027f72ef6f4ced49ae3df7cdaf82ed149a0b2"},"cell_type":"markdown","source":"This is where we split the training set into a train and a validate set. Note that we can't take the simple approach of randomly taking out 20% -- we need to take out whole matches."},{"metadata":{"trusted":true,"_uuid":"5c5c43f2b514697290dce90cd5d260e140d73dbb","_kg_hide-input":false,"_kg_hide-output":true},"cell_type":"code","source":"msk = train['matchId'].unique()[np.random.rand(len(train['matchId'].unique())) < 0.8]\ntrain_train = train[train['matchId'].isin(msk)].reset_index(drop=True)\ntrain_val = train[~train['matchId'].isin(msk)].reset_index(drop=True)\ntrain = None\nmsk = None\n\ngc.collect()\ntrain_train = feature_engineering(train_train)\ntrain_train, _ = reduce_mem_usage(train_train)\ntrain_val = feature_engineering(train_val)\ntrain_val, _ = reduce_mem_usage(train_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b4d2c14b278a4d632e99ac4d93b70502c5c815c1","_kg_hide-input":false,"_kg_hide-output":false},"cell_type":"code","source":"train_cols = [ele for ele in train_train.columns if ele not in ['Id', 'groupId', 'matchId', 'winPlacePerc', 'wppBin', 'wppBin2', 'wppBin3']]\ntarget = 'winPlacePerc'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7817613a740d75af1783f60da8445999a21b3de3"},"cell_type":"code","source":"m1inputs = Input(shape=(len(train_cols),))\n\nm1l1 = BatchNormalization() (m1inputs)\nm1l1 = Dense(128, activation='relu') (m1l1)\nm1l1 = BatchNormalization() (m1l1)\n\nm1l2 = Dense(128, activation='relu')(m1l1)\nm1l2 = BatchNormalization() (m1l2)\nm1l2 = Dropout(0.75) (m1l2)\n\nm1l3 = Dense(128, activation='relu')(m1l2)\nm1l3 = BatchNormalization() (m1l3)\n\nm1l4 = Dense(128, activation='relu')(m1l3)\nm1l4 = BatchNormalization() (m1l4)\nm1l4 = Dropout(0.75) (m1l4)\n\nm1output = Dense(100, activation='softmax') (m1l4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"93030443e154571dabeee988daef273ee005b448"},"cell_type":"code","source":"model1 = Model(inputs=[m1inputs], outputs=[m1output])\nadam = Adam(lr=0.0005, beta_1=0.9, beta_2=0.999, epsilon=None, amsgrad=False)\nmodel1.compile(optimizer=adam, loss='binary_crossentropy', metrics=['acc'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d79450602eba22686813c9e635abbae46efe1adc"},"cell_type":"code","source":"enc = OneHotEncoder()\nenc.fit(np.arange(100).reshape(-1,1))\n#enc.fit(train_train['wppBin'].values.reshape(-1,1))\n#enc.fit(np.expand_dims(np.array(range(100)),axis=0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"531bfac6c805fb48273368bea8ef11dc96a7a952","scrolled":false},"cell_type":"code","source":"model1.fit(train_train[train_cols], enc.transform(train_train['wppBin'].values.reshape(-1,1)), \n           epochs=20, batch_size=4096, \n           validation_data=(train_val[train_cols], enc.transform(train_val['wppBin'].values.reshape(-1,1))),\n           verbose=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3858eb0181e9dc5bd97f7708de53eae3754c0c86"},"cell_type":"code","source":"val_preds = model1.predict(train_val[train_cols], verbose=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ca379bc3d56aa3437fdce6805846c80ebddd4285"},"cell_type":"code","source":"val_preds_float = np.argmax(val_preds, axis=1) / (train_val['maxPlace'] + 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2985e047b2841d6a3377cc2d8d15b48c4f3a2f33"},"cell_type":"code","source":"plt.scatter(val_preds_float, train_val[target], marker='.')\nplt.plot([0, 1], [0,1], color='r')\nplt.xlabel('predicted')\nplt.ylabel('actual')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"13970d27761f9f2b92657c023486306e8db5a5bc"},"cell_type":"code","source":"print('estimated score from verify set:')\nnp.mean(np.abs(val_preds_float - train_val[target]))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b66de46940571b344e66c5e82ac668c0a38761f6"},"cell_type":"markdown","source":"That's not a good score. In addition, we see a couple problems. First, we have predicted values over 1, which we know shouldn't happen. This occurs because the network hasn't figured out that it should only look at bins up to maxPlace. Second, it almost always undershoots the actual value, but the effect is smallest at the bottom -- for the worst players. This makes sense, because the worst player will always be a 0, while a mid-level player may be anwhere from 10 to 50. So to fix these problems, let's make 2 more networks like this, except we'll change how maxPlace is turned into a discrete variable. For this network, we always had the worst player at 1. For the second, we'll always have the best player at 100, and for the third, the middle player will always be at 50."},{"metadata":{"trusted":true,"_uuid":"70e7d885a08b8e6ad05bc02188947b347f1370bd"},"cell_type":"code","source":"m2inputs = Input(shape=(len(train_cols),))\n\nm2l1 = BatchNormalization() (m2inputs)\nm2l1 = Dense(128, activation='relu') (m2l1)\nm2l1 = BatchNormalization() (m2l1)\n\nm2l2 = Dense(128, activation='relu')(m2l1)\nm2l2 = BatchNormalization() (m2l2)\nm2l2 = Dropout(0.75) (m2l2)\n\nm2l3 = Dense(128, activation='relu')(m2l2)\nm2l3 = BatchNormalization() (m2l3)\n\nm2l4 = Dense(128, activation='relu')(m2l3)\nm2l4 = BatchNormalization() (m2l4)\nm2l4 = Dropout(0.75) (m2l4)\n\nm2output = Dense(100, activation='softmax') (m2l4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"99adff4e1c058a61cd80ade208c34b6d79c98bb9"},"cell_type":"code","source":"model2 = Model(inputs=[m2inputs], outputs=[m2output])\nadam = Adam(lr=0.0005, beta_1=0.9, beta_2=0.999, epsilon=None, amsgrad=False)\nmodel2.compile(optimizer=adam, loss='binary_crossentropy', metrics=['acc'])\nmodel2.fit(train_train[train_cols], enc.transform(train_train['wppBin2'].values.reshape(-1,1)), \n           epochs=20, \n           batch_size=4096, \n           validation_data=(train_val[train_cols], enc.transform(train_val['wppBin2'].values.reshape(-1,1))),\n           verbose=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0e65e66faedfbb168b855b5f5110e51054639f31"},"cell_type":"code","source":"m3inputs = Input(shape=(len(train_cols),))\n\nm3l1 = BatchNormalization() (m3inputs)\nm3l1 = Dense(128, activation='relu') (m3l1)\nm3l1 = BatchNormalization() (m3l1)\n\nm3l2 = Dense(128, activation='relu')(m3l1)\nm3l2 = BatchNormalization() (m3l2)\nm3l2 = Dropout(0.75) (m3l2)\n\nm3l3 = Dense(128, activation='relu')(m3l2)\nm3l3 = BatchNormalization() (m3l3)\n\nm3l4 = Dense(128, activation='relu')(m3l3)\nm3l4 = BatchNormalization() (m3l4)\nm3l4 = Dropout(0.75) (m3l4)\n\nm3output = Dense(100, activation='softmax') (m3l4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f2e9f405d10db6f492e5ee333abed5ce9e0a2974"},"cell_type":"code","source":"model3 = Model(inputs=[m3inputs], outputs=[m3output])\nadam = Adam(lr=0.0005, beta_1=0.9, beta_2=0.999, epsilon=None, amsgrad=False)\nmodel3.compile(optimizer=adam, loss='binary_crossentropy', metrics=['acc'])\nmodel3.fit(train_train[train_cols], enc.transform(train_train['wppBin3'].values.reshape(-1,1)), \n           epochs=20, \n           batch_size=4096, \n           validation_data=(train_val[train_cols], enc.transform(train_val['wppBin3'].values.reshape(-1,1))),\n           verbose=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"582a4078f9d12646e0a31b1492fc8c273bc83524"},"cell_type":"code","source":"predsbin1 = model1.predict(train_val[train_cols], verbose=True)\npredsbin2 = model2.predict(train_val[train_cols], verbose=True)\npredsbin3 = model3.predict(train_val[train_cols], verbose=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"94491d36bf16b4e3a300cfb76ef9073206bca723"},"cell_type":"code","source":"tpredsbin1 = model1.predict(train_train[train_cols], verbose=True)\ntpredsbin2 = model2.predict(train_train[train_cols], verbose=True)\ntpredsbin3 = model3.predict(train_train[train_cols], verbose=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7c2bb36864e0b9f22c0d4153d0846c5e3332c78b"},"cell_type":"code","source":"trainMaxPlace = train_train['maxPlace']\nvalMaxPlace = train_val['maxPlace']\ntrainTar = train_train[target]\nvalTar = train_val[target]\ntrain_train = None\ntrain_verify = None\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5f9ad66570d425c02aa7d1a7cd4c1ac697a848b8"},"cell_type":"code","source":"preds1 = np.argmax(predsbin1,axis=1) / (valMaxPlace + 1)\npreds2 = (np.argmax(predsbin2,axis=1) - (100 - valMaxPlace)) / (valMaxPlace + 1)\npreds3 = (np.argmax(predsbin3,axis=1) - np.uint16(np.round((100 - valMaxPlace)*.5))) / (valMaxPlace + 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3db95c0c7bf1b69be5c7dc1e3781fb4664020749"},"cell_type":"code","source":"tpreds1 = np.argmax(tpredsbin1,axis=1) / (trainMaxPlace + 1)\ntpreds2 = (np.argmax(tpredsbin2,axis=1) - (100 - trainMaxPlace)) / (trainMaxPlace + 1)\ntpreds3 = (np.argmax(tpredsbin3,axis=1) - np.uint16(np.round((100 - trainMaxPlace)*.5))) / (trainMaxPlace + 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ed05ee92f28a218bc705f671b5946e76137e64a0"},"cell_type":"code","source":"print('Training scores:')\nprint(np.mean(np.abs(tpreds1 - trainTar)))\nprint(np.mean(np.abs(tpreds2 - trainTar)))\nprint(np.mean(np.abs(tpreds3 - trainTar)))\n\nprint('Validation scores')\n\nprint(np.mean(np.abs(preds1 - valTar)))\nprint(np.mean(np.abs(preds2 - valTar)))\nprint(np.mean(np.abs(preds3 - valTar)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1e031a3a9b46cefa649ab36e143f2791736dddfd"},"cell_type":"code","source":"train_outs = pd.DataFrame({'p1': tpreds1, \n                           'p2': tpreds2, \n                           'p3': tpreds3, \n                           'maxPlace': trainMaxPlace, \n                           'winPlacePerc': trainTar})\n\nval_outs = pd.DataFrame({'p1': preds1, \n                           'p2': preds2, \n                           'p3': preds3, \n                           'maxPlace': valMaxPlace, \n                           'winPlacePerc': valTar})\n\ntrain_outs.to_csv('train_out.csv')\nval_outs.to_csv('validate_out.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b1f5687d33807abf6d92990302d27b9a9479c017"},"cell_type":"markdown","source":"# Combining the 3 models\n\nFor this we'll use a simple XGB regressor, which trains very quickly since we only have 4 attributes. The performance improvement is considerable."},{"metadata":{"trusted":true,"_uuid":"c866f9876fe444814cff76bc2f4b323743b12978"},"cell_type":"code","source":"clf = xgb.XGBRegressor(max_depth=4,\n                       silent=False,\n                      learning_rate=0.6,\n                      n_estimators=100,\n                      n_jobs=-1)\n\nboost_cols = ['p1', 'p2', 'p3', 'maxPlace']\ntarget = 'winPlacePerc'\n\nclf.fit(train_outs[boost_cols], train_outs[target],\n        eval_set=[(train_outs[boost_cols], train_outs[target]), (val_outs[boost_cols], val_outs[target])], \n        verbose=False, eval_metric='mae', early_stopping_rounds=5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5151d52e9630bb0420731844abfb39de37c550c2"},"cell_type":"code","source":"xgb_val_preds = clf.predict(val_outs[boost_cols])\nprint('validation set mae: {}'.format(np.mean(np.abs(xgb_val_preds - val_outs[target]))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"98996c4bf38d80743aa25bf090692e710b034151"},"cell_type":"code","source":"try:\n    test = pd.read_csv('../input/test_V2.csv')\nexcept:\n    test = pd.read_csv('~/.kaggle/competitions/pubg-finish-placement-prediction/test_V2.csv')\n\ntest, _ = reduce_mem_usage(test)\ntest = feature_engineering(test, train=False)\ntest_ids = test['Id']\ntest = test[train_cols]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0fae6067757e5937590b74922ac7dc569f5055bb"},"cell_type":"code","source":"test, _ = reduce_mem_usage(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d9edb6ae1d102bba2624c5fcd42fc4c88a2dee61"},"cell_type":"code","source":"preds1 = model1.predict(test, verbose=True)\npreds2 = model2.predict(test, verbose=True)\npreds3 = model3.predict(test, verbose=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c799ae1247ae63e07197c34f8609c0290410f808"},"cell_type":"code","source":"testMaxPlace = test['maxPlace']\ntest = None\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"69128fcc334167ce801750e248a2ed999e42e389"},"cell_type":"code","source":"preds1 = np.argmax(preds1,axis=1) / (testMaxPlace + 1)\npreds2 = (np.argmax(preds2,axis=1) - (100 - testMaxPlace)) / (testMaxPlace + 1)\npreds3 = (np.argmax(preds3,axis=1) - np.uint16(np.round((100 - testMaxPlace)*.5))) / (testMaxPlace + 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2065cf6277db1ba71c37620d7dfe5df882eaf6b8"},"cell_type":"code","source":"test_outs = pd.DataFrame({'p1': preds1, \n                         'p2': preds2, \n                         'p3': preds3, \n                         'maxPlace': testMaxPlace})\n\ntest_outs.to_csv('test_out.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dbb7c636a69c1dfb08480badd34f7c77cd5c11aa"},"cell_type":"code","source":"test_preds = clf.predict(test_outs[boost_cols])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1471e832f0d5652315209ff3d6edabea0c6cbf8f"},"cell_type":"code","source":"submission = pd.DataFrame({'Id': test_ids, 'winPlacePerc': test_preds})\nsubmission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6d2dd3aa066ab43cdee2f23ef4736e8b522fafb3"},"cell_type":"markdown","source":"# Conclusion\n\nThanks for reading! You may notice that I only used a small portion of the training set - this is because if I use all of it, the notebook runs out of memory. If you have suggestions on this (or any other way to improve this), please comment below! Thank you!"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}