{"cells": [{"cell_type": "markdown", "source": ["**Author:** Raoul Malm  \n", "**Description:** Given is a training set of samples listing passengers who survived or did not survive the Titanic disaster. The goal is to construct a model that can predict from a test dataset not containing the survival information if these passengers in the test dataset survived or not. This is a supervised classification task. The individual steps for the solution are:\n", "- Analyze data\n", "- Manipulate data: complete, convert, create, delete features\n", "- Model data: kNN, SVC, Decision Tree, Random Forest, Neural Networks\n", "\n", "**Results:** Using a split of 90%/10% on the labeled training data this implementation, training on data of 801 passengers, achieves a 86% accuracy on the validation set of 90 passengers.  \n", "**Reference:** [Titanic Data Science Solutions by Manav Sehgal](https://www.kaggle.com/startupsci/titanic-data-science-solutions?scriptVersionId=1145136)\n", "\n"], "metadata": {"_uuid": "17aaaacd8c9701d4aecc4fcd650770fcc93e2147", "_cell_guid": "4d0f1227-9788-4cf8-a9cf-21bbfad4db3a"}}, {"cell_type": "markdown", "source": ["# Libraries and Settings"], "metadata": {"_uuid": "266d8130e738c87f2149790be96ffab9ca99ce94", "_cell_guid": "6bacf1cb-9713-450b-9107-60190cc8c767"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["# data analysis and wrangling\n", "import pandas as pd\n", "import numpy as np\n", "import random as rnd\n", "import sklearn.linear_model\n", "import sklearn.svm\n", "import sklearn.ensemble\n", "import sklearn.neighbors\n", "import sklearn.naive_bayes\n", "import sklearn.tree\n", "import sklearn.neural_network\n", "from subprocess import check_output\n", "import seaborn as sns\n", "import matplotlib.pyplot as plt\n", "import os\n", "import tensorflow as tf\n", "%matplotlib inline\n", "\n", "train_set_size = 891;\n", "valid_set_size = 0;\n", "\n", "#display parent directory and working directory\n", "print(os.path.dirname(os.getcwd())+':', os.listdir(os.path.dirname(os.getcwd())));\n", "print(os.getcwd()+':', os.listdir(os.getcwd()));"], "metadata": {"_uuid": "5cfd2df9914284894f08d8f702efed041897fbac", "scrolled": false, "_cell_guid": "811372be-a4a3-4b03-a1dd-66a36f96a834"}}, {"cell_type": "markdown", "source": ["# Analyze Data\n", "\n", "The train/test sets have 891/418 rows with 12/11 columns. The features are:\n", "- Survived: 0 = No, 1 = Yes \n", "- Pclass: Ticket class 1 = 1st, 2 = 2nd, 3 = 3rd \n", "- Name: Name of the passenger\n", "- Sex: male, female \n", "- Age: Age in years. Is fractional if less than 1. If the age is estimated, it is in the form of xx.5.\n", "- SibSp: # of siblings / spouses aboard the Titanic (Sibling = brother, sister, stepbrother, stepsister, Spouse = husband, wife). Mistresses and fianc\u00e9s were ignored\n", "- Parch: # of parents / children aboard the Titanic (Parent = mother, father, Child = daughter, son, stepdaughter, stepson). Some children travelled only with a nanny, therefore Parch=0 for them.\n", "- Ticket: Ticket number \n", "- Fare: Passenger fare \n", "- Cabin: Cabin number \n", "- Embarked: Port of Embarkation C = Cherbourg, Q = Queenstown, S = Southampton\n", "\n", "The features can be characterized by different types:\n", "- numerical: Age (continuous, float64), Fare (continuous, float64), SibSp (discrete, int64), Parch (discrete, int64)\n", "- categorial: Sex (string), Pclass (int64), Embarked (character), Survived (int64), Ticket (alphanumeric, string), Cabin (alphanumeric, string), Name (string)\n"], "metadata": {"_uuid": "76c019d3fd8b019a37badd2ca90dd68df746d3aa", "_cell_guid": "e35a0af9-f2f5-4c9d-b1ab-cd55d34dc368"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["# read data and have a first look at it\n", "train_df = pd.read_csv('../input/train.csv')\n", "test_df = pd.read_csv('../input/test.csv')\n", "combine = [train_df, test_df]\n", "train_df.info()\n", "print('_'*40)\n", "test_df.info()"], "metadata": {"_uuid": "ad8140a6b4de1b76e866685fd9c615e3b0a1b22d", "_cell_guid": "f696b514-65b0-4c68-afc9-24d9c7ad5b76"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["# look at the first five rows\n", "train_df.head()"], "metadata": {"_uuid": "fe63088da87c3e8c058d98759343029bf687867c", "_cell_guid": "c6a07f39-00ee-44e7-8c63-b8b1ab325425"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["# look at the first five rows\n", "test_df.head() "], "metadata": {"_uuid": "38c3f85679cf49944fd1bfc722d8088121dde7ae", "_cell_guid": "4e0f38e6-35f2-46dc-9c34-d0c88ac0d311"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["# describe numerical data\n", "train_df.describe()"], "metadata": {"_uuid": "788e866d0c76a0a37c90b8beea35c2cbf4b0face", "_cell_guid": "463d0a51-b3a1-4727-9468-74c3f0e45758"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["# describe numerical data\n", "test_df.describe()"], "metadata": {"_uuid": "47a2deaa936890dfb617a31e7db56b5dc0c8e715", "_cell_guid": "956858f4-7cda-4c5e-be06-e9a45caa708e"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["# describe object data\n", "train_df.describe(include=['O'])"], "metadata": {"_uuid": "48c9d13d71f64ce0280025b93540353ee703778d", "_cell_guid": "596013e4-944c-4c44-9536-3753fc029e7d"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["# describe object data\n", "test_df.describe(include=['O'])"], "metadata": {"_uuid": "9a01e53cae1125459c83d26367ad58f5bbd3038f", "scrolled": true, "_cell_guid": "27528cc9-a2b9-4bc4-a391-e30b38425905"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["# check Pclass - Survived correlation\n", "train_df[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean().sort_values(by='Survived', ascending=False)"], "metadata": {"_uuid": "2818d95b9ce19d3f1fa55895fb99abaca87fca49", "_cell_guid": "8b52a7ab-7b4b-4074-9461-66c3c0e27e1d"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["# check Sex - Survived correlation\n", "train_df[[\"Sex\", \"Survived\"]].groupby(['Sex'], as_index=False).mean().sort_values(by='Survived', ascending=False)"], "metadata": {"_uuid": "a13fda47b1f19675626428444e0d654674c6a6a4", "_cell_guid": "073264d9-9ee9-430a-a789-d7a9ea8ce819"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["# check SibSp - Survived correlation\n", "train_df[[\"SibSp\", \"Survived\"]].groupby(['SibSp'], as_index=False).mean().sort_values(by='Survived', ascending=False)"], "metadata": {"_uuid": "493254854cddf43d35ae88ecccd39fdf2fc32d61", "_cell_guid": "f06e19f9-41c7-48f5-99af-3131ebc1bcb8"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["# check Parch - Survived correlation\n", "train_df[[\"Parch\", \"Survived\"]].groupby(['Parch'], as_index=False).mean().sort_values(by='Survived', ascending=False)"], "metadata": {"_uuid": "5b62f8794633932930bc14526dbe428ab9c4ed61", "_cell_guid": "f11b085e-12ab-4ae4-a30e-6f7fa84cdec1"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Age histograms depending on Survived\n", "grid = sns.FacetGrid(train_df, col='Survived');\n", "grid.map(plt.hist, 'Age', bins=20);"], "metadata": {"_uuid": "25ea5d350a8282b17fa20723bfd421495659c671", "_cell_guid": "b9e3c595-52fc-4ce1-bca8-3d3c569d1155"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Age histograms depending on Survived, Pclass\n", "grid = sns.FacetGrid(train_df, col='Survived', row='Pclass', size=2.2, aspect=1.6)\n", "grid.map(plt.hist, 'Age', alpha=.5, bins=20)\n", "grid.add_legend();"], "metadata": {"_uuid": "71efb8aa83cda4bb1a9e668d204fa4f180429433", "_cell_guid": "9a71b19e-956e-4852-8b85-76aa31822268"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Survived values depending on Embarked, Sex\n", "grid = sns.FacetGrid(train_df, row='Embarked', size=2.2, aspect=1.6);\n", "grid.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', palette='deep');\n", "grid.add_legend();"], "metadata": {"_uuid": "489e7932e751bf8e946d3c1b8aab1e233785d4a9", "_cell_guid": "537c62c3-1f43-4dbb-83a8-f7f0f0cf60d5"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Fare depending on Embarked, Survived, Sex\n", "grid = sns.FacetGrid(train_df, row='Embarked', col='Survived', size=2.2, aspect=1.6)\n", "grid.map(sns.barplot, 'Sex', 'Fare', alpha=.5, ci=None)\n", "grid.add_legend()"], "metadata": {"_uuid": "da065d1b2edd2423f6dd6212848a9e200213cc79", "_cell_guid": "a83cdbab-e89e-4590-b449-170f3aef58e8"}}, {"cell_type": "markdown", "source": ["# Manipulate Data\n", "\n", "By having analyzed the data we will perform the following steps:\n", "\n", "- create new feature: Title\n", "- delete features: Ticket, Cabin, Name, PassengerId\n", "- convert features: Sex\n", "- complete and convert feature: Age\n", "- create new features: IsAlone, Age*Class\n", "- complete and convert feature: Embarked \n", "- complete and convert feature: Fare"], "metadata": {"_uuid": "bd727fbb9d8a43457f301c93cf0dc8fe815e7579", "_cell_guid": "f5e6bedd-9ad3-4324-93d6-b6d8836c0c49"}}, {"cell_type": "markdown", "source": ["### Create new feature: Title"], "metadata": {"_uuid": "ff612dee502f55a7352ad74f1a1c1bc5e1ac3897", "_cell_guid": "ede3c3c4-dc9f-4800-b439-50b73ab93ca5"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["# extract title from Name and then create new feature: Title  \n", "for dataset in combine:\n", "    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n", "\n", "pd.crosstab(train_df['Title'], train_df['Sex'])\n", "\n", "# reduce the number of titles\n", "for dataset in combine:\n", "    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col',\n", "                                                 'Don', 'Dr', 'Major', 'Rev', 'Sir',\n", "                                                 'Jonkheer', 'Dona'], 'Rare')\n", "\n", "    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n", "    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n", "    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n", "    \n", "#train_df[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()\n", "#print('train_df.shape=',train_df.shape)\n", "\n", "# map the title to int64\n", "title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\n", "for dataset in combine:\n", "    dataset['Title'] = dataset['Title'].map(title_mapping)\n", "    dataset['Title'] = dataset['Title'].fillna(0)\n", "\n", "train_df.head()"], "metadata": {"_uuid": "048ae6c19ad3a4dc573453f50d34bc1e26f58dee", "_cell_guid": "3c6470f3-e3fe-46de-8482-1e1036eb99ab"}}, {"cell_type": "markdown", "source": [" ### Delete features: Ticket, Cabin, Name, PassengerId"], "metadata": {"_uuid": "30c909298fc876c03b542b7eb36f26997aa3282f", "_cell_guid": "02a9e9f6-c72d-49b8-9f03-407934ac5dda"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["# delete columns: Ticket, Cabin, Name, PassengerId\n", "train_df = train_df.drop(['Name', 'PassengerId', 'Ticket', 'Cabin'], axis=1)\n", "test_df = test_df.drop(['Name', 'Ticket', 'Cabin'], axis=1)\n", "combine = [train_df, test_df]\n", "print(\"train_df = \", train_df.shape)\n", "print(\"test_df = \", test_df.shape)"], "metadata": {"_uuid": "5b0b89370e7b7362db6acb6915eb6c9feae05d66", "_cell_guid": "339881dc-a267-4553-b4df-5ac1c34d09de"}}, {"cell_type": "markdown", "source": [" ### Convert features: Sex"], "metadata": {"_uuid": "30c5a5ca5805e67eb96d87edae088b9a0501a67e", "_cell_guid": "6915c616-8fbf-479c-9350-e966102c1ab1"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["# convert variable 'Sex' into type int64\n", "for dataset in combine:\n", "    dataset['Sex'] = dataset['Sex'].map( {'female': 1, 'male': 0} ).astype(int)\n", "\n", "train_df.head()"], "metadata": {"_uuid": "152ea9309d211154ce6cc0a94c649f7e0d1cb506", "_cell_guid": "4bad8e4c-27f5-4ec7-b5f1-5fa9a51f4c0a"}}, {"cell_type": "markdown", "source": ["### Complete and convert feature: Age"], "metadata": {"_uuid": "7e9cddf2d40b2eedc7ffa33d9e1d64b1f1300862", "_cell_guid": "dd33c23a-35a6-4614-b032-89e4ba55787d"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["# complete missing age entries by using information on Sex, Pclass\n", "guess_ages = np.zeros((2,3));\n", "\n", "for dataset in combine:\n", "    for i in range(0, 2):\n", "        for j in range(0, 3):\n", "            guess_df = dataset[(dataset['Sex'] == i) & \n", "                               (dataset['Pclass'] == j+1)]['Age'].dropna()\n", "\n", "            # age_mean = guess_df.mean()\n", "            # age_std = guess_df.std()\n", "            # age_guess = rnd.uniform(age_mean - age_std, age_mean + age_std)\n", "\n", "            age_guess = guess_df.median()\n", "            guess_ages[i,j] = int(age_guess/0.5 + 0.5 ) * 0.5\n", "            #print(age_guess)\n", "            \n", "    for i in range(0, 2):\n", "        for j in range(0, 3):\n", "            dataset.loc[(dataset.Age.isnull()) & (dataset.Sex == i) & \n", "                        (dataset.Pclass == j+1),'Age'] = guess_ages[i,j]\n", "\n", "    dataset['Age'] = dataset['Age'].astype(int)\n", "\n", "train_df.head()"], "metadata": {"_uuid": "f1ba3ac193e6a7f7bf4ae3ed2633eee18a663d9b", "_cell_guid": "92666ab6-92e2-496c-983d-9809331c2199"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["# create new feature AgeBand\n", "train_df['AgeBand'] = pd.cut(train_df['Age'], 5)\n", "train_df[['AgeBand', 'Survived']].groupby(['AgeBand'], as_index=False).mean().sort_values(by='AgeBand', ascending=True)"], "metadata": {"_uuid": "f32868c32eeb97af1bdf95d6516cfbcb6b23e949", "_cell_guid": "c41b12f4-62de-4ea4-b085-f1ea982b45fe"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Replace Age with ordinals based on the bands in AgeBand\n", "for dataset in combine:    \n", "    dataset.loc[ dataset['Age'] <= 16, 'Age'] = 0\n", "    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n", "    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n", "    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n", "    dataset.loc[ dataset['Age'] > 64, 'Age']\n", "train_df.head()"], "metadata": {"_uuid": "1b7a96abfbd2d6a933883ddf870cd7d9f21818aa", "_cell_guid": "80329730-e5f2-4e99-b975-bd4dfa108991"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["# remove AgeBand\n", "train_df = train_df.drop(['AgeBand'], axis=1)\n", "combine = [train_df, test_df]\n", "train_df.head()"], "metadata": {"_uuid": "471a7f63edbe0b7be514fa5ce77c4038ad3bd7f6", "scrolled": true, "_cell_guid": "27b9268a-c1cf-4cba-93f8-59c2205718b7"}}, {"cell_type": "markdown", "source": ["### Create new features: IsAlone, Age*Class"], "metadata": {"_uuid": "ba8c386ddd84948b992f2b4adc69cc233c675d0a", "_cell_guid": "0ac53154-9ff5-4c1c-b5f3-2c9b5d91fbdd"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["# create new feature FamilySize\n", "for dataset in combine:\n", "    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n", "\n", "train_df[['FamilySize', 'Survived']].groupby(['FamilySize'], as_index=False).mean().sort_values(by='Survived', ascending=False)"], "metadata": {"_uuid": "064ba062be5d0cc303081b20e7e5b85c39b15894", "_cell_guid": "2d076f61-8c88-4509-bd19-828d536cf926"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["# create new feature IsAlone\n", "for dataset in combine:\n", "    dataset['IsAlone'] = 0\n", "    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\n", "\n", "train_df[['IsAlone', 'Survived']].groupby(['IsAlone'], as_index=False).mean()\n"], "metadata": {"_uuid": "ef1d0f1c394bb86a81d13c0ac1d7bf8858fd2212", "_cell_guid": "8b10a488-878d-4460-a642-131568285796"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["# remove features: Parch, SibSp, FamilySize\n", "#train_df = train_df.drop(['Parch', 'SibSp', 'FamilySize'], axis=1)\n", "#test_df = test_df.drop(['Parch', 'SibSp', 'FamilySize'], axis=1)\n", "train_df = train_df.drop(['Parch', 'SibSp'], axis=1)\n", "test_df = test_df.drop(['Parch', 'SibSp'], axis=1)\n", "combine = [train_df, test_df]\n", "train_df.head()"], "metadata": {"_uuid": "b5317852a9d312efac586a7c7ef8898354d5e294", "_cell_guid": "c32dd512-ae87-4a0c-a5e1-5fb0bc997eda"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["# create new feature Age*Class\n", "for dataset in combine:\n", "    dataset['Age*Class'] = dataset.Age * dataset.Pclass\n", "\n", "#train_df.loc[:, ['Age*Class', 'Age', 'Pclass']].head(10)\n", "train_df.head()"], "metadata": {"_uuid": "d7e4de39b1ac2addbb1c6d8bce5e13ecc5188c87", "_cell_guid": "c9eaa4c9-c3af-4241-b117-8a5ea69fd3f1"}}, {"cell_type": "markdown", "source": ["### Complete and convert feature: Embarked "], "metadata": {"_uuid": "489a58dd294ce81096511b194f7f75f970c83e60", "_cell_guid": "c44e4349-77e0-492c-8d8d-a0001c85bbfa"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["# most frequent occurence of Embarked value\n", "freq_port = train_df.Embarked.dropna().mode()[0]\n", "print(freq_port);\n", "\n", "# replace na entries with most frequent value of Embarked\n", "for dataset in combine:\n", "    dataset['Embarked'] = dataset['Embarked'].fillna(freq_port)\n", "    \n", "train_df[['Embarked', 'Survived']].groupby(['Embarked'], as_index=False).mean().sort_values(by='Survived', ascending=False)"], "metadata": {"_uuid": "d1c424c98e58b98e3582013d8fdfd3ed47f2d378", "_cell_guid": "09ca0df6-2880-474c-bc23-19e94a766281"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["# map Embarked values to integer values\n", "for dataset in combine:\n", "    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\n", "\n", "train_df.head()"], "metadata": {"_uuid": "0f18ea6356be3e906ab5592cdce6e910932b8a85", "_cell_guid": "924e0324-b852-42b8-a934-a7e06bfb852c"}}, {"cell_type": "markdown", "source": ["### Complete and convert feature: Fare"], "metadata": {"_uuid": "ef90aa54ac2e6b864bba561be4392a27d1c8051b", "_cell_guid": "4d5a0773-53de-4c7e-ab7c-709e0ef16efd"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["# complete feature Fare\n", "test_df['Fare'].fillna(test_df['Fare'].dropna().median(), inplace=True)\n", "test_df.head()"], "metadata": {"_uuid": "961a2c418877d7c17280a90189a772476dd15315", "_cell_guid": "335505b5-fe55-49d6-99df-a42703b6a00f"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["# create feature FareBand\n", "train_df['FareBand'] = pd.qcut(train_df['Fare'], 6)\n", "train_df[['FareBand', 'Survived']].groupby(['FareBand'], as_index=False).mean().sort_values(by='FareBand', ascending=True)"], "metadata": {"_uuid": "b87bd5292ed86a448dcbd999e6b534cfced94cd5", "_cell_guid": "7c18595a-fff1-4ad0-91e9-db32a018cedf"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["# replace feature Fare by ordinals based on FareBand\n", "for dataset in combine:\n", "    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] = 0\n", "    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n", "    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n", "    dataset.loc[ dataset['Fare'] > 31, 'Fare'] = 3\n", "    dataset['Fare'] = dataset['Fare'].astype(int)\n", "\n", "train_df = train_df.drop(['FareBand'], axis=1)\n", "combine = [train_df, test_df]\n", "    \n", "train_df.head()"], "metadata": {"_uuid": "a2b444fb5267852f76935e0e995a3ce7a05e0878", "_cell_guid": "e6d4340f-b368-4c2b-b84d-792178e7d03d"}}, {"cell_type": "markdown", "source": ["\n", "# Model Data\n", "\n", "Supervised learning plus cassification limits the number of machine learning algorithms to: \n", "    - Logistic Regression\n", "    - kNN (k-Nearest Neighbors)\n", "    - SVM (Support Vector Machine) with different kernels\n", "    - Gaussian Naive Bayes\n", "    - Decision Tree\n", "    - Random Forrest\n", "    - Perceptron\n", "    - Multi-layer Perceptron\n", "    - Deep Neural Network"], "metadata": {"_uuid": "cc3e41c2e9b3bbdd2272991cf9489439a21aeac4", "_cell_guid": "20ffee26-bb39-4a33-8e22-c71a5c8f8427"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["# create subsets for training, validation and testing\n", "X_train = train_df.drop(\"Survived\", axis=1)[0:train_set_size]\n", "Y_train = train_df[\"Survived\"][0:train_set_size]\n", "if valid_set_size > 0:\n", "    X_valid = train_df.drop(\"Survived\", axis=1)[train_set_size:train_set_size+valid_set_size]\n", "    Y_valid = train_df[\"Survived\"][train_set_size:train_set_size+valid_set_size]\n", "else:\n", "    X_valid = train_df.drop(\"Survived\", axis=1)[801:]\n", "    Y_valid = train_df[\"Survived\"][801:]\n", "    \n", "X_test  = test_df.drop(\"PassengerId\", axis=1).copy()\n", "\n", "print(\"training data: \", 'X_train.shape = ', X_train.shape, 'Y_train.shape = ', Y_train.shape)\n", "print(\"validation data: \", 'X_valid.shape = ', X_valid.shape, 'Y_valid.shape = ', Y_valid.shape)\n", "print(\"test data: \", 'X_test.shape = ', X_test.shape)"], "metadata": {"_uuid": "1bd3e2715c87bcffa4a1f3a9efdc32a87e47f208", "scrolled": true, "_cell_guid": "f67936b8-5a1b-4654-80aa-005b09274f15"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["# normalize features\n", "X_train_norm = (X_train)/(X_train.max()-X_train.min());\n", "X_valid_norm = (X_valid)/(X_valid.max()-X_valid.min());\n", "X_test_norm = (X_test)/(X_test.max()-X_test.min());"], "metadata": {"collapsed": true, "_uuid": "a57c726fe307caec353d2c205b35c1009825db94", "_cell_guid": "708802d3-9546-4ee9-9e69-b0a83621ca7e"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["\"\"\"\n", "# skip features\n", "feature = 'Age*Class';\n", "if feature in X_train.columns:\n", "    X_train = X_train.drop(feature, axis=1)\n", "if feature in X_valid.columns:\n", "    X_valid = X_valid.drop(feature, axis=1)\n", "if feature in X_test.columns:\n", "    X_test = X_test.drop(feature, axis=1)\n", "\"\"\"   \n", "print('X_train.columns = ', X_train.columns.values)\n", "print('X_valid.columns = ', X_valid.columns.values)\n", "print('X_test.columns = ', X_test.columns.values)\n"], "metadata": {"_uuid": "2229c780ad9c31786cabc945084bb49ebfbab772", "_cell_guid": "10ba259a-00bf-4764-9df5-fcfa734ed8b8"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["## Logistic Regression as a benchmark model\n", "\n", "logreg = sklearn.linear_model.LogisticRegression()\n", "logreg.fit(X_train_norm, Y_train)\n", "Y_log_pred = logreg.predict(X_test_norm)\n", "acc_log_train = np.round(logreg.score(X_train_norm, Y_train), 4)\n", "acc_log_valid = np.round(logreg.score(X_valid_norm, Y_valid), 4)\n", "print('Logistic Regression: train/valid Acc = %.4f/%.4f'%(acc_log_train, acc_log_valid))\n", "coeff_df = pd.DataFrame(X_train.columns.delete(0))\n", "coeff_df.columns = ['Feature']\n", "coeff_df[\"Correlation\"] = pd.Series(logreg.coef_[0])\n", "coeff_df.sort_values(by='Correlation', ascending=False)"], "metadata": {"_uuid": "05c78c65f10ed33df9d76032b21ec03867647ed9", "_cell_guid": "27f7f7f1-aed6-49e5-9f6c-498d64e40ca7"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["## Further Machine Learning Algorithms\n", "\n", "# support vector machine with rbf kernel\n", "svc_rbf = sklearn.svm.SVC(kernel='rbf')\n", "svc_rbf.fit(X_train_norm, Y_train)\n", "Y_pred_svc_rbf = svc_rbf.predict(X_test_norm)\n", "acc_svc_rbf_train = np.round(svc_rbf.score(X_train_norm, Y_train), 4)\n", "acc_svc_rbf_valid = np.round(svc_rbf.score(X_valid_norm, Y_valid), 4)\n", "print('SVC rbf kernel: train/valid Acc = %.4f/%.4f'%(acc_svc_rbf_train, acc_svc_rbf_valid))\n", "\n", "# support vector machine with linear kernel\n", "svc_linear = sklearn.svm.SVC(kernel='linear')\n", "svc_linear.fit(X_train_norm, Y_train)\n", "Y_pred_svc_linear = svc_linear.predict(X_test_norm)\n", "acc_svc_linear_train = np.round(svc_linear.score(X_train_norm, Y_train), 4)\n", "acc_svc_linear_valid = np.round(svc_linear.score(X_valid_norm, Y_valid), 4)\n", "print('SVC linear kernel: train/valid Acc = %.4f/%.4f'%(acc_svc_linear_train, acc_svc_linear_valid))\n", "\n", "# k-Nearest-Neighbour Algorithm\n", "knn = sklearn.neighbors.KNeighborsClassifier(n_neighbors = 5)\n", "knn.fit(X_train_norm, Y_train)\n", "Y_pred_knn = knn.predict(X_test_norm)\n", "acc_knn_train = np.round(knn.score(X_train_norm, Y_train), 4)\n", "acc_knn_valid = np.round(knn.score(X_valid_norm, Y_valid), 4)\n", "print('kNN: train/valid Acc = %.4f/%.4f'%(acc_knn_train, acc_knn_valid))\n", "\n", "# Gaussian Naive Bayes\n", "gaussianNB = sklearn.naive_bayes.GaussianNB()\n", "gaussianNB.fit(X_train_norm, Y_train)\n", "Y_pred_gaussianNB = gaussianNB.predict(X_test_norm)\n", "acc_gaussianNB_train = np.round(gaussianNB.score(X_train_norm, Y_train), 4)\n", "acc_gaussianNB_valid = np.round(gaussianNB.score(X_valid_norm, Y_valid), 4)\n", "print('Gaussian Naive Bayes: train/valid Acc = %.4f/%.4f'%(acc_gaussianNB_train, acc_gaussianNB_valid))\n", "\n", "# Decision Tree\n", "decision_tree = sklearn.tree.DecisionTreeClassifier()\n", "decision_tree.fit(X_train_norm, Y_train)\n", "Y_pred_decision_tree = decision_tree.predict(X_test_norm)\n", "acc_decision_tree_train = np.round(decision_tree.score(X_train_norm, Y_train), 4)\n", "acc_decision_tree_valid = np.round(decision_tree.score(X_valid_norm, Y_valid), 4)\n", "print('Decision Tree: train/valid Acc = %.4f/%.4f'%(acc_decision_tree_train, acc_decision_tree_valid))\n", "\n", "# Random Forest\n", "random_forest = sklearn.ensemble.RandomForestClassifier(n_estimators=10)\n", "random_forest.fit(X_train_norm, Y_train)\n", "Y_pred_random_forest = random_forest.predict(X_test_norm)\n", "random_forest.score(X_train_norm, Y_train)\n", "acc_random_forest_train = np.round(random_forest.score(X_train_norm, Y_train), 4)\n", "acc_random_forest_valid = np.round(random_forest.score(X_valid_norm, Y_valid), 4)\n", "print('Random Forest: train/valid Acc = %.4f/%.4f'%(acc_random_forest_train, acc_random_forest_valid))\n", "\n", "# Perceptron\n", "perceptron = sklearn.linear_model.Perceptron(max_iter = 10000, tol = 1e-6, shuffle = True)\n", "perceptron.fit(X_train_norm, Y_train)\n", "Y_pred_perceptron = perceptron.predict(X_test_norm)\n", "acc_perceptron_train = np.round(perceptron.score(X_train_norm, Y_train), 4)\n", "acc_perceptron_valid = np.round(perceptron.score(X_valid_norm, Y_valid), 4)\n", "print('Perceptron: train/valid Acc = %.4f/%.4f'%(acc_perceptron_train, acc_perceptron_valid))\n", "\n", "# Multi Layer Perceptron\n", "mlp = sklearn.neural_network.MLPClassifier(activation='relu', alpha=1e-05, batch_size='auto',\n", "                                           max_iter = 10000, tol = 1e-6, shuffle = False, \n", "                                           hidden_layer_sizes=(64,32,16), solver ='adam',\n", "                                           learning_rate = 'adaptive',\n", "                                           learning_rate_init=0.001, verbose=False);\n", "mlp.fit(X_train_norm, Y_train)\n", "Y_pred_mlp = mlp.predict(X_test_norm)\n", "acc_mlp_train = np.round(mlp.score(X_train_norm, Y_train), 4)\n", "acc_mlp_valid = np.round(mlp.score(X_valid_norm, Y_valid), 4)\n", "print('MLP: train/valid Acc = %.4f/%.4f'%(acc_mlp_train, acc_mlp_valid))"], "metadata": {"_uuid": "e5d76b8e14e7d077a2f3759fd7af371188202d85", "scrolled": true, "_cell_guid": "25d15df0-e81a-4f3e-a6e1-0a9e6728bf6e"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["## Deep Neural Network\n", "\n", "x_size = X_train_norm.shape[1]; # number of features\n", "y_size = 1; # binary variable\n", "n_n_fc1 = 128; # number of neurons of first layer\n", "n_n_fc2 = 64; # number of neurons of second layer\n", "n_n_fc3 = 32; # number of neurons of third layer\n", "\n", "# variables for input and output \n", "x_data = tf.placeholder('float', shape=[None, x_size])\n", "y_data = tf.placeholder('float', shape=[None, y_size])\n", "\n", "# 1.layer: fully connected\n", "W_fc1 = tf.Variable(tf.truncated_normal(shape = [x_size, n_n_fc1], stddev = 0.1))\n", "b_fc1 = tf.Variable(tf.constant(0.1, shape = [n_n_fc1]))  \n", "h_fc1 = tf.nn.relu(tf.matmul(x_data, W_fc1) + b_fc1)\n", "\n", "# dropout\n", "tf_keep_prob = tf.placeholder('float')\n", "h_fc1_drop = tf.nn.dropout(h_fc1, tf_keep_prob)\n", "\n", "# 2.layer: fully connected\n", "W_fc2 = tf.Variable(tf.truncated_normal(shape = [n_n_fc1, n_n_fc2], stddev = 0.1)) \n", "b_fc2 = tf.Variable(tf.constant(0.1, shape = [n_n_fc2]))  \n", "h_fc2 = tf.nn.relu(tf.matmul(h_fc1_drop, W_fc2) + b_fc2) \n", "\n", "# dropout\n", "h_fc2_drop = tf.nn.dropout(h_fc2, tf_keep_prob)\n", "\n", "# 3.layer: fully connected\n", "W_fc3 = tf.Variable(tf.truncated_normal(shape = [n_n_fc2, n_n_fc3], stddev = 0.1)) \n", "b_fc3 = tf.Variable(tf.constant(0.1, shape = [n_n_fc3]))  \n", "h_fc3 = tf.nn.relu(tf.matmul(h_fc2_drop, W_fc3) + b_fc3) \n", "\n", "# dropout\n", "h_fc3_drop = tf.nn.dropout(h_fc3, tf_keep_prob)\n", "\n", "# 3.layer: fully connected\n", "W_fc4 = tf.Variable(tf.truncated_normal(shape = [n_n_fc3, y_size], stddev = 0.1)) \n", "b_fc4 = tf.Variable(tf.constant(0.1, shape = [y_size]))  \n", "z_pred = tf.matmul(h_fc3_drop, W_fc4) + b_fc4  \n", "\n", "# cost function\n", "cross_entropy = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=y_data, logits=z_pred));\n", "\n", "# optimisation function\n", "tf_learn_rate = tf.placeholder(dtype='float', name=\"tf_learn_rate\")\n", "train_step = tf.train.AdamOptimizer(tf_learn_rate).minimize(cross_entropy)\n", "\n", "# evaluation\n", "y_pred = tf.nn.sigmoid(z_pred);\n", "y_pred_class = tf.cast(tf.greater(y_pred, 0.5),'float')\n", "accuracy = tf.reduce_mean(tf.cast(tf.equal(y_pred_class, y_data ), 'float'))\n", "\n", "# start TensorFlow session and initialize global variables\n", "sess = tf.InteractiveSession() \n", "sess.run(tf.global_variables_initializer())  \n", "\n", "keep_prob = 0.2; # dropout regularization with keeping probability\n", "learn_rate_range = [0.01,0.005,0.0025,0.001];\n", "learn_rate_step = 500;\n", "\n", "x_train_batch = X_train_norm.iloc[:,:].values.astype('float');\n", "y_train_batch = Y_train.iloc[:].values.reshape(Y_train.shape[0],1).astype('float');\n", "\n", "x_valid_batch = X_valid_norm.iloc[:,:].values.astype('float');\n", "y_valid_batch = Y_valid.iloc[:].values.reshape(Y_valid.shape[0],1).astype('float');\n", "\n", "x_test_batch = X_test_norm.iloc[:,:].values.astype('float');\n", "\n", "n_epoch = 1000; # number of epochs\n", "train_loss, train_acc, valid_loss, valid_acc = np.array([]), np.array([]), np.array([]), np.array([]);\n", "n_step = -1;\n", "\n", "# training model\n", "for i in range(0,n_epoch):\n", "    \n", "    if i%learn_rate_step == 0:\n", "        n_step += 1;\n", "        learn_rate = learn_rate_range[n_step];\n", "        print('set learnrate = ', learn_rate)\n", "        \n", "    sess.run(train_step, feed_dict={x_data: x_train_batch, y_data: y_train_batch, tf_keep_prob: keep_prob, \n", "                                    tf_learn_rate: learn_rate})\n", "    \n", "    if i%100==0:\n", "        train_loss = np.append(train_loss, sess.run(cross_entropy, feed_dict={x_data: x_train_batch, \n", "                                                                              y_data: y_train_batch, \n", "                                                                              tf_keep_prob: 1.0}));\n", "    \n", "        train_acc = np.append(train_acc, accuracy.eval(feed_dict={x_data: x_train_batch, \n", "                                                                  y_data: y_train_batch, \n", "                                                                  tf_keep_prob: 1.0}));      \n", "    \n", "        valid_loss = np.append(valid_loss, sess.run(cross_entropy, feed_dict={x_data: x_valid_batch, \n", "                                                                              y_data: y_valid_batch, \n", "                                                                              tf_keep_prob: 1.0}));\n", "    \n", "        valid_acc = np.append(valid_acc, accuracy.eval(feed_dict={x_data: x_valid_batch, \n", "                                                                  y_data: y_valid_batch, \n", "                                                                  tf_keep_prob: 1.0}));      \n", "    \n", "        print('%d epoch: train/val loss = %.4f/%.4f, train/val acc = %.4f/%.4f'%(i+1,\n", "                                                                                 train_loss[-1],\n", "                                                                                 valid_loss[-1],\n", "                                                                                 train_acc[-1],\n", "                                                                                 valid_acc[-1]))\n", "\n", "acc_DNN_train = train_acc[-1];\n", "acc_DNN_valid = valid_acc[-1];\n", "# prediction for test set\n", "Y_pred_DNN = y_pred_class.eval(feed_dict={x_data: x_test_batch,tf_keep_prob: 1.0}).astype('int').flatten()\n", "\n", "sess.close();"], "metadata": {"_uuid": "1ec800285f3b259c739b42da52c5c8ad48e3b12f", "_cell_guid": "2873867e-926f-471e-8787-e4453679d4ac"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["# model summary\n", "models = pd.DataFrame({\n", "    'Model': ['SVC with rbf kernel', 'kNN', 'Logistic Regression', \n", "              'Random Forest', 'Gaussian Naive Bayes', 'Perceptron', \n", "              'MLP', 'SVC with linear kernel', 'Decision Tree', 'Deep Neural Network'],\n", "    'Train Acc': [acc_svc_rbf_train, acc_knn_train, acc_log_train, \n", "              acc_random_forest_train, acc_gaussianNB_train, acc_perceptron_train, \n", "              acc_mlp_train, acc_svc_linear_train, acc_decision_tree_train, acc_DNN_train],\n", "    'Valid Acc': [acc_svc_rbf_valid, acc_knn_valid, acc_log_valid, \n", "              acc_random_forest_valid, acc_gaussianNB_valid, acc_perceptron_valid, \n", "              acc_mlp_valid, acc_svc_linear_valid, acc_decision_tree_valid, acc_DNN_valid]})\n", "models.sort_values(by='Valid Acc', ascending=False)"], "metadata": {"_uuid": "9476d3d481c3306eb4a71945801fbd734a15e77c", "scrolled": true, "_cell_guid": "2c24065a-4aed-4bb7-a7db-f14a0a56716c"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["# submit the best results\n", "submission = pd.DataFrame({\n", "        \"PassengerId\": test_df[\"PassengerId\"],\n", "        \"Survived\": Y_pred_DNN\n", "    })\n", "\n", "#if not os.path.exists(os.path.dirname(os.getcwd())+'/output'): \n", "#    print('create directory ', os.path.dirname(os.getcwd())+'/output')\n", "#    os.makedirs(os.path.dirname(os.getcwd())+'/output')\n", "#submission.to_csv('../output/submission.csv', index=False)\n", "submission.to_csv('submission.csv', index=False)"], "metadata": {"collapsed": true, "_uuid": "e44f786a96daaf149f8744f163748831ba8f1576", "_cell_guid": "a9812ec5-e121-469e-ae19-c90efecf2896"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": [], "metadata": {"collapsed": true, "_uuid": "b2948a1583a04172cc2d363d83930646b1444e07", "_cell_guid": "d4e36479-4084-4e58-aa57-6813ca201148"}}], "nbformat": 4, "nbformat_minor": 1, "metadata": {"language_info": {"version": "3.6.3", "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py", "codemirror_mode": {"version": 3, "name": "ipython"}, "mimetype": "text/x-python", "name": "python"}, "kernelspec": {"language": "python", "name": "python3", "display_name": "Python 3"}}}