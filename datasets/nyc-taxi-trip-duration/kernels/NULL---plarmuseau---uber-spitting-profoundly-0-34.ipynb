{"nbformat_minor": 1, "cells": [{"metadata": {"_uuid": "ba7c8fb259d027104ae89c92b8d1d80cb9121e25", "trusted": false, "_cell_guid": "1a73ba86-389f-4050-939e-f17fc3362032", "_execution_state": "idle"}, "outputs": [], "source": "import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport scipy as sp\nimport matplotlib\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom subprocess import check_output\ntrain = pd.read_csv('../input/train.csv')[:700000]\ntest = pd.read_csv('../input/test.csv')\n\nprint (train.shape,train.columns)\nprint(test.shape,test.columns)\ntest['split']=1\ntrain['split']=0\ntotal=train.append(test)", "cell_type": "code", "execution_count": 1}, {"metadata": {"_uuid": "3a6bf649e972715bf505b6a4874bd7b75a359b16", "trusted": false, "_cell_guid": "fc3bd7f3-8f6a-4959-9dbe-490752d1eeda", "_execution_state": "idle"}, "outputs": [], "source": "# lat and long number comes from & credit to DrGuillermo: Animation\nxlim = [-74.03, -73.77]\nylim = [40.63, 40.85]\ntotal = total[(total.pickup_longitude> xlim[0]) & (total.pickup_longitude < xlim[1])]\ntotal = total[(total.dropoff_longitude> xlim[0]) & (total.dropoff_longitude < xlim[1])]\ntotal = total[(total.pickup_latitude> ylim[0]) & (total.pickup_latitude < ylim[1])]\ntotal = total[(total.dropoff_latitude> ylim[0]) & (total.dropoff_latitude < ylim[1])]", "cell_type": "code", "execution_count": 2}, {"metadata": {"_uuid": "79f97fb0ae35cc295593e5a0db0629f7243343d7", "_cell_guid": "a2c02c2a-3fda-47a3-a35d-65381058c3ea", "_execution_state": "idle"}, "outputs": [], "source": "# Calculate Distance and compass direction", "cell_type": "markdown", "execution_count": null}, {"metadata": {"_uuid": "5c7bcff7fb89d476519a0b7dcd815061cc7db8a1", "trusted": false, "_cell_guid": "f661c93f-41d1-4ca9-90a1-4dee43245711", "_execution_state": "idle"}, "outputs": [], "source": "from math import radians, cos, sin, asin, sqrt, atan2,degrees\n\ndef distance(row):\n#     lon1, lat1, lon2, lat2):\n    \"\"\"\n    Calculate the circle distance between two points in lat and lon\n    on the earth (specified in decimal degrees)\n    returning distance in miles\n    \"\"\"\n    # need to convert decimal degrees to radians \n    # a unit of angle, equal to an angle at the center of a circle whose arc is equal in length to the radius.\n    lon1, lat1, lon2, lat2 = row['pickup_longitude'], row['pickup_latitude'], row['dropoff_longitude'], row['dropoff_latitude']\n    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n\n    # haversine formula \n    dlon = lon2 - lon1 \n    dlat = lat2 - lat1 \n    #a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n    #c = 2 * asin(sqrt(a)) \n    c = abs(dlon)+abs(dlat)\n    r = 6371 # Radius of earth in kilometers. Use 3956 for miles\n    return c * r\n\n\n#applying to the dataset\ntotal['distance'] = total.apply(distance, axis=1)\n", "cell_type": "code", "execution_count": 4}, {"metadata": {"_uuid": "2d171c7d14b2f8da445aa3073df47820fdc961f2", "trusted": false, "_cell_guid": "96f5f7e4-1fd1-487c-995b-e7ac5494349a", "_execution_state": "idle"}, "outputs": [], "source": "total.dtypes", "cell_type": "code", "execution_count": 5}, {"metadata": {"_uuid": "12f37b8b2faa44849c977b746c1b216e330697aa", "_cell_guid": "d5ce3c38-0b76-455e-a0e1-5959837863b9", "_execution_state": "idle"}, "outputs": [], "source": "# Extracting Hour, Day of the Week and Month\n# dividing pickup and dropoff in squares /blocks\n#estimating the manhattan distance, speed, direction traveltime per distance\n#", "cell_type": "markdown", "execution_count": null}, {"metadata": {"_uuid": "6f57691895c804ad2cdd5e7a949be0026b36e2dc", "trusted": false, "_cell_guid": "5c0b411f-1574-4e98-bd1c-eac9e82be588", "_execution_state": "idle"}, "outputs": [], "source": "#total transformation time\ntotal['pickup_datetime'] = pd.to_datetime(total['pickup_datetime'])\ntotal[\"p_day\"] = total[\"pickup_datetime\"].dt.strftime('%u').astype(int)\ntotal[\"p_hour\"] = total[\"pickup_datetime\"].dt.strftime('%H').astype(int)\ntotal[\"p_min\"] = total[\"pickup_datetime\"].dt.strftime('%M').astype(int)\ntotal[\"p_month\"]= total[\"pickup_datetime\"].dt.strftime('%m').astype(int)\ntotal['p_day_hour'] = total['p_day'] * 24 + total['p_hour']\ntotal['p_min'] = total['pickup_datetime'].dt.minute\n\n#speed\ntotal[\"log_speed\"]= np.log(total['distance']/total['trip_duration']*3600+1)  #log( km/h  )\ntotal[\"p_x\"]=((total['pickup_longitude']+74.25)*110).round(0)   #2miles square\ntotal[\"p_y\"]=((total['pickup_latitude']-40.6)*110).round(0)\ntotal[\"d_x\"]=((total['dropoff_longitude']+74.25)*110).round(0)\ntotal[\"d_y\"]=((total['dropoff_latitude']-40.6)*110).round(0)\ntotal['p_block']=total[\"p_x\"]*1000+total[\"p_y\"]        # qiving unique block numbers\ntotal['d_block']=total[\"d_x\"]*1000+total[\"d_y\"]\ntotal[\"manhat\"]=(total[\"p_x\"]-total[\"d_x\"]).abs()+(total[\"p_y\"]-total[\"d_y\"]).abs()  # estimating block distance\ntotal[\"log_manh_speed\"]= np.log(total['manhat']/total['trip_duration']*3600+1)\ntotal['log_trip_duration'] = np.log(total['trip_duration'].values + 1)\ntotal['time_km']=total['log_trip_duration']/np.log(total['distance']+1)\ntotal['time_km']=total['time_km'].replace([np.inf, -np.inf], np.nan).fillna(value=-1)\ntotal['lon_dist']=( total['pickup_longitude']-total['dropoff_longitude'] ) * 110\ntotal['lat_dist']=( total['pickup_latitude']-total['dropoff_latitude'] ) * 110\ntotal['lon_speed']=total['lon_dist']/total['trip_duration']*3600\ntotal['lat_speed']=total['lat_dist']/total['trip_duration']*3600\n # log trip duration is normal distribution\n\n\ntotal['store_and_fwd_flag'] = 1 * (total.store_and_fwd_flag.values == 'Y') # flag Y >>>> 1\ntotal.dtypes\ntotal.head(10)\n", "cell_type": "code", "execution_count": 6}, {"metadata": {"_uuid": "18305ad167cd3f7ce202b4324332db2f7cd68dc9", "_cell_guid": "73f4d8d7-ef6e-49d9-a2e2-4cb4b26a4bdf", "_execution_state": "idle"}, "outputs": [], "source": "# Log normalises time - distance - speed\n", "cell_type": "markdown", "execution_count": null}, {"metadata": {"_uuid": "e9f48ef7c999fd59996034e6b24907706e778cf6", "trusted": false, "_cell_guid": "9d5df231-d755-4418-ac00-af8252fbcae4", "_execution_state": "idle"}, "outputs": [], "source": "#split again\ntrain=total[total['split']==0]\ntest=total[total['split']==1]\n\n# the block distance shows 0,1,2,3 blocks distance are dominant\nplt.hist(np.log(train['manhat']+1).values, bins=100,color='g')\nplt.xlabel('log(block-trip_duration)')\nplt.ylabel('number of train records')\nplt.show()\n\n# the block distance shows 0,1,2,3 blocks distance are dominant\nplt.hist(np.log(train['distance']+1).values, bins=100,color='g')\nplt.xlabel('log(block distance)')\nplt.ylabel('number of train records')\nplt.show()\n\n# the block distance shows 0,1,2,3 blocks distance are dominant\nplt.hist(train['log_speed'].values, bins=100,color='g')\nplt.xlabel('log(speed)')\nplt.ylabel('number of train records')\nplt.show()\n\n# the block distance shows 0,1,2,3 blocks distance are dominant\nplt.hist(train['log_trip_duration'].values, bins=100,color='g')\nplt.xlabel('log(trip duration)')\nplt.ylabel('number of train records')\nplt.show()", "cell_type": "code", "execution_count": 7}, {"metadata": {"_uuid": "27f1d6b020852981572e757eca82a6e704b4bded", "_cell_guid": "df67f40a-4f86-4cdb-a92f-0b6bc4b3cf3c", "_execution_state": "idle"}, "outputs": [], "source": "#Day - hour analysis\n\nYou see a very nice and logically mean travelling speed increasing at night and decreasing during daytime in function of day/hour. Inreasing during weekend.\n\nNYC is probably permanently congested, which makes the timing forecast medium uber-unpredictable.\n\nNice to see the direction chaging day / night cyclus\n", "cell_type": "markdown", "execution_count": null}, {"metadata": {"_uuid": "671a23a8eefc6d5754b058b78ea7628fdd9f80d1", "trusted": false, "_cell_guid": "f44f86f4-2ef5-4002-9d49-f7cee71da07d", "_execution_state": "idle"}, "outputs": [], "source": "import seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\ntrain=total[total['split']==0]\ngroep=train.groupby(['p_day_hour','d_block'])['log_speed'].describe().fillna(method='bfill')\ngroep['eff']=groep['std']/groep['mean']\ngroep['eff2']=groep['eff']*groep['std']\n\n\ndef clust(x):\n    kl=0\n    if x<0.1:   # low variability cluster\n        kl=1\n    if x>0.1 and x<0.3: # moderate variability cluster, process with short adjustments\n        kl=2\n    if x>0.3: # high variability class, process times with long outages, failures of tests\n        kl=4\n    return kl\n\ngroep['clust']=groep['eff2'].map(clust)\nprint(groep)\ngroep.columns=['count','mean','min','25p','50p','75p','std','max','eff','eff2','clust']\n#append data to total to take with analysis later\ntotal=pd.merge(total,groep, how='outer', left_on=['p_day_hour','d_block'],suffixes=('', '_DH'), right_index=True)\n\n", "cell_type": "code", "execution_count": 8}, {"metadata": {"_uuid": "8d7a4a4f2499c1b86cf9f8dbbcc4e9b853106220", "_cell_guid": "d652a4ad-5de6-4af5-93a8-10fd6528204c", "_execution_state": "idle"}, "outputs": [], "source": "# make the block combinations and estimate the log-speed : here we see a kind of forecastibility, a narrowing of the error...\n\n", "cell_type": "markdown", "execution_count": null}, {"metadata": {"_uuid": "e7637dd4582c2fc51f37f233b21f3626f2e859ad", "trusted": false, "_cell_guid": "5d284297-a17f-4f9e-9b27-fdb7704a70c0", "_execution_state": "idle"}, "outputs": [], "source": "groep=train.groupby(['p_x','p_y','d_x','d_y'])['log_speed'].describe()  #.fillna(method='bfill')\ngroep['eff']=groep['std']/groep['mean']\ngroep['eff2']=groep['eff']*groep['std']\n\n\ndef clust(x):\n    kl=0\n    if x<0.1:   # low variability cluster\n        kl=1\n    if x>0.1 and x<0.3: # moderate variability cluster, process with short adjustments\n        kl=2\n    if x>0.3: # high variability class, process times with long outages, failures of tests\n        kl=4\n    return kl\n\ngroep['clust']=groep['eff2'].map(clust)\n#print(groep)\ngroep.columns=['count','mean','min','25p','50p','75p','std','max','eff','eff2','clust']\ntotal=pd.merge(total,groep, how='outer', left_on=['p_x','p_y','d_x','d_y'],suffixes=('', '_PD'), right_index=True)\n\n", "cell_type": "code", "execution_count": 10}, {"metadata": {"_uuid": "54c0c6dcbb354bc7bbfad65686e03bbca1c902b1", "trusted": false, "_cell_guid": "b552f1c4-e8b9-443f-a27b-270ab8a6281f", "_execution_state": "idle"}, "outputs": [], "source": "from sklearn.decomposition import PCA, FastICA,TruncatedSVD\nfrom sklearn.random_projection import GaussianRandomProjection,SparseRandomProjection\nfrom sklearn.cluster import KMeans\nfrom mpl_toolkits.mplot3d import Axes3D\nimport seaborn as sns\nfrom pandas.plotting import scatter_matrix\n# INPUT df  (dataframe en welke kolommen je gebruikt om te klusteren)\n# define 'clust' groep\n# define drop colomns\n\ndef plot_results(results):\n    results=pd.DataFrame(results[:1000])\n    results['d_block']=labels    \n    \n    sns.set(style=\"ticks\")\n    sns.pairplot(results,hue='d_block')\n    plt.show()\n    # To getter a better understanding of interaction of the dimensions\n    # plot the first three PCA dimensions\n    fig = plt.figure(1, figsize=(12, 12))\n    ax = Axes3D(fig, elev=-150, azim=110)\n    ax.scatter(results[0], results[1], results[2], c=results['d_block'], cmap=plt.cm.Paired)\n    ax.set_title(\"cluster 3D\")\n    ax.set_xlabel(\"1st eigenvector\")\n    ax.w_xaxis.set_ticklabels([])\n    ax.set_ylabel(\"2nd eigenvector\")\n    ax.w_yaxis.set_ticklabels([])\n    ax.set_zlabel(\"3rd eigenvector\")\n    ax.w_zaxis.set_ticklabels([])\n\n    plt.show()\n#-------------------------------------\ntrain=total[total['split']==0]\n\nlabels =total['d_block']\ndrop_columns=['dropoff_datetime', 'id', 'split', 'pickup_datetime']\ndrop_columns=list(set(drop_columns))\n#y values df_new['y']\n# X = all the variables X10-X300 not dupl, not singular\nX = total.drop(drop_columns,axis=1)\nprint(X.columns)\n#ndex1=[t for t in range(0,len(train))]\n#X.index=index1 #drop is moved up is already happened\nX=X.replace([np.inf, -np.inf], np.nan).fillna(value=0)\n#print(X)\nn_comp = 5  #define number of clusters\n#-------------------------------------\n\n\n\nprint('-------Sparse Random Projection---------')\n# SRP\nsrp = SparseRandomProjection(n_components=n_comp, dense_output=True, random_state=420)\n#normalizer = Normalizer(copy=False)\n#lsa = make_pipeline(srp, normalizer)\nresults = srp.fit_transform(X)\n#plot_results(results)\n\n#append best clusters\n#Append decomposition components to datasets  # to do in next part\nfor i in range(1, n_comp + 1):\n    total['srp_' + str(i)] = results[:,i - 1]", "cell_type": "code", "execution_count": 9}, {"metadata": {"_uuid": "1eaddb1f0d7d9468f1c09ccade8a8b6fc7f3fb2f", "trusted": false, "_cell_guid": "e6858942-adb0-42ca-8dfc-a6762ca452ee", "_execution_state": "idle"}, "outputs": [], "source": "print(total)", "cell_type": "code", "execution_count": 10}, {"metadata": {"_uuid": "c651d9a4426477aa1561b4de0ebdd149addc3399", "trusted": false, "_cell_guid": "a3e3a5f4-7ecf-4bdc-921c-b4b15f5fc3e7", "_execution_state": "idle"}, "outputs": [], "source": "from collections import Counter\ndef todrop_col(df,tohold):\n    # use todrop_col(dataframe,['listtohold'])\n    # Categorical features\n    df.replace([np.inf, -np.inf], np.nan).fillna(value=-1)\n    \n    cat_cols = []\n    for c in df.columns:\n        if df[c].dtype == 'object':\n            cat_cols.append(c)\n        if df[c].dtype == 'datetime64[ns]':\n            cat_cols.append(c)\n    print('Categorical columns:', cat_cols)\n    \n    \n    # Constant columns\n    cols = df.columns.values    \n    const_cols = []\n    for c in cols:   \n        if len(df[c].unique()) == 1:\n            const_cols.append(c)\n    print('Constant cols:', const_cols)\n    \n    \n    # Dublicate features\n    d = {}; done = []\n    cols = df.columns.values\n    for c in cols:\n        d[c]=[]\n    for i in range(len(cols)):\n        if i not in done:\n            for j in range(i+1, len(cols)):\n                if df[cols[i]].dtype == df[cols[j]].dtype:\n                    if all(df[cols[i]] == df[cols[j]]):\n                        done.append(j)\n                        d[cols[i]].append(cols[j])\n    dub_cols = []\n    for k in d.keys():\n        if len(d[k]) > 0: \n            # print k, d[k]\n            dub_cols += d[k]        \n    print('Dublicates:', dub_cols)\n    \n    kolom=list(set(dub_cols+const_cols+cat_cols))\n    kolom=[k for k in kolom if k not in tohold]\n    \n    return kolom\n\ndef tree_col(df,splitcol,splitval,groupcol):\n    #use tree_col(dataframe,column that splits,vale to split, column that groups)\n    #sklear feature selection\n    import sklearn    \n    from sklearn.svm import LinearSVC\n    from sklearn.feature_selection import SelectFromModel\n    from sklearn.ensemble import ExtraTreesClassifier\n    \n    tabel = df[df[splitcol]==splitval]\n    label = tabel[groupcol].round(0)\n    feat = df.columns  \n    clf = ExtraTreesClassifier()\n    clf = clf.fit(tabel[feat], label)\n    model = SelectFromModel(clf, prefit=True)\n    interesting_cols = model.transform(tabel[feat])\n    print('Treeclassifier cols',interesting_cols.shape)\n    tabel2=pd.DataFrame(interesting_cols,index=tabel.index)\n    feat2=tabel2.columns\n    feat3=[]\n    for ci in feat:\n        for cj in feat2:\n            if all(tabel[ci] == tabel2[cj]):\n                feat3.append(ci) \n    #print('interesting Treecolumns',feat3)\n    \n    return feat3\n\ntrain1=total[total['split']==0]\nprint(train1.dtypes)\nprint(todrop_col(train1,['d_block']))\n\ntrain1=train1.drop(['dropoff_datetime','pickup_datetime','id','25p', 'clust'],axis=1).replace([np.inf, -np.inf], np.nan).fillna(value=-1)\n\nprint(tree_col(train1,'split',0,'d_block'))", "cell_type": "code", "execution_count": null}, {"metadata": {"_uuid": "518459ec0eb8a1d983ee84b0ab4a15d397cee2be", "trusted": false, "scrolled": false, "_cell_guid": "1c2564ef-0296-486b-b6d2-a52c1fb46e49", "_execution_state": "idle"}, "outputs": [], "source": "import xgboost as xgb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import MiniBatchKMeans\n\ntrain1=total[total['split']==0]\ntest1=total[total['split']==1]\n\n#feature_names = ['dropoff_latitude', 'dropoff_longitude', 'pickup_latitude', 'compass', 'p_hour','p_day','mean', 'p_block', 'd_block', 'manhat','log_trip_duration','time_km','lon_dist','lat_dist','lon_speed','lat_speed']\n#feature_names = ['dropoff_latitude', 'dropoff_longitude', 'distance','d_x', 'd_y', 'p_block', 'd_block', 'count_PD', 'mean_PD', 'min_PD', '50p_PD', '75p_PD', 'std_PD', 'max_PD', 'eff2_PD','lon_dist','lat_dist','lon_speed','lat_speed']\n#feature_names = ['dropoff_latitude', 'dropoff_longitude', 'p_y', 'd_x', 'd_y', 'p_block', 'd_block', 'manhat', 'count_PD', 'mean_PD', 'min_PD', '25p_PD', '50p_PD', '75p_PD', 'std_PD', 'max_PD', 'eff_PD', 'eff2_PD','count_DH','50p_lat', 'id', 'count_lat', 'eff2', 'split', '50p_lon', 'dropoff_datetime', 'min_lon', 'eff2_lat', '25p', '75p_lon', 'clust_lat', 'std_lon', '75p_lat', 'std_lat', '25p_lon', 'eff2_lon', 'pickup_datetime', 'max_lat', 'clust', 'eff_lat', 'clust_lon', 'min_lat', 'count', 'std', '25p_lat', 'count_lon', 'max', 'eff_lon', 'eff', 'mean_lon', '75p', 'min', 'max_lon', '50p', 'mean_lat', 'mean']\n#feature_names = ['dropoff_latitude', 'dropoff_longitude', 'p_y', 'd_x', 'd_y', 'p_block', 'd_block', 'manhat', 'count_PD', 'mean_PD', 'min_PD', '25p_PD', '50p_PD', '75p_PD', 'std_PD', 'max_PD', 'eff_PD', 'eff2_PD','count_DH','50p_lat', 'id', 'count_lat', 'eff2', 'split', '50p_lon', 'dropoff_datetime', 'min_lon', 'eff2_lat', '25p', '75p_lon', 'clust_lat', 'std_lon', '75p_lat', 'std_lat', '25p_lon', 'eff2_lon', 'pickup_datetime', 'max_lat', 'clust', 'eff_lat', 'clust_lon', 'min_lat', 'count', 'std', '25p_lat', 'count_lon', 'max', 'eff_lon', 'eff', 'mean_lon', '75p', 'min', 'max_lon', '50p', 'mean_lat', 'mean']\nfeature_names =['dropoff_latitude', 'dropoff_longitude', 'd_x', 'd_y', 'd_block', 'count', 'count_PD', 'mean_PD', 'std_PD', 'max_PD']\nfeature_names =['dropoff_latitude', 'dropoff_longitude', 'p_y', 'd_x', 'd_y', 'd_block', 'count', 'count_PD', 'mean_PD', '25p_PD', '50p_PD', '75p_PD', 'std_PD', 'max_PD', 'eff_PD', 'srp_3', 'srp_5']\nprint(np.setdiff1d(train1.columns, test1.columns))\ndo_not_use_for_training = ['id', 'log_trip_duration', 'pickup_datetime', 'dropoff_datetime', 'trip_duration', 'check_trip_duration','pickup_date']\ny = train['log_speed']\nfeature_names=[k for k in feature_names if k not in do_not_use_for_training]\nXtr, Xv, ytr, yv = train_test_split(train1[feature_names].values, y, test_size=0.2, random_state=2017)\ndtrain = xgb.DMatrix(Xtr, label=ytr)\ndvalid = xgb.DMatrix(Xv, label=yv)\ndtest = xgb.DMatrix(test1[feature_names].values)\nwatchlist = [(dtrain, 'train'), (dvalid, 'valid')]\n\n# Try different parameters! My favorite is random search :)\nxgb_pars = {'min_child_weight': 100, 'eta': 0.5, 'colsample_bytree': 0.3, 'max_depth': 10,\n            'subsample': 0.8, 'lambda': 1., 'nthread': -1, 'booster' : 'gbtree', 'silent': 1,\n            'eval_metric': 'rmse', 'objective': 'reg:linear'}\n\n# You could try to train with more epoch\nmodel = xgb.train(xgb_pars, dtrain, 200, watchlist, early_stopping_rounds=50,\n                  maximize=False, verbose_eval=50)", "cell_type": "code", "execution_count": 1}, {"metadata": {"_uuid": "a64332e33dc784325e19e13bf3b81eebefae93d6", "trusted": false, "_cell_guid": "6014d60d-b54c-4003-a605-cfd654537530", "_execution_state": "idle"}, "outputs": [], "source": "print('Modeling RMSLE %.5f' % model.best_score)\n", "cell_type": "code", "execution_count": 28}, {"metadata": {"_uuid": "5298de4c5cf3e314a3fc7a227ff45df3a6ddf9ca", "trusted": false, "_cell_guid": "e12e3f77-4be7-4491-934b-6c87529692a8", "_execution_state": "idle"}, "outputs": [], "source": "feature_importance_dict = model.get_fscore()\nfs = ['f%i' % i for i in range(len(feature_names))]\nf1 = pd.DataFrame({'f': list(feature_importance_dict.keys()), 'importance': list(feature_importance_dict.values())})\nf2 = pd.DataFrame({'f': fs, 'feature_name': feature_names})\nfeature_importance = pd.merge(f1, f2, how='right', on='f')\nfeature_importance[['feature_name', 'importance']].sort_values(by='importance', ascending=False)", "cell_type": "code", "execution_count": 29}, {"metadata": {"_uuid": "a2df0c9a5805330b55a3e09f0b635ad32a03c06a", "_cell_guid": "c245608f-fb6f-4d22-944c-04513caaed27", "_execution_state": "idle"}, "outputs": [], "source": "#using the 'pickup-block' - 'dropoff-block' versus logspeed", "cell_type": "markdown", "execution_count": null}, {"metadata": {"_uuid": "6f7049a925f648a6f4b54ee6310ef9e49f3a2ed4", "trusted": false, "_cell_guid": "44519084-eea5-467d-a707-73d2b6570fd2", "_execution_state": "idle"}, "outputs": [], "source": "ypred = model.predict(dvalid)\nfig,ax = plt.subplots(ncols=2)\nax[0].scatter(ypred, yv, s=0.1, alpha=0.1)\nax[0].set_xlabel('log(prediction)')\nax[0].set_ylabel('log(ground truth)')\nax[1].scatter(np.exp(ypred), np.exp(yv), s=0.1, alpha=0.1)\nax[1].set_xlabel('prediction')\nax[1].set_ylabel('ground truth')\nplt.show()", "cell_type": "code", "execution_count": 30}, {"metadata": {"_uuid": "16825f8474320e91b94cf0dfc0b490886e8efc97", "trusted": false, "_cell_guid": "dcf76fcb-d259-4c57-a68b-523a826967de", "_execution_state": "idle"}, "outputs": [], "source": "ytest = model.predict(dtest)\nprint('Test shape OK.') if test.shape[0] == ytest.shape[0] else print('Oops')\ntest['trip_duration'] = np.exp(ytest) - 1\ntest[['id', 'trip_duration']].to_csv('d:\\input\\paul_xgb_submission.csv.gz', index=False, compression='gzip')\n", "cell_type": "code", "execution_count": 25}], "metadata": {"language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "codemirror_mode": {"version": 3, "name": "ipython"}, "file_extension": ".py", "name": "python", "pygments_lexer": "ipython3", "version": "3.6.1"}, "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}}, "nbformat": 4}