{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "b510a016-f951-6be4-296e-8c4ab0c1cf1f"
      },
      "source": [
        "# Sberbank Russian Housing Market - Exploratory Data Analysis 2\n",
        "\n",
        "You can see the first part of my analysis here: \n",
        "https://www.kaggle.com/rikhard/eda-01-missing-data-structure-and-regressors/notebook\n",
        "\n",
        "In this kernel, we will look at the macro data. At the beginning, we will follow the same structure that we used in our previous analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d35a9553-65c6-45be-b804-4bb407c42d08"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "70068742-dd13-f774-9132-8d54b68e73ee"
      },
      "outputs": [],
      "source": [
        "# Load train data\n",
        "df = pd.read_csv('../input/macro.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "1aa5b35c-c436-d6c7-7a81-4fdae4b45d1e"
      },
      "source": [
        "## Data description\n",
        "\n",
        "First, let's see the size and types of data we have:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "fec6f897-99d6-ee2c-e28b-052916a67efd"
      },
      "outputs": [],
      "source": [
        "print(df.columns.tolist())\n",
        "print('\\nNumber of columns on train data:',len(df.columns))\n",
        "print('\\nNumber of data points:',len(df))\n",
        "print('\\nNumber of unique timestamp data points:',len(df['timestamp'].unique()))\n",
        "print('\\nData types:',df.dtypes.unique())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "d305757b-d64a-f239-58c0-a774b3258302"
      },
      "source": [
        "It seems we have non numerical data here. Let's take a look at it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "68712cc9-dd60-d7e7-f8a3-208f6640f70b"
      },
      "outputs": [],
      "source": [
        "df.select_dtypes(include=['O']).columns.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d08d3063-a784-bee7-7ddc-d61bcbcdf46b"
      },
      "outputs": [],
      "source": [
        "df.select_dtypes(include=['O']).dropna().head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "fba1cc44-c67b-2f59-8b72-00a214618db0"
      },
      "source": [
        "### Some data needs to be processed\n",
        "\n",
        "These columns should also be numeric, so there must be some bad data in them. If we try to use the pandas method 'pd.to_numeric()', we will find that it can't parse the strings in this columns and if we try to force the conversion, it will return all NaNs in the three columns. The problem is that those numbers have the symbol \",\" instead of \".\" so the parser doesn't recognize them as numbers. The easiest way to overcome this problem is to replace the commas per dots and then use the pandas method to change the column dtype:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "30d162d9-0bba-a872-e05c-e479ea093436"
      },
      "outputs": [],
      "source": [
        "object_cols = df.select_dtypes(include=['O']).columns.tolist()[1:]\n",
        "for col in object_cols:\n",
        "    df[col] = df[col].apply(lambda s: str(s).replace(',','.'))\n",
        "df[object_cols] = df[object_cols].apply(pd.to_numeric, errors='coerce')\n",
        "df[object_cols].dropna().head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "8f158956-daa6-7003-73ba-bc9c6e1d9944"
      },
      "outputs": [],
      "source": [
        "df.select_dtypes(include=['O']).columns.tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "a59a99fa-cd44-f53b-71de-f16930b971e5"
      },
      "source": [
        "Problem solved. Now let's see how many NaNs are there in the macro data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "e18a6e57-f727-d974-a151-31288e562e59"
      },
      "outputs": [],
      "source": [
        "# Get the number of NaN's for each column, discarding those with zero NaN's\n",
        "ranking = df.loc[:,df.isnull().any()].isnull().sum().sort_values()\n",
        "# Turn into %\n",
        "x = ranking.values/len(df)\n",
        "\n",
        "# Plot bar chart\n",
        "index = np.arange(len(ranking))\n",
        "plt.bar(index, x)\n",
        "plt.xlabel('Features')\n",
        "plt.ylabel('% NaN observations')\n",
        "plt.title('% of null data points for each feature')\n",
        "plt.show()\n",
        "\n",
        "print('Features:',ranking.index.tolist())\n",
        "print('\\nNumber of columns which have any NaN:',df.isnull().any().sum(),'/',len(df.columns))\n",
        "print('\\nNumber of rows which have any NaN:',df.isnull().any(axis=1).sum(),'/',len(df))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "dbe97696-2631-8d09-7614-b7a5e2bb8f86"
      },
      "source": [
        "There is a lot of NaNs in the macro data. But that is not the only problem:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d6876b92-79b1-4a26-7cfb-cd7d4285c8aa"
      },
      "outputs": [],
      "source": [
        "df.iloc[1000:1010,:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "c7cd7846-da2c-b882-4abc-d018a2c910f0"
      },
      "source": [
        "## Separating macro data depending on its period\n",
        "\n",
        "We have mixed periods of data: annual, quarterly, monthly, daily ... And the main problem is not NaNs, but duplicate data. We will have to write an algorythm to separate the data depending on its period. We will begin creating new time features:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "83c09ae4-72cc-4a20-2f92-04998e40563b"
      },
      "outputs": [],
      "source": [
        "df['year'] = df['timestamp'].apply(lambda f: f.split('-')[0])\n",
        "df['month'] = df['timestamp'].apply(lambda f: f.split('-')[1])\n",
        "df['day'] = df['timestamp'].apply(lambda f: f.split('-')[2])\n",
        "df['quarter'] = np.floor((df['month'].values.astype('int')-1)/3).astype('int')+1\n",
        "df['year-quarter'] = df['year'] +'-Q' + df['quarter'].astype('str')\n",
        "df['year-month'] = df['year'] +'-'+ df['month']\n",
        "del df['quarter']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "06caa611-fcc7-00de-d564-637c5081f60c"
      },
      "source": [
        "Next we create a function that calculates the period of a data series:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "42f3e4c2-25b6-5607-e7d4-3c34034daf5e"
      },
      "outputs": [],
      "source": [
        "def get_periods(df):\n",
        "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
        "    var = df.columns[1]\n",
        "    df['shift'] = df[var].shift()\n",
        "    df = df.dropna()\n",
        "    index = df[df[var]-df['shift']!=0].index\n",
        "    if len(df.loc[:,var].unique())>3:\n",
        "        return df.loc[index,'timestamp'].diff().mean().round('D')\n",
        "    else: return -1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "63533a7b-bdaf-240e-1737-70eca93296b5"
      },
      "outputs": [],
      "source": [
        "periods = list()\n",
        "for i,col in enumerate(df.columns[1:-5]):\n",
        "    periods.append(get_periods(df[['timestamp',col]].copy()))\n",
        "    print(i,col,periods[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "00691285-8cad-f3ef-2d5a-d2e97dbfefb5"
      },
      "outputs": [],
      "source": [
        "set(periods)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "bb8e3c1d-0d10-9804-55d9-657f70e69ea4"
      },
      "source": [
        "Yet we have to write another algorythm to purify that set and get the final time categories for macro data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "5f97c75e-8c8a-765e-ffc3-e3229d79b733"
      },
      "outputs": [],
      "source": [
        "final_periods = dict()\n",
        "for col, period in zip(df.columns[1:-5], periods):\n",
        "    if period == -1: continue\n",
        "    elif period <= pd.Timedelta('2 days'): final_periods[col] = 1\n",
        "    elif period <= pd.Timedelta('35 days'): final_periods[col] = 30\n",
        "    elif period <= pd.Timedelta('100 days'): final_periods[col] = 90\n",
        "    elif period <= pd.Timedelta('370 days'): final_periods[col] = 365\n",
        "    \n",
        "print('Number of columns of interest:',len(final_periods))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "c0fa01aa-c690-17f7-c76f-3f8bde79f66e"
      },
      "source": [
        "## Plotting the macro data\n",
        "\n",
        "Now that we have preprocessed the data, we can plot all the macro features:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "e7093b49-428d-5805-f0d3-4410fc062ffd"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "for key, value in final_periods.items():\n",
        "    resume = pd.DataFrame(columns=[key,'mean'])\n",
        "    if value == 1:\n",
        "        resume[key] = df[key]\n",
        "        resume['mean'] = resume[key].dropna().rolling(30,center=True).mean()\n",
        "    elif value == 30:\n",
        "        resume[key] = df.groupby('year-month').agg('mean')[key]\n",
        "        resume['mean'] = resume[key].dropna().rolling(12,center=True).mean()\n",
        "    elif value == 90:\n",
        "        resume[key] = df.groupby('year-quarter').agg('mean')[key]\n",
        "        resume['mean'] = resume[key].dropna().rolling(4,center=True).mean()\n",
        "    elif value == 365:\n",
        "        resume[key] = df.groupby('year').agg('mean')[key]\n",
        "    resume.plot()\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "918a966c-5bd3-f03e-d0da-5820abb7e014"
      },
      "source": [
        "\n",
        "This is all for the moment. If you liked this analysis, please upvote! Thank's!"
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}