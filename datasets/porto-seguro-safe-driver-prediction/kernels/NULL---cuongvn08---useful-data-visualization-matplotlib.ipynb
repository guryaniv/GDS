{"cells": [{"outputs": [], "metadata": {"_cell_guid": "74c43420-a733-43d8-80c1-a178e12fb0c8", "_uuid": "1f70666ea8d1f4149e585bcc2ad7eb85ba4f6529", "collapsed": true}, "cell_type": "code", "source": ["import numpy as np\n", "import pandas as pd\n", "from collections import Counter\n", "import matplotlib.pyplot as plt\n", "import seaborn as sns\n", "import gc\n", "import xgboost as xgb"], "execution_count": 2}, {"outputs": [], "metadata": {"_cell_guid": "e9ea19a5-3c5f-43ac-8739-0a3b90536eb6", "_uuid": "b56b0d5cad317feafa21f92ea70929231cd4d538"}, "cell_type": "code", "source": ["def visualize():\n", "    # config\n", "    train_path = '../input/train.csv'\n", "\n", "    # load data\n", "    print('\\nloading data ... ')\n", "    train_df = pd.read_csv(train_path)\n", "    train_df.drop(['id'], axis = 1, inplace = True)\n", "    print('train shape: ', train_df.shape)\n", "    print('feature types: ', Counter(train_df.dtypes.values))\n", "    print('features: ')\n", "    for feature in train_df:\n", "        print(' ', feature)\n", "\n", "    # check NA ratio\n", "    # replace -1 by NA because -1 in data indicate that the feature was missing \n", "    print('\\nchecking NA ratio ... ')\n", "    train_df_copied = train_df\n", "    train_df_copied = train_df_copied.replace(-1, np.NaN) # Values of -1 indicate that the feature was missing from the observation\"\n", "\n", "    na_ratio = (train_df_copied.isnull().sum() / len(train_df_copied)).sort_values(ascending=False)\n", "    print('NA ratio: ')\n", "    print(na_ratio)\n", "\n", "    del train_df_copied\n", "    gc.collect()\n", "    \n", "    # show the target feature\n", "    print('\\nshowing the target feature ... ')\n", "    zero_count = (train_df['target']==0).sum()\n", "    one_count = (train_df['target']==1).sum()\n", "    plt.bar(np.arange(2), [zero_count, one_count])\n", "    plt.show()\n", "    \n", "    print('target 0: ', zero_count)\n", "    print('target 1: ', one_count)\n", "    \n", "    # show feature's distribution\n", "    print('\\ndislaying distribution of features ... ')\n", "    for feature in train_df:\n", "        plt.figure(figsize=(8,6))\n", "        plt.scatter(range(train_df.shape[0]), np.sort(train_df[feature].values))\n", "        plt.xlabel('index', fontsize=12)\n", "        plt.ylabel(feature, fontsize=12)\n", "        plt.show() \n", "        \n", "    # compute features's correlation\n", "    print('\\ncomputing correlation of the features and showing the most positive and negative correlated features ... ')\n", "    f, ax = plt.subplots(figsize = (15, 15))\n", "    plt.title('correlation of continuous features')\n", "    sns.heatmap(train_df.corr(), ax = ax)\n", "    plt.show()\n", "    \n", "    corr_values = train_df.corr().unstack().sort_values(ascending=False)\n", "    print(type(corr_values))\n", "    for pair, value in corr_values.iteritems():\n", "        if abs(value) > 0.3 and abs(value) < 1.0:\n", "            print(pair, value)\n", "    \n", "    # binary features inpection\n", "    print('\\n inspecting binary features ... ')\n", "    bin_cols = [col for col in train_df.columns if '_bin' in col]\n", "    zero_list = []\n", "    one_list = []\n", "    for col in bin_cols:\n", "        zero_list.append((train_df[col]==0).sum())\n", "        one_list.append((train_df[col]==1).sum())\n", "        \n", "    plt.figure(figsize = (10, 10))\n", "    p1 = plt.bar(np.arange(len(bin_cols)), zero_list, width = 0.5)\n", "    p2 = plt.bar(np.arange(len(bin_cols)), one_list, bottom = zero_list, width = 0.5)\n", "    plt.xticks(np.arange(len(bin_cols)), bin_cols, rotation = 90)\n", "    plt.legend((p1[0], p2[0]), ('zero count', 'one count'))\n", "    plt.show()\n", "    \n", "    # compute feature importance\n", "    print('\\n computing feature importance ... ')\n", "    xgb_params = {\n", "        'eta': 0.05,\n", "        'max_depth': 8,\n", "        'subsample': 0.7,\n", "        'colsample_bytree': 0.7,\n", "        'objective': 'reg:linear',\n", "        'silent': 1,\n", "        'seed' : 0\n", "    }\n", "    \n", "    train_y = train_df['target'].values\n", "    train_x = train_df.drop(['target'], axis=1)\n", "\n", "    d_train = xgb.DMatrix(train_x, train_y, feature_names=train_x.columns.values)\n", "    model = xgb.train(dict(xgb_params, silent=0), d_train, num_boost_round = 100)\n", "    \n", "    importance = model.get_fscore()\n", "    features_df = pd.DataFrame()\n", "    features_df['feature'] = importance.keys()\n", "    features_df['fscore'] = importance.values()\n", "    features_df['fscore'] = features_df['fscore'] / features_df['fscore'].sum()\n", "    features_df.sort_values(by = ['fscore'], ascending = True, inplace = True)\n", "    \n", "    plt.figure()\n", "    features_df.plot(kind = 'barh', x = 'feature', y='fscore', legend = False, figsize = (10, 10))\n", "    plt.title('XGBoost Feature Importance')\n", "    plt.xlabel('fscore')\n", "    plt.ylabel('features')\n", "    plt.show()\n", "\n", "    print(features_df)\n", "    \n", "    # release\n", "    del train_df\n", "    gc.collect()\n", "    \n", "if __name__ == \"__main__\":\n", "    visualize()\n", "    print('\\n\\n\\nThe end.')"], "execution_count": 3}], "nbformat_minor": 1, "nbformat": 4, "metadata": {"kernelspec": {"language": "python", "display_name": "Python 3", "name": "python3"}, "language_info": {"codemirror_mode": {"version": 3, "name": "ipython"}, "nbconvert_exporter": "python", "version": "3.6.3", "file_extension": ".py", "pygments_lexer": "ipython3", "mimetype": "text/x-python", "name": "python"}}}