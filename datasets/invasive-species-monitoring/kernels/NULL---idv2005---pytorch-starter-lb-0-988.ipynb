{"nbformat": 4, "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3", "language": "python"}, "language_info": {"version": "3.6.1", "nbconvert_exporter": "python", "file_extension": ".py", "pygments_lexer": "ipython3", "name": "python", "codemirror_mode": {"name": "ipython", "version": 3}, "mimetype": "text/x-python"}}, "cells": [{"execution_count": null, "source": "This is my first notebook, it references pytorch example: [Pytorch transfer learning tutorial][1]\nIt may not exactly give you 0.988 due to random seed, but should be very close.\nIt won't be able to run on kaggle since time limitation. Full code can be downloaded from here: [code on github][2]\n\n\n  [1]: http://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html#\n  [2]: https://github.com/chicm/kaggle-invasive-species/blob/master/train.py", "cell_type": "markdown", "metadata": {"collapsed": false, "_execution_state": "idle", "_cell_guid": "7980c218-1696-4eaa-afb1-8e596c5de530", "_uuid": "ff092f8120b63b96a926513fa9fd97e8a7bb88ec"}, "outputs": []}, {"execution_count": null, "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.", "cell_type": "code", "metadata": {"trusted": false, "_execution_state": "idle", "_cell_guid": "e42c3c1e-7e0b-47be-a32d-ed647b4cf39c", "_uuid": "95d2fbe4ffe277edbc374f5c59919713faf14a15"}, "outputs": []}, {"execution_count": null, "source": "from PIL import Image\nimport torch\nimport torch.utils.data as data\nfrom torchvision import datasets, models, transforms\nfrom torch.autograd import Variable", "cell_type": "code", "metadata": {"collapsed": false, "_execution_state": "idle", "_cell_guid": "9af05fc2-d58f-4e88-8c41-b27b432dd95d", "trusted": false, "_uuid": "0b5e421714409e18351df62baecbe299691058b0"}, "outputs": []}, {"execution_count": null, "source": "data_dir = '../input'", "cell_type": "code", "metadata": {"collapsed": false, "_execution_state": "idle", "_cell_guid": "3c6552b1-8ba5-4200-904c-5f508f2bf92f", "trusted": false, "_uuid": "fa8fca838e24099b9294abf23e5dd4da5eac6274"}, "outputs": []}, {"execution_count": null, "outputs": [], "cell_type": "markdown", "metadata": {"collapsed": false, "_execution_state": "idle", "_cell_guid": "1517177b-7e6c-4b10-80b6-4740b8202fd6", "_uuid": "63a3f261339767e0e043699776f7646835fcd191"}, "source": "Define a custom dataset to read image files."}, {"execution_count": null, "source": "# define custom dataset\ndef load_img(filename):\n    img_path = data_dir + '/train/' + str(filename) + '.jpg'\n    with open(img_path, 'rb') as f:\n        with Image.open(f) as img_f:\n            return img_f.convert('RGB').resize((320,320))\n# define custom dataset\nclass MyDataSet(data.Dataset):\n    def __init__(self, filename, training=True, validating=False, train_percent=0.85, transforms=None):\n        df = pd.read_csv(filename)\n        if training:\n            split_index = (int)(df.values.shape[0]*train_percent)\n            if validating:\n                split_data = df.values[split_index:]\n            else:\n                split_data = df.values[:split_index]\n            imgs = [None]*split_data.shape[0]\n            labels = [None]*split_data.shape[0]\n            for i, row in enumerate(split_data):\n                fn, labels[i] = row\n                imgs[i] = load_img(fn)\n        else:\n            imgs = [None]*df.values.shape[0]\n            for i, row in enumerate(df.values):\n                fn, _ = row\n                imgs[i] = load_img(fn)\n        self.imgs = imgs\n        self.training = training\n        self.transforms = transforms\n        self.num = len(imgs)\n        if self.training:\n            self.labels = np.array(labels, dtype=np.float32)\n    def __getitem__(self, index):\n        if not self.transforms is None:\n            img = self.transforms(self.imgs[index])\n        if self.training:\n            return img, self.labels[index]\n        else:\n            return img\n    def __len__(self):\n        return self.num", "cell_type": "code", "metadata": {"collapsed": false, "_execution_state": "idle", "_cell_guid": "d36576b6-ef76-475c-a456-c4d9529528ab", "trusted": false, "_uuid": "bff9916d03abd18988f3f2ca4ac08738a9e052f7"}, "outputs": []}, {"execution_count": null, "source": "# define data augumentations\nimport random\ndef randomRotate(img):\n    angel = random.randint(0,4) * 90\n    return img.rotate(angel)\n \ndata_transforms = {\n    'train': transforms.Compose([\n        transforms.RandomSizedCrop(224),\n        transforms.RandomHorizontalFlip(),\n        transforms.Lambda(lambda x: randomRotate(x)),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n    'test': transforms.Compose([\n        transforms.Scale(224),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ])\n}", "cell_type": "code", "metadata": {"collapsed": false, "_execution_state": "idle", "_cell_guid": "8fb1f4b7-7b7b-49a1-a87a-99ec42c75d01", "trusted": false, "_uuid": "24763bad9e63e0a0e7466ec07595d82d30a1377f"}, "outputs": []}, {"execution_count": null, "source": "def get_data_loader(filename=data_dir+'/train_labels.csv', training=True, validating=False, shuffle=True):\n    if training and not validating:\n        transkey = 'train'\n    else:\n        transkey = 'test'\n    dset = MyDataSet(filename, training=training, validating=validating, transforms=data_transforms[transkey])\n    loader = torch.utils.data.DataLoader(dset, batch_size=8, shuffle=shuffle, num_workers=4)\n    loader.num = dset.num\n    return loader", "cell_type": "code", "metadata": {"collapsed": false, "_execution_state": "idle", "_cell_guid": "5f9e53b4-7bfd-4971-972a-10c3b66449b7", "trusted": false, "_uuid": "a22213606a97728fe30825cfed8e28f538a92302"}, "outputs": []}, {"execution_count": null, "source": "def lr_scheduler(optimizer, epoch, init_lr=0.001, lr_decay_epoch=8):\n    lr = 0\n    for param_group in optimizer.param_groups:\n        lr = param_group['lr']\n    if epoch % lr_decay_epoch == 0 and epoch >= lr_decay_epoch:\n        lr = lr * 0.6\n    if epoch % lr_decay_epoch == 0 and epoch >= lr_decay_epoch:\n        print('LR is set to {}'.format(lr))\n        for param_group in optimizer.param_groups:\n            param_group['lr'] = lr\n    return optimizer    ", "cell_type": "code", "metadata": {"collapsed": false, "_execution_state": "idle", "_cell_guid": "f1320121-db98-45f8-b467-26ec3dfa3b0d", "trusted": false, "_uuid": "3194ae67c00f08ae04653e22203d27ce06d14ca0"}, "outputs": []}, {"execution_count": null, "source": "weight_file = 'best_model.pth'\n\ndef do_train(net, criterion, optimizer, lr_scheduler, epochs=100):\n    data_loaders = {'train': get_data_loader(), 'valid': get_data_loader(validating=True)}\n    best_model = net\n    best_acc = 0\n    for epoch in range(epochs):\n        print('Epoch {}/{}'.format(epoch, epochs))\n        for phase in ['train', 'valid']:\n            if phase == 'train':\n                optimizer = lr_scheduler(optimizer, epoch)\n                net.train(True)\n            else:\n                net.train(False)\n            running_loss = 0.\n            running_corrects = 0\n            for imgs, labels in data_loaders[phase]:\n                imgs, labels = Variable(imgs.cuda()), Variable(labels.cuda())\n                optimizer.zero_grad()\n                outputs = net(imgs)\n                preds = torch.ge(outputs.data, 0.5).resize_(labels.data.size())\n                loss = criterion(outputs, labels)\n                if phase == 'train':\n                    loss.backward()\n                    optimizer.step()\n                running_loss += loss.data[0]\n                running_corrects += torch.sum(preds.int() == labels.data.int())\n            epoch_loss = running_loss / data_loaders[phase].num\n            epoch_acc = running_corrects / data_loaders[phase].num\n            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n            if phase == 'valid' and epoch_acc > best_acc:\n                best_acc = epoch_acc\n                torch.save(net.state_dict(), weight_file)\n                best_model = net\n    print('Best validation accuracy: {:4f}'.format(best_acc))\n    return best_model\n                ", "cell_type": "code", "metadata": {"collapsed": false, "_execution_state": "idle", "_cell_guid": "47a1c9bc-f5e6-442d-9ac0-d949e09a8aca", "trusted": false, "_uuid": "7b44f8ab352d30342759efba14a350167f70b6c2"}, "outputs": []}, {"execution_count": null, "source": "import torch.nn as nn\ndef get_dense201():\n    net = models.densenet201(pretrained=True)\n    net.classifier = nn.Sequential(nn.Linear(net.classifier.in_features, 1), nn.Sigmoid())\n    return net.cuda()\n", "cell_type": "code", "metadata": {"collapsed": false, "_execution_state": "idle", "_cell_guid": "de8fdce3-158d-4817-81b6-88b003d6435e", "trusted": false, "_uuid": "ee648e1fc96e669afca1942d6ea32eb35465d36a"}, "outputs": []}, {"execution_count": null, "source": "def train_net():\n    net = get_dense201()\n    criterion = nn.BCELoss()\n    optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n    do_train(net, criterion, optimizer, lr_scheduler)", "cell_type": "code", "metadata": {"collapsed": false, "_execution_state": "idle", "_cell_guid": "c251560c-f78d-4dac-bc21-2a140ad37060", "trusted": false, "_uuid": "cb46bd2ee2d696ba8d0c20fa5baec066e77c8b97"}, "outputs": []}, {"execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"collapsed": false, "_execution_state": "idle", "_cell_guid": "51b6e1ca-006d-45af-8d36-a42569e5d26a", "trusted": false, "_uuid": "2e5d0bb3e21235fe2022eb7140ee5c8460128fab"}, "source": "def predict(net):\n    loader = get_data_loader(filename=data_dir+'/sample_submission.csv', training=False, shuffle=False)\n    preds = []\n    net.eval()\n    for i, img in enumerate(loader, 0):\n        inputs = Variable(img.cuda())\n        outputs = net(inputs)\n        pred = outputs.data.cpu().tolist()\n        for p in pred:\n            preds.append(p)\n    return np.array(preds)"}, {"execution_count": null, "source": "def submit(preds, filename):\n    df = pd.read_csv(data_dir + '/sample_submission.csv')\n    df['invasive'] = preds\n    print(df.head())\n    df.to_csv(filename, index=False)", "cell_type": "code", "metadata": {"trusted": false, "_execution_state": "idle", "_cell_guid": "8490ec95-aa01-4631-a88f-143e890e636d", "_uuid": "4048cf1241a8c68c3facfefc45e3ab0ca7166833", "collapsed": false}, "outputs": []}, {"execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"collapsed": false, "_execution_state": "idle", "_cell_guid": "76dd593f-c41d-4043-94fe-3357a0c4759d", "trusted": false, "_uuid": "5709d99121c2e21a7adc6ce2d151fd8844e887a5"}, "source": "if True:\n    train_net()\n\nif True:\n    net = get_dense201()\n    net.load_state_dict(torch.load(weight_file))\n    preds = predict(net)\n    submit(preds, 'submission1.csv')"}, {"execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"collapsed": false, "_execution_state": "idle", "_cell_guid": "0501e9ef-07d4-4ab6-a404-b2356cb7df8d", "trusted": false, "_uuid": "ef92854aad5d0cf3e3ccb106b0bde06e4f5767e0"}, "source": ""}], "nbformat_minor": 0}