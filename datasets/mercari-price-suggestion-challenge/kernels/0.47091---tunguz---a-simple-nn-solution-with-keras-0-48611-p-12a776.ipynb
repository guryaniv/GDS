{"metadata": {"kernelspec": {"display_name": "Python 3", "name": "python3", "language": "python"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "version": "3.6.3", "nbconvert_exporter": "python", "name": "python", "mimetype": "text/x-python", "pygments_lexer": "ipython3"}}, "cells": [{"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "e4561cb64722ffd7adb74a8800f89973bc3fe87a", "collapsed": true, "_cell_guid": "f49a0842-1c3d-4bee-8b3e-3caf6111bb60"}, "source": ["import numpy as np\n", "import pandas as pd\n", "\n", "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler\n", "from sklearn.cross_validation import train_test_split\n", "\n", "import matplotlib.pyplot as plt\n", "%matplotlib inline \n", "\n", "import math\n", "\n", "\n", "from subprocess import check_output\n", "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))"], "outputs": []}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "56874f4a359241631a1f1e4788da09e7b09182b8", "collapsed": true, "_cell_guid": "17b85633-4dae-4a49-9573-5d5c3b1b9699"}, "source": ["def rmsle(y, y_pred):\n", "    assert len(y) == len(y_pred)\n", "    to_sum = [(math.log(y_pred[i] + 1) - math.log(y[i] + 1)) ** 2.0 for i,pred in enumerate(y_pred)]\n", "    return (sum(to_sum) * (1.0/len(y))) ** 0.5\n", "#Source: https://www.kaggle.com/marknagelberg/rmsle-function"], "outputs": []}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "80cd3abaf2d32b2c8133719d9a7bbb977adf91cb", "collapsed": true, "_cell_guid": "a76efe2c-34bb-458a-8611-1dcabda65b31"}, "source": ["#LOAD DATA\n", "print(\"Loading data...\")\n", "train = pd.read_table(\"../input/train.tsv\")\n", "test = pd.read_table(\"../input/test.tsv\")\n", "print(train.shape)\n", "print(test.shape)"], "outputs": []}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "967a5f2b26862be11cdae8cb0fb7402fd486135c", "collapsed": true, "_cell_guid": "4e506bee-5541-4879-aaea-159dee4416b8"}, "source": ["#HANDLE MISSING VALUES\n", "print(\"Handling missing values...\")\n", "def handle_missing(dataset):\n", "    dataset.category_name.fillna(value=\"missing\", inplace=True)\n", "    dataset.brand_name.fillna(value=\"missing\", inplace=True)\n", "    dataset.item_description.fillna(value=\"missing\", inplace=True)\n", "    return (dataset)\n", "\n", "train = handle_missing(train)\n", "test = handle_missing(test)\n", "print(train.shape)\n", "print(test.shape)"], "outputs": []}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "1aa305c5227f3a4d6b5a5bdf6aa55fe55cf46548", "collapsed": true, "_cell_guid": "39007c90-161e-4ef0-b0a5-fc16f04665b4"}, "source": ["train.head(3)"], "outputs": []}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "cf9985161bcd94612b1359177e23124efd3410cb", "collapsed": true, "_cell_guid": "204de9ca-7ea5-481b-9039-743bbab1084f"}, "source": ["#PROCESS CATEGORICAL DATA\n", "print(\"Handling categorical variables...\")\n", "le = LabelEncoder()\n", "\n", "le.fit(np.hstack([train.category_name, test.category_name]))\n", "train.category_name = le.transform(train.category_name)\n", "test.category_name = le.transform(test.category_name)\n", "\n", "le.fit(np.hstack([train.brand_name, test.brand_name]))\n", "train.brand_name = le.transform(train.brand_name)\n", "test.brand_name = le.transform(test.brand_name)\n", "del le\n", "\n", "train.head(3)\n"], "outputs": []}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "ff62bf6d3681b84510026bfa3514963513f2e8f2", "collapsed": true, "_cell_guid": "b122ac7a-c203-463f-aa92-b843b039fce3"}, "source": ["#PROCESS TEXT: RAW\n", "print(\"Text to seq process...\")\n", "from keras.preprocessing.text import Tokenizer\n", "raw_text = np.hstack([train.item_description.str.lower(), train.name.str.lower()])\n", "\n", "print(\"   Fitting tokenizer...\")\n", "tok_raw = Tokenizer()\n", "tok_raw.fit_on_texts(raw_text)\n", "print(\"   Transforming text to seq...\")\n", "\n", "train[\"seq_item_description\"] = tok_raw.texts_to_sequences(train.item_description.str.lower())\n", "test[\"seq_item_description\"] = tok_raw.texts_to_sequences(test.item_description.str.lower())\n", "train[\"seq_name\"] = tok_raw.texts_to_sequences(train.name.str.lower())\n", "test[\"seq_name\"] = tok_raw.texts_to_sequences(test.name.str.lower())\n", "train.head(3)"], "outputs": []}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "335a3d95444a2602598aebbfa6de0a8a3f5a1814", "collapsed": true, "_cell_guid": "9da5165e-0692-4c1a-a5e7-3fb1c6c1a4fd"}, "source": ["#SEQUENCES VARIABLES ANALYSIS\n", "max_name_seq = np.max([np.max(train.seq_name.apply(lambda x: len(x))), np.max(test.seq_name.apply(lambda x: len(x)))])\n", "max_seq_item_description = np.max([np.max(train.seq_item_description.apply(lambda x: len(x)))\n", "                                   , np.max(test.seq_item_description.apply(lambda x: len(x)))])\n", "print(\"max name seq \"+str(max_name_seq))\n", "print(\"max item desc seq \"+str(max_seq_item_description))"], "outputs": []}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "32ffa5a96deaeb6214469a2594475f7b5a15bc92", "collapsed": true, "_cell_guid": "3e87bf13-093e-42a3-bef9-ed10322944df"}, "source": ["train.seq_name.apply(lambda x: len(x)).hist()"], "outputs": []}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "bdfde374cbe6f2edc9c453f91ef90c757c8d8f06", "collapsed": true, "_cell_guid": "09349ced-fcac-4406-8876-a00fc269218d"}, "source": ["train.seq_item_description.apply(lambda x: len(x)).hist()"], "outputs": []}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "4cae7d35e8b44e04df93c9f8084ec67ef40a1828", "collapsed": true, "_cell_guid": "e1b9db3a-63de-4bd9-9bad-2db899176053"}, "source": ["#EMBEDDINGS MAX VALUE\n", "#Base on the histograms, we select the next lengths\n", "MAX_NAME_SEQ = 10\n", "MAX_ITEM_DESC_SEQ = 75\n", "MAX_TEXT = np.max([np.max(train.seq_name.max())\n", "                   , np.max(test.seq_name.max())\n", "                  , np.max(train.seq_item_description.max())\n", "                  , np.max(test.seq_item_description.max())])+2\n", "MAX_CATEGORY = np.max([train.category_name.max(), test.category_name.max()])+1\n", "MAX_BRAND = np.max([train.brand_name.max(), test.brand_name.max()])+1\n", "MAX_CONDITION = np.max([train.item_condition_id.max(), test.item_condition_id.max()])+1"], "outputs": []}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "53474065f08e5d469e4a65bd272c0b316ae26b1e", "collapsed": true, "_cell_guid": "cb88f6b3-44ab-4ab4-831a-6a3ebdd0e10d"}, "source": ["#SCALE target variable\n", "train[\"target\"] = np.log(train.price+1)\n", "target_scaler = MinMaxScaler(feature_range=(-1, 1))\n", "train[\"target\"] = target_scaler.fit_transform(train.target.reshape(-1,1))\n", "pd.DataFrame(train.target).hist()"], "outputs": []}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "38f7f125582a88eba6410d93306df119e454db82", "collapsed": true, "_cell_guid": "abf9cd11-efa2-4a2c-b844-cdf2fc23bbd8"}, "source": ["#EXTRACT DEVELOPTMENT TEST\n", "dtrain, dvalid = train_test_split(train, random_state=123, train_size=0.99)\n", "print(dtrain.shape)\n", "print(dvalid.shape)"], "outputs": []}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "2f876f24ae0e5a82bdf9b0568f796cce5e2fed2c", "collapsed": true, "_cell_guid": "22f34be1-bd4a-4ad1-a931-62873816e8ee"}, "source": ["#KERAS DATA DEFINITION\n", "from keras.preprocessing.sequence import pad_sequences\n", "\n", "def get_keras_data(dataset):\n", "    X = {\n", "        'name': pad_sequences(dataset.seq_name, maxlen=MAX_NAME_SEQ)\n", "        ,'item_desc': pad_sequences(dataset.seq_item_description, maxlen=MAX_ITEM_DESC_SEQ)\n", "        ,'brand_name': np.array(dataset.brand_name)\n", "        ,'category_name': np.array(dataset.category_name)\n", "        ,'item_condition': np.array(dataset.item_condition_id)\n", "        ,'num_vars': np.array(dataset[[\"shipping\"]])\n", "    }\n", "    return X\n", "\n", "X_train = get_keras_data(dtrain)\n", "X_valid = get_keras_data(dvalid)\n", "X_test = get_keras_data(test)"], "outputs": []}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "8af261410908e6c713ca7de01c3c2c549debf779", "collapsed": true, "_cell_guid": "79428c10-e754-4e0e-a8be-ecbe74ef55ff"}, "source": ["#KERAS MODEL DEFINITION\n", "from keras.layers import Input, Dropout, Dense, BatchNormalization, Activation, concatenate, GRU, Embedding, Flatten, BatchNormalization\n", "from keras.models import Model\n", "from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping\n", "from keras import backend as K\n", "\n", "def get_callbacks(filepath, patience=2):\n", "    es = EarlyStopping('val_loss', patience=patience, mode=\"min\")\n", "    msave = ModelCheckpoint(filepath, save_best_only=True)\n", "    return [es, msave]\n", "\n", "def rmsle_cust(y_true, y_pred):\n", "    first_log = K.log(K.clip(y_pred, K.epsilon(), None) + 1.)\n", "    second_log = K.log(K.clip(y_true, K.epsilon(), None) + 1.)\n", "    return K.sqrt(K.mean(K.square(first_log - second_log), axis=-1))\n", "\n", "def get_model():\n", "    #params\n", "    dr_r = 0.15\n", "    \n", "    #Inputs\n", "    name = Input(shape=[X_train[\"name\"].shape[1]], name=\"name\")\n", "    item_desc = Input(shape=[X_train[\"item_desc\"].shape[1]], name=\"item_desc\")\n", "    brand_name = Input(shape=[1], name=\"brand_name\")\n", "    category_name = Input(shape=[1], name=\"category_name\")\n", "    item_condition = Input(shape=[1], name=\"item_condition\")\n", "    num_vars = Input(shape=[X_train[\"num_vars\"].shape[1]], name=\"num_vars\")\n", "    \n", "    #Embeddings layers\n", "    emb_name = Embedding(MAX_TEXT, 50)(name)\n", "    emb_item_desc = Embedding(MAX_TEXT, 50)(item_desc)\n", "    emb_brand_name = Embedding(MAX_BRAND, 10)(brand_name)\n", "    emb_category_name = Embedding(MAX_CATEGORY, 10)(category_name)\n", "    emb_item_condition = Embedding(MAX_CONDITION, 5)(item_condition)\n", "    \n", "    #rnn layer\n", "    rnn_layer1 = GRU(16) (emb_item_desc)\n", "    rnn_layer2 = GRU(8) (emb_name)\n", "    \n", "    #main layer\n", "    main_l = concatenate([\n", "        Flatten() (emb_brand_name)\n", "        , Flatten() (emb_category_name)\n", "        , Flatten() (emb_item_condition)\n", "        , rnn_layer1\n", "        , rnn_layer2\n", "        , num_vars\n", "    ])\n", "    main_l = Dropout(dr_r) (Dense(128) (main_l))\n", "    main_l = Dropout(dr_r) (Dense(64) (main_l))\n", "    \n", "    #output\n", "    output = Dense(1, activation=\"linear\") (main_l)\n", "    \n", "    #model\n", "    model = Model([name, item_desc, brand_name\n", "                   , category_name, item_condition, num_vars], output)\n", "    model.compile(loss=\"mse\", optimizer=\"adam\", metrics=[\"mae\", rmsle_cust])\n", "    \n", "    return model\n", "\n", "    \n", "model = get_model()\n", "model.summary()\n", "    \n", "5"], "outputs": []}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "5fa44a0c7fcca9b0f0c15719be8ac38a2ee7cea8", "collapsed": true, "_cell_guid": "b19fde2e-e454-48fd-b336-de0b15051fc8"}, "source": ["#FITTING THE MODEL\n", "BATCH_SIZE = 10000\n", "epochs = 3\n", "\n", "model = get_model()\n", "model.fit(X_train, dtrain.target, epochs=epochs, batch_size=BATCH_SIZE\n", "          , validation_data=(X_valid, dvalid.target)\n", "          , verbose=1)"], "outputs": []}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "5065d64200d6c4675901e727ddf87afc8136a0c3", "collapsed": true, "_cell_guid": "0bfc00c9-37d7-4ebe-9f9c-2bf0661aef3c"}, "source": ["#EVLUEATE THE MODEL ON DEV TEST: What is it doing?\n", "val_preds = model.predict(X_valid)\n", "val_preds = target_scaler.inverse_transform(val_preds)\n", "val_preds = np.exp(val_preds)+1\n", "\n", "#mean_absolute_error, mean_squared_log_error\n", "y_true = np.array(dvalid.price.values)\n", "y_pred = val_preds[:,0]\n", "v_rmsle = rmsle(y_true, y_pred)\n", "print(\" RMSLE error on dev test: \"+str(v_rmsle))"], "outputs": []}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "59772da91cbcb781396138d860d7875a23accaa9", "collapsed": true, "_cell_guid": "996256b1-8628-4a8a-9e75-2eae09fa7d6e"}, "source": ["#CREATE PREDICTIONS\n", "preds = model.predict(X_test, batch_size=BATCH_SIZE)\n", "preds = target_scaler.inverse_transform(preds)\n", "preds = np.exp(preds)-1\n", "\n", "submission = test[[\"test_id\"]]\n", "submission[\"price\"] = preds"], "outputs": []}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "270683b6ea8391d536aa86deeb7d0b381a9006ba", "collapsed": true, "_cell_guid": "8b76a6c9-fa82-4011-82ab-0384b8e2e5b4"}, "source": ["submission.to_csv(\"./myNNsubmission.csv\", index=False)\n", "submission.price.hist()\n"], "outputs": []}, {"cell_type": "markdown", "metadata": {"_uuid": "d0e07027d44dcda199db405c974a05276a3fb4ac", "_cell_guid": "4f194864-1f59-4b94-871b-3b3ee6f01fd2"}, "source": ["This was just an example how nn can solve this problems. Potencial improvements of the kernel:\n", "    - Increase the embeddings factos\n", "    - Decrease the batch size\n", "    - Add Batch Normalization\n", "    - Try LSTM, Bidirectional RNN, stack RNN\n", "    - Try with more dense layers or more rnn outputs\n", "    -  etc. Or even try a new architecture!\n", "    \n", "Any comment will be welcome. Thanks!\n", " \n", "    "]}], "nbformat_minor": 1, "nbformat": 4}