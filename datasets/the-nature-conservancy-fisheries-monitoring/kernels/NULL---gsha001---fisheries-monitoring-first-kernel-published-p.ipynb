{"metadata": {"anaconda-cloud": {}, "language_info": {"version": "3.5.3", "pygments_lexer": "ipython3", "codemirror_mode": {"version": 3, "name": "ipython"}, "mimetype": "text/x-python", "name": "python", "file_extension": ".py", "nbconvert_exporter": "python"}, "kernelspec": {"language": "python", "name": "conda-env-aind-dog-py", "display_name": "Python [conda env:aind-dog]"}}, "cells": [{"metadata": {}, "source": ["# 1. Import Datasets"], "cell_type": "markdown"}, {"metadata": {}, "outputs": [], "execution_count": null, "source": ["from sklearn.datasets import load_files\n", "from keras.utils import np_utils\n", "import numpy as np\n", "from glob import glob"], "cell_type": "code"}, {"metadata": {"collapsed": true}, "outputs": [], "execution_count": null, "source": ["#function to load the dataset\n", "def load_dataset(path):\n", "    data = load_files(path)\n", "    fish_files = np.array(data['filenames'])\n", "    fish_target = np_utils.to_categorical(np.array(data['target']), 8)\n", "    return fish_files,fish_target"], "cell_type": "code"}, {"metadata": {}, "outputs": [], "execution_count": null, "source": ["#loading the paths of training set\n", "train_files, train_targets = load_dataset('fishImages/train')\n", "\n", "#loading the paths of testing set\n", "test_files, _ = load_dataset('fishImages/test')\n", "\n", "#printing the number of samples in test and trainig sets.\n", "print (\"There are %d images in training dataset\"%len(train_files))\n", "print (\"There are %d images in the training set\"%len(test_files))"], "cell_type": "code"}, {"metadata": {}, "source": ["# 2. Visualizations\n"], "cell_type": "markdown"}, {"metadata": {"scrolled": true}, "outputs": [], "execution_count": null, "source": ["import matplotlib.pyplot as plt\n", "import cv2\n", "import seaborn as sns\n", "\n", "sns.set(color_codes=True)\n", "\n", "#finding the number of samles in each class\n", "[ALB, BET, DOL, LAG, NoF, OTHER, SHARK, YFT] = sum(train_targets)\n", "\n", "\n", "fish_count =[ALB, BET, DOL, LAG, NoF, OTHER, SHARK, YFT]\n", "\n", "x=np.arange(8)\n", "\n", "#plotting the barplot between name of classes and number of samples in each class \n", "fig, ax = plt.subplots()\n", "plt.bar(x, fish_count)\n", "plt.xticks(x, ('ALB', 'BET', 'DOL', 'LAG', 'NoF', 'OTHER', 'SHARK', 'YFT'))\n", "plt.xlabel(\"Class Name\")\n", "plt.ylabel(\"Number of samples in the class\")\n", "plt.show()"], "cell_type": "code"}, {"metadata": {"scrolled": false}, "outputs": [], "execution_count": null, "source": ["#function for plotting a histogram for the color intensity of an image\n", "\n", "def intensity_dist(path):\n", "    #reading the image from its path\n", "    img = cv2.imread(path)\n", "    color = ('b','g','r')\n", "    #calculating the number of pixels of each color\n", "    for i, col in enumerate(color):\n", "        histr = cv2.calcHist([img], [i], None, [256], [0,256])\n", "        plt.plot(histr, color=col)\n", "    print(\"Histogram for color Internsity of the image below:\")\n", "    \n", "    #showing the histogram\n", "    plt.xlabel(\"value of the pixel for the given channel\")\n", "    plt.ylabel(\"Number of pixels\")\n", "    plt.show()\n", "    \n", "    #showing the image\n", "    plt.imshow(img)\n", "    plt.show()\n", "    height, width, channels = img.shape\n", "    print(\"Size of the image - (%d , %d)\"%(height,width)) \n", "    print(\"-\"*100)\n", "    \n", "intensity_dist(train_files[56])\n", "intensity_dist(train_files[667])\n", "intensity_dist(train_files[660])\n", "intensity_dist(train_files[1547])\n"], "cell_type": "code"}, {"metadata": {}, "outputs": [], "execution_count": null, "source": ["#intensity_dist(train_files[1147])\n", "#intensity_dist(test_files[12455])\n", "intensity_dist(test_files[60])"], "cell_type": "code"}, {"metadata": {}, "source": ["# 3. Data pre-processing"], "cell_type": "markdown"}, {"metadata": {"collapsed": true}, "outputs": [], "execution_count": null, "source": ["from keras.preprocessing import image\n", "from tqdm import tqdm\n", "\n", "#converting image to tensor\n", "def path_to_tensor(img_path):\n", "    # loads RGB image\n", "    img = image.load_img(img_path, target_size=(224,224))\n", "    #convering the image to 3-D tensor with shape (224,224,3)\n", "    x = image.img_to_array(img)\n", "    #convert 3D tensor to 4D tensor\n", "    return np.expand_dims(x, axis=0)\n", "\n", "def paths_to_tensor(img_paths):\n", "    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]\n", "    return np.vstack(list_of_tensors)"], "cell_type": "code"}, {"metadata": {}, "outputs": [], "execution_count": null, "source": ["from PIL import ImageFile                            \n", "ImageFile.LOAD_TRUNCATED_IMAGES = True \n", "\n", "#preprocessing the data\n", "test_tensors = paths_to_tensor(test_files).astype('float32')/255"], "cell_type": "code"}, {"metadata": {}, "outputs": [], "execution_count": null, "source": ["train_tensors = paths_to_tensor(train_files).astype('float32')/255"], "cell_type": "code"}, {"metadata": {}, "outputs": [], "execution_count": null, "source": ["#shape of the tensor\n", "print(np.shape(train_tensors))"], "cell_type": "code"}, {"metadata": {}, "source": ["# 4. Creating Benchmark Model"], "cell_type": "markdown"}, {"metadata": {"scrolled": true}, "outputs": [], "execution_count": null, "source": ["from keras.layers import Dense, Conv2D, MaxPooling2D, GlobalAveragePooling2D, Dropout\n", "from keras.models import Sequential\n", "\n", "benchmark = Sequential()\n", "\n", "# model Convolution layer\n", "benchmark.add(Conv2D(filters=16,kernel_size=2,strides=1,activation='relu',input_shape=(224,224,3)))\n", "# Max Pooling layer to reduce the dimensionality\n", "benchmark.add(MaxPooling2D(pool_size=2,strides=2))\n", "#Dropout layer, for turning off each node with the probability of 0.3\n", "benchmark.add(Dropout(0.3))\n", "benchmark.add(Conv2D(filters=32, kernel_size=2,strides=1,activation='relu'))\n", "benchmark.add(Dropout(0.3))\n", "benchmark.add(GlobalAveragePooling2D())\n", "#A fully connected dense layer with 8 nodes (no of classes of fish)\n", "benchmark.add(Dense(8,activation='softmax'))\n", "benchmark.summary()"], "cell_type": "code"}, {"metadata": {}, "source": ["### Compiling the Model"], "cell_type": "markdown"}, {"metadata": {"collapsed": true}, "outputs": [], "execution_count": null, "source": ["benchmark.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"], "cell_type": "code"}, {"metadata": {}, "source": ["### Train the Model"], "cell_type": "markdown"}, {"metadata": {}, "outputs": [], "execution_count": null, "source": ["from keras.callbacks import ModelCheckpoint, EarlyStopping\n", "\n", "\n", "epochs = 5\n", "\n", "#checkpointer saves the best weights.\n", "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.benchmark.hdf5', verbose=1, save_best_only=True)\n", "\n", "benchmark.fit(train_tensors, train_targets, batch_size=20, epochs=epochs, callbacks=[checkpointer], validation_split=0.2, verbose=1)"], "cell_type": "code"}, {"metadata": {}, "source": ["# 5. Making Predictions for Benchmark"], "cell_type": "markdown"}, {"metadata": {}, "source": ["### Loading the  weights of Benchmark model"], "cell_type": "markdown"}, {"metadata": {"collapsed": true}, "outputs": [], "execution_count": null, "source": ["benchmark.load_weights('saved_models/weights.best.benchmark.hdf5')"], "cell_type": "code"}, {"metadata": {}, "source": ["### Predictions"], "cell_type": "markdown"}, {"metadata": {}, "outputs": [], "execution_count": null, "source": ["benchmark_model_prediction = [benchmark.predict(np.expand_dims(img_tensor, axis=0)) for img_tensor in test_tensors]"], "cell_type": "code"}, {"metadata": {}, "source": ["### Processing the Predictions"], "cell_type": "markdown"}, {"metadata": {}, "outputs": [], "execution_count": null, "source": ["#visaulizing the array\n", "print(benchmark_model_prediction[:][0])"], "cell_type": "code"}, {"metadata": {}, "outputs": [], "execution_count": null, "source": ["#swapping the axes of the benchmark_model_prediction for easy handling\n", "benchmark_model_prediction = np.swapaxes(benchmark_model_prediction,0,1)\n", "\n", "#creating a pandas dataframe for with benchmark model's prediction\n", "df_pred_model1 = pd.DataFrame(benchmark_model_prediction[0][:], columns=['ALB','BET','DOL','LAG','NoF','OTHER','SHARK','YFT'])\n", "\n", "#first five rows of df_pred_model1 dataframe\n", "print(df_pred_model1[:5])"], "cell_type": "code"}, {"metadata": {}, "outputs": [], "execution_count": null, "source": ["#extracting name of the image form its path\n", "image_names = [test_files[i][22:] for i in range(len(test_files))]\n", "\n", "\n", "#adjusting the filename of the image to match the submission guidelines\n", "for i in range(13153):\n", "    if image_names[i][5]=='_':\n", "        image_names[i] = \"test_stg2/\" + image_names[i]"], "cell_type": "code"}, {"metadata": {}, "outputs": [], "execution_count": null, "source": ["#adding image names to our dataframe\n", "df_pred_model1['image'] = pd.DataFrame(image_names)\n", "\n", "#reindexing the dataframe\n", "df_pred_model1 = df_pred_model1.reindex_axis(['image','ALB','BET','DOL','LAG','NoF','OTHER','SHARK','YFT'], axis=1)\n", "\n", "#printing the first five rows of dataframe\n", "print(df_pred_model1[:5])"], "cell_type": "code"}, {"metadata": {}, "source": ["### Generating .csv file for submission"], "cell_type": "markdown"}, {"metadata": {"collapsed": true}, "outputs": [], "execution_count": null, "source": ["df_pred_model1.to_csv('submission0.csv',index=False)"], "cell_type": "code"}, {"metadata": {"collapsed": true}, "source": [".\n", "\n", "\n", ".\n", "\n", "\n", ".\n", "\n", "\n", "\n", "\n", "\n", "# Score Achieved by Model 1 - 2.00267\n", "\n", "\n", ".\n", "\n", "\n", ".\n", "\n", "\n", ".\n", "\n", "\n"], "cell_type": "markdown"}, {"metadata": {}, "source": ["# 6. Model 2 (Using Transfer Learning, Extracted VGG-19 features)"], "cell_type": "markdown"}, {"metadata": {}, "outputs": [], "execution_count": null, "source": ["from keras.applications.vgg19 import VGG19\n", "from keras.preprocessing import image\n", "from keras.applications.vgg19 import preprocess_input\n", "from keras.models import Model\n", "from keras.layers import Input\n", "import numpy as np\n", "\n", "#Extracting the weights of VGG19 model pretrained on Imagenet\n", "#defing the Input shape\n", "input_tensor = Input(shape=(224,224,3))\n", "#extracting the weights wof VGG19, without top layers\n", "#and MaxPooling as pooling layer\n", "base_model = VGG19(input_tensor=input_tensor, weights='imagenet', include_top=False, pooling=max)\n", "#removing the last layer\n", "output = base_model.get_layer(index = -1).output\n", "#defining the model\n", "VGG19_model2 = Model(base_model.input, output)\n", "VGG19_model2.summary()"], "cell_type": "code"}, {"metadata": {}, "source": ["### Extracting VGG19 features for training and testing datasets"], "cell_type": "markdown"}, {"metadata": {"collapsed": true}, "outputs": [], "execution_count": null, "source": ["VGG19_features = [VGG19_model2.predict(np.expand_dims(train_tensor, axis=0)) for train_tensor in train_tensors]\n", "\n", "VGG19_features_test = [VGG19_model2.predict(np.expand_dims(test_tensor, axis=0)) for test_tensor in test_tensors]"], "cell_type": "code"}, {"metadata": {}, "outputs": [], "execution_count": null, "source": ["print (\"Shape of VGG_19_features: {0}\".format(np.shape(VGG19_features)))\n", "\n", "print (\"Shape of VGG_19_features_test: {0}\".format(np.shape(VGG19_features_test)))\n"], "cell_type": "code"}, {"metadata": {}, "source": ["### Pre-processing the features"], "cell_type": "markdown"}, {"metadata": {}, "outputs": [], "execution_count": null, "source": ["#VGG_19_features having 5 dimensions, so we have to squeeze it to a 4 dim array by removing extra dimension\n", "squeezed_VGG19_train = np.squeeze(VGG19_features, axis=1)\n", "#squeezing the test features\n", "squeezed_VGG19_test = np.squeeze(VGG19_features_test, axis=1)\n", "\n", "print (\"Shape of squeezed_VGG19_train: {0}\".format(np.shape(squeezed_VGG19_train)))\n", "print (\"Shape of squeezed_VGG_19_test: {0}\".format(np.shape(squeezed_VGG19_test)))\n"], "cell_type": "code"}, {"metadata": {}, "source": ["### Defining the Model architecture"], "cell_type": "markdown"}, {"metadata": {}, "outputs": [], "execution_count": null, "source": ["from keras.models import Sequential\n", "from keras.layers import MaxPooling2D, GlobalMaxPooling2D, Dense\n", "\n", "fish_model = Sequential()\n", "#adding a GlobalMaxPooling2D layer with with input shape same as the shape of Squeezed_VGG19_train.\n", "fish_model.add(GlobalMaxPooling2D(input_shape=squeezed_VGG19_train.shape[1:]))\n", "#adding a fully connected dense layer with relu activation function\n", "fish_model.add(Dense(1024, activation='relu'))\n", "#adding a dense layer with softmax activation function.\n", "#no of nodes are same as the number of classes of fish.\n", "fish_model.add(Dense(8, activation = 'softmax'))\n", "fish_model.summary()"], "cell_type": "code"}, {"metadata": {}, "source": ["### Compiling the Model 2"], "cell_type": "markdown"}, {"metadata": {"collapsed": true}, "outputs": [], "execution_count": null, "source": ["#compiling the model with rmsprop optimizer\n", "fish_model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])"], "cell_type": "code"}, {"metadata": {}, "source": ["### Training Model 2"], "cell_type": "markdown"}, {"metadata": {}, "outputs": [], "execution_count": null, "source": ["#training fish_model on the trainig dataset\n", "from keras.callbacks import ModelCheckpoint\n", "\n", "#checkpointer for saving only best weights\n", "checkpointer_VGG = ModelCheckpoint(filepath='saved_models/weights.best.VGG19.hdf5', verbose=1, save_best_only=True)\n", "\n", "fish_model.fit(squeezed_VGG19_train,train_targets,validation_split=0.3,batch_size=20,\n", "               epochs=5,callbacks=[checkpointer_VGG],verbose=1)"], "cell_type": "code"}, {"metadata": {}, "source": ["# 7. Making Predictions with Model 2"], "cell_type": "markdown"}, {"metadata": {}, "source": ["### Loading the weights "], "cell_type": "markdown"}, {"metadata": {"collapsed": true}, "outputs": [], "execution_count": null, "source": ["fish_model.load_weights('saved_models/weights.best.VGG19.hdf5')"], "cell_type": "code"}, {"metadata": {}, "source": ["### Prediction"], "cell_type": "markdown"}, {"metadata": {"collapsed": true}, "outputs": [], "execution_count": null, "source": ["#making the predictions from fish_model\n", "fish_model_prediction = [fish_model.predict(np.expand_dims(feature, axis=0)) for feature in squeezed_VGG19_test]"], "cell_type": "code"}, {"metadata": {}, "source": ["### Pre-processing the predictions"], "cell_type": "markdown"}, {"metadata": {}, "outputs": [], "execution_count": null, "source": ["print(fish_model_prediction[1])"], "cell_type": "code"}, {"metadata": {}, "outputs": [], "execution_count": null, "source": ["print(np.shape(fish_model_prediction))"], "cell_type": "code"}, {"metadata": {"collapsed": true}, "outputs": [], "execution_count": null, "source": ["#swapping the axes for better handling\n", "fish_model_prediction = np.swapaxes(fish_model_prediction,0,1)"], "cell_type": "code"}, {"metadata": {}, "outputs": [], "execution_count": null, "source": ["import pandas as pd\n", "\n", "#creating a pandas dataframe for with benchmark model's prediction\n", "df_pred_fish_model = pd.DataFrame(fish_model_prediction[0][:], columns=['ALB','BET','DOL','LAG','NoF','OTHER','SHARK','YFT'])"], "cell_type": "code"}, {"metadata": {}, "outputs": [], "execution_count": null, "source": ["print(df_pred_fish_model[:5])"], "cell_type": "code"}, {"metadata": {"collapsed": true}, "outputs": [], "execution_count": null, "source": ["#extracting name of the image form its path\n", "image_names = [test_files[i][22:] for i in range(len(test_files))]\n", "\n", "\n", "#adjusting the filename of the image to match the submission guidelines\n", "for i in range(13153):\n", "    if image_names[i][5]=='_':\n", "        image_names[i] = \"test_stg2/\" + image_names[i]"], "cell_type": "code"}, {"metadata": {}, "outputs": [], "execution_count": null, "source": ["#adding image names to our dataframe\n", "df_pred_fish_model['image'] = pd.DataFrame(image_names)\n", "\n", "#reindexing the dataframe\n", "df_pred_fish_model = df_pred_fish_model.reindex_axis(['image','ALB','BET','DOL','LAG','NoF','OTHER','SHARK','YFT'], axis=1)\n", "\n", "#printing the first five rows of dataframe\n", "print(df_pred_fish_model[:5])"], "cell_type": "code"}, {"metadata": {}, "source": ["### Generating .csv file for submission"], "cell_type": "markdown"}, {"metadata": {}, "outputs": [], "execution_count": null, "source": ["df_pred_fish_model.to_csv('submission2.csv',index=False)"], "cell_type": "code"}, {"metadata": {"collapsed": true}, "source": [".\n", "\n", "\n", ".\n", "\n", "\n", ".\n", "\n", "\n", "\n", "\n", "\n", "# Score Achieved by Model 2 - 2.28866\n", "\n", "\n", ".\n", "\n", "\n", ".\n", "\n", "\n", ".\n", "\n", "\n"], "cell_type": "markdown"}, {"metadata": {}, "source": ["# 8. Model-3 (Using less layers of VGG19)"], "cell_type": "markdown"}, {"metadata": {}, "outputs": [], "execution_count": null, "source": ["from keras.applications.vgg19 import VGG19\n", "from keras.preprocessing import image\n", "from keras.applications.vgg19 import preprocess_input\n", "from keras.models import Model\n", "from keras.layers import Input\n", "import numpy as np\n", "\n", "#Extracting the weights of VGG19 model pretrained on Imagenet\n", "#defing the Input shape\n", "input_tensor = Input(shape=(224,224,3))\n", "#extracting the weights wof VGG19, without top layers\n", "#and MaxPooling as pooling layer\n", "base_model = VGG19(input_tensor=input_tensor, weights='imagenet', include_top=False, pooling=max)\n", "#removing the last 11 layers\n", "output = base_model.get_layer(index = -11).output\n", "#defining the model\n", "VGG19_model3 = Model(base_model.input, output)\n", "VGG19_model3.summary()"], "cell_type": "code"}, {"metadata": {}, "source": ["### Extracting VGG19 features for training and testing datasets"], "cell_type": "markdown"}, {"metadata": {"collapsed": true}, "outputs": [], "execution_count": null, "source": ["VGG_19_features_2 = [VGG19_model3.predict(np.expand_dims(train_tensor, axis=0)) for train_tensor in train_tensors]\n", "\n", "VGG_19_features_test_2 = [VGG19_model3.predict(np.expand_dims(test_tensor, axis=0)) for test_tensor in test_tensors]"], "cell_type": "code"}, {"metadata": {}, "source": ["### Pre-processing the features"], "cell_type": "markdown"}, {"metadata": {}, "outputs": [], "execution_count": null, "source": ["squeezed_VGG19_train_2 = np.squeeze(VGG_19_features_2, axis=1)\n", "print (\"Shape of squeezed_VGG19_train_2: {0}\".format(np.shape(squeezed_VGG19_train_2)))"], "cell_type": "code"}, {"metadata": {}, "source": ["## MEMORY ERROR"], "cell_type": "markdown"}, {"metadata": {"scrolled": true}, "outputs": [], "execution_count": null, "source": ["squeezed_VGG19_test_2 = np.squeeze(VGG_19_features_test_2, axis=1)\n", "\n", "print (\"Shape of squeezed_VGG_19_test_2: {0}\".format(np.shape(squeezed_VGG19_test_2)))"], "cell_type": "code"}, {"metadata": {}, "source": ["### MEMORY ERROR"], "cell_type": "markdown"}, {"metadata": {}, "source": ["# 9. Model 4 (New Model From scratch)"], "cell_type": "markdown"}, {"metadata": {}, "outputs": [], "execution_count": null, "source": ["from keras.layers import Dense, Conv2D, MaxPooling2D, GlobalAveragePooling2D, Dropout\n", "from keras.models import Sequential\n", "\n", "model4 = Sequential()\n", "\n", "# model Convolution layer\n", "model4.add(Conv2D(filters=16,kernel_size=2,strides=1,activation='relu',input_shape=(224,224,3)))\n", "# Max Pooling layer to reduce the dimensionality\n", "model4.add(MaxPooling2D(pool_size=2,strides=2))\n", "#Dropout layer, for turning off each node with the probability of 0.2\n", "model4.add(Dropout(0.2))\n", "model4.add(Conv2D(filters=32, kernel_size=2,strides=1,activation='relu'))\n", "model4.add(MaxPooling2D(pool_size=2,strides=2))\n", "model4.add(Dropout(0.2))\n", "model4.add(Conv2D(filters=64,kernel_size=2,strides=1,activation='relu'))\n", "model4.add(MaxPooling2D(pool_size=2,strides=2))\n", "model4.add(Dropout(0.2))\n", "model4.add(GlobalAveragePooling2D())\n", "#A fully connected dense layer with 8 nodes (no of classes of fish)\n", "model4.add(Dense(8,activation='softmax'))\n", "model4.summary()"], "cell_type": "code"}, {"metadata": {}, "source": ["### Compiling the Model"], "cell_type": "markdown"}, {"metadata": {"collapsed": true}, "outputs": [], "execution_count": null, "source": ["model4.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"], "cell_type": "code"}, {"metadata": {}, "source": ["### Train the Model"], "cell_type": "markdown"}, {"metadata": {}, "outputs": [], "execution_count": null, "source": ["from keras.callbacks import ModelCheckpoint, EarlyStopping\n", "\n", "\n", "epochs = 10\n", "\n", "#checkpointer saves the weight of the best model only\n", "checkpointer_4 = [EarlyStopping(monitor='val_loss',min_delta=0.01, patience=0, verbose=1), ModelCheckpoint(filepath='saved_models/weights.best.from_scratch_6.hdf5',\n", "                                  verbose=1, save_best_only=True)]\n", "\n", "model4.fit(train_tensors, train_targets, batch_size=20, epochs=epochs, callbacks=checkpointer_4, validation_split=0.3, verbose=1)"], "cell_type": "code"}, {"metadata": {}, "source": ["# 10. Making Predictions for Model 4"], "cell_type": "markdown"}, {"metadata": {}, "source": ["### Loading the  weights"], "cell_type": "markdown"}, {"metadata": {"collapsed": true}, "outputs": [], "execution_count": null, "source": ["#loading the weights of pretrained model\n", "model4.load_weights('saved_models/weights.best.from_scratch_6.hdf5')"], "cell_type": "code"}, {"metadata": {}, "source": ["### Predictions"], "cell_type": "markdown"}, {"metadata": {"collapsed": true}, "outputs": [], "execution_count": null, "source": ["#making predictions\n", "model4_prediction = [model4.predict(np.expand_dims(img_tensor, axis=0)) for img_tensor in test_tensors]"], "cell_type": "code"}, {"metadata": {}, "source": ["### Processing the Predictions"], "cell_type": "markdown"}, {"metadata": {"collapsed": true}, "outputs": [], "execution_count": null, "source": ["#swapping the axes of the model4_prediction for easy handling\n", "model4_prediction = np.swapaxes(model4_prediction,0,1)"], "cell_type": "code"}, {"metadata": {}, "outputs": [], "execution_count": null, "source": ["import pandas as pd\n", "\n", "#creating a pandas dataframe for with benchmark model's prediction\n", "df_pred_model4 = pd.DataFrame(model4_prediction[0][:], columns=['ALB','BET','DOL','LAG','NoF','OTHER','SHARK','YFT'])"], "cell_type": "code"}, {"metadata": {"collapsed": true}, "outputs": [], "execution_count": null, "source": ["#extracting name of the image form its path\n", "image_names = [test_files[i][22:] for i in range(len(test_files))]\n", "\n", "\n", "#adjusting the filename of the image to match the submission guidelines\n", "for i in range(13153):\n", "    if image_names[i][5]=='_':\n", "        image_names[i] = \"test_stg2/\" + image_names[i]"], "cell_type": "code"}, {"metadata": {}, "outputs": [], "execution_count": null, "source": ["#adding image names to our dataframe\n", "df_pred_model4['image'] = pd.DataFrame(image_names)\n", "\n", "#reindexing the dataframe\n", "df_pred_model4 = df_pred_model4.reindex_axis(['image','ALB','BET','DOL','LAG','NoF','OTHER','SHARK','YFT'], axis=1)"], "cell_type": "code"}, {"metadata": {}, "source": ["### Generating .csv file for submission"], "cell_type": "markdown"}, {"metadata": {}, "outputs": [], "execution_count": null, "source": ["df_pred_model4.to_csv('submission4.csv',index=False)"], "cell_type": "code"}, {"metadata": {"collapsed": true}, "source": [".\n", "\n", "\n", ".\n", "\n", "\n", ".\n", "\n", "\n", "\n", "\n", "\n", "# Score Achieved by Model 4 - 1.65209\n", "\n", "\n", ".\n", "\n", "\n", ".\n", "\n", "\n", ".\n", "\n", "\n"], "cell_type": "markdown"}, {"metadata": {}, "source": ["# 11. Model 5 (Refining Model 4)"], "cell_type": "markdown"}, {"metadata": {"scrolled": true}, "outputs": [], "execution_count": null, "source": ["from keras.layers import Dense, Conv2D, MaxPooling2D, GlobalAveragePooling2D, Dropout\n", "from keras.models import Sequential\n", "\n", "model5 = Sequential()\n", "\n", "#Model architecture.\n", "#Convolution layer\n", "model5.add(Conv2D(filters=32,kernel_size=2,strides=1,activation='relu',input_shape=(224,224,3)))\n", "# Max Pooling layer to reduce the dimensionality\n", "model5.add(MaxPooling2D(pool_size=2,strides=2))\n", "#Dropout layer, for turning off each node with the probability of 0.5\n", "model5.add(Dropout(0.5))\n", "model5.add(Conv2D(filters=64, kernel_size=2,strides=1,activation='relu'))\n", "model5.add(MaxPooling2D(pool_size=2,strides=2))\n", "#Dropout layer, for turning off each node with the probability of 0.4\n", "model5.add(Dropout(0.4))\n", "model5.add(Conv2D(filters=128,kernel_size=2,strides=1,activation='relu'))\n", "model5.add(MaxPooling2D(pool_size=2,strides=2))\n", "#Dropout layer, for turning off each node with the probability of 0.2\n", "model5.add(Dropout(0.2))\n", "#Global Average Pooling layer for object localization\n", "model5.add(GlobalAveragePooling2D())\n", "#A fully connected dense layer with 8 nodes (no of classes of fish)\n", "model5.add(Dense(8,activation='softmax'))\n", "#printing the summary of the architecture\n", "model5.summary()"], "cell_type": "code"}, {"metadata": {}, "source": ["### Compiling the Model"], "cell_type": "markdown"}, {"metadata": {"collapsed": true}, "outputs": [], "execution_count": null, "source": ["model5.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"], "cell_type": "code"}, {"metadata": {}, "source": ["### Train the Model"], "cell_type": "markdown"}, {"metadata": {"scrolled": true}, "outputs": [], "execution_count": null, "source": ["from keras.callbacks import ModelCheckpoint, EarlyStopping\n", "\n", "#number of epochs\n", "epochs = 5\n", "batch_size=20\n", "#split the training data into training and validation datasets (30% for validation and 70 % for training).\n", "validation_split=0.3\n", "# print the progress\n", "verbose=0.1\n", "\n", "#checkpointer saves the weight of the best model only\n", "checkpointer_5 = ModelCheckpoint(filepath='saved_models/weights.best.from_scratch.hdf5', verbose=1, save_best_only=True)\n", "\n", "model5.fit(train_tensors, train_targets, batch_size=batch_size, epochs=epochs, \n", "           callbacks=[checkpointer_5], validation_split=validation_split, verbose=verbose)"], "cell_type": "code"}, {"metadata": {}, "source": ["# 12. Making Predictions for Model 5"], "cell_type": "markdown"}, {"metadata": {}, "source": ["### Loading the  weights"], "cell_type": "markdown"}, {"metadata": {"collapsed": true}, "outputs": [], "execution_count": null, "source": ["model5.load_weights('saved_models/weights.best.from_scratch.hdf5')"], "cell_type": "code"}, {"metadata": {}, "source": ["### Predictions"], "cell_type": "markdown"}, {"metadata": {}, "outputs": [], "execution_count": null, "source": ["model5_prediction = [model5.predict(np.expand_dims(img_tensor, axis=0)) for img_tensor in test_tensors]"], "cell_type": "code"}, {"metadata": {}, "source": ["### Processing the Predictions"], "cell_type": "markdown"}, {"metadata": {}, "outputs": [], "execution_count": null, "source": ["#visaulizing the array\n", "print(model5_prediction[:][0])"], "cell_type": "code"}, {"metadata": {}, "outputs": [], "execution_count": null, "source": ["#swapping the axes of the model4_prediction for easy handling\n", "model5_prediction = np.swapaxes(model5_prediction,0,1)\n", "\n", "#creating a pandas dataframe for with benchmark model's prediction\n", "df_pred_model5 = pd.DataFrame(model5_prediction[0][:], columns=['ALB','BET','DOL','LAG','NoF','OTHER','SHARK','YFT'])\n", "\n", "#first five rows of df_pred_model1 dataframe\n", "print(df_pred_model5[:5])"], "cell_type": "code"}, {"metadata": {}, "outputs": [], "execution_count": null, "source": ["#extracting name of the image form its path\n", "image_names = [test_files[i][22:] for i in range(len(test_files))]\n", "\n", "\n", "#adjusting the filename of the image to match the submission guidelines\n", "for i in range(13153):\n", "    if image_names[i][5]=='_':\n", "        image_names[i] = \"test_stg2/\" + image_names[i]"], "cell_type": "code"}, {"metadata": {}, "outputs": [], "execution_count": null, "source": ["#adding image names to our dataframe\n", "df_pred_model5['image'] = pd.DataFrame(image_names)\n", "\n", "#reindexing the dataframe\n", "df_pred_model5 = df_pred_model5.reindex_axis(['image','ALB','BET','DOL','LAG','NoF','OTHER','SHARK','YFT'], axis=1)\n", "\n", "#printing the first five rows of dataframe\n", "print(df_pred_model5[:5])"], "cell_type": "code"}, {"metadata": {}, "source": ["### Generating .csv file for submission"], "cell_type": "markdown"}, {"metadata": {"collapsed": true}, "outputs": [], "execution_count": null, "source": ["df_pred_model5.to_csv('submission5.csv',index=False)"], "cell_type": "code"}, {"metadata": {"collapsed": true}, "source": [".\n", "\n", "\n", ".\n", "\n", "\n", ".\n", "\n", "\n", "\n", "\n", "\n", "# Score Achieved by Model 5 - 1.56079\n", "\n", "\n", ".\n", "\n", "\n", ".\n", "\n", "\n", ".\n", "\n", "\n"], "cell_type": "markdown"}, {"metadata": {}, "source": ["# 13. Model 6 (Refining Model 5)"], "cell_type": "markdown"}, {"metadata": {}, "outputs": [], "execution_count": null, "source": ["from keras.layers import Dense, Conv2D, MaxPooling2D, GlobalAveragePooling2D, Dropout\n", "from keras.models import Sequential\n", "\n", "model6 = Sequential()\n", "\n", "# model Convolution layer\n", "model6.add(Conv2D(filters=32,kernel_size=2,strides=1,activation='relu',input_shape=(224,224,3)))\n", "# Max Pooling layer to reduce the dimensionality\n", "model6.add(MaxPooling2D(pool_size=2,strides=2))\n", "#Dropout layer, for turning off each node with the probability of 0.2 \n", "model6.add(Dropout(0.2))\n", "model6.add(Conv2D(filters=64, kernel_size=2,strides=1,activation='relu'))\n", "model6.add(MaxPooling2D(pool_size=2,strides=2))\n", "#Dropout layer, for turning off each node with the probability of 0.2\n", "model6.add(Dropout(0.2))\n", "model6.add(Conv2D(filters=128,kernel_size=2,strides=1,activation='relu'))\n", "model6.add(MaxPooling2D(pool_size=2,strides=2))\n", "#Dropout layer, for turning off each node with the probability of 0.2\n", "model6.add(Dropout(0.2))\n", "model6.add(GlobalAveragePooling2D())\n", "#A fully connected dense layer with 8 nodes (no of classes of fish)\n", "model6.add(Dense(8,activation='softmax'))\n", "model6.summary()"], "cell_type": "code"}, {"metadata": {}, "source": ["### Compiling the Model"], "cell_type": "markdown"}, {"metadata": {"collapsed": true}, "outputs": [], "execution_count": null, "source": ["model6.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"], "cell_type": "code"}, {"metadata": {}, "source": ["### Train the Model"], "cell_type": "markdown"}, {"metadata": {}, "outputs": [], "execution_count": null, "source": ["from keras.callbacks import ModelCheckpoint, EarlyStopping\n", "\n", "\n", "epochs = 10\n", "\n", "#checkpointer saves the weight of the best model only\n", "checkpointer_6 = [EarlyStopping(monitor='val_loss',min_delta=0.01, patience=0, verbose=1), ModelCheckpoint(filepath='saved_models/weights.best.from_scratch_5.hdf5',\n", "                                  verbose=1, save_best_only=True)]\n", "\n", "model6.fit(train_tensors, train_targets, batch_size=20, epochs=epochs, callbacks=checkpointer_6, validation_split=0.3, verbose=1)"], "cell_type": "code"}, {"metadata": {}, "source": ["# 14. Making Predictions for Model 6"], "cell_type": "markdown"}, {"metadata": {}, "source": ["### Loading the  weights"], "cell_type": "markdown"}, {"metadata": {"collapsed": true}, "outputs": [], "execution_count": null, "source": ["#loading the weights of pretrained model\n", "model6.load_weights('saved_models/weights.best.from_scratch_5.hdf5')"], "cell_type": "code"}, {"metadata": {}, "source": ["### Predictions"], "cell_type": "markdown"}, {"metadata": {"collapsed": true}, "outputs": [], "execution_count": null, "source": ["#making predictions\n", "model6_prediction = [model6.predict(np.expand_dims(img_tensor, axis=0)) for img_tensor in test_tensors]"], "cell_type": "code"}, {"metadata": {}, "source": ["### Processing the Predictions"], "cell_type": "markdown"}, {"metadata": {"collapsed": true}, "outputs": [], "execution_count": null, "source": ["#swapping the axes of the model6_prediction for easy handling\n", "model6_prediction = np.swapaxes(model6_prediction,0,1)"], "cell_type": "code"}, {"metadata": {}, "outputs": [], "execution_count": null, "source": ["import pandas as pd\n", "\n", "#creating a pandas dataframe for with benchmark model's prediction\n", "df_pred_model6 = pd.DataFrame(model6_prediction[0][:], columns=['ALB','BET','DOL','LAG','NoF','OTHER','SHARK','YFT'])"], "cell_type": "code"}, {"metadata": {"collapsed": true}, "outputs": [], "execution_count": null, "source": ["#extracting name of the image form its path\n", "image_names = [test_files[i][22:] for i in range(len(test_files))]\n", "\n", "\n", "#adjusting the filename of the image to match the submission guidelines\n", "for i in range(13153):\n", "    if image_names[i][5]=='_':\n", "        image_names[i] = \"test_stg2/\" + image_names[i]"], "cell_type": "code"}, {"metadata": {}, "outputs": [], "execution_count": null, "source": ["#adding image names to our dataframe\n", "df_pred_model6['image'] = pd.DataFrame(image_names)\n", "\n", "#reindexing the dataframe\n", "df_pred_model6 = df_pred_model6.reindex_axis(['image','ALB','BET','DOL','LAG','NoF','OTHER','SHARK','YFT'], axis=1)"], "cell_type": "code"}, {"metadata": {}, "source": ["### Generating .csv file for submission"], "cell_type": "markdown"}, {"metadata": {}, "outputs": [], "execution_count": null, "source": ["df_pred_model6.to_csv('submission6.csv',index=False)"], "cell_type": "code"}, {"metadata": {"collapsed": true}, "source": [".\n", "\n", "\n", ".\n", "\n", "\n", ".\n", "\n", "\n", "\n", "\n", "\n", "# Score Achieved by Model 6 - 1.51518\n", "\n", "\n", ".\n", "\n", "\n", ".\n", "\n", "\n", ".\n", "\n", "\n"], "cell_type": "markdown"}, {"metadata": {"collapsed": true}, "outputs": [], "execution_count": null, "source": [], "cell_type": "code"}], "nbformat": 4, "nbformat_minor": 1}