{"cells":[{"metadata":{"_uuid":"a0cb8aeade67b94440b7608796603c134ae200cc"},"cell_type":"markdown","source":"# Don't Call me Turkey!\n\nThis kernel will show three approaches to tackle the challenge [Don't Call me Turkey!](https://www.kaggle.com/c/dont-call-me-turkey) using TensorFlow Estimator API\n\n* Logistic Regression \n* Multilayer Perceptron\n* LSTM\n"},{"metadata":{"trusted":false,"_uuid":"5191d6beefa63323da0753160417c6c6f9aa8012"},"cell_type":"code","source":"from __future__ import division\n\nimport tensorflow as tf\nimport numpy as np\nimport json\nimport pandas as pd","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bfd8bd3d4c545c07bf995c9b3e6cdf5013fffb19"},"cell_type":"markdown","source":"# Tensorflow model using canned estimators"},{"metadata":{"_uuid":"e304f1d59b87d067e47fd1183a67274ccd11e9b6"},"cell_type":"markdown","source":"## Read dataset"},{"metadata":{"trusted":false,"_uuid":"da5b94f925585c392ac5e592297e32b5188155a7"},"cell_type":"code","source":"train_raw = json.load(open('../input/train.json','r'))\ntest_raw = json.load(open('../input/test.json','r'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"c84b6bdca3fbfc65a1db66cc002b6b09b87d612f"},"cell_type":"code","source":"train_raw[0].keys()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"293523cf98b23e530574e9b21fc509bd71fe4e90"},"cell_type":"code","source":"X_train_np = np.array([ np.mean(sample['audio_embedding'],axis=0) for sample in train_raw]).astype(np.float32)\nX_train_id = [sample['vid_id'] for sample in train_raw]\n\nY_train_np = np.array([ sample['is_turkey'] for sample in train_raw]).astype(np.float32)\n\nX_test_np = np.array([ np.mean(sample['audio_embedding'],axis=0) for sample in test_raw]).astype(np.float32)\nX_test_id = [sample['vid_id'] for sample in test_raw]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6bf5c3a859165b1976442cdfc130dbd1061d1ed8"},"cell_type":"markdown","source":"## Canned Estimators"},{"metadata":{"_uuid":"f4ec40fca27675ab2c02cd7952ad2e4b14ec44b9"},"cell_type":"markdown","source":"### Feature Columns\n\nAlthough not mandatory, dealing with [feature columns makes](https://www.tensorflow.org/guide/feature_columns) life easier when building models with [Estimator API](https://www.tensorflow.org/guide/estimators). \n\nThink of a feature column as an intermediary entity between the raw data and the model itself. \n\nIt helps the model to interpret/transform the data that comes from the input function.\n\nThere are available the following feature column types: \n\n* [numeric_column](https://www.tensorflow.org/api_docs/python/tf/feature_column/numeric_column): Treat raw data as a numeric scalar or matrix\n* [bucketized_column](https://www.tensorflow.org/api_docs/python/tf/feature_column/bucketized_column): Map a numeric scalar into buckets, as one-hot representation, given a boundary list\n* [categorical_column_with_identity](https://www.tensorflow.org/api_docs/python/tf/feature_column/categorical_column_with_identity): Map an integer input into a one-hot representation\n* [categorical_column_with_vocabulary_list](https://www.tensorflow.org/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_list): Map a string input into a one-hot representation given a Python list.\n* [categorical_column_with_vocabulary_file](https://www.tensorflow.org/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file): Map a string input into a one-hot representation given a vocabulary list as a text file\n* [categorical column with hash_bucket](https://www.tensorflow.org/api_docs/python/tf/feature_column/categorical_column_with_hash_bucket): Map a string or integer input into a hash ID as a sparse tensor.\n* [embedding_column](https://www.tensorflow.org/api_docs/python/tf/feature_column/embedding_column): Map a categorical feature into a low dimensional representation\n* [indicator_column](https://www.tensorflow.org/api_docs/python/tf/feature_column/indicator_column): Map a categorical feature into multi-hot encoding representation\n* [crossed_column](https://www.tensorflow.org/api_docs/python/tf/feature_column/crossed_column): Map a tuple of categorical features into a hash ID as a sparse tensor\n\n\nIn our case we have a feature embedding of size 128 for each 10 frames (1sec each) per audio.\n\nThe first approach will build a simple Logistic Regression taking the average embedding as input.\nSo we'll end up with just one feature of size 128"},{"metadata":{"_uuid":"cc3bf907b489228ad680be9d8ad24620cdbd0b1e"},"cell_type":"markdown","source":"### Define normalizer_fn\n\nZNorm"},{"metadata":{"trusted":false,"_uuid":"4e34ccf8046e793499a97ab343e23cb176ca23ef"},"cell_type":"code","source":"def znorm_fn(input_data):\n    mean = np.mean(input_data,axis=0)\n    std = np.std(input_data,axis=0)\n    \n    def _znorm_fn(col):        \n        return (col - mean)/std\n    \n    return _znorm_fn","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"ac0ad54493ff95b574b0c58eaac2337855d747e0"},"cell_type":"code","source":"norm_fn = znorm_fn(X_train_np)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"4d94528c643e16b79655fd5dccfb50cfa2b0baf0"},"cell_type":"code","source":"feature_columns = [\n    tf.feature_column.numeric_column('average_embedding',shape=[128],normalizer_fn=norm_fn)]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3c4b19cf8a45e79d2605d384c0f3d22d45064d3f"},"cell_type":"markdown","source":"### Input Function"},{"metadata":{"trusted":false,"_uuid":"ff0a1cde2621b32eb195f1bb52741c2a20f649d0"},"cell_type":"code","source":"batch_size = 256\nepochs = 100","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"d4ca6eb60c0551f4ecd5a6b6a07a1e65e710c735"},"cell_type":"code","source":"def input_fn():\n    dataset = tf.data.Dataset.from_tensor_slices(({'average_embedding':X_train_np,'vid_id':X_train_id},Y_train_np))    \n    dataset = dataset.shuffle(100*batch_size).repeat(epochs).batch(batch_size)\n    return dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"01676b109356747e12d1765fcfef2e8667839e5e"},"cell_type":"code","source":"def predict_input_fn():\n    dataset = tf.data.Dataset.from_tensor_slices(({'average_embedding':X_test_np,'vid_id':X_test_id}))    \n    dataset = dataset.batch(batch_size)\n    return dataset","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cd7420d7a48cf8b5c1cb79fd157072e54aa03fa4"},"cell_type":"markdown","source":"### Logistic Regression\n\n**OBS**: **vid_id** is being forwarded to be output on predictions. \n\nRecall that although **vid_id** is a feature, it is not being used to train the model, once who defines which features will be used is **feature_columns**\n"},{"metadata":{"trusted":false,"_uuid":"44229c90e8a2adc6060697011cc17a598ce525bf"},"cell_type":"code","source":"lr = tf.estimator.LinearClassifier(feature_columns=feature_columns,model_dir='./logistic_regression_trained_models')\nlr = tf.contrib.estimator.forward_features(lr,'vid_id')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"84098f1feeab69c4a2d2c7acc047a6ee2616344c"},"cell_type":"code","source":"lr.train(input_fn=input_fn)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d8e23f4c66e73c14ce7c30440bf1053c23a4e7d6"},"cell_type":"markdown","source":"#### Predictions"},{"metadata":{"trusted":false,"_uuid":"ad1d842f81c4ec58c7cef5378589245c93aca2f4"},"cell_type":"code","source":"with open('submission_logistic_regression.csv','w') as f:\n    f.write('vid_id,is_turkey\\n')\n    for prediction in lr.predict(input_fn=predict_input_fn):    \n        f.write(\"{},{}\\n\".format(prediction['vid_id'],prediction['class_ids'][0]))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"da4e44007bdcc968f1769b7cdfaa148ea4386ea9"},"cell_type":"markdown","source":"### Multilayer Perceptron"},{"metadata":{"trusted":false,"_uuid":"a5e48253539b87e4c7eb627748e7748b60ee1e3d"},"cell_type":"code","source":"dnn = tf.estimator.DNNClassifier(feature_columns=feature_columns,\n                                 model_dir='./dnn_trained_models',\n                                 n_classes=2,\n                                 dropout=0.2,                                \n                                 hidden_units = [128,32,8,2])\ndnn = tf.contrib.estimator.forward_features(dnn,'vid_id')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"fc98873fa4125b94174ea2f740f511ac710721af"},"cell_type":"code","source":"dnn.train(input_fn=input_fn)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"50f1f817589d8858f354f3525fd0743ec6607056"},"cell_type":"markdown","source":"#### Predictions"},{"metadata":{"trusted":false,"_uuid":"2398c7fe6f03606fdc39949f769c4b2fad1bb92b"},"cell_type":"code","source":"with open('submission_dnn.csv','w') as f:\n    f.write('vid_id,is_turkey\\n')\n    for prediction in lr.predict(input_fn=predict_input_fn):    \n        f.write(\"{},{}\\n\".format(prediction['vid_id'],prediction['class_ids'][0]))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ff19463786fdc2a861a4b552f227ce5c71bdd2d9"},"cell_type":"markdown","source":"## LSTM \n\nAs there isn't a canned estimator for LSTM, we need to code a custom one by ourselves"},{"metadata":{"trusted":false,"_uuid":"8740cbd22c1dece8aabcb72606f5eec185a4f3c6"},"cell_type":"code","source":"train_raw = json.load(open('../input/train.json','r'))\ntest_raw = json.load(open('../input/test.json','r'))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d5fd1c4946c0800307c053614b23dcb8cf893d7b"},"cell_type":"markdown","source":"### Read dataset"},{"metadata":{"trusted":false,"_uuid":"8a0ce3aeb737b6bc7adad81cdba28b7120b5f326"},"cell_type":"code","source":"batch_size = 256\nepochs = 20","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"90625d85c81a822ccab7a71b3a2e7ea842c0120e"},"cell_type":"code","source":"lstm_X_train = []\nlstm_X_train_id = []\nlstm_Y_train = []\n\nlstm_X_test = []\nlstm_X_test_id = []\n\nfor train_sample in train_raw:        \n    # Perform padding\n    if len(train_sample['audio_embedding']) < 10:\n        while len(train_sample['audio_embedding']) <10:\n            train_sample['audio_embedding'].append(np.mean(train_sample['audio_embedding'],axis=0))\n    \n    \n    lstm_X_train.append(train_sample['audio_embedding'])\n    lstm_Y_train.append(train_sample['is_turkey'])\n    lstm_X_train_id.append(train_sample['vid_id'])    \n\nfor test_sample in test_raw:       \n        \n    # Perform padding\n    if len(test_sample['audio_embedding']) < 10:\n        while len(test_sample['audio_embedding']) <10:\n            test_sample['audio_embedding'].append(np.mean(test_sample['audio_embedding'],axis=0))\n                \n    lstm_X_test.append(test_sample['audio_embedding'])    \n    lstm_X_test_id.append(test_sample['vid_id'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"4689a67fb2a5277ced293bb2de6c50a01d6c8ae1"},"cell_type":"code","source":"lstm_X_train_np = np.array(lstm_X_train).astype(np.float32)\nlstm_Y_train_np = np.array(lstm_Y_train).astype(np.float32)\n\nlstm_X_test_np = np.array(lstm_X_test).astype(np.float32)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4683e898efbe9cbd9b0201f79b752df4496dd348"},"cell_type":"markdown","source":"### Split a portion of trainset for evaluation"},{"metadata":{"trusted":false,"_uuid":"7e7c94ef36894bab92b2cd2b2e46e6ebc92f3a49"},"cell_type":"code","source":"from sklearn.model_selection import StratifiedShuffleSplit","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"ace0a0e15d998a3411911944e738ff5217c9f67e"},"cell_type":"code","source":"sss = StratifiedShuffleSplit(1,train_size=0.9)\nlstm_train_index, lstm_eval_index = next(sss.split(lstm_X_train_np, lstm_Y_train_np))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"ba70a56818bdb067f9de0fc9044565b5ccc1966e"},"cell_type":"code","source":"lstm_X_train_train_np = lstm_X_train_np[lstm_train_index]\nlstm_X_train_eval_np = lstm_X_train_np[lstm_eval_index]\n\nlstm_Y_train_train_np = lstm_Y_train_np[lstm_train_index]\nlstm_Y_train_eval_np = lstm_Y_train_np[lstm_eval_index]\n\nlstm_X_train_train_id = list(np.array(lstm_X_train_id)[lstm_train_index])\nlstm_X_train_eval_id = list(np.array(lstm_X_train_id)[lstm_eval_index])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"688c3842903fbef6a83b946e37baaf885dc2d910"},"cell_type":"code","source":"print(lstm_X_train_train_np.shape)\nprint(lstm_X_train_eval_np.shape)\n\nprint(lstm_Y_train_train_np.shape)\nprint(lstm_Y_train_eval_np.shape)\n\nprint(lstm_X_test_np.shape)\n\nprint(len(lstm_X_train_train_id))\nprint(len(lstm_X_train_eval_id))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c370bd555d740c870df4818744827ef74d0ba01f"},"cell_type":"markdown","source":"### Define Estimator input function\n\nHere it is defined how and which data will be consumed by the model during training, evaluation and prediction"},{"metadata":{"trusted":false,"_uuid":"ad25ea7323c14ab9e5d2ea844fa836972ff7d06c"},"cell_type":"code","source":"def znorm_fn(input_data):\n    mean = np.mean(input_data,axis=0)\n    std = np.std(input_data,axis=0)\n    \n    def _znorm_fn(mini_batch):        \n        norm_mini_batch = tf.map_fn(lambda c: (c-mean)/std,mini_batch)                \n        return norm_mini_batch\n    \n    return _znorm_fn","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"80882124a6aa5248ea2c46ef1192f49f2ed6f3a5"},"cell_type":"code","source":"norm_fn = znorm_fn(lstm_X_train_train_np)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"66501e220d12a4c135a61c6b43da0a72e403c850"},"cell_type":"code","source":"feature_columns = [\n    tf.feature_column.numeric_column('audio_embedding',shape=[128],normalizer_fn=norm_fn)]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"df71cfa65015e84ce388dfbf571a44ca294b8e87"},"cell_type":"markdown","source":"#### Input function for training and evaluation"},{"metadata":{"trusted":false,"_uuid":"9d13ac83a526586bb448319ad843bcb225574a5c"},"cell_type":"code","source":"def lstm_train_eval_input_fn(X,Y,vid_id,batch_size):\n    \n    def raw_input_fn():\n        dataset = tf.data.Dataset.from_tensor_slices(({'audio_embedding':X,'vid_id':vid_id},Y))    \n        dataset = dataset.batch(batch_size)\n        return dataset\n    \n    return raw_input_fn\n\nlstm_train_input_fn = lstm_train_eval_input_fn(lstm_X_train_train_np,\n                                               lstm_Y_train_train_np,\n                                               lstm_X_train_train_id,\n                                              batch_size)\n\nlstm_eval_input_fn = lstm_train_eval_input_fn(lstm_X_train_eval_np,\n                                              lstm_Y_train_eval_np,\n                                              lstm_X_train_eval_id,\n                                             batch_size)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"790fc4cc7e87bca3449ddee35fe7aecc121f77cb"},"cell_type":"markdown","source":"#### Input function for prediction"},{"metadata":{"trusted":false,"_uuid":"c7a436754f035919d53eda1432f77fff74690718"},"cell_type":"code","source":"def lstm_predict_input_fn():\n    dataset = tf.data.Dataset.from_tensor_slices(({'audio_embedding':lstm_X_test_np,'vid_id':lstm_X_test_id}))    \n    dataset = dataset.batch(batch_size)\n    return dataset","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"95e6a82565462e9b1580552ede1021bc80ff41d7"},"cell_type":"markdown","source":"### Define metrics to be measured during evalution when training"},{"metadata":{"trusted":false,"_uuid":"9e273fbb9b030069b740026e7288002e4315b044"},"cell_type":"code","source":"def metric_ops(target,predictions):    \n    return {\n        'Accuracy': tf.metrics.accuracy(\n        labels=target,\n        predictions=predictions,\n        name='accuracy')\n    }","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"22a957ced469873e9ad94f93620d5d10832c90a9"},"cell_type":"markdown","source":"#### Model Parameters"},{"metadata":{"trusted":false,"_uuid":"5e8230fc1cb5f37c3334d8bb171708f832a03394"},"cell_type":"code","source":"params = tf.contrib.training.HParams(\n    learning_rate = 5e-3,\n    dropout=0.3,\n    reg_val=1e-3,\n    feature_columns = feature_columns # here you pass the feature columns to the model_fn (see below)\n)\n\nrun_config = tf.estimator.RunConfig(\n    model_dir='./lstm_model',\n    save_summary_steps=100,    \n    save_checkpoints_steps=100,\n    keep_checkpoint_max=3,    \n)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"59e9b89f9be203dbf38d01d1fa769bd62f1eb162"},"cell_type":"markdown","source":"### model_fn\n\nHere it is defined the network architecture"},{"metadata":{"trusted":false,"_uuid":"c6121b781466866c9fe8c9a1196d30abfcc3e974"},"cell_type":"code","source":"def model_fn(features, labels, mode, params):\n            \n    is_training = mode == tf.estimator.ModeKeys.TRAIN               \n\n    # Apply transformations on input the features given the feature_columns\n    input_features = tf.feature_column.input_layer(features, params.feature_columns)\n    \n    # Unfortunately, one needs to reshape the input tensor\n    # because the transformation above cancels the second dimension (window size)\n    input_features = tf.reshape(input_features,[-1,10,128])\n\n    # Unstack the second dimension (window size) to return a list of tensors\n    feature_sequence = tf.unstack(input_features,axis=1)    \n    \n    num_units = [128,64,8]\n    \n    # Create Bidirectional LSTM network\n    lstm_forward_cells = [tf.nn.rnn_cell.LSTMCell(num_units=n) for n in num_units]\n    lstm_backward_cells = [tf.nn.rnn_cell.LSTMCell(num_units=n) for n in num_units]\n    \n    stacked_lstm_forward_cell = tf.nn.rnn_cell.MultiRNNCell(lstm_forward_cells)\n    stacked_lstm_backward_cell = tf.nn.rnn_cell.MultiRNNCell(lstm_backward_cells)\n    \n    lstm_output, _,_ = tf.nn.static_bidirectional_rnn(stacked_lstm_forward_cell,\n                                                   stacked_lstm_backward_cell,\n                                                   feature_sequence,\n                                                   dtype=tf.float32)\n    last_lstm_output = lstm_output[-1]\n    \n\n    net = tf.layers.dropout(last_lstm_output,params.dropout,training=is_training)\n    \n    net = tf.layers.dense(inputs=net,units=8,activation=tf.nn.relu,\n                          kernel_regularizer=tf.contrib.layers.l2_regularizer(params.reg_val),\n                          kernel_initializer=tf.contrib.layers.variance_scaling_initializer(),\n                          bias_initializer=tf.contrib.layers.variance_scaling_initializer(),\n                          bias_regularizer=tf.contrib.layers.l2_regularizer(params.reg_val))    \n        \n    net = tf.layers.dense(inputs=net,units=2,activation=tf.nn.relu,\n                          kernel_regularizer=tf.contrib.layers.l2_regularizer(params.reg_val),\n                          kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                          bias_initializer=tf.contrib.layers.xavier_initializer(),\n                          bias_regularizer=tf.contrib.layers.l2_regularizer(params.reg_val))\n    net = tf.nn.softmax(net)\n\n    predictions = {\n        'class_id':tf.argmax(net, axis=1, name=\"prediction\"),\n        'class_proba': net}    \n\n\n    total_loss = None\n    loss = None\n    train_op = None\n    eval_metric_ops = {} \n        \n    if mode != tf.estimator.ModeKeys.PREDICT:\n        \n        # As target is just 0 or 1, it is necessary to transform to one-hot encoding\n        target = tf.one_hot(tf.cast(labels,dtype=tf.uint8),depth=2)        \n        \n         # IT IS VERY IMPORTANT TO RETRIEVE THE REGULARIZATION LOSSES BY HAND\n        reg_loss = tf.losses.get_regularization_loss()        \n        loss = tf.losses.softmax_cross_entropy(target, net)                    \n        total_loss = loss + reg_loss\n\n        learning_rate = tf.constant(params.learning_rate, name='fixed_learning_rate')            \n        optimizer = tf.train.AdamOptimizer(learning_rate)\n#         optimizer = tf.train.RMSPropOptimizer(learning_rate)\n\n        if is_training:\n            # If you plan to use batch_norm layers, you DO must get this collection in order to perform updates on batch_norm variables\n            update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n            with tf.control_dependencies(update_ops):\n                train_op = optimizer.minimize(                    \n                    loss=total_loss, global_step=tf.train.get_global_step())\n\n        eval_metric_ops = metric_ops(labels, predictions['class_id'])\n\n    return tf.estimator.EstimatorSpec(\n        mode=mode,\n        predictions=predictions,\n        loss=total_loss,\n        train_op=train_op,\n        eval_metric_ops=eval_metric_ops)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"58532981a19f7ce7fde68bdec18256196847359b"},"cell_type":"markdown","source":"#### Instantiate estimator"},{"metadata":{"trusted":false,"_uuid":"18387dc5dd72d4e13fb1a6454c4b8efe99dd8eef"},"cell_type":"code","source":"lstm = tf.estimator.Estimator(\n    model_fn=model_fn,\n    params=params,\n    config=run_config\n)\n\nlstm = tf.contrib.estimator.forward_features(lstm,'vid_id')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"65c5730858987882ea7c9b0469833124a3bc986a"},"cell_type":"markdown","source":"#### Train model"},{"metadata":{"trusted":false,"_uuid":"6210c419430f4a2a30c784c73abb8184e93e8d6c"},"cell_type":"code","source":"# If you want to train without any evaluation, uncomment the line below\n# lstm.train(input_fn=lstm_train_input_fn)\n\n\nlstm_train_spec = tf.estimator.TrainSpec(input_fn=lstm_train_input_fn,\n                                        max_steps=int(epochs*(len(lstm_X_train_train_id)/batch_size)))\nlstm_eval_spec = tf.estimator.EvalSpec(input_fn=lstm_eval_input_fn,\n                                       start_delay_secs=30,\n                                       throttle_secs=15,\n                                      steps=None)\n\ntf.estimator.train_and_evaluate(estimator=lstm,\n                                train_spec=lstm_train_spec,\n                                eval_spec=lstm_eval_spec)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2b187ae5f6827b915d44318753245abd7c783d8e"},"cell_type":"markdown","source":"#### Make predictions\n\nAs predictions will be made on a sliding window, it is necessary to aggretate all windows' predictions"},{"metadata":{"trusted":false,"_uuid":"bc779f7e9d544185d5261b358e1fb7abf32d1d9d"},"cell_type":"code","source":"raw_predictions = {'vid_id':[],'is_turkey':[]}\n\nfor prediction in lstm.predict(input_fn=lstm_predict_input_fn):    \n    raw_predictions['vid_id'].append(prediction['vid_id'])\n    raw_predictions['is_turkey'].append(prediction['class_proba'][1])    \n\nraw_predictions_df = pd.DataFrame(raw_predictions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"63efd8f55321a0bf7d5bfc7f0dfe2fba9be82479"},"cell_type":"code","source":"raw_predictions_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"b1d8fe520e8ce8f63b3c84c872d73ca299caebd4"},"cell_type":"code","source":"raw_predictions_df.to_csv('./submission_lstm.csv',index=None,columns=['vid_id','is_turkey'])","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":2},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython2","version":"2.7.13"}},"nbformat":4,"nbformat_minor":1}