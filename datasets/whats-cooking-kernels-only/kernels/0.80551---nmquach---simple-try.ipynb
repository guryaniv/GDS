{"cells":[{"metadata":{"trusted":true,"_uuid":"d67f15306d0e75c4f5fb173fb10797cc95c6775b"},"cell_type":"code","source":"from __future__ import print_function\n\nimport logging\nimport numpy as np\nimport pandas as pd\n\nfrom optparse import OptionParser\nimport sys\nfrom time import time\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom nltk.stem import WordNetLemmatizer\n\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.svm import SVC\n\nfrom sklearn.svm import LinearSVC\nimport xgboost as xgb\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import LinearSVC\nfrom sklearn.linear_model import PassiveAggressiveClassifier\n\nfrom sklearn.utils.extmath import density\nfrom sklearn import metrics\nimport re\n\n# Display progress logs on stdout\nlogging.basicConfig(level=logging.INFO,\n                    format='%(asctime)s %(levelname)s %(message)s')\n\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ac12e42b2ecfb900c251b0fd8e19698e7fb589fb"},"cell_type":"code","source":"np.set_printoptions(precision=6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f724536483ecc23b1fce27a4d0f7ff4ad009f144"},"cell_type":"code","source":"#DATA_DIR='../data/cooking'\nDATA_DIR='../input'\nTRAIN_FILE=DATA_DIR + '/train.json'\nTEST_FILE=DATA_DIR + '/test.json'\nSUBMIT_FILE= 'sample_submission.csv'\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"683f66188173fc3d505ad90b43508d9d657224a9"},"cell_type":"code","source":"def clean_data(X):\n    X['ingredients'] = X['ingredients'].map(lambda l: [x.lower() for x in l])\n    X['ingredients'] = X['ingredients'].map(lambda l: [re.sub(r'\\(\\s*[^\\s]*\\s+oz\\.\\s*\\)\\s*', '', x).strip() for x in l])\n    X['ingredients'] = X['ingredients'].map(lambda l: [x.replace(\"-\", \" \") for x in l])\n    X['ingredients'] = X['ingredients'].map(lambda l: [x.replace(\"half & half\", \"half milk cream\") for x in l])\n    X['ingredients'] = X['ingredients'].map(lambda l: [x.replace(\" & \", \" \") for x in l])\n    X['ingredients'] = X['ingredients'].map(lambda l: [re.sub(r'\\d+%\\s+less\\s+sodium ', '', x).strip() for x in l])\n    # lemmatizer\n    lemmatizer = WordNetLemmatizer()\n    X['ingredients'] = X['ingredients'].map(lambda l: [lemmatizer.lemmatize(x) for x in l])\n    \n    return X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6704142c3a1cfc32796d878c4c87bab8436a4439"},"cell_type":"code","source":"def words_to_text(list_of_words, add_separate_words=True):\n    # words_to_text(df['ingredients'][10], add_separate_words=True)\n    if isinstance(list_of_words, list):\n        l = list_of_words\n    else:\n        l = eval(list_of_words)\n    # concatenates list of words into a text \n    s = ''\n    for i, w_0 in enumerate(l):\n        if i > 0:\n            s = s + ' '\n        w_0 = w_0.strip()\n        w_1 = w_0.replace(' ', '_').replace(',','_vir_') # avoid space cut between words\n        s = s + w_1\n        if add_separate_words and ' ' in w_0:\n            s = s + ' ' + w_0\n    return s\n\ndef vectorize(X_words, sublinear_tf=True, max_df=0.75):\n    print(\"Extracting vectorized feats from the training data using a sparse vectorizer\")\n    t0 = time()\n    vectorizer = TfidfVectorizer(sublinear_tf=True, \n                                 max_df=max_df,\n                                 stop_words='english')\n    X_vectorized = vectorizer.fit_transform(X_words)\n    duration = time() - t0\n    print(\"done in %fs \" % (duration))\n    print(\"n_samples: %d, n_features: %d\" % X_vectorized.shape)\n    print()\n    return X_vectorized, vectorizer, duration\n\n# gets feature_names:\n# vectorizer.get_feature_names()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"44998b53428f99aefd6068cc2e05d24c54babb6c"},"cell_type":"code","source":"# #############################################################################\n# Benchmark classifiers\ndef benchmark(clf, X_train, y_train, X_test, y_test):\n    print('_' * 80)\n    print(\"Training {}: \".format(str(type(clf))))\n    print(clf)\n    t0 = time()\n    clf.fit(X_train, y_train)\n    train_time = time() - t0\n    print(\"train time: %0.3fs\" % train_time)\n\n    t0 = time()\n    y_pred = clf.predict(X_test)\n    test_time = time() - t0\n    print(\"test time:  %0.3fs\" % test_time)\n\n    score = metrics.accuracy_score(y_test, y_pred)\n    print(\"accuracy score:   %0.3f\" % score)\n\n    \"\"\"\n    if hasattr(clf, 'coef_'):\n        print(\"dimensionality: %d\" % clf.coef_.shape[1])\n        print(\"density: %f\" % density(clf.coef_))\n        print(\"top 10 keywords per class:\")\n        target_names=y_encoded_classes_names\n        for i, label in enumerate(target_names):\n            top10 = np.argsort(clf.coef_[i])[-10:]\n            print(\"%s: %s\" % (label, \" \".join(X_column_names[top10])))\n        print()\n    \"\"\"\n\n    cm = metrics.confusion_matrix(y_test, y_pred)\n    # print(metrics.confusion_matrix(y_test, y_pred))\n    # print()\n    clf_descr = str(clf).split('(')[0]\n    ret = {\n        'classifier': clf,\n        'accuracy_score': score, \n        'confusion_matrix': cm, \n        'train_time': train_time, \n        'test_time': test_time, \n        'y_pred': y_pred\n    }\n    return ret","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1ec1b24f1074525de17824d8f65f30e32cb580c3"},"cell_type":"code","source":"import itertools\nnp.set_printoptions(precision=2)\ndef plot_confusion_matrix(cm, classes,\n                          normalize_axis=None, # 0 : sum of each row = 1, 1: sum of columns = 1, None\n                          cmap=plt.cm.Reds # https://matplotlib.org/users/colormaps.html\n                         ):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize_axis is not None:\n        title = \"Normalized confusion matrix (axis={})\".format(normalize_axis)\n        cm = cm.astype('float') / cm.sum(axis=normalize_axis)[:, np.newaxis]\n    else:\n        title = 'Confusion matrix, without normalization'\n    print(title)\n    #print(cm)\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.tick_params(labelsize=22)\n    plt.title(title, fontsize=30)\n    # plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize_axis is not None else 'd'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\",\n                 fontsize=16)\n\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')    \n    plt.tight_layout()\n    return cm # cm normalized\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ae0a0d8ac5d08e61d65ef0e7b2a345f555d5ee3c"},"cell_type":"code","source":"def top_feature_importances(clf, max_feats):\n    if hasattr(clf, 'feature_importances_'):\n        importances = clf.feature_importances_\n        indices = np.argsort(importances)[::-1][:max_feats]\n        names = [X_column_names[i] for i in indices]\n        stds = [np.std([tree.feature_importances_[i] for tree in clf.estimators_])\n               for i in indices]\n        importances = importances[indices]\n        return importances, names, indices, stds\n    if hasattr(clf, 'coef_'):\n        print(\"dimensionality: %d\" % clf.coef_.shape[1])\n        print(\"density: %f\" % density(clf.coef_))\n        print(\"\\ntop 10 most important features per class:\")\n        ret = {}\n        for i, label in enumerate(y_encoded_classes_names):\n            importances = clf.coef_[i][-max_feats:]\n            indices = np.argsort(clf.coef_[i])[-max_feats:]\n            names = [X_column_names[i] for i in indices]\n            print(\"%s :  %s\\n\" % (label, \" \".join(names)))\n            ret[label] = (importances, names, indices)\n        return ret\n    return None\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"841a8f01320629cbae3c9d900b171f111a5f7b36"},"cell_type":"code","source":"# plot feature importances   \ndef plot_top_feature_importances(clf, max_feats=20, class_name=None):\n    # feature importances\n    top = top_feature_importances(clf, max_feats)\n    if isinstance(top, list):\n        # case of tree methods\n        importances, feat_names, indices, stds = top\n    elif isinstance(top, dict):\n        # svm methods\n        importances, feat_names, indices = top[class_name]\n        print('importances lenth: ', len(importances))\n        stds=None\n    \n    # Plot the feature importances of the forest\n    plt.figure(figsize=(20, int(0.6*max_feats)))\n    plt.tick_params(labelsize=13)\n    plt.title(\"{} top feature importances\".format(max_feats), fontsize=18)\n    #plt.bar(range(max_feats), importances, color=\"r\", yerr=stds, align=\"center\")\n    #plt.xticks(range(max_feats), feat_names)\n    plt.barh(range(max_feats), importances, color=\"r\", yerr=stds, align=\"center\")\n    plt.yticks(range(max_feats), feat_names)\n    ax = plt.gca(); ax.invert_yaxis()  # inverse y axis\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ea30fd341bd7540ec51902e81b7755c27937442b"},"cell_type":"markdown","source":"# Load dataset"},{"metadata":{"trusted":true,"_uuid":"2ef328c27d98c117a8236c38aef86f67913f90ca"},"cell_type":"code","source":"df = pd.read_json(TRAIN_FILE, encoding='iso-8859-1')\n# clean data\nclean_data(df)\ndf.head(3)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"aaf02f32ecb178ed8eba058c2fd213b77d39ca81"},"cell_type":"markdown","source":"# Vectorize"},{"metadata":{"trusted":true,"_uuid":"e4f84ee9de69f3a3940c78f61e1847338566a6b0"},"cell_type":"code","source":"X_words = df['ingredients'].map(words_to_text)\nX_words.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9a4d4a549edc0dc1729bc4a2509440a25a58fc8b"},"cell_type":"code","source":"X_vects, Vectorizer_inst, Vectorizer_duration = vectorize(X_words)\nX_column_names = Vectorizer_inst.get_feature_names()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9a0d68c9ebc763d7a979c036166f595ae387c84c"},"cell_type":"markdown","source":"# Encode target"},{"metadata":{"trusted":true,"_uuid":"9588812a045c886c64aa1ff644727e39ca50f19c"},"cell_type":"code","source":"# encode target\nfrom sklearn.preprocessing import LabelEncoder\nencoder = LabelEncoder()\ny_encoded = encoder.fit_transform(df['cuisine'])\n# classe names corresponding to encoded values\n# y_encoded_classes_names = [encoder.inverse_transform(c) for c in range(20)]\ny_encoded_classes_names = encoder.inverse_transform(range(20))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cc7a83dc14a424a9cff29da4f64358071f2db2fb"},"cell_type":"markdown","source":"# Split dataset"},{"metadata":{"trusted":true,"_uuid":"67ea293a0bf88c857580535447a3640f0f8df423"},"cell_type":"code","source":"# using scipy.sparse.csr.csr_matrix to speed up training\nfrom sklearn.model_selection import train_test_split\nXs_train, Xs_test, y_train, y_test = train_test_split(X_vects, y_encoded, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1b8580e16421ac15bdb914680972edb7278ac981"},"cell_type":"markdown","source":"# Best Estimator"},{"metadata":{"trusted":true,"_uuid":"1c1103b7b08dd1b733b9ff05cae4a687859ed49b"},"cell_type":"code","source":"BEST_PARAMS = {\n    'C': 80,\n    'kernel': 'rbf',\n    'gamma': 1.7,\n    'coef0': 1,\n    'cache_size': 500\n}\nBEST_ESTIMATOR = OneVsRestClassifier(\n        SVC(**BEST_PARAMS))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8b5412d1eaafe8efbf0503854905ea7abbfdb6aa"},"cell_type":"code","source":"RESULT = benchmark(BEST_ESTIMATOR, Xs_train, y_train, Xs_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0b909e0a3f8e7ec7a3877a6122a578ea537cdf3f"},"cell_type":"code","source":"cm = RESULT['confusion_matrix']\nplt.figure(figsize=(18,18))\ncm_norm = plot_confusion_matrix(cm, \n                      classes=y_encoded_classes_names,\n                      normalize_axis=None, # unnormalized\n                      cmap=plt.cm.RdPu)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1005dff4c405ff9f46e64ae800498a71547a715e"},"cell_type":"code","source":"# Show what's wrong\npd.set_option('display.max_colwidth', 120)\n\ny_pred = RESULT['y_pred']\nX_test_words = Vectorizer_inst.inverse_transform(Xs_test)\ny_test_class = encoder.inverse_transform(y_test)\ny_test_pred_class = encoder.inverse_transform(y_pred)\ndf_check = pd.DataFrame({\n    'TRUE_CLASS': y_test_class,\n    'PRED_CLASS': y_test_pred_class,\n    'ingredients': X_test_words,\n     })\ndf_ko = df_check[df_check['TRUE_CLASS'] != df_check['PRED_CLASS']]\nprint('Some misclassfied cases:')\ndf_ko.sample(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4e0b56bee3b3cf227c825f1e22698df03e142c91"},"cell_type":"code","source":"df_ko[(df_ko['PRED_CLASS'] == 'italian') & (df_ko['TRUE_CLASS'] == 'french')].head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"48ebb9ddcee2135e8e4114947e11589922a494b6"},"cell_type":"code","source":"df_ko[(df_ko['PRED_CLASS'] == 'french') & (df_ko['TRUE_CLASS'] == 'italian')].head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"716e0548c1cb35b1906668c19ae3c32b58b2b84a"},"cell_type":"code","source":"df_ko[(df_ko['PRED_CLASS'] == 'southern_us') & (df_ko['TRUE_CLASS'] == 'indian')]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"043c3ff1769374f10543da86a3d1343478941cf9"},"cell_type":"markdown","source":"# Submission"},{"metadata":{"trusted":true,"_uuid":"06f768fe7a1874a3c43c57bc06194650c0cc01d4"},"cell_type":"code","source":"# read dataset for submission\ndf_test_subm = pd.read_json(TEST_FILE, encoding='iso-8859-1')\ndf_test_subm.set_index('id', inplace=True)\ndf_test_subm.head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bac029003bd9a1e04f056ac2451663be19b41360"},"cell_type":"code","source":"# clean data\nclean_data(df_test_subm)\n\nX_test_subm_words = df_test_subm['ingredients'].map(words_to_text)\nX_test_subm_vects = Vectorizer_inst.transform(X_test_subm_words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"db9e7e040acc9135144e974fbf66b65267292945"},"cell_type":"code","source":"y_test_subm_encoded = BEST_ESTIMATOR.predict(X_test_subm_vects)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"308dded84f2dcd9b16b42911a783bfc4ed7f4d08"},"cell_type":"code","source":"# Decode class names\ny_test_subm = [y_encoded_classes_names[i] for i in y_test_subm_encoded]\ny_test_subm[:3]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"980f23195717762b8c534c964680a698716b6ca3"},"cell_type":"code","source":"df_test_subm['cuisine'] = y_test_subm\ndf_test_subm.head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e9ce8f727a3eacce393904bb38322bb9c7cf7d58"},"cell_type":"code","source":"df_test_subm['cuisine'].to_csv(SUBMIT_FILE, header=True)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.4"}},"nbformat":4,"nbformat_minor":1}