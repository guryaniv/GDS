{"cells":[{"metadata":{"_uuid":"336eb071e6e09c21acb2c7068f13cc0535a43af2"},"cell_type":"markdown","source":"\n# Introduction\nTitanic is a very interesting dataset; it contains only 891 rows in its training dataset. This almost means that for all the proposed features and ML models, we must pay extra attention to overfitting. We need to find the simplest model that generalizes well. In fact, my best public score comes from my lowest cross validation accuracy. All of my previous submissions had much higher cross validation scores, which meant they were all overfitted.\n\nFor exploring the Titanic dataset, I found the following kernels useful:\n* [Divide and Conquer](https://www.kaggle.com/pliptor/divide-and-conquer-0-82296)\n* [Exploring Survival on the Titanic](https://www.kaggle.com/mrisdal/exploring-survival-on-the-titanic)\n\nIn this kernel, the contribution has two-fold:\n1. I introduce a clustered-based feature related to ticket number, assuming its ticket system resembles how we book the ticket for an airplane.\n2. I try to simplify features by inspecting their distributions in order to reduce the model complexity.\n\nHere we go!"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","scrolled":true,"trusted":true},"cell_type":"code","source":"# Standard library import for python.\n\nimport numpy as np \nimport pandas as pd \n\nimport os\n\nimport seaborn as sns\n\nfrom matplotlib import pyplot as plt\nfrom scipy.cluster.hierarchy import dendrogram, linkage\nfrom scipy.cluster.hierarchy import fcluster\n\nfrom sklearn.cluster import FeatureAgglomeration\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.metrics import f1_score, accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import LinearSVC\nfrom sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.model_selection import GridSearchCV\n\n# Let's make sure data source files are there.\nprint(os.listdir(\"../input\"))","execution_count":1,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","scrolled":true,"trusted":true},"cell_type":"code","source":"# Load in the train and test datasets from the CSV files.\ntrain = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\ndata = pd.concat([train, test])\ntrain.shape","execution_count":3,"outputs":[]},{"metadata":{"_cell_guid":"9e111009-7933-423c-aab5-4ac2de08846f","_uuid":"6c2d56373a9d4743b172ee438bcc9a9057174692"},"cell_type":"markdown","source":"# Feature Preprocessing and Engineering\nWhen looking at the training dataset, it only contains 891 rows, which is a very small dataset. This almost means that this competition almost always will need to fight against the overfitting. If we have lots of data, algorithm such as GBDT would help us find a good split and thus no need to bucketize the features. However, since the training dataset is so small, the strategy is to find a **simplest** model which has lowest validation error.\n\n\n### Here we mainly doing the following:\n1. Handling missing data.\n2. Categorical feature encoding. \n3. Extracting features from texts. "},{"metadata":{"_cell_guid":"4f087c9c-6ddc-47c3-a894-332d9a3d4207","_uuid":"609bf1d0ff30ec98e7fa8d58c12a355e321fb43d","collapsed":true,"trusted":true},"cell_type":"code","source":"sex_label = LabelEncoder()\ncabin_label = LabelEncoder()\nembarked_label = LabelEncoder()\nfamily_name_label = LabelEncoder()\ntitle_label = LabelEncoder()\ntitle_remap_label = LabelEncoder()\n\ndata['Sex_Code'] = sex_label.fit_transform(data.Sex)\ndata['Cabin_Prefix'] = data.Cabin.str.get(0).fillna('Z')\ndata['Cabin_Code'] = cabin_label.fit_transform(data.Cabin.str.get(0).fillna('Z'))\ndata['Has_Cabin'] = (data.Cabin.str.get(0).fillna('Z') != 'Z').astype('int32')\ndata['Embarked_fillZ'] = data.Embarked.fillna('Z')\ndata['Embarked_Code'] = embarked_label.fit_transform(data.Embarked.fillna('S')) # 'S' has highest occurrence. \ndata['FamilySize'] = data.Parch + data.SibSp + 1\ndata['BigFamily'] = data.FamilySize.apply(lambda s: s if s < 5 else 5)\ndata['IsAlone'] = data.FamilySize == 1\ndata['FamilyName'] = data.Name.str.extract('(\\w+),', expand=False)\ndata['FamilyName_Code'] = family_name_label.fit_transform(data.FamilyName)\ndata['Title'] = data.Name.str.extract('([A-Za-z]+)\\.', expand=False)\nmapping = {\n    'Mlle': 'Miss',\n    'Ms': 'Miss', \n    'Dona': 'Mrs',\n    'Mme': 'Miss',\n    'Lady': 'Mrs', \n    'Capt': 'Honorable', \n    'Countess': 'Honorable', \n    'Major': 'Honorable', \n    'Col': 'Honorable', \n    'Sir': 'Honorable', \n    'Don': 'Honorable',\n    'Jonkheer': 'Honorable', \n    'Rev': 'Honorable',\n    'Dr': 'Honorable'\n}\ndata['Title_Remap'] = data.Title.replace(mapping)\ndata['Title_Code'] = title_label.fit_transform(data.Title)\ndata['Title_Remap_Code'] = title_remap_label.fit_transform(data.Title_Remap)\ndata.Age = data.Age.fillna(data.Age.median())\ndata.Fare = data.Fare.fillna(data.Fare.median())","execution_count":4,"outputs":[]},{"metadata":{"_cell_guid":"6d8d89d5-bd73-4b95-b38d-2996816e89be","_uuid":"5135cda38675ba6459c3138c5a1537bb98399c8d","trusted":true},"cell_type":"code","source":"data.head(3)","execution_count":5,"outputs":[]},{"metadata":{"_cell_guid":"53eb0755-060e-4865-ac49-5b1d5d9077dc","_uuid":"7cc8c0798a8ca429f5e335bfcc7cf466341d80f3"},"cell_type":"markdown","source":"# Feature Exploration\nThe purpose of feature exploration is to gain the insight of the engineered features, and thus may be able to help us come up with better feature or drop unuseful features.\n\nHere is the utility function which I draw the survival rate conditioned on feature numbers, and also plot their distribution with histogram. "},{"metadata":{"_cell_guid":"482a71e4-a256-437b-b311-c58806e250da","_uuid":"6e250fd55e2eaf3d99258cc449651a174c6631c5","collapsed":true,"trusted":true},"cell_type":"code","source":"def inspect_feature_plot(data, feat):\n    fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(20, 6))\n    sns.barplot(data[feat], data.Survived, ax=ax1)\n    sns.countplot(data[feat], ax=ax2)","execution_count":9,"outputs":[]},{"metadata":{"_uuid":"2cbb155a6e626fc693f48395a7a4c96ca9e0e77d"},"cell_type":"markdown","source":"# Sex\nIt's not a secret that females were more likely to be rescued in Titanic, and thus a very good feature candidate."},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"99837c87db81b54461217f5a94bdce3b9e4b2dbf"},"cell_type":"code","source":"inspect_feature_plot(data, 'Sex')","execution_count":10,"outputs":[]},{"metadata":{"_cell_guid":"900224ec-3250-4f40-9b25-f6b5b3231e6c","_uuid":"ca5a687201409e874eb49cb30eae6c55bb94b9b0"},"cell_type":"markdown","source":"## Cabin\nHere we have used Cabin prefix as features; the original idea was assuming that Cabin prefix could have mapped the corresponding location of the ship, and there determined the survival rate. However, the following chart shows **Cabin** prefix is too granular, and may suffer from overfitting. For example, 'G', 'A', 'T' are too few to be statistically meaningful. Instead, **Has_Cabin** looks like a better and simpler feature."},{"metadata":{"_cell_guid":"2d9ae877-d2f0-4da5-b7cf-b575f8ab39c3","_uuid":"058cc0273859839239e3c3a065573923904650c3","scrolled":false,"trusted":true},"cell_type":"code","source":"inspect_feature_plot(data, 'Cabin_Prefix')","execution_count":13,"outputs":[]},{"metadata":{"_cell_guid":"d59e26e2-6cdf-4f9f-bcb6-8f2213e87e16","_uuid":"fc7e816090a83f20412d68fcc6de7efc871811a0","scrolled":true,"trusted":true},"cell_type":"code","source":"inspect_feature_plot(data, 'Has_Cabin')","execution_count":14,"outputs":[]},{"metadata":{"_cell_guid":"b0285f91-573a-4049-aa10-1030a33a7352","_uuid":"66bd402797f601d31a055342af3c2d824f58bf76"},"cell_type":"markdown","source":"## Embarked\nVery few row has null 'Embarked', and those entries turned out to be survived. It is better to fill null with 'S', which occurred the frequently (and thus more likely to be guessed correctly when this feature is missing)."},{"metadata":{"_cell_guid":"d86d8e60-70ff-4469-a291-7b40446006a0","_uuid":"4425ccb23cbf6d0f2f3da546ac22c556027b3d48","scrolled":false,"trusted":true},"cell_type":"code","source":"inspect_feature_plot(data, 'Embarked_fillZ') ","execution_count":16,"outputs":[]},{"metadata":{"_cell_guid":"d4b47fb2-7369-4273-8de9-9dff5c865b8a","_uuid":"d339fb0b48a064aa72f292518a5cbbf4fc3e8e19"},"cell_type":"markdown","source":"## Family - Family Size, Family Name, Family Survived\nFamily size looks like a good feature, but the number of big family is few, and thus let's squash it to create another feature **BigFamily** which squashes >=5, i.e. [1, 2, 3, 4, >=5], which is more granular than **IsAlone**. \n\n**FamilySurvived** feature gave an estimate that if you know one of your family member survives, how likely you'll survive? This assumes family tends to move together."},{"metadata":{"_cell_guid":"6cc130de-e439-467a-97c2-9f19caa009f8","_uuid":"10f7e74670da4c0bf7c51e1292214c214f6acd51","scrolled":true,"trusted":true},"cell_type":"code","source":"inspect_feature_plot(data, 'FamilySize') ","execution_count":21,"outputs":[]},{"metadata":{"_cell_guid":"a91634bd-ef45-4f94-8e9c-d758572ccaf1","_uuid":"d747fa8c86dc8b92e4f9ac8da93dfbaba7479f6c","scrolled":true,"trusted":true},"cell_type":"code","source":"inspect_feature_plot(data[:891], 'BigFamily')\nprint(data[['PassengerId', 'BigFamily']].groupby('BigFamily').count().rename(columns={'PassengerId': 'Number'}))","execution_count":22,"outputs":[]},{"metadata":{"_cell_guid":"d13e675a-3176-445b-8586-071d96be879a","_uuid":"c83461aa46c5909ae6e45a507d8e43f89eedcb43","scrolled":true,"trusted":true},"cell_type":"code","source":"for feat in ('IsAlone', 'Parch', 'SibSp'):\n    inspect_feature_plot(data[:891], feat)","execution_count":23,"outputs":[]},{"metadata":{"_cell_guid":"11a19ee7-7951-41ca-8765-86f29b379695","_uuid":"a0c2f29a19ad2cfcd4d5f77d4698b23500ecb3b6","collapsed":true,"scrolled":false,"trusted":true},"cell_type":"code","source":"m = data[['FamilyName', 'Survived']].groupby('FamilyName').max()\nc = data[['FamilyName', 'PassengerId']].groupby('FamilyName').count()\nm = m.rename(columns={'Survived': 'FamilySurvived'})\nc = c.rename(columns={'PassengerId': 'FamilyMemberCount'})\nm = m.where(m.join(c).FamilyMemberCount > 1, other=-1, axis=1).fillna(-1).join(c)\nm.FamilySurvived = m.FamilySurvived.astype('int32')\n\njoined_data = data.join(m, on='FamilyName')","execution_count":24,"outputs":[]},{"metadata":{"_cell_guid":"aa149dbd-b982-4c49-8436-a3ef52273b95","_uuid":"fa44bf735a350b396dec1450ef4531ea72cec655"},"cell_type":"markdown","source":"### Family Survived\nFamily tends to move together, and thus, if one of the family member survivied, and how does that impact the person being survived? We fill with -1, if the family member count <=1, meaning, we don't know much about that family, and hence we try to tell the model little information when the member count is low."},{"metadata":{"_cell_guid":"fa43c4fc-8d7f-4ee7-b7f3-cb24df2081e2","_uuid":"410ea441c0edd9fccef84ff0c455440e79a7d320","trusted":true},"cell_type":"code","source":"inspect_feature_plot(joined_data[:891], 'FamilySurvived')","execution_count":30,"outputs":[]},{"metadata":{"_cell_guid":"a4a2cc98-0628-4302-9131-6a0b36e9baf0","_uuid":"3df0154f04d0d05be1a51576875aeedd9675b033","trusted":true},"cell_type":"code","source":"inspect_feature_plot(joined_data[:891], 'FamilyMemberCount')","execution_count":31,"outputs":[]},{"metadata":{"_cell_guid":"8c50bcea-69c7-4652-8e49-897246909827","_uuid":"3dec11e895f8f3d8b1da0b5b82f5452d9f3cc195"},"cell_type":"markdown","source":"# Title\nTitle are extracted from the name of the passenger. There are many rare titles which tell us little information. Thus, we'll need to re-map them. Out of rare titles, there are two categories: **foreign language** and **honorable titles**. For foreign language such as *Mlle*, we'll remap it into *Ms* (English), where notable titles like Rev, we remap them into *Honorable*."},{"metadata":{"_cell_guid":"ed183498-227b-4b48-a852-3a981a68d898","_uuid":"25f44443df1f91002c2ad4d4ed4efb9d303524bf","trusted":true},"cell_type":"code","source":"for feat in ('Title', 'Title_Remap'):\n    inspect_feature_plot(data, feat)\n    print(data[['PassengerId', feat]].groupby(feat).count().rename(columns={'PassengerId': 'Number'}))","execution_count":32,"outputs":[]},{"metadata":{"_cell_guid":"6320eccb-941b-42d1-9618-ee51ea28b059","_uuid":"d3a8a7c88d2351ac39cb3484977b1e59d9c78123"},"cell_type":"markdown","source":"# Pclass\nPclass is a feature that can capture if the passenger is a rich person. As shown below, the fare is much more expensive in class 1 than class 3. In class 3, Fare distribution looks non-separable for survival, whereas class 1, and 2 seems to be a good predictor, and hence a good feature candidate."},{"metadata":{"_cell_guid":"4155da02-4662-4c4e-96bc-fde167b35604","_uuid":"4de7dbe9b16974ec8ae0dbecc799400515088f86","scrolled":true,"trusted":true},"cell_type":"code","source":"feat = 'Pclass'\ninspect_feature_plot(joined_data, feat)\nprint(data[['PassengerId', feat]].groupby(feat).count().rename(columns={'PassengerId': 'Number'}))","execution_count":33,"outputs":[]},{"metadata":{"_cell_guid":"d27ce850-abe5-439a-8dbb-9570c37f72d3","_uuid":"8960b07f39a59fea2067ea8ae2df5ac57ece6584"},"cell_type":"markdown","source":"### Price distribution for Pclass"},{"metadata":{"_cell_guid":"d4c6d9ff-2cd0-4248-a5c5-449b435110ac","_uuid":"36002e27ea01ade8646d27348b15a657ebd9afdf","scrolled":false,"trusted":true},"cell_type":"code","source":"facet = sns.FacetGrid( data, hue = 'Survived' , row = 'Pclass', aspect = 8)\nfacet.map( sns.distplot , 'Fare' )\nfacet.set( xlim=( 0 , data[ 'Fare' ].max() ) )\nfacet.add_legend()","execution_count":34,"outputs":[]},{"metadata":{"_cell_guid":"c49514f6-4c65-4449-aa3b-d9a6241273bc","_uuid":"92be4824393c730cf3d053039226ed64e98f407b"},"cell_type":"markdown","source":"## Ticket Number\nMost ticket system uses serial number. Here we assume that if ticket number is close to one another, then they are probably bought around the same time. Note that for those tickets bought around the same time, their location in ship would be likely to be close as well, and hence we're doing **hierachical clustering** of ticket number. We assign the cluter number as the new feature."},{"metadata":{"_cell_guid":"e6163349-7ad2-4344-a22d-818dbb0142d1","_uuid":"6c3934ce116f2f1aa6d3110b02dd38a4757b0c02","trusted":true},"cell_type":"code","source":"ticket = data.Ticket.str.extract('(\\d+$)', expand=False).fillna(0).astype(int).ravel()\nn_cluster = []\nfor max_d in range(1,201,2):\n    Z = linkage(ticket.reshape(data.shape[0], 1), 'single')\n    clusters = fcluster(Z, max_d, criterion='distance')\n    data['Ticket_Code'] = clusters\n    n_cluster.append( data.Ticket_Code.unique().shape[0] )\n\nlen(n_cluster)\nd = pd.concat([pd.Series(n_cluster, name=\"Cluster_Count\", dtype='int32'),\n               pd.Series(range(1,201,2), name=\"Distance_Threshold\", dtype='int32')], axis=1)\nsns.regplot('Distance_Threshold', 'Cluster_Count', d)\n","execution_count":35,"outputs":[]},{"metadata":{"_cell_guid":"8f7e79f1-cb8a-4a59-b72f-e74aeaea4476","_uuid":"2708bd524f0c3a640edb05f28d1619316b929bb6","collapsed":true,"scrolled":false,"trusted":true},"cell_type":"code","source":"optimal_d = 20\nZ = linkage(ticket.reshape(data.shape[0], 1), 'single')\nclusters = fcluster(Z, optimal_d, criterion='distance')\njoined_data['Ticket_Code'] = clusters","execution_count":36,"outputs":[]},{"metadata":{"_cell_guid":"c27ceaab-839a-4264-b4b3-3604f586f97f","_uuid":"a611b3b376f0739a16b65cf1bd899c36eb34d2eb"},"cell_type":"markdown","source":"### Ticket Number Remap\nSmall ticket number cluster doesn't tell us too much information, let's squash them to reduce the complexity."},{"metadata":{"_cell_guid":"fd4d189c-c0d7-4bfe-820a-84db0a9b1e5f","_uuid":"5acc3053b59a87460fe740aeaa4fc22533f08dc0","trusted":true},"cell_type":"code","source":"import itertools\ncount = joined_data[['PassengerId', 'Ticket_Code']].groupby('Ticket_Code').count().rename(columns={'PassengerId': 'Number'})\njoined_data['Ticket_Code_Remap'] = joined_data.Ticket_Code.replace(dict(zip(count.index[count.Number <= 10], itertools.cycle([0]))))\n\nfor feat in ('Ticket_Code_Remap', 'Ticket_Code'):\n    inspect_feature_plot(joined_data, feat)","execution_count":38,"outputs":[]},{"metadata":{"_cell_guid":"5bac369d-ef06-40a2-a48b-7483aa0e6a91","_uuid":"48999033cda38e350cfe4e59c304a79b6db63434"},"cell_type":"markdown","source":"### Ticket Number Cluster\nLet's inspect intersting ticket clusters.\n\n* Deadly ticket clusters: \n  * Cluster 89. Looks like Sage family is all gone. :(\n  * Cluster 186. Pclass 3 - economic class, mostly single person.\n* Highly survived ticket cluster: 127. Only one man died."},{"metadata":{"_cell_guid":"b683c5da-f011-480e-9d6f-a73a886b28ad","_uuid":"05cec4253df58509ba6e59f08ca5231f7751d0aa","trusted":true},"cell_type":"code","source":"joined_data[['FamilyName',\n      'Name',\n      'Age', \n      'Fare', \n      'BigFamily', \n      'Pclass',\n      'Has_Cabin',\n      'Embarked',\n      'Sex', \n      'Title',\n      'Ticket_Code',\n      'Ticket_Code_Remap',\n      'Survived']][joined_data.Ticket_Code==89].sort_values(by='FamilyName')","execution_count":58,"outputs":[]},{"metadata":{"_cell_guid":"f8efca1d-dc89-48cf-82ea-cd6fc4389157","_uuid":"900409d5302c853aaec0351c1259a0a0d5851a42","trusted":true},"cell_type":"code","source":"joined_data[['FamilyName',\n      'Name',\n      'Age', \n      'Fare', \n      'BigFamily', \n      'Pclass',\n      'Has_Cabin',\n      'Embarked',\n      'Sex', \n      'Title',\n      'Ticket_Code',\n      'Ticket_Code_Remap',\n      'Survived']][joined_data.Ticket_Code==186].sort_values(by='FamilyName')","execution_count":59,"outputs":[]},{"metadata":{"_cell_guid":"98dd5015-adc3-4429-b556-0ac0d54792cc","_uuid":"0c39be43100c752d8399e7afe37b6726d3fed2b8","trusted":true},"cell_type":"code","source":"joined_data[['FamilyName',\n      'Name',\n      'Age', \n      'Fare', \n      'BigFamily', \n      'Pclass',\n      'Has_Cabin',\n      'Embarked',\n      'Sex', \n      'Title',\n      'Ticket_Code',\n      'Ticket_Code_Remap',\n      'Survived']][joined_data.Ticket_Code==127].sort_values(by='FamilyName')","execution_count":60,"outputs":[]},{"metadata":{"_cell_guid":"d22bafdf-ce35-49e1-a9c7-98d7aac9e8e0","_uuid":"396481c2e7c83ebf969e4fcb4c49349a3dfe66a3","trusted":true},"cell_type":"code","source":"selected_features = ['Age', \n                     'Fare', \n                     'BigFamily', \n                     'Pclass',\n                     'Has_Cabin',\n                     'Embarked_Code',\n                     'Sex_Code', \n                     'Title_Remap_Code',\n                     'Ticket_Code_Remap',\n                     'FamilySurvived',\n                    ]\none_hot_features = ['Pclass',\n                    'BigFamily',\n                    'FamilySurvived',\n                    'Embarked_Code',\n                    'Title_Remap_Code',\n                    'Ticket_Code_Remap',\n                   ]\nselected_data = joined_data[selected_features]\nprint('Does the following feature contain any NaN? ')\nfor f in selected_features:\n    print('%s: %s' % (f, repr(selected_data[f].isna().any())))","execution_count":42,"outputs":[]},{"metadata":{"_cell_guid":"6a59e04f-353f-4e9d-b06a-da65f5a2d71e","_uuid":"f3f8b6db8d72746497bcf52bce0b32bde66c7750","collapsed":true,"trusted":true},"cell_type":"code","source":"selected_data_one_hot = pd.get_dummies(selected_data,\n                                       columns = one_hot_features)\nrescaling_features = ['Age', 'Fare']\nstd_scaler = StandardScaler()\nfor f in rescaling_features:\n    selected_data_one_hot[f] = std_scaler.fit_transform(selected_data_one_hot[f].values.reshape(-1, 1))\n\ntrain_x = selected_data[:train.shape[0]]\ntest_x = selected_data[train.shape[0]:]\n\ntrain_x_one_hot = selected_data_one_hot[:train.shape[0]]\ntest_x_one_hot = selected_data_one_hot[train.shape[0]:]\ntrain_y = data[:train.shape[0]].Survived","execution_count":43,"outputs":[]},{"metadata":{"_cell_guid":"667dad04-948c-4eeb-8dc3-6e36805984ae","_uuid":"091c449ec588c83e086d1dbb50f6f44bf3728f7d","scrolled":true,"trusted":true},"cell_type":"code","source":"parameters = {'n_estimators': [10,50,100,200],\n              'learning_rate': [0.05, 0.1],\n              'max_depth': [2,3,4],\n              'min_samples_leaf': [2,3],\n              'verbose': [0]}\n\ngrid_obj = GridSearchCV(GradientBoostingClassifier(), parameters, scoring = 'roc_auc', cv = 4, n_jobs = 4, verbose = 1)\ngrid_obj = grid_obj.fit(train_x, train_y)\ngb = grid_obj.best_estimator_              \ngb","execution_count":44,"outputs":[]},{"metadata":{"_cell_guid":"4d4c0ba7-5ebb-4940-90b7-1b485306f127","_uuid":"a0b652780cad4450992a63ae7cc90751fade24aa","collapsed":true,"trusted":true},"cell_type":"code","source":"model = gb.fit(train_x, train_y)\npred_y = gb.predict(train_x)\nf1 = f1_score(train_y, pred_y)\nacc = accuracy_score(train_y, pred_y)","execution_count":45,"outputs":[]},{"metadata":{"_cell_guid":"11cd6f25-e420-4189-bfe9-c977fe0608a4","_uuid":"c01984a1461098daa8ae0613cfbdc8386b27ab08","trusted":true},"cell_type":"code","source":"f1","execution_count":46,"outputs":[]},{"metadata":{"_cell_guid":"32c81a5e-e1c5-4e96-baa3-7f8d5994c4bd","_uuid":"27279d9fcf7836ec5c437db57be881a337f84b8c","trusted":true},"cell_type":"code","source":"acc","execution_count":47,"outputs":[]},{"metadata":{"_cell_guid":"509cac0a-0780-4003-bc2f-a13f81472f44","_uuid":"a2d3ba83b02f63b7368393df8d3d320290aee150","collapsed":true,"trusted":true},"cell_type":"code","source":"test_y = pd.Series(gb.predict(test_x), name=\"Survived\", dtype='int32')\nresults = pd.concat([data[train.shape[0]:].PassengerId, test_y], axis=1)","execution_count":48,"outputs":[]},{"metadata":{"_cell_guid":"a53d5d2e-fc13-435f-ae37-81952cc30cff","_uuid":"30c1f6cd7c92357a01daa20393a1d384e735c79a","collapsed":true,"trusted":true},"cell_type":"code","source":"results.to_csv(\"gbdt_csv_to_submit.csv\",index=False)","execution_count":49,"outputs":[]},{"metadata":{"_uuid":"7b92d39fe2595c0761f8c3959d689f9b7c80d261"},"cell_type":"markdown","source":"### Feature importance\nThis gives us estimates that how well our engineered features look like for a GBDT."},{"metadata":{"_cell_guid":"2c6613ed-7452-48a5-a1e9-9e32e9d8022b","_uuid":"a9869c1aae35fc9951d7927f6827baaf7d1df2c0","scrolled":true,"trusted":true},"cell_type":"code","source":"feat_importance = list(zip(train_x.columns.values, gb.feature_importances_))\nfeat_importance.sort(key=lambda x:x[1])\nfeat_importance","execution_count":51,"outputs":[]},{"metadata":{"_uuid":"b920205f251214c2a2eec1d684ab266aadd33b44"},"cell_type":"markdown","source":"# Iterating the Models\nHere, we try to make the iterations efficiently by abstracting the comparsion with a training config. So that we can easily add a new comparision without writing codes."},{"metadata":{"_cell_guid":"fca05772-57be-4feb-9fe8-42d1a5d10af5","_uuid":"e5f35015505f0e4eb21f4c82e31a888b0bdaf2d0","collapsed":true,"trusted":true},"cell_type":"code","source":"training_config = {\n    'gbdt': {\n        'clf': GradientBoostingClassifier(),\n        'parameters': {\n            'n_estimators': [10,50,100,200],\n            'learning_rate': [0.05, 0.1],\n            'max_depth': [2,3,4],\n            'min_samples_leaf': [2,3],\n        },\n        'n_jobs': 4,\n        'one_hot': False\n    },\n    'logit' : {\n        'clf': LogisticRegression(),\n        'parameters': {\n            'penalty': ['l1', 'l2'],\n            'C': list(np.arange(0.5, 8.0, 0.1))\n        }\n    },\n    'svm': {\n        'clf': LinearSVC(),\n        'parameters': {\n            'penalty': ['l2'],\n            'loss': ['hinge', 'squared_hinge'],\n            'C': list(np.arange(0.5, 8.0, 0.1))\n        }\n    },\n    'rf': {\n        'clf': RandomForestClassifier(),\n        'parameters': {\n            'n_estimators': [10,50,100,200],\n            'criterion': ['gini', 'entropy'],\n            'max_depth': [2,3,4],\n            'min_samples_leaf': [2,3],\n        },\n        'n_jobs': 4,\n        'one_hot': False\n    },\n    'ada': {\n        'clf': AdaBoostClassifier(),\n        'parameters': {\n            'n_estimators': [10,50,100,200],\n            'learning_rate': [0.05, 0.1, 0.5, 1.0, 2.0],\n        },\n        'n_jobs': 4,\n        'one_hot': False\n    }\n}\n\n# Change the following line if you only want to re-run subset of experiments\nexp_to_run = training_config.keys()","execution_count":52,"outputs":[]},{"metadata":{"_cell_guid":"1b8bf0ec-3381-490f-ac0f-6faec6d10ebe","_uuid":"20d3ba0c4d1a70ec64693458a0b20f5c2c342b81","scrolled":false,"trusted":true},"cell_type":"code","source":"results = { 'name': [], 'f1': [], 'accuracy': [] }\ntrain_pred = {}\ntest_pred = {}\nfor name in exp_to_run:\n    conf = training_config[name]\n    clf = conf['clf']\n    parameters = conf['parameters']\n    n_jobs = conf.get('n_jobs', 1)\n    one_hot = conf.get('one_hot', True)\n\n    print('=' * 20)\n    print('Starting training:', name)\n    grid_obj = GridSearchCV(clf, parameters, scoring = 'roc_auc', cv = 4, n_jobs = n_jobs, verbose = 1)\n    train_X = train_x_one_hot if one_hot else train_x\n    \n    print('Number of Features:', train_X.columns.shape[0])\n    grid_obj = grid_obj.fit(train_X, train_y)\n    best_clf = grid_obj.best_estimator_ \n    \n    print('Best classifier:', repr(best_clf))\n    model = best_clf.fit(train_X, train_y)\n    pred_y = best_clf.predict(train_X)\n    train_pred[name] = pred_y\n\n    f1 = f1_score(train_y, pred_y)\n    acc = accuracy_score(train_y, pred_y)\n    results['name'].append(name)\n    results['f1'].append(f1)\n    results['accuracy'].append(acc)\n    \n    test_X = test_x_one_hot if one_hot else test_x\n    test_y = pd.Series(best_clf.predict(test_X), name=\"Survived\", dtype='int32')\n    test_pred[name] = test_y\n    output = pd.concat([test.PassengerId, test_y], axis=1)\n    \n    output_filename = name + \"_csv_to_submit.csv\"\n    print('Writing submission file:', output_filename)\n    output.to_csv(output_filename, index=False)\n","execution_count":53,"outputs":[]},{"metadata":{"_cell_guid":"aba67e82-691d-46e8-832e-62862508c248","_uuid":"0216ccf0b58c4def20d1193ec45571539b16889c","trusted":true},"cell_type":"code","source":"# Hard voting classifier\npred_y = pd.DataFrame.from_dict(train_pred).mean(axis=1) > 0.5\nf1 = f1_score(train_y, pred_y)\nacc = accuracy_score(train_y, pred_y)\nresults['name'].append('voting')\nresults['f1'].append(f1)\nresults['accuracy'].append(acc)\n\ntest_y = pd.Series(pd.DataFrame.from_dict(test_pred).mean(axis=1) > 0.5,\n                   name=\"Survived\", \n                   dtype='int32')\noutput = pd.concat([test.PassengerId, test_y], axis=1)\noutput_filename = 'voting_csv_to_submit.csv'\nprint('Writing submission file:', output_filename)\noutput.to_csv(output_filename, index=False)","execution_count":54,"outputs":[]},{"metadata":{"_uuid":"009e3c2e8813cc73fc8c4c72bb0c45ee468293b0"},"cell_type":"markdown","source":"# Results\nHere is a table that compares its accuracy and f1 score with different algorithm."},{"metadata":{"_cell_guid":"dea924b4-79a8-40dd-9dea-dcdcfbe81db6","_uuid":"a3c98e900ea0cdd88fbd9d587700fd2c5a434461","trusted":true,"scrolled":true},"cell_type":"code","source":"pd.DataFrame.from_dict(results)","execution_count":57,"outputs":[]},{"metadata":{"_cell_guid":"8bf0da67-ce0a-40c7-a494-b56bbd49e3cb","_uuid":"5273a6ebf421e34ea024c80c5b6c8d4f2eeb1a99","collapsed":true,"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}