{"cells":[{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom itertools import islice\nimport textwrap\nfrom sklearn.model_selection import train_test_split\n\n\nwrapper = textwrap.TextWrapper(initial_indent='', width=70,\n                               subsequent_indent=' '*3)\n\nimport nltk\nnltk.download('wordnet')\nnltk.download('punkt')\nnltk.download('stopwords')\nnltk.download('averaged_perceptron_tagger')\nnltk.download('vader_lexicon')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f627dfd19139c6670994136a4bc148ef1cb4e776"},"cell_type":"code","source":"train_df = pd.read_csv('../input/train.csv')\ntest_df = pd.read_csv('../input/test.csv')\n\ntext_column = 'text'\nlabel = 'author'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3066473c011150bbd947c91928bd4806f0972526"},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a7c5279b3da9d290a840cc37f332b2f32701590a"},"cell_type":"markdown","source":"### 3.3. Create features based on each author's word frequency"},{"metadata":{"_uuid":"239996eb12f3b0af267210413d01a006e439c2e7"},"cell_type":"markdown","source":"### 3.3.0. Import packages"},{"metadata":{"trusted":true,"_uuid":"ce3015546f72e043419164becce32f752146e37c"},"cell_type":"code","source":"import string\n\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nimport matplotlib.pyplot as plt\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.decomposition import TruncatedSVD\n\nimport xgboost as xgb\nfrom sklearn.metrics import log_loss\nfrom sklearn.model_selection import KFold\nfrom sklearn.naive_bayes import MultinomialNB\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\n\nenglish_stopwords = set(stopwords.words(\"english\"))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f65422874fb507d87d4331f4b7469978c05bf682"},"cell_type":"markdown","source":"\n#### 3.3.1. Creating word frequency matrix"},{"metadata":{"trusted":true,"_uuid":"72e8f168b6c8c1d3b9bd5bb4e4cf64f76c183691"},"cell_type":"code","source":"# import stemmer and lemmatizer\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.stem.porter import PorterStemmer\n\n# define LemmaCountVectorizer which will find all unique word and their occurrences\nporter_stemmer = PorterStemmer()\nlemm = WordNetLemmatizer()\n\nclass LemmaCountVectorizer(CountVectorizer):\n    def build_analyzer(self):\n        analyzer = super(LemmaCountVectorizer, self).build_analyzer()\n        return lambda doc: (porter_stemmer.stem(lemm.lemmatize(w)) for w in analyzer(doc))\n\n# Seperate text by author\neap_text = list(train_df[train_df['author'] == 'EAP'][text_column].values)\nhpl_text = list(train_df[train_df['author'] == 'HPL'][text_column].values)\nmws_text = list(train_df[train_df['author'] == 'MWS'][text_column].values)\n\nauthor_text_dict = dict(zip([0,1,2], [eap_text,hpl_text, mws_text]))\n\n# apply LemmaCountVectorizer to the full text\nfull_text = eap_text + mws_text + hpl_text\n\nfull_tf_vectorizer = LemmaCountVectorizer(max_df=0.95, \n                                       min_df=2,\n                                       stop_words='english',\n                                       decode_error='ignore')\nfull_tf = full_tf_vectorizer.fit_transform(full_text)\nfull_feature_names = full_tf_vectorizer.get_feature_names()\n# full_count_vec = np.asarray(full_tf.sum(axis=0)).ravel()\n# full_zipped = list(zip(full_feature_names, full_count_vec))\n\n# create dataframe to store the word frequency for each author (initialized with zeros):\n# rows - represents the authors\n# columns represents each unique word (after stemming and lemmatizing)\n# so each cell in our new dataframe, represents how many occurrences each author has for each word in all of his lines\nauthor_word_freq_df = pd.DataFrame(0.0, index=[0,1,2], columns=full_feature_names)\n\n# dictionary contains each word count for each author\nauthor_wordcount_dict = {}\n\nfor author, text in author_text_dict.items():\n  tf_vectorizer = LemmaCountVectorizer(max_df=0.95, \n                                       min_df=2,\n                                       stop_words='english',\n                                       decode_error='ignore')\n  tf = tf_vectorizer.fit_transform(text)\n  feature_names = tf_vectorizer.get_feature_names()\n  count_vec = np.asarray(tf.sum(axis=0)).ravel()\n  zipped = list(zip(feature_names, count_vec))\n  author_wordcount_dict[author] = zipped","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f5806d259a5a0490e612d9d58cd502542c89d8ce"},"cell_type":"markdown","source":"#### 3.3.2. Filling the frequency matrix and calculate word count differences and ratio"},{"metadata":{"trusted":true,"_uuid":"02522882499a36c734b6ef2a758db7b85cd70db2"},"cell_type":"code","source":"# fill the word frequency dataframe by each author word count\nfor author, zipped in author_wordcount_dict.items():\n  for word, count in zipped:\n    author_word_freq_df[word.lower()][author] = count\n\n# transpose the dataframe, now the rows contains the unique words, columns contains authors\ntransposed_freq_df = author_word_freq_df.T\n\n# Create new columns:\n# 1. 0_count,1_count, 2_count - represeting the word count difference between the authors\n# 2. 0_ratio,1_ratio, 2_ratio - represeting the word count ratio between the authors\n\ntransposed_freq_df['0_count'] = transposed_freq_df[0] - transposed_freq_df[1] - transposed_freq_df[2]\ntransposed_freq_df['1_count'] = transposed_freq_df[1] - transposed_freq_df[0] - transposed_freq_df[2]\ntransposed_freq_df['2_count'] = transposed_freq_df[2] - transposed_freq_df[0] - transposed_freq_df[1]\n\n# epsilon is used to prevent division by zero, when a certain word is used by only one author\nepsilon = 1 \ntransposed_freq_df['0_ratio'] = (transposed_freq_df[0] + epsilon) /(transposed_freq_df[1] + transposed_freq_df[2] + epsilon)\ntransposed_freq_df['1_ratio'] = (transposed_freq_df[1] + epsilon) /(transposed_freq_df[0] + transposed_freq_df[2] + epsilon)\ntransposed_freq_df['2_ratio'] = (transposed_freq_df[2] + epsilon) /(transposed_freq_df[0] + transposed_freq_df[1] + epsilon)\n\ntransposed_freq_df.sort_values(by='0_ratio', ascending=False)\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"877d5bf8cf842cf65620fd9fafbcecb9169ea73f"},"cell_type":"markdown","source":"#### 3.3.3. accumulated score for count difference and ratio "},{"metadata":{"trusted":true,"_uuid":"df4d676e860db7580f1d1fa7f484d12ba2b961e0"},"cell_type":"code","source":"def calc_count_score(text, author):\n  word_list = word_tokenize(text)\n  score = 0\n    \n  for word in word_list:\n    lemm_word = porter_stemmer.stem(lemm.lemmatize(word))    \n    \n    if lemm_word in transposed_freq_df.index:\n      score = score + transposed_freq_df[str(author)+'_count'][lemm_word]\n    \n  score = score / len(word_list)\n  return score\n\ndef calc_ratio_score(text, author):\n  word_list = word_tokenize(text)\n  score = 1\n    \n  for word in word_list:\n    lemm_word = porter_stemmer.stem(lemm.lemmatize(word))    \n    \n    if lemm_word in transposed_freq_df.index:\n      \n      score = score * transposed_freq_df[str(author)+'_ratio'][lemm_word]\n    \n  return score / len(word_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"69a7119f86d8c3389b46c964fd3e3bb12ccfc3bf"},"cell_type":"code","source":"\ntrain_df['eap_freq_count_score'] = train_df[text_column].apply(lambda row: calc_count_score(row, 0))\ntrain_df['hpl_freq_count_score'] = train_df[text_column].apply(lambda row: calc_count_score(row, 1))\ntrain_df['mws_freq_count_score'] = train_df[text_column].apply(lambda row: calc_count_score(row, 2))\n\ntrain_df['eap_freq_ratio_score'] = train_df[text_column].apply(lambda row: calc_ratio_score(row, 0))\ntrain_df['hpl_freq_ratio_score'] = train_df[text_column].apply(lambda row: calc_ratio_score(row, 1))\ntrain_df['mws_freq_ratio_score'] = train_df[text_column].apply(lambda row: calc_ratio_score(row, 2))\n\ntest_df['eap_freq_count_score'] = test_df[text_column].apply(lambda row: calc_count_score(row, 0))\ntest_df['hpl_freq_count_score'] = test_df[text_column].apply(lambda row: calc_count_score(row, 1))\ntest_df['mws_freq_count_score'] = test_df[text_column].apply(lambda row: calc_count_score(row, 2))\n\ntest_df['eap_freq_ratio_score'] = test_df[text_column].apply(lambda row: calc_ratio_score(row, 0))\ntest_df['hpl_freq_ratio_score'] = test_df[text_column].apply(lambda row: calc_ratio_score(row, 1))\ntest_df['mws_freq_ratio_score'] = test_df[text_column].apply(lambda row: calc_ratio_score(row, 2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8431ee8960eea08247b5bae7d41e4caabb92821f"},"cell_type":"code","source":"import string\ndef unique_word_fraction(text):\n    \"\"\"function to calculate the fraction of unique words on total words of the text\"\"\"\n    text_splited = text.split(' ')\n    text_splited = [''.join(c for c in s if c not in string.punctuation) for s in text_splited]\n    text_splited = [s for s in text_splited if s]\n    word_count = text_splited.__len__()\n    unique_count = list(set(text_splited)).__len__()\n    return (unique_count/word_count)\n\n\neng_stopwords = set(stopwords.words(\"english\"))\ndef stopwords_count(text):\n    \"\"\" Number of stopwords fraction in a text\"\"\"\n    text = text.lower()\n    text_splited = text.split(' ')\n    text_splited = [''.join(c for c in s if c not in string.punctuation) for s in text_splited]\n    text_splited = [s for s in text_splited if s]\n    word_count = text_splited.__len__()\n    stopwords_count = len([w for w in text_splited if w in eng_stopwords])\n    return (stopwords_count/word_count)\n\n\ndef punctuations_fraction(text):\n    \"\"\"functiopn to claculate the fraction of punctuations over total number of characters for a given text \"\"\"\n    char_count = len(text)\n    punctuation_count = len([c for c in text if c in string.punctuation])\n    return (punctuation_count/char_count)\n\n\ndef char_count(text):\n    \"\"\"function to return number of chracters \"\"\"\n    return len(text)\n\ndef fraction_noun(text):\n    \"\"\"function to give us fraction of noun over total words \"\"\"\n    text_splited = text.split(' ')\n    text_splited = [''.join(c for c in s if c not in string.punctuation) for s in text_splited]\n    text_splited = [s for s in text_splited if s]\n    word_count = text_splited.__len__()\n    pos_list = nltk.pos_tag(text_splited)\n    noun_count = len([w for w in pos_list if w[1] in ('NN','NNP','NNPS','NNS')])\n    return (noun_count/word_count)\n\ndef fraction_adj(text):\n    \"\"\"function to give us fraction of adjectives over total words in given text\"\"\"\n    text_splited = text.split(' ')\n    text_splited = [''.join(c for c in s if c not in string.punctuation) for s in text_splited]\n    text_splited = [s for s in text_splited if s]\n    word_count = text_splited.__len__()\n    pos_list = nltk.pos_tag(text_splited)\n    adj_count = len([w for w in pos_list if w[1] in ('JJ','JJR','JJS')])\n    return (adj_count/word_count)\n\ndef fraction_verbs(text):\n    \"\"\"function to give us fraction of verbs over total words in given text\"\"\"\n    text_splited = text.split(' ')\n    text_splited = [''.join(c for c in s if c not in string.punctuation) for s in text_splited]\n    text_splited = [s for s in text_splited if s]\n    word_count = text_splited.__len__()\n    pos_list = nltk.pos_tag(text_splited)\n    verbs_count = len([w for w in pos_list if w[1] in ('VB','VBD','VBG','VBN','VBP','VBZ')])\n    return (verbs_count/word_count)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6c76b2801d5df6b74d0f46155557b84104a254df"},"cell_type":"code","source":"# create new features for the traing and test dataframes\nfor df in [train_df, test_df]:  \n  \n  # Number of characters in the text\n  df['char_count'] = df[text_column].apply(lambda text: len(text))\n\n  # Number of words in the text\n  df['word_count'] = df[text_column].apply(lambda text: len(word_tokenize(text)))\n\n  # Number of unique words in the text\n  df['unique_word_count'] = df[text_column].apply(lambda text: len(set(word_tokenize(text))))\n\n  # Number of stopwords\n  df['stopwords_count'] = df[text_column].apply(lambda text: len([word for word in word_tokenize(str(text).lower()) if word in english_stopwords]))\n  \n  # Number of punctuations\n  df['punctuations_count'] = df[text_column].apply(lambda text: len([word for word in word_tokenize(text) if word in string.punctuation]))\n\n  # Number of capitalized words\n  df['capitalized_count'] = df[text_column].apply(lambda text: len([word for word in word_tokenize(text) if word.istitle()]))\n  \n  # Number of uppercase words\n  df[\"upper_words_count\"] = df[text_column].apply(lambda text: len([word for word in word_tokenize(text) if word.isupper()]))\n\n  # mean word length\n  df['mean_word_len'] = df[text_column].apply(lambda text: np.mean([len(word) for word in word_tokenize(text)]))\n\n  # the ratio of the punctuation\n  df['punctuations_fraction'] = df[text_column].apply(lambda row: punctuations_fraction(row))\n  \n  # Ratio of nouns\n  df['fraction_noun'] = df[text_column].apply(lambda row: fraction_noun(row))\n  \n  # Ratio of adjective\n  df['fraction_adj'] = df[text_column].apply(lambda row: fraction_adj(row))\n  \n  # Ratio of verbs\n  df['fraction_verbs'] = df[text_column].apply(lambda row: fraction_verbs(row))\n  \n  # Add feature for each stopword in the nltk English stopwords list, representing how many instances each stopword has in the current sentence\n  for idx, curr_stopword in enumerate(english_stopwords):\n    df[curr_stopword] = df[text_column].apply(lambda text: len([word for word in word_tokenize(str(text).lower()) if word == curr_stopword]))\n\n\nnew_features = ['char_count', 'word_count', 'unique_word_count', 'stopwords_count', 'punctuations_count'\n                , 'capitalized_count', 'mean_word_len', 'upper_words_count', 'punctuations_fraction', 'fraction_noun', 'fraction_adj', 'fraction_verbs']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"346f16fcd7528aa2e35b470d850f2baa4a1a6b48"},"cell_type":"code","source":"# train_id = train_df['id'].values\ntest_id = test_df['id'].values\n\nauthor_mapping_dict = {'EAP': 0, 'HPL': 1, 'MWS': 2}\ncols_to_drop = ['id', 'text']\nX_train = train_df.drop(cols_to_drop+['author'], axis=1)\nX_test = test_df.drop(cols_to_drop, axis=1)\n\ny_train = train_df['author'].map(author_mapping_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0d2fe947ed6e76ed79401289a516c60879cfdf24"},"cell_type":"code","source":"import xgboost as xgb\n\nxgb_clf = xgb.XGBClassifier(objective='multi:softprob',\n                            colsample_bytree = 0.3,\n                            learning_rate = 0.1,\n                            max_depth = 3, \n                            alpha = 10,\n                            n_estimators = 10, num_round=2000)\nxgb_clf.fit(X_train, y_train)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"74a4cb638b48079a55fd9d4e29b5be9f27fcf8ac"},"cell_type":"code","source":"y_pred = xgb_clf.predict_proba(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eee60414f8387c8ac11d10edc5b88d610d63d15c"},"cell_type":"code","source":"y_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"868ace721577be48ee615dd8fdb4bed0a369a428"},"cell_type":"code","source":"out_df = pd.DataFrame(y_pred)\nout_df.columns = ['EAP', 'HPL', 'MWS']\nout_df.insert(0, 'id', test_id)\nout_df.to_csv(\"sub_fe.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d39c3d6810d3941296a776b91a7b86288f7f1244"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}