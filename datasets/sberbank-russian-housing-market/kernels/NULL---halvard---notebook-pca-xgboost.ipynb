{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "3160eb95-0cd4-321c-cc3c-bc99a87991da"
      },
      "source": ""
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "33437048-6e52-d184-bc84-37a4b7760b9b"
      },
      "outputs": [],
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load in \n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the \"../input/\" directory.\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
        "\n",
        "from subprocess import check_output\n",
        "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n",
        "\n",
        "# Any results you write to the current directory are saved as output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "0e3725f9-fbfb-e8f0-cdc3-8ca07d11d892"
      },
      "outputs": [],
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn import linear_model\n",
        "from sklearn import preprocessing\n",
        "import xgboost as xgb\n",
        "from sklearn.decomposition import PCA\n",
        "color = sns.color_palette()\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "pd.options.mode.chained_assignment = None  # default='warn'\n",
        "pd.set_option('display.max_columns', 500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "96c63a85-4dca-4fc4-4e22-5654eda3a993"
      },
      "outputs": [],
      "source": [
        "train_df = pd.read_csv(\"../input/train.csv\", parse_dates=['timestamp'])\n",
        "test_df = pd.read_csv(\"../input/test.csv\", parse_dates=['timestamp'])\n",
        "macro_df = pd.read_csv(\"../input/macro.csv\", parse_dates=['timestamp'])\n",
        "train_df = pd.merge(train_df, macro_df, how='left', on='timestamp').convert_objects(convert_numeric=True)\n",
        "test_df = pd.merge(test_df, macro_df, how='left', on='timestamp').convert_objects(convert_numeric=True)\n",
        "print(train_df.shape, test_df.shape)\n",
        "id_test = test_df.id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "5fd05b51-9257-1312-e0a4-fccf97484bdd"
      },
      "outputs": [],
      "source": [
        "# we will add here some data cleaning\n",
        "\n",
        "# Go to logarithms for target varialbe:\n",
        "train_df['price_doc'] = np.log(1 + train_df['price_doc'])   \n",
        "\n",
        "vars_to_logs = ['area_m']\n",
        "for var in vars_to_logs:\n",
        "    train_df[var] = np.log(1 + train_df[var])\n",
        "    test_df[var] = np.log(1 + test_df[var])\n",
        "\n",
        "columns = list(set(list(train_df.columns.values) + list(test_df.columns.values))) \n",
        "trash = ['child_on_acc_pre_school', 'old_education_build_share', 'modern_education_share']\n",
        "\n",
        "columns_class = [f for f in columns if train_df[f].dtype=='object'] + [\n",
        "        'ID_railroad_station_walk', 'ID_railroad_station_avto', 'ID_big_road1', \n",
        "        'ID_big_road2', 'ID_bus_terminal', 'ID_metro']\n",
        "\n",
        "for f in columns_class:\n",
        "    train_df[f] = train_df[f].fillna(-1)\n",
        "    test_df[f] = test_df[f].fillna(-1)\n",
        "\n",
        "# Computing average / mean / std per class\n",
        "for f in columns_class:\n",
        "    if f != 'price_doc' and not f in trash:\n",
        "        print (f)\n",
        "        # Median\n",
        "        a = dict(train_df.groupby(f).median()['price_doc'])\n",
        "        train_df[f + '_' + 'median'] = train_df[f].map(a).fillna(0)\n",
        "        test_df[f + '_' + 'median'] = test_df[f].map(a).fillna(0)\n",
        "        \n",
        "        # Mean\n",
        "        a = dict(train_df.groupby(f).mean()['price_doc'])\n",
        "        train_df[f + '_' + 'mean'] = train_df[f].map(a).fillna(0)\n",
        "        test_df[f + '_' + 'mean'] = test_df[f].map(a).fillna(0)\n",
        "        \n",
        "        # std\n",
        "        a = dict(train_df.groupby(f).std()['price_doc'])\n",
        "        train_df[f + '_' + 'std'] = train_df[f].map(a).fillna(0)\n",
        "        test_df[f + '_' + 'std'] = test_df[f].map(a).fillna(0)\n",
        "\n",
        "\n",
        "def convert_categoricals_to_dummies(train_df):\n",
        "    for f in columns:\n",
        "        if f != 'price_doc' and train_df[f].dtype=='object' and not f in trash:  \n",
        "            print (f)\n",
        "            # Converting categorical data to dummy variables\n",
        "            dummies = pd.get_dummies(train_df[f]).rename(columns=lambda x: f + \"_\" + str(x))\n",
        "            train_df = pd.concat([train_df, dummies], axis=1).drop(f, axis=1)\n",
        "    return train_df\n",
        "\n",
        "train_df = convert_categoricals_to_dummies(train_df)\n",
        "test_df = convert_categoricals_to_dummies(test_df)\n",
        "\n",
        "# This one is not present in test dataset, so we delete it\n",
        "train_df = train_df.drop('sub_area_Poselenie Klenovskoe', 1)\n",
        "\n",
        "#dealing with missing data\n",
        "#missing data\n",
        "total_train = train_df.isnull().sum().sort_values(ascending=False)\n",
        "total_test = test_df.isnull().sum().sort_values(ascending=False)\n",
        "\n",
        "train_df = train_df.drop((total_train[total_train > 0]).index,1, errors=\"ignore\")\n",
        "train_df = train_df.drop((total_test[total_test > 0]).index,1, errors=\"ignore\")\n",
        "test_df = test_df.drop((total_train[total_train > 0]).index,1, errors=\"ignore\")\n",
        "test_df = test_df.drop((total_test[total_test > 0]).index,1, errors=\"ignore\")\n",
        "\n",
        "    \n",
        "# Deleting outliers...\n",
        "train_df = train_df.drop(train_df[train_df['full_sq'] > 2000].index)\n",
        "\n",
        "        \n",
        "print (\"Just to override previous one\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b9cd5315-dded-19d9-9499-acaba80f4181"
      },
      "outputs": [],
      "source": [
        "features = list(train_df.columns)\n",
        "features_remove = ['id', 'price_doc', 'timestamp']\n",
        "features_m = [x for x in features if x not in features_remove and x not in trash]\n",
        "\n",
        "#print (train_df.dtypes[:])\n",
        "# Get only numerical variables\n",
        "features_numbers = [features_m[i] for i in range(len(features_m)) if train_df.dtypes[i] == \"int64\" or train_df.dtypes[i] == \"float64\"]\n",
        "print (features_numbers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d7237573-9485-ba39-1720-5a91ecdc6bf6"
      },
      "outputs": [],
      "source": [
        "#train_norm_df = (train_df[features_numbers] - train_df[features_numbers].mean()) / (train_df[features_numbers].max() - train_df[features_numbers].min())\n",
        "#test_norm_df = (test_df[features_numbers] - train_df[features_numbers].mean()) / (train_df[features_numbers].max() - train_df[features_numbers].min())\n",
        "\n",
        "#train_norm_df = preprocessing.normalize(train_df[features_numbers])\n",
        "\n",
        "train_norm_df = (train_df[features_numbers] - train_df[features_numbers].mean()) / train_df[features_numbers].std()\n",
        "test_norm_df = (test_df[features_numbers] - train_df[features_numbers].mean()) / train_df[features_numbers].std()\n",
        "\n",
        "#train_norm_df = train_df[features_numbers]\n",
        "#test_norm_df = test_df[features_numbers]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "89db0d00-ced7-814b-b3f1-e68e63970cec"
      },
      "outputs": [],
      "source": [
        "pca = PCA(n_components=200)\n",
        "pca.fit(train_norm_df)\n",
        "\n",
        "train_pca_df = pca.transform(train_norm_df)\n",
        "test_pca_df = pca.transform(test_norm_df)\n",
        "print(pca.explained_variance_ratio_) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "fb146d2b-9f33-3b3b-53c8-a112eec0b77e"
      },
      "outputs": [],
      "source": [
        "pca = PCA(n_components=200)\n",
        "pca.fit(train_norm_df)\n",
        "\n",
        "train_pca_df = pca.transform(train_norm_df)\n",
        "test_pca_df = pca.transform(test_norm_df)\n",
        "print(pca.explained_variance_ratio_) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "013fd730-423d-86a2-f6fe-4683c3bc2706"
      },
      "outputs": [],
      "source": [
        "xgb_params = {\n",
        "    'eta': 0.025,\n",
        "    'max_depth': 15,\n",
        "    'subsample': 0.7,\n",
        "    'colsample_bytree': 0.7,\n",
        "    'objective': 'reg:linear',\n",
        "    'eval_metric': 'rmse',\n",
        "    'silent': 1,\n",
        "    'alpha' : 0.5,\n",
        "    'lambda':0.0\n",
        "}\n",
        "\n",
        "dtrain = xgb.DMatrix(train_pca_df.as_matrix(), train_df['price_doc'].as_matrix(), feature_names=features_numbers)\n",
        "dtest = xgb.DMatrix(test_pca_df.as_matrix(), feature_names=features_numbers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "e6a67afb-2392-2c1f-f996-97e718bce715"
      },
      "outputs": [],
      "source": [
        "cv_output = xgb.cv(xgb_params, dtrain, num_boost_round=10000, early_stopping_rounds=200,\n",
        "    verbose_eval=50, show_stdv=False)\n",
        "cv_output[['train-rmse-mean', 'test-rmse-mean']].plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "4f4ab2e9-25f3-57a7-519f-722a56616147"
      },
      "outputs": [],
      "source": [
        "num_boost_rounds = len(cv_output)\n",
        "model = xgb.train(dict(xgb_params, silent=0), dtrain, num_boost_round= num_boost_rounds)\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots(1, 1, figsize=(8, 13))\n",
        "xgb.plot_importance(model, max_num_features=50, height=0.5, ax=ax)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d433371c-ba5f-e184-006c-c01821f29199"
      },
      "outputs": [],
      "source": [
        "y_predict = np.exp(model.predict(dtest)) - 1\n",
        "output = pd.DataFrame({'id': id_test, 'price_doc': y_predict})\n",
        "output.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "bc664804-c6c1-793f-d2ae-33cec3f06df3"
      },
      "outputs": [],
      "source": [
        "output.to_csv('xgbSub.csv', index=False)"
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}