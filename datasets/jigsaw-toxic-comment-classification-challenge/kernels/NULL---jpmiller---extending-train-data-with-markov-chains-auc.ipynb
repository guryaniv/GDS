{"nbformat_minor": 1, "cells": [{"source": ["The goal of this script is to improve classification by extending the dataset. My inspiration came from Pavel Ostyakov's [A simple technique for extending dataset](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/discussion/48038).\n", "\n", "My method involves creating synthetic training data using a Markov chain generator. People far smarter than me figured out how to adapt Markov chains for this purpose and create a package called [Markovify](https://github.com/jsvine/markovify) for easy use.\n", "\n", "The technique here does not improve the AUC score over a baseline script, at least not the way I'm doing it. Even so, I found the computer-generated comments to be fascinating!\n", "\n", "   - UPDATE Jan 29: Changed the eval to use a single, stratified validation set for 'threat'. Added logloss metric.\n", "   - UPDATE Feb 03: Changed base algorithm. Changed metric to AUC. Changed some content.\n", "\n", "### 1. Create Some Data\n", "\n", "Import packages and data as usual..."], "cell_type": "markdown", "metadata": {"_cell_guid": "b8ed3608-a001-450f-a4ff-df084cfb066c", "_uuid": "ed0967a09a1b39bb7bf5d7c551f7d93b860598cb"}}, {"source": ["# %autosave 600\n", "import numpy as np\n", "import pandas as pd\n", "import markovify as mk\n", "\n", "train = pd.read_csv('../input/train.csv')\n", "train.head()"], "execution_count": null, "cell_type": "code", "metadata": {"collapsed": true, "_cell_guid": "2788bcf1-935a-4725-8ac3-ceade79617e8", "_uuid": "2962fea7e7e0c971957396ce555f54cbfc95f59c"}, "outputs": []}, {"source": ["Jagan shows us the breakdown of classes in his [Stop the S@# EDA](https://www.kagg**le.com/jagangupta/stop-the-s-toxic-comments-eda). 'threat' is very imbalanced, which may make it a good candidate for upsampling.\n", "\n", "<img src=\"https://www.kaggle.io/svf/2240904/531400eed7c5659dbfa7af1ebaf64e45/__results___files/__results___10_0.png\" />\n", "\n", "\n", "We're looking at one category at a time so I don't think we care about the class of the other categories at this time. I could be wrong. \n"], "cell_type": "markdown", "metadata": {"_cell_guid": "6add350d-1382-4042-8012-c4b996b04f5c", "_uuid": "a893612bb1f88f98dfe7c26b903353ac8be5db69"}}, {"source": ["tox = train.loc[train['threat'] == 1, ['comment_text']].reset_index(drop=True)\n", "tox.head(10)"], "execution_count": null, "cell_type": "code", "metadata": {"collapsed": true, "_cell_guid": "7f2052cd-e0b1-4efc-a5b7-adaebb6dd9a8", "_uuid": "0d8a9bcc82c5960f1429e1cd19e7bf7ab537ae0c"}, "outputs": []}, {"source": ["Nasty stuff! These people need to relax.\n", "\n", "From here I take a simple approach and load the raw texts into one document. No text processing or anything. \n", "Also I'll get the median length of the comments to provide a more consistent output."], "cell_type": "markdown", "metadata": {"_cell_guid": "d3f6fdf5-e57d-4386-afe8-38ea78d0b081", "_uuid": "5f1ab0ee55bdcc9330dc8dbc54d289078737adc4"}}, {"source": ["doc = tox['comment_text'].tolist()\n", "\n", "nchar = int(tox.comment_text.str.len().median())\n", "nchar\n"], "execution_count": null, "cell_type": "code", "metadata": {"collapsed": true, "_cell_guid": "0bbd53b2-8f73-4d28-881c-a9955f4e9101", "_uuid": "79365925736a09d9995f7e6c0c004a6a2786a1b5"}, "outputs": []}, {"source": ["Now comes the magic which is so easy with Markovify. Create a text_model object and produce some comments..."], "cell_type": "markdown", "metadata": {"_cell_guid": "af82fba1-62b2-44e4-a6f4-c3306eed310e", "_uuid": "831f72406fbfaef63a031606e3620f268f709f05"}}, {"source": ["text_model = mk.Text(doc)\n", "for i in range(10):\n", "    print(text_model.make_short_sentence(nchar))"], "execution_count": null, "cell_type": "code", "metadata": {"collapsed": true, "_cell_guid": "1170c35e-41fb-42e5-83a4-35bca6bb889f", "_uuid": "79f32870942708d73d29c8c010ec8fec9b720e5a"}, "outputs": []}, {"source": ["Only two lines of code and you too can sound like an angry 5th grader!"], "cell_type": "markdown", "metadata": {"_cell_guid": "46fc3439-f61a-43d0-83ff-f6ac4b9d6790", "_uuid": "6e0d70e140d6f93f8cc232f08addc628d3fc02cb"}}, {"source": ["### 2. Check for Improvement\n", "\n", "This is fun and all, but we want to see if it helps classify the comments. I'll plagiarize [Logistic regression with words and char n-grams](https://www.kaggle.com/thousandvoices/logistic-regression-with-words-and-char-n-grams) by thousandvoices.\n"], "cell_type": "markdown", "metadata": {"_cell_guid": "cee7b94b-0b66-45d8-bda2-807859c7cfd0", "_uuid": "13b30574b85f8f79159c5ad0dd860253e216e7ba"}}, {"source": ["import numpy as np\n", "import pandas as pd\n", "import markovify as mk\n", "\n", "from sklearn.feature_extraction.text import TfidfVectorizer\n", "from sklearn.linear_model import LogisticRegression\n", "from sklearn.model_selection import cross_val_score\n", "from scipy.sparse import hstack\n", "from scipy.special import logit, expit"], "execution_count": null, "cell_type": "code", "metadata": {"collapsed": true, "_cell_guid": "3a6b5626-0211-4cf4-8c6c-2fdac02cac99", "_uuid": "c0c581883c45900ffcf5c7c6f380d66ec2eaec2b"}, "outputs": []}, {"source": ["runrows = 40000  # None\n", "\n", "\n", "train = pd.read_csv('../input/train.csv', nrows=runrows).fillna(' ')\n", "test = pd.read_csv('../input/test.csv', nrows=runrows).fillna(' ')\n", "\n", "class_names = list(train)[-6:]\n", "train_base_text = train['comment_text']\n", "test_text = test['comment_text']\n", "\n", "maxfeats = 200  # 20000\n"], "execution_count": null, "cell_type": "code", "metadata": {"collapsed": true, "_cell_guid": "5c7261bf-d2a3-4c66-bc7b-6806d37e109d", "_uuid": "8c6de8287a9dc8134e4a0a0ca478e524e66f9b0a"}, "outputs": []}, {"source": ["This part takes a little over an hour to run, which is why I used the limits above. I had to run the script on my machine to finish."], "cell_type": "markdown", "metadata": {"_cell_guid": "eaea598a-3083-4a6c-b25f-08ad6d95d671", "_uuid": "98bdb371fc421dff628a9b4783db99684d9a5312"}}, {"source": ["word_vectorizer = TfidfVectorizer(\n", "    sublinear_tf=True,\n", "    strip_accents='unicode',\n", "    analyzer='word',\n", "    token_pattern=r'\\w{1,}',\n", "    ngram_range=(1, 1),\n", "    max_features=maxfeats)\n", "\n", "char_vectorizer = TfidfVectorizer(\n", "    sublinear_tf=True,\n", "    strip_accents='unicode',\n", "    analyzer='char',\n", "    ngram_range=(1, 4),\n", "    max_features=maxfeats)\n", "\n", "predictions = {'id': test['id']}\n", "losses = []\n", "\n", "for cl in class_names:\n", "    \n", "    #preprocess\n", "    print('starting {}'.format(cl))\n", "    class_df = train.loc[train[cl] == 1, ['comment_text']].reset_index(drop=True)\n", "    class_list = class_df['comment_text'].tolist() \n", "    \n", "    nchar = int(class_df.comment_text.str.len().median())\n", "    \n", "    # only augment the smallest classes\n", "    ll = class_df.shape[0]\n", "    if ll < 7500:\n", "        count = int(ll/2)\n", "    else:\n", "        count = 0\n", "        \n", "    # generate markovified text\n", "    mkv_text = []\n", "    text_model = mk.Text(class_list)\n", "    for i in range(count):\n", "        new = text_model.make_short_sentence(nchar)\n", "        mkv_text.append(new)\n", "       \n", "    train_ls = train_base_text.tolist() + mkv_text\n", "    train_text = pd.Series(train_ls)    \n", "    all_text = train_text.append(test_text)\n", "    \n", "    # create tfidf features\n", "    word_vectorizer.fit(all_text)\n", "    train_word_features = word_vectorizer.transform(train_text)\n", "    test_word_features = word_vectorizer.transform(test_text)\n", "    \n", "    char_vectorizer.fit(all_text)\n", "    train_char_features = char_vectorizer.transform(train_text)\n", "    test_char_features = char_vectorizer.transform(test_text)\n", "    \n", "    train_features = hstack([train_char_features, train_word_features])\n", "    test_features = hstack([test_char_features, test_word_features])\n", "    \n", "    train_base_tgt = train[cl]\n", "    train_class = np.ones(count)\n", "    train_target = np.append(train_base_tgt, train_class)\n", "    \n", "    # train and predict\n", "    classifier = LogisticRegression(solver='sag', n_jobs=-1)\n", "    classifier.fit(train_features, train_target)\n", "    predictions[cl] = expit(logit(classifier.predict_proba(test_features)[:, 1]) - 0.5)\n"], "execution_count": null, "cell_type": "code", "metadata": {"collapsed": true, "_cell_guid": "c6136dad-5408-485e-ac20-f4b58bf89ee2", "_uuid": "a2b5b02cf5979450452dbce79eaae7136ee41e40"}, "outputs": []}, {"source": ["submission = pd.DataFrame.from_dict(predictions)\n", "submission.to_csv('submission_mk.csv', index=False)\n"], "execution_count": null, "cell_type": "code", "metadata": {"collapsed": true, "_cell_guid": "abc57438-f188-4b94-81bd-4fcbab852e10", "_uuid": "33f49cb7cbaf3c4ddab229b4718cd182ab294450"}, "outputs": []}, {"source": ["As mentioned at the beginning, the final result is not as good as the baseline. This script scores 0.977 vs 0.978 on the test set. ryches says below in the comments:\n", "> Very interesting to generate data like this but I think that the bag of words is holding it back because the markov is just picking things likely to go together in different orders so on a certain level it is just shuffling words around and then your bag of words model is ignoring the order anyway.\n", "\n", "There may be other features that survive the model, but not these ones. Thanks for reading!"], "cell_type": "markdown", "metadata": {"_cell_guid": "71455b01-91fa-4bce-a675-b0a880c31872", "_uuid": "4adfbc4607a58b17d18e86787a2fe489243570a0"}}, {"source": [], "execution_count": null, "cell_type": "code", "metadata": {"collapsed": true, "_cell_guid": "ef010c82-dc1f-49a4-a06b-274cc70e5de2", "_uuid": "a7e7e4a092bc9da488ec3387d628c8edb592c815"}, "outputs": []}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "Python 3", "name": "python3", "language": "python"}, "varInspector": {"types_to_exclude": ["module", "function", "builtin_function_or_method", "instance", "_Feature"], "window_display": true, "cols": {"lenVar": 40, "lenType": 16, "lenName": 16}, "kernels_config": {"python": {"delete_cmd_prefix": "del ", "library": "var_list.py", "varRefreshCmd": "print(var_dic_list())", "delete_cmd_postfix": ""}, "r": {"delete_cmd_prefix": "rm(", "library": "var_list.r", "varRefreshCmd": "cat(var_dic_list()) ", "delete_cmd_postfix": ") "}}, "position": {"top": "172px", "height": "652px", "width": "350px", "right": "20px", "left": "1458px"}}, "language_info": {"pygments_lexer": "ipython3", "version": "3.6.4", "mimetype": "text/x-python", "file_extension": ".py", "name": "python", "codemirror_mode": {"name": "ipython", "version": 3}, "nbconvert_exporter": "python"}}}