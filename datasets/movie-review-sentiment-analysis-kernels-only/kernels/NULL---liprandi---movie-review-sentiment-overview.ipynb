{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\ntrain_data = pd.read_csv('../input/train.tsv',delimiter='\\t')\ntest_data = pd.read_csv('../input/test.tsv',delimiter='\\t')\n\ntrain_data.head()\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"print(len(train_data), len(test_data))\n# The ratio of test data to train data is 0.3 to 0.7, which is fairly common ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7a270b317908cee29d872b69e2284233e7254d7d"},"cell_type":"code","source":"# First we will try logistic regression\n\n# We will re-structure sentiment into positive and negative, by removing the neutral sentiments (with a score of 3) and \n# saying a review has a positive sentiment if it has a score greater than 3, and a negative sentiment if it has a score\n# smaller than 3.\n\ntrain_data_binary = train_data[train_data['Sentiment'] != 3]\n\ntrain_data_binary['Positively Rated'] = np.where(train_data_binary['Sentiment'] > 3, 1, 0)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"79b2f040a1de5fdd62ee9e6456cbc7395424f889"},"cell_type":"code","source":"# We will split our training data into a train set and a test set, since our test data 'test.tsv' is unlabeled.\n# This way we can measure how well our model is predicting the Sentiment.\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(train_data_binary['Phrase'], \n                                                    train_data_binary['Positively Rated'],\n                                                    test_size=0.3, \n                                                    random_state=0)\nlen(X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b393f809712ba722c483014cd42128b5a1663042"},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\n\n# Fit the CountVectorizer to the training data\nvect = CountVectorizer().fit(X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3f4c458de3272a9b21ecbb3abc541292875f6379"},"cell_type":"code","source":"print(len(vect.get_feature_names()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"75a5a58ca6d0f314ef94e2f73d5f8a269c0c90c1"},"cell_type":"code","source":"# We obtain over 15000 features this way. We can reduce the number of features by restricting to words that appear a minimum\n# number of times.\nvect1 = CountVectorizer(min_df=10).fit(X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4ff7dcec510d88417dbf8345084da4e170f8b0b3"},"cell_type":"code","source":"print(len(vect1.get_feature_names()))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"86e9d2c68963cc51af81156fe16a29483fb8c2f3"},"cell_type":"code","source":"# We apply logistic regression to the data\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_auc_score\n\nX_train_vectorized = vect.transform(X_train)\n\nmodel = LogisticRegression()\nmodel.fit(X_train_vectorized, y_train)\n\npredictions = model.predict(vect.transform(X_test))\n\nprint('AUC: ', roc_auc_score(y_test, predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a450fb463e5bc7cd6d96ced49e40c09a23020af3"},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_auc_score\n\nX_train_vectorized = vect1.transform(X_train)\n\nmodel = LogisticRegression()\nmodel.fit(X_train_vectorized, y_train)\n\npredictions = model.predict(vect1.transform(X_test))\n\nprint('AUC: ', roc_auc_score(y_test, predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a7045a2735aa5621840fd989d8043fdd0f7f9028"},"cell_type":"code","source":"# We see that by requiring that the frequencies are at least 10, we reduce the number of features from 15124 to 6674\n# without reducing our score significantly.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"27d8c462992eaa792337e84bc2d9b39a7a6a3c30"},"cell_type":"code","source":"# We try adding n-grams for n=2,3\nvect = CountVectorizer(min_df=10, ngram_range=(1,3)).fit(X_train)\n\nX_train_vectorized = vect.transform(X_train)\n\nlen(vect.get_feature_names())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ef19b10a0485774706165eafef51ec4bf7d6f2f3"},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_auc_score\n\nX_train_vectorized = vect.transform(X_train)\n\nmodel = LogisticRegression()\nmodel.fit(X_train_vectorized, y_train)\n\npredictions = model.predict(vect.transform(X_test))\n\nprint('AUC: ', roc_auc_score(y_test, predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2f127b63708606b89ef0d31fda4a0f2a032ed204"},"cell_type":"code","source":"# We try with tfidf vectorizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"903382d25b4e05726f37ca71bb64b51e0bec42ee"},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\nvect = TfidfVectorizer(min_df=5).fit(X_train)\nlen(vect.get_feature_names())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d330dc830ed0b45c842d8c15b49d2d84593864fc"},"cell_type":"code","source":"X_train_vectorized = vect.transform(X_train)\n\nmodel = LogisticRegression()\nmodel.fit(X_train_vectorized, y_train)\n\npredictions = model.predict(vect.transform(X_test))\n\nprint('AUC: ', roc_auc_score(y_test, predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"093ca2dc7b3d54cac0c2436b1b458b46ca2f282f"},"cell_type":"code","source":"# tfidf doesn't seem to be working better","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ad9cdbc9ed7b7ee686fc412bd6335c6c3159b8b5"},"cell_type":"code","source":"#  we now try naive bayes multinomial classifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f0197625d96aa6fa13f9de08603c0c8c0143763b"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(train_data['Phrase'], \n                                                    train_data['Sentiment'],\n                                                    test_size=0.3, \n                                                    random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6521a64ac99a1654b76c094c92e06e5c1a1ce3e0"},"cell_type":"code","source":"from sklearn import naive_bayes\nfrom sklearn import metrics\n\nvect = CountVectorizer(min_df=10, ngram_range=(1,3)).fit(X_train)\n\nX_train_vectorized = vect.transform(X_train)\nNB= naive_bayes.MultinomialNB()\nNB.fit(X_train_vectorized,y_train)\npredictions = NB.predict(vect.transform(X_test))\nmetrics.f1_score(y_test,predictions, average= 'micro')\n\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}