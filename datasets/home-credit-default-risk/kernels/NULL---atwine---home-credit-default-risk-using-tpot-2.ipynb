{"cells":[{"metadata":{"_uuid":"e07accd9d8aa5b51845df1ec14f4a2e3d91b35ad"},"cell_type":"markdown","source":"# Home Credit Default Risk"},{"metadata":{"_uuid":"82274f40c631c4b6e6acb34e64ed440a1e727100"},"cell_type":"markdown","source":"By Atwine Mugume Twinamatsiko"},{"metadata":{"_uuid":"c4c8bb04855fc0b4dd75580ea6f8b5e197fb6d03"},"cell_type":"markdown","source":"I have studied some good examples and I hope to make my predictions better. In this notebook I am going to apply some of the techniques I have learnt over the past weeks of reading.\n\nI will clearly note and demontrate what i am doing for easy following:"},{"metadata":{"_uuid":"b4329ea6da39dae6fcf7985bd5c1feee9fbe6e92"},"cell_type":"markdown","source":"### Importing the necesary Libraries"},{"metadata":{"trusted":false,"_uuid":"1760af0726492eede85c0fb34f36c46fe3745581"},"cell_type":"code","source":"import matplotlib.pyplot as plt #this is for visualization\nimport numpy as np #this is for matrix calculation and maths\nimport pandas as pd #this is for data manipulation\nimport seaborn as sns #this is also to help in visualization \n#import featuretools as ft #used for auto feature generation where we have many datasets\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom sklearn.model_selection import train_test_split #to help in spliting the data\nfrom sklearn.metrics import mean_squared_error #this is to test the accuracy of the model\nfrom sklearn import cross_validation #to help in feature selection\n\n# I need to install xgboost as my ML method and other parameters I may have forgotten\n# I am going to need feature tools since I have many dataframes so that I can create more features\n\n# Suppress warnings \nimport warnings \nwarnings.filterwarnings('ignore')\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"732f845cb18040b9e2e66a51a9305a2943d6cb5d"},"cell_type":"markdown","source":"### Injest the data into data frames\n"},{"metadata":{"trusted":false,"_uuid":"b5bebb2142ac6a39b62edc2e0d43a3e4171a728c"},"cell_type":"code","source":"df_test = pd.read_csv('../input/application_test.csv')\ndf_train = pd.read_csv('../input/application_train.csv')\ndf_bureau = pd.read_csv('../input/bureau_balance.csv')\ncredit_card_bal = pd.read_csv('../input/credit_card_balance.csv')\ninstalments = pd.read_csv('../input/installments_payments.csv')\nPOS_cash = pd.read_csv('../input/POS_CASH_balance.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f90ea2cf245c53a5d1c571ab7fbfa9df20c799e4"},"cell_type":"markdown","source":"# Initial Data Exploration\n### Look into the training data set to get some exploration going on;\n#### What are we looking for?\n> Data anomalies\n\n> Missing data\n\n> Multicollinearity\n\n> Standardizing the data after which we will do feature engineering"},{"metadata":{"scrolled":true,"trusted":false,"_uuid":"07cc38e4a6df48530bcf55fc7ed46c0e0a9dfc34"},"cell_type":"code","source":"#how big are our data sets?\ndf_bureau.shape,df_test.shape,df_train.shape,credit_card_bal.shape,instalments.shape,POS_cash.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"a9062656f1f93797b91c20c490733ca744666d41"},"cell_type":"code","source":"#I want to look at the variables in the train dataset because it is the most important dataset\ndf_train.describe()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false,"_uuid":"01c96095384fe61946c61e510ac3000e8bcbe65d"},"cell_type":"code","source":"#how many empty values do we have\ndf_train.isnull().any().sum()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6c7ec5ce3359c7b1b1ec1cb7263a7216f69b834d"},"cell_type":"markdown","source":"### Explore the Target Column"},{"metadata":{"trusted":false,"_uuid":"539eb6bf8af2388b7ee864b2308c8a1de6bb0ce2"},"cell_type":"code","source":"df_train['TARGET'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"4bdf658c6a1929a90a5da86e9996c36ae327b246"},"cell_type":"code","source":"#let us look at the distribution of the target\ndf_train['TARGET'].plot.hist()\n#so we see here that there are more defaults than those that have paid their loans back","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"47f514281bdb7118ef44ea2a4ef2e82f17fb0861"},"cell_type":"code","source":"#check for missing values:\ndef miss_val(df):\n    #the number of missing values\n    val_miss = df.isnull().sum()\n    \n    #percentage of the missing values\n    perc_miss = 100* df.isnull().sum()/len(df)\n    \n    #put the two together to form a table\n    miss_table = pd.concat([val_miss,perc_miss], axis= 1)\n    \n    #rename the columns\n    fin_mis_table = miss_table.rename(columns = {0:'Missing Values', 1: 'Percentage'})\n    \n    #sort the values in descending order\n    final_table = fin_mis_table[fin_mis_table.iloc[:,1]!=0].sort_values('Percentage', ascending = False ).round(2)\n    \n    return final_table","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"2ab4f15cfdcf8dd3a08406a073eddf5e773d649e"},"cell_type":"code","source":"miss_val(df_train).head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"ed605bf157faad51c6a612503ad68063f3b5caba"},"cell_type":"code","source":"#how many data types do we have\ndf_train.dtypes.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"ece3e2168eba6fe6adaff49991859e196ef945a5"},"cell_type":"code","source":"#we have 16 data types that are non numeric\n#lets see them\ndf_train.select_dtypes('object').apply(pd.Series.nunique, axis = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"998593e02c8fa4e570f4a0c58982b3f811fdb21d"},"cell_type":"code","source":"#since we have many values that are categorical we need to encode them because they are not easily handled\n\n#create the encoder object\n# labE = LabelEncoder()\n# le_count = 0 #to keep track of the encoded\n\n# for col in df_train:\n#     if df_train[col].dtype == 'object':\n#         #we want to encode the labels with fewer labels\n#         if len(list(df_train[col].unique())) <= 2:\n#             #train on the data\n#             labE.fit(df_train[col])\n#             #transform all the dataframes.\n#             df_test[col] = labE.transform(df_test[col])\n#             df_train[col]= labE.transform(df_train[col])\n            \n#             le_count += 1\n            \n# print('Were transformed', le_count)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"b52220d0c0c3ad8da59df53fe5724a8aaa201337"},"cell_type":"code","source":"#one hot encoding\ndf_test = pd.get_dummies(df_test)\ndf_train = pd.get_dummies(df_train)\n\nprint('Shape', df_test.shape)\nprint('Shape', df_train.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"c5ec0f517928086729658c8ccebf88018a3c2c80"},"cell_type":"code","source":"#one hot encoding creates many more columns and so the dataframes are not aligned \n#let's align the datasets\n\n#first we take out the target column\ntarget_label = df_train['TARGET']\n\ndf_train,df_test = df_train.align(df_test,axis=1,join='inner')\n\n#replace the target column\ndf_train['TARGET'] = target_label\n\nprint('Test Shape',df_test.shape)\nprint('Train Train', df_train.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"77dbae508f3eecdfc46de144e2b87adc7e8de2f7"},"cell_type":"code","source":"#there is an anomaly in the number of days worked, the days are so many so we will replace them\n#CREATE A FLAG COLUMN FOR THE ANOMALY DAYS\ndf_train['DAYS_EMPLOYED_ANOMALY'] = df_train['DAYS_EMPLOYED']== 365243\n\n#replace the days which are abnormal\ndf_train['DAYS_EMPLOYED'].replace({365243: np.nan}, inplace = True)\n\ndf_train['DAYS_EMPLOYED'].plot.hist()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3a918dd285d7e448a87b42c56da9b1a314be1620"},"cell_type":"markdown","source":"## The next issue we need to look at is correlations\n\nVery high correlation means we maybe inputing data of the same type into the model hence overfitting. So this kind of data should be removed from the equation such that, the model will have some real data to work with"},{"metadata":{"trusted":false,"_uuid":"dfefbd3cc74c360da6c8ec78c439cb78924c32d9"},"cell_type":"code","source":"#from numpy we have a correlation function, using pearson's correlation\ncorrelation = df_train.corr()['TARGET'].sort_values()\n\nprint('Least correlated', correlation.tail(10))\nprint('Most correlated', correlation.head(10))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"acddc7ef7558dbb87d8537854e59844c7d05161e"},"cell_type":"code","source":"#there is a high correlation between target and the days birth\ndf_train['DAYS_BIRTH'] = abs(df_train['DAYS_BIRTH'])\ndf_train['DAYS_BIRTH'].corr(df_train['TARGET'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"60373f32ff24585d96d5fc98af2e69989e45df05"},"cell_type":"code","source":"# let's plot the fig and see how it looks like\nplt.style.use('fivethirtyeight')\n\nplt.hist(df_train['DAYS_BIRTH']/365, bins=10, color= 'blue', edgecolor = 'k')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8a1e406c7e3292c54f9a068280c31d5119b0a862"},"cell_type":"markdown","source":"# Feature Engineering:\nFor feature engineering,  I want to use feature tools to create new features \n- There are some ways to create features one of which is polynomial features\n- But first we will use random forest to look at the most important features in the dataframe"},{"metadata":{"_uuid":"888d085217efde1244d68aa6654504454db301ac"},"cell_type":"markdown","source":"### Polinomial Features:\n\nThese are features with powers, for example if we have x, the x^2\n- we use the variables with the highest negative correlation to create the features, the reason we do this is because, the lesser the correlation, the better the variable is to help separate the differences and there by helping the model."},{"metadata":{"trusted":false,"_uuid":"2da052f8e39b84006247cc5a77c67228f14403e7"},"cell_type":"code","source":"#first we create a list of the variables\npoly_train = df_train[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH', 'TARGET']]\npoly_test = df_test[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH']]\n\n#to handle missing values we use the imputer to fill them in based on the mean\nfrom sklearn.preprocessing import Imputer\nimputer = Imputer(strategy = 'median')\n\n#we take the target column out because we don't want to add anything\npoly_target = poly_train['TARGET']\n\n#lets drop it from the poly_train df\npoly_train = poly_train.drop(columns = ['TARGET'])\n\n#now lets impute the values into the dataframes\npoly_train = imputer.fit_transform(poly_train)\npoly_test = imputer.transform(poly_test)\n\n#lets now bring in the polynomial feature creator\n#we will create the features to the 4 degree to prevent over fitting\n\nfrom sklearn.preprocessing import PolynomialFeatures\n\n#we create the polynomial feature object\npoly_creator = PolynomialFeatures(degree=4)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"0b8e94695201f6410a11a89ae998b7e980520bb8"},"cell_type":"code","source":"#we fit the poly features/ train the features on the training data\npoly_creator.fit(poly_train)\n\n\n#now we need to  in put the data frames created\npoly_train_ft = poly_creator.transform(poly_train)\npoly_test_ft = poly_creator.transform(poly_test)\n\nprint('Poly Features Shape:', poly_train_ft.shape)\nprint('Poly Features Shape:', poly_test_ft.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"52b300f59a37dea350314471516178aa8b05df35"},"cell_type":"markdown","source":"The number of features created have been increased and it could cause over fitting, in order to see the features created we use the code below."},{"metadata":{"scrolled":true,"trusted":false,"_uuid":"2d19518694b5be67318188c8a9f9c66dd6a1a70f"},"cell_type":"code","source":"poly_creator.get_feature_names(input_features = ['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH'])[:20]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"b13aea2fe729d354c246dfc55b9f6f44e326e3e4"},"cell_type":"code","source":"poly_df = pd.DataFrame(poly_train_ft)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"126c0ca6314b935603fb4320aea5d872b06cb331"},"cell_type":"code","source":"poly_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"7a3aa77174382e49e3e634607b4a5004b8f370ed"},"cell_type":"code","source":"type(poly_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"c381243395f5fc353b603c00d8c3d91fd0c1d8cb"},"cell_type":"code","source":"#return the target column into the data created\npoly_df['TARGET']= poly_target\n\n#lets now look at the features correlation\npoly_corr = poly_df.corr()['TARGET'].sort_values()\n\n#display the top 10 and bottom 10\nprint(poly_corr.head(10))\nprint('-'*20)\nprint(poly_corr.tail(10))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fce84d99981bde06d1d17311b76d0cc452902e8f"},"cell_type":"markdown","source":"Right now we need to transform these df into the right data frames so we can test them"},{"metadata":{"trusted":false,"_uuid":"6dbdd60d3199dfb3c9e1bd780e827695123c198b"},"cell_type":"code","source":"#we want a data frame from the info above WITH THE HIGHLY CORRELATED FEATURES\npoly_df_ft = pd.DataFrame(poly_test_ft, columns = poly_creator.get_feature_names(['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH']))","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false,"_uuid":"83e50503396f7a976052d40774eec3cdd3a62dfa"},"cell_type":"code","source":"'TARGET' in poly_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"b00bb4949f5cdb2172cb4cd4f36fde6cfa487f4d"},"cell_type":"code","source":"poly_df_ft.shape","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false,"_uuid":"3ac6a9f0500616a1f45f2a5e782b38ca413f8a31"},"cell_type":"code","source":"'TARGET' in poly_df_ft","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"a7c1c7a253f12cce79b949fbc4ec2b46f850e9b8"},"cell_type":"code","source":"Class = pd.DataFrame(poly_target[:48744], dtype= float)\nClass.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"4edc6faca3044530fa190bb4389abb5bbd63c795"},"cell_type":"code","source":"poly_df_ft['TARGET'] = Class['TARGET']","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"e0c36dd7fdd815ca8069dd6273f6fac998aa3464"},"cell_type":"code","source":"'TARGET' in poly_df_ft","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3140387d12ad46208b2313f405d9cabbea20fbc4"},"cell_type":"markdown","source":"## Data Munging\n\nWe have to change the variable name to 'class'"},{"metadata":{"trusted":false,"_uuid":"c342a414ac4064ee991af3a589a9a4a6f6b005f9"},"cell_type":"code","source":"#rename the target variable in the dataframe before to class\npoly_df_ft.rename(columns= {'TARGET':'class'}, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"2ceb7dc83f018e501c52b01840cfc6da5f5f820e"},"cell_type":"code","source":"'class' in poly_df_ft","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false,"_uuid":"3e584e47982bba02a24b23d3ed298dde1b447167"},"cell_type":"code","source":"poly_df_ft.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"22fd384837dbc900308355bb77154c9bb95e14a0"},"cell_type":"code","source":"#separate the target variable\ntarget_class = poly_df_ft['class']","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"2ebcf08496075d2a23bef25738b8a474c70ee5f7"},"cell_type":"code","source":"poly_df_ft.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"41829e1a796c72376835a15169fc69e930ac90d4"},"cell_type":"markdown","source":"## We are now going to use TPOT:\n\nThis algorithm is useful in automated Machine Learning a new area in machine learning.\n\nThe algorithm will help us to predict the best model to use for this task after the initial analysis\n\n"},{"metadata":{"trusted":false,"_uuid":"267748fe9cc294e1dee6f49f9d64d0998f87f4eb"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom tpot import TPOTClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"235321f9fa6463d184e70137e85f28eeda105c88"},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(poly_df_ft, target_class,\n                                                    train_size=0.75, test_size=0.25)\nX_train.shape, X_test.shape, y_train.shape, y_test.shape","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false,"_uuid":"75a734c02999e26dfeb14effc108f2cde0c6fda0"},"cell_type":"code","source":"# tpot = TPOTClassifier(verbosity=2, generations=50, max_time_mins=600, cv=4, n_jobs= 4, config_dict= 'TPOT sparse')\n# tpot.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f773088724afe61bdf7548270144c215afe866c6"},"cell_type":"markdown","source":"## Result after 10 hours"},{"metadata":{"trusted":false,"_uuid":"a2132d2eac1f8c644441db743dda047640aff8ea"},"cell_type":"code","source":"# tpot.export('10_hrs_bernouli.py')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"d256b204433c1d40cc98d9b2fefc1157b413dddc"},"cell_type":"code","source":"# %load 10_hrs_bernouli.py\n# import numpy as np\n# import pandas as pd\n# from sklearn.model_selection import train_test_split\n# from sklearn.naive_bayes import BernoulliNB\n\n# # NOTE: Make sure that the class is labeled 'target' in the data file\n# tpot_data = pd.read_csv('PATH/TO/DATA/FILE', sep='COLUMN_SEPARATOR', dtype=np.float64)\n# features = tpot_data.drop('target', axis=1).values\n# training_features, testing_features, training_target, testing_target = \\\n#             train_test_split(features, tpot_data['target'].values, random_state=42)\n\n# Score on the training set was:1.0\n# exported_pipeline = BernoulliNB(alpha=0.1, fit_prior=True)\n\n# exported_pipeline.fit(training_features, training_target)\n# results = exported_pipeline.predict(testing_features)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"d16ff655f780b2618189e91e8bcd1a4d1ef1631d"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"509e2ace17c4fbae1619c7c64dd14f26cf042c51"},"cell_type":"markdown","source":"### My own implementation of the model talked above"},{"metadata":{"trusted":false,"_uuid":"093120c0b59b02190f064fdf228cbdc5e5360584"},"cell_type":"code","source":"# from sklearn.naive_bayes import BernoulliNB","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"60a4a10a2433b6e6832f2c7e27e3677f66da60ac"},"cell_type":"code","source":"# model = BernoulliNB(alpha=0.1, fit_prior=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"ec5c4c531687f461aadbe3c962de61f6b226a36d"},"cell_type":"code","source":"# model.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"3e8daebf38e74e7f186f180646679f23c7471658"},"cell_type":"code","source":"# pred1 = model.predict(X_test)\n# pred1.shape, y_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"e5c96384ba551a342a2ca7449ca68861bd20e052"},"cell_type":"code","source":"#this is the tool i will use to measure how accurate the algorithm is\n# from sklearn.metrics import accuracy_score\n# accuracy_score(pred1,y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"f73695409081b84dd047168d8447c981a8a33b6e"},"cell_type":"code","source":"# pred2 = model.predict(poly_test_ft)\n# pred2","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"174e25be537a85155d39b1f1ae78d06f753a1243"},"cell_type":"code","source":"# submit = df_test[['SK_ID_CURR']]\n# submit['TARGET']= pred2","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"08e3e962354ba6a907daac184ceb6fa0f559829b"},"cell_type":"code","source":"# submit.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"dd7a95d8d50195b468e7c6e8187a36a07b10d3c9"},"cell_type":"code","source":"# submit.to_csv('tpot1.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"a9b6aff2cb9ba8f239986fe5fce2cbd3005dc414"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9cd015ce07a42b18fa779db8a8dba317256ff857"},"cell_type":"markdown","source":"## Third TPOT algorithm after 7 hrs"},{"metadata":{"trusted":false,"_uuid":"51549b70859f5bc034afe785466b22aff76e8191"},"cell_type":"code","source":"# tpot.export('tpot_random_classifier_credit_risk.py')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"9eda84ece8f6998c7cc07b00fdf400492003c29b"},"cell_type":"code","source":"# %load tpot_random_classifier_credit_risk.py\n# import numpy as np\n# import pandas as pd\n# from sklearn.ensemble import RandomForestClassifier\n# from sklearn.model_selection import train_test_split\n\n# # NOTE: Make sure that the class is labeled 'target' in the data file\n# tpot_data = pd.read_csv('PATH/TO/DATA/FILE', sep='COLUMN_SEPARATOR', dtype=np.float64)\n# features = tpot_data.drop('target', axis=1).values\n# training_features, testing_features, training_target, testing_target = \\\n#             train_test_split(features, tpot_data['target'].values, random_state=42)\n\n# # Score on the training set was:1.0\n# exported_pipeline = RandomForestClassifier(bootstrap=False, criterion=\"entropy\", max_features=0.45, min_samples_leaf=13, min_samples_split=9, n_estimators=100)\n\n# exported_pipeline.fit(training_features, training_target)\n# results = exported_pipeline.predict(testing_features)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"efe9d0ca1c6edbc962c5c48bbd6fe49719e63751"},"cell_type":"code","source":"# from sklearn.ensemble import RandomForestClassifier\n# model2 = RandomForestClassifier(bootstrap=False, criterion=\"entropy\", max_features=0.45, min_samples_leaf=13, min_samples_split=9, n_estimators=100)\n# model2.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"4b196c97508b4a0bd8b971db16181951fe0fbff9"},"cell_type":"code","source":"# pred3 = model2.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"549c2ac0bc8d58cf6180a4306804431699b39d89"},"cell_type":"code","source":"# accuracy_score(pred3, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"934cbefa87528223cca26d12f445bbdddd9b9d32"},"cell_type":"code","source":"# pred4 = model2.predict(poly_test_ft)\n# pred4","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"8917dd263e7051e095e08b42a19e07942214fb3c"},"cell_type":"code","source":"# submit2 = df_test[['SK_ID_CURR']]\n# submit2['TARGET']= pred4","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"e4910096d749092e79d38239e7ebe4cd1f0ba0c3"},"cell_type":"code","source":"# submit2.to_csv('tpot2.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"352c432d0f9ab12147731992095200cedab92a8e"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"00f3b64ce65bba5fe007573e5b78cb9e3523ecb2"},"cell_type":"markdown","source":"### Second TPOT algorithm after 4 hours\n3 threads"},{"metadata":{"trusted":false,"_uuid":"5de9be181ced623c010c52717e7e64f778974e3e"},"cell_type":"code","source":" tpot.export('tpot_bernouli_home_credit_risk.py')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"b70a56cc786039d7fcbe3d7a3abe6b69adcc65ad"},"cell_type":"code","source":"# %load tpot_bernouli_home_credit_risk.py\n# import numpy as np\n# import pandas as pd\n# from sklearn.model_selection import train_test_split\n# from sklearn.naive_bayes import BernoulliNB\n\n# NOTE: Make sure that the class is labeled 'target' in the data file\n# tpot_data = pd.read_csv('PATH/TO/DATA/FILE', sep='COLUMN_SEPARATOR', dtype=np.float64)\n# features = tpot_data.drop('target', axis=1).values\n# training_features, testing_features, training_target, testing_target = \\\n#             train_test_split(features, tpot_data['target'].values, random_state=42)\n\n# Score on the training set was:1.0\n# exported_pipeline = BernoulliNB(alpha=100.0, fit_prior=False)\n\n# exported_pipeline.fit(training_features, training_target)\n# results = exported_pipeline.predict(testing_features)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"f03dbae8c4c22bbd0f42b897625e94210f9a623e"},"cell_type":"code","source":"# tpot.score(X_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"087c279e97ee73a07b940a85ec4db2c128fadb4f"},"cell_type":"markdown","source":"### First TPOT algorithm"},{"metadata":{"trusted":false,"_uuid":"78b1d3788ef4b5021be9ce72c65f06d58660b2d7"},"cell_type":"code","source":"#here we export the final pipeline that was extracted by tpot\n# tpot.export('tpot_home_credit_risk.py')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"c7b6fe98e50e6e3611319e74cb145071d950da43"},"cell_type":"code","source":"#i need to rename the class variable to target so it can be read by tpot\n# poly_df_ft.rename(columns={'class':'target'},inplace= True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"7acf7186bb93bb33f1214ebee730277bc22f50de"},"cell_type":"code","source":"poly_df_ft.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"2cef492b41054f09f7c07451d6c2050c3006a8b1"},"cell_type":"code","source":"#we need to export the results into a csv file\n# poly_df_ft.to_csv('new_train_data.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"dafe132d8650dbe202a9fadcb12735fbc39fce10"},"cell_type":"code","source":"# %load tpot_home_credit_risk.py\n# import numpy as np\n# import pandas as pd\n# from sklearn.ensemble import GradientBoostingClassifier\n# from sklearn.model_selection import train_test_split\n\n# # NOTE: Make sure that the class is labeled 'target' in the data file\n# tpot_data = pd.read_csv('new_train_data.csv', sep=',', dtype=np.float64)\n# features = tpot_data.drop('target', axis=1).values\n# training_features, testing_features, training_target, testing_target = \\\n#             train_test_split(features, tpot_data['target'].values, random_state=42)\n\n# # Score on the training set was:1.0\n# exported_pipeline = GradientBoostingClassifier(learning_rate=0.1, max_depth=9, max_features=0.8, min_samples_leaf=1, min_samples_split=18, n_estimators=100, subsample=0.45)\n\n# exported_pipeline.fit(training_features, training_target)\n# results = exported_pipeline.predict(testing_features)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"fc1cb60e0e95be667742243b422446e7f9421f92"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}