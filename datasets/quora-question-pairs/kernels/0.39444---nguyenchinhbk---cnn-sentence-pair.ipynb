{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Any results you write to the current directory are saved as output.","execution_count":1,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true},"cell_type":"markdown","source":"### Data Preprocessing"},{"metadata":{"_cell_guid":"c2835421-37b7-4a34-9582-cba09d5ef478","_uuid":"700ae6587349ff0f7c922e096139014f0879a250","trusted":true},"cell_type":"code","source":"df_train = pd.read_csv(\"../input/train.csv\")\nprint('Total number of question pairs for training: {}'.format(len(df_train)))\nprint('Duplicate pairs: {}%'.format(round(df_train['is_duplicate'].mean()*100, 2)))\nqids = pd.Series(df_train['qid1'].tolist() + df_train['qid2'].tolist())\nprint('Total number of questions in the training data: {}'.format(len(\n    np.unique(qids))))\nprint('Number of questions that appear multiple times: {}'.format(np.sum(qids.value_counts() > 1)))\n\nplt.figure(figsize=(12, 5))\nplt.hist(qids.value_counts(), bins=50)\nplt.yscale('log', nonposy='clip')\nplt.title('Log-Histogram of question appearance counts')\nplt.xlabel('Number of occurences of question')\nplt.ylabel('Number of questions')\nprint()","execution_count":2,"outputs":[]},{"metadata":{"_cell_guid":"2194aef0-1b96-4c72-ab80-6ff434ff32a5","_uuid":"5bc4237572886b82319077e622b0b521a4f887d2","trusted":true},"cell_type":"code","source":"df_test = pd.read_csv('../input/test.csv')\ndf_test.head()","execution_count":3,"outputs":[]},{"metadata":{"_cell_guid":"28fb0ce3-aeba-44e8-ac50-e6bc4e5e9d32","_uuid":"b97fb0da46439d484421a104804ae323d1efde21","collapsed":true,"trusted":true},"cell_type":"code","source":"train_qs = pd.Series(df_train['question1'].tolist() + df_train['question2'].tolist()).astype(str)\ntest_qs = pd.Series(df_test['question1'].tolist() + df_test['question2'].tolist()).astype(str)","execution_count":4,"outputs":[]},{"metadata":{"_cell_guid":"064e554a-85ff-44ec-a275-75ce0dd9f19b","_uuid":"3562e00c065de9ab30478814b66c0d189f91b1b3","trusted":true},"cell_type":"code","source":"import os\nimport re\nimport csv\nimport codecs\n\nfrom nltk.corpus import stopwords\n\ndef text_to_wordlist(text, remove_stopwords=False, stem_words=False):\n    # Clean the text, with the option to remove stopwords and to stem words.\n    \n    # Convert words to lower case and split them\n    text = text.lower().split()\n\n    # Optionally, remove stop words\n    if remove_stopwords:\n        stops = set(stopwords.words(\"english\"))\n        text = [w for w in text if not w in stops]\n    \n    text = \" \".join(text)\n\n    # Clean the text\n    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n    text = re.sub(r\"what's\", \"what is \", text)\n    text = re.sub(r\"\\'s\", \" \", text)\n    text = re.sub(r\"\\'ve\", \" have \", text)\n    text = re.sub(r\"can't\", \"cannot \", text)\n    text = re.sub(r\"n't\", \" not \", text)\n    text = re.sub(r\"i'm\", \"i am \", text)\n    text = re.sub(r\"\\'re\", \" are \", text)\n    text = re.sub(r\"\\'d\", \" would \", text)\n    text = re.sub(r\"\\'ll\", \" will \", text)\n    text = re.sub(r\",\", \" \", text)\n    text = re.sub(r\"\\.\", \" \", text)\n    text = re.sub(r\"!\", \" ! \", text)\n    text = re.sub(r\"\\/\", \" \", text)\n    text = re.sub(r\"\\^\", \" ^ \", text)\n    text = re.sub(r\"\\+\", \" + \", text)\n    text = re.sub(r\"\\-\", \" - \", text)\n    text = re.sub(r\"\\=\", \" = \", text)\n    text = re.sub(r\"'\", \" \", text)\n    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n    text = re.sub(r\":\", \" : \", text)\n    text = re.sub(r\" e g \", \" eg \", text)\n    text = re.sub(r\" b g \", \" bg \", text)\n    text = re.sub(r\" u s \", \" american \", text)\n    text = re.sub(r\"\\0s\", \"0\", text)\n    text = re.sub(r\" 9 11 \", \"911\", text)\n    text = re.sub(r\"e - mail\", \"email\", text)\n    text = re.sub(r\"j k\", \"jk\", text)\n    text = re.sub(r\"\\s{2,}\", \" \", text)\n    \n    # Optionally, shorten words to their stems\n    if stem_words:\n        text = text.split()\n        stemmer = SnowballStemmer('english')\n        stemmed_words = [stemmer.stem(word) for word in text]\n        text = \" \".join(stemmed_words)\n    \n    # Return a list of words\n    return(text)\n\ntexts_1 = [] \ntexts_2 = []\nlabels = []\nwith codecs.open(\"../input/train.csv\", encoding='utf-8') as f:\n    reader = csv.reader(f, delimiter=',')\n    header = next(reader)\n    for values in reader:\n        texts_1.append(text_to_wordlist(values[3]))\n        texts_2.append(text_to_wordlist(values[4]))\n        labels.append(int(values[5]))\nprint('Found %s texts in train.csv' % len(texts_1))","execution_count":5,"outputs":[]},{"metadata":{"_cell_guid":"e89b7195-8eb3-49fa-9429-2f8865b261b1","_uuid":"bc56dd06e9410e6d297156b9d4fb4dcaa7e49efc","trusted":true},"cell_type":"code","source":"test_texts_1 = [] \ntest_texts_2 = []\ntest_ids = []\nwith codecs.open(\"../input/test.csv\", encoding='utf-8') as f:\n    reader = csv.reader(f, delimiter=',')\n    header = next(reader)\n    for values in reader:\n        test_texts_1.append(text_to_wordlist(values[1]))\n        test_texts_2.append(text_to_wordlist(values[2]))\n        test_ids.append(values[0])\nprint('Found %s texts in test.csv' % len(test_texts_1))","execution_count":6,"outputs":[]},{"metadata":{"_cell_guid":"8f7f6dfc-50c0-47fe-bb2d-70ca25fee1da","_uuid":"05e6cc1759d4f0cb384aa8d409ebe9fbe343f42b","trusted":true},"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\n\ntokenizer = Tokenizer(num_words=200000)\ntokenizer.fit_on_texts(texts_1 + texts_2 + test_texts_1 + test_texts_2)\n\nsequences_1 = tokenizer.texts_to_sequences(texts_1)\nsequences_2 = tokenizer.texts_to_sequences(texts_2)\ntest_sequences_1 = tokenizer.texts_to_sequences(test_texts_1)\ntest_sequences_2 = tokenizer.texts_to_sequences(test_texts_2)\n\nword_index = tokenizer.word_index\nprint('Found %s unique tokens' % len(word_index))","execution_count":7,"outputs":[]},{"metadata":{"_cell_guid":"0233027f-492f-4239-80a5-4e4378ead109","_uuid":"87966e4b057f358aae5edc10e84cb41753f62896","trusted":true},"cell_type":"code","source":"from keras.preprocessing.sequence import pad_sequences\n\ndata_1 = pad_sequences(sequences_1, maxlen=30)\ndata_2 = pad_sequences(sequences_2, maxlen=30)\nlabels = np.array(labels)\nprint('Shape of data tensor:', data_1.shape)\nprint('Shape of label tensor:', labels.shape)\n\ntest_data_1 = pad_sequences(test_sequences_1, maxlen=30)\ntest_data_2 = pad_sequences(test_sequences_2, maxlen=30)\ntest_ids = np.array(test_ids)","execution_count":8,"outputs":[]},{"metadata":{"_cell_guid":"d7cad30a-8800-4470-b9b5-b2f06228759b","_uuid":"aafa2efac6e592dd6c1f4c5e355aa4b51ad78238"},"cell_type":"markdown","source":"### Basic CNN**"},{"metadata":{"_cell_guid":"814f0da8-8756-4fda-a002-fd411622d88e","_uuid":"99ab59b632ebb6dcfcb72bdba20095b253ee51fb","trusted":true},"cell_type":"code","source":"from keras.models import Sequential, Model\nfrom keras.layers import Dense, Dropout, Flatten, Input, MaxPooling1D, Conv1D, Conv2D, Embedding, Dot\nfrom keras.layers.merge import Concatenate\nfrom keras import backend as K\n\ninput1 = Input((data_1.shape[1],))\ninput2 = Input((data_2.shape[1],))\nEmbedding_matrix = Embedding(len(word_index)+1, 300, input_length=data_1.shape[1], name=\"embedding\")\nz1 = Embedding_matrix(input1)\nz2 = Embedding_matrix(input2)\nConv = Conv1D(filters=200,\n                    kernel_size=3,\n                    padding=\"valid\",\n                    activation=\"relu\",\n                    strides=1)\nconv1 = Conv(z1)\nconv2 = Conv(z2)\nconv1 = MaxPooling1D(pool_size=2)(conv1)\nconv2 = MaxPooling1D(pool_size=2)(conv2)\nconv1 = Flatten()(conv1)\nconv2 = Flatten()(conv2)\nsimilar_score = Dense(2800, activation=None)(conv1)\nsimilar_score = Dot(1)([similar_score, conv2])\nz = Concatenate()([conv1, similar_score, conv2])\nz = Dropout(0.8)(z)\nz = Dense(100, activation=\"relu\")(z)\nmodel_output = Dense(1, activation=\"sigmoid\")(z)\nmodel = Model([input1, input2], model_output)\nmodel.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])","execution_count":27,"outputs":[]},{"metadata":{"_cell_guid":"5ed6fb59-b41a-4ea5-9eb7-e80b93e668a2","_uuid":"6fc0b6b7df75444c49822d76766ac3a3f34cd678","trusted":true},"cell_type":"code","source":"print(data_1.shape)","execution_count":28,"outputs":[]},{"metadata":{"_cell_guid":"346859b3-6332-4810-8368-4a76609f83c4","_uuid":"8727bf07af29aa44be3f122f363a0c87e78cfe72","scrolled":true,"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":29,"outputs":[]},{"metadata":{"_cell_guid":"75664e69-40fc-4d16-994f-87cc63eadeab","_uuid":"da61d6fd9f5775fc0174e0ec4d1e0a474f662aab","collapsed":true,"trusted":true},"cell_type":"code","source":"model.fit([data_1, data_2], labels, batch_size=120, epochs=3)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"de507c4b-1796-4b43-bf48-ad2a00e6b3cd","_uuid":"73095cdc4568cb832c272e97606fef282208c7d1","collapsed":true,"trusted":true},"cell_type":"code","source":"predict_label = model.predict([test_data_1, test_data_2])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"d0d9ac22-92cb-490a-aa06-37318d0c6ab3","_uuid":"8ce967b850a14f2ef7f2ade1a6f1f2113475be2f","collapsed":true,"trusted":true},"cell_type":"code","source":"submission_df = {\"test_id\": test_ids, \"is_duplicate\": predict_label[:,0]}\nsubmission = pd.DataFrame(submission_df)\nsubmission.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"7e47aee7-5c51-4fff-aba9-a5f113922ea1","_uuid":"51357b154326ed05521daf871b0659ec6260ec69","collapsed":true,"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}