{"metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"version": "3.6.1", "pygments_lexer": "ipython3", "codemirror_mode": {"version": 3, "name": "ipython"}, "mimetype": "text/x-python", "file_extension": ".py", "nbconvert_exporter": "python", "name": "python"}}, "nbformat_minor": 2, "cells": [{"cell_type": "markdown", "metadata": {"_uuid": "fc61def627716e44c77ac655a975f923ed3cf79a"}, "outputs": [], "source": "# Exploratory Data Analysis of Zillow Real Estate Data\n\nIn this competition, Zillow has provided a file of Zillow estimates, house sale transaction data together with some meta-data (e.g.: number of rooms etc.). The goal of this competition is to come up with an algorithm that predicts the residual error, that is the log error defined as the difference between the Zestimate and actual sales price.\n\nIn other words, the competition focuses on trying to predict when the Zillow Estimate is more reliable and when less so.\n\nThe purpose of this Jupyter notebook is to perform an initial analysis of the data at hand.\n\nSpecifically, I want to:\n\n- Observe the format of the data input files.\n- Explore completeness of data for each factor.\n- Observe the distribution of the residual error.\n- Explore any correlated factors.\n- Explore log error with relationship to various factors.", "execution_count": null}, {"cell_type": "code", "metadata": {"trusted": false, "collapsed": true, "_uuid": "4b8d1da48352b923836a1fc984081822af255bbc"}, "outputs": [], "source": "import pandas as pd\nimport holoviews as hv\nimport seaborn as sb\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport scipy.stats as stats\nimport pylab\n%matplotlib inline", "execution_count": 86}, {"cell_type": "markdown", "metadata": {"_uuid": "cb4db84ff39ee18874e15f4737df4ab206d383bc"}, "outputs": [], "source": "## File format exploration\n\nFirst let's explore how the different input files look like.", "execution_count": null}, {"cell_type": "code", "metadata": {"trusted": false, "_uuid": "4682c7c83f25acb888c243c725a41caf62b2decc"}, "outputs": [], "source": "train_df = pd.read_csv('./train/train_2016.csv')\nproperty_df = pd.read_csv('./train/properties_2016.csv')", "execution_count": 87}, {"cell_type": "code", "metadata": {"trusted": false, "_uuid": "5a598c6ac2d5f0d3d15c6ab9df2455d66fa3a2e9"}, "outputs": [], "source": "train_df.head(5)", "execution_count": 88}, {"cell_type": "code", "metadata": {"trusted": false, "_uuid": "bb056d7b1f8fe62b16b68df166b6044e22643db8"}, "outputs": [], "source": "property_df.head(5)", "execution_count": 89}, {"cell_type": "markdown", "metadata": {"_uuid": "1d43b8179bbdf6676568b36fc09dc5ba7a42402c"}, "outputs": [], "source": "These two files are very simple. Train file contains just the logerror together wtih parcelid and transactiondate\n\nProperty file contains a list of features. These two files are linked via \"parcelid\". Just looking at the head, there appear to be a lot of \"gaps\" of information for many of the features. So one important thing to do is to explore how complete each feature is.\n\nHow many features are there?", "execution_count": null}, {"cell_type": "code", "metadata": {"trusted": false, "_uuid": "1098b3d6ebeeb725f4f545354fa894bcb2772874"}, "outputs": [], "source": "len(property_df.columns)", "execution_count": 90}, {"cell_type": "markdown", "metadata": {"_uuid": "f596a78a57e5f14ff8b2c2677a16e3e942eb27e7"}, "outputs": [], "source": "There appear to be 57 features. Some of these names are really long and almost incomprehensible. I want to rename these columns. Furthermore, it's important to note that although columns like \"airconditioningtypeid\" look numeric (ie: float), these are actually categorical in nature and not numerical which is important to think about when trying to use these features. \n\nThe following columns are actually categorical:\n\n- 'airconditioningtypeid'\n- 'architecturalstyletypeid'\n- 'buildingqualitytypeid'\n- 'buildingclasstypeid'\n- 'heatingorsystemtypeid'\n- 'pooltypeid10'\n- 'pooltypeid2'\n- 'pooltypeid7'\n- 'propertycountylandusecode'\n- 'propertylandusetypeid'\n- 'propertyzoningdesc'\n- 'rawcensustractandblock'\n- 'censustractandblock'\n- 'regionidcounty'\n- 'regionidcity'\n- 'regionidzip'\n- 'regionidneighborhood'\n- 'storytypeid'\n- 'typeconstructiontypeid'\n\nThese columns contain boolean values:\n\n- 'taxdelinquencyflag'\n- 'fireplaceflag'\n\nThese columns are especially interesting:\n\n- 'fips': Things in here look like numbers. But this number is actually a way to describe which city, county, state a propery is located in.\n- 'rawcensustractandblock': Appears to be a combination of fips code, tract number and block number separated by a \".\" . \n\nThese categorical variables/features will have to be converted to dummy variables before doing any machine learning on them (except for if we are using decision trees). Related categorical variables could be converted to bag of words in more advanced explorations/tests.\n\nWhen I rename the columns, categorical columns that are encoded via numbers will be prefixed with \"id\". Boolean features will have \"has_\". All others will just be num, count, or area.", "execution_count": null}, {"cell_type": "code", "metadata": {"trusted": false, "_uuid": "3b16d1d9be55160a3fbf08e48dc748612fe2522c"}, "outputs": [], "source": "property_df.columns", "execution_count": 91}, {"cell_type": "markdown", "metadata": {"_uuid": "9ba6cbfce619b27b6aef176ffe6d630f14a89890"}, "outputs": [], "source": "I am making these variable names more concise and consistent.", "execution_count": null}, {"cell_type": "code", "metadata": {"trusted": false, "collapsed": true, "_uuid": "ac8552cdd3f1bdc58e6dd821aca0e621ac43bc85"}, "outputs": [], "source": "property_df.columns = ['parcelid', \n'ac_id', \n'id_arci_style',\n'area_basement', \n'num_bathroom', \n'num_bedroom', \n'id_build_class',\n'id_build_quality', \n'calculatedbathnbr', \n'id_decktype',\n'area_first_floor', \n'area_total_calc',\n'area_fin_living', \n'area_fin_perim_living', \n'area_fin_total_area',\n'area_first_floor_2', \n'area_base', \n'fips', \n'num_fire',\n'num_fullbath', \n'num_garagecar', \n'area_garage', \n'has_spa',\n'id_heating_system_id', \n'latitude', \n'longitude', \n'area_lotsize',\n'num_pool', \n'area_pool_total', \n'id_spa_tub', \n'id_pool_spa_hottub', \n'id_pool_no_hottub',\n'id_zone_county_landusecode', \n'id_zone_landuse',\n'zone_property', \n'rawcensustractandblock', \n'region_city',\n'region_county', \n'region_neighborhood',\n'region_zip', \n'num_room',\n'id_storytype', \n'num_3_4_bath', \n'typeconstructiontypeid',\n'num_unit', \n'area_patio_yd', \n'area_shed_yd', \n'year_built',\n'num_stories', \n'has_fireplace', \n'assessed_home_value',\n'assessed_parcel_value', \n'assessmentyear', \n'landtaxvaluedollarcnt',\n'tax_amount', \n'tax_is_delinquent', \n'tax_delinquency_year',\n'censustractandblock']", "execution_count": 92}, {"cell_type": "markdown", "metadata": {"_uuid": "bd5b37e2eed2505ed41e1dcf84e8c9ce936944c4"}, "outputs": [], "source": "## Exploration of Data Completeness", "execution_count": null}, {"cell_type": "code", "metadata": {"trusted": false, "_uuid": "02f949f98dda0073b77ccad015af76043ff47cac"}, "outputs": [], "source": "missing_df = property_df.isnull().sum(axis=0).reset_index()\nmissing_df.columns = ['column_name', 'missing_count']\nfilled_df = property_df.notnull().sum(axis=0).reset_index()\nfilled_df.columns = ['column_name', 'filled_count']\nmerged_df = pd.merge(missing_df, filled_df, on=['column_name'])\nmerged_df['fraction_filled'] = merged_df['filled_count']/(merged_df['missing_count'] + merged_df['filled_count'])*100\nmerged_df = merged_df.loc[merged_df['missing_count']>0]\nmerged_df = merged_df.sort_values(by='missing_count', ascending=True)\nmerged_df.head()", "execution_count": 93}, {"cell_type": "code", "metadata": {"trusted": false, "_uuid": "a15259e04677a3289039141cf346dbbdd8301cc6"}, "outputs": [], "source": "fig = plt.figure(figsize=(30, 20))\nsn_plot = sb.barplot(x='fraction_filled', y='column_name', data=merged_df)\nsn_plot.set_xlabel('Percent', fontsize = 25)\nsn_plot.set_ylabel('Feature', fontsize = 25)\nsn_plot.set_title('Feature Completeness', fontsize=50)", "execution_count": 94}, {"cell_type": "markdown", "metadata": {"collapsed": true, "_uuid": "d864c39c9c678c9297a0c111eda00fb01c20fed9"}, "outputs": [], "source": "It appears that a many features are very incomplete. It's very tempting to eliminate features that do not have much content from the analysis. If I look at the features though I see things like:\n\n- num_pool\n- has_spa\n- id_spa_tub\n- area_pool_total\n- has_fireplace\n- area_basement\n\nThese are features that most homes do not have. I will have to explore these features to determine if it is likely that Null/None in this case means that the house does not have this particular feature and so it was not filled in. \n\nOne thing to explore later in the analysis is to determine the impact of the sparsely populated fields on log error with the following options:\n\n- looking only at filled in information.\n- looking at the information content of nan on log error\n- looking at the information with imputing information.", "execution_count": null}, {"cell_type": "markdown", "metadata": {"_uuid": "f4ccc58fa18042fc5f835ee1b6c978f3e3928364"}, "outputs": [], "source": "## Observe the distribution of the residual error\n", "execution_count": null}, {"cell_type": "code", "metadata": {"trusted": false, "_uuid": "1d244ec227b8815705377c27f1ba82e27e8ad388"}, "outputs": [], "source": "hv.notebook_extension('bokeh')\nfrequencies, edges = np.histogram(train_df['logerror'], 500)\nhv.Histogram(frequencies, edges)", "execution_count": 95}, {"cell_type": "code", "metadata": {"trusted": false, "_uuid": "9327d33bcb810d1b1ef1a15ab9afd9992add5816"}, "outputs": [], "source": "stats.probplot(train_df['logerror'], dist='norm', plot=pylab)", "execution_count": 96}, {"cell_type": "markdown", "metadata": {"_uuid": "915fb77aef44cd6ae47784b4d46c06c8b3f3adb5"}, "outputs": [], "source": "This looks quite tight actually but is not normally distributed. The deviations from the line outside the interval -2, and 2 suggests a distribution with heavy tails, that is, the ends of the distribution contain values that are more extreme than you would expect with a normal distribution. The graph looks symmetrical. If one were to look at the absolute log error, the shape of the histogram would look very similar. We would just start at 0 and kind of double the values. This brings up a point of note in terms of granularity:\n\n- Absolute log error: Less granular. We can tell how good a particular feature is in estimating the sales price.\n- Log error: More granular. We can now also tell if a feature also tends to over or underestimate.", "execution_count": null}, {"cell_type": "markdown", "metadata": {"_uuid": "7f00853c524ff77086db18177b7a19c612a899c5"}, "outputs": [], "source": "## Explore any correlated factors.\n\nLet's start easy and just observe if there are any significant 2-d correlations between numerical features.", "execution_count": null}, {"cell_type": "code", "metadata": {"trusted": false, "collapsed": true, "_uuid": "c439445305929c6617afcb565f04c2cb567fd8da"}, "outputs": [], "source": "full_train_df = pd.merge(train_df, property_df, on=['parcelid'])", "execution_count": 97}, {"cell_type": "code", "metadata": {"trusted": false, "collapsed": true, "_uuid": "d14b8334c859ae0a7d0b74b60baf204bffd84651"}, "outputs": [], "source": "non_categorical_features_df = full_train_df[[\n    'logerror',\n    'transactiondate',\n    'area_basement', \n    'num_bathroom', \n    'num_bedroom', \n    'calculatedbathnbr', \n    'area_first_floor', \n    'area_total_calc',\n    'area_fin_living', \n    'area_fin_perim_living', \n    'area_fin_total_area',\n    'area_first_floor_2', \n    'area_base', \n    'num_fire',\n    'num_fullbath', \n    'num_garagecar', \n    'area_garage', \n    'has_spa',\n    'latitude', \n    'longitude', \n    'area_lotsize',\n    'num_pool', \n    'area_pool_total', \n    'num_room',\n    'num_3_4_bath', \n    'num_unit', \n    'area_patio_yd', \n    'area_shed_yd', \n    'year_built',\n    'num_stories', \n    'has_fireplace', \n    'assessed_home_value',\n    'assessed_parcel_value', \n    'assessmentyear', \n    'landtaxvaluedollarcnt',\n    'tax_amount', \n    'tax_is_delinquent', \n    'tax_delinquency_year',\n]]", "execution_count": 98}, {"cell_type": "code", "metadata": {"trusted": false, "_uuid": "200b83cf672f57b490820812e7808663ac4a8fac"}, "outputs": [], "source": "cor_mat = non_categorical_features_df.corr(method='spearman')\nsb.heatmap(cor_mat)", "execution_count": 99}, {"cell_type": "markdown", "metadata": {"_uuid": "2ea7b7197ae9f5a6fcc877026e003be8530a8ce3"}, "outputs": [], "source": "We can see that no single variable is strongly correlated with log error (it wouldn't be a challenge otherwise :-).\n\nOtherwise, high correlation can be observed in places that make sense. For example:\n\nassessed_home_value highly correates with:\n- parcel_value\n- landtaxvalueddollarscnt\n- tax_amount\n\nThere is also some moderate correlation with:\n- area_basement\n- num_bathroom\n- num_bedroom\n- area_total_calc\n- etc...\n\n...basically things that correlate with the size of the house.", "execution_count": null}, {"cell_type": "markdown", "metadata": {"_uuid": "2275c88820248fc4ed0d1a29d7eef144ac945331"}, "outputs": [], "source": "## Exploring the Effects of Time on logerror\n\nOne variable not assessed in the above correlation analysis is the effect of time on logerror. My expectation is that sales are down in the winter time. And with decreased sales, the logerror would be higher somehow as well since there are fewer data points to get better estimates on. This is just my guess though and so I have to actually confirm this.\n\nTo be filled in....", "execution_count": null}, {"cell_type": "markdown", "metadata": {"_uuid": "b56997b24da4f7de051ad77bdf71c6ed73b11ea1"}, "outputs": [], "source": "## Conclusions from First Analysis:\n\nFor features that are not very complete, we need to evaluate if\n- imputing values makes sense. For things like pool size it may (nan = 0). But for things like area_total_calc it may not make sense (it's not clear how the fill-in should best be computed).\n- sparse features have an impact on logerror.\n\nWe need to convert \"appaerent\" numerical features to categorical features. One way to do this is to convert the various types of \"id\" columns to binary dummy variables. A question to the community would be whether these id variables could also be treated as a bag of words which I have heard of before but have not used yet.\n\nAnother question to explore/ask the community is whether for decision-tree-based ML algorithms it makes sense to apply the above mentioned techniques at all or just leave each feature as is.\n\nPersonally, I think I would want to apply a model zoo to this challenge just to have a comparison on the performance of simple ML. Then also add some deep learning architectures and/or decision-tree-based methods.\n\nThe combined models could then be used in some kind of weak learner model of StackNet model.", "execution_count": null}, {"cell_type": "code", "metadata": {"trusted": false, "collapsed": true, "_uuid": "90272967e6e16ff5e173556eda7fe995a6e30f46"}, "outputs": [], "source": "", "execution_count": null}], "nbformat": 4}