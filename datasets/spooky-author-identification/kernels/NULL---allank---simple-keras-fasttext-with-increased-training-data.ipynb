{"metadata": {"language_info": {"file_extension": ".py", "version": "3.6.3", "pygments_lexer": "ipython3", "name": "python", "mimetype": "text/x-python", "nbconvert_exporter": "python", "codemirror_mode": {"name": "ipython", "version": 3}}, "kernelspec": {"display_name": "Python 3", "name": "python3", "language": "python"}}, "nbformat_minor": 1, "cells": [{"metadata": {"_uuid": "872d52a2365d7d717bbf3e8b5cab0f3691221ce6", "_cell_guid": "e6746c13-b3ce-4b20-beb1-5c0f8c047902"}, "cell_type": "markdown", "source": ["> This fork retains the text data in lines longer than 256 characters by creating new training data entries with them"]}, {"metadata": {"_uuid": "7ccc3b4516a4fcde346a162ae4f9461016bbfbbf", "_cell_guid": "d5736ce6-a0b0-4cf0-beaf-a73837398da9"}, "cell_type": "markdown", "source": ["# **1. Few Preprocessings**\n", "# **2. Model: FastText by Keras**"]}, {"metadata": {"_uuid": "b05ef71268db76a4e2565177bf6a5668a5fc428e", "_kg_hide-input": false, "_cell_guid": "93e00783-a024-4e87-a5e1-6709cb8cc981", "_kg_hide-output": true}, "execution_count": null, "cell_type": "code", "source": ["import numpy as np\n", "\n", "import pandas as pd\n", "\n", "from collections import defaultdict\n", "\n", "import keras\n", "from keras.layers import Dense, GlobalAveragePooling1D, Embedding\n", "import keras.backend as K\n", "from keras.callbacks import EarlyStopping\n", "from keras.models import Sequential\n", "\n", "from keras.preprocessing.sequence import pad_sequences\n", "from keras.preprocessing.text import Tokenizer\n", "from keras.utils import to_categorical\n", "from sklearn.model_selection import train_test_split\n", "\n", "\n", "np.random.seed(7)"], "outputs": []}, {"metadata": {"_uuid": "d700f739101e37903112e1de293323dcfbb577be", "_cell_guid": "a5cc2c3e-7960-482e-b548-c447b89925ec", "collapsed": true}, "execution_count": null, "cell_type": "code", "source": ["df = pd.read_csv('./../input/train.csv')\n", "# The below lines are moved to after the processing, since it will be creating new entries in the training data\n", "# a2c = {'EAP': 0, 'HPL' : 1, 'MWS' : 2}\n", "# y = np.array([a2c[a] for a in df.author])\n", "# y = to_categorical(y)"], "outputs": []}, {"metadata": {"_uuid": "a01bab31ed7b8a55820612063576963488d99eb6", "_cell_guid": "a45cb3ba-d1bc-48e0-956c-27d0f49a9943"}, "cell_type": "markdown", "source": ["# 1. **Few Preprocessings**\n", "\n", "In traditional NLP tasks, preprocessings play an important role, but...\n", "\n", "## **Low-frequency words**\n", "In my experience, fastText is very fast, but I need to delete rare words to avoid overfitting.\n", "\n", "**NOTE**:\n", "Some keywords are rare words, such like *Cthulhu* in *Cthulhu Mythos* of *Howard Phillips Lovecraft*.\n", "But these are useful for this task.\n", "\n", "## **Removing Stopwords**\n", "\n", "Nothing.\n", "To identify author from a sentence, some stopwords play an important role because one has specific usages of them.\n", "\n", "## **Stemming and Lowercase**\n", "\n", "Nothing.\n", "This reason is the same for stopwords removing.\n", "And I guess some stemming rules provided by libraries is bad for this task because all author is the older author.\n", "\n", "## **Cutting long sentence**\n", "\n", "~~Too long documents are cut.~~ Longer documents are used to create multiple entries for the same author from the single long line.\n", "\n", "## **Punctuation**\n", "\n", "Because I guess each author has unique punctuations's usage in the novel, I separate them from words.\n", "\n", "e.g. `Don't worry` -> `Don ' t worry`\n", "\n", "## **Is it slow?**\n", "\n", "Don't worry! FastText is a very fast algorithm if it runs on CPU. "]}, {"metadata": {"_uuid": "0023cd1542d866d931deb8472f8a0d6fb0262d9a", "_cell_guid": "8182b25a-f490-4b41-9865-ee1c04afecee"}, "cell_type": "markdown", "source": ["# **Let's check character distribution per author**"]}, {"metadata": {"_uuid": "246a428ca3a063294c15c8c08d234ecf01e4ddbb", "_cell_guid": "c1d00b0d-90e0-4f19-842c-51a82de42a10", "collapsed": true, "_kg_hide-output": true}, "execution_count": null, "cell_type": "code", "source": ["counter = {name : defaultdict(int) for name in set(df.author)}\n", "for (text, author) in zip(df.text, df.author):\n", "    text = text.replace(' ', '')\n", "    for c in text:\n", "        counter[author][c] += 1\n", "\n", "chars = set()\n", "for v in counter.values():\n", "    chars |= v.keys()\n", "    \n", "names = [author for author in counter.keys()]\n", "\n", "print('c ', end='')\n", "for n in names:\n", "    print(n, end='   ')\n", "print()\n", "for c in chars:    \n", "    print(c, end=' ')\n", "    for n in names:\n", "        print(counter[n][c], end=' ')\n", "    print()\n"], "outputs": []}, {"metadata": {"_uuid": "8e72d6f22587780364ed24cae13ece4a403479dd", "_cell_guid": "7a3fdf4e-039d-4c93-bc21-9bad7dfc6ff8"}, "cell_type": "markdown", "source": ["# **Summary of character distribution**\n", "\n", "- HPL and EAP used non ascii characters like a `\u00e4`.\n", "- The number of punctuations seems to be good feature\n"]}, {"metadata": {"_uuid": "fee49fd9139b78ae03603d7d37eafa38f3cb29dc", "_cell_guid": "ce97fc0a-b85c-4f34-92c5-ae66a0730ace"}, "cell_type": "markdown", "source": ["# **Preprocessing**\n", "\n", "My preproceeings are \n", "\n", "- Separate punctuation from words\n", "- Remove lower frequency words ( <= 2)\n", "- Cut a longer document which contains `256` words"]}, {"metadata": {"_uuid": "999012010cd8b9b20d3c5b16c11a2374a5ce44c0", "_cell_guid": "72ff2ff5-0945-4f39-8b02-39e4d5df16c5", "collapsed": true}, "execution_count": null, "cell_type": "code", "source": ["def preprocess(text):\n", "#   Added code to strip leading spaces\n", "    text = text.strip()\n", "    text = text.replace(\"' \", \" ' \")\n", "    signs = set(',.:;\"?!')\n", "    prods = set(text) & signs\n", "    if not prods:\n", "        return text\n", "\n", "    for sign in prods:\n", "        text = text.replace(sign, ' {} '.format(sign) )\n", "    return text"], "outputs": []}, {"metadata": {"_uuid": "755a3735358494113eb40789419407bb6a82efce", "_cell_guid": "7495acce-6127-497e-87cd-46a76e408b67", "collapsed": true}, "execution_count": null, "cell_type": "code", "source": ["# Pre-process the text outside of the create_docs function\n", "df['text'] = df['text'].apply(preprocess)"], "outputs": []}, {"metadata": {"_uuid": "ae3b130a1a33cdb1bd5f4a8c2cd1886d2568b2ee", "_cell_guid": "f06dbf45-3b18-4ef5-8481-0f8cd8cb01b5", "collapsed": true}, "execution_count": null, "cell_type": "code", "source": ["# Split lines on 256 characters, retaining integrity of word boundaries\n", "x = df.assign(**{'text':df['text'].str.wrap(256).str.split('\\n')})\n", "df = pd.DataFrame({\n", "    col:np.repeat(x[col].values, x['text'].str.len())\n", "    for col in x.columns.difference(['text'])\n", "    }).assign(**{'text':np.concatenate(x['text'].values)})[x.columns.tolist()]"], "outputs": []}, {"metadata": {"_uuid": "d994151fa7c5ca5eb863d07025c26871ae507229", "_cell_guid": "3c9310f8-aa7b-4aae-8a8d-67c31a172a7d"}, "execution_count": null, "cell_type": "code", "source": ["df.shape"], "outputs": []}, {"metadata": {"_uuid": "791b9c55790956f07d30466df621a51dc17d402a", "_cell_guid": "5a548ef9-65f3-42cd-9456-a6c75907942c"}, "execution_count": null, "cell_type": "code", "source": ["df = df[df['text'].str.split().apply(len) >= 5]\n", "df.shape"], "outputs": []}, {"metadata": {"_uuid": "e762c9afedd23d916426bfebcbe6c34fbfedde9e", "_cell_guid": "72357954-6d42-4cfa-9ddc-7ce3696c0834", "collapsed": true}, "execution_count": null, "cell_type": "code", "source": ["# Now we can create the y values:\n", "a2c = {'EAP': 0, 'HPL' : 1, 'MWS' : 2}\n", "y = np.array([a2c[a] for a in df.author])\n", "y = to_categorical(y)"], "outputs": []}, {"metadata": {"_uuid": "53f325a090a44f7109f0537022398797704cdc80", "_cell_guid": "f123742f-540f-438d-aba3-ebbca69235be", "collapsed": true}, "execution_count": null, "cell_type": "code", "source": ["def create_docs(df, n_gram_max=2):\n", "    def add_ngram(q, n_gram_max):\n", "            ngrams = []\n", "            for n in range(2, n_gram_max+1):\n", "                for w_index in range(len(q)-n+1):\n", "                    ngrams.append('--'.join(q[w_index:w_index+n]))\n", "            return q + ngrams\n", "        \n", "    docs = []\n", "    for doc in df.text:\n", "#       preprocess already run\n", "#       doc = preprocess(doc).split()        \n", "        doc = doc.split()\n", "        docs.append(' '.join(add_ngram(doc, n_gram_max)))\n", "    \n", "    return docs"], "outputs": []}, {"metadata": {"_uuid": "150f9f6643e6753386b2021ac812ecc0cac66202", "_cell_guid": "888047de-806e-4ad2-9fff-18b4d6583d30", "collapsed": true}, "execution_count": null, "cell_type": "code", "source": ["min_count = 2\n", "\n", "docs = create_docs(df)\n", "tokenizer = Tokenizer(lower=False, filters='')\n", "num_words = sum([1 for _, v in tokenizer.word_counts.items() if v >= min_count])\n", "\n", "tokenizer = Tokenizer(num_words=num_words, lower=False, filters='')\n", "tokenizer.fit_on_texts(docs)\n", "docs = tokenizer.texts_to_sequences(docs)\n", "\n", "maxlen = 256\n", "\n", "docs = pad_sequences(sequences=docs, maxlen=maxlen)"], "outputs": []}, {"metadata": {"_uuid": "b9e353b548b0dfbd4b42a40d8a2643efeb359a20", "_cell_guid": "f9ebc033-2a26-4656-9472-8990c1a27c79"}, "cell_type": "markdown", "source": ["# **2. Model: FastText by Keras**\n", "\n", "FastText is very fast and strong baseline algorithm for text classification based on Continuous Bag-of-Words model a.k.a Word2vec.\n", "\n", "FastText contains only three layers:\n", "\n", "1. Embeddings layer: Input words (and word n-grams) are all words in a sentence/document\n", "2. Mean/AveragePooling Layer: Taking average vector of Embedding vectors\n", "3. Softmax layer\n", "\n", "There are some implementations of FastText:\n", "\n", "- Original library provided by Facebook AI research: https://github.com/facebookresearch/fastText\n", "- Keras: https://github.com/fchollet/keras/blob/master/examples/imdb_fasttext.py\n", "- Gensim: https://radimrehurek.com/gensim/models/wrappers/fasttext.html\n", "\n", "Original Paper: https://arxiv.org/abs/1607.01759 : More detail information about fastText classification model"]}, {"metadata": {"_uuid": "8b56b2ef90e519b939b7bf9ec5a146f749807b02", "_cell_guid": "636eb75e-6fba-413e-996d-1395609b422c"}, "cell_type": "markdown", "source": ["# My FastText parameters are:\n", "\n", "- The dimension of word vector is 20\n", "- Optimizer is `Adam`\n", "- Inputs are words and word bi-grams\n", "  - you can change this parameter by passing the max n-gram size to argument of `create_docs` function.\n"]}, {"metadata": {"_uuid": "bba1d1a6416876e74ed688f56e4d5bc4990ec12a", "_cell_guid": "393d1ddb-0a87-42a3-8575-53ff7abff1da", "collapsed": true}, "execution_count": null, "cell_type": "code", "source": ["input_dim = np.max(docs) + 1\n", "embedding_dims = 20"], "outputs": []}, {"metadata": {"_uuid": "e6c16572e6b32923af39dfd29467e32b52561bb1", "_cell_guid": "2e3e1e3e-22f4-4727-ba6c-67f7b3e80d2f", "collapsed": true}, "execution_count": null, "cell_type": "code", "source": ["model = Sequential()\n", "model.add(Embedding(input_dim=input_dim, output_dim=embedding_dims))\n", "model.add(GlobalAveragePooling1D())\n", "model.add(Dense(3, activation='softmax'))\n", "\n", "model.compile(loss='categorical_crossentropy',\n", "              optimizer='adam',\n", "              metrics=['accuracy'])"], "outputs": []}, {"metadata": {"_uuid": "22e57e010206a3044adf7b82160c7c3ca78030f8", "_cell_guid": "0db889db-0b3e-4025-8847-e3eb5f853f37", "collapsed": true, "scrolled": true}, "execution_count": null, "cell_type": "code", "source": ["epochs = 45\n", "x_train, x_test, y_train, y_test = train_test_split(docs, y, test_size=0.15)\n", "\n", "n_samples = x_train.shape[0]\n", "\n", "hist = model.fit(x_train, y_train,\n", "                 batch_size=16,\n", "                 validation_data=(x_test, y_test),\n", "                 epochs=epochs,\n", "                 callbacks=[EarlyStopping(patience=2, monitor='val_loss')])"], "outputs": []}, {"metadata": {"_uuid": "f5d9f261c1befdb6b05bbd39536c78ad74396d90", "_cell_guid": "52a07f6b-2a3a-401a-aee3-5421c53f0588", "collapsed": true}, "execution_count": null, "cell_type": "code", "source": ["test_df = pd.read_csv('../input/test.csv')\n", "# We've commented this out of the create_docs function, so need to run it manually here:\n", "test_df['text'] = test_df['text'].apply(preprocess)\n", "docs = create_docs(test_df)\n", "docs = tokenizer.texts_to_sequences(docs)\n", "docs = pad_sequences(sequences=docs, maxlen=maxlen)\n", "y = model.predict_proba(docs)\n", "\n", "result = pd.read_csv('../input/sample_submission.csv')\n", "for a, i in a2c.items():\n", "    result[a] = y[:, i]"], "outputs": []}, {"metadata": {"_uuid": "eaf5b87353d9a1e7367def64b453555c23d24e7a", "_cell_guid": "409b8663-bbf5-4757-99c1-40010264de04", "collapsed": true}, "execution_count": null, "cell_type": "code", "source": ["# to_submit=result"], "outputs": []}], "nbformat": 4}