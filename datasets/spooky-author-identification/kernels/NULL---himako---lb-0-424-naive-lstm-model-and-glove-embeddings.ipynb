{"cells": [{"source": ["# Introduction\n", "\n", "This is a simple LSTM based classifier.\n", "\n", "Currently, I have nothing for preprocessing."], "cell_type": "markdown", "metadata": {"_cell_guid": "d0659a2a-8fd5-49f0-b09c-f30dccbedd1e", "_uuid": "c7f541c706b0e6ad1ded324b1867c60577d4f5aa", "collapsed": true}}, {"outputs": [], "metadata": {"_cell_guid": "1a2d449c-2b3a-425b-a9cf-cb13ab0a4e33", "_uuid": "c42e0493931797763cd4e7df061e8c9fdaa65a3a", "collapsed": true}, "cell_type": "code", "source": ["!mkdir log\n", "!mkdir model\n", "!mkdir submission"], "execution_count": 1}, {"outputs": [], "metadata": {"_cell_guid": "93051e5c-5bef-4457-9016-97b20917d4de", "_uuid": "2c92a83bd9093e7b5dc574a3b6f3f2b2aeba044b"}, "cell_type": "code", "source": ["from sklearn.model_selection import train_test_split\n", "from sklearn.preprocessing import LabelBinarizer\n", "from sklearn.metrics import log_loss\n", "\n", "from keras.preprocessing.text import Tokenizer\n", "from keras.preprocessing.text import text_to_word_sequence\n", "from keras.preprocessing.sequence import pad_sequences\n", "from keras.models import Sequential\n", "from keras.layers import Embedding\n", "from keras.layers import Bidirectional\n", "from keras.layers import LSTM\n", "from keras.layers import Dense\n", "from keras.layers import Dropout\n", "from keras.backend import tensorflow_backend\n", "from keras.callbacks import EarlyStopping\n", "from keras.callbacks import ModelCheckpoint\n", "from keras.callbacks import CSVLogger\n", "\n", "from gensim.models import KeyedVectors\n", "\n", "import pandas\n", "import numpy"], "execution_count": 2}, {"source": ["## Build the neural network using Keras\n", "\n", "I use [Keras](http://github.com/fchollet/keras) to build neural network.\n", "\n", "My network is very simple: one Embedding layer, one Bidirectional LSTM layer and one Fully-connected layer.\n", "\n", "I use Adam whose parameters are default values to optimize the network."], "cell_type": "markdown", "metadata": {"_cell_guid": "d59fffaf-b625-4cc9-b37d-56a3d0bfabdf", "_uuid": "35d1a7d96f18d4bfb19f46c8e9f469256e5adc5f"}}, {"outputs": [], "metadata": {"_cell_guid": "0d70440a-20d3-4c1e-a818-04b934c51d41", "_uuid": "df7222478c0353d4520a3272fffe207855ebad83", "collapsed": true}, "cell_type": "code", "source": ["def build_model(n_word, n_dim, n_hidden, syn0=None, trainable=True):\n", "    model = Sequential()\n", "\n", "    if syn0 is not None:\n", "        model.add(Embedding(input_dim=n_word+1, output_dim=n_dim, weights=[syn0], trainable=trainable))\n", "        \n", "    else:\n", "        model.add(Embedding(input_dim=n_word+1, output_dim=n_dim, trainable=trainable))\n", "\n", "    model.add(Dropout(0.5))\n", "    model.add(Bidirectional(LSTM(n_hidden)))    \n", "    model.add(Dense(50))\n", "    model.add(Dense(3, activation='softmax'))\n", "    return model"], "execution_count": 3}, {"source": ["## Build the embedding matrix\n", "\n", "Next, We create a embedding matrix.\n", "\n", "Embedding matrix is initiarized randomly.\n", "\n", "I use [Gensim](https://github.com/RaRe-Technologies/gensim) for preparing word-embedding."], "cell_type": "markdown", "metadata": {"_cell_guid": "4e409cde-44f8-4fa0-9a43-0de5be3d1973", "_uuid": "d60badf66525f2a7f414755c0531c586cf23c258"}}, {"outputs": [], "metadata": {"_cell_guid": "c3e6cd90-0d69-49aa-a4e9-91beadef5ca9", "_uuid": "b4f7c0f0b6b8081a6738e1da1dd9bafe0e037e6f", "collapsed": true}, "cell_type": "code", "source": ["def build_embedding(n_word, n_dim, pretrain=False):\n", "    syn0 = numpy.random.random((n_word+1, n_dim))\n", "\n", "    if pretrain:\n", "        embedding_model = KeyedVectors.load(f'embedding/glove.6B.{n_dim}d')\n", "        for word, index in tokenizer.word_index.items():\n", "            try:\n", "                vector = embedding_model.word_vec(word)\n", "                index = tokenizer.word_index[word]\n", "                syn0[index, :] = vector\n", "\n", "            except Exception as e:\n", "                pass\n", "\n", "    return syn0"], "execution_count": 4}, {"source": ["## Run experiment\n", "\n", "Train, evaluate, create submission!\n", "\n", "You can see the log of training in log directory."], "cell_type": "markdown", "metadata": {"_cell_guid": "c7bee84a-5b85-4385-ab6e-2d35f88bf18f", "_uuid": "3192a58449a1037f3e0441819ec140279fb0dc15"}}, {"outputs": [], "metadata": {"_cell_guid": "e4c1fc11-d5b6-4df3-9a05-d6d1e121f197", "_uuid": "a9fb5cfc6da07270de9ec32e42712f24081c324e", "collapsed": true}, "cell_type": "code", "source": ["def experiment(n_word, n_dim, n_hidden, pretrain=True, trainable=True, batch_size=128):\n", "    syn0  = build_embedding(n_word, n_dim, pretrain=pretrain)\n", "    \n", "    if pretrain:\n", "        model = build_model(n_word, n_dim, n_hidden, syn0=syn0, trainable=trainable)\n", "    \n", "    else:\n", "        model = build_model(n_word, n_dim, n_hidden, trainable=trainable)\n", "\n", "    model_name = f'modelBiLSTM.embedding{n_dim}.n_hidden{n_hidden}.trainable{trainable}.pretrain{pretrain}'  # NOQA\n", "    print()\n", "    print()\n", "    print('# params')\n", "    print(f'n_word     : {n_word}')\n", "    print(f'n_dim      : {n_dim}')\n", "    print(f'n_hidden   : {n_hidden}')\n", "    print(f'pretrain   : {pretrain}')\n", "    print(f'trainable  : {trainable}')\n", "    print(f'destination: {model_name}')\n", "    \n", "    callbacks = []\n", "    callbacks.append(EarlyStopping(patience=3))\n", "    callbacks.append(ModelCheckpoint(filepath=f'model/{model_name}.hdf5', save_best_only=True))\n", "    callbacks.append(CSVLogger(filename=f'log/{model_name}.csv'))\n", "\n", "    model.summary()\n", "    model.compile('adam', 'categorical_crossentropy')\n", "    model.fit(X_train, y_train, batch_size=batch_size, epochs=100,\n", "              validation_data=[X_val, y_val], callbacks=callbacks)\n", "    model.load_weights(f'model/{model_name}.hdf5')\n", "    \n", "    y_val_pred_prob = model.predict(X_val, batch_size=batch_size)\n", "    val_loss = log_loss(y_val, y_val_pred_prob)\n", "    \n", "    print(f'{model_name}: {val_loss}')\n", "    \n", "    y_test_prob = model.predict(X_test, batch_size=batch_size)\n", "    test_data['EAP'] = y_test_prob[:, 0]\n", "    test_data['HPL'] = y_test_prob[:, 1]\n", "    test_data['MWS'] = y_test_prob[:, 2]\n", "\n", "    test_data[['id', 'EAP', 'HPL', 'MWS']].to_csv(f'submission/{model_name}.csv', index=False)\n", "    \n", "    return 0"], "execution_count": 5}, {"outputs": [], "metadata": {"_cell_guid": "368124bf-c30a-4ca2-b4a9-1827b50a754f", "_uuid": "c699179c9da01e4b9197ba1d95da21bf17a22d9d", "collapsed": true}, "cell_type": "code", "source": ["train_data = pandas.read_csv('../input/train.csv', index_col=False)\n", "test_data  = pandas.read_csv('../input/test.csv',  index_col=False)"], "execution_count": 6}, {"outputs": [], "metadata": {"_cell_guid": "6a1c0a96-c807-4c4d-92bf-955f23922c43", "_uuid": "5fafab8f51fde498c2e2ac15e5c1f80077b778a2"}, "cell_type": "code", "source": ["train_data.head()"], "execution_count": 7}, {"outputs": [], "metadata": {"_cell_guid": "67bc2eac-3128-4623-a45a-eefbb90d5230", "_uuid": "8c09e3beee4d6486dad5ca05076fbb5f0f6b2d09"}, "cell_type": "code", "source": ["test_data.head()"], "execution_count": 8}, {"outputs": [], "metadata": {"_cell_guid": "8d407d31-2a51-4eba-9f17-163460a96d98", "_uuid": "275a5dfe8f3b60ab38c07378742b8ebca99e32e7"}, "cell_type": "code", "source": ["all_text = pandas.concat([train_data.text, test_data.text])\n", "n_train = train_data.shape[0]\n", "\n", "print(f'n_train: {n_train}')"], "execution_count": 9}, {"outputs": [], "metadata": {"_cell_guid": "f263451a-745f-4638-ac79-fffbcdc7990e", "_uuid": "4a53f3eb95e2fc4e3dfac54642d3f583e324ebb6", "collapsed": true}, "cell_type": "code", "source": ["tokenizer = Tokenizer()\n", "tokenizer.fit_on_texts(all_text)\n", "\n", "labelbinarizer = LabelBinarizer()\n", "labelbinarizer.fit(train_data['author'])\n", "\n", "X = tokenizer.texts_to_sequences(train_data.text)\n", "X = pad_sequences(X)\n", "\n", "y = labelbinarizer.fit_transform(train_data['author'])"], "execution_count": 10}, {"source": ["## Create validation dataset"], "cell_type": "markdown", "metadata": {"_cell_guid": "0cf19a57-67d9-46f1-befa-0e8cc3c0a5ef", "_uuid": "6766cf51c2d7e87fb84d3e9aaa9ab51c7454df4f"}}, {"outputs": [], "metadata": {"_cell_guid": "43a4fd77-d54c-4563-abb2-fdc8a00b637c", "_uuid": "8f7465494fd053f29105664981ca7fc1d21086c2"}, "cell_type": "code", "source": ["X_train, X_val, y_train, y_val = train_test_split(X, y)\n", "n_word   = len(tokenizer.word_index)\n", "\n", "print(f'vocaburary size: {n_word}')"], "execution_count": 11}, {"outputs": [], "metadata": {"_cell_guid": "b21f807d-7ded-4c29-b90b-3a39b91d7329", "_uuid": "0c8d6f2203a0725fb0645782eb3e5ae804efc3cd", "collapsed": true}, "cell_type": "code", "source": ["X_test = tokenizer.texts_to_sequences(test_data.text)\n", "X_test = pad_sequences(X_test)"], "execution_count": 12}, {"outputs": [], "metadata": {"_cell_guid": "0ec70a47-d14c-4f7b-8ebb-8fbb3187770f", "_uuid": "ec332723c49b0252cbf2634554cf8f8bc1b1c20b"}, "cell_type": "code", "source": ["experiment(n_word, n_dim=50, n_hidden=50, pretrain=False, trainable=True)"], "execution_count": 13}, {"source": ["## Pretrained Embedding [optional]\n", "\n", "You want to use pretrained word-embeddings? OK, go on to the next!\n", "\n", "\n", "### Repos overview\n", "\n", "The directory tree of my repos is following.\n", "\n", "```\n", ".                                                                            \n", "\u251c\u2500\u2500 embedding                                                                                   \u251c\u2500\u2500 libexec                                                               \n", "\u2502\u00a0\u00a0 \u2514\u2500\u2500 binary_convert.py  bn# describe later\n", "\u251c\u2500\u2500 log                                                                                         \u251c\u2500\u2500 model                                                                   \n", "\u251c\u2500\u2500 spooky.ipynb  # this notebook\n", "\u2514\u2500\u2500 submission\n", "```\n", "\n", "\n", "#### Download and unzip\n", "\n", "I use GloVe 6 billion embeddings.\n", "\n", "Please download [GloVe](https://nlp.stanford.edu/projects/glove/) embeddings.\n", "\n", "To down load them, exec following commands.\n", "\n", "__NOTE__: I don't test this on kernel because glove.6B.zip is a large file. If you want test, please do it on your computer.\n", "\n", "```bash\n", "mkdir embedding\n", "cd embedding\n", "\n", "wget http://nlp.stanford.edu/data/glove.6B.zip\n", "unzip glove.6B.zip\n", "\n", "cd ..\n", "\n", "for fname in `ls embedding/*.txt`; do\n", "  python -m gensim.scripts.glove2word2vec -i $fname -o `echo $fname | sed -s 's/txt/w2v/'`;\n", "done\n", "```\n", "\n", "\n", "#### Convert text file to binary models\n", "\n", "Now, I have pretrained embeddings in `repo/embedding`.\n", "\n", "Then, I create gensim binary objects by `python libexec/convert.py`.\n", "\n", "`libexec/convert.py` looks below.\n", "\n", "\n", "```python\n", "from gensim.models import KeyedVectors\n", "import glob\n", "\n", "for fname in glob.glob('embedding/glove.6B.*.w2v'):\n", "    e = KeyedVectors.load_word2vec_format(fname)\n", "    e.save(fname.replace('.w2v', ''))\n", "```\n", "\n", "(exec `rm embedding/*.txt embedding/*.w2v embedding/glove.6B.zip` to cleanup.)\n", "\n", "\n", "## Enjoy!!\n", "\n", "All preparations complete!\n", "\n", "We can use pre-trained word-embeddings to train our model.\n", "\n", "But currently, as [this discussion](https://www.kaggle.com/c/spooky-author-identification/discussion/42316) says,\n", "I cannot improve the score (It seems supress over-fitting)."], "cell_type": "markdown", "metadata": {"_cell_guid": "a398775f-5c23-4627-96dd-f1ecf34bcd19", "_uuid": "c73b5d7155dc9005c2cd1bd944e7140bcd2b16a6"}}, {"outputs": [], "metadata": {"_cell_guid": "a58207dd-2f88-425c-88f6-e5b8d0c6baab", "_uuid": "078b46954b31639460150f16eb466d0ebdee9415", "collapsed": true}, "cell_type": "code", "source": ["# use pre-trained word-embeddings\n", "experiment(n_word, n_dim=50, n_hidden=50, pretrain=True, trainable=True) # not tested on kernel"], "execution_count": null}], "nbformat_minor": 1, "nbformat": 4, "metadata": {"kernelspec": {"language": "python", "display_name": "Python 3", "name": "python3"}, "language_info": {"codemirror_mode": {"version": 3, "name": "ipython"}, "nbconvert_exporter": "python", "version": "3.6.3", "file_extension": ".py", "pygments_lexer": "ipython3", "mimetype": "text/x-python", "name": "python"}}}