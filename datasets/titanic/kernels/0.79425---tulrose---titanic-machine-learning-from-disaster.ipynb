{"cells":[{"metadata":{"_uuid":"76885caaac8701cbf6ed56d73fb6b62c8faadb3d"},"cell_type":"markdown","source":""},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true,"collapsed":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns #seaborn visualisation\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6ee5b7b68d79245ecd07f0506e03c925d17eda8a"},"cell_type":"markdown","source":"# Acquire training and testing data using panda"},{"metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"trusted":true,"_uuid":"a7672df5f6b60e11d1a686232434b0826bc69bfe","scrolled":false,"collapsed":true},"cell_type":"code","source":"train_data = pd.read_csv('../input/train.csv')\ntest_data = pd.read_csv('../input/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"566d8130b1b581f9e06b8996f68dc58f4b6312f3"},"cell_type":"markdown","source":"1. First we must understand what our data looks like and get its overview\n2. Secondly, we inspect if any of the features have missing values."},{"metadata":{"_kg_hide-output":true,"trusted":true,"scrolled":true,"_uuid":"9bd923d2ebc87831868e93b73b8eb568220d1687","collapsed":true},"cell_type":"code","source":"# We will find total number of data and data types of different features.\n\ntrain_data.info()\n\n#We can see from the data above, that Age and Cabin features have a lot of missing values.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"82a897d9173f737f0cca57a5fcd2c2c6933f8c78"},"cell_type":"markdown","source":"We can get more numerical information about our data using describe function in pandas."},{"metadata":{"trusted":true,"_uuid":"51f9d7c731e90cf6e278ce6538a014f5a18ccfee","collapsed":true},"cell_type":"code","source":"train_data.describe(include = 'all')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ded7acac57212d262f3b599f158a81beea694515"},"cell_type":"markdown","source":"# 1. Exploratory Data Analysis: \n### Analyse, Explore and Identify pattern in data"},{"metadata":{"trusted":true,"_uuid":"457bcc5efac3ca70b239a7b810991fcaba8c4586","collapsed":true},"cell_type":"code","source":"# First, we plot a graph to get an idea of the number of people who survived and who died\ntrain_data.Survived.value_counts().plot(kind='bar')\nimport matplotlib.pyplot as plt\nplt.title('Death (0) vs Survival (1)')\nprint(\"It is clear that fewer people were able to survive and more than 500 people died!\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3665c64def068e061888a93f63385b0bd975a8b1"},"cell_type":"markdown","source":"Let us start by understanding correlations between Age and our solution goal (Survived).\n\nA histogram chart is useful for analyzing continous numerical variables like Age where banding or ranges will help identify useful patterns. The histogram can indicate distribution of samples using automatically defined bins or equally ranged bands. This helps us answer questions relating to specific bands (Did infants have better survival rate?)\n\nNote that x-axis in historgram visualizations represents the count of samples or passengers.\n"},{"metadata":{"trusted":true,"_uuid":"2258983b592c74cf1231b0ea7e123c317f2bcc16","collapsed":true},"cell_type":"code","source":"g = sns.FacetGrid(train_data, col='Survived')\ng.map(plt.hist, 'Age', bins=20)\nfig = plt.figure(figsize=(25, 7))\n\nsns.violinplot(x='Sex', y='Age', \n               hue='Survived', data=train_data, \n               split=True,\n               palette={0: \"r\", 1: \"g\"}\n              );","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6cdfdc0e2f930f714ab60077f6456767bb1deb63"},"cell_type":"markdown","source":"\n**Observations.**\n\n- Infants (Age <=4) had high survival rate.\n- Oldest passengers (Age = 80) survived.\n- Large number of 15-25 year olds did not survive.\n- Most passengers are in 15-35 age range.\n\n**Decisions.**\n\nThis simple analysis confirms our assumptions as decisions for subsequent workflow stages.\n\n- We should consider Age in our model training.\n- Complete the Age feature for null values\n- We should band age groups"},{"metadata":{"_uuid":"e45bcf9c8cef809358f8965e59ee940d01deceee"},"cell_type":"markdown","source":"Let us consider correlating Embarked, Sex and Pclass"},{"metadata":{"trusted":true,"_uuid":"031cf2a344f98739d4750e4cb0245ff6ad70ae31","collapsed":true},"cell_type":"code","source":"grid = sns.FacetGrid(train_data, row='Embarked', aspect=1.6)\ngrid.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', palette='deep')\ngrid.add_legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"60af52957706ead93d79e7745f539e220784ed84"},"cell_type":"markdown","source":"**Observations.**\n\n- Female passengers had much better survival rate than males.\n- Exception in Embarked=C where males had higher survival rate. This could be a correlation between Pclass and Embarked and in turn Pclass and Survived, not necessarily direct correlation between Embarked and Survived.\n- Males had better survival rate in Pclass=3 when compared with Pclass=2 for C and Q ports. \n- Ports of embarkation have varying survival rates for Pclass=3 and among male passengers. \n\n**Decisions.**\n\n- Add Sex feature to model training.\n- Complete and add Embarked feature to model training."},{"metadata":{"_uuid":"30df6ae9d2700047c4755b112b075f04148129d4"},"cell_type":"markdown","source":"Lets see how Class affected the Survival rates of passengers."},{"metadata":{"trusted":true,"_uuid":"d2bb9525bfc1e64628ea49b8a4e372f465f1b362","collapsed":true},"cell_type":"code","source":"train_data.Pclass.value_counts().plot(kind='barh')\nplt.title('Class Distribution')\nplt.xlabel('No of Passengers')\nplt.ylabel('Class of Ticket')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8f2297e89b38e3219d824303c1f0a729ce55dfa0","collapsed":true},"cell_type":"code","source":"grid = sns.FacetGrid(train_data, col='Survived', row='Pclass', aspect=1.6)\ngrid.map(plt.hist, 'Age', alpha=.5, bins=20)\ngrid.add_legend()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"44b1152fdf060be511a8678042bc561fd7f25cd6"},"cell_type":"markdown","source":"**Observations.**\n\n- Pclass=3 had most passengers, however most did not survive. \n- Infant passengers in Pclass=2 and Pclass=3 mostly survived. \n- Most passengers in Pclass=1 survived.\n- Pclass varies in terms of Age distribution of passengers.\n\n**Decisions.**\n\n- Consider Pclass for model training."},{"metadata":{"_kg_hide-input":true,"_uuid":"f9c50a36dad5e14fd45b4a94244e3524ebd96d96"},"cell_type":"markdown","source":"We can consider correlating Embarked (Categorical non-numeric), Sex (Categorical non-numeric), Fare (Numeric continuous), with Survived (Categorical numeric)."},{"metadata":{"trusted":true,"_uuid":"3768023f7e5424ff96f76900f79efa940a6345bf","collapsed":true},"cell_type":"code","source":"grid = sns.FacetGrid(train_data, row='Embarked', col='Survived', aspect=1.6)\ngrid.map(sns.barplot, 'Sex', 'Fare', alpha=.5, ci=None)\ngrid.add_legend()\n\nfig = plt.figure(figsize=(25, 7))\nsns.violinplot(x='Embarked', y='Fare', hue='Survived', data=train_data, split=True, palette={0: \"r\", 1: \"g\"});\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"448c4d6876104c0a30884cdfd05cf09c3b899687"},"cell_type":"markdown","source":"**Observations.**\n\n- Higher fare paying passengers had better survival. Confirms our assumption for creating (#4) fare ranges.\n- Port of embarkation correlates with survival rates. Confirms correlating (#1) and completing (#2).\n\n**Decisions.**\n\n- Consider banding Fare feature."},{"metadata":{"_uuid":"c022769286c6fa206d2d6ac0f88e0a6c56f5d9b0"},"cell_type":"markdown","source":"# 2. Feature Engineering: \n \nIn the previous part, we flirted with the data and spotted some interesting correlations.\n\nIn this part, we'll see how to process and transform these variables in such a way the data becomes manageable by a machine learning algorithm.\n\nWe'll also create, or \"engineer\" additional features that will be useful in building the model.\n\n Basically: \n  1. Filling missing values (to Age, Embarked, Fare)\n  2. Coverting categorical variables to integers (Age and Embarked)\n  3. Creating new feature extracting from existing or combine existing features to create a new feature\n"},{"metadata":{"_uuid":"5f474e84ccee5731b0869b4f036c90ef04957c2c"},"cell_type":"markdown","source":"### But first, let's define a print function that asserts whether or not a feature has been processed."},{"metadata":{"trusted":true,"_uuid":"5a337310d8bbadde5f825c5fc2bf3883ee6ac4ab","collapsed":true},"cell_type":"code","source":"def status(feature):\n    print ('Processing', feature, ':ok')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"69511c27bdf3538fe40d490eac094e0f68ff600f"},"cell_type":"markdown","source":"One trick when starting a machine learning problem is to append the training set to the test set together.\n\nWe'll engineer new features using the train set to prevent information leakage. Then we'll add these variables to the test set."},{"metadata":{"trusted":true,"_uuid":"f9880a74fc72507480730fe953e318828115c753","collapsed":true},"cell_type":"code","source":"def get_combined_data():\n    \n    # extracting and then removing the targets from the training data \n    targets = train_data.Survived\n    train_data.drop(['Survived'], 1, inplace=True)\n    \n    # merging train data and test data for future feature engineering\n    # we'll also remove the PassengerID since this is not an informative feature\n    combined = train_data.append(test_data)\n    combined.reset_index(inplace=True)\n    combined.drop(['index', 'PassengerId'], inplace=True, axis=1)\n    \n    return combined","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f43fa69d8a6a3754cbebe30b68fad854d8487fcc","collapsed":true},"cell_type":"code","source":"combined = get_combined_data()\nprint (combined.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"3034c81d4e05a33a3ae3bca7eb7c077f7fbb8e21"},"cell_type":"markdown","source":"> # 1. Extracting the passenger titles"},{"metadata":{"_uuid":"3d7c450653b3ebb28027a4d46c3d05ac6378826e"},"cell_type":"markdown","source":"When looking at the passenger names one could wonder how to process them to extract a useful information.\n\nIf you look closely at these first examples:\n\nBraund, Mr. Owen Harris\nHeikkinen, Miss. Laina\nOliva y Ocana, Dona. Fermina\nPeter, Master. Michael J\nYou will notice that each name has a title in it ! This can be a simple Miss. or Mrs. but it can be sometimes something more sophisticated like Master, Sir or Dona. In that case, we might introduce an additional information about the social status by simply parsing the name and extracting the title and converting to a binary variable.\n\nLet's first see what the different titles are in the train set"},{"metadata":{"trusted":true,"_uuid":"8cd9300647b7fc696e1c81eaf20a124e12d4da90","collapsed":true},"cell_type":"code","source":"titles = set()\nfor name in train_data['Name']:\n    titles.add(name.split(',')[1].split('.')[0].strip())\n\nprint(titles)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"ae41c24141606805a800dd25727221d243d1be70"},"cell_type":"code","source":"Title_Dictionary = {\n    \"Capt\": \"Officer\",\n    \"Col\": \"Officer\",\n    \"Major\": \"Officer\",\n    \"Jonkheer\": \"Royalty\",\n    \"Don\": \"Royalty\",\n    \"Sir\" : \"Royalty\",\n    \"Dr\": \"Officer\",\n    \"Rev\": \"Officer\",\n    \"the Countess\":\"Royalty\",\n    \"Mme\": \"Mrs\",\n    \"Mlle\": \"Miss\",\n    \"Ms\": \"Mrs\",\n    \"Mr\" : \"Mr\",\n    \"Mrs\" : \"Mrs\",\n    \"Miss\" : \"Miss\",\n    \"Master\" : \"Master\",\n    \"Lady\" : \"Royalty\"\n}\n\ndef get_titles():\n    # we extract the title from each name\n    combined['Title'] = combined['Name'].map(lambda name:name.split(',')[1].split('.')[0].strip())\n    \n    # a map of more aggregated title\n    # we map each title\n    combined['Title'] = combined.Title.map(Title_Dictionary)\n    status('Title')\n    return combined","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f1609c5c9d76355fc2468768d22fb5cb1ec786a1","collapsed":true},"cell_type":"code","source":"combined = get_titles()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3bd44eef6c93923b94ca0f85fd15252b3fa72eba","collapsed":true},"cell_type":"code","source":"combined[combined['Title'].isnull()]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5b12c1146449ddc9ecaecc7e390f92b5666bcd7e"},"cell_type":"markdown","source":"There is indeed a NaN value in the line 1305. In fact the corresponding name is Oliva y Ocana, Dona. Fermina.\n\nThis title was not encoutered in the train dataset."},{"metadata":{"_uuid":"5031339c2b3e7388795e2a59b3ae2a2584ccfb4a"},"cell_type":"markdown","source":"># 2. Processing the ages"},{"metadata":{"_uuid":"0519086db8843e2873aaa604455e5e2f358695d7"},"cell_type":"markdown","source":"We have seen in the first part that the Age variable was missing 177 values. This is a large number ( ~ 13% of the dataset). Simply replacing them with the mean or the median age might not be the best solution since the age may differ by groups and categories of passengers.\n\nTo understand why, let's group our dataset by sex, Title and passenger class and for each subset compute the median age.\n\nTo avoid data leakage from the test set, we fill in missing ages in the train using the train set and we fill in ages in the test set using values calculated from the train set as well."},{"metadata":{"_uuid":"2c29dabebbdb27344be3c4731d3d92ff8a025efa"},"cell_type":"markdown","source":"Number of missing ages in train set"},{"metadata":{"_uuid":"4b772ef80cd633d44951e58b526ea51017141f7a","trusted":true,"collapsed":true},"cell_type":"code","source":"print(combined.iloc[:891].Age.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"64d274bf4cc4f27a71931d2dbf4b443d789669b8"},"cell_type":"markdown","source":"Number of missing ages in test set\n\n"},{"metadata":{"trusted":true,"_uuid":"73ccfba05dbc682b07b378cebb55054bbd0ce64a","collapsed":true},"cell_type":"code","source":"print(combined.iloc[891:].Age.isnull().sum())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"edf5c1177e2f657e2bba29433485988a31298529"},"cell_type":"code","source":"grouped_train = combined.iloc[:891].groupby(['Sex','Pclass','Title'])\ngrouped_median_train = grouped_train.median()\ngrouped_median_train = grouped_median_train.reset_index()[['Sex', 'Pclass', 'Title', 'Age']]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2939809f60ae7965b58f13e3efe9d81b837fd07f","collapsed":true},"cell_type":"code","source":"grouped_median_train.head()\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a1385046b3079b5141346b35af408ce350a4a8e9"},"cell_type":"markdown","source":"This dataframe will help us impute missing age values based on different criteria.\n\nLook at the median age column and see how this value can be different based on the Sex, Pclass and Title put together.\n\nFor example:\n\nIf the passenger is female, from Pclass 1, and from royalty the median age is 40.5.\nIf the passenger is male, from Pclass 3, with a Mr title, the median age is 26.\nLet's create a function that fills in the missing age in combined based on these different attributes."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"81a86ea7a207401f7866de6907a07862cb5347b4"},"cell_type":"code","source":"def fill_age(row):\n    condition = (\n        (grouped_median_train['Sex'] == row['Sex']) & \n        (grouped_median_train['Title'] == row['Title']) & \n        (grouped_median_train['Pclass'] == row['Pclass'])\n    ) \n    return grouped_median_train[condition]['Age'].values[0]\n\n\ndef process_age():\n    global combined\n    # a function that fills the missing values of the Age variable\n    combined['Age'] = combined.apply(lambda row: fill_age(row) if np.isnan(row['Age']) else row['Age'], axis=1)\n    combined.loc[ combined['Age'] <= 16, 'Age'] = 0\n    combined.loc[(combined['Age'] > 16) & (combined['Age'] <= 32), 'Age'] = 1\n    combined.loc[(combined['Age'] > 32) & (combined['Age'] <= 48), 'Age'] = 2\n    combined.loc[(combined['Age'] > 48) & (combined['Age'] <= 64), 'Age'] = 3\n    combined.loc[ combined['Age'] > 64, 'Age'] = 4\n    combined['Age'] = combined['Age'].astype(int)\n    status('age')\n    return combined","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"643dcddd2baee3ed7da683a5a03e1fc528d5d025","collapsed":true},"cell_type":"code","source":"combined = process_age()\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"256338103eebc3fddd6ba0bbe54ef4e970231677"},"cell_type":"markdown","source":"Perfect. The missing ages have been replaced.\n\nHowever, we notice a missing value in Fare, two missing values in Embarked and a lot of missing values in Cabin. We'll come back to these variables later.\n"},{"metadata":{"_uuid":"6d52a128c9e84b0111e80c36cb82a4157ec44b2c"},"cell_type":"markdown","source":"# 3. Processing names"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"25510fa44c01257ad5625434383b6ebd7a024a20"},"cell_type":"code","source":"def process_names():\n    global combined\n    # we clean the Name variable\n    combined.drop('Name', axis=1, inplace=True)\n    \n    # encoding in dummy variable\n    titles_dummies = pd.get_dummies(combined['Title'], prefix='Title')\n    combined = pd.concat([combined, titles_dummies], axis=1)\n    \n    # removing the title variable\n    combined.drop('Title', axis=1, inplace=True)\n    \n    status('names')\n    return combined","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d4f7c109a290949c5f94144a63bbdc591aa47be2"},"cell_type":"markdown","source":"This function drops the Name column since we won't be using it anymore because we created a Title column.\n\nThen we encode the title values using a dummy encoding."},{"metadata":{"trusted":true,"_uuid":"54765c2e20892ce2b0663d14caedc519b3ef0054","collapsed":true},"cell_type":"code","source":"combined = process_names()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c760f67aba14e56e98addbe86b6b02d857e5b901","collapsed":true},"cell_type":"code","source":"combined.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a0be38b28131c8fc70f7fc36d5a47a95c3e9ae17"},"cell_type":"markdown","source":"># 4. Processing Fare"},{"metadata":{"_uuid":"db837b7a0277dc4826d812f80c7e8886da039115"},"cell_type":"markdown","source":"Let's imputed the missing fare value by the average fare computed on the train set (only one value is missing)."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"a912ab51a2366ae5ffde8c555a64e8422ef6d949"},"cell_type":"code","source":"def process_fares():\n    global combined\n    # there's one missing fare value - replacing it with the mean.\n    combined.Fare.fillna(combined.iloc[:891].Fare.mean(), inplace=True)\n    status('fare')\n    return combined","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e6b8c576698029b87d186652d0588f2f3c0b77f6","collapsed":true},"cell_type":"code","source":"combined = process_fares()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9dcda6aaf608bad25ab9c4bbc43b5bfbc9036eab"},"cell_type":"markdown","source":"># 5. Processing Embarked"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"103c7178978101c71ce42150c5a84430db5a823b"},"cell_type":"code","source":"def process_embarked():\n    global combined\n    # two missing embarked values - filling them with the most frequent one in the train  set(S)\n    freq_port = train_data.Embarked.dropna().mode()[0]\n    combined.Embarked.fillna(freq_port, inplace=True)\n    # dummy encoding \n    embarked_dummies = pd.get_dummies(combined['Embarked'], prefix='Embarked')\n    combined = pd.concat([combined, embarked_dummies], axis=1)\n    combined.drop('Embarked', axis=1, inplace=True)\n    status('embarked')\n    return combined\nfreq_port = train_data.Embarked.dropna().mode()[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1dbb0ea3f6922fec83514c3e4057b3ad3204e32e","collapsed":true},"cell_type":"code","source":"combined = process_embarked()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c05eb24f180c699f847c986df7a9fcfac2938fdc","collapsed":true},"cell_type":"code","source":"combined.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2214f3bdb7bc96c2811a2748928e47b92c4eda01"},"cell_type":"markdown","source":"># 6. Processing Cabin"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"d5982712059c017afea7329898cbb5ed56ac16e0"},"cell_type":"code","source":"train_cabin, test_cabin = set(), set()\n\nfor c in combined.iloc[:891]['Cabin']:\n    try:\n        train_cabin.add(c[0])\n    except:\n        train_cabin.add('U')\n        \nfor c in combined.iloc[891:]['Cabin']:\n    try:\n        test_cabin.add(c[0])\n    except:\n        test_cabin.add('U')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"26e8050c9c875db5402a8727f25bccf2544d4a07","collapsed":true},"cell_type":"code","source":"print(train_cabin)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a007dc8a241a69a8f5e4361d2a76ffadfed0b467","collapsed":true},"cell_type":"code","source":"print(test_cabin)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"9af80518789f913671f54ad2b4d0124147f7f3ff"},"cell_type":"code","source":"def process_cabin():\n    global combined    \n    # replacing missing cabins with U (for Uknown)\n    combined.Cabin.fillna('U', inplace=True)\n    \n    # mapping each Cabin value with the cabin letter\n    combined['Cabin'] = combined['Cabin'].map(lambda c: c[0])\n    \n    # dummy encoding ...\n    cabin_dummies = pd.get_dummies(combined['Cabin'], prefix='Cabin')    \n    combined = pd.concat([combined, cabin_dummies], axis=1)\n\n    combined.drop('Cabin', axis=1, inplace=True)\n    status('cabin')\n    return combined","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"77dfc96e02b16b3db2aceb484ab3eaf7cafdcd08"},"cell_type":"markdown","source":"This function replaces NaN values with U (for Unknow). It then maps each Cabin value to the first letter. Then it encodes the cabin values using dummy encoding again."},{"metadata":{"trusted":true,"_uuid":"4e0c2e4d93aa1f772382e3b608e54eeff9ea9c26","collapsed":true},"cell_type":"code","source":"combined = process_cabin()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"882e0ae82b6e1c0647e8e5b1aea8ad3f5c5e1826","collapsed":true},"cell_type":"code","source":"combined.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8fe4d5b0b292e21a282cf9ab26973b19c74e0691"},"cell_type":"markdown","source":"> # 7. Processing Sex"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"30e7e71c92d57fd9775ec4ae4c87a6dc71404592"},"cell_type":"code","source":"def process_sex():\n    global combined\n    # mapping string values to numerical one \n    combined['Sex'] = combined['Sex'].map({'male':1, 'female':0})\n    status('Sex')\n    return combined","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"31eaefcc51be002f91cec3a10a88d6ce194c3118"},"cell_type":"markdown","source":"This function maps the string values male and female to 1 and 0 respectively."},{"metadata":{"trusted":true,"_uuid":"698dd34f7cad02e0cdb5c78abb5252d710835946","collapsed":true},"cell_type":"code","source":"combined = process_sex()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"65f599acfccb7782cdce4c98c5d2f73d2b9bc3bc"},"cell_type":"markdown","source":"># 8. Processing Pclass"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"e69beb9a6874603afedd6461044d60ffbdbe24a6"},"cell_type":"code","source":"def process_pclass():\n    \n    global combined\n    # encoding into 3 categories:\n    pclass_dummies = pd.get_dummies(combined['Pclass'], prefix=\"Pclass\")\n    \n    # adding dummy variable\n    combined = pd.concat([combined, pclass_dummies],axis=1)\n    \n    # removing \"Pclass\"\n    combined.drop('Pclass',axis=1,inplace=True)\n    \n    status('Pclass')\n    return combined","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c762851b91877ac116eddd0657991b4fcd015056"},"cell_type":"markdown","source":"This function encodes the values of Pclass (1,2,3) using a dummy encoding.\n\n"},{"metadata":{"trusted":true,"_uuid":"4ef2869a014389741f8290bb68faef7bf8af0c5c","collapsed":true},"cell_type":"code","source":"combined = process_pclass()\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1adcf2f959a47f3051b5cbc86bfef9680ce62a5b"},"cell_type":"markdown","source":"># 9. Processing Ticket"},{"metadata":{"_uuid":"fc020087da198f6f3226c34adf9f42ae8af1565c"},"cell_type":"markdown","source":"Let's first see how the different ticket prefixes we have in our dataset"},{"metadata":{"trusted":true,"_uuid":"b33ac45af80d257183d521a0812322a3fa176d83","collapsed":true},"cell_type":"code","source":"combined.Ticket.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"325dc6f2ae05f366fb06046dc394af38765d35d8"},"cell_type":"code","source":"def cleanTicket(ticket):\n    ticket = ticket.replace('.', '')\n    ticket = ticket.replace('/', '')\n    ticket = ticket.split()\n    ticket = map(lambda t : t.strip(), ticket)\n    ticket = list(filter(lambda t : not t.isdigit(), ticket))\n    if len(ticket) > 0:\n        return ticket[0]\n    else: \n        return 'XXX'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"022e8cc2b07453d8984f6b06fe1905e4d0f3cd19"},"cell_type":"code","source":"tickets = set()\nfor t in combined['Ticket']:\n    tickets.add(cleanTicket(t))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"feb830f014b9ed2e18c133607376bb6d7ac66d3f","collapsed":true},"cell_type":"code","source":"print(len(tickets))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"087b32fa5e069d9c4420bb2d6a53d44840b56f57","collapsed":true},"cell_type":"code","source":"def process_ticket():\n    \n    global combined\n\n    # Extracting dummy variables from tickets:\n\n    combined['Ticket'] = combined['Ticket'].map(cleanTicket)\n    tickets_dummies = pd.get_dummies(combined['Ticket'], prefix='Ticket')\n    combined = pd.concat([combined, tickets_dummies], axis=1)\n    combined.drop('Ticket', inplace=True, axis=1)\n\n    status('Ticket')\n    return combined","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"108b3404744d72c9876faa014fc6788188c94cb4","collapsed":true},"cell_type":"code","source":"combined = process_ticket()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1f0ff0eab162485e9e32dc7751de9ae01d1fdb35"},"cell_type":"markdown","source":"># 9. Processing Family\nThis part includes creating new variables based on the size of the family (the size is by the way, another variable we create).\n\nThis creation of new variables is done under a realistic assumption: Large families are grouped together, hence they are more likely to get rescued than people traveling alone."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"d4dcf579888da0baa809451030bb2757726c4eed"},"cell_type":"code","source":"def process_family():\n    \n    global combined\n    # introducing a new feature : the size of families (including the passenger)\n    combined['FamilySize'] = combined['Parch'] + combined['SibSp'] + 1\n    \n    # introducing other features based on the family size\n    combined['Singleton'] = combined['FamilySize'].map(lambda s: 1 if s == 1 else 0)\n    combined['SmallFamily'] = combined['FamilySize'].map(lambda s: 1 if 2 <= s <= 4 else 0)\n    combined['LargeFamily'] = combined['FamilySize'].map(lambda s: 1 if 5 <= s else 0)\n    combined.drop('Parch', inplace=True, axis=1)\n    combined.drop('SibSp', inplace=True, axis=1)\n    status('family')\n    return combined","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6c9620ce875d162c06c4d611cce86455e2c1137d"},"cell_type":"markdown","source":"This function introduces 4 new features:\n\nFamilySize : the total number of relatives including the passenger (him/her)self.\nSigleton : a boolean variable that describes families of size = 1\nSmallFamily : a boolean variable that describes families of 2 <= size <= 4\nLargeFamily : a boolean variable that describes families of 5 < size"},{"metadata":{"trusted":true,"_uuid":"004bc3bd8ea3978840d92f752fdb64c92d82a550","collapsed":true},"cell_type":"code","source":"combined = process_family()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"22497bb4ee190a2dc4b36b195c3c48fcc990dc37","collapsed":true},"cell_type":"code","source":"print(combined.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d2671aefc1f0b7027ba514506276ebf78626fd5a"},"cell_type":"markdown","source":"We end up with a total of 67 features."},{"metadata":{"trusted":true,"_uuid":"33bfb578410e6f591f4e95c7683fa8c0b4f0a57c","collapsed":true},"cell_type":"code","source":"#We encode Sex as 0 or 1 for male or female.\n#from sklearn.preprocessing import LabelEncoder\n#labelencoder = LabelEncoder()\n#train_data.Sex = labelencoder.fit_transform(train_data.Sex)\n#test_data.Sex = labelencoder.fit_transform(test_data.Sex)\n#print('Label Encoder for Sex, Done!')\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b963fb0ce761311299a13bd8eb3694da4fd0f6cc"},"cell_type":"markdown","source":"## **We are almost done with our data preprocessing. Now we are ready to train a model and predict the required solution.**"},{"metadata":{"_uuid":"3d20af1a39da7573c7ce670950fbca0154a58b1f"},"cell_type":"markdown","source":"# 3. Machine Learning Models: Training and Evaluation\nMachine learning has around 60+ predictive modelling algorithms to choose from. So we must understand the type of problem at hand and solution requirement to narrow down to  few models which we can evaluate.\n\nHere want to identify relationship between output (Survived or not) with other variables or features (Gender, Age, Port...). As such it is a Supervised Machine Learning problem dealing with classification (Survived or not) problem.\n\nWe will be using four classification algorithms which are -\n1. Logistic Regression\n2. Support Vector Machine\n3. K Nearest Neighbors and \n4. Random Forest\n5. XGBoost"},{"metadata":{"_uuid":"23eab14f8743e5b0ded1b9828edb6ad4a0e4f5ad"},"cell_type":"markdown","source":"Back to our problem, we now have to:\n\nBreak the combined dataset in train set and test set.\nUse the train set to build a predictive model.\nEvaluate the model using the train set.\nTest the model using the test set and generate and output file for the submission.\nKeep in mind that we'll have to reiterate on 2. and 3. until an acceptable evaluation score is achieved.\n\nLet's start by importing the useful libraries."},{"metadata":{"trusted":true,"_uuid":"68d5a2c62ab49fb90a7590650cdbac7798c2942c","collapsed":true},"cell_type":"code","source":"from sklearn.pipeline import make_pipeline\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble.gradient_boosting import GradientBoostingClassifier\n\nfrom sklearn.feature_selection import SelectKBest, SelectFromModel\n\n# Splitting the dataset into the Training set and Test set\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV, cross_val_score\n\nfrom sklearn.linear_model import LogisticRegression, LogisticRegressionCV\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3fff26b5d1d9b5ec7a9d33cbd667c81c6a61415d"},"cell_type":"markdown","source":" We'll define a small scoring function recover the train set and the test set from the combined dataset."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"7050b7c7cc6208648dd44fc48c93aaf52b2df1b8"},"cell_type":"code","source":"def compute_score(clf, X, y, scoring='accuracy'):\n    xval = cross_val_score(clf, X, y, cv = 5, scoring=scoring)\n    return np.mean(xval)\n\ndef recover_train_test_target():\n    global combined\n    \n    targets = pd.read_csv('../input/train.csv', usecols=['Survived'])['Survived'].values\n    train = combined.iloc[:891]\n    test = combined.iloc[891:]\n    \n    return train, test, targets","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"de635e5a9e7a7f769e927542a59bb30a8e3046a3","collapsed":true},"cell_type":"code","source":"train, test, targets = recover_train_test_target()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f37263c273b92f5f3c139161f667a0da3bc5a6e7"},"cell_type":"markdown","source":"### Feature Selection\nWe've come up to more than 30 features so far. This number is quite large.\n\nWhen feature engineering is done, we usually tend to decrease the dimensionality by selecting the \"right\" number of features that capture the essential.\n\nIn fact, feature selection comes with many benefits:\n\nIt decreases redundancy among the data\nIt speeds up the training process\nIt reduces overfitting\nTree-based estimators can be used to compute feature importances, which in turn can be used to discard irrelevant features."},{"metadata":{"trusted":true,"_uuid":"a0ff2d0f791aae405b73aedfe35651f4d535fd52","collapsed":true},"cell_type":"code","source":"clf = RandomForestClassifier(n_estimators=50, max_features='sqrt')\nclf = clf.fit(train, targets)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b0b335960aa9dceb1783b6848574bae19ef9cbf9"},"cell_type":"markdown","source":"Let's have a look at the importance of each feature."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"e3b9892a78287a64889cbab176b006c9e11aecc1"},"cell_type":"code","source":"features = pd.DataFrame()\nfeatures['feature'] = train.columns\nfeatures['importance'] = clf.feature_importances_\nfeatures.sort_values(by=['importance'], ascending=True, inplace=True)\nfeatures.set_index('feature', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e5bb7f3c7ec2ed24642e2c14ac8eae29211cad59","collapsed":true},"cell_type":"code","source":"features.plot(kind='barh', figsize=(25, 25))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"24b4cba38cdd275f739588af0add623d693098ae"},"cell_type":"markdown","source":"As you may notice, there is a great importance linked to Title_Mr, Age, Fare, and Sex.\nLet's now transform our train set and test set in a more compact datasets."},{"metadata":{"trusted":true,"_uuid":"f4b209d26acc92bf8e2d38fbcd8093db6a402f91","collapsed":true},"cell_type":"code","source":"model = SelectFromModel(clf, prefit=True)\ntrain_reduced = model.transform(train)\nprint(train_reduced.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5b603d35e5e60744833be345d590d34a861969fa","collapsed":true},"cell_type":"code","source":"test_reduced = model.transform(test)\nprint(test_reduced.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8112a06e32ad3eb7831290f0b03a4c429a8b2656"},"cell_type":"markdown","source":"Now we're down to a lot less features.\n\nWe'll see if we'll use the reduced or the full version of the train set."},{"metadata":{"_uuid":"81dd7f31f77fffc8e307304b11342d105b57b307"},"cell_type":"markdown","source":"# Data Correlation Analysis\nWe will visualize the correlation of features that we found above using heatmap. In heat map, we will use absolute values of the correlation, this make variables which have close to 0 correlation appear dark, and everything which is correlated (or anti-correlated) is bright. The shades of the color gives the relative strength of correlation.\n\n##### Note: np.abs() : gives absolute value element wise"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"dc9c0a39c6c330a2290f71fd0cf25303e17f745a"},"cell_type":"code","source":"corr = train.corr()\nsns.heatmap(np.abs(corr), xticklabels = corr.columns, yticklabels = corr.columns)\nprint('Heatmap of the correlation values for the ')\n\ncorr_test = train_reduced.corr()\nsns.heatmap(np.abs(corr_test), xticklabels = corr_test.columns, yticklabels = corr_test.columns)\nprint('Heatmap of the correlation values (reduced)')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0218508b3c8f66d7fa08c3c07b268980e0159905"},"cell_type":"markdown","source":"## Let's try different base models"},{"metadata":{"trusted":true,"_uuid":"5391fe1efaf21780fec51d33834b95699cb9be14","collapsed":true},"cell_type":"code","source":"logreg = LogisticRegression()\nlogreg_cv = LogisticRegressionCV()\nrf = RandomForestClassifier()\ngboost = GradientBoostingClassifier()\nsvc = SVC()\nknn = KNeighborsClassifier()\n\nmodels = [logreg, logreg_cv, rf, gboost, svc, knn]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"286408c060f2e8a50dab7f750d60ab9a0a2b44d9","collapsed":true},"cell_type":"code","source":"for model in models:\n    print('Cross-validation of : {0}'.format(model.__class__))\n    score = compute_score(clf=model, X=train_reduced, y=targets, scoring='accuracy')\n    print('CV score = {0}'.format(score))\n    print('****')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e67124acd4581f5368e022f3ee0ea9e83019dc60","collapsed":true},"cell_type":"code","source":"for model in models:\n    print('Cross-validation of : {0}'.format(model.__class__))\n    score = compute_score(clf=model, X=train, y=targets, scoring='accuracy')\n    print('CV score = {0}'.format(score))\n    print('****')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7ef59f5ee8e362e046c14f531871e9e413fe3b65"},"cell_type":"markdown","source":"We will be using a Random Forest model. It may not be the best model for this task but we'll tune it. This work can be applied to different models.\n\nRandom Forest are quite handy. They do however come with some parameters to tweak in order to get an optimal model for the prediction task."},{"metadata":{"_uuid":"7cb68cfdbfda410c8891a5924201132429bfeecb"},"cell_type":"markdown","source":">### Hyper-parameter Tuning for dealing with Overfitting and Underfitting"},{"metadata":{"trusted":true,"_uuid":"7742b87848921361a911901e079c03844da248b3","collapsed":true},"cell_type":"code","source":"# Applying Grid Search to find the best model and the best parameters\nfrom sklearn.model_selection import GridSearchCV\nparameters = {\n             'max_depth' : [4, 6, 8],\n             'n_estimators': [10, 50,70],\n             'max_features': ['sqrt', 'auto', 'log2'],\n             'min_samples_split': [0.001,0.003,0.01],\n             'min_samples_leaf': [1, 3, 10],\n             'bootstrap': [True,False],\n             }\nforest = RandomForestClassifier()\ncross_validation = StratifiedKFold(n_splits=5)\n\ngrid_search = GridSearchCV(estimator = forest,\n                           param_grid = parameters,\n                           scoring = 'accuracy',\n                           cv = cross_validation,\n                           verbose = 1\n                          )\ngrid_search = grid_search.fit(train_reduced, targets)\nparams = grid_search.best_params_\n\nprint('Best score: {}'.format(grid_search.best_score_))\nprint('Best parameters: {}'.format(grid_search.best_params_))\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"401b1289fd6ca6e7fd96cef6cd65827d15a22a9a"},"cell_type":"markdown","source":"###### Now we use the result of Best params as hyperparameters to train our final machine learning \"model\"  "},{"metadata":{"trusted":true,"_uuid":"4d874e27b161ef21a077e9e6d86b7a75ee5af684","collapsed":true},"cell_type":"code","source":"model = RandomForestClassifier(**params)\n\nmodel.fit(train_reduced, targets)\nprint(compute_score(clf=model, X=train_reduced, y=targets, scoring='accuracy'))\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b4fe3391798ec5a543ed712b37446453195a2bfe"},"cell_type":"markdown","source":"### So we obtain a 82+% classification accuracy with our machine learning model. We will submit our prediction to check how good we did in our test data set. We will use the predictions made from Random Forest y_pred."},{"metadata":{"trusted":true,"_uuid":"104b7dfa2ff75f0f36aadda1dcb02aa24d41c040","collapsed":true},"cell_type":"code","source":"y_pred = model.predict(test_reduced).astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7b6e9cb7ccb4ea2445eea4cace74954e970d6c03","collapsed":true},"cell_type":"code","source":"test_data = pd.read_csv('../input/test.csv')\nsubmission = pd.DataFrame({\n        \"PassengerId\": test_data[\"PassengerId\"],\n        \"Survived\": y_pred\n    })\nsubmission.to_csv('titanic.csv', index=False)\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}