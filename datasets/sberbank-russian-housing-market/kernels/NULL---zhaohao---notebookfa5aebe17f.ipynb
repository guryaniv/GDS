{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "6c3eab30-b1b1-3b6f-658d-568fe5d5c062"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "from sklearn.model_selection import cross_val_score, train_test_split\n",
        "%matplotlib inline\n",
        "import xgboost as xgb\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "8702b68f-d722-5672-29bd-100fa384fc2a"
      },
      "outputs": [],
      "source": [
        "total = pd.read_csv('../input/train.csv')\n",
        "test = pd.read_csv('../input/test.csv')\n",
        "macro = pd.read_csv('../input/macro.csv')\n",
        "\n",
        "df_total = pd.merge(total, macro, on='timestamp', how='left')\n",
        "df_total.drop('id', axis = 1, inplace = True)\n",
        "df_total['price_doc'] = np.log1p(df_total['price_doc'])\n",
        "\n",
        "df_test = pd.merge(test, macro, on='timestamp', how='left')\n",
        "df_test.drop('id', axis = 1, inplace = True)\n",
        "df_all = pd.concat([df_total,df_test], keys = ['total','test'])\n",
        "\n",
        "print ('total: ', total.shape)\n",
        "# print total.head()\n",
        "print ('test: ', test.shape)\n",
        "# print test.head()\n",
        "print ('macro: ', macro.shape)\n",
        "# print macro.head()\n",
        "print ('all: ', df_all.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "94e2e5ab-0bce-0170-dbac-0cf366ee3fb8"
      },
      "outputs": [],
      "source": [
        "def missingPattern(df):\n",
        "    numGroup = list(df._get_numeric_data().columns)\n",
        "    catGroup = list(set(df.columns) - set(numGroup))\n",
        "    print('Total categorical/numerical variables are %s/%s' % (len(catGroup), len(numGroup)))\n",
        "    \n",
        "    #missing data\n",
        "    n = df.shape[0]\n",
        "    count = df.isnull().sum()\n",
        "    percent = 1.0 * count / n\n",
        "    dtype = df.dtypes\n",
        "    # correlation\n",
        "    missing_data = pd.concat([count, percent,dtype], axis=1, keys=['Count', 'Percent', 'Type'])\n",
        "    missing_data.sort_values('Count', ascending = False, inplace = True)\n",
        "    missing_data = missing_data[missing_data['Count'] > 0]\n",
        "    print ('Total missing columns is %s' % len(missing_data))\n",
        "\n",
        "    return numGroup, catGroup, missing_data\n",
        "\n",
        "numGroup, catGroup, missing_data = missingPattern(df_all)\n",
        "missing_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "7d980b5e-4274-341d-1c16-091cbebff727"
      },
      "outputs": [],
      "source": [
        "import operator\n",
        "def getCorr(df, numGroup, eps, *verbose):\n",
        "    corr = df[numGroup].corr()\n",
        "#     plt.figure(figsize=(8, 6))\n",
        "#     plt.pcolor(corr, cmap=plt.cm.Blues)\n",
        "#     plt.show()\n",
        "    corr.sort_values([\"price_doc\"], ascending = False, inplace = True)\n",
        "    highCorrList = list(corr.price_doc[abs(corr.price_doc)>eps].index)\n",
        "    if verbose:\n",
        "        print (\"Find most important features relative to target\")\n",
        "        print (corr.price_doc[abs(corr.price_doc)>eps])\n",
        "    return corr, highCorrList\n",
        "corr, highCorrList = getCorr(df_all.ix['total',:], numGroup, 0.4, True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c3f601cf-b561-4e3a-6a7f-5b08cdef69da"
      },
      "outputs": [],
      "source": [
        "# for numerical variable, draw scatter plot(x vs y) and histogram plot(total vs test)    \n",
        "def scatterplotNum(df, varNum, ax):\n",
        "    plt.scatter(df[varNum], df['price_doc'])\n",
        "    plt.xlabel(varNum)\n",
        "    plt.ylabel('Price_doc')\n",
        "\n",
        "def hishplotNum(df, varNum, ax):\n",
        "    plt.hist(df.ix['total',varNum], bins = 50, alpha = 0.4)\n",
        "    plt.hist(df.ix['test',varNum], bins = 50, color = 'r', alpha = 0.4)\n",
        "    plt.xlabel(varNum)\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.legend(('total','test'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "995eeecb-9773-ad56-1fe2-43fd1ce369aa"
      },
      "outputs": [],
      "source": [
        "high_missing_data = missing_data[missing_data['Percent'] > 0.5]\n",
        "print (high_missing_data.index)\n",
        "XYcorr = corr['price_doc'].to_dict()\n",
        "\n",
        "for i in XYcorr:\n",
        "    if i != 'price_doc' and XYcorr[i] > -1 and i in high_missing_data.index:\n",
        "        fig = plt.figure(i)\n",
        "        ax1 = fig.add_subplot(1,1,1)\n",
        "        scatterplotNum(df_all.ix['total'], i, ax1)\n",
        "        plt.title('correlation is %.4f' %(XYcorr[i]))\n",
        "\n",
        "#         ax2 = fig.add_subplot(1,2,2)\n",
        "#         hishplotNum(pd.concat([total,test],keys = ['total','test']), i, ax2)\n",
        "#         plt.title('correlation is %.4f' %(XYcorr[i]))\n",
        "        plt.gcf().set_size_inches(6, 4)\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "8a88d23b-55d2-d31c-4ef3-359cda3489ee"
      },
      "outputs": [],
      "source": [
        "# remove the heavy missing features\n",
        "for i in high_missing_data.index:\n",
        "    df_all.drop(i, axis = 1, inplace = True)\n",
        "\n",
        "print('all: ', df_all.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d6cd46cd-960e-c0c2-72e7-2127b3432a16"
      },
      "outputs": [],
      "source": [
        "# total missing\n",
        "# macro missing\n",
        "basic_missing = list((set(missing_data.index) - set(high_missing_data.index)) & set(total.columns))\n",
        "macro_missing = list((set(missing_data.index) - set(high_missing_data.index)) & set(macro.columns))\n",
        "print('missing in basic: ', len(basic_missing))\n",
        "print('missing in macro: ', len(macro_missing))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "db71d342-a989-e933-6dd6-eb6b4ea78389"
      },
      "outputs": [],
      "source": [
        "### for macro info, look at the missing value info(mean, std) groupby yr, year_month\n",
        "df_all['timestamp'] = pd.to_datetime(df_all['timestamp'])\n",
        "df_all['year'] = df_all.timestamp.dt.year\n",
        "df_all['year_month'] = df_all.timestamp.dt.month + df_all.timestamp.dt.year * 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "634c299b-701a-8048-d4a0-d0c5008f9cc3"
      },
      "outputs": [],
      "source": [
        "# life_sq and full_sq are highly related to price_doc\n",
        "# life_sq <= full_sq and full_sq has no missing value\n",
        "\n",
        "# life_sq or full_sq <= 5\n",
        "df_all['life_sq'][df_all['life_sq']<=5] = df_all['full_sq'][df_all['life_sq']<=5]\n",
        "df_all['full_sq'][df_all['full_sq']<=5] = df_all['life_sq'][df_all['full_sq']<=5]\n",
        "\n",
        "\n",
        "# # life_sq or full_sq > 200 \n",
        "df_all['life_sq'].ix['total'][1084] = 28.1\n",
        "df_all['life_sq'].ix['total'][4385] = 42.6\n",
        "df_all['life_sq'].ix['total'][9237] = 30.1\n",
        "df_all['life_sq'].ix['total'][9256] = 45.8\n",
        "df_all['life_sq'].ix['total'][9646] = 80.2\n",
        "df_all['life_sq'].ix['total'][13546] = 74.78\n",
        "df_all['life_sq'].ix['total'][13629] = 25.9\n",
        "df_all['life_sq'].ix['total'][21080] = 34.9\n",
        "df_all['life_sq'].ix['total'][26342] = 43.5\n",
        "\n",
        "df_all['life_sq'].ix['test'][601] = 74.2\n",
        "df_all['life_sq'].ix['test'][1896] = 36.1\n",
        "df_all['life_sq'].ix['test'][2031] = 23.7\n",
        "df_all['life_sq'].ix['test'][2791] = 86.9\n",
        "df_all['life_sq'].ix['test'][5187] = 28.3\n",
        "\n",
        "df_all['full_sq'].ix['total'][1478] = 35.3\n",
        "df_all['full_sq'].ix['total'][1610] = 39.4\n",
        "df_all['full_sq'].ix['total'][2425] = 41.2\n",
        "df_all['full_sq'].ix['total'][2780] = 72.9\n",
        "df_all['full_sq'].ix['total'][3527] = 53.3\n",
        "df_all['full_sq'].ix['total'][5944] = 63.4\n",
        "df_all['full_sq'].ix['total'][7207] = 46.1\n",
        "\n",
        "\n",
        "# life_sq > full_sq\n",
        "df_all['life_sq'][df_all.life_sq > df_all.full_sq] = df_all['full_sq'][df_all.life_sq > df_all.full_sq]\n",
        "\n",
        "# kitch_sq > full_sq\n",
        "\n",
        "df_all['kitch_sq'][df_all.kitch_sq > df_all.full_sq] = \\\n",
        "            df_all['full_sq'][df_all.kitch_sq > df_all.full_sq] - df_all['life_sq'][df_all.kitch_sq > df_all.full_sq]\n",
        "\n",
        "\n",
        "# else\n",
        "# floor > max_floor\n",
        "df_all['max_floor'][df_all.floor > df_all.max_floor] = \\\n",
        "        df_all['floor'][df_all.floor > df_all.max_floor] + df_all['max_floor'][df_all.floor > df_all.max_floor]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ae194c79-52e3-4e8f-7ce7-6687fc98a475"
      },
      "outputs": [],
      "source": [
        "# fill the missing value in train and test\n",
        "def basicmissingFill(df):\n",
        "    # num variables\n",
        "    # pre-processing\n",
        "    n = df.shape[0]\n",
        "    \n",
        "    \n",
        "    df_all['life_sq'][df_all.life_sq.isnull()] = df_all['full_sq'][df_all.life_sq.isnull()]\n",
        "\n",
        "\n",
        "    df['state'] = df['state'].replace({33:3})\n",
        "    df['build_year'][df['build_year'] == 20052009] = 2005\n",
        "    df['build_year'][df['build_year'] == 4965] = float('nan')\n",
        "    df['build_year'][df['build_year'] == 0] = float('nan')\n",
        "    df['build_year'][df['build_year'] == 1] = float('nan')\n",
        "    df['build_year'][df['build_year'] == 3] = float('nan')\n",
        "    df['build_year'][df['build_year'] == 71] = float('nan')\n",
        "    df['build_year'][df['build_year'] == 20] = 2000\n",
        "    df['build_year'][df['build_year'] == 215] = 2015\n",
        "    df['build_year'].ix['total'][13117] = 1970\n",
        "\n",
        "\n",
        "    \n",
        "    # zero-filling count feature \n",
        "    zero_fil = ['build_count_brick','build_count_block','build_count_mix','build_count_before_1920',\\\n",
        "               'build_count_1921-1945','build_count_1946-1970','build_count_1971-1995','build_count_after_1995',\\\n",
        "               'build_count_monolith','build_count_slag','build_count_wood','build_count_panel','build_count_frame',\\\n",
        "               'build_count_foam','preschool_quota']\n",
        "    for i in zero_fil:\n",
        "        df[i] = df[i].fillna(0)\n",
        "    \n",
        "    # mode-filling: count feature and ID\n",
        "    mode_fil = ['state','ID_railroad_station_walk','build_year','material','num_room']\n",
        "    for i in mode_fil:\n",
        "        df[i] = df[i].fillna(df[i].mode()[0]) \n",
        "\n",
        "    # mean-filling\n",
        "    mean_fil = ['cafe_avg_price_500','cafe_avg_price_1000','cafe_avg_price_1500','cafe_avg_price_2000',\\\n",
        "               'cafe_avg_price_3000','cafe_avg_price_5000','cafe_sum_500_max_price_avg','cafe_sum_500_min_price_avg',\\\n",
        "               'cafe_sum_1000_max_price_avg','cafe_sum_1000_min_price_avg','cafe_sum_1500_max_price_avg',\\\n",
        "               'cafe_sum_1500_min_price_avg','cafe_sum_2000_max_price_avg','cafe_sum_2000_min_price_avg',\\\n",
        "               'cafe_sum_3000_max_price_avg','cafe_sum_3000_min_price_avg','cafe_sum_5000_max_price_avg',\\\n",
        "               'cafe_sum_5000_min_price_avg','railroad_station_walk_min','railroad_station_walk_km',\\\n",
        "               'school_quota','raion_build_count_with_material_info','prom_part_5000',\\\n",
        "               'raion_build_count_with_builddate_info','green_part_2000','metro_km_walk','metro_min_walk',\\\n",
        "               'hospital_beds_raion']\n",
        "    for i in mean_fil:\n",
        "        grouped = df[['year',i]].groupby('year')\n",
        "        df[i] = grouped.transform(lambda x: x.fillna(x.mean()))\n",
        "        \n",
        "    # exception: 'kitch_sq','floor','max_floor'\n",
        "    df['kitch_sq'][df.kitch_sq.isnull()] = df['full_sq'][df.kitch_sq.isnull()] - df['life_sq'][df.kitch_sq.isnull()]\n",
        "    df['floor'] = df['floor'].fillna(df['floor'].mean())\n",
        "    df['max_floor'][df.max_floor.isnull()] = df['floor'][df.max_floor.isnull()]\n",
        "    \n",
        "    #================\n",
        "    # Cat. variables\n",
        "    df['product_type'] = df['product_type'].fillna(df['product_type'].mode()[0])\n",
        "    \n",
        "    return df\n",
        "\n",
        "df_all = basicmissingFill(df_all)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "dac84269-50f5-4a55-9c19-cce9ca33823e"
      },
      "outputs": [],
      "source": [
        "print('basic_missing filling finished: ', df_all[basic_missing].isnull().sum().sum() == 7662)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "addb7c67-0c54-5004-251a-a1351edaaa8c"
      },
      "outputs": [],
      "source": [
        "# for Cat features in macro_missing\n",
        "macro_missing_obj = []\n",
        "for i in macro_missing:\n",
        "    if df_all[i].dtype == object:\n",
        "        grouped = df_all[['year',i]].groupby(['year',i])\n",
        "        print (grouped.agg(len))\n",
        "        macro_missing_obj.append(i)\n",
        "        print (missing_data.ix[i])\n",
        "        print ('\\n')\n",
        "# consider to drop macro_missing_obj\n",
        "for i in macro_missing_obj:\n",
        "    df_all.drop(i, axis = 1, inplace = True)\n",
        "    macro_missing.remove(i)\n",
        "\n",
        "print('macro missing features count: ', len(macro_missing) )\n",
        "print ('df_all shape: ', df_all.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "8d7312ed-5117-c9ff-9818-a52bc49517a6"
      },
      "outputs": [],
      "source": [
        "# for num features in macro_missing\n",
        "# filling strategy: for each feature->if 2015 is not null: fillna the mean(2015) else: fillna the mean(2014)\n",
        "def macromissingFill(df):\n",
        "    for i in macro_missing:\n",
        "        fill2014 = np.nanmean(df[i][df['year']==2014])\n",
        "        fill2015 = np.nanmean(df[i][df['year']==2015])\n",
        "        # income_per_cap: the only macro_missing feature which is not agg by year\n",
        "        if ~np.isnan(fill2015):\n",
        "            df[i] = df[i].fillna(fill2015)\n",
        "        else:\n",
        "            df[i] = df[i].fillna(fill2014)\n",
        "\n",
        "    return df\n",
        "\n",
        "df_all = macromissingFill(df_all)\n",
        "print ('macro_missing filling finished: ', df_all[macro_missing].isnull().sum().sum() == 0)\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "7e945eec-80f0-dbac-797c-aa39f4e4be30"
      },
      "outputs": [],
      "source": ""
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "bb8c7e8d-e37a-3bc0-8032-fee3945524b7"
      },
      "outputs": [],
      "source": ""
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ad55a930-a574-3ed0-7f4c-92f023e28f67"
      },
      "outputs": [],
      "source": ""
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "0e8f315c-5eda-2cdc-8283-67e1c310f7da",
        "collapsed": true
      },
      "outputs": [],
      "source": ""
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "af451fd1-0bfb-0f2e-a2a1-2ecaeed2fde6"
      },
      "outputs": [],
      "source": ""
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "91c401e8-320d-0dbd-9ad0-8aa930d3f9f2"
      },
      "outputs": [],
      "source": ""
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "98ad4274-580c-08a5-d9f5-a4744588a2f6"
      },
      "outputs": [],
      "source": ""
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "6733dd8f-7e80-6cba-b4f8-3ec3ed3c8910"
      },
      "outputs": [],
      "source": [
        "\n",
        "Ytotal = df_total_num['price_doc'].as_matrix()\n",
        "testId = list(df_test_num['id'])\n",
        "df_total_num.drop('price_doc', axis = 1, inplace = True)\n",
        "df_test_num.drop('id', axis = 1, inplace = True)\n",
        "print 'Current training numerical variables count is %d '  %(df_total_num.shape[1])\n",
        "print 'Current training categorical variables count is %d '  %(df_total_cat.shape[1])\n",
        "print 'Current test numerical variables count is %d '  %(df_test_num.shape[1])\n",
        "print 'Current test categorical variables count is %d '  %(df_test_cat.shape[1])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "35cd478c-f7dc-51f7-a580-781da318beaf"
      },
      "source": ""
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "a4a636d5-6c73-642c-eb1a-d4e29f7477d4"
      },
      "outputs": [],
      "source": [
        "# numerical variables mean-variance normalization\n",
        "# merge numerical & categorical variables\n",
        "from sklearn import preprocessing\n",
        "\n",
        "# Xtotal = np.hstack((df_total_num.as_matrix(),df_total_cat.as_matrix()))\n",
        "# Xtest = np.hstack((df_test_num.as_matrix(),df_test_cat.as_matrix()))\n",
        "# Xtotal = df_total_num.as_matrix()\n",
        "# Xtest = df_test_num.as_matrix()\n",
        "\n",
        "\n",
        "# scaler = preprocessing.StandardScaler().fit(df_total_num)\n",
        "# # Xtotal = np.hstack((scaler.transform(df_total_num.as_matrix()),df_total_cat.as_matrix()))\n",
        "# # Xtest = np.hstack((scaler.transform(df_test_num.as_matrix()),df_test_cat.as_matrix()))\n",
        "# Xtotal = scaler.transform(df_total_num.as_matrix())\n",
        "# Xtest = scaler.transform(df_test_num.as_matrix())\n",
        "\n",
        "Xtr, Xval, Ytr, Yval = train_test_split(Xtotal, Ytotal, test_size = 0.25, random_state = 200)\n",
        "print \"Xtr : \" , Xtr.shape\n",
        "print \"Xval : \" , Xval.shape\n",
        "print \"Ytr : \" , Ytr.shape\n",
        "print \"Yval : \" , Yval.shape\n",
        "print \"Xtest : \", Xtest.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "201ce529-5d51-78ca-795b-43e33f63f21d"
      },
      "source": ""
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "023c809d-389b-1e9c-5418-4f0432ce6b3e",
        "collapsed": true
      },
      "outputs": [],
      "source": ""
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "12ea9c3c-acfb-0e28-5716-9a900bbb833c",
        "collapsed": true
      },
      "outputs": [],
      "source": ""
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "f8594316-c95d-3382-bfbf-dd3809fadbc1"
      },
      "source": ""
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "51a91556-bdc5-0773-7394-a7551836cf98",
        "collapsed": true
      },
      "outputs": [],
      "source": ""
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "98c00807-de04-fa08-90c3-1e96601dcb6a"
      },
      "source": ""
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "100f3e27-a35f-dd63-1de2-1bdfca8d71cd",
        "collapsed": true
      },
      "outputs": [],
      "source": ""
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}