{"cells":[{"metadata":{"_uuid":"f76cd3a466a6ee8e3f7822ef9ac2e2e56aaa7a60"},"cell_type":"markdown","source":"Hi there! This is my first kernel dealing with textual data so any constructive feedabacks are higly appreciated.\n\nThis dataset contains data of over 7 topics namely biology, robotics, cryptography, diy, travel, cooking, robotics and physics extracted from Stack Exchange. Each of these topics except physics have been classified as to which topic data belongs. So our task is to do predictions on unseen physics questions.\n\nSince our data won't be related to each other for example tags in travel won't be related to tags in cryptography hence I will be using unsupervised learning on physics dataset which is the test dataset. "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"#Importing all the neccesary libraries\nimport numpy as np \nimport pandas as pd \n\nimport os\nprint(os.listdir(\"../input\"))\n\nimport regex as re\nimport string\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom wordcloud import WordCloud\n\nimport matplotlib.pyplot as plt\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"data={'bio':pd.read_csv('../input/biology.csv',index_col=0),\n      'robo':pd.read_csv('../input/robotics.csv',index_col=0),\n      'cryp':pd.read_csv('../input/crypto.csv',index_col=0),\n      'diy':pd.read_csv('../input/diy.csv',index_col=0),\n      'cooking':pd.read_csv('../input/cooking.csv',index_col=0),\n      'travel':pd.read_csv('../input/travel.csv',index_col=0),\n      'test':pd.read_csv('../input/test.csv',index_col=0),\n     }\ndata['robo'][:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"366cb5190c19f8b61a194e823f5fd31e8504cec8"},"cell_type":"code","source":"stops = set(stopwords.words(\"english\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e2357a716d60c63ec842d93cd8d722a580642d75"},"cell_type":"code","source":"def clean_data(data_content):\n    \n    content = data_content.apply(lambda x: re.sub(r'\\<[^<>]*\\>','',x.lower()))\n    content = content.apply(lambda x: re.sub(r'[^a-zA-Z0-9]',' ',x))\n    content = content.apply(lambda x: word_tokenize(x))\n    #Removing stopwords\n    content = content.apply(lambda x: [i for i in x if i not in stops])\n    return(content)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2a9e4ac7aa5b0c1730a72e93dafa5591ff9abb76"},"cell_type":"code","source":"for df in data:\n    data[df].content = clean_data(data[df].content)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b84566349793cfe83c154d6606f9456ec34d8f75"},"cell_type":"code","source":"data['robo'].content[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dfd36dac2ee37079c1874e2c866c633f129f7724"},"cell_type":"code","source":"for df in data:\n    data[df].title = clean_data(data[df].title)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3f09b8fbe99140d78961f52a1056baa651db94d2"},"cell_type":"code","source":"data['robo'].title[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c3e09ff9960311d3e285ce1a18f22bbc64b025a4"},"cell_type":"code","source":"text = ' '\nfor x in data['robo'].content:\n    for y in x:\n        text+=' '+y\nprint(text[:500])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d3528b780e59dda1e476af3e3c297f248d29ba31"},"cell_type":"code","source":"plt.figure(figsize=(20,10))\nwc = WordCloud(max_words=1000,random_state=1).generate(text)\nplt.imshow(wc)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"86567735dfb23fdf6e8c32782ba8db085ad927d0"},"cell_type":"code","source":"cooking = ' '\nfor x in data['cooking'].title:\n    for y in x:\n        cooking+=' '+y\nprint(cooking[:500])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9114ead33b786fd5fcf0bb5a6e2b19d1511f0c08"},"cell_type":"code","source":"plt.figure(figsize=(20,10))\nwf = WordCloud(background_color='white',max_words=1000,random_state=1).generate(cooking)\nplt.imshow(wf)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ee2a6103e475c600e239f95513c9207e2977aad7"},"cell_type":"markdown","source":"## WordCloud for cryptography"},{"metadata":{"trusted":true,"_uuid":"1eb8a17caa47cd31f55b4fa58f8784c96bf59e9a"},"cell_type":"code","source":"crypt = ' '\nfor i in data['cryp'].content:\n    for j in i:\n        crypt+=' '+j","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"60317584632f52b320b0c7ad2c857c51b20addb1"},"cell_type":"code","source":"plt.figure(figsize=(20,10))\nwg = WordCloud(background_color='black',max_words=1000,random_state=1).generate(crypt)\nplt.imshow(wg)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9670d21354a062046e4a058b9324bc938112f0be"},"cell_type":"markdown","source":"## **Stemming ** "},{"metadata":{"trusted":true,"_uuid":"075857d029705d3eb06418c4d4080869f969ef07"},"cell_type":"code","source":"wordnet = WordNetLemmatizer()\ndata['test'].title = data['test'].title.apply(lambda x:[wordnet.lemmatize(i,pos='v') for i in x])\ndata['test'].content = data['test'].content.apply(lambda x:[wordnet.lemmatize(i,pos='v') for i in x])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3c2fe8ac6b73ffcf723d3ca2ac1533f0cbe35b52"},"cell_type":"code","source":"tst = ' '\nfor i in data['test'].title:\n    for j in i:\n        tst+=' '+j     ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a00b3892ec39b165b762846807f7e5155d18ddbd"},"cell_type":"markdown","source":"## WordCloud for physics dataset"},{"metadata":{"trusted":true,"_uuid":"64199bc7fa784236c4cf53377cf2c081e696a382"},"cell_type":"code","source":"plt.figure(figsize=(20,10))\nphy = WordCloud(background_color='white',max_words=1000,random_state=1).generate(tst)\nplt.imshow(phy)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b0f84763f764a755a00da13a01eca7785345e47b"},"cell_type":"markdown","source":"Vectrorizing data using TfidVectrorizer which uses the concept of term frequency and inverse document frequency to get rid of all non-consequential tokens from being vectorized.\nFor more details see https://www.quora.com/How-does-TfidfVectorizer-work-in-laymans-terms"},{"metadata":{"trusted":true,"_uuid":"38ffdc7773f9af6b47c1e6ee5d121df719deb835"},"cell_type":"code","source":"def identity_tokenizer(text):\n  return text\nvect = TfidfVectorizer(tokenizer=identity_tokenizer,lowercase=False)\nx = vect.fit_transform(data['test'].title.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e390f73188cf69739f15745f413513f667ddfbb7"},"cell_type":"code","source":"indices = np.argsort(vect.idf_)[::-1]\nfeatures = vect.get_feature_names()\ntop_n = 50\ntop_features = [features[i] for i in indices[:top_n]]\ntop_features","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8a711407a705eb87c93d6e4185d60c8424da8af6"},"cell_type":"markdown","source":"**k-means clustering**\n\nIn general, k-means is the first choice for clustering because of its simplicity. Here, the user has to define the number of clusters (Post on how to decide the number of clusters would be dealt later). The clusters are formed based on the closeness to the center value of the clusters. The initial center value is chosen randomly. K-means clustering is top-down approach, in the sense, we decide the number of clusters (k) and then group the data points into k clusters."},{"metadata":{"trusted":true,"_uuid":"877f6cfbbbdcbbabe439872ffae4edfa6fe68289"},"cell_type":"code","source":"from sklearn.cluster import KMeans\nmodel = KMeans(n_clusters=20, init='k-means++', max_iter=100, n_init=1)\nmodel.fit(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"849d0a5678ba43cf52f3ab25e550b074c188342c"},"cell_type":"code","source":"print(\"Top terms per cluster:\")\norder_centroids = model.cluster_centers_.argsort()[:, ::-1]\nterms = vect.get_feature_names()\nfor i in range(20):\n    print (\"Cluster %d:\" % i,)\n    for ind in order_centroids[i, :10]:\n        print (' %s' % terms[ind],)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8cc008154a6404a16cbda36b6e20f0ba3d2b13c3"},"cell_type":"markdown","source":"So from the output we can infer following points:\n\nCluster 1 classifies text related to 'angular momentum', 'torque' which can be associated to 'motor'\n\nCluster 2 is related to 'visible light source' \n\nCluster 3 deal with 'kinetic' and ' potential' energy which can be used to explain 'energy conservation'\n\nCluster 4 possibly relates to 'physics equations'\n\nCluster 10 has term like 'singularity' which is related to 'black hole'\n\nSimilarly we can draw other conclusions too.\n\nAny feedbacks to improve it further are appreciated.\n\nThank you!"},{"metadata":{"trusted":true,"_uuid":"b4a4f76e0b7ba8b464faeeac7178a80dd59195e2"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}