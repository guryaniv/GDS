{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "e03eed99-4a6a-1493-5aa4-39e5c5ddd14b",
        "_active": false
      },
      "source": "## In this kernel we use a random forest to predict house prices.  \n\nThere are 6 parts to this kernel:\n\n1. Import the libraries and data \n1. Prepare the data \n1. Train a decision tree\n1. Train the random forest \n1. Understand the forest \n1. Create predictions",
      "execution_count": null,
      "outputs": [],
      "execution_state": "idle"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "df2b9108-3c34-477b-797c-e3e4a7edb7df",
        "_active": false
      },
      "source": "## 1a. Import the libraries we are going to use\nHere we need two full libraries:\n**numpy** (linear algebra and mathematics) and **pandas** (data manipulation and i/o)\n\nWe also need some bits from **sklearn** - in particular the RandomForestRegressor and the preprocessing unit.\n\nIt is good practice to only import the bits you need from sklearn as it is quite a big library.",
      "execution_count": null,
      "outputs": [],
      "execution_state": "idle"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "819578df-38d2-d569-10c3-8545484286ce",
        "_active": false
      },
      "outputs": [],
      "source": "import numpy as np\nimport pandas as pd\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor # import the random forest model\nfrom sklearn import  preprocessing # used for label encoding and imputing NaNs\n\nimport datetime as dt # we will need this to convert the date to a number of days since some point\n\nfrom sklearn.tree import export_graphviz\n# import pydotplus\nimport six\n\nimport matplotlib.pyplot as plt\n%matplotlib inline",
      "execution_state": "idle"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "7bb4ec51-5f50-ff29-ee73-935f6b985392",
        "_active": false
      },
      "source": "## 1b. Next we import the data",
      "execution_count": null,
      "outputs": [],
      "execution_state": "idle"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "55d4452a-6959-5067-8230-5d238e5d3077",
        "_active": false
      },
      "outputs": [],
      "source": "df_train = pd.read_csv('../input/train.csv', parse_dates=['timestamp'])\ndf_test = pd.read_csv('../input/test.csv', parse_dates=['timestamp'])\ndf_macro = pd.read_csv('../input/macro.csv', parse_dates=['timestamp'])\n\ndf_train.head()",
      "execution_state": "idle"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "903d8f4e-1148-f09e-9911-5b4aafa05632",
        "_active": false
      },
      "source": "##  2. Data preparation\n\n - Create a vector containing the id's for our predictions\n - Create a vector of the target variables in the training set\n - Create joint train and test set to make data wrangling quicker and consistent on train and test\n - Removing the id (could it be a useful source of leakage?)\n - Convert the date into a number (of days since some point)\n - Deal with categorical variables\n - Deal with missing values",
      "execution_count": null,
      "outputs": [],
      "execution_state": "idle"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "61a219ad-60f1-8e3b-ab3d-7d998a58305d",
        "_active": false
      },
      "outputs": [],
      "source": "# Create a vector containing the id's for our predictions\nid_test = df_test.id\n\n#Create a vector of the target variables in the training set\n# Transform target variable so that loss function is correct (ie we use RMSE on transormed to get RMLSE)\n# ylog1p_train will be log(1+y), as suggested by https://github.com/dmlc/xgboost/issues/446#issuecomment-135555130\nylog1p_train = np.log1p(df_train['price_doc'].values)\ndf_train = df_train.drop([\"price_doc\"], axis=1)\n\n# Create joint train and test set to make data wrangling quicker and consistent on train and test\ndf_train[\"trainOrTest\"] = \"train\"\ndf_test[\"trainOrTest\"] = \"test\"\ndf_all = pd.concat([df_train, df_test])\n\n# Removing the id (could it be a useful source of leakage?)\ndf_all = df_all.drop(\"id\", axis=1)",
      "execution_state": "idle"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c7cb59ea-409a-6d2b-9e4c-903092d7cf3c",
        "_active": false
      },
      "outputs": [],
      "source": "# Convert the date into a number (of days since some point)\nfromDate = min(df_all['timestamp'])\ndf_all['timedelta'] = (df_all['timestamp'] - fromDate).dt.days.astype(int)\nprint(df_all[['timestamp', 'timedelta']].head())\ndf_all.drop('timestamp', axis = 1, inplace = True)",
      "execution_state": "idle"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "87c8afe3-2d57-2dd4-a13d-aa9191d02120",
        "_active": false
      },
      "source": "### Encoding categorical features\nWe will take a naive approach and assign a numeric value to each categorical feature in our training and test sets. \nSklearn's preprocessing unit has a tool called LabelEncoder() which can do just that for us. ",
      "execution_count": null,
      "outputs": [],
      "execution_state": "idle"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ae63e45c-96fe-04ca-6788-892130adf651",
        "_active": false
      },
      "outputs": [],
      "source": "for c in df_all.columns:\n    if df_all[c].dtype == 'object':\n        lbl = preprocessing.LabelEncoder()\n        lbl.fit(list(df_all[c].values)) \n        df_all[c] = lbl.transform(list(df_all[c].values))",
      "execution_state": "idle"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "d3265980-e62e-6b8f-4cfb-82eaffbd2b57",
        "_active": false
      },
      "source": "### Addressing problems with NaN in the data\n\nAs we saw from our EDA there were quite a lot of NaN in the data. Our model won't know what to do with these so we need to replace them with something sensible.\n\nThere are quite a few options we can use - the mean, median, most_frequent, or a numeric value like 0. Playing with these will give different results, for now I have it set to use the mean.\n\n This uses the mean of the column in which the missing value is located. ",
      "execution_count": null,
      "outputs": [],
      "execution_state": "idle"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "8609a033-3d9b-cbf6-ac39-f3a49e58ac1f",
        "_active": false
      },
      "outputs": [],
      "source": "# Create a list of columns that have missing values and an index (True / False)\ndf_missing = df_all.isnull().sum(axis = 0).reset_index()\ndf_missing.columns = ['column_name', 'missing_count']\nidx_ = df_missing['missing_count'] > 0\ndf_missing = df_missing.ix[idx_]\ncols_missing = df_missing.column_name.values\nidx_cols_missing = df_all.columns.isin(cols_missing)",
      "execution_state": "idle"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "f5c99dc6-69b4-c789-6c76-d14cd545e764",
        "_active": false
      },
      "outputs": [],
      "source": "# Instantiate an imputer\nimputer = preprocessing.Imputer(missing_values='NaN', strategy = 'most_frequent', axis = 0)\n\n# Fit the imputer using all of our data (but not any dates)\nimputer.fit(df_all.ix[:, idx_cols_missing])\n\n# Apply the imputer\ndf_all.ix[:, idx_cols_missing] = imputer.transform(df_all.ix[:, idx_cols_missing])",
      "execution_state": "idle"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "59d409aa-e486-f447-fa27-d03c3c51c240",
        "_active": false
      },
      "outputs": [],
      "source": "# See the results - note how all missing are replaced with the mode\ndf_all.head()",
      "execution_state": "idle"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "956ac088-dcda-7673-357c-f36659ea255b",
        "_active": false
      },
      "outputs": [],
      "source": "# Prepare separate train and test datasets\nidx_train = df_all['trainOrTest'] == 1\nidx_test = df_all['trainOrTest'] == 0\n\nx_train = df_all[idx_train]\nx_test = df_all[idx_test]",
      "execution_state": "idle"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "a5a236f8-3b6a-dbfc-5562-deed152efbdb",
        "_active": false
      },
      "source": "## The three step process below is common across many sklearn models\n\n**Step 1:** we create an object which is the type of model we want to fit (we have called this object \"Model\"). In this case we are dealing with a regression problem and want to fit a Random Forest model so we choose RandomForestRegressor.  \"Model\" is an instance of a RandomForestRegressor.\n\n**Step 2:** We train the model. We do this with our x and y training data. Remember that the y_train set is just the prediction we would like to make - in this instance the price price_doc. The x_train data is the information we are going to use to make that prediction. \n\n**Step 3:** Once we have fit the model we can then use it to make a prediction. We do this by called Model.Predict. We are looking to predict the house prices for our test data so we pass the test-data to the predict method and assign it to y_predict. This will contain our predicted set of house prices. ",
      "execution_count": null,
      "outputs": [],
      "execution_state": "idle"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "424ba02f-1a84-ba06-410b-362a42633760",
        "_active": false
      },
      "source": "### 3. Let's practise on a simple decision tree",
      "execution_count": null,
      "outputs": [],
      "execution_state": "idle"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "51853920-b654-8e7b-9e4f-661da8209d3f",
        "_active": false,
        "collapsed": false
      },
      "outputs": [],
      "source": "# Step 1: Instantiate a decision tree regressor\n# Choose a depth for the tree - something 3, 4 or 5 - not too large\nModel = DecisionTreeRegressor(max_depth = 3)",
      "execution_state": "idle"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "8135bcb2-edc1-4c4f-995e-0b433fd7016f",
        "_active": false,
        "collapsed": false
      },
      "outputs": [],
      "source": "# Step 2: Train the tree\n# The .fit method takes two main arguments, the features (in our case x_train) and \n# the target variable (in our case ylog1p_train)\n# Fill them in below and submit the code to train the tree\nModel.fit(X = x_train, y = ylog1p_train)",
      "execution_state": "idle"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "8d4757da-d896-73ce-3778-380dd6b258a9",
        "_active": false,
        "collapsed": false
      },
      "outputs": [],
      "source": "# Step 3: Make predictions \n# The predict method takes one main argument - the examples for which\n# we want to predict the target variable.  Here we will use the training data \n# itself i.e. x_train.  Fill this in below\nylog_pred = Model.predict(X = x_train )",
      "execution_state": "idle"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "83b3430f-ab1e-04ea-2349-3cd5f48233a2",
        "_active": false
      },
      "outputs": [],
      "source": "# Check the training error\n\n# Is the training error a reasonable estiamte of how this tree will perform on unseen data?\nnp.sqrt(np.mean((ylog_pred - ylog1p_train)**2))",
      "execution_state": "idle"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "1a1e2139-805b-a1ee-3790-79f1d0b3cd13",
        "_active": false
      },
      "outputs": [],
      "source": "# !!DO NOT RUN!!\n# The code below will not work in this kernel because pydotplus is not available\n# Now plot the tree \ndotfile = six.StringIO()\n\nexport_graphviz(Model, \n                out_file = dotfile, \n                max_depth = 3,\n                feature_names = x_train.columns,\n                filled = True,\n                rounded = True)\n\npydotplus.graph_from_dot_data(dotfile.getvalue()).write_png('DTR.png')",
      "execution_state": "idle"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "eb87cbf9-55d7-6119-9d3f-d8b14c2bdebe",
        "_active": false
      },
      "source": "### 4. Now a random forest\n\nThe parameter labelled n_estimators below, indicates the number of trees we would like in our forest.  The first time you run this kernel, we suggest you use something small  - between 10 and 50 just to check that the run time is not too slow.  If the run time is reasonable you can increase it (to 100 or more - but not now!!) in order to get better performance.",
      "execution_count": null,
      "outputs": [],
      "execution_state": "idle"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "37ac400b-b6d4-0a4e-2c14-efaaf7023893",
        "_active": false,
        "collapsed": false
      },
      "outputs": [],
      "source": "# Step 1: Instantiate a random forest regressor\nModel = RandomForestRegressor(n_estimators = 30, \n                              random_state = 2017, \n                              oob_score = True, \n                              max_features = 20,\n                              min_samples_leaf = 8)",
      "execution_state": "idle"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "659ce615-002a-fd38-2b3c-7b72183e2c4b",
        "_active": false,
        "collapsed": false
      },
      "outputs": [],
      "source": "# Step 2: Train the forest\n# Again fill in X and y below with x_train and ylog1p_train\nModel.fit(X = x_train , y = ylog1p_train)",
      "execution_state": "idle"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ec2c597e-6194-aae0-e410-44252b5ade0a",
        "_active": false,
        "collapsed": false
      },
      "outputs": [],
      "source": "# Step 3: Make predictions \n# Create predictions for the examples in x_train\nylog_pred = Model.predict(X = x_train )",
      "execution_state": "idle"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "a629aa22-6a34-59ae-b040-33ba75978da6",
        "_active": false
      },
      "source": "### Check the performance of the random forest",
      "execution_count": null,
      "outputs": [],
      "execution_state": "idle"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ba3ad55c-080e-57dd-2fc0-f0ca55ae1929",
        "_active": false
      },
      "outputs": [],
      "source": "# Check the training error\nnp.sqrt(np.mean((ylog_pred - ylog1p_train)**2)) # about 0.37 (if you use 100 trees)",
      "execution_state": "idle"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "5bfd93c8-8dbd-1d06-9dd0-50bbd424659f",
        "_active": false
      },
      "source": "The training error looks pretty good.  But it is only over the training data.  We don't know how well this forest does on data it has not seen.  Well actually we do.  Since the data to grow each tree is a bootstrap with replacement, only about 2/3 of the data is used each time.  We can use the \"OOB\" data to estimate performance on unseen data",
      "execution_count": null,
      "outputs": [],
      "execution_state": "idle"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "eb246566-bd75-07bf-c94e-16ab84adb6b4",
        "_active": false
      },
      "outputs": [],
      "source": "\nnp.sqrt(np.mean((Model.oob_prediction_ - ylog1p_train)**2)) # 0.47 slightly better than a simple tree.",
      "execution_state": "idle"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "5e741a7d-d75e-6c03-f87e-826304aefc7e",
        "_active": false
      },
      "source": "## 5a. What do these trees look like?",
      "execution_count": null,
      "outputs": [],
      "execution_state": "idle"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "7ccde92b-4ab9-96bb-938d-99e80ced271d",
        "_active": false
      },
      "outputs": [],
      "source": "# !!DO NOT RUN!!\n# The code below will not work in this kernel because pydotplus is not available\nfor idx in range(3):\n    dotfile = six.StringIO()\n    \n    export_graphviz(Model.estimators_[idx], \n                    out_file = dotfile, \n                    max_depth = 2,\n                    feature_names = x_train.columns)\n    \n    pydotplus.graph_from_dot_data(dotfile.getvalue()).write_png('dtree'+ str(idx) + '.png')",
      "execution_state": "idle"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "775824de-e24f-78ad-19d9-4d91588056d8",
        "_active": false
      },
      "source": "## 5b. Actual versus expected",
      "execution_count": null,
      "outputs": [],
      "execution_state": "idle"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "98686ea8-a969-0ea2-f97b-78d9edf7f43a",
        "_active": false
      },
      "outputs": [],
      "source": "fig, ax = plt.subplots()\nplt.scatter(Model.oob_prediction_, ylog1p_train)\nx = np.linspace(*ax.get_xlim())\nax.plot(x, x, color = 'black')\nplt.show()",
      "execution_state": "idle"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "3eb8bbd3-1d11-0ac8-85cc-51f49d4c2bff",
        "_active": false
      },
      "source": "## 5c. Variable importance",
      "execution_count": null,
      "outputs": [],
      "execution_state": "idle"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "2c1929dc-1be6-9b2a-2589-b4707dab754f",
        "_active": false
      },
      "outputs": [],
      "source": "# Create a dataframe of the variable importances\ndf_ = pd.DataFrame(df_all.columns, columns = ['feature'])\ndf_['fscore'] = Model.feature_importances_[:, ]",
      "execution_state": "idle"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "9d01df3d-95fd-3e9f-4cbf-35939b8033a1",
        "_active": false
      },
      "outputs": [],
      "source": "# Plot the relative importance of the top 10 features\ndf_['fscore'] = df_['fscore'] / df_['fscore'].max()\ndf_.sort_values('fscore', ascending = False, inplace = True)\ndf_ = df_[0:10]\ndf_.sort_values('fscore', ascending = True, inplace = True)\ndf_.plot(kind='barh', x='feature', y='fscore', legend=False, figsize=(6, 10))\nplt.title('Random forest feature importance', fontsize = 24)\nplt.xlabel('')\nplt.ylabel('')\nplt.xticks([], [])\nplt.yticks(fontsize=20)\nplt.show()\n#plt.gcf().savefig('feature_importance_xgb.png')",
      "execution_state": "idle"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "3db02b86-b24e-a817-3a85-a0d2b33392fe",
        "_active": false
      },
      "source": "## 6. Create the predictions",
      "execution_count": null,
      "outputs": [],
      "execution_state": "idle"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "41559cab-54f5-dd66-dae4-c6855c3dd604",
        "_active": false
      },
      "outputs": [],
      "source": "# Create the predictions\n\nylog_pred = Model.predict(x_test)\ny_pred = np.exp(ylog_pred) - 1",
      "execution_state": "idle"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "27fe5e9b-eb63-441f-de9a-853453dff42b",
        "_active": false
      },
      "source": "### Output the data to CSV for submission\nFinally we take the id_test vector we created earlier and combine it with our y_predictions to create our CSV for output. \n\nWe are utilising the very useful panda's data frame to do this and it's associated method \"to_csv\" can write our file out.",
      "execution_count": null,
      "outputs": [],
      "execution_state": "idle"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "715d3304-ae55-e13c-890f-385ac45b6608",
        "_active": false
      },
      "outputs": [],
      "source": "output = pd.DataFrame({'id': id_test, 'price_doc': y_pred})\n\noutput.to_csv('RandomForest_2.csv', index=False)",
      "execution_state": "idle"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "3ac9bd39-ddae-f4f6-30da-46528fbbd564",
        "_active": false
      },
      "outputs": [],
      "source": null,
      "execution_state": "idle"
    }
  ]
}