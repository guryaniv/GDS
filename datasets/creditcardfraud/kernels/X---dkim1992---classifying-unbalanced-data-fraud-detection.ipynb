{"nbformat": 4, "cells": [{"metadata": {"_uuid": "a4a3b3eb6d9e99b8e35681d8c09c129e17136277", "_cell_guid": "b6fba26e-9af5-405c-88dd-6c5cc7cac543"}, "source": ["# Credit Card Fraud Detection\n", "\n", "This problem is a binary classification with huge imbalance in class sizes which is common in a lot of fraud detection cases.\n", "\n", "We will handle this unbalanced data using various oversampling and undersampling techniques.\n", "\n", "1. Undersampling the majority class\n", "2. Oversampling the minority class \n", "3. Using K-means clustering to undersample\n", "4. Using SMOTE(Synthetic Minority Over-Sampling Technique) to oversample\n", "\n", "Credit to: https://www.kdnuggets.com/2017/06/7-techniques-handle-imbalanced-data.html for the ideas.\n", "\n", "We will shy away from tedius repetitions of hyperparameter tuning using cross validation and focus on th\n", "e effects of these sampling techniques."], "cell_type": "markdown"}, {"outputs": [], "execution_count": null, "metadata": {"_uuid": "28ab060adc0268bae22d181994cc782523467151", "_cell_guid": "879eaf84-8041-4232-9d35-0f02c0d08acc"}, "source": ["# This Python 3 environment comes with many helpful analytics libraries installed\n", "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n", "# For example, here's several helpful packages to load in \n", "\n", "import numpy as np # linear algebra\n", "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n", "\n", "# Input data files are available in the \"../input/\" directory.\n", "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n", "\n", "from subprocess import check_output\n", "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n", "\n", "# Any results you write to the current directory are saved as output.\n", "\n", "\n", "\n", "\n"], "cell_type": "code"}, {"outputs": [], "execution_count": null, "metadata": {"_uuid": "3a08cddb299ece0952f12672dab564ee2c09e0f6", "_cell_guid": "851b9987-e8ef-4c43-a3a4-431f76a7035d"}, "source": ["data = pd.read_csv(\"../input/creditcard.csv\")\n", "\n", "data.sample(5)"], "cell_type": "code"}, {"outputs": [], "execution_count": null, "metadata": {"_uuid": "a31c4ac53141c2063b721a6beef376483a63ea7f", "_cell_guid": "96be5028-6153-4a28-8fd9-ba29e63ed041"}, "source": ["data.info()"], "cell_type": "code"}, {"metadata": {"_uuid": "2888917a8f4d75fce6b96615361599b34d06cd17", "_cell_guid": "cf19ddb1-e296-4d8c-b673-66086e238bac"}, "source": ["We have 28 features + time stamp + amount and a class label.\n", "Let's scale and split the data into training and test sets."], "cell_type": "markdown"}, {"outputs": [], "execution_count": null, "metadata": {"_uuid": "d9d8985794db2222a8c0cb60a0a60f2a68c65b29", "collapsed": true, "_cell_guid": "c15e695f-1e0f-4e54-b376-aca8bb0097f2"}, "source": ["import pandas as pd \n", "import matplotlib.pyplot as plt \n", "import seaborn as sns \n", "import numpy as np \n", "%matplotlib inline\n", "from sklearn.preprocessing import StandardScaler\n", "from sklearn.model_selection import train_test_split"], "cell_type": "code"}, {"outputs": [], "execution_count": null, "metadata": {"_uuid": "119cd09ea8f480ab4cc6a7c796e717b6f6703ba8", "_cell_guid": "de8ff961-0b1b-4e37-8a91-b9a44abee37f"}, "source": ["sns.countplot(data.Class)\n", "d = data.Class.value_counts()\n", "print(d)\n", "print('Fraud cases are only {:f}% of all cases.'.format(d[1]*100/len(data)))"], "cell_type": "code"}, {"outputs": [], "execution_count": null, "metadata": {"_uuid": "a2ae26900eaffb9a53c40c66a4977a51745e8da4", "_cell_guid": "0a2731b9-f83e-4ed1-ab66-a64447637e1e"}, "source": ["sns.kdeplot(data = data[data.Class == 1].Amount, label = 'Fraud', bw=50)\n", "sns.kdeplot(data = data[data.Class == 0].Amount, label = 'Normal', bw=50)\n", "plt.legend();\n"], "cell_type": "code"}, {"metadata": {"_uuid": "7936b0f8f61f50ff61646c43a9fe98348a904c13", "_cell_guid": "8f72f3b8-f964-4fff-b4d8-c492c8d319b8"}, "source": ["We see that a lot number of fraud transactions are small amounts.\n", "\n", "For simplicity, we'll ignore timestamps."], "cell_type": "markdown"}, {"outputs": [], "execution_count": null, "metadata": {"_uuid": "6306954d5b6445e98c587676448cf48daf09b9ac", "_cell_guid": "92d9e97e-8cb0-420d-b530-ae7e7b41ea8f"}, "source": ["data.drop(\"Time\", axis = 1, inplace = True)\n", "data.columns"], "cell_type": "code"}, {"outputs": [], "execution_count": null, "metadata": {"_uuid": "9bdec8c8132ef61cea1d3f656caee0d21d33c5bd", "collapsed": true, "_cell_guid": "737c892e-08fa-4ca9-8ec7-3fab25a79306"}, "source": ["X = data.drop(\"Class\", axis = 1)\n", "y = data.Class\n", "\n", "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=333, stratify = y)\n", "\n"], "cell_type": "code"}, {"outputs": [], "execution_count": null, "metadata": {"_uuid": "0890c6703b733de85b96636693dba9939d81ef84", "_cell_guid": "963b323d-382a-463b-a0c7-58474e36e936"}, "source": ["print(len(y_train[y_train == 1])/len(y_train))\n", "print(len(y_test[y_test == 1])/len(y_test))"], "cell_type": "code"}, {"outputs": [], "execution_count": null, "metadata": {"_uuid": "80a683f08d0473dce30d15f2f8a86a6fca01fa06", "_cell_guid": "04bf0214-349e-407a-9e3e-859543954a42"}, "source": ["X_train.Amount.shape"], "cell_type": "code"}, {"outputs": [], "execution_count": null, "metadata": {"_uuid": "453a10de12330d353a80185cc501a27a6f8e0d1d", "_cell_guid": "989a5b76-4a7f-4680-97ff-484b09a63c60"}, "source": ["scaler = StandardScaler()\n", "scaler.fit(X_train.Amount.reshape(-1, 1))\n", "X_train.Amount = scaler.transform(X_train.Amount.reshape(-1,1))\n", "X_test.Amount = scaler.transform(X_test.Amount.reshape(-1,1))"], "cell_type": "code"}, {"outputs": [], "execution_count": null, "metadata": {"_uuid": "2a6f62f7335d2a8a9b0c522014313f366ff44c76", "_cell_guid": "c228c831-246a-4246-baf5-f5530955296a"}, "source": ["X_train.Amount.describe(), X_test.Amount.describe()"], "cell_type": "code"}, {"outputs": [], "execution_count": null, "metadata": {"_uuid": "7faaabfe85ce5c4d4524f162ed90ab02b7824463", "collapsed": true, "_cell_guid": "c05ab41b-4979-4a7e-a9f2-a7f9244a7208"}, "source": ["\n", "from sklearn.linear_model import LogisticRegression\n", "from sklearn.ensemble import RandomForestClassifier\n", "from sklearn.metrics import confusion_matrix\n", "from sklearn.metrics import classification_report"], "cell_type": "code"}, {"metadata": {"_uuid": "612c0dfc566cf558ead3fb1c9bc9ec5542c78123", "_cell_guid": "027be00b-be06-4a9e-85b8-0ac7e99c5e17"}, "source": ["# 0. No under or oversampling\n", "First, we need a baseline metric with a simple logistic regression trained on the original unbalanced data.\n", "\n", "Since we want to avoid false negatives (frauds that go undetected), recall is what we should to look at."], "cell_type": "markdown"}, {"outputs": [], "execution_count": null, "metadata": {"_uuid": "39897c56b1d6ea2f9d5a37fce8fc07663d43dc68", "collapsed": true, "_cell_guid": "a08718ea-56c3-4a9a-9762-d79f5943b149"}, "source": ["\n", "clf = LogisticRegression()\n", "clf.fit(X_train, y_train)\n", "y_pred = clf.predict(X_test)"], "cell_type": "code"}, {"outputs": [], "execution_count": null, "metadata": {"_uuid": "1c26157c113f1de3f3568d777e1a0f7fafe270d8", "scrolled": true, "_cell_guid": "0ad163bf-59cc-469f-bddc-43dbd460d500"}, "source": ["\n", "\n", "cm = confusion_matrix(y_test, y_pred)\n", "df_cm = pd.DataFrame(cm, range(2), range(2))\n", "plt.figure(figsize = (10,7))\n", "sns.heatmap(df_cm, annot=True)\n", "\n", "print(classification_report(y_test,y_pred))"], "cell_type": "code"}, {"outputs": [], "execution_count": null, "metadata": {"_uuid": "1ad81d96cecbde44c3725f013b803d11e71a2ebb", "collapsed": true, "_cell_guid": "14d1cd51-65de-452a-aea4-0e6137859307"}, "source": ["\n", "clf = RandomForestClassifier(n_estimators = 10)\n", "clf.fit(X_train, y_train)\n", "y_pred = clf.predict(X_test)"], "cell_type": "code"}, {"outputs": [], "execution_count": null, "metadata": {"_uuid": "5dbb7421d1ca732b475168ff337ba482963e473a", "_cell_guid": "b71d2b1f-8b93-4c41-92d4-ec8c1457bb45"}, "source": ["\n", "cm = confusion_matrix(y_test, y_pred)\n", "df_cm = pd.DataFrame(cm, range(2), range(2))\n", "plt.figure(figsize = (10,7))\n", "sns.heatmap(df_cm, annot=True)\n", "\n", "print(classification_report(y_test,y_pred))"], "cell_type": "code"}, {"metadata": {"_uuid": "aa05456e4f05e6b7a89c494eecebe91c57758faa", "_cell_guid": "a70b5e3b-b8f2-4b43-9850-e37b3d8e08bb"}, "source": ["We see that without under or over sampling, we achieve a mere 65% recall on the test set with logistic regression.\n", "As expected, random forest classifier works better than logistic regression even without hyperparameter tuning due to its ensemble nature."], "cell_type": "markdown"}, {"metadata": {"_uuid": "0bde2543c172225c89608301804dd7be9cc38a26", "_cell_guid": "3929857d-86e1-4110-b2d2-44594cf28057"}, "source": ["# 1. Undersampling\n", "\n", "Here we will undersample the majority class (normal) without replacement to match the number of the minority class (fraud).\n", "\n", "\n"], "cell_type": "markdown"}, {"outputs": [], "execution_count": null, "metadata": {"_uuid": "6a8a62b4de9fdefbf40ed8dc58052e5ee1790c13", "collapsed": true, "_cell_guid": "fa36bc6f-297f-4484-b8c3-e8b6a02a5b6e"}, "source": ["from sklearn.utils import resample\n", "\n", "X_train_normal = X_train[y_train == 0]\n", "y_train_normal = y_train[y_train == 0]\n", "X_train_fraud = X_train[y_train == 1]\n", "y_train_fraud = y_train[y_train == 1]\n", "\n", "X_train_normal, y_train_normal = resample(X_train_normal, y_train_normal, n_samples = len(y_train_fraud), replace = False, random_state = 333)\n", "\n", "X_train_undersample = pd.concat([X_train_normal, X_train_fraud], ignore_index=True)\n", "y_train_undersample = pd.concat([y_train_normal, y_train_fraud], ignore_index=True)\n"], "cell_type": "code"}, {"outputs": [], "execution_count": null, "metadata": {"_uuid": "6df7d51661e2d68c9b960731da6b38a68f8985b9", "_cell_guid": "502621d8-2991-4a33-b085-8375c6b86b65"}, "source": ["print(type(y_train_undersample))\n", "print(y_train_undersample.value_counts())\n", "print(len(X_train_undersample))"], "cell_type": "code"}, {"outputs": [], "execution_count": null, "metadata": {"_uuid": "0685d0e507b1432cdcd7756e5ac738212e00dce7", "collapsed": true, "_cell_guid": "efd691e9-a09c-4112-840c-ac66681a2917"}, "source": ["\n", "clf = LogisticRegression()\n", "clf.fit(X_train_undersample, y_train_undersample)\n", "y_pred = clf.predict(X_test)"], "cell_type": "code"}, {"outputs": [], "execution_count": null, "metadata": {"_uuid": "9f79c0c56a139c0c7a2079e886ab4ba04767bae3", "_cell_guid": "689c7984-ba39-40d1-8794-59fb0308fcae"}, "source": ["\n", "cm = confusion_matrix(y_test, y_pred)\n", "df_cm = pd.DataFrame(cm, range(2), range(2))\n", "plt.figure(figsize = (10,7))\n", "sns.heatmap(df_cm, annot=True)\n", "\n", "print(classification_report(y_test,y_pred))"], "cell_type": "code"}, {"outputs": [], "execution_count": null, "metadata": {"_uuid": "9670021b384a8a2f627f74fbde96963421733726", "collapsed": true, "_cell_guid": "1d96b652-654d-4acc-9652-00cde0bcd13c"}, "source": ["\n", "clf = RandomForestClassifier(n_estimators = 100)\n", "clf.fit(X_train_undersample, y_train_undersample)\n", "y_pred = clf.predict(X_test)\n", "\n"], "cell_type": "code"}, {"outputs": [], "execution_count": null, "metadata": {"_uuid": "1acbf9f766afb9861ec0cd7d166b55287866aee5", "_cell_guid": "67d0496e-c004-443c-b42b-96005df17837"}, "source": ["\n", "cm = confusion_matrix(y_test, y_pred)\n", "df_cm = pd.DataFrame(cm, range(2), range(2))\n", "plt.figure(figsize = (10,7))\n", "sns.heatmap(df_cm, annot=True)\n", "\n", "print(classification_report(y_test,y_pred))"], "cell_type": "code"}, {"metadata": {"_uuid": "535ec3ba04999867b680cbc7ea9148c749236d00", "_cell_guid": "52b00ac9-5fc4-4466-95e7-74f7490d771a"}, "source": ["Even with simple undersampling, the recall accuracy greatly improved.\n", "However, precision dropped significantly. This means a lot of normal cases are predicted as frauds.\n", "\n"], "cell_type": "markdown"}, {"metadata": {"_uuid": "b5c8c46c36e8e2e49798953a1ec1a51e21bb3d79", "_cell_guid": "b1bf8c39-4762-4613-88d5-df6c90a21f60"}, "source": ["   # 2. Oversampling\n", "   \n", "   Here we will oversample the minority class (fraud) with replacement to match the number of the majority class (normal)."], "cell_type": "markdown"}, {"outputs": [], "execution_count": null, "metadata": {"_uuid": "3548c4bd0e142e473221ce6cc83c75dec5508583", "collapsed": true, "_cell_guid": "95954011-7902-4546-8ecc-077abd17617b"}, "source": ["X_train_normal = X_train[y_train == 0]\n", "y_train_normal = y_train[y_train == 0]\n", "X_train_fraud = X_train[y_train == 1]\n", "y_train_fraud = y_train[y_train == 1]\n", "\n", "X_train_fraud, y_train_fraud = resample(X_train_fraud, y_train_fraud, n_samples = len(y_train_normal), replace = True, random_state = 333)\n", "\n", "X_train_oversample = pd.concat([X_train_normal, X_train_fraud], ignore_index=True)\n", "y_train_oversample = pd.concat([y_train_normal, y_train_fraud], ignore_index=True)\n"], "cell_type": "code"}, {"outputs": [], "execution_count": null, "metadata": {"_uuid": "6c30c7a9d3ea2ea5eaf17aee56f0ac02cb1c4cc2", "_cell_guid": "8dfe4c65-dc3c-4854-8700-d10ed85f93c3"}, "source": ["print(y_train_oversample.value_counts())\n", "print(len(X_train_oversample))"], "cell_type": "code"}, {"outputs": [], "execution_count": null, "metadata": {"_uuid": "bec90ee316bb729683781ff82918ad21e3a7fc9e", "collapsed": true, "_cell_guid": "46c3b4cd-ba84-4f12-a69b-3b6c7aa98968"}, "source": ["clf = LogisticRegression()\n", "clf.fit(X_train_oversample, y_train_oversample)\n", "y_pred = clf.predict(X_test)"], "cell_type": "code"}, {"outputs": [], "execution_count": null, "metadata": {"_uuid": "2a781e740087835806b2ba9357f1c6d0fd7c751c", "_cell_guid": "26057e10-d2bc-448a-8adf-71fb10401fde"}, "source": ["cm = confusion_matrix(y_test, y_pred)\n", "df_cm = pd.DataFrame(cm, range(2), range(2))\n", "plt.figure(figsize = (10,7))\n", "sns.heatmap(df_cm, annot=True)\n", "\n", "print(classification_report(y_test,y_pred))"], "cell_type": "code"}, {"outputs": [], "execution_count": null, "metadata": {"_uuid": "82e5a2f426bc93cb50dc2178760f93dbb1f9368c", "collapsed": true, "_cell_guid": "dadd0309-074e-4030-ba69-d52e52f5c72e"}, "source": ["\n", "clf = RandomForestClassifier(n_estimators = 100)\n", "clf.fit(X_train_oversample, y_train_oversample)\n", "y_pred = clf.predict(X_test)"], "cell_type": "code"}, {"outputs": [], "execution_count": null, "metadata": {}, "source": ["cm = confusion_matrix(y_test, y_pred)\n", "df_cm = pd.DataFrame(cm, range(2), range(2))\n", "plt.figure(figsize = (10,7))\n", "sns.heatmap(df_cm, annot=True)\n", "\n", "print(classification_report(y_test,y_pred))"], "cell_type": "code"}, {"metadata": {}, "source": ["Oversampling results in recall similar to undersampling in this case."], "cell_type": "markdown"}, {"metadata": {}, "source": ["# 3. Using K-means clustering to undersample\n", "\n", "We will use K-means clustering to reduce the number of majority class instances to match the number of minority class instances.\n", "The process can be seen as vector quantizing the normal cases.\n", "The number of clusters should be equal to the number of minority class instances (frauds).\n"], "cell_type": "markdown"}, {"outputs": [], "execution_count": null, "metadata": {}, "source": ["from sklearn.cluster import KMeans\n", "\n", "X_train_normal = X_train[y_train == 0]\n", "y_train_normal = y_train[y_train == 0]\n", "X_train_fraud = X_train[y_train == 1]\n", "y_train_fraud = y_train[y_train == 1]\n", "\n", "len(y_train_normal), len(y_train_fraud)"], "cell_type": "code"}, {"outputs": [], "execution_count": null, "metadata": {}, "source": ["n_clusters = len(y_train_fraud)\n", "kmeans = KMeans(n_clusters = n_clusters, random_state = 333).fit(X_train_normal)\n", "X_train_normal = kmeans.cluster_centers_\n", "\n", "X_train_normal.shape"], "cell_type": "code"}, {"outputs": [], "execution_count": null, "metadata": {}, "source": ["X_train_normal = kmeans.cluster_centers_\n", "X_train_normal = pd.DataFrame(X_train_normal, columns = X_train.columns)\n", "X_train_normal.sample(5)"], "cell_type": "code"}, {"outputs": [], "execution_count": null, "metadata": {}, "source": ["y_train_normal = y_train_normal[:n_clusters]\n", "y_train_normal.shape"], "cell_type": "code"}, {"outputs": [], "execution_count": null, "metadata": {}, "source": ["X_train_kmeans = pd.concat([X_train_normal, X_train_fraud], ignore_index=True)\n", "y_train_kmeans = pd.concat([y_train_normal, y_train_fraud], ignore_index=True)\n", "\n", "print(y_train_kmeans.value_counts())\n", "print(len(X_train_kmeans))\n", "\n", "X_train_kmeans.isnull().values.any(), y_train_kmeans.isnull().values.any()"], "cell_type": "code"}, {"outputs": [], "execution_count": null, "metadata": {}, "source": ["clf = LogisticRegression()\n", "clf.fit(X_train_kmeans, y_train_kmeans)\n", "y_pred = clf.predict(X_test)"], "cell_type": "code"}, {"outputs": [], "execution_count": null, "metadata": {}, "source": ["cm = confusion_matrix(y_test, y_pred)\n", "df_cm = pd.DataFrame(cm, range(2), range(2))\n", "plt.figure(figsize = (10,7))\n", "sns.heatmap(df_cm, annot=True)\n", "\n", "print(classification_report(y_test,y_pred))"], "cell_type": "code"}, {"outputs": [], "execution_count": null, "metadata": {"collapsed": true}, "source": ["clf = RandomForestClassifier(n_estimators = 100)\n", "clf.fit(X_train_kmeans, y_train_kmeans)\n", "y_pred = clf.predict(X_test)"], "cell_type": "code"}, {"outputs": [], "execution_count": null, "metadata": {}, "source": ["cm = confusion_matrix(y_test, y_pred)\n", "df_cm = pd.DataFrame(cm, range(2), range(2))\n", "plt.figure(figsize = (10,7))\n", "sns.heatmap(df_cm, annot=True)\n", "\n", "print(classification_report(y_test,y_pred))"], "cell_type": "code"}, {"metadata": {}, "source": ["For logitic regression, recall stayed the same.\n", "For random forest classifier, recall improved 92 percent. significantly at the cost of precision."], "cell_type": "markdown"}, {"metadata": {}, "source": ["# 4. Using SMOTE(Synthetic Minority Over-Sampling Technique) to oversample\n", "\n", "To oversample, SMOTE chooses the midpoint between a minority sample and one of its k-nearest neighbors, and adds random perturbation to synthesize a new sample It is one of the most popular ways to deal with imbalanced data.\n", "\n", "Source: https://www.cs.cmu.edu/afs/cs/project/jair/pub/volume16/chawla02a-html/chawla2002.html\n", "http://contrib.scikit-learn.org/imbalanced-learn/stable/generated/imblearn.over_sampling.SMOTE.html"], "cell_type": "markdown"}, {"outputs": [], "execution_count": null, "metadata": {"collapsed": true}, "source": ["from imblearn.over_sampling import SMOTE\n", "os = SMOTE(random_state=0)"], "cell_type": "code"}, {"outputs": [], "execution_count": null, "metadata": {}, "source": ["X_train_smote, y_train_smote = os.fit_sample(X_train,y_train)\n", "type(X_train_smote), type(y_train_smote)\n", "\n", "X_train_smote = pd.DataFrame(X_train_smote, columns= X_train.columns)\n", "y_train_smote= pd.Series(y_train_smote)"], "cell_type": "code"}, {"outputs": [], "execution_count": null, "metadata": {}, "source": ["#check the number of each class \n", "y_train_smote.value_counts()"], "cell_type": "code"}, {"outputs": [], "execution_count": null, "metadata": {"collapsed": true}, "source": ["clf = LogisticRegression()\n", "clf.fit(X_train_smote, y_train_smote)\n", "y_pred = clf.predict(X_test)"], "cell_type": "code"}, {"outputs": [], "execution_count": null, "metadata": {}, "source": ["cm = confusion_matrix(y_test, y_pred)\n", "df_cm = pd.DataFrame(cm, range(2), range(2))\n", "plt.figure(figsize = (10,7))\n", "sns.heatmap(df_cm, annot=True)\n", "\n", "print(classification_report(y_test,y_pred))"], "cell_type": "code"}, {"outputs": [], "execution_count": null, "metadata": {"collapsed": true}, "source": ["\n", "clf = RandomForestClassifier(n_estimators = 100)\n", "clf.fit(X_train_smote, y_train_smote)\n", "y_pred = clf.predict(X_test)"], "cell_type": "code"}, {"outputs": [], "execution_count": null, "metadata": {}, "source": ["cm = confusion_matrix(y_test, y_pred)\n", "df_cm = pd.DataFrame(cm, range(2), range(2))\n", "plt.figure(figsize = (10,7))\n", "sns.heatmap(df_cm, annot=True)\n", "\n", "print(classification_report(y_test,y_pred))"], "cell_type": "code"}, {"metadata": {"_kg_hide-output": false}, "source": ["Recall suffered but precision greatly improved.\n", "Using SMOTE with random forest classifier yields the best F1 score(geometric mean of precision and recall)."], "cell_type": "markdown"}, {"metadata": {}, "source": ["# Summary\n", "\n", "With some tweaking of threshold and hyperparameter tuning, SMOTE looks the most promising since it achieved both high recall and precision.\n", "\n", "Undersampling by using K-means centroids yields the best recall at the cost of very low precision."], "cell_type": "markdown"}, {"outputs": [], "execution_count": null, "metadata": {"collapsed": true}, "source": [], "cell_type": "code"}], "nbformat_minor": 1, "metadata": {"language_info": {"nbconvert_exporter": "python", "codemirror_mode": {"name": "ipython", "version": 3}, "mimetype": "text/x-python", "version": "3.6.3", "file_extension": ".py", "name": "python", "pygments_lexer": "ipython3"}, "kernelspec": {"language": "python", "name": "python3", "display_name": "Python 3"}}}