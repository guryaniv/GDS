{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"Thanks for this https://www.kaggle.com/risntforpirates/petfinder-simple-lgbm and this https://www.kaggle.com/skooch/petfinder-simple-lgbm-baseline main parts.\n\nAnd for this picture part: https://www.kaggle.com/christofhenkel/extract-image-features-from-pretrained-nn.\n\nJust combined two notebooks together to show how to add image features to your lbgm."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import json\n\nimport scipy as sp\nimport pandas as pd\nimport numpy as np\n\nfrom functools import partial\nfrom math import sqrt\n\nfrom sklearn.metrics import cohen_kappa_score, mean_squared_error\nfrom sklearn.metrics import confusion_matrix as sk_cmatrix\nfrom sklearn.model_selection import StratifiedKFold\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD\n\nfrom collections import Counter\n\nimport lightgbm as lgb\nnp.random.seed(369)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6686cdc69bee6da85c2d91a5ab4519dbeddd6263"},"cell_type":"code","source":"# The following 3 functions have been taken from Ben Hamner's github repository\n# https://github.com/benhamner/Metrics\ndef confusion_matrix(rater_a, rater_b, min_rating=None, max_rating=None):\n    \"\"\"\n    Returns the confusion matrix between rater's ratings\n    \"\"\"\n    assert(len(rater_a) == len(rater_b))\n    if min_rating is None:\n        min_rating = min(rater_a + rater_b)\n    if max_rating is None:\n        max_rating = max(rater_a + rater_b)\n    num_ratings = int(max_rating - min_rating + 1)\n    conf_mat = [[0 for i in range(num_ratings)]\n                for j in range(num_ratings)]\n    for a, b in zip(rater_a, rater_b):\n        conf_mat[a - min_rating][b - min_rating] += 1\n    return conf_mat\n\n\ndef histogram(ratings, min_rating=None, max_rating=None):\n    \"\"\"\n    Returns the counts of each type of rating that a rater made\n    \"\"\"\n    if min_rating is None:\n        min_rating = min(ratings)\n    if max_rating is None:\n        max_rating = max(ratings)\n    num_ratings = int(max_rating - min_rating + 1)\n    hist_ratings = [0 for x in range(num_ratings)]\n    for r in ratings:\n        hist_ratings[r - min_rating] += 1\n    return hist_ratings\n\n\ndef quadratic_weighted_kappa(y, y_pred):\n    \"\"\"\n    Calculates the quadratic weighted kappa\n    axquadratic_weighted_kappa calculates the quadratic weighted kappa\n    value, which is a measure of inter-rater agreement between two raters\n    that provide discrete numeric ratings.  Potential values range from -1\n    (representing complete disagreement) to 1 (representing complete\n    agreement).  A kappa value of 0 is expected if all agreement is due to\n    chance.\n    quadratic_weighted_kappa(rater_a, rater_b), where rater_a and rater_b\n    each correspond to a list of integer ratings.  These lists must have the\n    same length.\n    The ratings should be integers, and it is assumed that they contain\n    the complete range of possible ratings.\n    quadratic_weighted_kappa(X, min_rating, max_rating), where min_rating\n    is the minimum possible rating, and max_rating is the maximum possible\n    rating\n    \"\"\"\n    rater_a = y\n    rater_b = y_pred\n    min_rating=None\n    max_rating=None\n    rater_a = np.array(rater_a, dtype=int)\n    rater_b = np.array(rater_b, dtype=int)\n    assert(len(rater_a) == len(rater_b))\n    if min_rating is None:\n        min_rating = min(min(rater_a), min(rater_b))\n    if max_rating is None:\n        max_rating = max(max(rater_a), max(rater_b))\n    conf_mat = confusion_matrix(rater_a, rater_b,\n                                min_rating, max_rating)\n    num_ratings = len(conf_mat)\n    num_scored_items = float(len(rater_a))\n\n    hist_rater_a = histogram(rater_a, min_rating, max_rating)\n    hist_rater_b = histogram(rater_b, min_rating, max_rating)\n\n    numerator = 0.0\n    denominator = 0.0\n\n    for i in range(num_ratings):\n        for j in range(num_ratings):\n            expected_count = (hist_rater_a[i] * hist_rater_b[j]\n                              / num_scored_items)\n            d = pow(i - j, 2.0) / pow(num_ratings - 1, 2.0)\n            numerator += d * conf_mat[i][j] / num_scored_items\n            denominator += d * expected_count / num_scored_items\n\n    return (1.0 - numerator / denominator)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"07bb950a60b1297c9f7e3a4c249b692681d0acd4"},"cell_type":"code","source":"class OptimizedRounder(object):\n    def __init__(self):\n        self.coef_ = 0\n\n    def _kappa_loss(self, coef, X, y):\n        X_p = np.copy(X)\n        for i, pred in enumerate(X_p):\n            if pred < coef[0]:\n                X_p[i] = 0\n            elif pred >= coef[0] and pred < coef[1]:\n                X_p[i] = 1\n            elif pred >= coef[1] and pred < coef[2]:\n                X_p[i] = 2\n            elif pred >= coef[2] and pred < coef[3]:\n                X_p[i] = 3\n            else:\n                X_p[i] = 4\n\n        ll = quadratic_weighted_kappa(y, X_p)\n        return -ll\n\n    def fit(self, X, y):\n        loss_partial = partial(self._kappa_loss, X=X, y=y)\n        initial_coef = [0.5, 1.5, 2.5, 3.5]\n        self.coef_ = sp.optimize.minimize(loss_partial, initial_coef, method='nelder-mead')\n\n    def predict(self, X, coef):\n        X_p = np.copy(X)\n        for i, pred in enumerate(X_p):\n            if pred < coef[0]:\n                X_p[i] = 0\n            elif pred >= coef[0] and pred < coef[1]:\n                X_p[i] = 1\n            elif pred >= coef[1] and pred < coef[2]:\n                X_p[i] = 2\n            elif pred >= coef[2] and pred < coef[3]:\n                X_p[i] = 3\n            else:\n                X_p[i] = 4\n        return X_p\n\n    def coefficients(self):\n        return self.coef_['x']\n    \ndef rmse(actual, predicted):\n    return sqrt(mean_squared_error(actual, predicted))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3018122b69c2772c2eaa40e582e1109204c764a6"},"cell_type":"markdown","source":"## Image Features"},{"metadata":{"trusted":true,"_uuid":"bfae8f48a49cd7cd17da8882db583650be454979"},"cell_type":"code","source":"import cv2\nimport pandas as pd\nimport numpy as np\nimport os\nfrom tqdm import tqdm, tqdm_notebook\nfrom keras.applications.densenet import preprocess_input, DenseNet121\n\ntrain_df = pd.read_csv('../input/petfinder-adoption-prediction/train/train.csv')\nimg_size = 256\nbatch_size = 16","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0a46d4a555cdccf5be76d7d83d83e20f4aac4471"},"cell_type":"code","source":"pet_ids = train_df['PetID'].values\nn_batches = len(pet_ids) // batch_size + 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3ccd87396d825eb92e9458fd18c345170ed861ce"},"cell_type":"code","source":"def resize_to_square(im):\n    old_size = im.shape[:2] # old_size is in (height, width) format\n    ratio = float(img_size)/max(old_size)\n    new_size = tuple([int(x*ratio) for x in old_size])\n    # new_size should be in (width, height) format\n    im = cv2.resize(im, (new_size[1], new_size[0]))\n    delta_w = img_size - new_size[1]\n    delta_h = img_size - new_size[0]\n    top, bottom = delta_h//2, delta_h-(delta_h//2)\n    left, right = delta_w//2, delta_w-(delta_w//2)\n    color = [0, 0, 0]\n    new_im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT,value=color)\n    return new_im\n\ndef load_image(path, pet_id):\n    image = cv2.imread(f'{path}{pet_id}-1.jpg')\n    new_image = resize_to_square(image)\n    new_image = preprocess_input(new_image)\n    return new_image","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"875593b9781d3574f08a12fa0969d6396a1bb16e"},"cell_type":"code","source":"from keras.models import Model\nfrom keras.layers import GlobalAveragePooling2D, Input, Lambda, AveragePooling1D\nimport keras.backend as K\ninp = Input((256,256,3))\nbackbone = DenseNet121(input_tensor = inp, \n                       weights=\"../input/densenet-keras/DenseNet-BC-121-32-no-top.h5\",\n                       include_top = False)\nx = backbone.output\nx = GlobalAveragePooling2D()(x)\nx = Lambda(lambda x: K.expand_dims(x,axis = -1))(x)\nx = AveragePooling1D(4)(x)\nout = Lambda(lambda x: x[:,:,0])(x)\n\nm = Model(inp,out)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"655739ef48459186cd72ca55a127c4ed12cd3491"},"cell_type":"code","source":"features = {}\nfor b in tqdm_notebook(range(n_batches)):\n    start = b*batch_size\n    end = (b+1)*batch_size\n    batch_pets = pet_ids[start:end]\n    batch_images = np.zeros((len(batch_pets),img_size,img_size,3))\n    for i,pet_id in enumerate(batch_pets):\n        try:\n            batch_images[i] = load_image(\"../input/petfinder-adoption-prediction/train_images/\", pet_id)\n        except:\n            pass\n    batch_preds = m.predict(batch_images)\n    for i,pet_id in enumerate(batch_pets):\n        features[pet_id] = batch_preds[i]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7ccad0df8125ee7380c35e98233e4ff75de1e2cc"},"cell_type":"code","source":"train_feats = pd.DataFrame.from_dict(features, orient='index')\ntrain_feats.columns = ['pic_'+str(i) for i in range(train_feats.shape[1])]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"42cba91ec7ffc56ce2bf383007ac93ebcf96aa6c"},"cell_type":"code","source":"test_df = pd.read_csv('../input/petfinder-adoption-prediction/test/test.csv')\n\npet_ids = test_df['PetID'].values\nn_batches = len(pet_ids) // batch_size + 1\n\nfeatures = {}\nfor b in tqdm_notebook(range(n_batches)):\n    start = b*batch_size\n    end = (b+1)*batch_size\n    batch_pets = pet_ids[start:end]\n    batch_images = np.zeros((len(batch_pets),img_size,img_size,3))\n    for i,pet_id in enumerate(batch_pets):\n        try:\n            batch_images[i] = load_image(\"../input/petfinder-adoption-prediction/test_images/\", pet_id)\n        except:\n            pass\n    batch_preds = m.predict(batch_images)\n    for i,pet_id in enumerate(batch_pets):\n        features[pet_id] = batch_preds[i]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"50624432e0dc9d9fd874df6d16f7a9aed656ffae"},"cell_type":"code","source":"test_feats = pd.DataFrame.from_dict(features, orient='index')\ntest_feats.columns = ['pic_'+str(i) for i in range(test_feats.shape[1])]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c5a60377a6df0caea0500086bca7e7401d2f8369"},"cell_type":"code","source":"test_feats = test_feats.reset_index()\ntest_feats.rename({'index': 'PetID'}, axis='columns', inplace=True)\n\ntrain_feats = train_feats.reset_index()\ntrain_feats.rename({'index': 'PetID'}, axis='columns', inplace=True)\n\ntest_feats.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4f7fced709912906b7f119eeb22fe57fa4d42e8a"},"cell_type":"code","source":"print('Train')\ntrain = pd.read_csv(\"../input/petfinder-adoption-prediction/train/train.csv\")\nprint(train.shape)\n\nprint('Test')\ntest = pd.read_csv(\"../input/petfinder-adoption-prediction/test/test.csv\")\nprint(test.shape)\n\nprint('Breeds')\nbreeds = pd.read_csv(\"../input/petfinder-adoption-prediction/breed_labels.csv\")\nprint(breeds.shape)\n\nprint('Colors')\ncolors = pd.read_csv(\"../input/petfinder-adoption-prediction/color_labels.csv\")\nprint(colors.shape)\n\nprint('States')\nstates = pd.read_csv(\"../input/petfinder-adoption-prediction/state_labels.csv\")\nprint(states.shape)\n\ntarget = train['AdoptionSpeed']\ntrain_id = train['PetID']\ntest_id = test['PetID']\n\n\ntrain = pd.merge(train, train_feats, how='left', on='PetID')\ntest = pd.merge(test, test_feats, how='left', on='PetID')\n\ntrain.drop(['AdoptionSpeed', 'PetID'], axis=1, inplace=True)\ntest.drop(['PetID'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"15869ad10f8f15697823b562cde3965dcd785786"},"cell_type":"code","source":"doc_sent_mag = []\ndoc_sent_score = []\nnf_count = 0\nfor pet in train_id:\n    try:\n        with open('../input/petfinder-adoption-prediction/train_sentiment/' + pet + '.json', 'r') as f:\n            sentiment = json.load(f)\n        doc_sent_mag.append(sentiment['documentSentiment']['magnitude'])\n        doc_sent_score.append(sentiment['documentSentiment']['score'])\n    except FileNotFoundError:\n        nf_count += 1\n        doc_sent_mag.append(-1)\n        doc_sent_score.append(-1)\n\ntrain.loc[:, 'doc_sent_mag'] = doc_sent_mag\ntrain.loc[:, 'doc_sent_score'] = doc_sent_score\n\ndoc_sent_mag = []\ndoc_sent_score = []\nnf_count = 0\nfor pet in test_id:\n    try:\n        with open('../input/petfinder-adoption-prediction/test_sentiment/' + pet + '.json', 'r') as f:\n            sentiment = json.load(f)\n        doc_sent_mag.append(sentiment['documentSentiment']['magnitude'])\n        doc_sent_score.append(sentiment['documentSentiment']['score'])\n    except FileNotFoundError:\n        nf_count += 1\n        doc_sent_mag.append(-1)\n        doc_sent_score.append(-1)\n\ntest.loc[:, 'doc_sent_mag'] = doc_sent_mag\ntest.loc[:, 'doc_sent_score'] = doc_sent_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ca77fdca904f232c3396e748d998746fe71ad939"},"cell_type":"code","source":"## WITHOUT ERROR FIXED\ntrain_desc = train.Description.fillna(\"none\").values\ntest_desc = test.Description.fillna(\"none\").values\n\ntfv = TfidfVectorizer(min_df=3,  max_features=10000,\n        strip_accents='unicode', analyzer='word', token_pattern=r'\\w{1,}',\n        ngram_range=(1, 3), use_idf=1, smooth_idf=1, sublinear_tf=1,\n        stop_words = 'english')\n    \n# Fit TFIDF\ntfv.fit(list(train_desc))\nX =  tfv.transform(train_desc)\nX_test = tfv.transform(test_desc)\nprint(\"X (tfidf):\", X.shape)\n\nsvd = TruncatedSVD(n_components=200)\nsvd.fit(X)\n# print(svd.explained_variance_ratio_.sum())\n# print(svd.explained_variance_ratio_)\nX = svd.transform(X)\nprint(\"X (svd):\", X.shape)\n\nX = pd.DataFrame(X, columns=['svd_{}'.format(i) for i in range(200)])\ntrain = pd.concat((train, X), axis=1)\nX_test = svd.transform(X_test)\nX_test = pd.DataFrame(X_test, columns=['svd_{}'.format(i) for i in range(200)])\ntest = pd.concat((test, X_test), axis=1)\n\nprint(\"train:\", train.shape)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b330aeb515bb02adff24d9fce561991134f1dc40"},"cell_type":"code","source":"vertex_xs = []\nvertex_ys = []\nbounding_confidences = []\nbounding_importance_fracs = []\ndominant_blues = []\ndominant_greens = []\ndominant_reds = []\ndominant_pixel_fracs = []\ndominant_scores = []\nlabel_descriptions = []\nlabel_scores = []\nnf_count = 0\nnl_count = 0\nfor pet in train_id:\n    try:\n        with open('../input/petfinder-adoption-prediction/train_metadata/' + pet + '-1.json', 'r') as f:\n            data = json.load(f)\n        vertex_x = data['cropHintsAnnotation']['cropHints'][0]['boundingPoly']['vertices'][2]['x']\n        vertex_xs.append(vertex_x)\n        vertex_y = data['cropHintsAnnotation']['cropHints'][0]['boundingPoly']['vertices'][2]['y']\n        vertex_ys.append(vertex_y)\n        bounding_confidence = data['cropHintsAnnotation']['cropHints'][0]['confidence']\n        bounding_confidences.append(bounding_confidence)\n        bounding_importance_frac = data['cropHintsAnnotation']['cropHints'][0].get('importanceFraction', -1)\n        bounding_importance_fracs.append(bounding_importance_frac)\n        dominant_blue = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color']['blue']\n        dominant_blues.append(dominant_blue)\n        dominant_green = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color']['green']\n        dominant_greens.append(dominant_green)\n        dominant_red = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color']['red']\n        dominant_reds.append(dominant_red)\n        dominant_pixel_frac = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['pixelFraction']\n        dominant_pixel_fracs.append(dominant_pixel_frac)\n        dominant_score = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['score']\n        dominant_scores.append(dominant_score)\n        if data.get('labelAnnotations'):\n            label_description = data['labelAnnotations'][0]['description']\n            label_descriptions.append(label_description)\n            label_score = data['labelAnnotations'][0]['score']\n            label_scores.append(label_score)\n        else:\n            nl_count += 1\n            label_descriptions.append('nothing')\n            label_scores.append(-1)\n    except FileNotFoundError:\n        nf_count += 1\n        vertex_xs.append(-1)\n        vertex_ys.append(-1)\n        bounding_confidences.append(-1)\n        bounding_importance_fracs.append(-1)\n        dominant_blues.append(-1)\n        dominant_greens.append(-1)\n        dominant_reds.append(-1)\n        dominant_pixel_fracs.append(-1)\n        dominant_scores.append(-1)\n        label_descriptions.append('nothing')\n        label_scores.append(-1)\n\nprint(nf_count)\nprint(nl_count)\ntrain.loc[:, 'vertex_x'] = vertex_xs\ntrain.loc[:, 'vertex_y'] = vertex_ys\ntrain.loc[:, 'bounding_confidence'] = bounding_confidences\ntrain.loc[:, 'bounding_importance'] = bounding_importance_fracs\ntrain.loc[:, 'dominant_blue'] = dominant_blues\ntrain.loc[:, 'dominant_green'] = dominant_greens\ntrain.loc[:, 'dominant_red'] = dominant_reds\ntrain.loc[:, 'dominant_pixel_frac'] = dominant_pixel_fracs\ntrain.loc[:, 'dominant_score'] = dominant_scores\ntrain.loc[:, 'label_description'] = label_descriptions\ntrain.loc[:, 'label_score'] = label_scores\n\n\nvertex_xs = []\nvertex_ys = []\nbounding_confidences = []\nbounding_importance_fracs = []\ndominant_blues = []\ndominant_greens = []\ndominant_reds = []\ndominant_pixel_fracs = []\ndominant_scores = []\nlabel_descriptions = []\nlabel_scores = []\nnf_count = 0\nnl_count = 0\nfor pet in test_id:\n    try:\n        with open('../input/petfinder-adoption-prediction/test_metadata/' + pet + '-1.json', 'r') as f:\n            data = json.load(f)\n        vertex_x = data['cropHintsAnnotation']['cropHints'][0]['boundingPoly']['vertices'][2]['x']\n        vertex_xs.append(vertex_x)\n        vertex_y = data['cropHintsAnnotation']['cropHints'][0]['boundingPoly']['vertices'][2]['y']\n        vertex_ys.append(vertex_y)\n        bounding_confidence = data['cropHintsAnnotation']['cropHints'][0]['confidence']\n        bounding_confidences.append(bounding_confidence)\n        bounding_importance_frac = data['cropHintsAnnotation']['cropHints'][0].get('importanceFraction', -1)\n        bounding_importance_fracs.append(bounding_importance_frac)\n        dominant_blue = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color']['blue']\n        dominant_blues.append(dominant_blue)\n        dominant_green = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color']['green']\n        dominant_greens.append(dominant_green)\n        dominant_red = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color']['red']\n        dominant_reds.append(dominant_red)\n        dominant_pixel_frac = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['pixelFraction']\n        dominant_pixel_fracs.append(dominant_pixel_frac)\n        dominant_score = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['score']\n        dominant_scores.append(dominant_score)\n        if data.get('labelAnnotations'):\n            label_description = data['labelAnnotations'][0]['description']\n            label_descriptions.append(label_description)\n            label_score = data['labelAnnotations'][0]['score']\n            label_scores.append(label_score)\n        else:\n            nl_count += 1\n            label_descriptions.append('nothing')\n            label_scores.append(-1)\n    except FileNotFoundError:\n        nf_count += 1\n        vertex_xs.append(-1)\n        vertex_ys.append(-1)\n        bounding_confidences.append(-1)\n        bounding_importance_fracs.append(-1)\n        dominant_blues.append(-1)\n        dominant_greens.append(-1)\n        dominant_reds.append(-1)\n        dominant_pixel_fracs.append(-1)\n        dominant_scores.append(-1)\n        label_descriptions.append('nothing')\n        label_scores.append(-1)\n\nprint(nf_count)\ntest.loc[:, 'vertex_x'] = vertex_xs\ntest.loc[:, 'vertex_y'] = vertex_ys\ntest.loc[:, 'bounding_confidence'] = bounding_confidences\ntest.loc[:, 'bounding_importance'] = bounding_importance_fracs\ntest.loc[:, 'dominant_blue'] = dominant_blues\ntest.loc[:, 'dominant_green'] = dominant_greens\ntest.loc[:, 'dominant_red'] = dominant_reds\ntest.loc[:, 'dominant_pixel_frac'] = dominant_pixel_fracs\ntest.loc[:, 'dominant_score'] = dominant_scores\ntest.loc[:, 'label_description'] = label_descriptions\ntest.loc[:, 'label_score'] = label_scores","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1975a457daa49cd7577f6708a0df05f22024df68"},"cell_type":"code","source":"train.drop(['Name', 'RescuerID', 'Description'], axis=1, inplace=True)\ntest.drop(['Name', 'RescuerID', 'Description'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"26dacfa3e6fd3f8b99ec7dc0b93b99f60b60e428"},"cell_type":"code","source":"numeric_cols = ['Age', 'Quantity', 'Fee', 'VideoAmt', 'PhotoAmt', 'AdoptionSpeed', \n                'doc_sent_mag', 'doc_sent_score', 'dominant_score', 'dominant_pixel_frac', \n                'dominant_red', 'dominant_green', 'dominant_blue', 'bounding_importance', \n                'bounding_confidence', 'vertex_x', 'vertex_y', 'label_score'] +\\\n               [col for col in train.columns if col.startswith('pic') or col.startswith('svd')]\ncat_cols = list(set(train.columns) - set(numeric_cols))\ntrain.loc[:, cat_cols] = train[cat_cols].astype('category')\ntest.loc[:, cat_cols] = test[cat_cols].astype('category')\nprint(train.shape)\nprint(test.shape)\n\n# get the categorical features\nfoo = train.dtypes\ncat_feature_names = foo[foo == \"category\"]\ncat_features = [train.columns.get_loc(c) for c in train.columns if c in cat_feature_names]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8cee4ed7a0fcf99e9ce12b1cf6a785c2947514b4"},"cell_type":"code","source":"train = train.drop('label_description',axis=1)\ntest= test.drop('label_description', axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1273e5bdee0ed1b8a51097f8b1c85a2c08113a52","scrolled":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0a0dd796fdf5fa0662d52ce611eb8321db66067b"},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f58e6fc6c9902c2039d99091f3f3d28fdad6217b"},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e891cc665ccf654b15ece70e8d33191694daaf68"},"cell_type":"code","source":"col_vals_dict= {col: list(train[col].unique()) for col in train.select_dtypes(['category'])}\nembed_cols = []\nfor c in col_vals_dict:\n    if len(col_vals_dict[c])>=2:\n        embed_cols.append(c)\n        print(c + ': %d values' % len(col_vals_dict[c])) #look at value counts to know the embedding dimensions  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"40f0fb5863e1efaf4f42652e61153dde7e93b84e"},"cell_type":"code","source":"num_train = train.select_dtypes(['float64','int64'])\nnum_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"243a1778aa65d3a9323674f825752b8570734559"},"cell_type":"code","source":"pic_cols = [col for col in num_train.columns if col.startswith('pic')]\nsvd_cols = [col for col in num_train.columns if col.startswith('svd')]\nmeta_cols = ['vertex_x', 'vertex_y', 'bounding_confidence', 'bounding_importance',\n             'dominant_blue','dominant_green', 'dominant_red' ,'dominant_pixel_frac' , 'dominant_score','label_score']\nsent_cols = ['doc_sent_mag','doc_sent_score']\nothernum_cols = list(set(num_train.columns) - set(meta_cols)- set(pic_cols)- set(svd_cols)- set(sent_cols))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c0dd0c5bf356b9a59af5a192b62d1f270f749b75"},"cell_type":"code","source":"from keras.callbacks import *\n\nclass CyclicLR(Callback):\n    \"\"\"This callback implements a cyclical learning rate policy (CLR).\n    The method cycles the learning rate between two boundaries with\n    some constant frequency, as detailed in this paper (https://arxiv.org/abs/1506.01186).\n    The amplitude of the cycle can be scaled on a per-iteration or \n    per-cycle basis.\n    This class has three built-in policies, as put forth in the paper.\n    \"triangular\":\n        A basic triangular cycle w/ no amplitude scaling.\n    \"triangular2\":\n        A basic triangular cycle that scales initial amplitude by half each cycle.\n    \"exp_range\":\n        A cycle that scales initial amplitude by gamma**(cycle iterations) at each \n        cycle iteration.\n    For more detail, please see paper.\n    \n    # Example\n        ```python\n            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n                                step_size=2000., mode='triangular')\n            model.fit(X_train, Y_train, callbacks=[clr])\n        ```\n    \n    Class also supports custom scaling functions:\n        ```python\n            clr_fn = lambda x: 0.5*(1+np.sin(x*np.pi/2.))\n            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n                                step_size=2000., scale_fn=clr_fn,\n                                scale_mode='cycle')\n            model.fit(X_train, Y_train, callbacks=[clr])\n        ```    \n    # Arguments\n        base_lr: initial learning rate which is the\n            lower boundary in the cycle.\n        max_lr: upper boundary in the cycle. Functionally,\n            it defines the cycle amplitude (max_lr - base_lr).\n            The lr at any cycle is the sum of base_lr\n            and some scaling of the amplitude; therefore \n            max_lr may not actually be reached depending on\n            scaling function.\n        step_size: number of training iterations per\n            half cycle. Authors suggest setting step_size\n            2-8 x training iterations in epoch.\n        mode: one of {triangular, triangular2, exp_range}.\n            Default 'triangular'.\n            Values correspond to policies detailed above.\n            If scale_fn is not None, this argument is ignored.\n        gamma: constant in 'exp_range' scaling function:\n            gamma**(cycle iterations)\n        scale_fn: Custom scaling policy defined by a single\n            argument lambda function, where \n            0 <= scale_fn(x) <= 1 for all x >= 0.\n            mode paramater is ignored \n        scale_mode: {'cycle', 'iterations'}.\n            Defines whether scale_fn is evaluated on \n            cycle number or cycle iterations (training\n            iterations since start of cycle). Default is 'cycle'.\n    \"\"\"\n\n    def __init__(self, base_lr=0.001, max_lr=0.006, step_size=2000., mode='triangular',\n                 gamma=1., scale_fn=None, scale_mode='cycle'):\n        super(CyclicLR, self).__init__()\n\n        self.base_lr = base_lr\n        self.max_lr = max_lr\n        self.step_size = step_size\n        self.mode = mode\n        self.gamma = gamma\n        if scale_fn == None:\n            if self.mode == 'triangular':\n                self.scale_fn = lambda x: 1.\n                self.scale_mode = 'cycle'\n            elif self.mode == 'triangular2':\n                self.scale_fn = lambda x: 1/(2.**(x-1))\n                self.scale_mode = 'cycle'\n            elif self.mode == 'exp_range':\n                self.scale_fn = lambda x: gamma**(x)\n                self.scale_mode = 'iterations'\n        else:\n            self.scale_fn = scale_fn\n            self.scale_mode = scale_mode\n        self.clr_iterations = 0.\n        self.trn_iterations = 0.\n        self.history = {}\n\n        self._reset()\n\n    def _reset(self, new_base_lr=None, new_max_lr=None,\n               new_step_size=None):\n        \"\"\"Resets cycle iterations.\n        Optional boundary/step size adjustment.\n        \"\"\"\n        if new_base_lr != None:\n            self.base_lr = new_base_lr\n        if new_max_lr != None:\n            self.max_lr = new_max_lr\n        if new_step_size != None:\n            self.step_size = new_step_size\n        self.clr_iterations = 0.\n        \n    def clr(self):\n        cycle = np.floor(1+self.clr_iterations/(2*self.step_size))\n        x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1)\n        if self.scale_mode == 'cycle':\n            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(cycle)\n        else:\n            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(self.clr_iterations)\n        \n    def on_train_begin(self, logs={}):\n        logs = logs or {}\n\n        if self.clr_iterations == 0:\n            K.set_value(self.model.optimizer.lr, self.base_lr)\n        else:\n            K.set_value(self.model.optimizer.lr, self.clr())        \n            \n    def on_batch_end(self, epoch, logs=None):\n        \n        logs = logs or {}\n        self.trn_iterations += 1\n        self.clr_iterations += 1\n\n        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n        self.history.setdefault('iterations', []).append(self.trn_iterations)\n\n        for k, v in logs.items():\n            self.history.setdefault(k, []).append(v)\n        \n        K.set_value(self.model.optimizer.lr, self.clr())\n    \nclass QWKEvaluation(Callback):\n    def __init__(self, validation_data=(), interval=1):\n        super(Callback, self).__init__()\n\n        self.interval = interval\n        self.history = []\n        self.X_val, self.y_val = validation_data\n        \n    def on_epoch_end(self, epoch, logs={}):\n        if epoch % self.interval == 0:\n            y_pred = self.model.predict(self.X_val, batch_size=1000, verbose=0)\n            y_pred = eval_predict(self.y_val, y_pred)\n            score = quadratic_weighted_kappa(self.y_val, y_pred)\n            print(\"QWK - epoch: %d - score: %.6f \\n\" % (epoch+1, score))\n            self.history.append(score)\n            if score >= max(self.history): self.model.save('checkpoint.h5')\ndef eval_predict(y=[], y_pred=[], coeffs=None, ret_coeffs=False):\n    optR = OptimizedRounder()\n    if not coeffs:\n        optR.fit(y_pred.reshape(-1,), y)\n        coeffs = optR.coefficients()\n    if ret_coeffs: return optR.coefficients()\n    return optR.predict(y_pred, coeffs).reshape(-1,)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"983c9642d4f77441b72e69b3e888e9bb2eaf647e"},"cell_type":"code","source":"from keras.models import Model\nfrom keras.layers import Input, Dense, Concatenate, Reshape, Dropout\nfrom keras.layers.embeddings import Embedding\nfrom keras.layers import Input, Dropout, Dense, BatchNormalization, Lambda, concatenate, GRU, Embedding, Flatten, add, multiply, GlobalMaxPool1D\nfrom keras.models import Model\nfrom keras.callbacks import ModelCheckpoint, Callback, EarlyStopping\nfrom keras import backend as K\nfrom keras import optimizers\nfrom keras import initializers\nimport tensorflow as tf\n\nseed = 2019\nnp.random.seed(seed)\ntf.set_random_seed(seed)\n\ndef get_model(dr=0.15, seed=2019):\n    np.random.seed(seed)\n    tf.set_random_seed(seed)\n    sess = tf.Session()\n    K.set_session(sess)\n    \n    #embedding layers\n    inputs = []\n    embeddings = []\n    \n    input_type = Input(shape=[1], name='Type')\n    embedding = Embedding(2,1, input_length=1)(input_type)\n    embedding = Reshape(target_shape=(1,))(embedding)\n    inputs.append(input_type)\n    embeddings.append(embedding)\n    \n    input_breed1 = Input(shape=[1],name= 'Breed1')\n    embedding = Embedding(176,10, input_length=1)(input_breed1)\n    embedding = Reshape(target_shape=(10,))(embedding)\n    inputs.append(input_breed1)\n    embeddings.append(embedding)\n    \n    input_breed2 = Input(shape=[1], name= 'Breed2')\n    embedding = Embedding(135,10, input_length=1)(input_breed2)\n    embedding = Reshape(target_shape=(10,))(embedding)\n    inputs.append(input_breed2)\n    embeddings.append(embedding)\n    \n    input_gender = Input(shape=[1], name= 'Gender')\n    embedding = Embedding(3,2, input_length=1)(input_gender)\n    embedding = Reshape(target_shape=(2,))(embedding)\n    inputs.append(input_gender)\n    embeddings.append(embedding)\n    \n    input_color1 = Input(shape=[1], name= 'Color1')\n    embedding = Embedding(7,4, input_length=1)(input_color1)\n    embedding = Reshape(target_shape=(4,))(embedding)\n    inputs.append(input_color1)\n    embeddings.append(embedding)\n    \n    input_color2 = Input(shape=[1], name= 'Color2')\n    embedding = Embedding(7,4, input_length=1)(input_color2)\n    embedding = Reshape(target_shape=(4,))(embedding)\n    inputs.append(input_color2)\n    embeddings.append(embedding)\n    \n    input_color3 = Input(shape=[1], name= 'Color3')\n    embedding = Embedding(6,3, input_length=1)(input_color3)\n    embedding = Reshape(target_shape=(3,))(embedding)\n    inputs.append(input_color3)\n    embeddings.append(embedding)\n    \n    input_maturity_size = Input(shape=[1], name= 'MaturitySize')\n    embedding = Embedding(4,2, input_length=1)(input_maturity_size)\n    embedding = Reshape(target_shape=(2,))(embedding)\n    inputs.append(input_maturity_size)\n    embeddings.append(embedding)\n    \n    input_fur_length = Input(shape=[1], name= 'FurLength')\n    embedding = Embedding(3,2, input_length=1)(input_fur_length)\n    embedding = Reshape(target_shape=(2,))(embedding)\n    inputs.append(input_fur_length)\n    embeddings.append(embedding)\n    \n    input_vaccinated = Input(shape=[1], name= 'Vaccinated')\n    embedding = Embedding(3,2, input_length=1)(input_vaccinated)\n    embedding = Reshape(target_shape=(2,))(embedding)\n    inputs.append(input_vaccinated)\n    embeddings.append(embedding)\n    \n    input_dewormed = Input(shape=[1], name='Dewormed')\n    embedding = Embedding(3,2, input_length=1)(input_vaccinated)\n    embedding = Reshape(target_shape=(2,))(embedding)\n    inputs.append(input_dewormed)\n    embeddings.append(embedding)\n    \n    input_sterilized = Input(shape=[1], name='Sterilized')\n    embedding = Embedding(3,2, input_length=1)(input_sterilized)\n    embedding = Reshape(target_shape=(2,))(embedding)\n    inputs.append(input_sterilized)\n    embeddings.append(embedding)\n    \n    input_health = Input(shape=[1], name='Health')\n    embedding = Embedding (3,2, input_length=1)(input_health)\n    embedding = Reshape(target_shape=(2,))(embedding)\n    inputs.append(input_health)\n    embeddings.append(embedding)\n    \n    input_state = Input(shape=[1], name='State')\n    embedding = Embedding (14,7, input_length=1)(input_health)\n    embedding = Reshape(target_shape=(7,))(embedding)\n    inputs.append(input_state)\n    embeddings.append(embedding)\n    \n    # numeric cols & batch normalization   \n    input_pic = Input(shape=[num_train[pic_cols].shape[1]], name='pic_col')\n    bn_pic = BatchNormalization()(input_pic)\n    inputs.append(input_pic)\n    embeddings.append(bn_pic)\n    \n    input_svd = Input(shape=[num_train[svd_cols].shape[1]], name='svd_col')\n    bn_svd = BatchNormalization()(input_svd)\n    inputs.append(input_svd)\n    embeddings.append(bn_svd)\n    \n    input_meta = Input(shape=[num_train[meta_cols].shape[1]], name='meta_col')\n    bn_meta = BatchNormalization()(input_meta)\n    inputs.append(input_meta)\n    embeddings.append(bn_meta)\n    \n    input_sent = Input(shape=[num_train[sent_cols].shape[1]], name='sent_col')\n    bn_sent = BatchNormalization()(input_sent)\n    inputs.append(input_sent)\n    embeddings.append(bn_sent)\n    \n    input_other = Input(shape=[num_train[othernum_cols].shape[1]], name='other_col')\n    bn_other = BatchNormalization()(input_other)\n    inputs.append(input_other)\n    embeddings.append(bn_other)\n    \n    # model compiling \n    \n    main_l= Concatenate()(embeddings)\n    main_l = BatchNormalization()(main_l)\n    main_l= Dropout(dr)(Dense(250, activation = 'relu')(main_l))\n    #main_l = BatchNormalization()(main_l)\n    main_l= Dropout(dr)(Dense(100, activation = 'relu')(main_l))\n    #main_l = BatchNormalization()(main_l)\n    main_l = Dense(50, activation='relu')(main_l)\n    #main_l = BatchNormalization()(main_l)\n    main_l= Dense(10, activation = 'relu')(main_l)\n    #main_l = BatchNormalization()(main_l)\n           \n    #output\n    output = Dense(1, activation = 'linear')(main_l)\n    \n    #model\n    model = Model(inputs, output)\n    #optimizer = optimizers.Adam(lr=0.01, decay=3e-5)\n    optimizer= optimizers.Adam()\n    model.compile(loss='logcosh', optimizer=optimizer)\n\n    return model, sess\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"59be3a17883da8e0a26795eeeee786374d6c4dbb"},"cell_type":"code","source":"def preproc(df):\n    input_list = []\n    #the cols to be embedded: rescaling to range [0, # values)\n    for c in embed_cols:\n        raw_vals = np.unique(df[c])\n        val_map = {}\n        for i in range(len(raw_vals)):\n            val_map[raw_vals[i]] = i       \n        input_list.append(df[c].map(val_map).fillna(0).values)\n     \n    #the rest of the columns\n  \n    input_list.append(df[pic_cols].values)\n    input_list.append(df[svd_cols].values)\n    input_list.append(df[meta_cols].values)\n    input_list.append(df[sent_cols].values)\n    input_list.append(df[othernum_cols].values)\n\n    return input_list\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"69c2aba239ed00cd56a69104cbe7366f49cca96d","scrolled":false},"cell_type":"code","source":"epochs = 5\nBATCH_SIZE = 1000\nN_SPLITS = 5\nmodel, sess = get_model()\nK.set_session(sess)\n\n\ndef run_cv_model(train, target, model_fn, eval_fn=None, label='model'):\n    kf = StratifiedKFold(n_splits=N_SPLITS, random_state=1906, shuffle=True)\n    fold_splits = kf.split(train, target)\n    cv_scores = []\n    qwk_scores = []\n    pred_full_test = 0\n    pred_train = np.zeros((train.shape[0],))\n    all_coefficients = np.zeros((N_SPLITS, 4))\n    feature_importance_df = pd.DataFrame()\n    i = 1\n    for train_index, val_index in fold_splits:\n        print('Started ' + label + ' fold ' + str(i) + '/' + str(N_SPLITS))\n        X_train = preproc(train.iloc[train_index])\n        X_val = preproc(train.iloc[val_index])\n        y_train = target.iloc[train_index].values\n        y_val = target.iloc[val_index].values\n        X_test = preproc(test.copy())\n        pred_val_y, pred_test_y, coefficients, qwk = model_fn(X_train, X_val, y_train, y_val,X_test)\n        pred_full_test = pred_full_test + pred_test_y\n        pred_train[val_index] = pred_val_y\n        all_coefficients[i-1, :] = coefficients\n        if eval_fn is not None:\n            cv_score = eval_fn(y_val, pred_val_y)\n            cv_scores.append(cv_score)\n            qwk_scores.append(qwk)\n            print(label + ' cv score {}: RMSE {} QWK {}'.format(i, cv_score, qwk))           \n        i += 1\n    print('{} cv RMSE scores : {}'.format(label, cv_scores))\n    print('{} cv mean RMSE score : {}'.format(label, np.mean(cv_scores)))\n    print('{} cv std RMSE score : {}'.format(label, np.std(cv_scores)))\n    print('{} cv QWK scores : {}'.format(label, qwk_scores))\n    print('{} cv mean QWK score : {}'.format(label, np.mean(qwk_scores)))\n    print('{} cv std QWK score : {}'.format(label, np.std(qwk_scores)))\n    pred_full_test = pred_full_test / float(N_SPLITS)\n    results = {'label': label,\n               'train': pred_train,'cv': cv_scores, 'qwk': qwk_scores,'test': pred_full_test,\n               'coefficients': all_coefficients}\n    return results\n\n\ndef runNN(X_train, X_val, y_train, y_val, X_test):\n    model, sess = get_model()\n    K.set_session(sess)\n    \n    clr_tri = CyclicLR(base_lr=2e-3, max_lr=4e-2, step_size=len(X_train)//batch_size, mode=\"triangular2\")\n    qwk_eval = QWKEvaluation(validation_data=(X_val, y_val), interval=1)\n\n    history = model.fit(X_train, y_train, epochs=epochs, batch_size=BATCH_SIZE, validation_data=(X_val, y_val),callbacks=[clr_tri, qwk_eval])\n            \n    print('Predict 1/2')\n    y_pred = model.predict(X_val,batch_size=BATCH_SIZE).ravel()\n    optR = OptimizedRounder()\n    optR.fit(y_pred, y_val)\n    coefficients = optR.coefficients()\n    y_pred_k = optR.predict(y_pred, coefficients)\n    print(\"Valid Counts = \", Counter(y_val))\n    print(\"Predicted Counts = \", Counter(y_pred_k))\n    print(\"Coefficients = \", coefficients)\n    qwk = quadratic_weighted_kappa(y_val, y_pred_k)\n    print(\"QWK = \", qwk)\n    print('Predict 2/2')\n    test_pred = model.predict(X_test,batch_size=BATCH_SIZE).ravel()\n    return y_pred, test_pred, coefficients, qwk\n\nresults = run_cv_model(train, target, runNN, rmse, 'NN')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ba2aedbf0628c89c3e3a0ac4244a278c7d0596ca"},"cell_type":"code","source":"optR = OptimizedRounder()\ncoefficients_ = np.mean(results['coefficients'], axis=0)\nprint(coefficients_)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2583e2421387e310c4fd95cabda5098734fee931"},"cell_type":"code","source":"coefficients_[0] = 1.645\ncoefficients_[1] = 2.115\ncoefficients_[3] = 2.84\ntrain_predictions = results['train']\ntrain_predictions = optR.predict(train_predictions, coefficients_).astype(int)\nCounter(train_predictions)\nprint(quadratic_weighted_kappa(train_predictions,target))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1fc1f7e1c203317d8c649d488314503b2f474fb8"},"cell_type":"code","source":"optR = OptimizedRounder()\ncoefficients_ = np.mean(results['coefficients'], axis=0)\nprint(coefficients_)\n# manually adjust coefs\ncoefficients_[0] = 1.645\ncoefficients_[1] = 2.115\ncoefficients_[3] = 2.84\ntest_predictions = results['test']\ntest_predictions = optR.predict(test_predictions, coefficients_).astype(int)\nCounter(test_predictions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f019ea6325e7b9eb5a5d4e5dea1ad47e67829b25"},"cell_type":"code","source":"print(\"True Distribution:\")\nprint(pd.value_counts(target, normalize=True).sort_index())\nprint(\"Test Predicted Distribution:\")\nprint(pd.value_counts(test_predictions, normalize=True).sort_index())\nprint(\"Train Predicted Distribution:\")\nprint(pd.value_counts(train_predictions, normalize=True).sort_index())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"68c546b35ba092c7e566ef1e930572886a1fc024"},"cell_type":"code","source":"pd.DataFrame(sk_cmatrix(target, train_predictions), index=list(range(5)), columns=list(range(5)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e8e8422d3f7ca7d3ecdf2ff8f7e7ec76623dca8e"},"cell_type":"code","source":"quadratic_weighted_kappa(target, train_predictions)\nrmse(target, results['train'])\nsubmission = pd.DataFrame({'PetID': test_id, 'AdoptionSpeed': test_predictions})\nsubmission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e2cddebf3df3dddbecac8c8d364eaafacea3d62d"},"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}