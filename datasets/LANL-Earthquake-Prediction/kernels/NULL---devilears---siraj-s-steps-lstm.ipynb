{"cells":[{"metadata":{"_uuid":"7b1bf02a652f9d059568e248816413739a811c90"},"cell_type":"markdown","source":"# A kernel modeled after Siraj's recent video\nSiraj probably needs no introduction. He recently did a [video on this earthquake competition](https://www.youtube.com/watch?v=TffGdSsWKlA), which is informative to data science noobs like me. I like the step by step approach. Watch that video if you want to see how to construct a kernel from scratch, but do note that it runs on Google's Colab framework.\n\nFollowing Siraj's step by step approach, I'm following my own steps in constructing this kernel in a Kaggle environment. I added the submission steps, since I want to use this kernel as a template with other models instead of the LSTM. \n\n## My Steps\n1. Install the Dependencies\n1. Import the Dataset\n1. Exploratory Data Analysis\n1. Feature Engineering\n1. Implement Model\n1. Evaluate Model\n1. Finalise Submission\n1. Future Directions\n\n## Install the Dependencies\nFortunately, the kaggle dependencies are already installed. I opted to install model specific dependencies in that code block, since the code is easier to modify in terms of swopping out models. It's also easier to see what you have imported and what is still missing in this way."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\n# Installing dependencies\n#data preprocessing\nimport pandas as pd\n#math operations\nimport numpy as np\n#data visualization\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"## Import the Dataset\nSince we aren't using the Google Colab framework, we can import the dataset directly. "},{"metadata":{"trusted":true,"_uuid":"d91c5674070f84a38a6e25b6ab8a58b34a27994e"},"cell_type":"code","source":"# Extract training data into a dataframe for further manipulation\ntrain = pd.read_csv('../input/train.csv', nrows=6000000, dtype={'acoustic_data': np.int16, 'time_to_failure': np.float64})\n\n# Print first 10 entries\ntrain.head(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"69f88065c08ee49b6addbef976eca917636dd576"},"cell_type":"markdown","source":"## Exploratory Data Analysis\nFor this, we just visualise the data and see what we can deduce from the graphs."},{"metadata":{"trusted":true,"_uuid":"ec82e1a615678bbde3f2f1d3524e3331fc79e398"},"cell_type":"code","source":"#visualize 1% of samples data, first 100 datapoints\ntrain_ad_sample_df = train['acoustic_data'].values[::100]\ntrain_ttf_sample_df = train['time_to_failure'].values[::100]\n\n#function for plotting based on both features\ndef plot_acc_ttf_data(train_ad_sample_df, train_ttf_sample_df, title=\"Acoustic data and time to failure: 1% sampled data\"):\n    fig, ax1 = plt.subplots(figsize=(12, 8))\n    plt.title(title)\n    plt.plot(train_ad_sample_df, color='r')\n    ax1.set_ylabel('acoustic data', color='r')\n    plt.legend(['acoustic data'], loc=(0.01, 0.95))\n    ax2 = ax1.twinx()\n    plt.plot(train_ttf_sample_df, color='b')\n    ax2.set_ylabel('time to failure', color='b')\n    plt.legend(['time to failure'], loc=(0.01, 0.9))\n    plt.grid(True)\n\nplot_acc_ttf_data(train_ad_sample_df, train_ttf_sample_df)\ndel train_ad_sample_df\ndel train_ttf_sample_df","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"721073de2aa8c4ae5c557e0c6ee7c6b5f80eae7f"},"cell_type":"markdown","source":"## Feature Engineering\nAdding statistical features. Siraj adds a few of the most common ones, plus he has a handy description of most of these in his [notebook on github](https://github.com/llSourcell/Kaggle_Earthquake_challenge/blob/master/Earthquake_Challenge.ipynb). \n\nThis kernel, called [Earthquakes FE More Features and Samples](https://www.kaggle.com/artgor/earthquakes-fe-more-features-and-samples), contains more features. There are also links to more ideas if you want to engineer additional features. For this kernel, I'm sticking to Siraj's basics."},{"metadata":{"trusted":true,"_uuid":"ec2bf170a4bc2af0a1c28f46c20b68ff5af11051"},"cell_type":"code","source":"# let's create a function to generate some statistical features based on the training data\ndef gen_features(X):\n    strain = []\n    strain.append(X.mean())\n    strain.append(X.std())\n    strain.append(X.min())\n    strain.append(X.max())\n    strain.append(X.kurtosis())\n    strain.append(X.skew())\n    strain.append(np.quantile(X,0.01))\n    strain.append(np.quantile(X,0.05))\n    strain.append(np.quantile(X,0.95))\n    strain.append(np.quantile(X,0.99))\n    strain.append(np.abs(X).max())\n    strain.append(np.abs(X).mean())\n    strain.append(np.abs(X).std())\n    return pd.Series(strain)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"414361a56616b9dce41424285ba0859ca5c8829f"},"cell_type":"markdown","source":"For this, note how the chunks are used to iterate through this fairly large dataset. Each chunk is processed, and the results are appended to the *X_train* and *y_train* dataframes. This is somewhat different from some of the other kernels that I used, where all the chunks weren't iterated through and the training data was just limited."},{"metadata":{"trusted":true,"_uuid":"f9b157d842d0c799a7b483f6bd327b18c1fe2e5a"},"cell_type":"code","source":"train = pd.read_csv('../input/train.csv', iterator=True, chunksize=150_000, dtype={'acoustic_data': np.int16, 'time_to_failure': np.float64})\n\nX_train = pd.DataFrame()\ny_train = pd.Series()\nfor df in train:\n    ch = gen_features(df['acoustic_data'])\n    X_train = X_train.append(ch, ignore_index=True)\n    y_train = y_train.append(pd.Series(df['time_to_failure'].values[-1]))\n    \n\n# Let's see what we have done\nX_train.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1b34ec7db9df118192efc89816e9fb3d7257b2ab"},"cell_type":"markdown","source":"## Implement Model\nIn this case, I am implementing an [LSTM Model](https://machinelearningmastery.com/multivariate-time-series-forecasting-lstms-keras/). \n\nSiraj implemented an SVR and a CatBoost model. Since I want to use this as a template, I didn't replicate the SVR or CatBoost model here, but decided to implement a different one. If you are interested in the others, have a look at Siraj's work. The SVR seems to get a better score than the CatBoost model.\n\nI ran into some troupble with the shapes of the training and testing data. The LSTM model itself is not that important to me here, since I only want to use this kernel as a template for other models. Regardless, if you are interested, this model obtained a score of 1,664 in the competition.\n\n"},{"metadata":{"trusted":true,"_uuid":"690627e7f3a11d5814b64bb062ebc8e85cab3aff"},"cell_type":"code","source":"# LSTM Model\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\n\nscaler = StandardScaler()\nscaler.fit(X_train)\nX_train_scaled = scaler.transform(X_train)\n\n# split into train and test sets\ntrain_X, X_test, train_y, y_test = train_test_split(X_train_scaled, y_train, test_size=0.2)\nprint(train_X.shape)\nprint(X_test.shape)\nprint(train_y.shape)\nprint(y_test.shape)\n\n# reshape to correct dimensions\ntrain_X = train_X.reshape(3356, 13, 1)\nX_test = X_test.reshape(839,13,1)\n\n# Model\nmodel = Sequential()\nmodel.add(LSTM(50, input_shape=(13,1)))\nmodel.add(Dense(1))\nmodel.compile(loss='mae', optimizer='adam')\n\n# fit network\nhistory = model.fit(train_X, \n                    train_y, \n                    epochs=15, \n                    validation_data=(X_test, y_test),\n                    batch_size=64,\n                    verbose=2, \n                    shuffle=False)\n\nmodel.summary()\n\n# plot history\nplt.plot(history.history['loss'], label='train')\nplt.plot(history.history['val_loss'], label='test')\nplt.legend()\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"94cb798c98bf452c5a81ee72ab0ab4960d05c5ce"},"cell_type":"markdown","source":"## Evaluate Model\nDo the hyper parameter tuning or other means of evaluating the model here.\n\nI commented this out since it's not related to the LSTM model above.  "},{"metadata":{"trusted":true,"_uuid":"8a7b2318ebc1f38f7b1b478d7ad9b330591584c0"},"cell_type":"code","source":"# Evaluate model\n'''scaler = StandardScaler()\nscaler.fit(X_train)\nX_train_scaled = scaler.transform(X_train)\n\n# Parameters for hyper parameter tuning\n#parameters = [{'gamma': [0.001, 0.005, 0.01, 0.02, 0.05, 0.1],\n#               'C': [0.1, 0.2, 0.25, 0.5, 1, 1.5, 2, 2.5, 3.0, 5]}]\n\n# Hyper parameter tuning\n#reg1 = GridSearchCV(SVR(kernel='rbf', tol=0.01), parameters, cv=5, scoring='neg_mean_absolute_error')\n#reg1.fit(X_train_scaled, y_train.values.flatten())\n#y_pred1 = reg1.predict(X_train_scaled)\n\n#print(\"Best CV score: {:.4f}\".format(reg1.best_score_))\n#print(reg1.best_params_)\n\nmodel.compile(optimizer=adam(lr=0.0005), loss=\"mae\")\n\nmodel.fit(X_train_scaled, y_train.values.flatten())\ny_pred1 = model.predict(X_trained_scaled)\n\n\n\nhistory = model.fit_generator(train_gen,\n                              steps_per_epoch=1000,\n                              epochs=30,\n                              verbose=0,\n                              callbacks=cb,\n                              validation_data=valid_gen,\n                              validation_steps=200)''' \n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ce15e862f2c592102253a339b8cf622019561abb"},"cell_type":"markdown","source":"## Finalise Submission\nSiraj doesn't provide the code for this. I had to glean this from other kernels. So far, we have only seen training data. We haven't dealt with test data, so that needs to be done as well."},{"metadata":{"trusted":true,"_uuid":"6dcfd6760d343d801ad82716482abdb9e7225056"},"cell_type":"code","source":"# submission format\nsubmission = pd.read_csv('../input/sample_submission.csv', index_col='seg_id')\nX_test = pd.DataFrame()\n\n# prepare test data\nfor seg_id in submission.index:\n    seg = pd.read_csv('../input/test/' + seg_id + '.csv')\n    ch = gen_features(seg['acoustic_data'])\n    X_test = X_test.append(ch, ignore_index=True)\n\nX_test = scaler.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f77b86f347708aca857cf6bec6d7381ba2d014b"},"cell_type":"markdown","source":"### Generate predictions for the submission.\n\nThe prediction here is from the LSTM model. You would have to reinitialise the SVR with the hyper parameter tuning values and get your prediction from that if you want to submit SVR model data."},{"metadata":{"trusted":true,"_uuid":"461614a70baee26d83a57154838a4b68a772c669"},"cell_type":"code","source":"# model of choice here\n#(2624, 13)\nX_test = X_test.reshape(2624, 13, 1)\ny_pred2 = model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"04f467672bc68c5fc5e0698927f9a8aac12f8d78"},"cell_type":"code","source":"# write submission file\nsubmission['time_to_failure'] = y_pred2\nsubmission.to_csv('submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"483ef86e8a3c9ad40c211b02c91b198526b7d6ce"},"cell_type":"markdown","source":"## Future Directions\n1. Recurrent Networks. See my kernel, [RNN Starter Kernel](https://www.kaggle.com/devilears/rnn-starter-kernel-with-notebook), for starter code.\n1. Genetic Algorithms.\n1. [Ordinary Differential Equation Network](https://towardsdatascience.com/paper-summary-neural-ordinary-differential-equations-37c4e52df128). See also [Neural Ordinary Differential Equations](https://paperswithcode.com/paper/neural-ordinary-differential-equations)."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}