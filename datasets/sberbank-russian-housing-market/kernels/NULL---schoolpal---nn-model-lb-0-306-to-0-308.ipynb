{"metadata": {"kernelspec": {"name": "python3", "language": "python", "display_name": "Python 3"}, "language_info": {"pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py", "name": "python", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "version": "3.6.1"}}, "nbformat": 4, "nbformat_minor": 0, "cells": [{"metadata": {"collapsed": false, "_cell_guid": "4c7235d8-915f-4f6b-9ee5-7d6aced76c31", "_execution_state": "idle", "_uuid": "80f20448cf12747dad231f924012f135cb391856"}, "source": "Updates:\n\nThe stacking model:\nhttps://www.kaggle.com/schoolpal/nn-stacking-magic-no-magic-30409-private-31063\n\n----------------------------------\n\nThis is the DNN model which also used as one basis model for the stacking. The LB score at 0.308, and LB 0.306 when linearly combined with the XGB and LGB results produced using\nhttps://www.kaggle.com/schoolpal/lgbm-lb-0-3093-0-3094 and https://www.kaggle.com/schoolpal/modifications-to-reynaldo-s-script. \n\nThe model uses feature from to other models (XGB, LGB), so it is also a stacking model.  If the XGB and LGB scores were not used, the DNN's performance is at LB 0.312-0.313,.\n\nThe macro data and a few FE were used for the DNN model. If you are interested, read the comments in prepare_data() and nn().\n\nI set the epochs to 1 in order to bypass the limit on running time for kernel. The epochs should be 40. \n\nIf you want to try it on your own machine, use Tensorflow backend. You will need GPU to run this script (took 10 minutes on GTX 1080). Btw, I am migrating to Pytorch.", "execution_count": null, "cell_type": "markdown", "outputs": []}, {"metadata": {"_cell_guid": "a06f794f-826b-4be3-8fd5-ad1a7172bd0f", "_execution_state": "idle", "_uuid": "5c88eb90fca061c08d97efa80aced76efe8db780", "trusted": false}, "source": "\nimport os\nimport sys\nfrom keras.layers.advanced_activations import *\nfrom keras.callbacks import LearningRateScheduler\nimport pickle\n\nfrom keras.layers.merge import *\nfrom keras.layers.noise import *\nfrom sklearn.preprocessing import StandardScaler,MinMaxScaler,Normalizer,Imputer,RobustScaler\nfrom keras.optimizers import SGD,RMSprop,Adam,Adadelta\nfrom keras.models import Sequential,Model\nfrom keras.layers import Dense, Dropout, Activation, Input, Embedding,Flatten\nfrom keras.layers.normalization import BatchNormalization\nimport pandas as pd\nimport numpy as np\nnp.random.seed(1)\nimport pdb\nimport keras\nNORM=False\nLOG=True\ndef last_days(train,day_range=30,cols=['full_sq'],changes=False):\n    days=train.set_index('timestamp').groupby(pd.TimeGrouper('D'))\n    for col in cols:\n        means=days[col].mean()\n        if changes:\n            means=means.pct_change()\n        means_avg=dict()\n        for i in range(len(means)):\n            key=means.index[i].year*10000+means.index[i].month*100+means.index[i].day\n            if i>0:\n                start=max(i-day_range,0)\n                means_avg[key]=means.iloc[start:i].mean()\n            else:\n                means_avg[key]=0\n        val_list=[]\n        for i in range(len(train)):\n            t=train.iloc[i].timestamp\n            key=t.year*10000+t.month*100+t.day\n            val_list.append(means_avg[key])\n        train[col+'_avg'+str(day_range)]=val_list\n    return train\ndef fill_maxfloor(df_all):\n    # Bhavesh Ghodasara's idea\n    apartments=df_all.sub_area + df_all['ID_bus_terminal'].astype(str)+df_all['bus_terminal_avto_km'].astype(str)\n    df_all['an']=apartments\n    df_all.loc[df_all['max_floor']==0,'max_floor']=np.NaN\n    grouped_years=df_all.groupby(['an']).max_floor\n    an_years=grouped_years.median()\n    an_years_max=grouped_years.max()\n    an_years_min=grouped_years.min()\n    an_years[an_years_max-an_years_min>1]=np.NaN\n    df_all=df_all.join(an_years,on='an',rsuffix='_an')\n    df_all.loc[df_all.max_floor.isnull(),'max_floor']=df_all['max_floor_an'][df_all.max_floor.isnull()]\n    df_all.drop(['an','max_floor_an'],inplace=True,axis=1)\n    return df_all\n\ndef fill_years(df_all,threshold=3,preprocess=True):\n    # Bhavesh Ghodasara's idea\n    apartments=df_all.sub_area + df_all['ID_bus_terminal'].astype(str)+df_all['bus_terminal_avto_km'].astype(str)\n    df_all['an']=apartments\n    build_years=df_all.build_year.copy()\n    if preprocess:\n        df_all.loc[df_all['build_year']<1,'build_year']=np.NaN\n    grouped_years=df_all.groupby(['an']).build_year\n    an_years=grouped_years.median()\n    an_years_max=grouped_years.max()\n    an_years_min=grouped_years.min()\n    an_years[an_years_max-an_years_min>threshold]=np.NaN\n    df_all=df_all.join(an_years,on='an',rsuffix='_an')\n    df_all.loc[df_all.build_year.isnull(),'build_year']=df_all['build_year_an'][df_all.build_year.isnull()]\n    if not preprocess:\n        df_all.loc[df_all.build_year.isnull(),'build_year']=build_years[df_all.build_year.isnull()]\n    df_all.drop(['an','build_year_an'],inplace=True,axis=1)\n\n    return df_all\ndef step_decay(epoch):\n    lr=0.01\n    start=15\n    step=5\n    if epoch<start:\n        return lr\n    else:\n        lr=lr/np.power(2.0,(1+(epoch-start)/step))\n        return lr\ndef bad_weights(train,prices,price_sq):\n    weights=np.ones(len(price_sq))\n    weights[(train.product_type=='Investment') & (price_sq<40000)]=0.1\n    return weights\n\ndef get_excluded():\n    # Taken from wti200's kernel\n    excluded={\n        \"young_male\", \"school_education_centers_top_20_raion\", \"0_17_female\", \"railroad_1line\", \"7_14_female\", \"0_17_all\", \"children_school\",\"ecology\", \"16_29_male\", \"mosque_count_3000\", \"female_f\", \"church_count_1000\", \"railroad_terminal_raion\",\"mosque_count_5000\", \"big_road1_1line\", \"mosque_count_1000\", \"7_14_male\", \"0_6_female\", \"oil_chemistry_raion\",\"young_all\", \"0_17_male\", \"ID_bus_terminal\", \"university_top_20_raion\", \"mosque_count_500\",\"ID_big_road1\",\"ID_railroad_terminal\", \"ID_railroad_station_walk\", \"ID_big_road2\", \"ID_metro\", \"ID_railroad_station_avto\",\"0_13_all\", \"mosque_count_2000\", \"work_male\", \"16_29_all\", \"young_female\", \"work_female\", \"0_13_female\",\"ekder_female\", \"7_14_all\", \"big_church_count_500\",\"leisure_count_500\", \"cafe_sum_1500_max_price_avg\", \"leisure_count_2000\",\"office_count_500\", \"male_f\", \"nuclear_reactor_raion\", \"0_6_male\", \"church_count_500\", \"build_count_before_1920\",\"thermal_power_plant_raion\", \"cafe_count_2000_na_price\", \"cafe_count_500_price_high\",\"market_count_2000\", \"museum_visitis_per_100_cap\", \"trc_count_500\", \"market_count_1000\", \"work_all\", \"additional_education_raion\",\"build_count_slag\", \"leisure_count_1000\", \"0_13_male\", \"office_raion\",\"raion_build_count_with_builddate_info\", \"market_count_3000\", \"ekder_all\", \"trc_count_1000\", \"build_count_1946-1970\",\"office_count_1500\", \"cafe_count_1500_na_price\", \"big_church_count_5000\", \"big_church_count_1000\", \"build_count_foam\",\"church_count_1500\", \"church_count_3000\", \"leisure_count_1500\",\"16_29_female\", \"build_count_after_1995\", \"cafe_avg_price_1500\", \"office_sqm_1000\", \"cafe_avg_price_5000\", \"cafe_avg_price_2000\",\"big_church_count_1500\", \"full_all\", \"cafe_sum_5000_min_price_avg\",\"office_sqm_2000\", \"church_count_5000\",\"0_6_all\", \"detention_facility_raion\", \"cafe_avg_price_3000\"\"young_male\", \"school_education_centers_top_20_raion\", \"0_17_female\", \"railroad_1line\", \"7_14_female\", \"0_17_all\", \"children_school\",\"ecology\", \"16_29_male\", \"mosque_count_3000\", \"female_f\", \"church_count_1000\", \"railroad_terminal_raion\",\"mosque_count_5000\", \"big_road1_1line\", \"mosque_count_1000\", \"7_14_male\", \"0_6_female\", \"oil_chemistry_raion\",\"young_all\", \"0_17_male\", \"ID_bus_terminal\", \"university_top_20_raion\", \"mosque_count_500\",\"ID_big_road1\",\"ID_railroad_terminal\", \"ID_railroad_station_walk\", \"ID_big_road2\", \"ID_metro\", \"ID_railroad_station_avto\",\"0_13_all\", \"mosque_count_2000\", \"work_male\", \"16_29_all\", \"young_female\", \"work_female\", \"0_13_female\",\"ekder_female\", \"7_14_all\", \"big_church_count_500\",\"leisure_count_500\", \"cafe_sum_1500_max_price_avg\", \"leisure_count_2000\",\"office_count_500\", \"male_f\", \"nuclear_reactor_raion\", \"0_6_male\", \"church_count_500\", \"build_count_before_1920\",\"thermal_power_plant_raion\", \"cafe_count_2000_na_price\", \"cafe_count_500_price_high\",\"market_count_2000\", \"museum_visitis_per_100_cap\", \"trc_count_500\", \"market_count_1000\", \"work_all\", \"additional_education_raion\",\"build_count_slag\", \"leisure_count_1000\", \"0_13_male\", \"office_raion\",\"raion_build_count_with_builddate_info\", \"market_count_3000\", \"ekder_all\", \"trc_count_1000\", \"build_count_1946-1970\",\"office_count_1500\", \"cafe_count_1500_na_price\", \"big_church_count_5000\", \"big_church_count_1000\", \"build_count_foam\",\"church_count_1500\", \"church_count_3000\", \"leisure_count_1500\",\"16_29_female\", \"build_count_after_1995\", \"cafe_avg_price_1500\", \"office_sqm_1000\", \"cafe_avg_price_5000\", \"cafe_avg_price_2000\",\"big_church_count_1500\", \"full_all\", \"cafe_sum_5000_min_price_avg\",\"office_sqm_2000\", \"church_count_5000\",\"0_6_all\", \"detention_facility_raion\", \"cafe_avg_price_3000\"\n    }\n    return excluded\n\n\n", "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"collapsed": false, "_cell_guid": "819d4595-181c-40a1-89c8-3b787dd77469", "_execution_state": "idle", "_uuid": "9ad52bd80dc14f9671edcc98003aa06eca1ef121", "trusted": false}, "source": "def prepare_data():\n    # Copied wti200's kernel: from https://www.kaggle.com/wti200/deep-neural-network-for-starters-r\n    excluded=get_excluded()\n    df_train = pd.read_csv(\"../input/train.csv\", parse_dates=['timestamp'])\n    df_test = pd.read_csv(\"../input/test.csv\", parse_dates=['timestamp'])\n\n    #-------------------------------------\n    # Note that the following is essential to get good performance:\n    # You can produce these pkl by using the two kernels with 5 fold non-shuffle traning, as we usually did in stacking\n    #-------------------------------------\n    # https://www.kaggle.com/schoolpal/lgbm-lb-0-3093-0-3094\n    # https://www.kaggle.com/schoolpal/modifications-to-reynaldo-s-script\n    \n    # (xgb_train,xgb_test)=pickle.load(open('xgb_predicted.pkl'))\n    # (xgb_train_log,xgb_test_log)=pickle.load(open('xgb_predicted_log.pkl'))\n    # (lgb_train,lgb_test)=pickle.load(open('lgb_predicted.pkl'))\n    \n    # df_train['xgb_score']=xgb_train\n    # df_train['log_xgb_score']=np.log(xgb_train)\n    # df_train['lgb_score']=lgb_train\n    # df_train['lgb_score_log']=np.log(lgb_train)\n    ## df_train['log_xgb_score']=xgb_train_log\n    ## df_train['log_xgb_score_log']=np.log(xgb_train_log)\n    # df_test['xgb_score']=xgb_test\n    # df_test['log_xgb_score']=np.log(xgb_test)\n    # df_test['lgb_score']=lgb_test\n    # df_test['lgb_score_log']=np.log(lgb_test)\n    ## df_test['log_xgb_score']=xgb_test_log\n    ## df_test['log_xgb_score_log']=np.log(xgb_test_log)\n\n    # Magic number from Andy's script (Louis?)\n    df_train['price_doc']*=0.969\n\n    full_sq=df_train.full_sq.copy()\n    full_sq[full_sq<5]=np.NaN\n    \n    price_sq=df_train.price_doc/full_sq\n    #Remove the extreme prices, took from someone's kernel (sry)\n    df_train=df_train[(price_sq<600000) & (price_sq>10000)]\n    price_sq=price_sq[(price_sq<600000) & (price_sq>10000)]\n\n    y_train=df_train.price_doc\n    df_train.drop(['price_doc'],inplace=True,axis=1)\n    num_train=df_train.shape[0]\n    da=pd.concat([df_train,df_test])\n    da=da.reset_index(drop=True)\n    '''\n    The feature enginering part, most of the FE were took from other peole's kernel.\n    last_days method adds the mean of full_sq for all the house sold in last 30 days.\n    This feature was motivated from my autoregression model for monthly prices. What does this feature capture? I tried daily sum of full_sq which clearly indicates the supply and demand. However, the local CV results of monthly price prediction actually prefer mean! I think  this feature somehow captured the supply and demand for luxury or economic properties.\n'''\n    da=last_days(da)\n    # These two features are only necessary as I removed the outlier feature values (> 4 SD) for all features, but these two are important to keep.\n    da['build_year1']=((da['build_year']==1) & (da.product_type=='OwnerOccupier')).astype(int)\n    da['build_year0']=((da['build_year']==0) & (da.product_type=='OwnerOccupier')).astype(int)\n\n    # Fill some missing values based on location (Bhavesh Ghodasara's idea for\n    # identify location)\n    da=fill_years(da)\n    da=fill_maxfloor(da)\n\n    # Not necessary, I just fix it in order to calculate price per square meter for the sample weights\n    da.loc[da['life_sq']<5,'life_sq']=np.NaN\n    da.loc[da['full_sq']<5,'full_sq']=np.NaN\n\n    # 0.7 come from the mean ratio (0.65?) between full_sq and life_sq,0.65 also works\n    da['life_sq']=np.where(da.life_sq.isnull(),da.full_sq*0.7,da.life_sq)\n    da['build_year']=np.where((da.build_year>1690) & (da.build_year<2020),da.build_year,np.NaN)\n    da['max_floor']=np.where(da.max_floor<da.floor,da.floor+1,da.max_floor)\n    da['material']=da['material'].astype(str)\n    da.loc[da.state==33,'state']=3\n\n    to_remove=[]\n    product_types=pd.factorize(da.product_type)[0]\n    product_types_string=da.product_type.copy()\n\n    da['month']=da.timestamp.dt.year.astype(str)\n\n    # The year_month feature was added to nullify  the effect of\n    # \"year_month\" as I set the year_month of the test data to be NaN\n    # I hope to nullify any effect of time. This is equivalent to say that we don't know the time for test data.\n    # Any time effect must be learned from macro feature\n\n    da['year_month']=da.timestamp.dt.year\n    da['year_month']=(da['year_month']*100+da.timestamp.dt.month)\n    da.loc[da['year_month']>201506,'year_month']=np.NaN\n    da['year_month']=da['year_month'].astype(str)\n    \n    df_cat=None\n    for c in da.columns:\n        if da[c].dtype=='object':\n            oh=pd.get_dummies(da[c],prefix=c)\n            \n            if df_cat is None:\n                df_cat=oh\n            else:\n                df_cat=pd.concat([df_cat,oh],axis=1)\n            to_remove.append(c)\n    da.drop(to_remove,inplace=True,axis=1)\n    # Remove rare one hot encoded features\n    to_remove=[]\n    if df_cat is not None:\n        sums=df_cat.sum(axis=0)\n        to_remove=sums[sums<200].index.values\n        df_cat=df_cat.loc[:,df_cat.columns.difference(to_remove)]\n        da = pd.concat([da, df_cat], axis=1)\n    if excluded is not None:\n        for c in excluded:\n            if c in da.columns:\n                da.drop([c],inplace=True,axis=1)\n    # These additional features are taken from\n    # https://www.kaggle.com/wti200/deep-neural-network-for-starters-r\n    da['na_count']=da.isnull().sum(axis=1)\n    da['rel_floor']=da.floor/da.max_floor\n    da['diff_floor']=da.max_floor-da.floor\n    da['rel_kitchen_sq']=da.kitch_sq-da.full_sq\n    da['rel_life_sq']=da.life_sq/da.full_sq\n    da['rel_kitch_life']=da.kitch_sq/da.life_sq\n    da['rel_sq_per_floor']=da.full_sq/da.floor\n    da['diff_life_sq']=da.full_sq-da.life_sq\n    da['building_age']=da.timestamp.dt.year-da.build_year\n    \n    da['new_house_own']=((da['building_age']<=0) & (product_types_string=='OwnerOccupier')).astype(int)\n    da['old_house_own']=((da['building_age']>0) & (product_types_string=='OwnerOccupier')).astype(int)\n    # Macro features, finally!!!\n    # The unemployment info for 2016 was missing. So the unemployment rate were taken from OCED website\n    # The original unemployment data is useful, but OCED's data is better (LB score)\n    # These macro features are selected from my autoregresion time series model\n    # for the monthly mean prices based on the local CV results. \"eurrub\" and \"brent\" for Investment properties, and \"unemployment\" for OwerOccupier. \n    macro_cols=['timestamp','brent','eurrub','unemployment']\n    macro=pd.read_csv('../input/macro.csv',parse_dates=['timestamp'])\n    # Load the OCED unemployment\n    # macro=macro_lib.fix(macro)\n    macro=macro.loc[:,macro_cols]\n    da=da.join(macro.set_index('timestamp'),on='timestamp')\n    da[da==np.inf]=np.NaN\n    if 'index' in da.columns:\n        da.drop(['index'],inplace=True,axis=1)\n    # Give tax-purpose properties a very low sample weights\n    sample_weights=bad_weights(df_train,y_train,price_sq)\n    train=da[:num_train].drop(['timestamp','id'],axis=1)\n    test=da[num_train:].drop(['timestamp','id'],axis=1)\n    # identify the binary features for excluding them from scaling\n    bin_inds=[]\n    for c in train.columns:\n        if train.loc[:,c].unique().shape[0]==2 and train.loc[:,c].unique().sum()==1:\n            bin_inds.append(train.columns.get_loc(c))\n    return train,test,y_train,da[num_train:].id,bin_inds,sample_weights\n\n\ndef norm(train,test,feature_names):\n    all_data=np.vstack((train,test))\n    original=all_data.copy()\n    if len(bin_inds)>0:\n        bin_data=original[:,bin_inds].astype(int)\n        bin_data[np.isnan(bin_data)]=0\n        all_data=np.delete(all_data,bin_inds,axis=1)\n        feature_names=[feature_names[i] for i in range(len(feature_names)) if i not in bin_inds]\n    skip_inds=['xgb_score','xgb_score_log','log_xgb_score','log_xgb_score_log','rf_score','rf_score_log']\n    skip_inds=[feature_names.index(ind) for ind in skip_inds if ind in feature_names]\n\n    # Simple mean imputer, with standard scaler we can make all\n    # NaN value zero, essentially cancel out its effect.\n    imputer=Imputer(strategy='mean',copy=True,axis=0)\n    all_data=imputer.fit_transform(all_data)\n    # Remove all the feature values have SD greater than 4, we will skip\n    # for the price feature as they are critical for our performance\n    STD_LIMIT=4\n    to_remove=[]\n    for ci in range(all_data.shape[1]):\n        if ci in skip_inds:\n            continue\n        dc=all_data[:,ci].copy()\n        mean=np.mean(all_data[:,ci])\n        std=np.std(all_data[:,ci])\n        if std==0:\n            to_remove.append(ci)\n        else:\n            all_data[(dc-mean)/float(std)>STD_LIMIT,ci]=mean\n            all_data[(dc-mean)/float(std)<-STD_LIMIT,ci]=mean\n    # Remove empty feature\n    all_data=np.delete(all_data,to_remove,axis=1)\n\n    train=all_data[0:train.shape[0],:]\n    test=all_data[train.shape[0]:,:]\n\n    scaler=StandardScaler()\n    scaler.fit(np.vstack((train,test)))\n    train=scaler.transform(train)\n    test=scaler.transform(test)\n\n    if len(bin_inds)>0:\n        all_data=np.vstack((train,test))\n        all_data=np.hstack((all_data,bin_data))\n        train=all_data[0:train.shape[0],:]\n        test=all_data[train.shape[0]:,:]\n\n    # I don't want to do instance normalization as it will change\n    # the logic of these noisy features for no reason.  However, as we can't use\n    # BatchNormalization, DNN model converge faster and work better\n    # with normalized data. I did the outlier value removal, so this should has\n    # little impact on the real performance?\n    norm=Normalizer(norm='l2')\n    norm.fit(np.vstack((train,test)))\n    train=norm.transform(train)\n    test=norm.transform(test)\n    return train,test\ndef nn(train,test,y_train,bin_inds,sample_weights,feature_names):\n    train,test=norm(train,test,feature_names)\n    if LOG:\n        y_train=np.log1p(y_train)\n    graph_in=Input(shape=(train.shape[1],),name='feature_data')\n    \n    # I used to have an embedding for product types !\n    # product_type_in=Input(shape=(1,),name='product_type')\n\n    model=Sequential()\n    # Data augumentation using GassianDropout, basicly simulate the\n    # random effect in this dataset :->\n    out=GaussianDropout(0.1)(graph_in)\n    out=Dense(2048)(out)\n    out=Activation('relu')(out)\n    if NORM:\n        # It's a pitty that I cannot use batchnorm, which make things harder to learn\n        out=BatchNormalization()(out)\n    out=Dropout(0.3)(out) \n    out=Dense(1024)(out)\n    if NORM:\n        out=BatchNormalization()(out)\n    out=Activation('relu')(out)\n    out=Dropout(0.3)(out)\n    out=Dense(512)(out)\n    if NORM:\n        out=BatchNormalization()(out)\n    out=Activation('relu')(out)\n    out=Dropout(0.3)(out)\n    out=Dense(1)(out)\n    graph=Model(inputs=[graph_in],outputs=out)\n    print(graph.summary())\n    model.add(graph)\n    # Decaying learning rate\n    lrate=LearningRateScheduler(step_decay)\n    # Use clipnorm to prevent gradient explosion\n    optimizer=SGD(lr=0.0, momentum=0.5,nesterov=True,clipnorm=100)\n    model.compile(loss = 'mse', optimizer = optimizer)\n    # You must set the shuffle to False! as the instances are very dependent!\n    # -------------------------------------------\n    # Note: I used epochs=40 for the real script\n    # ------------------------------------------\n    model.fit({'feature_data':train},y_train,batch_size=16,verbose=1,epochs=1,callbacks=[lrate],shuffle=False,sample_weight=sample_weights)\n    if LOG:\n        predicted=np.expm1(model.predict({'feature_data':test})[:,0])\n    else:\n        predicted=model.predict({'feature_data':test})[:,0]\n\n    return predicted\n\n", "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"collapsed": false, "_cell_guid": "d77352d6-22a1-4e1c-9bda-1470d64f7a36", "_execution_state": "idle", "_uuid": "be401023143c182cc2dc3497adca9bebe7b26f8b", "trusted": false}, "source": "\nif __name__=='__main__':\n    train,test,y_train,id_test,bin_inds,sample_weights=prepare_data()\n    predicted = nn(train.values,test.values,y_train.values,bin_inds,sample_weights,train.columns.tolist())\n    output = pd.DataFrame({'id': id_test, 'price_doc': predicted})\n    output.to_csv('dnn.csv', index=False)", "execution_count": null, "cell_type": "code", "outputs": []}]}