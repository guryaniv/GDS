{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true,"collapsed":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport keras\nfrom sklearn import tree\nfrom keras.layers import Dense\nfrom keras.models import Sequential\nfrom sklearn.ensemble import RandomForestClassifier\nfrom imblearn.over_sampling import RandomOverSampler ","execution_count":77,"outputs":[]},{"metadata":{"_kg_hide-input":false,"_kg_hide-output":true,"trusted":true,"collapsed":true,"_uuid":"38f3da08fbb025e70667b8eb2e713cb505e8bd7d"},"cell_type":"code","source":"np.random.seed(0)\ndata = pd.read_csv(\"../input/creditcard.csv\")","execution_count":78,"outputs":[]},{"metadata":{"_cell_guid":"2e36406e-190d-446a-aedc-e85041b1681c","_uuid":"f6b116be832fd7e76d204a15c37eb857efd46ff5"},"cell_type":"markdown","source":"## Setup\nImporting tools, loading the data, splitting up our data into a train and test set.\nI hid this code, but take a peek if you're curious."},{"metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"trusted":true,"_uuid":"259296ff243c724900cd9c31ab1507ad588e252a","collapsed":true},"cell_type":"code","source":"# This is a function for calculating the F1 Score. \n# A much better way at guaging how well the algorithm \n# is doing than simply by accuracy. \n# Learn more: https://en.wikipedia.org/wiki/F1_score\n# Credit to: https://stackoverflow.com/a/45305384\nfrom keras import backend as K\n\ndef f1(y_true, y_pred):\n    def recall(y_true, y_pred):\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n        recall = true_positives / (possible_positives + K.epsilon())\n        return recall\n    def precision(y_true, y_pred):\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n        precision = true_positives / (predicted_positives + K.epsilon())\n        return precision\n    precision = precision(y_true, y_pred)\n    recall = recall(y_true, y_pred)\n    return 2*((precision*recall)/(precision+recall+K.epsilon()))","execution_count":79,"outputs":[]},{"metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"trusted":true,"collapsed":true,"_uuid":"70fa17ee5c1bbbb19e4f38daecb37a68c06c04df"},"cell_type":"code","source":"# This is a little macro to make a confusion matrix as output, nothing special.\nfrom sklearn.metrics import confusion_matrix\n\ndef evaluate(model, X_val, Y_val, silent=False):\n    predictions = model.predict(X_val)\n    predictions = np.around(predictions).flatten()\n    results = np.equal(Y_val, predictions)\n    acc = float(np.sum(results)/len(results))\n    suspected_fraud = np.nonzero(predictions)[0]\n    real_fraud = np.nonzero(Y_val)[0]\n    fraud_results = results[real_fraud]\n    fraud_acc = float(np.sum(fraud_results)/len(fraud_results))\n    correct_count = np.sum(fraud_results)\n    cm = confusion_matrix(Y_val, predictions)\n    if not silent:\n        print(\"       \\tPREDICTED\")\n        print(\"TRUE  |\\tokay\\tfraud\")\n        print(\"okay  |\\t\"+str(cm[0][0])+\"\\t\"+str(cm[0][1]))\n        print(\"fraud |\\t\"+str(cm[1][0])+\"\\t\"+str(cm[1][1]))\n        print(\"overall accuracy: \"+str(acc))\n        print(\"fraud accuracy:   \"+str(fraud_acc))\n    return fraud_acc","execution_count":80,"outputs":[]},{"metadata":{"_cell_guid":"7f71eefd-9ce0-404c-bd1e-70b9b482014a","_uuid":"bc2cf743eace88e8fb172c90bc7ad3b112acc90d","trusted":true,"_kg_hide-input":true,"_kg_hide-output":false},"cell_type":"code","source":"#shuffling the data\ndata = data.sample(frac=1).reset_index(drop=True)\nfrauds = data[data['Class'] == 1]\nvalidation_portion = 0.15\nvalidation_cutoff_index = int(len(data)*validation_portion)\nvalidation_set = data[:validation_cutoff_index]\ntraining_set = data[validation_cutoff_index:]\n\ntraining_set = training_set.sample(frac=1).reset_index(drop=True)\nvalidation_set = validation_set.sample(frac=1).reset_index(drop=True)\n\nX=training_set.drop(columns=['Class'])\nY=training_set['Class']\nX_val=validation_set.drop(columns=['Class'])\nY_val=validation_set['Class']\n\nprint(\"Training size:   \"+str(len(X)))\nprint(\" fraud count: \"+str(len(Y[Y==1])))\nprint(\"Validation size: \"+str(len(X_val)))\nprint(\" fraud count: \"+str(len(Y_val[Y_val==1])))\n\nlabels = 'Okay', 'Fraud'\nsizes = [len(Y[Y==0]), len(Y[Y==1])]\ncolors = ['green', 'red']\nexplode = (0.1,0)\n# Plot\nplt.pie(sizes, labels=labels,\n        colors=colors,\n       explode=explode)\n \nplt.axis('equal')\nplt.show()","execution_count":81,"outputs":[]},{"metadata":{"_cell_guid":"d0265f99-5d9c-4a02-bd90-04d24f6f1975","_uuid":"4549d5b0f467ba00a9d8def3bec25b3b4af1b49b"},"cell_type":"markdown","source":"Judging from the output above, our data is clearly skewed. Skewed data is one of the worst things to deal with... so let's give it a shot and see what we can do!"},{"metadata":{"_cell_guid":"2c64fb8a-2db6-4727-a921-7a16fc0d0918","_uuid":"04c247590605a7808fb2bbec122bf6ce8ee69904"},"cell_type":"markdown","source":"## Neural Net\nNNs are all the rage these days, so let's throw an NN at this problem and see what happens."},{"metadata":{"_cell_guid":"0200d7a4-e6bd-4127-aab9-a248e6e65003","scrolled":false,"_uuid":"88af4363476f21fa3a2ae88aa3232a3a4c100baf","trusted":true,"_kg_hide-output":true,"collapsed":true},"cell_type":"code","source":"nn_model = Sequential()\nnn_model.add(Dense(100, input_dim=30, activation='relu')) # taking in the 30 inputs\nnn_model.add(Dense(200, activation='relu')) # layer of 200 neurons\nnn_model.add(Dense(200, activation='relu')) # layer of 200 neurons\nnn_model.add(Dense(300, activation='relu')) # layer of 300 neurons\nnn_model.add(Dense(500, activation='sigmoid')) # layer of 500 neurons w/ sigmoid activation\nnn_model.add(Dense(1, activation='sigmoid')) # final layer to say if its \n\nnum_epochs = 3\nbatch_size = 1024\nnn_model.compile(loss='logcosh',\n                 optimizer=keras.optimizers.RMSprop(lr=0.0001), \n                 metrics=[f1, 'accuracy'])\nnn_model.fit(X, Y, epochs=num_epochs, batch_size=batch_size)","execution_count":62,"outputs":[]},{"metadata":{"_cell_guid":"bac07db6-46dd-40cf-bcd8-15a589135163","scrolled":true,"_uuid":"60a1b26bcc8a4c4d108a067af4b1ff45dfc205b7","trusted":true,"collapsed":true},"cell_type":"code","source":"evaluate(nn_model, X_val, Y_val)","execution_count":63,"outputs":[]},{"metadata":{"_cell_guid":"26decf01-123a-48c2-96c8-bb7383fe1250","_uuid":"beabe8c30b103cf514ae836a841d2852eb3c56b1"},"cell_type":"markdown","source":"That was as terrible as it could be. It marked all of them as being non-fraudulent. Additionally, it took quite a while to train, and this wasn't even that much training compared to what the norm is. This would be real bad for a credit card company."},{"metadata":{"_uuid":"68e3f46a4488598c48a90c78e71d55cec8b15999"},"cell_type":"markdown","source":"## Decision Tree\nPerhaps a differnet approach would work better, possibly a decision tree? We'll use the default settings so it'll be a quick estimate of how a DTree can do."},{"metadata":{"_cell_guid":"26257f39-e2a2-4a5f-b6eb-476042f86e38","_uuid":"cc722876e791441313801bc180ba23e2d7d3bc0f","trusted":true,"collapsed":true},"cell_type":"code","source":"tree_model = tree.DecisionTreeClassifier()\ntree_model.fit(X, Y)","execution_count":64,"outputs":[]},{"metadata":{"_uuid":"958dba317f8a4f9b890dcb2cc85a0d986049c27c"},"cell_type":"markdown","source":"Now let's try some predictions."},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"2361540de6e495bbb7a0b02ec79ed3734a7c665e","collapsed":true},"cell_type":"code","source":"evaluate(tree_model, X_val, Y_val)","execution_count":65,"outputs":[]},{"metadata":{"_uuid":"6a5ff20e866545696290545a76ec40850f62f58b"},"cell_type":"markdown","source":"Look at that, it's doing much better. Of the **74** cases of fraud, it got **59** of them correct, but missed **15**. It also incorrectly labeled **18** proper transactions as being fraudulent. However, in this situation, marking an okay transaction as fraud, is much better than letting a fraudulent transaction slip through the cracks.\n\nI think it's worth checking another style of ML to see if that can do any better."},{"metadata":{"trusted":true,"_uuid":"43ce18864f3d4c705f4dd5e7956b83c8f2669af7","_kg_hide-output":true,"scrolled":true,"collapsed":true},"cell_type":"code","source":"rf_model = RandomForestClassifier(max_depth=15,\n                                  warm_start=False,\n                                  n_jobs=-1,\n                                  random_state=0)\nrf_model.fit(X,Y)","execution_count":66,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"49eb4281c49e764f685dd0ca0aed408fbf4e3fca","collapsed":true},"cell_type":"code","source":"evaluate(rf_model, X_val, Y_val)","execution_count":67,"outputs":[]},{"metadata":{"_uuid":"ec255df0337c6386c6293f49bbbb8080081b9dfe"},"cell_type":"markdown","source":"That did one better for classifying fraud, but that's negligible. What really improved though, was classifying the proper transactions, going from **18** to **8**. As mentioned earlier, this metric is less important to correctly identifying fraud, but still an improvement.\n\n### Trying different class weights\nMaybe we can tweak this to work a little better. Let's add in some class weights and see if we can get a better value with that."},{"metadata":{"trusted":true,"_uuid":"94ce2347b7b878d626fee36e4df8623b8495661c","_kg_hide-output":true,"collapsed":true},"cell_type":"code","source":"okay_val=0.5\nfraud_min=0.05\nfraud_max=0.951\nfraud_incr=0.05\nfraud_range=np.arange(fraud_min, fraud_max, fraud_incr)\nresults = []\nfor fraud_weight in tqdm(fraud_range):\n    rf_model = RandomForestClassifier(max_depth=22,\n    #                                 0=okay, 1=fraud\n                                      class_weight={0:okay_val, 1:fraud_weight},\n                                      warm_start=False,\n                                      n_jobs=-1,\n                                      random_state=0)\n    rf_model.fit(X,Y)\n    results.append(evaluate(rf_model, X_val, Y_val, silent=True))","execution_count":69,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"821d0c7c0016e5c294cd897ecab86b932d8f3c33","collapsed":true},"cell_type":"code","source":"fig, ax = plt.subplots()\nax.plot(fraud_range, results)\n\nax.set(xlabel='fraud weight', ylabel='fraud_accuracy', title=\"okay weight = \"+str(okay_val))\nax.grid()\nplt.show()","execution_count":70,"outputs":[]},{"metadata":{"_uuid":"b7d148032cafe142ae01ed6b1f76871f9b39de8d"},"cell_type":"markdown","source":"Seems like a 0.45 paired with a 0.5 value works out best, but there doesn't seem to be a clear answer and the variation can just be chalked up to randomness.\n\nPerhaps we should try some sampeling techniques to get around these issues.\n\n# Random Over-Sampling"},{"metadata":{"trusted":true,"_uuid":"9cd89abd33c95db35adb385cb4c77e0e196d3e94"},"cell_type":"code","source":"ros = RandomOverSampler(random_state=0)\nX_resampled, Y_resampled = ros.fit_sample(X, Y)\n\nprint(\"new size:        \"+str(len(X_resampled)))\nprint(\"new fraud count: \"+str(len(X_resampled[Y_resampled==1])))\n\nlabels = 'Okay', 'Fraud'\nsizes = [len(Y_resampled[Y_resampled==0]), len(Y_resampled[Y_resampled==1])]\ncolors = ['green', 'red']\nexplode = (0.1,0)\n# Plot\nplt.pie(sizes, labels=labels,\n        colors=colors,\n       explode=explode)\n \nplt.axis('equal')\nplt.show()","execution_count":82,"outputs":[]},{"metadata":{"_uuid":"78dbcb98758aaf06e0681ba7dd4acd818f14f6f7"},"cell_type":"markdown","source":"## Neural Network (pt. 2)"},{"metadata":{"trusted":true,"_uuid":"eb8ee283cc39fa9a944625b87d6125d918846bb5"},"cell_type":"code","source":"nn2_model = Sequential()\nnn2_model.add(Dense(100, input_dim=30, activation='relu')) # taking in the 30 inputs\nnn2_model.add(Dense(200, activation='relu')) # layer of 200 neurons\nnn2_model.add(Dense(200, activation='relu')) # layer of 200 neurons\nnn2_model.add(Dense(300, activation='relu')) # layer of 300 neurons\nnn2_model.add(Dense(500, activation='sigmoid')) # layer of 500 neurons w/ sigmoid activation\nnn2_model.add(Dense(1, activation='sigmoid')) # final layer to say if its \n\nnum_epochs = 3\nbatch_size = 1024\nnn2_model.compile(loss='logcosh',\n                 optimizer=keras.optimizers.RMSprop(lr=0.0001), \n                 metrics=[f1, 'accuracy'])\nnn2_model.fit(X_resampled, Y_resampled, epochs=num_epochs, batch_size=batch_size)","execution_count":83,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"35d45efd38ccd7b84015559e101742efff5d2b4c"},"cell_type":"code","source":"evaluate(nn2_model, X_val, Y_val)","execution_count":84,"outputs":[]},{"metadata":{"_uuid":"ae05be3889a5bca5f983087dbcbbc57ec3daeec4"},"cell_type":"markdown","source":"Remember that neural net that got 0% for fraud detection, it just got a near perfect score! This just goes to show how awful skewed data can be.\n\nLet's try some of the other classifiers that did okay even when the data wasn't resampled."},{"metadata":{"trusted":true,"_uuid":"e1c552055a7ba1877924428c783af58706306380","collapsed":true},"cell_type":"code","source":"rf2_model = RandomForestClassifier(max_depth=15,\n                                  warm_start=False,\n                                  n_jobs=-1,\n                                  random_state=0)\nrf2_model.fit(X_resampled, Y_resampled)","execution_count":74,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"169e965623b73063abb0bc3a3a3c03e7dbe48261","collapsed":true},"cell_type":"code","source":"evaluate(rf2_model, X_val, Y_val)","execution_count":75,"outputs":[]},{"metadata":{"_uuid":"d0e29fe1a5aac868b1b2bb879b24c62adc3997b4"},"cell_type":"markdown","source":"Hmm, not as good as the NN, actually pretty similar to what we got last time with the DTree. This makes sense because the decision tree isn't affected by a skewed dataset as severely as NNs are."},{"metadata":{"_uuid":"0bf8e40f838476ef56fdf9c0533c9d1ede649bc1"},"cell_type":"markdown","source":"## Conclusion\nThis dataset was awfully skewed, so horrendously that a neural net went from 0% to 97% just by some simple random over sampling. Let's talk about that. Random over sampling takes the okay transactions and the fraudulent transactions and samples them with replacement so that the classes are balanced. In our case here, that means that the ~400 fraudulent transaction were multiplied into about 240k samples. We can naively assume that each original sample was duplicated about 600 times. This means that the neural network is very likely to be overfitted on those 400 original samples. Tread carefully when using random over sampling."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}