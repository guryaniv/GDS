{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "39aa54f0-51ce-5847-d1aa-f03c064ab21e"
      },
      "source": [
        "### 1. Read Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d8864ede-ffb8-3d2d-768c-8b2a66fed366"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "### Read Training Data and Testing Data\n",
        "train = pd.read_csv('../input/train.csv')\n",
        "test = pd.read_csv('../input/test.csv')\n",
        "train.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "d9d44a39-29ac-f649-517c-70e948b67fed"
      },
      "source": [
        "### 2. Data Pre-Processing\n",
        "    \n",
        "    -Simple Precessing\n",
        "    -Create Dummy Variables\n",
        "    -Normalize Numeical Variables and Run PCA\n",
        "    -Chcek Distribution of Target Variable \"Loss\" "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "116f0fc7-e8a1-89b1-0d93-99e6281140f9"
      },
      "source": [
        "    2.1 Simple Precessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "026ebc28-aa5d-7c06-eb42-d3ae6e6aaaff"
      },
      "outputs": [],
      "source": [
        "### Drop id and Seperate loss\n",
        "train_id = train['id']\n",
        "test_id =  test['id']\n",
        "train_label = train['loss']\n",
        "train.drop('id',axis=1,inplace=True)\n",
        "train.drop('loss',axis=1,inplace=True)\n",
        "test.drop('id',axis=1,inplace=True)\n",
        "print (train.shape, test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "7cd83898-fe5a-d57f-941c-92a065d958a2"
      },
      "source": [
        "    2.2 Create Dummy Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "9584c381-eece-5a1d-d89a-1cfed0716dd0"
      },
      "outputs": [],
      "source": [
        "### Column 0-115(index) are categorical, 116 - 129 are numeric\n",
        "### Create Dummy Variables \n",
        "dummies_train = pd.get_dummies(train[train.columns[0:116]])\n",
        "dummies_test = pd.get_dummies(test[test.columns[0:116]])\n",
        "print(\"The shape of train_new and test_new are: \" + \"%s and %s\" % (dummies_train.shape, dummies_test.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "6ff0aa7e-5316-dfc0-fe87-2be74d4a3796"
      },
      "outputs": [],
      "source": [
        "### Note we have different size for dummies_train and dummies_test!\n",
        "### get the columns in train that are not in test\n",
        "col_to_add = np.setdiff1d(dummies_train.columns, dummies_test.columns)\n",
        "\n",
        "### add these columns to test, setting them equal to zero\n",
        "for c in col_to_add:\n",
        "    dummies_test[c] = 0\n",
        "### select and reorder the test columns using the train columns\n",
        "dummies_test = dummies_test[dummies_train.columns]\n",
        "print(\"The shape of train_new and test_new are: \" + \"%s and %s\" % (dummies_train.shape, dummies_test.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "a96aad51-fb48-11ca-3285-9cd9b0576d68"
      },
      "outputs": [],
      "source": [
        "### Chcek if they have the same columns\n",
        "mismatch = 0\n",
        "for i in range(len(dummies_train.columns)):\n",
        "    if dummies_train.columns[i] != dummies_test.columns[i]:\n",
        "        mismatch += 1\n",
        "print(\"We have %d mismatch.\" % (mismatch))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "3e6af559-d0c5-ded5-baaa-6a2b4a0c4dc2"
      },
      "source": [
        "    2.3. Normalize Numeical Variables and Run PCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "fa8bad73-65a8-6b2d-0282-3c6b3de41cbd"
      },
      "outputs": [],
      "source": [
        "### Normalize numeric variables using Min-Max-Scaler\n",
        "from sklearn import preprocessing\n",
        "numeric_train = train[train.columns[116:129]]\n",
        "numeric_test = test[test.columns[116:129]]\n",
        "min_max_scaler = preprocessing.MinMaxScaler().fit(numeric_train)\n",
        "\n",
        "### Apply Min-Max-Scalar\n",
        "train_norm = min_max_scaler.transform(numeric_train)\n",
        "test_norm = min_max_scaler.transform(numeric_test)\n",
        "\n",
        "### Convert numeric to dataframe \n",
        "train_norm = pd.DataFrame(train_norm,columns=list(numeric_train.columns))\n",
        "test_norm = pd.DataFrame(test_norm,columns=list(numeric_test.columns))\n",
        "\n",
        "print(\"The shape of train_norm and test_norm are: \" + \"%s and %s\" % (train_norm.shape, test_norm.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ffe8d316-8dbe-6102-0b7f-1af3d8d51d4f"
      },
      "outputs": [],
      "source": [
        "### Chcek correlation on numeric variables\n",
        "print(numeric_train.corr())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ca772be2-b02d-9853-2d79-c49b29c77efa"
      },
      "outputs": [],
      "source": [
        "### Run PCA\n",
        "from sklearn import decomposition\n",
        "pca = decomposition.PCA(n_components=6)\n",
        "pca.fit(train_norm)\n",
        "train_norm_pca = pca.transform(train_norm)\n",
        "print(pca.explained_variance_ratio_) \n",
        "test_norm_pca = pca.transform(test_norm)\n",
        "print(pca.explained_variance_ratio_) \n",
        "### Convert numeric to dataframe \n",
        "train_norm_pca = pd.DataFrame(train_norm_pca,columns=[\"PCA1\",\"PCA2\",\"PCA3\",\"PCA4\",\"PCA5\",\"PCA6\"])\n",
        "test_norm_pca = pd.DataFrame(test_norm_pca,columns=[\"PCA1\",\"PCA2\",\"PCA3\",\"PCA4\",\"PCA5\",\"PCA6\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ca66e221-8416-3334-1e4c-1ccc8215a3a5"
      },
      "outputs": [],
      "source": [
        "### Put them back together\n",
        "train_new = pd.concat((dummies_train,train_norm_pca ),axis=1)\n",
        "test_new = pd.concat((dummies_test, test_norm_pca),axis=1)\n",
        "print(\"The shape of train_new and test_new are: \" + \"%s and %s\" % (train_new.shape, test_new.shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "02b3bbc0-8425-8695-fe39-64825fad3bad"
      },
      "source": [
        "    2.4. Chcek Distribution of Target Variable \"Loss\" "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "499d33a2-ee43-8e5f-d40a-36a6d36ce61a"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline  \n",
        "sns.distplot(train_label, color = 'b', hist_kws={'alpha': 0.9}, kde = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "821ea712-2669-1221-8fc3-c6c6ab6232f1"
      },
      "outputs": [],
      "source": [
        "### Try Log(\"loss\") - way better!\n",
        "train_label_log = np.log1p(train_label)\n",
        "sns.distplot(train_label_log, color = 'r', hist_kws={'alpha': 0.9}, kde = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "7b32bda5-e250-a5e7-1724-d4e6c1b1c3bb",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "### Delete redundant variables to free memory\n",
        "del dummies_test, dummies_train, numeric_train, numeric_test, train_norm, test_norm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ce05271b-f066-64f1-629c-a45bff458703",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "### 80% - 20% training and testing split, random_state=50\n",
        "from sklearn.cross_validation import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(train_new, train_label_log, test_size=0.2, random_state=50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "9b741fc9-334d-b0f8-debf-880b41c8f0ef"
      },
      "source": [
        "### Next Step: Feature Selection\n",
        "\n",
        "### Hope it helps!"
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}