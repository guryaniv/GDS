{"nbformat_minor": 1, "nbformat": 4, "cells": [{"cell_type": "markdown", "source": ["* Forked from **Jeremy Howard**'s NB-LR kernel: https://www.kaggle.com/jhoward/nb-svm-baseline-0-06-lb\n", "\n", "## Introduction\n", "\n", "This kernel shows how to use NBSVM (Naive Bayes - Support Vector Machine) to create a strong baseline for the [Toxic Comment Classification Challenge](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge) competition. NBSVM was introduced by Sida Wang and Chris Manning in the paper [Baselines and Bigrams: Simple, Good Sentiment and Topic Classi\ufb01cation](https://nlp.stanford.edu/pubs/sidaw12_simple_sentiment.pdf). In this kernel, we use sklearn's logistic regression, rather than SVM, although in practice the two are nearly identical (sklearn uses the liblinear library behind the scenes).\n"], "metadata": {"_cell_guid": "d3b04218-0413-4e6c-8751-5d8a404d73a9", "_uuid": "0bca9739b82d5d51e1229243e03ea1b6db35c17e"}}, {"cell_type": "code", "source": ["import pandas as pd, numpy as np\n", "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n", "from sklearn.feature_extraction.text import CountVectorizer\n", "from sklearn.model_selection import cross_val_predict\n", "import csv"], "execution_count": null, "outputs": [], "metadata": {"_cell_guid": "ef06cd19-66b6-46bc-bf45-184e12d3f7d4", "_uuid": "cca038ca9424a3f66e10262fc9129de807b5f855", "collapsed": true}}, {"cell_type": "code", "source": ["train = pd.read_csv('../input/train.csv')\n", "test = pd.read_csv('../input/test.csv')\n", "subm = pd.read_csv('../input/sample_submission.csv')"], "execution_count": null, "outputs": [], "metadata": {"_cell_guid": "a494f561-0c2f-4a38-8973-6b60c22da357", "_uuid": "f70ebe669fcf6b434c595cf6fb7a76120bf7809c", "collapsed": true}}, {"cell_type": "markdown", "source": ["## Looking at the data\n", "\n", "The training data contains a row per comment, with an id, the text of the comment, and 6 different labels that we'll try to predict."], "metadata": {"_cell_guid": "3996a226-e1ca-4aa8-b39f-6524d4dadb07", "_uuid": "2c18461316f17d1d323b1959c8eb4e5448e8a44e"}}, {"cell_type": "code", "source": ["train.head()"], "execution_count": null, "outputs": [], "metadata": {"_cell_guid": "5ddb337b-c9b2-4fec-9652-cb26769dc3c6", "scrolled": true, "_uuid": "5f5269c56ea6ded273881b0d4dcdb6af83a3e089"}}, {"cell_type": "markdown", "source": ["ddHere's a couple of examples of comments, one toxic, and one with no labels."], "metadata": {"_cell_guid": "b3b071fb-7a2c-4195-9817-b01983d11c0e", "_uuid": "004d2e823056e98afc5adaac433b7afbfe93b82d"}}, {"cell_type": "markdown", "source": ["The length of the comments varies a lot."], "metadata": {"_cell_guid": "2ea37597-02f7-43cf-ad16-a3d50aac1aba", "_uuid": "5c4c716de98a4b1c2ecc0e516e67813b4fc1473e"}}, {"cell_type": "code", "source": ["lens = train.comment_text.str.len()\n", "lens.mean(), lens.std(), lens.max()"], "execution_count": null, "outputs": [], "metadata": {"_cell_guid": "fd3fe158-4d7f-4b30-ac15-42605240ea4f", "_uuid": "9c1a3f81397199fa250a2b642edc7fbc5f9f504e", "collapsed": true}}, {"cell_type": "code", "source": ["lens.hist();"], "execution_count": null, "outputs": [], "metadata": {"_cell_guid": "d2e55012-4736-425f-84f3-c148ac1f4852", "_uuid": "eb68f1c83a5ad11e652ca5f2150993a06d43edb4", "collapsed": true}}, {"cell_type": "markdown", "source": ["We'll create a list of all the labels to predict, and we'll also create a 'none' label so we can see how many comments have no labels. We can then summarize the dataset."], "metadata": {"_cell_guid": "b8515824-b2dd-4c95-bbf9-dc74c80355db", "_uuid": "0151ab55887071aed82d297acb2c6545ed964c2b"}}, {"cell_type": "code", "source": ["label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n", "train['none'] = 1-train[label_cols].max(axis=1)\n", "train.describe()"], "execution_count": null, "outputs": [], "metadata": {"_cell_guid": "c66f79d1-1d9f-4d94-82c1-8026af198f2a", "_uuid": "4ba6ef86c82f073bf411785d971a694348c3efa9", "collapsed": true}}, {"cell_type": "code", "source": ["# add label col \n", "# https://stackoverflow.com/questions/44464280/mapping-one-hot-encoded-target-values-to-proper-label-names\n", "new_label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate','none']\n", "f, u = pd.factorize(new_label_cols)\n", "y_test  = np.array(\n", "    train[new_label_cols]\n", ")\n", "train[\"target\"]=[', '.join(u[y.astype(bool)]) for y in y_test]\n", "\n", "# train[\"target\"]=\n", "# labels = [', '.join(u[y.astype(int)]) for y in y_test]"], "execution_count": null, "outputs": [], "metadata": {"_cell_guid": "459f047f-3375-4da7-8ec0-0cc290a2d15d", "_uuid": "d0374a4b7fd4dbac31ea2ce1355029898b0913ec", "collapsed": true}}, {"cell_type": "code", "source": ["train.shape"], "execution_count": null, "outputs": [], "metadata": {"_cell_guid": "e416fd59-1588-49f4-88a2-557ab08745d8", "_uuid": "097dde9259217dba58cb0c2abf92cf6885090916", "collapsed": true}}, {"cell_type": "code", "source": ["train.head()"], "execution_count": null, "outputs": [], "metadata": {"_cell_guid": "6309d174-c22b-4d9d-8280-c0acd9aa6219", "_uuid": "70a693111edb1979959351ff05275f0a8d58bf0e", "collapsed": true}}, {"cell_type": "code", "source": ["train[label_cols].max(axis=1).describe()"], "execution_count": null, "outputs": [], "metadata": {"_cell_guid": "68962905-15fc-483e-909e-eb1e58e0098c", "scrolled": true, "_uuid": "8d9f66a506ea3ea1ba643dec16ec750cebc0191a", "collapsed": true}}, {"cell_type": "code", "source": ["len(train),len(test)"], "execution_count": null, "outputs": [], "metadata": {"_cell_guid": "9f6316e3-7e29-431b-abef-73acf4a08637", "_uuid": "b7b0d391248f929a026b16fc38936b7fc0176351", "collapsed": true}}, {"cell_type": "markdown", "source": ["There are a few empty comments that we need to get rid of, otherwise sklearn will complain."], "metadata": {"_cell_guid": "1b221e62-e23f-422a-939d-6747edf2d613", "_uuid": "bfdcf59624717b37ca4ffc0c99d2c28a2d419b06"}}, {"cell_type": "code", "source": ["# COMMENT = 'comment_text'\n", "# train[COMMENT].fillna(\"unknown\", inplace=True)\n", "# test[COMMENT].fillna(\"unknown\", inplace=True)"], "execution_count": null, "outputs": [], "metadata": {"_cell_guid": "fdba531c-7ef2-4967-88e2-fc2b04f6f2ef", "_uuid": "1e1229f403225f1889c7a7b4fc9be90fda818af5", "collapsed": true}}, {"cell_type": "code", "source": ["df = pd.concat([train['comment_text'], test['comment_text']], axis=0)"], "execution_count": null, "outputs": [], "metadata": {"_cell_guid": "dc15b813-6f92-46ee-b775-b681d9f5e832", "_uuid": "6f5dccdadc19b491c7b77737e1cfd885e4b93905", "collapsed": true}}, {"cell_type": "code", "source": ["pd.concat([train, test], axis=0).drop_duplicates(subset='comment_text').drop(\"id\",axis=1).to_csv('toxic_raw_text.csv.gz', index=False,compression=\"gzip\",quoting=csv.QUOTE_ALL)"], "execution_count": null, "outputs": [], "metadata": {"_cell_guid": "fa847b8f-11e5-42d4-b3f6-d27e31f6cb0d", "_uuid": "19bd1b88e2bb88bc16bac5f4dbfbf270cfa69153", "collapsed": true}}, {"cell_type": "code", "source": ["train[[\"id\",'comment_text',\"target\"]].to_csv('train_toxic_raw_v0.csv.gz', index=False,compression=\"gzip\",quoting=csv.QUOTE_ALL)"], "execution_count": null, "outputs": [], "metadata": {"_cell_guid": "0f40bfd5-d6b1-4424-8149-8e03690a9858", "_uuid": "c4906080347e915ae0e8b919aa888f31fde33dea", "collapsed": true}}, {"cell_type": "markdown", "source": ["## Building the model\n", "\n", "We'll start by creating a *bag of words* representation, as a *term document matrix*. We'll use ngrams, as suggested in the NBSVM paper."], "metadata": {"_cell_guid": "480780f1-00c0-4f9a-81e5-fc1932516a80", "_uuid": "f2e77e8e6df5e29b620c7a2a0add1438c35af932"}}, {"cell_type": "code", "source": ["n = train.shape[0]\n", "vec = CountVectorizer(ngram_range=(1,2),min_df=3, max_df=0.97,max_features = 60000) # could also try adding stop word removals, stemming, not lowercasing!\n", "\n", "vec.fit(df.values)\n", "trn_term_doc = vec.transform(train[COMMENT])\n", "test_term_doc = vec.transform(test[COMMENT])\n", "\n", "# trn_term_doc = vec.fit_transform(train[COMMENT])\n", "# test_term_doc = vec.transform(test[COMMENT])"], "execution_count": null, "outputs": [], "metadata": {"_cell_guid": "26952eec-a7fb-4469-af5b-78bf35de5b0e", "_uuid": "6bc08d6ac10871f09ac81adaa92930007d30fcf8", "collapsed": true}}, {"cell_type": "markdown", "source": ["Here's the basic naive bayes feature equation:"], "metadata": {"_cell_guid": "b9eb802a-160c-4781-9782-ab67dcdcb17c", "_uuid": "85c13f3ba3d86a42cb9640016efce7ffd13ea3cc"}}, {"cell_type": "code", "source": ["def pr(y_i, y):\n", "    p = x[y==y_i].sum(0)\n", "    return (p+1) / ((y==y_i).sum()+1)"], "execution_count": null, "outputs": [], "metadata": {"_cell_guid": "45fc6070-ba13-455b-9274-5c2611e2809c", "_uuid": "8b277f01cecd575ed4fcae2e630c0dd8ce979793", "collapsed": true}}, {"cell_type": "markdown", "source": ["We *binarize* the features as discussed in the NBSVM paper."], "metadata": {"_cell_guid": "12341e9e-ab19-44a1-9f10-20718238f85e", "_uuid": "8e165fbbcf3e4b22b4ee23dc4f9b3366613f6546"}}, {"cell_type": "code", "source": ["x=trn_term_doc.sign()\n", "test_x = test_term_doc.sign()"], "execution_count": null, "outputs": [], "metadata": {"_cell_guid": "2299d24b-5515-4d37-92d9-e7f6b16a290a", "_uuid": "926eaa2e40e588f4ef2b86e0a28f8e575c9ed5f4", "collapsed": true}}, {"cell_type": "markdown", "source": ["Fit a model for one dependent at a time:"], "metadata": {"_cell_guid": "92760538-b2df-4568-9b92-f0e7e27114d1", "_uuid": "d8b56c17c4a5fba190395acbe1f024d20f2382b5"}}, {"cell_type": "code", "source": ["def get_mdl(y):\n", "    y = y.values\n", "    r = np.log(pr(1,y) / pr(0,y))\n", "    m = LogisticRegression(C=0.1, dual=True) # ORIG\n", "#     m = LogisticRegressionCV(Cs=12)\n", "    x_nb = x.multiply(r)\n", "    return m.fit(x_nb, y), r"], "execution_count": null, "outputs": [], "metadata": {"_cell_guid": "b756c889-a383-4952-9ee9-eca79fd3454f", "_uuid": "8652ab2f5f84e77fa395252be9b60be1e44fd583", "collapsed": true}}, {"cell_type": "code", "source": ["preds = np.zeros((len(test), len(label_cols)))"], "execution_count": null, "outputs": [], "metadata": {"_cell_guid": "33fd5f8c-adfc-45a1-9fde-1769a0993e76", "_uuid": "0fa103b5406aabdc36ea9ef21612d343e4982fc4", "collapsed": true}}, {"cell_type": "code", "source": ["# for i, j in enumerate(label_cols):\n", "#     print('fit', j)\n", "#     m,r = get_mdl(train[j])\n", "#     preds[:,i] = m.predict_proba(test_x.multiply(r))[:,1]"], "execution_count": null, "outputs": [], "metadata": {"_cell_guid": "9f2dd982-9542-4d37-be77-087c293e3c99", "_uuid": "dfff4b395bad89158d49d6da3878607f284257eb", "collapsed": true}}, {"cell_type": "markdown", "source": ["And finally, create the submission file."], "metadata": {"_cell_guid": "c3f79dcf-a1db-4f81-9429-2c772d10ac76", "_uuid": "6f2b640da9c9cf88a08526ce8ce9a33c6bdc3bd5"}}, {"cell_type": "code", "source": ["# submid = pd.DataFrame({'id': subm[\"id\"]})\n", "# submission = pd.concat([submid, pd.DataFrame(preds, columns = label_cols)], axis=1)\n", "# submission.to_csv('submission.csv', index=False)"], "execution_count": null, "outputs": [], "metadata": {"_cell_guid": "bc6a4575-fbbb-47ea-81ac-91fa702dc194", "_uuid": "5dd033a93e6cf32cdbdaa0a8b05cd8d27de2b21d", "collapsed": true}}, {"cell_type": "code", "source": [], "execution_count": null, "outputs": [], "metadata": {"_cell_guid": "33427980-5316-4b66-bfd5-fcac6d391b8a", "_uuid": "cf33817f680c7c83a94697e7434a34558eb75a1c", "collapsed": true}}], "metadata": {"language_info": {"nbconvert_exporter": "python", "file_extension": ".py", "pygments_lexer": "ipython3", "codemirror_mode": {"name": "ipython", "version": 3}, "name": "python", "version": "3.6.3", "mimetype": "text/x-python"}, "kernelspec": {"name": "python3", "display_name": "Python 3", "language": "python"}}}