{"cells":[{"metadata":{"_uuid":"a009d54438624faa1acfa73e5e9afd7588a06253"},"cell_type":"markdown","source":"# Overview\nThis notebook creates a model from a training dataset of 250 samples of 300 variables with binary labels and generates predictions for a dataset of nearly 20,000 samples."},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"from pathlib import Path\n\nimport numpy as np\nimport pandas as pd\n\nfrom matplotlib import pyplot as plt\nimport seaborn\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a885199405dbecf640e6510c4e6dee63525c4df4"},"cell_type":"markdown","source":"# Load data\nGet the data from the csv's provide by Kaggle (clone using the API command `kaggle competitions download dont-overfit-ii`)."},{"metadata":{"_uuid":"0e518493707e07a40e992d31c33bfb0a0055a258","trusted":true},"cell_type":"code","source":"input_dir = Path('../input')\n\ntrain = pd.read_csv(input_dir / 'train.csv', index_col=0)\n\n# Split targets from inputs.\ntrain_targets = train[['target']].copy()\ntrain_inputs = train.drop('target', axis=1)\n\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bdf60830862948e7156cd98960ef2aebad64ea55","trusted":false},"cell_type":"code","source":"test_inputs = pd.read_csv(input_dir / 'test.csv', index_col=0)\ntest_inputs.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3e361b7b6a7918e8da2226d5f95e4f22f0c51f75","trusted":false},"cell_type":"code","source":"# Check dimensions\ntrain_inputs.shape, test_inputs.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e35ba5c8d23fe1af596e24f9f1f1587f7030876a"},"cell_type":"markdown","source":"# Exploration\nUnderstand the properties of the data and gather information that can be leveraged for modelling.\n\n## Distributions\nLook at the distributions of variables and test for normality."},{"metadata":{"scrolled":false,"_uuid":"949459cb789d2209d7e73abde20de51e96b01a2e","trusted":false},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(15, 10))\n\n# Plot individual histograms.\nfor col in train_inputs:\n    x = train_inputs[col].values \n    freq, bins = np.histogram(x, bins=10)\n    ax.plot(bins[:-1], freq, color='gray', alpha=0.1)\n    \n# Plot normal distribution for shape comparison.\ndef gaussian(x, m=0, s=1, norm=1):\n    return norm * np.exp(-((x - m) ** 2) / s)\n\n# Range for plotting.\nminimum = train_inputs.values.min()\nmaximum = train_inputs.values.max()\nx = np.linspace(minimum, maximum, 10)\n\n# Handpicked values for illustration.\nmean = -0.25\nstdev = 2\nnorm = 55\ny = gaussian(x, m=mean, s=stdev, norm=norm)\nax.plot(x, y, color='r', label='Gaussian(-0.25, 2)')\n\nax.set_xlabel('Value')\nax.set_ylabel('Freq')\nax.set_title('Distribution of training variables')\nax.legend(loc='upper right')\nax.grid()\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"58222281ebd6f8680a2f33c999132bc625160739"},"cell_type":"markdown","source":"A fairly naive approach of putting a Guassian (with some hand picked parameters) over the top seems to get a fairly reasonable approximation for the average shape of the distributions; but, there are clearly quite a few variables where this gives a poor approximation."},{"metadata":{"_uuid":"77f77a8229e3153b66b57524ce5f93965ed4372c","trusted":false},"cell_type":"code","source":"from scipy.stats import shapiro\n\n# Test for normality in each column using Shapiro-Wilks.\nnon_norm_cols = []\nfor col in train_inputs:\n    x = train_inputs[col].values\n    stat, pval = shapiro(x)\n    \n    if pval < 0.05:\n        print('P-val {0:.2f} ==> evidence column {1} not normally distributed'.format(pval, col))\n        non_norm_cols.append(col)\n        ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"61ebd9e92e21c0401d28eaf41f7d3fe886d583ae"},"cell_type":"markdown","source":"So there's reason to believe most of the columns approximately follow a normal distribution. Lets look at the ones with evidence to the contrary."},{"metadata":{"_uuid":"beadc25a894d6c49c0c789c08c4cad076d1a018b","trusted":false},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(15, 8))\ntrain_inputs[non_norm_cols].hist(ax=ax, bins=10);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c429bff4c41395b37a0e25f6a6c6a0e0916c79ea"},"cell_type":"markdown","source":"Just from looking at the plots, it is pretty clear why the distributions returns a significant Shapiro-Wilks. However, since they are not too far away from a normal distribution (i.e. they are generally peaked about a mean and fall off to either side), we shall ignore these for now and consider all of the variables to be approximately Guassian."},{"metadata":{"_uuid":"3abe0bf011d1b59c9067259099b98e1ea73413ec"},"cell_type":"markdown","source":"## Collinearity\nFor completeness, check for linear correlations (Pearson) and plot a heatmap."},{"metadata":{"_uuid":"a90e6f8b68af894ff0789f78054a168528ffbfc6","trusted":false},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(12, 10))\nseaborn.heatmap(train_inputs.corr(), cmap='bwr', vmin=-1, vmax=1, ax=ax)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"45fba7b5328836d58b45d3397db073d49da6dfe3"},"cell_type":"markdown","source":"It is clear there are no strong linear correlations between any of the columns. Thus, it seems that the training inputs represent 250 samples from something like a multivariate Gaussian distribution, where each dimension is independent of the others. In other words, we have 250 samples of 300 iid Gaussian random variables."},{"metadata":{"_uuid":"67cf81ec52d07b559ee30c12172056a607ffb132"},"cell_type":"markdown","source":"## Dimensionality reduction\nThe biggest obstacle to overcome is the high dimensionality relative to the number of samples; it seems sensible to try and reduce the dimensionality if we are to train any type of model given the data we have available.\n\nSee how the total explained variance changes with number of principle components in PCA."},{"metadata":{"_uuid":"d24df36eaad26c282f2a2e92df7a047526e8f987","trusted":false},"cell_type":"code","source":"from sklearn.decomposition import PCA\n\n# Try different numbers of components and plot falloff in variance.\npca = PCA(n_components=200)\nx = train_inputs.values\nx_trans = pca.fit_transform(x)\nn_comps = list(range(0, 200))\nvar = []\n    \nfor n in n_comps:\n    var.append(pca.explained_variance_ratio_[:n].sum())\n    \nfig, ax = plt.subplots(figsize=(10, 5))\nax.plot(n_comps, var)\nax.set_xlabel('N components')\nax.set_ylabel('Sum of explained variance')\nax.set_title('Explained variance versus number of principle components')\nax.grid()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b82a8570689271dd71078230bba85ef7977061dd","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}