{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "5604d949-2583-8d43-82fd-64fffd97024c"
      },
      "source": [
        "The Quora competition is my first competition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "0cf0ab31-d8b2-4924-f045-e78b0d1d0a82"
      },
      "outputs": [],
      "source": [
        "# data analysis\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random as rnd\n",
        "\n",
        "# visualisation\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# machine learning\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC, LinearSVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "48b16f7e-4a63-6fdc-b026-ea51e7c8107a"
      },
      "outputs": [],
      "source": [
        "train_df = pd.read_csv('../input/train.csv')\n",
        "test_df = pd.read_csv('../input/test.csv')\n",
        "combine = [train_df, test_df]\n",
        "train_df[train_df.isnull().values]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "243f4d7e-ee7b-c9e9-ea5c-778d338904d0"
      },
      "outputs": [],
      "source": [
        "train_df = train_df.drop([105780, 201841])\n",
        "train_df[train_df.isnull().values]\n",
        "train_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "80a10333-9c8d-7d42-1124-9ca0181b8f45"
      },
      "source": [
        "For two question pairs there was only one question given. Those question pairs were droped from the training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "cb9e496b-1ea3-20b0-d905-0c6db14ccd5e"
      },
      "outputs": [],
      "source": [
        "test_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "ac9a66e0-0062-752e-0055-6359565d383d"
      },
      "source": [
        "# Normalize the questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "98974f30-f874-29f5-7667-61c0b5eefb51"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.snowball import EnglishStemmer\n",
        "from nltk.stem import RegexpStemmer\n",
        "from nltk.stem import LancasterStemmer\n",
        "\n",
        "import string\n",
        "stops = set(stopwords.words(\"english\"))\n",
        "\n",
        "def normalize_question(question, stopwords_remove=True, stem=True, \n",
        "                       punctuation_remove=True, stem_method='default'):\n",
        "    \n",
        "    # Remove punctuation\n",
        "    if punctuation_remove:\n",
        "        question = question.translate(str.maketrans(\"\",\"\",string.punctuation))\n",
        "    \n",
        "    words = question.lower().split()\n",
        "    # Remove stopwords\n",
        "    if stopwords_remove:\n",
        "        words = [w for w in words if w not in stops]\n",
        "        \n",
        "    # Stem the words\n",
        "    if stem:\n",
        "        if stem_method == 'default':\n",
        "            st = EnglishStemmer()        \n",
        "        elif stem_method == 'snowball':\n",
        "            st = EnglishStemmer()            \n",
        "        elif stem_method == 'lancaster':\n",
        "            st = LancasterStemmer()        \n",
        "        elif stem_method == 'regexp':\n",
        "             st = RegexpStemmer()\n",
        "        else:\n",
        "            print('Enter a valid expression for stem_method.')      \n",
        "\n",
        "        words = [st.stem(w) for w in words]\n",
        "        \n",
        "    return ' '.join(words)\n",
        "\n",
        "\n",
        "def get_word_match(row):\n",
        "    count = 0\n",
        "    set2 = set(row['question2'].split())\n",
        "    for w in set(row['question1'].split()):\n",
        "        if w in set2:\n",
        "            count += 1\n",
        "    return count   \n",
        "\n",
        "\n",
        "def get_combined_word_set_length(row):\n",
        "    # The more equal words two questions have the smaller is the resulting\n",
        "    # set as they do not add up.\n",
        "    return len(set(row['question1'].split()) + set(row['question2'].split()))\n",
        "\n",
        "\n",
        "# Replace negations with antonyms\n",
        "\n",
        "# Run a spellchecker - autocorrection\n",
        "\n",
        "# Lemmatize instead of stemming\n",
        "\n",
        "# Calculate hight information words (tf-itf)\n",
        "\n",
        "# Calculate word similarities using wordnet\n",
        "from nltk.corpus import wordnet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "e66ee028-49b8-9c82-4ddb-34f219427eb6"
      },
      "outputs": [],
      "source": [
        "train_df['question1'] = [normalize_question(q) for q in train_df['question1']]\n",
        "train_df['question2'] = [normalize_question(q) for q in train_df['question2']]\n",
        "train_df['word_match'] = [get_word_match(row) for index, row in train_df.iterrows()]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "88e7b129-748f-a8b1-02d6-4884e2e02783"
      },
      "outputs": [],
      "source": [
        "X_train = train_df.drop([\"is_duplicate\",'id', 'qid1', 'qid2','question1','question2'], axis=1)\n",
        "Y_train = train_df[\"is_duplicate\"]\n",
        "X_test  = test_df.drop(\"test_id\", axis=1).copy()\n",
        "X_train.shape, Y_train.shape, X_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "8ef20bf6-2992-53fa-6a47-ccb0257ed894"
      },
      "outputs": [],
      "source": [
        "# Logistic Regression\n",
        "\n",
        "logreg = LogisticRegression()\n",
        "logreg.fit(X_train, Y_train)\n",
        "#Y_pred = logreg.predict(X_test)\n",
        "acc_log = round(logreg.score(X_train, Y_train) * 100, 2)\n",
        "acc_log"
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}