{"cells":[{"metadata":{"_uuid":"bfe1d8d752d2f79ce6999eb2f8e99004696ba151"},"cell_type":"markdown","source":"## Reference:\n* https://www.kaggle.com/phhasian0710/create-bounding-box-images-whale-recognition\n* https://www.kaggle.com/satian/seresnext101-pytorch-starter\n* https://www.kaggle.com/martinpiotte/bounding-box-model\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"%matplotlib inline\n\nimport os\nimport warnings\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\nfrom matplotlib import pyplot as plt\n\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nfrom torchvision import transforms\n\nfrom PIL import Image\nimport cv2\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\n\n\nwarnings.filterwarnings(\"ignore\",category=DeprecationWarning)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('../input/humpback-whale-identification/train.csv')\nsub = pd.read_csv('../input/humpback-whale-identification/sample_submission.csv')\n\nbest_model_path = '../input/bbox-seresnext101-pytorch-hung96ad/best_model.pth'\n\nTRN_IMGS_DIR = '../input/humpback-whale-identification/train/'\nTST_IMGS_DIR = '../input/humpback-whale-identification/test/'\n\nBBOX_TRAIN_CSV = '../input/box-whale/bounding/bounding_boxes_train.csv'\nBBOX_TEST_CSV = '../input/box-whale/bounding/bounding_boxes_test.csv'\nbbox_train = pd.read_csv(BBOX_TRAIN_CSV)\nbbox_test = pd.read_csv(BBOX_TEST_CSV)\nbbox_train = train_df.join(bbox_train.set_index('Image'), on='Image')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"90a555e56bdb4a5d64783b5242d8e443e5cb5c08"},"cell_type":"markdown","source":"## Creating Labels"},{"metadata":{"trusted":true,"_uuid":"604b0187753fe9ef8d1b50d40427b7ec10c67c0c"},"cell_type":"code","source":"def prepare_labels(y):\n    # From here: https://www.kaggle.com/pestipeti/keras-cnn-starter\n    values = np.array(y)\n    label_encoder = LabelEncoder()\n    integer_encoded = label_encoder.fit_transform(values)\n\n    onehot_encoder = OneHotEncoder(sparse=False)\n    integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n    onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n\n    y = onehot_encoded\n    return y, label_encoder","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5b783ac5c9fe72ab58d2d45a81629e8b398ec377"},"cell_type":"code","source":"y, label_encoder = prepare_labels(train_df['Id'])\nNCLASSES = len(y[0])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bcb4e47f2484a04c0e7625eafa6df8c054ed8052"},"cell_type":"markdown","source":"## Creating Augmentations"},{"metadata":{"trusted":true,"_uuid":"ad9bdb298eb5a1eb23695eb8e00fec0aab9cf4fe"},"cell_type":"code","source":"trn_trnsfms = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomVerticalFlip(),\n    transforms.RandomAffine(degrees=30),\n    transforms.ToTensor(),\n    transforms.Normalize(\n        mean=[0.485, 0.456, 0.406],\n        std=[0.229, 0.224, 0.225]\n    )\n])\n\ntst_trnsfms = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(\n        mean=[0.485, 0.456, 0.406],\n        std=[0.229, 0.224, 0.225]\n    )\n])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a47b91a0a2a166e71fddb069ff30b691a27d080b"},"cell_type":"markdown","source":"## Creating PyTorch Dataloader"},{"metadata":{"trusted":true,"_uuid":"6b52bac58231f6fb304f1d21e360530c6af91995"},"cell_type":"code","source":"class WhaleDataLoader(Dataset):\n    def __init__(self, image_folder, process='train', bbox=None, df=None, transform=None, y=None):\n        self.image_folder = image_folder\n        self.imgs_list = [img for img in os.listdir(image_folder)]\n        self.process = process\n        self.transform = transform\n        self.y = y\n        if self.process == 'train':\n            self.df = df.values\n        self.bbox = bbox.values\n    \n    def __len__(self):\n        return len(self.imgs_list)\n    \n    def __getitem__(self, idx):\n        if self.process == 'train':\n            img_name = os.path.join(self.image_folder, self.df[idx][0])\n            label = self.y[idx]\n        \n        elif self.process == 'test':\n            img_name = os.path.join(self.image_folder, self.imgs_list[idx])\n            label = np.zeros((NCLASSES,))\n        \n        img = Image.open(img_name).convert('RGB')\n        if self.process == 'train':\n            area = (self.bbox[idx][2], self.bbox[idx][3], self.bbox[idx][4], self.bbox[idx][5])\n        elif self.process == 'test':\n            area = (self.bbox[idx][1], self.bbox[idx][2], self.bbox[idx][3], self.bbox[idx][4])\n        img = img.crop(area)\n        \n        img = self.transform(img)\n        if self.process == 'train':\n            return img, label\n        elif self.process == 'test':\n            return img, label, self.imgs_list[idx]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cd40fc8811995ccb1174d05e2f8e4a2e37a5b222"},"cell_type":"code","source":"train_dataloader = WhaleDataLoader(image_folder = TRN_IMGS_DIR, process='train', bbox=bbox_train, df=train_df, transform=trn_trnsfms, y=y)\ntest_dataloader = WhaleDataLoader(image_folder = TST_IMGS_DIR, process='test', bbox=bbox_test,transform=tst_trnsfms)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c37075b224e3f10fc03b83c35221a14065752c8f"},"cell_type":"code","source":"batch_size = 10\nnum_workers = 4\n\ntrain_loader = DataLoader(train_dataloader, batch_size=batch_size, num_workers=num_workers, pin_memory=True, shuffle=True)\ntest_loader = DataLoader(test_dataloader, batch_size=batch_size, num_workers=num_workers, pin_memory=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bb592612d89777f48377531d4a4d5b064b0e4d8d"},"cell_type":"markdown","source":"# Creating SeResnet Model\n\nhttps://github.com/KaiyangZhou/deep-person-reid/blob/master/torchreid/models/senet.py"},{"metadata":{"trusted":true,"_uuid":"bc94f2504fac85e5a8a2c76b2aecd9f40de09c90"},"cell_type":"code","source":"\"\"\"\nResNet code gently borrowed from\nhttps://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py\n\"\"\"\nfrom __future__ import print_function, division, absolute_import\nfrom collections import OrderedDict\nimport math\n\nimport torch.nn as nn\nfrom torch.utils import model_zoo\n\npretrained_settings = {\n    'se_resnext101_32x4d': {\n        'imagenet': {\n            'url': 'http://data.lip6.fr/cadene/pretrainedmodels/se_resnext101_32x4d-3b2fe3d8.pth',\n            'input_space': 'RGB',\n            'input_size': [3, 224, 224],\n            'input_range': [0, 1],\n            'mean': [0.485, 0.456, 0.406],\n            'std': [0.229, 0.224, 0.225],\n            'num_classes': 1000\n        }\n    },\n}\n\n\nclass SEModule(nn.Module):\n\n    def __init__(self, channels, reduction):\n        super(SEModule, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.fc1 = nn.Conv2d(channels, channels // reduction, kernel_size=1,\n                             padding=0)\n        self.relu = nn.ReLU(inplace=True)\n        self.fc2 = nn.Conv2d(channels // reduction, channels, kernel_size=1,\n                             padding=0)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        module_input = x\n        x = self.avg_pool(x)\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        x = self.sigmoid(x)\n        return module_input * x\n\n\nclass Bottleneck(nn.Module):\n    \"\"\"\n    Base class for bottlenecks that implements `forward()` method.\n    \"\"\"\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out = self.se_module(out) + residual\n        out = self.relu(out)\n\n        return out\n\n\nclass SEBottleneck(Bottleneck):\n    \"\"\"\n    Bottleneck for SENet154.\n    \"\"\"\n    expansion = 4\n\n    def __init__(self, inplanes, planes, groups, reduction, stride=1,\n                 downsample=None):\n        super(SEBottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes * 2, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes * 2)\n        self.conv2 = nn.Conv2d(planes * 2, planes * 4, kernel_size=3,\n                               stride=stride, padding=1, groups=groups,\n                               bias=False)\n        self.bn2 = nn.BatchNorm2d(planes * 4)\n        self.conv3 = nn.Conv2d(planes * 4, planes * 4, kernel_size=1,\n                               bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.se_module = SEModule(planes * 4, reduction=reduction)\n        self.downsample = downsample\n        self.stride = stride\n\n\nclass SEResNetBottleneck(Bottleneck):\n    \"\"\"\n    ResNet bottleneck with a Squeeze-and-Excitation module. It follows Caffe\n    implementation and uses `stride=stride` in `conv1` and not in `conv2`\n    (the latter is used in the torchvision implementation of ResNet).\n    \"\"\"\n    expansion = 4\n\n    def __init__(self, inplanes, planes, groups, reduction, stride=1,\n                 downsample=None):\n        super(SEResNetBottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False,\n                               stride=stride)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, padding=1,\n                               groups=groups, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.se_module = SEModule(planes * 4, reduction=reduction)\n        self.downsample = downsample\n        self.stride = stride\n\n\nclass SEResNeXtBottleneck(Bottleneck):\n    \"\"\"\n    ResNeXt bottleneck type C with a Squeeze-and-Excitation module.\n    \"\"\"\n    expansion = 4\n\n    def __init__(self, inplanes, planes, groups, reduction, stride=1,\n                 downsample=None, base_width=4):\n        super(SEResNeXtBottleneck, self).__init__()\n        width = math.floor(planes * (base_width / 64)) * groups\n        self.conv1 = nn.Conv2d(inplanes, width, kernel_size=1, bias=False,\n                               stride=1)\n        self.bn1 = nn.BatchNorm2d(width)\n        self.conv2 = nn.Conv2d(width, width, kernel_size=3, stride=stride,\n                               padding=1, groups=groups, bias=False)\n        self.bn2 = nn.BatchNorm2d(width)\n        self.conv3 = nn.Conv2d(width, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.se_module = SEModule(planes * 4, reduction=reduction)\n        self.downsample = downsample\n        self.stride = stride\n\n\nclass SENet(nn.Module):\n\n    def __init__(self, block, layers, groups, reduction, dropout_p=0.2,\n                 inplanes=128, input_3x3=True, downsample_kernel_size=3,\n                 downsample_padding=1, num_classes=1000):\n        \"\"\"\n        Parameters\n        ----------\n        block (nn.Module): Bottleneck class.\n            - For SENet154: SEBottleneck\n            - For SE-ResNet models: SEResNetBottleneck\n            - For SE-ResNeXt models:  SEResNeXtBottleneck\n        layers (list of ints): Number of residual blocks for 4 layers of the\n            network (layer1...layer4).\n        groups (int): Number of groups for the 3x3 convolution in each\n            bottleneck block.\n            - For SENet154: 64\n            - For SE-ResNet models: 1\n            - For SE-ResNeXt models:  32\n        reduction (int): Reduction ratio for Squeeze-and-Excitation modules.\n            - For all models: 16\n        dropout_p (float or None): Drop probability for the Dropout layer.\n            If `None` the Dropout layer is not used.\n            - For SENet154: 0.2\n            - For SE-ResNet models: None\n            - For SE-ResNeXt models: None\n        inplanes (int):  Number of input channels for layer1.\n            - For SENet154: 128\n            - For SE-ResNet models: 64\n            - For SE-ResNeXt models: 64\n        input_3x3 (bool): If `True`, use three 3x3 convolutions instead of\n            a single 7x7 convolution in layer0.\n            - For SENet154: True\n            - For SE-ResNet models: False\n            - For SE-ResNeXt models: False\n        downsample_kernel_size (int): Kernel size for downsampling convolutions\n            in layer2, layer3 and layer4.\n            - For SENet154: 3\n            - For SE-ResNet models: 1\n            - For SE-ResNeXt models: 1\n        downsample_padding (int): Padding for downsampling convolutions in\n            layer2, layer3 and layer4.\n            - For SENet154: 1\n            - For SE-ResNet models: 0\n            - For SE-ResNeXt models: 0\n        num_classes (int): Number of outputs in `last_linear` layer.\n            - For all models: 1000\n        \"\"\"\n        super(SENet, self).__init__()\n        self.inplanes = inplanes\n        if input_3x3:\n            layer0_modules = [\n                ('conv1', nn.Conv2d(3, 64, 3, stride=2, padding=1,\n                                    bias=False)),\n                ('bn1', nn.BatchNorm2d(64)),\n                ('relu1', nn.ReLU(inplace=True)),\n                ('conv2', nn.Conv2d(64, 64, 3, stride=1, padding=1,\n                                    bias=False)),\n                ('bn2', nn.BatchNorm2d(64)),\n                ('relu2', nn.ReLU(inplace=True)),\n                ('conv3', nn.Conv2d(64, inplanes, 3, stride=1, padding=1,\n                                    bias=False)),\n                ('bn3', nn.BatchNorm2d(inplanes)),\n                ('relu3', nn.ReLU(inplace=True)),\n            ]\n        else:\n            layer0_modules = [\n                ('conv1', nn.Conv2d(3, inplanes, kernel_size=7, stride=2,\n                                    padding=3, bias=False)),\n                ('bn1', nn.BatchNorm2d(inplanes)),\n                ('relu1', nn.ReLU(inplace=True)),\n            ]\n        # To preserve compatibility with Caffe weights `ceil_mode=True`\n        # is used instead of `padding=1`.\n        layer0_modules.append(('pool', nn.MaxPool2d(3, stride=2,\n                                                    ceil_mode=True)))\n        self.layer0 = nn.Sequential(OrderedDict(layer0_modules))\n        self.layer1 = self._make_layer(\n            block,\n            planes=64,\n            blocks=layers[0],\n            groups=groups,\n            reduction=reduction,\n            downsample_kernel_size=1,\n            downsample_padding=0\n        )\n        self.layer2 = self._make_layer(\n            block,\n            planes=128,\n            blocks=layers[1],\n            stride=2,\n            groups=groups,\n            reduction=reduction,\n            downsample_kernel_size=downsample_kernel_size,\n            downsample_padding=downsample_padding\n        )\n        self.layer3 = self._make_layer(\n            block,\n            planes=256,\n            blocks=layers[2],\n            stride=2,\n            groups=groups,\n            reduction=reduction,\n            downsample_kernel_size=downsample_kernel_size,\n            downsample_padding=downsample_padding\n        )\n        self.layer4 = self._make_layer(\n            block,\n            planes=512,\n            blocks=layers[3],\n            stride=2,\n            groups=groups,\n            reduction=reduction,\n            downsample_kernel_size=downsample_kernel_size,\n            downsample_padding=downsample_padding\n        )\n        self.avg_pool = nn.AvgPool2d(7, stride=1)\n        self.dropout = nn.Dropout(dropout_p) if dropout_p is not None else None\n        self.last_linear = nn.Linear(512 * block.expansion, num_classes)\n\n    def _make_layer(self, block, planes, blocks, groups, reduction, stride=1,\n                    downsample_kernel_size=1, downsample_padding=0):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=downsample_kernel_size, stride=stride,\n                          padding=downsample_padding, bias=False),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, groups, reduction, stride,\n                            downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes, groups, reduction))\n\n        return nn.Sequential(*layers)\n\n    def features(self, x):\n        x = self.layer0(x)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        return x\n\n    def logits(self, x):\n        x = self.avg_pool(x)\n        if self.dropout is not None:\n            x = self.dropout(x)\n        x = x.view(x.size(0), -1)\n        x = self.last_linear(x)\n        return x\n\n    def forward(self, x):\n        x = self.features(x)\n        x = self.logits(x)\n        return x\n\n\ndef initialize_pretrained_model(model, num_classes, settings):\n    assert num_classes == settings['num_classes'], \\\n        'num_classes should be {}, but is {}'.format(\n            settings['num_classes'], num_classes)\n    model.load_state_dict(model_zoo.load_url(settings['url']))\n    model.input_space = settings['input_space']\n    model.input_size = settings['input_size']\n    model.input_range = settings['input_range']\n    model.mean = settings['mean']\n    model.std = settings['std']\n\ndef se_resnext101(num_classes=1000, pretrained='imagenet'):\n    model = SENet(SEResNeXtBottleneck, [3, 4, 23, 3], groups=32, reduction=16,\n                  dropout_p=None, inplanes=64, input_3x3=False,\n                  downsample_kernel_size=1, downsample_padding=0,\n                  num_classes=num_classes)\n    if pretrained is not None:\n        settings = pretrained_settings['se_resnext101_32x4d'][pretrained]\n        initialize_pretrained_model(model, num_classes, settings)\n    return model\n\ndef se_resnext101_pretrain(model, optimizer, pretrained='checkpoint.pth'):\n    checkpoint = torch.load(pretrained)\n    start_epoch = checkpoint['epoch']\n    model.load_state_dict(checkpoint['state_dict'])\n    optimizer.load_state_dict(checkpoint['optimizer'])\n    return model, optimizer, start_epoch\n\ndef save_checkpoint(state, is_best, fpath='checkpoint.pth'):\n    torch.save(state, fpath)\n    if is_best:\n        torch.save(state, 'best_model.pth')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0b331f5a8d6fee6e86c3e84d94f71a18701d3202"},"cell_type":"code","source":"model = se_resnext101()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"659b464449d0deb8932cc2dc3de7cd19216d3cde"},"cell_type":"code","source":"model.last_linear = nn.Linear(model.last_linear.in_features, NCLASSES)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"939c1043b420c21cbbf642171be8dd01300ec81b"},"cell_type":"code","source":"model = model.cuda()\ncriterion = nn.BCEWithLogitsLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.0005)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c8f6f86707e14f18851b25a8159219866c5eb1e6"},"cell_type":"markdown","source":" ## Load pretrain"},{"metadata":{"trusted":true,"_uuid":"711ae7644bd8485d7e1f90004cb25ce2de204c46"},"cell_type":"code","source":"model, optimizer, start_epoch = se_resnext101_pretrain(model, optimizer, best_model_path)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"915b829aa1739d50af53e5c4d73f29223d0ac53b"},"cell_type":"code","source":"scheduler = lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"04b495f25d06374b14b08c0e604332ae6b8d28ef"},"cell_type":"markdown","source":"## Predict"},{"metadata":{"trusted":true,"_uuid":"0b1054f578dd04a6100c3cf5d147ab7b5f12ef8a"},"cell_type":"code","source":"n_epochs = 13\nmean_losss = 99\nis_best = False\nfor epoch in range(1, n_epochs+1):\n    train_loss = []\n    \n    for batch_i, (data, target) in tqdm(enumerate(train_loader), total = len(train_loader)):\n        data, target = data.cuda(), target.cuda()\n\n        optimizer.zero_grad()\n        output = model(data)\n        loss = criterion(output, target.float())\n        train_loss.append(loss.item())\n\n        loss.backward()\n        optimizer.step()\n    \n    scheduler.step()\n    if mean_losss > np.mean(train_loss):\n        is_best = True\n        mean_losss = np.mean(train_loss)\n    else:\n        is_best = False\n    save_checkpoint({'epoch': epoch,\n                         'state_dict': model.state_dict(),\n                         'optimizer' : optimizer.state_dict(),\n                        }, is_best)\n    print(f'Epoch {epoch}, train loss: {np.mean(train_loss):.4f}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ea109cc5cea0a8fdc0240b3932e50addf3132657"},"cell_type":"code","source":"model.eval()\nfor (data, target, name) in tqdm(test_loader):\n    data = data.cuda()\n    output = model(data)\n    output = output.cpu().detach().numpy()\n    for i, (e, n) in enumerate(list(zip(output, name))):\n        sub.loc[sub['Image'] == n, 'Id'] = ' '.join(label_encoder.inverse_transform(e.argsort()[-5:][::-1]))\n        \nsub.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}