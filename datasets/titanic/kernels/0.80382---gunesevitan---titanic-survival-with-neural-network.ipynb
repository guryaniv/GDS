{"cells":[{"metadata":{"_uuid":"b92c0af57d3a181c7b72fce0a2e5b20164bd518b"},"cell_type":"markdown","source":"> ## 0. Setup"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"\n>> ## 0.1. Libraries\n* `NumPy` and `pandas` are used for exploratory data analysis in order to summarize the main characteristics of the data, and feature engineering\n* `matplotlib` is used for visualization in order to assist data analysis\n* `sklearn.preprocessing` is used for converting the categorical data into labels and one hot encoding\n* `keras` is used for the neural network"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nfrom matplotlib import rc\n\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler\nfrom tensorflow import keras","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f7dc1cb7b8813f315a8b30fd636b3d46b4270496"},"cell_type":"markdown","source":">> ### 0.2. Loading the data set\nAfter loading the train and test sets to the memory, copying them recursively with the `copy()` function because we don't want changes to be reflected to the original data frame. After that, assigning a name attribute for data frames for later use"},{"metadata":{"trusted":true,"_uuid":"467443fda7135a8ce89c4d537da3f3a8546e2384"},"cell_type":"code","source":"df_train_orig = pd.read_csv('../input/train.csv')\ndf_test_orig = pd.read_csv('../input/test.csv')\n\ndf_train = df_train_orig.copy(deep=True)\ndf_train.name = 'Training set'\ndf_test = df_test_orig.copy(deep=True)\ndf_test.name = 'Test set'\n\nprint('Number of Training Examples = {}'.format(df_train.shape[0]))\nprint('Number of Test Examples = {}'.format(df_test.shape[0]))\nprint('Training Input Shape = {}'.format(df_train.shape))\nprint('Training Output Shape = {}'.format(df_train['Survived'].shape[0]))\nprint('Test Input Shape = {}'.format(df_test.shape))\nprint('Test Output Shape = {}'.format(df_test.shape[0]))\nprint(df_train.columns)\nprint(df_test.columns)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dfabd70ff1cbd50e3107727e5bb630aa59110d83"},"cell_type":"markdown","source":"> ## 1. Data Analysis"},{"metadata":{"_uuid":"ff42f5c4bad84e23f78fa56b9e1a72abf577942d"},"cell_type":"markdown","source":">> ### 1.1. Overview\n* Using `info()` to get an overview of the types of the features\n* Using `sample(10)` to get random 10 rows from the training set"},{"metadata":{"trusted":true,"_uuid":"f02f321f8fd8b8c7c2a4aedb36ebe868ae51004e"},"cell_type":"code","source":"print(df_train_orig.info())\ndf_train_orig.sample(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"851ccf74127831d31ea0d7273b686f9a7cf20eee","scrolled":false},"cell_type":"code","source":"print(df_test_orig.info())\ndf_test_orig.sample(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3db8a853b0b33256f1b08f77d7edf0e9d1737d62"},"cell_type":"markdown","source":">> ### 1.2. Fixing the Missing Values\n* As seen from the random sample, some columns have null values. They have to be fixed but let's see which columns also have null values and how many. The `show_nulls` function below outputs the sum of null values in all columns in both training and test set\n* Training set have null values in `Age`,  `Cabin` and `Embarked` columns\n* Test set have null values in `Age`, `Fare` and `Cabin` columns"},{"metadata":{"trusted":true,"_uuid":"d4e8f7b72e2bd165cafa71d67c95f008e7c6101d"},"cell_type":"code","source":"def show_nulls(df):\n    print('{} columns with null values '.format(df.name))\n    print(df.isnull().sum())\n    print(\"\\n\")\n    \nfor df in [df_train, df_test]:\n    show_nulls(df)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c461e29e325b8a1289c7d6775e65312b5382587b"},"cell_type":"markdown","source":">> The count of missing values in `Age`, `Embarked` and `Fare` columns are relatively smaller compared to the total training or test examples, but more than 80% of the `Cabin` column is missing in both training and test sets. In this case, we fill the missing values of\n* `Age` column with median\n* `Embarked` column with mode since it is categorical\n* `Fare` column with median\n\n>> Even though the large portion of the `Cabin` column is missing, it can't be ignored completely because some cabins might have higher survival rate. The first letter of the cabin data is used as the tiers of the cabins. Only the first letter of the cabin data is kept, and rest of the cabin data isn't important. The missing cabin data is labeled as `X`.\n\n>> Finally `PassengerId` and `Ticket` columns are dropped because they are unique values and they don't have any impact on the survival of an individual."},{"metadata":{"trusted":true,"_uuid":"c22111ad7ed384fb48b6d108b4f38152e1b08e2f"},"cell_type":"code","source":"for df in [df_train, df_test]:    \n    df['Age'].fillna(df['Age'].median(), inplace=True)\n    df['Embarked'].fillna(df['Embarked'].mode()[0], inplace=True)\n    df['Fare'].fillna(df['Fare'].median(), inplace=True)\n    df['Cabin'] = df['Cabin'].apply(lambda x: x[0] if pd.notnull(x) else 'X')\n    \ndf_train.drop(['PassengerId','Ticket'], axis=1, inplace=True)\ndf_test.drop(['PassengerId', 'Ticket'], axis=1, inplace=True)\n\nprint(df_train.columns)\nprint(df_test.columns)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"37b10ee5bff1674227d5608cba7684a23d66c6d7"},"cell_type":"markdown","source":">> Checking the missing values again after filling them and there are no missing values left"},{"metadata":{"trusted":true,"_uuid":"9f5d0c750f29a207cc99317989301707e4300bce"},"cell_type":"code","source":"for df in [df_train, df_test]:\n    show_nulls(df)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"74e329558de204763ab4ae8b8c6c0e0c65c4fae6"},"cell_type":"markdown","source":">> ### 1.3. Interpreting the Cabins Feature\nThe missing values are fixed but `Cabin` feature needs further exploration. There is a connection between the cabin tiers and Pclass (socio-economic status). For example cabin `A`, `B` and `C` have only people from `Pclass 1` (upper class). From going cabin `A` to `X`, people from middle and lower class increases in the cabins. `T` is an exception, an outlier. It might be a king suit or something like that because there is only one person in the whole training and test set in that `T` cabin and that person is from upper class. Instead of creating one more column when it is one-hot encoded, I am dropping the person in the `T` cabin. If that record isn't dropped, training and test data shapes wouldn't match. Most of the people in the `X` are from middle and lower class. Actually `X` is not a cabin, it is the label of missing values. I think people didn't even bother recording their cabin names because they are not from upper class."},{"metadata":{"trusted":true,"_uuid":"b8b4d5e1b1efe85255bdf03a6dcd22d94620e5f4"},"cell_type":"code","source":"t_index = df_train[df_train.loc[:, 'Cabin'] == 'T'].index # Dropping the only row with 'Cabin' column as 'T'\ndf_train.drop(t_index, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"75d010bdefaa197ddeb9758daf850cbb1e8ce5bf"},"cell_type":"markdown","source":" >> First, starting by grouping up `Cabin` and `Pclass` columns. Training set and test set have similar `Pclass` distribution in the cabins, so I don't think model will overfit the data."},{"metadata":{"trusted":true,"_uuid":"4cb1d9e5d3f3b5e60614ad1734aa9e5928108d62"},"cell_type":"code","source":"df_train_cabin_pclass = df_train.groupby(['Cabin', 'Pclass']).count().drop(columns=['Survived', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']).rename(columns={'Name':'Count'})\ndf_train_cabin_pclass = df_train_cabin_pclass.transpose()\nprint('Training set grouped by Cabin and Pclass')\ndf_train_cabin_pclass","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"88a069f452c598c64043948732b2c303c1bb8ebc"},"cell_type":"code","source":"df_test_cabin_pclass = df_test.groupby(['Cabin', 'Pclass']).count().drop(columns=['Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']).rename(columns={'Name':'Count'})\ndf_test_cabin_pclass = df_test_cabin_pclass.transpose()\nprint('Test set grouped by Cabin and Pclass')\ndf_test_cabin_pclass","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1e21a204eeb7e2d7f4ded8134feb4e4ceaab83a8"},"cell_type":"markdown","source":">> Some of the `Cabin` columns doesn't have the value of every `Pclass`. Only the existing `Pclass` values are grouped within `Cabins`. In order to fix that, I created this helper function `get_pclass_counts` which writes `0` to `Pclass` if it doesn't exist in that `Cabin`. This will be useful when building a counter of `Pclass` values inside cabins. Training set and test set `Pclass` counts are displayed below the function definition."},{"metadata":{"trusted":true,"_uuid":"eecc37dbd93644ea1630a644f744151f77be0453"},"cell_type":"code","source":"def get_pclass_counts(df):\n    cabin_names = df.columns.levels[0]\n    cabins = {'A':{}, 'B':{}, 'C':{}, 'D':{}, 'E':{}, 'F':{}, 'G':{}, 'X':{}}\n    \n    for cabin in cabin_names:\n        for pclass in range(1,4):\n            try:\n                count = df[cabin][pclass][0] # Trying to get the count of person in that pclass\n                cabins[cabin][pclass] = count \n            except KeyError:\n                cabins[cabin][pclass] = 0 # If there is no one, assigning it to 0\n    return cabins","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"69b3db828e878a57b29be6c94b98c4244db9639a"},"cell_type":"code","source":"pclass_count_train = get_pclass_counts(df_train_cabin_pclass)\npclass_count_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ef00837b6e7ab8ab17f0c3996e0b72649c838221"},"cell_type":"code","source":"pclass_count_test = get_pclass_counts(df_test_cabin_pclass)\npclass_count_test","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2c005573f09dbbf6284ba9ba5dbba036e4d03a93"},"cell_type":"markdown","source":">> Then, I made this helper function `get_pclass_percentages` for converting counts into percentages for visualization. It basically divides every count to sum and multiplies it with 100. Training set and test set `Pclass` percentages in the cabins are displayed below the function definition."},{"metadata":{"trusted":true,"_uuid":"47229f371b92eca3528d2a63f3d362551d843c61"},"cell_type":"code","source":"def get_pclass_percentages(pclass_count):\n    df_pclass_count = pd.DataFrame(pclass_count)\n    \n    percentages = {}\n\n    for col in df_pclass_count.columns:\n        percentages[col] = [(count / df_pclass_count[col].sum()) * 100 for count in df_pclass_count[col]] # Dividing count by sum and multiplying with 100\n\n    return percentages","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"37729580db72f44789ab26220caf82d000677d46"},"cell_type":"code","source":"pclass_per_train = get_pclass_percentages(pclass_count_train)\npclass_per_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"563265032ca46ea3b4aee87bc5b4576f37806728"},"cell_type":"code","source":"pclass_per_test = get_pclass_percentages(pclass_count_test)\npclass_per_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8a3c666124812e61f5bd877baff9e487193a5ccf"},"cell_type":"code","source":"def plot_pclass_per(pclass_percentages):\n    df_pclass_percentages = pd.DataFrame(pclass_percentages).transpose()\n\n    bar_count = np.arange(8)  \n\n    bar_width = 0.85\n    cabin_names = ('A', 'B', 'C', 'D', 'E', 'F', 'G', 'X')\n\n    pclass1 = df_pclass_percentages[0]\n    pclass2 = df_pclass_percentages[1]\n    pclass3 = df_pclass_percentages[2]\n\n    plt.bar(bar_count, pclass1, color='#b5ffb9', edgecolor='white', width=bar_width, label=\"Pclass 1\")\n    plt.bar(bar_count, pclass2, bottom=pclass1, color='#f9bc86', edgecolor='white', width=bar_width, label=\"Pclass 2\")\n    plt.bar(bar_count, pclass3, bottom=pclass1 + pclass2, color='#a3acff', edgecolor='white', width=bar_width, label=\"Pclass 3\")\n\n    plt.xticks(bar_count, cabin_names)\n    plt.xlabel('Cabins')\n    plt.ylabel('Percentages')\n\n    plt.legend(loc='upper left', bbox_to_anchor=(1,1), ncol=1)\n    plt.title('Percentages of Pclass in Cabins')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f9cb287a82639a80484b265b53c9d32148a41986"},"cell_type":"markdown","source":">> Visualizing the percentages of `Pclass` inside cabins clearly illustrates that half of the cabins are mostly occupied by `Pclass 1` (high class). However, it doesn't necessarily mean that those cabins have higher survival rate though. Actually they might even sunk first before other cabins. That's why we also have to check survival rates by cabins."},{"metadata":{"trusted":true,"_uuid":"993793723107b4217ffcace4664bfff226e6d73b"},"cell_type":"code","source":"plot_pclass_per(pclass_per_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"98aaa074a5d55e69a961c9c1ce90b288abe8411a"},"cell_type":"code","source":"plot_pclass_per(pclass_per_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3379d2e79ecae2310868f029e362a738bc5495b1"},"cell_type":"markdown","source":" >> This time the `Cabin` and `Survived` columns are grouped. It can done only for training set because the test set doesn't have the `Survived` feature. This is the same process done for `Cabin` and `Pclass` features. First the count of individuals who have survived and not survived are displayed for every cabin."},{"metadata":{"trusted":true,"_uuid":"8e6d54febf20bb8a09a982d7b1585147dd5af3cb"},"cell_type":"code","source":"df_train_cabin_survived = df_train.groupby(['Cabin', 'Survived']).count().drop(columns=['Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked', 'Pclass']).rename(columns={'Name':'Count'})\ndf_train_cabin_survived = df_train_cabin_survived.transpose()\nprint('Training set grouped by Cabin and Survived')\ndf_train_cabin_survived","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c4597712e4d6339b08d442bdb3ab1e79cb08535e"},"cell_type":"code","source":"cabin_names = df_train_cabin_survived.columns.levels[0]\ncabin_survived = {'A':{}, 'B':{}, 'C':{}, 'D':{}, 'E':{}, 'F':{}, 'G':{}, 'X':{}}\n\nfor cabin in cabin_names:\n    for survive in range(0,2):\n        cabin_survived[cabin][survive] = df_train_cabin_survived[cabin][survive][0]\n        \ncabin_survived","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"321ba04909e053b1e38de461d0aefafbd9dc7719"},"cell_type":"code","source":"df_cabin_survived = pd.DataFrame(cabin_survived)\ndf_cabin_survived","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b3b54c521fb05736c01e638704c8156539b54c58"},"cell_type":"code","source":"survived_percentages = {}\ndf_cabin_survived = pd.DataFrame(cabin_survived)\n\nfor col in df_cabin_survived.columns:\n    survived_percentages[col] = [(count / df_cabin_survived[col].sum()) * 100 for count in df_cabin_survived[col]] # Dividing count by sum and multiplying with 100\n\nsurvived_percentages","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bc1f41eb26d6835a083b5a0caaeaaaff3ed85965"},"cell_type":"markdown","source":">> Looks like Cabin `B C D E` has the highest survival rate. Those cabins are occupied mostly by the upper class. `Cabin X` (missing cabin data) has the lowest survival rate which is mostly lower and middle class. To conclude cabins used by upper class individuals have higher survival rate than cabins used by lower and middle class individuals."},{"metadata":{"trusted":true,"_uuid":"54d8164a22f3580e6b6592248db829696f67cba7"},"cell_type":"code","source":"df_survived_percentages = pd.DataFrame(survived_percentages).transpose()\n\nbar_count = np.arange(8)  \n\nbar_width = 0.85\ncabin_names = ('A', 'B', 'C', 'D', 'E', 'F', 'G', 'X')\n\nnot_survived = df_survived_percentages[0]\nsurvived = df_survived_percentages[1]\n\nplt.bar(bar_count, not_survived, color='#b5ffb9', edgecolor='white', width=bar_width, label=\"Not Survived\")\nplt.bar(bar_count, survived, bottom=not_survived, color='#f9bc86', edgecolor='white', width=bar_width, label=\"Survived\")\n\nplt.xticks(bar_count, cabin_names)\nplt.xlabel('Cabins')\nplt.ylabel('Percentages')\n\nplt.legend(loc='upper left', bbox_to_anchor=(1,1), ncol=1)\nplt.title('Percentages of Survival in Cabins')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"83ed6dacdc116214381364c2b456a97253e708ef"},"cell_type":"markdown","source":">> ### 1.4. Checking the Distribution of Data\nThe output classes are not equally distributed, but the gap is not that big, so the bias is not significant. We don't need to balance the distribution in this case."},{"metadata":{"trusted":true,"_uuid":"c70aa13b7a552beb976574d52c1cd3da1cc1ee5c"},"cell_type":"code","source":"df_survive = df_train_orig['Survived'].value_counts()\nprint(df_survive)\nax = df_survive.plot.bar()\nax.set_xticklabels(('Not Survived', 'Survived'))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e14506a365afef0af44894e46642acf27ac2545f"},"cell_type":"markdown","source":">> ### 1.5. Feature Engineering\n* `Family_Members` is created by adding `SibSp`, `Parch` and `1`. Since we know that `SibSp` is siblings and spouse, and `Parch` is parents and children, we can add those columns to find the count of family members of the person. Finally, adding `1` is the person himself or herself\n* `Is_Alone` column is based on the number of `Family_Members`. If `Family_Members` is more than `1`, `Is_Alone` is set to `0`, otherwise it is set to `1`\n* `Title` column is created by extracting prefix before the `Name` column"},{"metadata":{"trusted":true,"_uuid":"6f9a5bbcd4008db177ab3b24c4b7c28e434790ce"},"cell_type":"code","source":"for df in [df_train, df_test]:    \n    df['Family_Members'] = df['SibSp'] + df['Parch'] + 1\n    \n    df['Is_Alone'] = 1\n    df['Is_Alone'].loc[df['Family_Members'] > 1] = 0\n    \n    df['Title'] = df['Name'].str.split(\", \", expand=True)[1].str.split(\".\", expand=True)[0]\n\ndf_train.sample(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a0abce3337d396a30f273fa75f246ea56bed823f"},"cell_type":"markdown","source":">> Since the `Title` column is categorical, we can group up some values to a big one. Titles like Master and Dr might have a higher priority at the evacuation, so this feature might be worth exploring. We are going group Titles that are coming after Dr to Other because their titles are not as significant as others I think."},{"metadata":{"trusted":true,"_uuid":"f4b2280c8640b9d7a8c2950eb879d6f26e45e91f"},"cell_type":"code","source":"df_train['Title'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"51700bdcea852e5c3bb29afe38ef15a54d8da44b"},"cell_type":"code","source":"df_test['Title'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b9b6a2b4be3911bcb538526d7e9d3d0e85663b6a"},"cell_type":"markdown","source":">> Titles that are less than 10, are grouped into `Other`."},{"metadata":{"trusted":true,"_uuid":"1379d7f0971d4864c6c995a07e340a3aebda98b9"},"cell_type":"code","source":"train_title_names = (df_train['Title'].value_counts() < 10)\ndf_train['Title'] = df_train['Title'].apply(lambda x: 'Other' if train_title_names.loc[x] == True else x)\n\ndf_train['Title'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bb13269de1f17c93066b15ae00f7c99039b31d3f"},"cell_type":"code","source":"test_title_names = (df_test['Title'].value_counts() < 10)\ndf_test['Title'] = df_test['Title'].apply(lambda x: 'Other' if test_title_names.loc[x] == True else x)\n\ndf_test['Title'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b39a06d42b89d64f59f51a21dabdc5fe27fdcdaa"},"cell_type":"markdown","source":">> ### 1.6. Categorical to Dummy\nCategorical data are transformed to numerical data with the `LabelEncoder()` from `sklearn.preprocessing`. It basically labels the categories from 0 to n."},{"metadata":{"trusted":true,"_uuid":"ace8595ade690454f515cf4d1279c0773d07e1e8"},"cell_type":"code","source":"df_train.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"43c4fbb6b0c35005ff6a3c0127dbb574350c7b62"},"cell_type":"code","source":"le = LabelEncoder()\nfor df in [df_train, df_test]:\n    df['Pclass'] = le.fit_transform(df['Pclass'])\n    df['Sex'] = le.fit_transform(df['Sex'])\n    df['Cabin'] = le.fit_transform(df['Cabin'])\n    df['Embarked'] = le.fit_transform(df['Embarked'])\n    df['Title'] = le.fit_transform(df['Title'])\n    \ndf_train.head(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7ac42fd925af8595788c4ebc89997b54aad202fa"},"cell_type":"markdown","source":">> The categorical columns (`Pclass`, `Sex`, `Cabin`, `Embarked`, `Title`) are converted to one-hot encoding with `get_dummies()` function then the previous categorical columns are dropped. Column names are reorganized."},{"metadata":{"trusted":true,"_uuid":"8674455d9133b79468abf5140237fd18ccd0bfc3"},"cell_type":"code","source":"df_train_dummy = pd.concat([df_train, pd.get_dummies(df_train['Pclass'])], axis=1)\ndf_train_dummy = pd.concat([df_train_dummy, pd.get_dummies(df_train['Sex'])], axis=1)\ndf_train_dummy = pd.concat([df_train_dummy, pd.get_dummies(df_train['Cabin'])], axis=1)\ndf_train_dummy = pd.concat([df_train_dummy, pd.get_dummies(df_train['Embarked'])], axis=1)\ndf_train_dummy = pd.concat([df_train_dummy, pd.get_dummies(df_train['Title'])], axis=1)\n\ndf_train_dummy.drop(columns=['Pclass', 'Sex', 'Embarked', 'Title', 'Name', 'Cabin'], inplace=True)\ndf_train_dummy.columns = ('Survived', 'Age', 'SibSp', 'Parch', 'Fare', 'Family_Members', 'Is_Alone',\n                         'Pclass_1', 'Pclass_2', 'Pclass_3', 'Female', 'Male', 'Cabin_A', 'Cabin_B', 'Cabin_C',\n                          'Cabin_D', 'Cabin_E', 'Cabin_F', 'Cabin_G', 'Cabin_X', 'Embarked_C', 'Embarked_Q', 'Embarked_S',\n                         'Title_Master', 'Title_Miss', 'Title_Mr', 'Title_Mrs', 'Title_Other',)\n   \ndf_train_dummy.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c6b24b7005a1ba53910f25dda1e05bb45cf82597"},"cell_type":"code","source":"df_test_dummy = pd.concat([df_test, pd.get_dummies(df_test['Pclass'])], axis=1)\ndf_test_dummy = pd.concat([df_test_dummy, pd.get_dummies(df_test['Sex'])], axis=1)\ndf_test_dummy = pd.concat([df_test_dummy, pd.get_dummies(df_test_dummy['Cabin'])], axis=1)\ndf_test_dummy = pd.concat([df_test_dummy, pd.get_dummies(df_test['Embarked'])], axis=1)\ndf_test_dummy = pd.concat([df_test_dummy, pd.get_dummies(df_test['Title'])], axis=1)\n\ndf_test_dummy.drop(columns=['Pclass', 'Sex', 'Embarked', 'Title', 'Name', 'Cabin'], inplace=True)\ndf_test_dummy.columns = ('Age', 'SibSp', 'Parch', 'Fare', 'Family_Members', 'Is_Alone',\n                         'Pclass_1', 'Pclass_2', 'Pclass_3', 'Female', 'Male', 'Cabin_A', 'Cabin_B', 'Cabin_C',\n                          'Cabin_D', 'Cabin_E', 'Cabin_F', 'Cabin_G', 'Cabin_X', 'Embarked_C', 'Embarked_Q', 'Embarked_S',\n                         'Title_Master', 'Title_Miss', 'Title_Mr', 'Title_Mrs', 'Title_Other',)\n\ndf_test_dummy.head(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ddd9825aa83611bcd7b4c4ec35bfce6ee67593f1"},"cell_type":"markdown","source":">> ### 1.7. Normalizing the Continous Data\nThe range of continous data is too wide, so we have to normalize them. There are many ways to normalize data. I did std normalization."},{"metadata":{"trusted":true,"_uuid":"c924d828b4645eff118d3370ed8e7a34b0aff581"},"cell_type":"code","source":"for df in [df_train_dummy, df_test_dummy]:\n    df['SibSp'] = (df['SibSp'] - df['SibSp'].mean()) / df['SibSp'].std()\n    df['Parch'] = (df['Parch'] - df['Parch'].mean()) / df['Parch'].std()\n    df['Family_Members'] = (df['Family_Members'] - df['Family_Members'].mean()) / df['Family_Members'].std()\n    df['Age'] = (df['Age'] - df['Age'].mean()) / df['Age'].std()\n    df['Fare'] = (df['Fare'] - df['Fare'].mean()) / df['Fare'].std()\n\ndf_train_dummy.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"99853e0e888be72cdc4dd134b1c2d846b23d0f75"},"cell_type":"code","source":"df_test_dummy.head(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1bf5feab77d0c69c02121edd8783a4131a59fe97"},"cell_type":"markdown","source":">> ### 1.8. Separating X and Y\n* The data is finally ready for training. The input (X) and output (Y) are separated here. `X_train` is basically all the columns except `Survived` since it is the output. `Y_train` is the `Survived` column. `df_test_dummy` doesn't need to be separated because it doesn't have`Survived` column anyway."},{"metadata":{"trusted":true,"_uuid":"88fd7a7dd04aea5147698cfeef26f90ebfda19b6"},"cell_type":"code","source":"X_train = df_train_dummy.drop(['Survived'], axis=1)\nY_train = df_train_dummy['Survived']\n\nX_train[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3cdaf7762bc32db92a691e94c890f2aeb102a349"},"cell_type":"code","source":"Y_train[:5]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1b3d47212b52113b53a3e6d0d0a4ee08aa97b02f"},"cell_type":"markdown","source":"> ## 2. Machine Learning (Neural Network)"},{"metadata":{"_uuid":"065e7bdaea84d248a9103e5bca8a63fb66768fea"},"cell_type":"markdown","source":">> ### 2.1 Neural Network\n* Using relu activation function on the hidden layers\n* The activation function of the last hidden layer is sigmoid because it is a binary classification problem"},{"metadata":{"trusted":true,"_uuid":"ee83da060c9330cf90f0287ad5cd75f0a950f8da"},"cell_type":"code","source":"model = keras.models.Sequential([\n    keras.layers.Dense(32, activation='relu', input_dim=27),\n    keras.layers.Dense(64, activation='relu'),\n    keras.layers.Dropout(0.25),\n    keras.layers.Dense(64, activation='relu'),\n    keras.layers.Dropout(0.25),\n    keras.layers.Dense(64, activation='relu'),\n    keras.layers.Dropout(0.25),\n    keras.layers.Dense(64, activation='relu'),\n    keras.layers.Dropout(0.25),\n    keras.layers.Dense(16, activation='relu'),\n    keras.layers.Dense(1, activation='sigmoid')\n])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b992f47393c7db42f644e1601e116a9c24af68a9"},"cell_type":"markdown","source":">> ### 2.2 Optimizer, Loss Function, Metrics and Callbacks\n* The optimizer is stochastic gradient descent with default parameters\n* The loss function is binary cross-entropy\n* Using accuracy for the metric\n* Creating a callback function which reduces the learning rate, If accuracy doesn't increase in 3 epochs."},{"metadata":{"trusted":true,"_uuid":"efb99dcb4ad1aa1c4e8229e41a9e2c2411009078"},"cell_type":"code","source":"optimizer = keras.optimizers.SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False)\nloss = 'binary_crossentropy'\nmetrics = ['accuracy']\n\nlearning_rate_reduction = keras.callbacks.ReduceLROnPlateau(monitor='val_acc', factor=0.5, patience=3, verbose=1, mode='auto', min_delta=0.0001, cooldown=0, min_lr=0)\n\nmodel.compile(optimizer=optimizer, loss=loss, metrics=metrics)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"652da20ccf8fb1b28b0f0c67172b67521243c156"},"cell_type":"code","source":"epochs = 50\nbatch_size = 8\n\nmodel.fit(X_train, Y_train, \n          epochs=epochs, \n          batch_size=batch_size, \n          callbacks=[learning_rate_reduction], \n          validation_data=(X_train, Y_train))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4c5bdc55e58b800165b000346caf8046919de772"},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8154ae17f15be72790a702ca957d6bbf40029705"},"cell_type":"markdown","source":"> ## 3. Result"},{"metadata":{"_uuid":"f75cdb159581e7359408d74ac71850298b0af692"},"cell_type":"markdown","source":">> ### 3.1 Predicting with the Trained Model\n* Predicting the classes of `X_test` with the model trained earlier"},{"metadata":{"trusted":true,"_uuid":"7d554b442c3d4364e2cee6ac02ca86aa88586a09"},"cell_type":"code","source":"Y_hat = model.predict_classes(df_test_dummy, batch_size=None, verbose=0)\nY_hat.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d2a60f244c010ea0dda17043504f48a51c8e3e6e"},"cell_type":"markdown","source":"> ## 4. Submission"},{"metadata":{"trusted":true,"_uuid":"e78fb513f18dc651e69e981e886b0d5445e4c4c4"},"cell_type":"code","source":"submission_df = pd.DataFrame(columns=['PassengerId', 'Survived'])\nsubmission_df['PassengerId'] = df_test_orig['PassengerId']\nsubmission_df['Survived'] = Y_hat","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"475a78f1720783453f5b16303c20f7ca365a00a4"},"cell_type":"code","source":"submission_df.head(30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d101d99c48a138ddb2f5012f03893eea30581d5a"},"cell_type":"code","source":"submission_df.to_csv('submissions.csv', header=True, index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c95d3a89555f073959769fc7e3dff6517b51e687"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}