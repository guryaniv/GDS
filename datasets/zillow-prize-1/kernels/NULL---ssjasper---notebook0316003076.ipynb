{"nbformat": 4, "cells": [{"source": ["import numpy as np # linear algebra\n", "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n", "from catboost import CatBoostRegressor\n", "from tqdm import tqdm\n", "import gc\n", "import datetime as dt\n", "\n", "print('Loading Properties ...')\n", "properties2016 = pd.read_csv('../input/properties_2016.csv', low_memory = False)\n", "properties2017 = pd.read_csv('../input/properties_2017.csv', low_memory = False)\n", "\n", "print('Loading Train ...')\n", "train2016 = pd.read_csv('../input/train_2016_v2.csv', parse_dates=['transactiondate'], low_memory=False)\n", "train2017 = pd.read_csv('../input/train_2017.csv', parse_dates=['transactiondate'], low_memory=False)\n", "\n", "def add_date_features(df):\n", "    df[\"transaction_year\"] = df[\"transactiondate\"].dt.year\n", "    df[\"transaction_month\"] = (df[\"transactiondate\"].dt.year - 2016)*12 + df[\"transactiondate\"].dt.month\n", "    df[\"transaction_day\"] = df[\"transactiondate\"].dt.day\n", "    df[\"transaction_quarter\"] = (df[\"transactiondate\"].dt.year - 2016)*4 +df[\"transactiondate\"].dt.quarter\n", "    df.drop([\"transactiondate\"], inplace=True, axis=1)\n", "    return df\n", "\n", "train2016 = add_date_features(train2016)\n", "train2017 = add_date_features(train2017)\n", "\n", "print('Loading Sample ...')\n", "sample_submission = pd.read_csv('../input/sample_submission.csv', low_memory = False)\n", "\n", "print('Merge Train with Properties ...')\n", "train2016 = pd.merge(train2016, properties2016, how = 'left', on = 'parcelid')\n", "train2017 = pd.merge(train2017, properties2017, how = 'left', on = 'parcelid')\n", "\n", "print('Tax Features 2017  ...')\n", "train2017.iloc[:, train2017.columns.str.startswith('tax')] = np.nan\n", "\n", "print('Concat Train 2016 & 2017 ...')\n", "train_df = pd.concat([train2016, train2017], axis = 0)\n", "test_df = pd.merge(sample_submission[['ParcelId']], properties2016.rename(columns = {'parcelid': 'ParcelId'}), how = 'left', on = 'ParcelId')\n", "\n", "del properties2016, properties2017, train2016, train2017\n", "gc.collect();\n", "\n", "print('Remove missing data fields ...')\n", "\n", "missing_perc_thresh = 0.98\n", "exclude_missing = []\n", "num_rows = train_df.shape[0]\n", "for c in train_df.columns:\n", "    num_missing = train_df[c].isnull().sum()\n", "    if num_missing == 0:\n", "        continue\n", "    missing_frac = num_missing / float(num_rows)\n", "    if missing_frac > missing_perc_thresh:\n", "        exclude_missing.append(c)\n", "print(\"We exclude: %s\" % len(exclude_missing))\n", "\n", "del num_rows, missing_perc_thresh\n", "gc.collect();\n", "\n", "print (\"Remove features with one unique value !!\")\n", "exclude_unique = []\n", "for c in train_df.columns:\n", "    num_uniques = len(train_df[c].unique())\n", "    if train_df[c].isnull().sum() != 0:\n", "        num_uniques -= 1\n", "    if num_uniques == 1:\n", "        exclude_unique.append(c)\n", "print(\"We exclude: %s\" % len(exclude_unique))\n", "\n", "print (\"Define training features !!\")\n", "exclude_other = ['parcelid', 'logerror','propertyzoningdesc']\n", "train_features = []\n", "for c in train_df.columns:\n", "    if c not in exclude_missing \\\n", "       and c not in exclude_other and c not in exclude_unique:\n", "        train_features.append(c)\n", "print(\"We use these for training: %s\" % len(train_features))\n", "\n", "print (\"Define categorial features !!\")\n", "cat_feature_inds = []\n", "cat_unique_thresh = 1000\n", "for i, c in enumerate(train_features):\n", "    num_uniques = len(train_df[c].unique())\n", "    if num_uniques < cat_unique_thresh \\\n", "       and not 'sqft' in c \\\n", "       and not 'cnt' in c \\\n", "       and not 'nbr' in c \\\n", "       and not 'number' in c:\n", "        cat_feature_inds.append(i)\n", "        \n", "print(\"Cat features are: %s\" % [train_features[ind] for ind in cat_feature_inds])\n", "\n", "print (\"Replacing NaN values by 0 !!\")\n", "train_df.fillna(0, inplace=True)\n", "test_df.fillna(0, inplace=True)\n", "\n", "\n", "print (\"remove outliers\")\n", "train_df=train_df[ train_df.logerror > -0.4 ]\n", "train_df=train_df[ train_df.logerror < 0.419 ]\n", "\n", "\n", "print (\"Training time !!\")\n", "X_train = train_df[train_features]\n", "y_train = train_df.logerror\n", "print(X_train.shape, y_train.shape)\n", "\n", "test_df['transactiondate'] = pd.Timestamp('2016-12-01') \n", "test_df = add_date_features(test_df)\n", "X_test = test_df[train_features]\n", "print(X_test.shape)\n", "\n", "num_ensembles = 5\n", "y_pred = 0.0\n", "for i in tqdm(range(num_ensembles)):\n", "    model = CatBoostRegressor(\n", "        iterations=630, learning_rate=0.03,\n", "        depth=6, l2_leaf_reg=3,\n", "        loss_function='MAE',\n", "        eval_metric='MAE',\n", "        random_seed=i)\n", "    model.fit(\n", "        X_train, y_train,\n", "        cat_features=cat_feature_inds)\n", "    y_pred += model.predict(X_test)\n", "y_pred /= num_ensembles\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "submission = pd.DataFrame({\n", "    'ParcelId': test_df['ParcelId'],\n", "})\n", "test_dates = {\n", "    '201610': pd.Timestamp('2016-10-01'),\n", "    '201611': pd.Timestamp('2016-11-01'),\n", "    '201612': pd.Timestamp('2016-12-01'),\n", "    '201710': pd.Timestamp('2017-10-01'),\n", "    '201711': pd.Timestamp('2017-11-01'),\n", "    '201712': pd.Timestamp('2017-12-02')\n", "}\n", "for label, test_date in test_dates.items():\n", "    print(\"Predicting for: %s ... \" % (label))\n", "    submission[label] = y_pred\n", "\n", "print( \"\\nCombined XGB/LGB/baseline/OLS predictions:\" )\n", "print( submission.head() )\n", "\n", "submission.to_csv('final_solution_0.csv', float_format='%.6f',index=False)"], "metadata": {"_uuid": "5db8a7a62b4feb94d0c743f3b735a64fd5045e0e", "_cell_guid": "1a910f61-bd78-44bf-ba00-03d118d46534", "_kg_hide-output": true}, "execution_count": null, "cell_type": "code", "outputs": []}, {"source": ["###########################################\n", "#scripts 2\n", "###########################################\n", "\n", "\n", "# Parameters\n", "XGB_WEIGHT = 0.6840\n", "BASELINE_WEIGHT = 0.0056\n", "OLS_WEIGHT = 0.0550\n", "\n", "XGB1_WEIGHT = 0.8083  # Weight of first in combination of two XGB models\n", "\n", "BASELINE_PRED = 0.0115   # Baseline based on mean of training data, per Oleg\n", "\n", "\n", "\n", "\n", "import numpy as np\n", "import pandas as pd\n", "import xgboost as xgb\n", "from sklearn.preprocessing import LabelEncoder\n", "import lightgbm as lgb\n", "import gc\n", "from sklearn.linear_model import LinearRegression\n", "import random\n", "import datetime as dt\n", "\n", "\n", "##### READ IN RAW DATA\n", "\n", "print( \"\\nReading data from disk ...\")\n", "prop_2016 = pd.read_csv('../input/properties_2016.csv')\n", "prop_2017 = pd.read_csv('../input/properties_2017.csv')\n", "train_2016 = pd.read_csv(\"../input/train_2016_v2.csv\")\n", "train_2017 = pd.read_csv(\"../input/train_2017.csv\")\n", "\n", "train = pd.concat([train_2016, train_2017], axis = 0)\n", "prop = pd.concat([prop_2016, prop_2017], axis = 0)\n", "\n", "\n", "################\n", "################\n", "##  LightGBM  ##\n", "################\n", "################\n", "\n", "# This section is (I think) originally derived from SIDHARTH's script:\n", "#   https://www.kaggle.com/sidharthkumar/trying-lightgbm\n", "# which was forked and tuned by Yuqing Xue:\n", "#   https://www.kaggle.com/yuqingxue/lightgbm-85-97\n", "# and updated by me (Andy Harless):\n", "#   https://www.kaggle.com/aharless/lightgbm-with-outliers-remaining\n", "# and a lot of additional changes have happened since then,\n", "#   the most recent of which are documented in my comments above\n", " \n", "\n", "##### PROCESS DATA FOR LIGHTGBM\n", "\n", "print( \"\\nProcessing data for LightGBM ...\" )\n", "for c, dtype in zip(prop.columns, prop.dtypes):\t\n", "    if dtype == np.float64:\t\t\n", "        prop[c] = prop[c].astype(np.float32)\n", "\n", "df_train = train.merge(prop, how='left', on='parcelid')\n", "df_train.fillna(df_train.median(),inplace = True)\n", "\n", "x_train = df_train.drop(['parcelid', 'logerror', 'transactiondate', 'propertyzoningdesc', \n", "                         'propertycountylandusecode', 'fireplacecnt', 'fireplaceflag'], axis=1)\n", "#x_train['Ratio_1'] = x_train['taxvaluedollarcnt']/x_train['taxamount']\n", "y_train = df_train['logerror'].values\n", "print(x_train.shape, y_train.shape)\n", "\n", "\n", "train_columns = x_train.columns\n", "\n", "for c in x_train.dtypes[x_train.dtypes == object].index.values:\n", "    x_train[c] = (x_train[c] == True)\n", "\n", "del df_train; gc.collect()\n", "\n", "x_train = x_train.values.astype(np.float32, copy=False)\n", "d_train = lgb.Dataset(x_train, label=y_train)\n", "\n", "\n", "\n", "##### RUN LIGHTGBM\n", "\n", "params = {}\n", "params['max_bin'] = 10\n", "params['learning_rate'] = 0.0021 # shrinkage_rate\n", "params['boosting_type'] = 'gbdt'\n", "params['objective'] = 'regression'\n", "params['metric'] = 'l1'          # or 'mae'\n", "params['sub_feature'] = 0.5      # feature_fraction -- OK, back to .5, but maybe later increase this\n", "params['bagging_fraction'] = 0.85 # sub_row\n", "params['bagging_freq'] = 40\n", "params['num_leaves'] = 512        # num_leaf\n", "params['min_data'] = 500         # min_data_in_leaf\n", "params['min_hessian'] = 0.05     # min_sum_hessian_in_leaf\n", "params['verbose'] = 0\n", "\n", "print(\"\\nFitting LightGBM model ...\")\n", "clf = lgb.train(params, d_train, 430)\n", "\n", "del d_train; gc.collect()\n", "del x_train; gc.collect()\n", "\n", "print(\"\\nPrepare for LightGBM prediction ...\")\n", "print(\"   Read sample file ...\")\n", "sample = pd.read_csv('../input/sample_submission1.csv')\n", "print(\"   ...\")\n", "sample['parcelid'] = sample['ParcelId']\n", "print(\"   Merge with property data ...\")\n", "df_test = sample.merge(prop, on='parcelid', how='left')\n", "print(\"   ...\")\n", "del sample, prop; gc.collect()\n", "print(\"   ...\")\n", "#df_test['Ratio_1'] = df_test['taxvaluedollarcnt']/df_test['taxamount']\n", "x_test = df_test[train_columns]\n", "print(\"   ...\")\n", "del df_test; gc.collect()\n", "print(\"   Preparing x_test...\")\n", "for c in x_test.dtypes[x_test.dtypes == object].index.values:\n", "    x_test[c] = (x_test[c] == True)\n", "print(\"   ...\")\n", "x_test = x_test.values.astype(np.float32, copy=False)\n", "\n", "print(\"\\nStart LightGBM prediction ...\")\n", "p_test = clf.predict(x_test)\n", "\n", "del x_test; gc.collect()\n", "\n", "print( \"\\nUnadjusted LightGBM predictions:\" )\n", "print( pd.DataFrame(p_test).head() )\n", "\n", "\n", "\n", "\n", "################\n", "################\n", "##  XGBoost   ##\n", "################\n", "################\n", "\n", "# This section is (I think) originally derived from Infinite Wing's script:\n", "#   https://www.kaggle.com/infinitewing/xgboost-without-outliers-lb-0-06463\n", "# inspired by this thread:\n", "#   https://www.kaggle.com/c/zillow-prize-1/discussion/33710\n", "# but the code has gone through a lot of changes since then\n", "\n", "\n", "##### RE-READ PROPERTIES FILE\n", "##### (I tried keeping a copy, but the program crashed.)\n", "\n", "print( \"\\nRe-reading properties file ...\")\n", "properties = pd.read_csv('../input/properties_2016.csv')\n", "\n", "\n", "\n", "##### PROCESS DATA FOR XGBOOST\n", "\n", "print( \"\\nProcessing data for XGBoost ...\")\n", "for c in properties.columns:\n", "    properties[c]=properties[c].fillna(-1)\n", "    if properties[c].dtype == 'object':\n", "        lbl = LabelEncoder()\n", "        lbl.fit(list(properties[c].values))\n", "        properties[c] = lbl.transform(list(properties[c].values))\n", "\n", "train_df = train.merge(properties, how='left', on='parcelid')\n", "x_train = train_df.drop(['parcelid', 'logerror','transactiondate'], axis=1)\n", "x_test = properties.drop(['parcelid'], axis=1)\n", "# shape        \n", "print('Shape train: {}\\nShape test: {}'.format(x_train.shape, x_test.shape))\n", "\n", "# drop out ouliers\n", "train_df=train_df[ train_df.logerror > -0.4 ]\n", "train_df=train_df[ train_df.logerror < 0.419 ]\n", "x_train=train_df.drop(['parcelid', 'logerror','transactiondate'], axis=1)\n", "y_train = train_df[\"logerror\"].values.astype(np.float32)\n", "y_mean = np.mean(y_train)\n", "\n", "print('After removing outliers:')     \n", "print('Shape train: {}\\nShape test: {}'.format(x_train.shape, x_test.shape))\n", "\n", "\n", "\n", "\n", "##### RUN XGBOOST\n", "\n", "print(\"\\nSetting up data for XGBoost ...\")\n", "# xgboost params\n", "xgb_params = {\n", "    'eta': 0.037,\n", "    'max_depth': 5,\n", "    'subsample': 0.80,\n", "    'objective': 'reg:linear',\n", "    'eval_metric': 'mae',\n", "    'lambda': 0.8,   \n", "    'alpha': 0.4, \n", "    'base_score': y_mean,\n", "    'silent': 1\n", "}\n", "\n", "dtrain = xgb.DMatrix(x_train, y_train)\n", "dtest = xgb.DMatrix(x_test)\n", "\n", "num_boost_rounds = 250\n", "print(\"num_boost_rounds=\"+str(num_boost_rounds))\n", "\n", "# train model\n", "print( \"\\nTraining XGBoost ...\")\n", "model = xgb.train(dict(xgb_params, silent=1), dtrain, num_boost_round=num_boost_rounds)\n", "\n", "print( \"\\nPredicting with XGBoost ...\")\n", "xgb_pred1 = model.predict(dtest)\n", "\n", "print( \"\\nFirst XGBoost predictions:\" )\n", "print( pd.DataFrame(xgb_pred1).head() )\n", "\n", "\n", "\n", "##### RUN XGBOOST AGAIN\n", "\n", "print(\"\\nSetting up data for XGBoost ...\")\n", "# xgboost params\n", "xgb_params = {\n", "    'eta': 0.033,\n", "    'max_depth': 6,\n", "    'subsample': 0.80,\n", "    'objective': 'reg:linear',\n", "    'eval_metric': 'mae',\n", "    'base_score': y_mean,\n", "    'silent': 1\n", "}\n", "\n", "num_boost_rounds = 150\n", "print(\"num_boost_rounds=\"+str(num_boost_rounds))\n", "\n", "print( \"\\nTraining XGBoost again ...\")\n", "model = xgb.train(dict(xgb_params, silent=1), dtrain, num_boost_round=num_boost_rounds)\n", "\n", "print( \"\\nPredicting with XGBoost again ...\")\n", "xgb_pred2 = model.predict(dtest)\n", "\n", "print( \"\\nSecond XGBoost predictions:\" )\n", "print( pd.DataFrame(xgb_pred2).head() )\n", "\n", "\n", "\n", "##### COMBINE XGBOOST RESULTS\n", "xgb_pred = XGB1_WEIGHT*xgb_pred1 + (1-XGB1_WEIGHT)*xgb_pred2\n", "#xgb_pred = xgb_pred1\n", "\n", "print( \"\\nCombined XGBoost predictions:\" )\n", "print( pd.DataFrame(xgb_pred).head() )\n", "\n", "del train_df\n", "del x_train\n", "del x_test\n", "del properties\n", "del dtest\n", "del dtrain\n", "del xgb_pred1\n", "del xgb_pred2 \n", "gc.collect()\n", "\n", "\n", "\n", "################\n", "################\n", "##    OLS     ##\n", "################\n", "################\n", "\n", "# This section is derived from the1owl's notebook:\n", "#    https://www.kaggle.com/the1owl/primer-for-the-zillow-pred-approach\n", "# which I (Andy Harless) updated and made into a script:\n", "#    https://www.kaggle.com/aharless/updated-script-version-of-the1owl-s-basic-ols\n", "\n", "np.random.seed(17)\n", "random.seed(17)\n", "\n", "train = pd.read_csv(\"../input/train_2016_v2.csv\", parse_dates=[\"transactiondate\"])\n", "properties = pd.read_csv(\"../input/properties_2016.csv\")\n", "submission1 = pd.read_csv(\"../input/sample_submission.csv\")\n", "print(len(train),len(properties),len(submission1))\n", "\n", "def get_features(df):\n", "    df[\"transactiondate\"] = pd.to_datetime(df[\"transactiondate\"])\n", "    df[\"transactiondate_year\"] = df[\"transactiondate\"].dt.year\n", "    df[\"transactiondate_month\"] = df[\"transactiondate\"].dt.month\n", "    df['transactiondate'] = df['transactiondate'].dt.quarter\n", "    df = df.fillna(-1.0)\n", "    return df\n", "\n", "def MAE(y, ypred):\n", "    #logerror=log(Zestimate)\u2212log(SalePrice)\n", "    return np.sum([abs(y[i]-ypred[i]) for i in range(len(y))]) / len(y)\n", "\n", "train = pd.merge(train, properties, how='left', on='parcelid')\n", "y = train['logerror'].values\n", "test = pd.merge(submission1, properties, how='left', left_on='ParcelId', right_on='parcelid')\n", "properties = [] #memory\n", "\n", "exc = [train.columns[c] for c in range(len(train.columns)) if train.dtypes[c] == 'O'] + ['logerror','parcelid']\n", "col = [c for c in train.columns if c not in exc]\n", "\n", "train = get_features(train[col])\n", "test['transactiondate'] = '2016-01-01' #should use the most common training date\n", "test = get_features(test[col])\n", "\n", "reg = LinearRegression(n_jobs=-1)\n", "reg.fit(train, y); print('fit...')\n", "print(MAE(y, reg.predict(train)))\n", "train = [];  y = [] #memory\n", "\n", "test_dates = ['2016-10-01','2016-11-01','2016-12-01','2017-10-01','2017-11-01','2017-12-01']\n", "test_columns = ['201610','201611','201612','201710','201711','201712']\n", "\n", "\n", "\n", "\n", "########################\n", "########################\n", "##  Combine and Save  ##\n", "########################\n", "########################\n", "\n", "\n", "##### COMBINE PREDICTIONS\n", "\n", "print( \"\\nCombining XGBoost, LightGBM, and baseline predicitons ...\" )\n", "lgb_weight = (1 - XGB_WEIGHT - BASELINE_WEIGHT) / (1 - OLS_WEIGHT)\n", "xgb_weight0 = XGB_WEIGHT / (1 - OLS_WEIGHT)\n", "baseline_weight0 =  BASELINE_WEIGHT / (1 - OLS_WEIGHT)\n", "pred0 = xgb_weight0*xgb_pred + baseline_weight0*BASELINE_PRED + lgb_weight*p_test\n", "\n", "\n", "\n", "\n", "\n", "print( \"\\nCombined XGB/LGB/baseline predictions:\" )\n", "print( pd.DataFrame(pred0).head() )\n", "\n", "print( \"\\nPredicting with OLS and combining with XGB/LGB/baseline predicitons: ...\" )\n", "for i in range(len(test_dates)):\n", "    test['transactiondate'] = test_dates[i]\n", "    pred = OLS_WEIGHT*reg.predict(get_features(test)) + (1-OLS_WEIGHT)*pred0\n", "    submission1[test_columns[i]] = [float(format(x, '.4f')) for x in pred]\n", "    print('predict...', i)\n", "\n", "print( \"\\nCombined XGB/LGB/baseline/OLS predictions:\" )\n", "print( submission1.head() )\n", "\n", "\n", "submission1.to_csv('final_solution_1.csv', float_format='%.6f',index=False)"], "metadata": {}, "execution_count": null, "cell_type": "code", "outputs": []}, {"source": [], "metadata": {"collapsed": true}, "execution_count": null, "cell_type": "code", "outputs": []}], "metadata": {"language_info": {"nbconvert_exporter": "python", "pygments_lexer": "ipython3", "codemirror_mode": {"version": 3, "name": "ipython"}, "version": "3.6.3", "mimetype": "text/x-python", "name": "python", "file_extension": ".py"}, "kernelspec": {"language": "python", "display_name": "Python 3", "name": "python3"}}, "nbformat_minor": 1}