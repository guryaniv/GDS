{"cells":[{"metadata":{"_uuid":"e44e5ca4e44464ffdcce0e840c543577bd6569bf"},"cell_type":"markdown","source":"# Overview of different categorical treatments\nThis is an overview of  of different ways one can treat categorical features in this competition. The following methods are compared:\n- OHE (one-hot-encoding);\n- label encoding with integers;\n- internal treatment in LightGBM, that implements a search for a next-to-optimal split strategy\n- target encoding using several approaches: \n  - [category_encoders.TargetEncoder](https://github.com/scikit-learn-contrib/categorical-encoding), \n  - **NEW** [xam.feature_extraction.SmoothTargetEncoder](https://github.com/MaxHalford/xam/blob/master/docs/feature-extraction.md#smooth-target-encoding) installing the `xam` package from github via the kernel interface,\n  - naive target-encoding with potential data leakage,\n  - target encoding with regularisation (all implementations have `transform` and `regularise` methods for validation/test and train samples, respectively):\n   - stratified KFold;\n   - expanding mean (a la catboost, I believe);\n   - **NEW** nested stratified KFold encoding, inspired by [this kernel by Eduardo Gomes](https://www.kaggle.com/tnarik/likelihood-encoding-of-categorical-features). It was reworked to implement it as a transformer and treatment of priors are different.\n  \nI'm aware of several other options for targer-encoding implementation, but those are not included here:\n  - smoothing + regularisation noise, following [this kernel by Olivier](https://www.kaggle.com/ogrellier/python-target-encoding-for-categorical-features). But it requires tuning of noise and smoothing parameters.\n  \n\nThis kernel inherited ideas and SW solutions from other public kernels and in such cases I will post direct references to the original results, such that you can get some additional insights from the source."},{"metadata":{"_uuid":"23f4895846a78a004aeecfb5f4c6783c383cb055"},"cell_type":"markdown","source":"## Import relevant libraries"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true,"_kg_hide-input":true,"collapsed":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline \nimport gc\n#plt.xkcd()\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nPATH = \"../input/\"\nprint(os.listdir(PATH))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0ed4d74b3e586cc5b1be41bf67756e370b86186a"},"cell_type":"markdown","source":"## Read in the data reducing memory pattern for variables.\nThe memory-footprint reduction was copied over from [this kernel](https://www.kaggle.com/gemartin/load-data-reduce-memory-usage)"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"259665371981ab1eeae11333963b96b6099fd9e0","_kg_hide-input":true},"cell_type":"code","source":"def reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n    \n    return df\n\n\ndef import_data(file):\n    \"\"\"create a dataframe and optimize its memory usage\"\"\"\n    df = pd.read_csv(file, parse_dates=True, keep_date_col=True)\n    df = reduce_mem_usage(df)\n    return df","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"collapsed":true},"cell_type":"code","source":"application_train = import_data(PATH+'application_train.csv')\napplication_test = import_data(PATH+'application_test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1a56b73ffe90bdab9015d385d68567cf03617bdf","collapsed":true},"cell_type":"code","source":"# replace values that in fact are NANs\napplication_train['DAYS_EMPLOYED'] = (application_train['DAYS_EMPLOYED'].apply(lambda x: x if x != 365243 else np.nan))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5fb1ac77952823a894bede3dc852b942bf6ee4a7"},"cell_type":"markdown","source":"## Data preprocessing\nThe here we will not do anything fancy and will just drop all features, that are known to have a large fraction of missing values and that have little effect on the model performance"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"3b5d60b0df693c67fbb18f68f9123e0b2b3ae7e0"},"cell_type":"code","source":"def feat_ext_source(df):\n    medi_avg_mode = [f_ for f_ in df.columns if '_AVG' in f_ or '_MODE' in f_ or '_MEDI' in f_]\n    df.drop(medi_avg_mode, axis=1, inplace=True)\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"aa46a91e767414557d54b0fa9d79bfc013812289"},"cell_type":"code","source":"application_train = feat_ext_source(application_train)\napplication_test  = feat_ext_source(application_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"87bce4fc7ddd06b3778cd5c0bcf99bcde57ce0ab"},"cell_type":"markdown","source":"## Categorical encoding\nThe function was taken from [this kernel](https://www.kaggle.com/sz8416/simple-intro-eda-baseline-model-with-gridsearch). It allows to do OneHotEncoding (OHE) keeping only those columns that are common to train and test samples. OHE is performed using `pd.get_dummies`, which allows to convert categorical features, while keeping numerical untouched."},{"metadata":{"trusted":true,"_uuid":"f4114dfe218a34a532275add442cb92a0414b3a4","_kg_hide-input":true,"collapsed":true},"cell_type":"code","source":"# use this if you want to convert categorical features to dummies\ndef cat_to_dummy(train, test):\n    train_d = pd.get_dummies(train, drop_first=False)\n    test_d = pd.get_dummies(test, drop_first=False)\n    # make sure that the number of features in train and test should be same\n    for i in train_d.columns:\n        if i not in test_d.columns:\n            if i!='TARGET':\n                train_d = train_d.drop(i, axis=1)\n    for j in test_d.columns:\n        if j not in train_d.columns:\n            if j!='TARGET':\n                test_d = test_d.drop(i, axis=1)\n    print('Memory usage of train increases from {:.2f} to {:.2f} MB'.format(train.memory_usage().sum() / 1024**2, \n                                                                            train_d.memory_usage().sum() / 1024**2))\n    print('Memory usage of test increases from {:.2f} to {:.2f} MB'.format(test.memory_usage().sum() / 1024**2, \n                                                                            test_d.memory_usage().sum() / 1024**2))\n    return train_d, test_d\n\n#application_train_ohe, application_test_ohe = cat_to_dummy(application_train, application_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"784d62ed0b08eca588085c15464cce4007458f95"},"cell_type":"markdown","source":"Add also int label encoding for a train+test pair implemented in the same manner"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"4bf82dab5971e542c488ed1e96fc5f9fa75b6e87","_kg_hide-input":true},"cell_type":"code","source":"# use this if you want to convert categorical features to int labels\ndef cat_to_int(train, test):\n    mem_orig_train = train.memory_usage().sum() / 1024**2\n    mem_orig_test  = test .memory_usage().sum() / 1024**2\n    categorical_feats = [ f for f in train.columns if train[f].dtype == 'object' or train[f].dtype.name == 'category' ]\n    for f_ in categorical_feats:\n        train[f_], indexer = pd.factorize(train[f_])\n        test[f_] = indexer.get_indexer(test[f_])\n    print('Memory usage of train increases from {:.2f} to {:.2f} MB'.format(mem_orig_train, \n                                                                            train.memory_usage().sum() / 1024**2))\n    print('Memory usage of test increases from {:.2f} to {:.2f} MB'.format(mem_orig_test, \n                                                                            test.memory_usage().sum() / 1024**2))\n    return categorical_feats, train, test\n\n#categorical_feats, application_train_le, application_test_le = cat_to_int(application_train, application_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7faf1adde941f470349ca125e4fdcc71ea55330f"},"cell_type":"markdown","source":"Define also target encoding using KFold regularisation as well as expanding mean regularisation a la CatBoost. Both methods are explained in (and inspired by) this nice course on coursera: https://www.coursera.org/learn/competitive-data-science/home/welcome, in particular the two videos from week3: [mean encoding](https://www.coursera.org/learn/competitive-data-science/lecture/b5Gxv/concept-of-mean-encoding) and [encoding regularisation](https://www.coursera.org/learn/competitive-data-science/lecture/LGYQ2/regularization). \n\nThese transformers were developed privately. So far i have them on kaggle only, but I plan to propagate them into a package on github later on. The final goal would be to have them as full transformers, i.e. to get rid of the `regularise` function, as it would not be compatible with pipelines. However, i did not find a way to reach it so far. Any ideas/suggestions are welcome!"},{"metadata":{"trusted":true,"_uuid":"41e810b51a69e157ef4e07f21e93615e6ab81069","_kg_hide-input":true,"collapsed":true},"cell_type":"code","source":"from sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.base import TransformerMixin\n\nclass TargetEncoder_Base(TransformerMixin):\n    '''\n    The base class to do basic target encoding. It has no regularisation method\n    '''\n    def __init__(self, cat_cols=None, y_name='TARGET', tr_type='basic', random_state=0, prefix='ENC', prefix_sep='_'):\n        self.cat_cols = cat_cols\n        self.gb       = dict()\n        self.y_name   = y_name\n        self.tr_type = tr_type\n        self.random_state = random_state\n        self.prefix   = '{}{}'.format(prefix, prefix_sep)\n        self.prior    = -1\n        super(TargetEncoder_Base, self).__init__()\n            \n    def transform(self, X, **transform_params):\n        X_ = X.copy(deep=True)\n        for f_ in self.cat_cols:\n            X_ [self.prefix + f_] = X_[f_].map(self.gb[f_]).astype(np.float32)\n            X_ [self.prefix + f_].fillna(self.prior, inplace=True)\n            del X_[f_]\n        return X_\n    \n    def fit(self, X, y=None, **fit_params):\n        self._prefit(X, y)\n                \n        #concatenate X and y to simplify usage (temporary object)\n        XY = self._getXY(X, y)\n        \n        if self.tr_type == 'basic':\n            # learn encodings from the full sample\n            for f_ in self.cat_cols:\n                self.gb[f_] = XY.groupby(f_)[self.y_name].mean()\n        else:\n            raise ValueError('Unknown value tr_type = {}'.format(self.tr_type))\n        \n        del XY   \n        return self\n    \n    def _prefit(self, X, y=None):\n        if y is None:\n            raise RuntimeError('TargetEncoder_KFold needs y to learn the transform')\n            \n        # deduce categorical columns, if user did not speficy anything\n        if self.cat_cols == None:\n            self.cat_cols = X.select_dtypes(include=['category', 'object']).columns.tolist()\n        # make sure that we store the list of categorical columns as a list\n        if not isinstance(self.cat_cols, list):\n            try:\n                self.cat_cols = self.cat_cols.tolist()\n            except:\n                RuntimeError('TargetEncoder_Base._prefit() fails to convert `cat_cols` into a list')\n                \n        #store the full sample mean for encoding of rare categories\n        self.prior = y.mean()\n        \n    def _getXY(self, X, y):\n        return pd.concat([X[self.cat_cols], y], axis=1)\n    \n    \n\nclass TargetEncoder_KFold(TargetEncoder_Base):\n    '''\n    Target encoding applying KFold regularisation\n    following procedure outlined in https://www.coursera.org/learn/competitive-data-science/lecture/LGYQ2/regularization\n    '''\n    def __init__(self, cv=5, **kwargs):\n        super(TargetEncoder_KFold, self).__init__(**kwargs)\n        self.cv_n     = cv\n        self.cv       = KFold(n_splits=cv, shuffle=True, random_state=self.random_state)\n    \n    def regularise(self, X, y=None, **transform_params):\n        # a dataframe to store OOF target encodings\n        oof = pd.DataFrame(np.zeros(shape=(X.shape[0], len(self.cat_cols))),\n                           index=X.index,\n                           columns=self.cat_cols)\n        #concatenate X and y to simplify usage (temporary object)\n        XY = self._getXY(X, y)\n        \n        # iterate over folds\n        for trn_idx, val_idx in self.cv.split(X, y):\n            trn = XY.iloc[trn_idx]\n            val = XY.iloc[val_idx]\n            # iterate over categorical features\n            for f_ in self.cat_cols:\n                # get target means for each class within category\n                te = trn.groupby(f_)[self.y_name].mean()\n                # encode the OOF partion\n                oof.iloc[val_idx, oof.columns.get_loc(f_)] = val[f_].map(te).astype(np.float32)\n        # do finla cosmetics and fill NAN\n        oof = oof.add_prefix(self.prefix).fillna(self.prior)\n        del XY\n        \n        X_ = X.drop(self.cat_cols, axis=1)\n        return pd.concat([X_, oof], axis=1)\n\n\nclass TargetEncoder_ExpandingMean(TargetEncoder_Base):\n    '''\n    Target encoding applying expanding mean regularisation\n    following procedure outlined in https://www.coursera.org/learn/competitive-data-science/lecture/LGYQ2/regularization\n    '''\n    def __init__(self, **kwargs):\n        super(TargetEncoder_ExpandingMean, self).__init__(**kwargs)\n    \n    def regularise(self, X, y=None, **transform_params):\n        X_ = X.copy(deep=True)\n        \n        # iterate over categorical features\n        for f_ in self.cat_cols:\n            gb = self._getXY(X_, y).groupby(f_)[self.y_name]\n            # calculate expanding mean\n            X_[self.prefix + f_] = ((gb.cumsum() - y) / gb.cumcount()).astype(np.float32).fillna(0)\n            del gb\n        \n        X_.drop(self.cat_cols, axis=1, inplace=True)\n        return X_\n\n\nclass TargetEncoder_NestedKFold(TargetEncoder_Base):\n    '''\n    That's a transformer-like implementation on the approach, \n    that is publicly available:\n    R in https://www.kaggle.com/raddar/raddar-extratrees\n    python in https://www.kaggle.com/tnarik/likelihood-encoding-of-categorical-features\n    '''\n    def __init__(self, cv_inner=5, cv_outer=10, **kwargs):\n        super(TargetEncoder_NestedKFold, self).__init__(**kwargs)\n        self.cv_inner_n     = cv_inner\n        self.cv_outer_n     = cv_outer\n        self.cv_outer       = KFold(n_splits=cv_outer, shuffle=True, random_state=self.random_state)\n        self.nan_tmp        = 'NAN_TMP'\n    \n    def regularise(self, X, y=None, **transform_params):\n        # a dataframe to store OOF target encodings\n        oof = pd.DataFrame(np.zeros(shape=(X.shape[0], len(self.cat_cols))),\n                           index=X.index,\n                           columns=self.cat_cols)\n        #concatenate X and y to simplify usage (temporary object)\n        XY = self._getXY(X, y)\n        \n        # iterate over folds\n        for split, (trn_idx, val_idx) in enumerate(self.cv_outer.split(X, y)):\n            # training k-1 folds are not needed, as encoding is stored in fit() already\n            val = XY.iloc[val_idx]\n            # iterate over categorical features\n            for f_ in self.cat_cols:\n                te = self.gb[f_ + '_' + str(split)]\n                # encode the OOF partion\n                oof.iloc[val_idx, oof.columns.get_loc(f_)] = val[f_].map(te).astype(np.float32)\n        # do finla cosmetics and fill NAN\n        oof = oof.add_prefix(self.prefix).fillna(self.prior)\n        del XY\n        \n        X_ = X.drop(self.cat_cols, axis=1)\n        return pd.concat([X_, oof], axis=1)\n        \n    def fit(self, X, y=None, **fit_params):\n        self = super(TargetEncoder_NestedKFold, self).fit(X, y, **fit_params)\n        \n        #concatenate X and y to simplify usage (temporary object)\n        XY = self._getXY(X, y)\n        \n        # A dictionary of all available classes\n        cat_classes = dict()\n        for f_ in self.cat_cols:\n            cat_classes[f_] = X[f_].unique().tolist()\n        \n        for outer_split, (outer_trn_idx, outer_val_idx) in enumerate(self.cv_outer.split(X, y)):\n            outer_trn = XY.iloc[outer_trn_idx]\n            # validation subsample is not used\n            \n            # The final encoding for each categorical variable: \n            # 1 pd.Series per each outer fold storing averaged over inner folds encodings\n            for f_ in self.cat_cols:\n                self.gb[f_ + '_' + str(outer_split)] = pd.Series(np.zeros((len(cat_classes[f_]),)), \n                                                                 index=cat_classes[f_], \n                                                                 dtype=np.float32)\n                \n            # Convert categoricals to 'object', as self.nan_tmp is not in categories and fillna does not work\n            outer_trn[self.cat_cols] = outer_trn[self.cat_cols].astype('object')\n            # Fill NaN as self.nan_tmp, as groupby does not group on them\n            outer_trn.fillna(self.nan_tmp, inplace=True)\n            \n            # Create a new cross-validator with a different random state per fold\n            self.cv_inner = KFold(n_splits=self.cv_inner_n,\n                                            shuffle=True,\n                                            random_state=self.random_state+outer_split)\n            # The target mean for the outer k-1 folds\n            outer_prior = outer_trn[self.y_name].mean()\n            \n            for inner_split, (inner_trn_idx, inner_val_idx) in enumerate(self.cv_inner.split(outer_trn.drop(self.y_name, axis=1), \n                                                                                           outer_trn[self.y_name])):\n                inner_trn = outer_trn.iloc[inner_trn_idx]\n                # validation subsample is not used\n                # The target mean for the inner k-1 folds\n                inner_prior = float(inner_trn[self.y_name].mean())\n                \n                for f_ in self.cat_cols:\n                    # get target means for each class within category ofr the k-1 folds of the inner CV loop \n                    # also change back from the temporary NaN naming to np.nan\n                    te = inner_trn.groupby(f_)[self.y_name].mean().astype(np.float32).rename(index={self.nan_tmp:np.nan})\n                    # add the inner-loop encoding for averaging\n                    self.gb[f_ + '_' + str(outer_split)] += te/self.cv_inner_n\n                    # also add inner-fold prior for the missing categories\n                    for miss_f_ in [f__ for f__ in cat_classes[f_] if f__ not in te]:\n                        self.gb[f_ + '_' + str(outer_split)].loc[miss_f_] += inner_prior/self.cv_inner_n\n        del XY\n        return self","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bfa35f60c92ae01d6de9c82ef52ffc1a0c00350d"},"cell_type":"markdown","source":"### Split the full sample into train/test (80/20)\nHere `StratifiedShuffleSplit` is used instead of the usual `train_test_split`. In practice it gives the same result, but in this particular case it is beneficial in a few ways:\n- it allows to obtain *indices* of entries in the train/test split, which is very handy as we want to use the same split, but to aply different data pre-processing (i.e. different encodings of categorical features);\n- it allows to easily switch to a CV scheme, if one wants to."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"eef48b303ef8ec845a2a029f3c86c4dddc099bb8"},"cell_type":"code","source":"from sklearn.model_selection import StratifiedShuffleSplit\nrs = StratifiedShuffleSplit(n_splits=1, test_size=.20, random_state=3)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a7ab531720fcbd7e953aeb839ea51f583e237d95"},"cell_type":"markdown","source":"### Define the main function to apply preprocessing, build and evaluate a lightgbm model"},{"metadata":{"trusted":true,"_uuid":"e2f85f25767ea626e15d0a369cd35fd1149b0f84","collapsed":true,"_kg_hide-input":true},"cell_type":"code","source":"import lightgbm as lgb\nimport category_encoders\nimport time\nimport xam\n# lightgbm early stopping\nfit_params={\"early_stopping_rounds\":30, \n            \"eval_metric\" : 'auc', \n            \"eval_set\" : None,\n            'eval_names': ['valid'],\n            'verbose': False,\n            'categorical_feature': 'auto'}\n\ndef fit_model(data_, trn_idx_, val_idx_, feats_, cat_enc_type='OHE'):\n    trn_x, trn_y = data_[feats_].iloc[trn_idx_], data_['TARGET'].iloc[trn_idx_]\n    val_x, val_y = data_[feats_].iloc[val_idx_], data_['TARGET'].iloc[val_idx_]\n    cols_cat = data_.select_dtypes(include=['category', 'object']).columns.tolist()\n    #if 'ExpandingMean' in cat_enc_type:\n    #        print(trn_x[cols_cat].head(10))\n    \n    time_start = time.time()\n    if cat_enc_type == 'OHE':\n        trn_x, val_x = cat_to_dummy(trn_x, val_x)\n    elif cat_enc_type == 'LabelEnc':\n        categorical_feats, trn_x, val_x = cat_to_int(trn_x, val_x)\n    elif  'TargetEnc' in cat_enc_type:\n        te = None\n        if 'category_encoders' in cat_enc_type:\n            te = category_encoders.TargetEncoder(cols=cols_cat, smoothing=1)\n            # category_encoders.TargetEncoder can not handle 'category' with empty empty class\n            # See https://github.com/scikit-learn-contrib/categorical-encoding/issues/86\n            for df in [trn_x, val_x]:\n                for f_ in cols_cat:\n                    df[f_]  = df[f_].astype('object')\n        elif 'xam_Smooth' in cat_enc_type:\n            te = xam.feature_extraction.SmoothTargetEncoder(prior_weight=1, columns=cols_cat, suffix='')\n        elif 'KFold' in cat_enc_type:\n            te = TargetEncoder_KFold(cv=5, cat_cols=None, random_state=1)\n        elif 'ExpandingMean' in cat_enc_type:\n            te = TargetEncoder_ExpandingMean(cat_cols=None, random_state=1)\n        elif 'NestedKFold' in cat_enc_type:\n            te = TargetEncoder_NestedKFold(cv_inner=5, cv_outer=5, cat_cols=None, random_state=1)\n        elif 'NoRegularisation' in cat_enc_type:\n            te = TargetEncoder_Base(cat_cols=None, random_state=1)\n        te = te.fit(trn_x, trn_y)\n        # transform the training set either simply or with regularisation\n        if 'NoRegularisation' in cat_enc_type or 'category_encoders' in cat_enc_type or 'xam' in cat_enc_type:\n            trn_x = te.transform(trn_x)\n        else:\n            trn_x = te.regularise(trn_x, trn_y)\n        # transform the validation set WITHOUT regularisation (also applies to test set, if one adds that)\n        val_x = te.transform(val_x)\n        \n        # explicit casting of remaining categorical columns (a feature of xam implementation)\n        if 'xam' in cat_enc_type:\n            for df in [trn_x, val_x]:\n                for c in cols_cat:\n                    df[c] = df[c].astype(np.float32)\n    elif 'LGBM_internal' in cat_enc_type:\n        pass\n    else:\n        raise ValueError('Unknown cat_enc_type value: ' + str(cat_enc_type))\n    # set the transformed validation set as evaluation sample in lightgbm\n    fit_params['eval_set'] = [(trn_x, trn_y), (val_x, val_y)]\n    fit_params['eval_names']= ['train', 'valid']\n    \n    # FIT A MODEL\n    # Model parameters were tuned in this kernel: https://www.kaggle.com/mlisovyi/lightgbm-hyperparameter-optimisation-lb-0-761\n    # n_estimators is set to a \"large value\". \n    # The actual number of trees build will depend on early stopping and 5000 define only the absolute maximum\n    clf = lgb.LGBMClassifier(max_depth=-1, n_jobs=4, n_estimators=5000, learning_rate=0.1, random_state=314, silent=True, metric='None')\n    opt_parameters = {'colsample_bytree': 0.9234, 'min_child_samples': 399, 'min_child_weight': 0.1, 'num_leaves': 13, 'reg_alpha': 2, 'reg_lambda': 5, 'subsample': 0.855}\n    clf.set_params(**opt_parameters)\n    clf.fit(trn_x, trn_y, **fit_params)\n    score_val = clf.best_score_['valid']['auc']\n    score_trn = clf.best_score_['train']['auc']\n    itr_trn = clf.best_iteration_\n    print('{}: {:f}'.format(cat_enc_type, score_val))\n    time_end = time.time()\n    \n    # cleanup to reduce memory footstep\n    del trn_x, trn_y, val_x, val_y\n    del clf\n    gc.collect()\n    \n    return score_val, score_trn, itr_trn, time_end-time_start","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cca390b63316f6e8c13435873c2f6cd8a7f46090"},"cell_type":"markdown","source":"### Evaluate models with different categorical treatment"},{"metadata":{"trusted":true,"_uuid":"2c28d43964c923b7b281acc15ba3738064916106","collapsed":true},"cell_type":"code","source":"feats = [f for f in application_train.columns if f not in ['SK_ID_CURR', 'TARGET']]\n\nte_types = ['OHE', 'LabelEnc', 'LGBM_internal',\n            'TargetEnc_category_encoders', 'TargetEnc_xam_Smooth', 'TargetEnc_NoRegularisation', \n            'TargetEnc_KFold', 'TargetEnc_ExpandingMean',\n            'TargetEnc_NestedKFold']\nperf = {}\n\nfor trn_idx, val_idx in rs.split(application_train, application_train['TARGET']):    \n    for te_ in te_types:\n        auc = fit_model(application_train, trn_idx, val_idx, feats, cat_enc_type=te_)\n        perf[te_] = auc","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7b53e5538847ecc282207e897f883e1013398e65"},"cell_type":"markdown","source":"## Visualise comparison of model performance"},{"metadata":{"trusted":true,"_uuid":"4264c1484c84ac301b6699efd6ce6fa9edcda7c4","collapsed":true},"cell_type":"code","source":"df_perf = pd.DataFrame(perf, index=['auc_valid', 'auc_train', 'ntrees', 'fit_time'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cb372267b91470fb7579a5fe38f77e6cd8e48a0c","collapsed":true},"cell_type":"code","source":"fig, ax = plt.subplots(ncols=2, nrows=2, figsize=(12,14))\nax = ax.flatten()\nfig.subplots_adjust(wspace=1.50)\nfor i,v in enumerate([('auc_valid', 'Validation ROC AUC', 'Blues'),\n                      ('auc_train', 'training ROC AUC', 'Oranges'),\n                      ('ntrees', 'Optimal number of trees', 'Greens'),\n                      ('fit_time', 'Fit time', 'Purples')\n                     ]):\n    sns.heatmap(df_perf.loc[v[0],:].to_frame(), cmap=v[2], annot=True, xticklabels=False,fmt='.4g', ax=ax[i])#, vmin=0.75, vmax=0.8\n    ax[i].set_title(v[1])\nfig.savefig('Performance.png')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"26778824f5dd41ac2e279a9d2721857ebd792cb1"},"cell_type":"markdown","source":"# Observations\n- The difference in the final model performance is small. Variation of the random seed in `StratifiedShuffleSplit` changes the outcome. However, on the full sample joining all tables together i see a consistent improvement of O(0.001) on TargetEncoding with regularisation over the internal LightGBM method;\n- The TargetEncoding methods lead to smaller number of trees that are needed to achieve the same performance, thus training time is reduced (important for model training on the full dataset)\n- Nested and simple KFold regularisation on the training sample  perform surprisingly identical. There might be something overlooked in the implementation.\n- `category_encoders.TargetEncoder` performs the same as the basing target encoding with no regularisation."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"ba52923a40738a5c9499112ab3df1ad8bdef00a3"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}