{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "fa31842e-9387-e376-50bd-ee1040a8aeb6"
      },
      "source": [
        "Hi, Kagglers!\n",
        "\n",
        "Hereafter I will try to publish **some basic approaches to climb up the Leaderboard**\n",
        "\n",
        "**Competition goal**\n",
        "\n",
        "In this competition, Daimler is challenging Kagglers to tackle the curse of dimensionality and reduce the time that cars spend on the test bench.\n",
        "<br>Competitors will work with a dataset representing different permutations of Mercedes-Benz car features to predict the time it takes to pass testing. <br>Winning algorithms will contribute to speedier testing, resulting in lower carbon dioxide emissions without reducing Daimler\u2019s standards. \n",
        "\n",
        "**The Notebook offers basic Keras skeleton to start with**\n",
        "\n",
        "### Stay tuned, this notebook will be updated on a regular basis\n",
        "**P.s. Upvotes and comments would let me update it faster and in a more smart way :)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "f374fecd-0331-b78b-b0d5-d8a674b501ef"
      },
      "source": ""
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "00245f2e-d3a7-4084-9a5f-09083382cad1"
      },
      "outputs": [],
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# preprocessing/decomposition\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
        "from sklearn.decomposition import PCA, FastICA, FactorAnalysis, KernelPCA\n",
        "\n",
        "# keras \n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import Dense, Dropout, BatchNormalization, Activation\n",
        "from keras.wrappers.scikit_learn import KerasRegressor\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "# model evaluation\n",
        "from sklearn.model_selection import cross_val_score, KFold, train_test_split\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "\n",
        "# supportive models\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "# feature selection (from supportive model)\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "\n",
        "# to make results reproducible\n",
        "seed = 42 # was 42"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "3f096068-d22d-829f-34e1-3aa220725168"
      },
      "source": ""
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "1078c0db-29e2-318a-6181-cc8ecdfc37cb"
      },
      "outputs": [],
      "source": [
        "# Read datasets\n",
        "train = pd.read_csv('../input/train.csv')\n",
        "test = pd.read_csv('../input/test.csv')\n",
        "\n",
        "# save IDs for submission\n",
        "id_test = test['ID'].copy()\n",
        "\n",
        "# glue datasets together\n",
        "total = pd.concat([train, test], axis=0)\n",
        "print('initial shape: {}'.format(total.shape))\n",
        "\n",
        "# binary indexes for train/test set split\n",
        "is_train = ~total.y.isnull()\n",
        "\n",
        "# find all categorical features\n",
        "cf = total.select_dtypes(include=['object']).columns\n",
        "\n",
        "# make one-hot-encoding convenient way - pandas.get_dummies(df) function\n",
        "dummies = pd.get_dummies(\n",
        "    total[cf],\n",
        "    drop_first=False # you can set it = True to ommit multicollinearity (crucial for linear models)\n",
        ")\n",
        "\n",
        "print('oh-encoded shape: {}'.format(dummies.shape))\n",
        "\n",
        "# get rid of old columns and append them encoded\n",
        "total = pd.concat(\n",
        "    [\n",
        "        total.drop(cf, axis=1), # drop old\n",
        "        dummies # append them one-hot-encoded\n",
        "    ],\n",
        "    axis=1 # column-wise\n",
        ")\n",
        "\n",
        "print('appended-encoded shape: {}'.format(total.shape))\n",
        "\n",
        "# recreate train/test again, now with dropped ID column\n",
        "train, test = total[is_train].drop(['ID'], axis=1), total[~is_train].drop(['ID', 'y'], axis=1)\n",
        "\n",
        "# drop redundant objects\n",
        "del total\n",
        "\n",
        "# check shape\n",
        "print('\\nTrain shape: {}\\nTest shape: {}'.format(train.shape, test.shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "485c2995-f1a6-4f6f-e59e-64cf7d65b9db"
      },
      "source": ""
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "bd78230e-8075-a9a2-eedf-d40a5be61d0e"
      },
      "outputs": [],
      "source": [
        "# Calculate additional features: dimensionality reduction components\n",
        "n_comp=10 # was 10\n",
        "\n",
        "# uncomment to scale data before applying decompositions\n",
        "# however, all features are binary (in [0,1] interval), i don't know if it's worth something\n",
        "train_scaled = train.drop('y', axis=1).copy()\n",
        "test_scaled = test.copy()\n",
        "'''\n",
        "ss = StandardScaler()\n",
        "ss.fit(train.drop('y', axis=1))\n",
        "\n",
        "train_scaled = ss.transform(train.drop('y', axis=1).copy())\n",
        "test_scaled = ss.transform(test.copy())\n",
        "'''\n",
        "\n",
        "# PCA\n",
        "pca = PCA(n_components=n_comp, random_state=seed)\n",
        "pca2_results_train = pca.fit_transform(train_scaled)\n",
        "pca2_results_test = pca.transform(test_scaled)\n",
        "\n",
        "# ICA\n",
        "ica = FastICA(n_components=n_comp, random_state=42)\n",
        "ica2_results_train = ica.fit_transform(train_scaled)\n",
        "ica2_results_test = ica.transform(test_scaled)\n",
        "\n",
        "# Append it to dataframes\n",
        "for i in range(1, n_comp+1):\n",
        "    train['pca_' + str(i)] = pca2_results_train[:,i-1]\n",
        "    test['pca_' + str(i)] = pca2_results_test[:, i-1]\n",
        "    \n",
        "    train['ica_' + str(i)] = ica2_results_train[:,i-1]\n",
        "    test['ica_' + str(i)] = ica2_results_test[:, i-1]\n",
        "   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "af5f7ca0-ff7a-a4aa-6865-b332ca19cfbc"
      },
      "source": ""
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c99937b6-e3c1-4514-a2d6-1cedf0d5d5ff"
      },
      "outputs": [],
      "source": [
        "# create augmentation by feature importances as additional features\n",
        "t = train['y']\n",
        "tr = train.drop(['y'], axis=1)\n",
        "\n",
        "# Tree-based estimators can be used to compute feature importances\n",
        "clf = GradientBoostingRegressor(\n",
        "                max_depth=4, \n",
        "                learning_rate=0.005, \n",
        "                random_state=seed, \n",
        "                subsample=0.95, \n",
        "                n_estimators=200\n",
        ")\n",
        "\n",
        "# fit regressor\n",
        "clf.fit(tr, t)\n",
        "\n",
        "# df to hold feature importances\n",
        "features = pd.DataFrame()\n",
        "features['feature'] = tr.columns\n",
        "features['importance'] = clf.feature_importances_\n",
        "features.sort_values(by=['importance'], ascending=True, inplace=True)\n",
        "features.set_index('feature', inplace=True)\n",
        "\n",
        "# select best features\n",
        "model = SelectFromModel(clf, prefit=True)\n",
        "train_reduced = model.transform(tr)\n",
        "\n",
        "\n",
        "test_reduced = model.transform(test.copy())\n",
        "\n",
        "# dataset augmentation\n",
        "train = pd.concat([train, pd.DataFrame(train_reduced)], axis=1)\n",
        "test = pd.concat([test, pd.DataFrame(test_reduced)], axis=1)\n",
        "\n",
        "# check new shape\n",
        "print('\\nTrain shape: {}\\nTest shape: {}'.format(train.shape, test.shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "ad81cc31-cd26-83cf-4df6-75742836c39e"
      },
      "source": ""
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "bbd46fc6-2fac-4e2d-0492-1a6e4ec45df6"
      },
      "outputs": [],
      "source": [
        "# define custom R2 metrics for Keras backend\n",
        "from keras import backend as K\n",
        "\n",
        "def r2_keras(y_true, y_pred):\n",
        "    SS_res =  K.sum(K.square( y_true - y_pred )) \n",
        "    SS_tot = K.sum(K.square( y_true - K.mean(y_true) ) ) \n",
        "    return ( 1 - SS_res/(SS_tot + K.epsilon()) )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "88c418a9-ed38-45d3-cdd8-7225bcbabd1b"
      },
      "source": ""
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "31424416-a7c5-6aa0-9c5a-8412bb9e9819"
      },
      "outputs": [],
      "source": [
        "# base model architecture definition\n",
        "def model():\n",
        "    model = Sequential()\n",
        "    #input layer\n",
        "    model.add(Dense(input_dims, input_dim=input_dims))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dropout(0.3))\n",
        "    # hidden layers\n",
        "    model.add(Dense(input_dims))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation(act_func))\n",
        "    model.add(Dropout(0.3))\n",
        "    \n",
        "    model.add(Dense(input_dims//2))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation(act_func))\n",
        "    model.add(Dropout(0.3))\n",
        "    \n",
        "    model.add(Dense(input_dims//4, activation=act_func))\n",
        "    \n",
        "    # output layer (y_pred)\n",
        "    model.add(Dense(1, activation='linear'))\n",
        "    \n",
        "    # compile this model\n",
        "    model.compile(loss='mean_squared_error', # one may use 'mean_absolute_error' as alternative\n",
        "                  optimizer='adam',\n",
        "                  metrics=[r2_keras] # you can add several if needed\n",
        "                 )\n",
        "    \n",
        "    # Visualize NN architecture\n",
        "    print(model.summary())\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "2c64b578-8b9a-1519-3262-379c63c2abb7"
      },
      "outputs": [],
      "source": [
        "# initialize input dimension\n",
        "input_dims = train.shape[1]-1\n",
        "\n",
        "#activation functions for hidden layers\n",
        "act_func = 'tanh' # could be 'relu', 'sigmoid', ...\n",
        "\n",
        "# make np.seed fixed\n",
        "np.random.seed(seed)\n",
        "\n",
        "# initialize estimator, wrap model in KerasRegressor\n",
        "estimator = KerasRegressor(\n",
        "    build_fn=model, \n",
        "    nb_epoch=100, \n",
        "    batch_size=20,\n",
        "    verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b256a8e3-2af9-142a-3e91-dd85dbb4c806"
      },
      "outputs": [],
      "source": [
        "# X, y preparation\n",
        "X, y = train.drop('y', axis=1).values, train.y.values\n",
        "print(X.shape)\n",
        "\n",
        "# X_test preparation\n",
        "X_test = test\n",
        "print(X_test.shape)\n",
        "\n",
        "# train/validation split\n",
        "X_tr, X_val, y_tr, y_val = train_test_split(\n",
        "    X, \n",
        "    y, \n",
        "    test_size=0.2, \n",
        "    random_state=seed\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "7fe6fa28-eddf-9afb-ec8c-59c2bedc9e0a"
      },
      "outputs": [],
      "source": [
        "# define path to save model\n",
        "import os\n",
        "model_path = 'keras_model.h5'\n",
        "\n",
        "# prepare callbacks\n",
        "callbacks = [\n",
        "    EarlyStopping(\n",
        "        monitor='val_loss', \n",
        "        patience=10, # was 10\n",
        "        verbose=1),\n",
        "    \n",
        "    ModelCheckpoint(\n",
        "        model_path, \n",
        "        monitor='val_loss', \n",
        "        save_best_only=True, \n",
        "        verbose=0)\n",
        "]\n",
        "\n",
        "# fit estimator\n",
        "estimator.fit(\n",
        "    X_tr, \n",
        "    y_tr, \n",
        "    epochs=10, # increase it to 20-100 to get better results\n",
        "    validation_data=(X_val, y_val),\n",
        "    verbose=2,\n",
        "    callbacks=callbacks,\n",
        "    shuffle=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "0346a9f2-10bc-ad1a-93eb-337cb644fee3"
      },
      "source": ""
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d5365903-19fd-ca65-dae9-caaa82b23cb5"
      },
      "outputs": [],
      "source": [
        "# if best iteration's model was saved then load and use it\n",
        "if os.path.isfile(model_path):\n",
        "    estimator = load_model(model_path, custom_objects={'r2_keras': r2_keras})\n",
        "\n",
        "# check performance on train set\n",
        "print('MSE train: {}'.format(mean_squared_error(y_tr, estimator.predict(X_tr))**0.5)) # mse train\n",
        "print('R^2 train: {}'.format(r2_score(y_tr, estimator.predict(X_tr)))) # R^2 train\n",
        "\n",
        "# check performance on validation set\n",
        "print('MSE val: {}'.format(mean_squared_error(y_val, estimator.predict(X_val))**0.5)) # mse val\n",
        "print('R^2 val: {}'.format(r2_score(y_val, estimator.predict(X_val)))) # R^2 val\n",
        "pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "fb113fea-c0e7-c3de-5cde-bc78a019a28d"
      },
      "source": ""
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "80255a67-494c-c7ae-2a4d-a98fb08cbbc1"
      },
      "outputs": [],
      "source": [
        "# predict results\n",
        "res = estimator.predict(X_test.values).ravel()\n",
        "\n",
        "# create df and convert it to csv\n",
        "output = pd.DataFrame({'id': id_test, 'y': res})\n",
        "output.to_csv('keras-baseline.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b83d560f-075d-a89a-a09b-335b2b269e40",
        "collapsed": true
      },
      "outputs": [],
      "source": ""
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}