{"cells":[{"metadata":{"_uuid":"7f9f7f55e38fb99bc68c1deedbffd2f8a2b8bb3f"},"cell_type":"markdown","source":"# Overview\nA simpe notebook to preprocess the input and create possibly useful time features, train a model, and then make predictions on the unknown dataset[](http://)"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"%matplotlib inline\nimport numpy as np\nimport pandas as pd\nimport sklearn\nimport os\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nbase_dir = os.path.join('..', 'input')\nstart_date = pd.to_datetime('2013-01-01 ')\ndef read_and_parse(in_path):\n    in_df = pd.read_csv(in_path)\n    in_df['date'] = pd.to_datetime(in_df['date'])\n    in_df['elapsed_days'] = (in_df['date']-start_date).dt.days\n    # time tags\n    for x_col in 'dayofweek', 'dayofyear', 'day', 'weekday', 'week', 'month', 'year':\n        in_df[x_col] = getattr(in_df['date'].dt, x_col)\n    in_df.drop(['date'], 1, inplace = True)\n    return in_df\ndef show_sales(in_df):\n    sns.factorplot('elapsed_days', 'sales', hue = 'store', facet = 'item', data = in_df)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"full_df = read_and_parse(os.path.join(base_dir, 'train.csv'))\nfull_df.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f19b5a8a69c04019b9c6b9e5055ffaf742918247"},"cell_type":"markdown","source":"## View by item"},{"metadata":{"trusted":true,"_uuid":"830a99e1b66e2297ec75a38411895d274ef81f29"},"cell_type":"code","source":"fig, m_axs = plt.subplots(3, 3, figsize = (15, 25))\nfor c_ax, (item_id, c_df) in zip(m_axs.flatten(), \n                          full_df.groupby('item')):\n    n_df = c_df\n    out_pvt = c_df.pivot_table(values='sales', \n                        aggfunc='mean', \n                        index=['elapsed_days'], \n                        columns = 'store').reset_index()\n    \n    c_ax.matshow(out_pvt.values[:,1:])\n    c_ax.set_title('Item: {}'.format(item_id))\n    #c_ax.axis('off')\n    c_ax.set_ylabel('Time')\n    c_ax.set_aspect(0.025)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5a85d7f9ccab84321dfac0611fb324494d78e5eb"},"cell_type":"markdown","source":"## View by store "},{"metadata":{"trusted":true,"_uuid":"8fb29220f0a07089015b7f542feab3f9c6a66872"},"cell_type":"code","source":"fig, m_axs = plt.subplots(3, 3, figsize = (15, 25))\nfor c_ax, (store_id, c_df) in zip(m_axs.flatten(), \n                          full_df.groupby('store')):\n    n_df = c_df\n    out_pvt = c_df.pivot_table(values='sales', \n                        aggfunc='mean', \n                        index=['elapsed_days'], \n                        columns = 'item').reset_index()\n    \n    c_ax.matshow(out_pvt.values[:,1:])\n    c_ax.set_title('Store: {}'.format(store_id))\n    #c_ax.axis('off')\n    c_ax.set_ylabel('Time')\n    c_ax.set_aspect(0.025)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"03efc9f9ad6b6db65f855c7fb1874638aa9d8df6"},"cell_type":"code","source":"test_out_df = read_and_parse(os.path.join(base_dir, 'test.csv'))\ntest_out_df.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d5351fa27b1b4eb56c08c18eee70d365a92b3c15"},"cell_type":"markdown","source":"# Show the data\nHere we just try to see how the data compare (blue is what we have and orange is what we have to predict)"},{"metadata":{"trusted":true,"_uuid":"314f50d25a5df588fdc7907d12b52163f268231c"},"cell_type":"code","source":"test_store = 9\ntest_item = 36\nfrom sklearn.dummy import DummyRegressor\n\nq_string = f'store=={test_store} and item=={test_item}'\nsample_train_df = full_df.query(q_string).copy()\nfig, ax1 = plt.subplots(1,1,figsize = (20, 10))\nsample_train_df.plot(x = 'elapsed_days', y = 'sales', ax = ax1)\ny_vec = sample_train_df.pop('sales')\n\n# predict and show the test data\ndummy_reg = DummyRegressor(strategy = 'median')\ndummy_reg.fit(sample_train_df, y_vec)\nsample_test_df = test_out_df.query(q_string).copy()\nsample_test_df['sales'] = dummy_reg.predict(sample_test_df[sample_train_df.columns])\nsample_test_df.plot(x = 'elapsed_days', y = 'sales', ax = ax1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9cbd78c5d07e2d6e7c9fdf61362f15351cafed32"},"cell_type":"markdown","source":"# Make Silly Predictions\nMake the silliest of predictions using the dummy regressor"},{"metadata":{"trusted":true,"_uuid":"f60d5410ea3a8a34675ccecc315385463842e8e7","collapsed":true},"cell_type":"code","source":"out_rows = []\nfor c_grp, c_train_df in full_df.groupby(['store', 'item']):\n    y_vec = c_train_df.pop('sales')\n    c_grp_string = f'store=={c_grp[0]} and item=={c_grp[1]}'\n    out_df = test_out_df.query(c_grp_string).copy()\n    dummy_reg = DummyRegressor(strategy = 'median')\n    dummy_reg.fit(c_train_df, y_vec)\n    out_df['sales'] = dummy_reg.predict(out_df[c_train_df.columns])\n    out_rows += [out_df]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1fd5cabea3c8ce85cf22c833f594d251a7ab376a"},"cell_type":"code","source":"full_out_df = pd.concat(out_rows)\nfull_out_df.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4df97c1824424387bad659f590fecabfb915cabc","collapsed":true},"cell_type":"code","source":"full_out_df[['id', 'sales']].to_csv('prediction.csv', index = False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}