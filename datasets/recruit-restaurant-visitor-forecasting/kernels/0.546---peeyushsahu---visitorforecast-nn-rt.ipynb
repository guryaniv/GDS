{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport geohash as gs\nimport time\nimport pprint\n%matplotlib inline\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":20,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# Read all the dataframes into memory and print headers for information\n\nall_df = {}\nfor file in os.listdir(\"../input\"):\n    print('Name:', file)\n    name = pd.read_csv(\"../input/\"+file)\n    print(name.head())\n    print(name.shape)\n    print('#'*10)\n\nair_store_info = pd.read_csv(\"../input/air_store_info.csv\")\nair_reserve = pd.read_csv(\"../input/air_reserve.csv\")\nair_visit_data = pd.read_csv(\"../input/air_visit_data.csv\")\nstore_id_relation = pd.read_csv(\"../input/store_id_relation.csv\")\nhpg_reserve = pd.read_csv(\"../input/hpg_reserve.csv\")\nhpg_store_info = pd.read_csv(\"../input/hpg_store_info.csv\")\nsample_submission = pd.read_csv(\"../input/sample_submission.csv\")\ndate_info = pd.read_csv(\"../input/date_info.csv\")\n","execution_count":2,"outputs":[]},{"metadata":{"_cell_guid":"cc323709-6144-462f-8143-6c5054842f60","_uuid":"8720dddf4e870ad2cc05b3774023e8dde0acb516"},"cell_type":"markdown","source":"## Combining dataframes\n### Not all hpg ids have air ids\nBecause of the inner join we are lossing a lot of data. We have to check whether it is worth removing these datapoints later."},{"metadata":{"scrolled":false,"_cell_guid":"fa45657f-0480-4bae-9d17-abec65edacb5","_uuid":"4c2c8a1562a507e278bbea7b8511cc4ac3c3f994","trusted":true},"cell_type":"code","source":"# First hpg_dataframes\nprint('Shape before merge:', hpg_reserve.shape, store_id_relation.shape, hpg_store_info.shape)\nhpg_com_reserve = pd.merge(hpg_reserve, store_id_relation, on='hpg_store_id', how='inner').merge(hpg_store_info, on='hpg_store_id', how='inner')\nprint('Shape after merge:', hpg_com_reserve.shape)\nhpg_com_reserve.index = range(len(hpg_com_reserve))\nhpg_com_reserve.head()\n","execution_count":3,"outputs":[]},{"metadata":{"_cell_guid":"c9481266-ff96-4ba8-8e2b-b76f7b97c699","_uuid":"3a6dd470e78aa9827355c5ea30197730eb1a2339","trusted":true},"cell_type":"code","source":"# Second air_dataframes\nprint('Shape before merge:', air_reserve.shape, air_store_info.shape)\nair_com_reserve = pd.merge(air_reserve, air_store_info, on='air_store_id', how='inner')\nprint('Shape after merge:', air_com_reserve.shape)\nair_com_reserve.index = range(len(air_com_reserve))\nair_com_reserve.head()","execution_count":4,"outputs":[]},{"metadata":{"_cell_guid":"cfc0481b-3c09-418e-bd44-12db9900621a","_uuid":"b5b26a987c56c1cf341fac584da4d8cf07343f0a"},"cell_type":"markdown","source":"### Dealing with geographical coordinates\nI want to use geographical coordinates but how??? \nFirst I have vizualized geo coordinates using a scatter plot below.  \n1. We can do a knn clustering to divide them based on clusters and use cluster ids.\n2. We can use geohash, these are used for giving one value to lat and lon.\nex. gs.encode(35.658068, 139.751599); 'xn76u5k5239h'\n"},{"metadata":{"_cell_guid":"bcb07f89-28ef-44e4-b2bb-b148fa5076f0","_uuid":"e803c4814be6bda6309b1812d0a1ac85b0975efc","trusted":true},"cell_type":"code","source":"# Plotting latitude and longitude\ng1 = plt.scatter(air_com_reserve.longitude, air_com_reserve.latitude, label='air_coor')\ng2 = plt.scatter(hpg_com_reserve.longitude, hpg_com_reserve.latitude, c='r', marker='+', label='hpg_coor')\nplt.legend()\nplt.grid()\nplt.xlabel('Longitude')\nplt.ylabel('Latitude')\nplt.title('This looks like japan :D')\nplt.show()","execution_count":5,"outputs":[]},{"metadata":{"_cell_guid":"3f1322b5-25bd-4165-9135-1d259a9b6533","collapsed":true,"_uuid":"e40209231c75847697045aadeacb58fdc9e1d00a","trusted":false},"cell_type":"code","source":"# converting latitude and longitude to geohash\n# I am not using area name because not all of the entries have them\n'''\ngeohash = []\nfor ind, row in air_com_reserve.iterrows():\n    #air_com_reserve.loc[ind, 'geohash'] = gs.encode(row['latitude'], row['longitude'])\n    geohash.append(gs.encode(row['latitude'], row['longitude']))\n    #air_com_reserve = pd.concat([air_com_reserve, pd.Series({'geohash':geohash})], axis=1)\n    \nprint(air_com_reserve.head())\n\ngeohash = []\nfor ind1, row1 in hpg_com_reserve.iterrows():\n    #hpg_com_reserve.loc[ind1, 'geohash'] = gs.encode(row1['latitude'], row1['longitude'])\n    geohash.append(gs.encode(row1['latitude'], row1['longitude']))\n    #hpg_com_reserve = pd.concat([hpg_com_reserve, pd.Series({'geohash':geohash})], axis=1)\n    \nprint(hpg_com_reserve.head())\n'''","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"20cf56ad-35f1-4e90-a177-fa4d00fe4caf","_uuid":"67d13b53dd7ebd047ffa4564449ac791c239e2dd"},"cell_type":"markdown","source":"### Convert date time columns to python datetime"},{"metadata":{"_cell_guid":"ed637973-6b73-44c2-9ef9-cc2674431991","collapsed":true,"_uuid":"e31cbba71225ee0fdf0ff58e80ad727a851a9f09","trusted":true},"cell_type":"code","source":"# Not all air ids in air_com_resrve dataset exist in hpg_com_resrve (there are some new restaurants are also there)\n# lets combine air and hpg dfs\nhpg_com_reserve = hpg_com_reserve.drop('hpg_store_id', axis=1)\nhpg_com_reserve = hpg_com_reserve.rename(columns={'hpg_genre_name':'genre_name', 'hpg_area_name':'area_name'})\nair_com_reserve = air_com_reserve.rename(columns={'air_genre_name':'genre_name', 'air_area_name':'area_name'})","execution_count":6,"outputs":[]},{"metadata":{"_cell_guid":"c915b47f-d57a-4d9e-9a16-6d2178b428a6","_uuid":"6a03d7c0c0eaef204a9d92863456cc491d5740a8","trusted":true},"cell_type":"code","source":"# Concat both dataframes\nair_hpg_com_reserve = pd.concat([air_com_reserve, hpg_com_reserve], ignore_index=True)\nprint(air_hpg_com_reserve.shape)","execution_count":7,"outputs":[]},{"metadata":{"_cell_guid":"133d22b2-c3cd-424f-b547-c64ffbdeb740","_uuid":"4fa5857dbea8990ca18c166720937a40bb922124","trusted":true},"cell_type":"code","source":"# DF used: air_hpg_com_reserve, air_visit_data\n# Building new features\n#air_hpg_com_reserve = pd.concat([air_com_reserve, hpg_com_reserve])\nair_hpg_com_reserve['visit_datetime'] = pd.to_datetime(air_hpg_com_reserve['visit_datetime'])\nair_hpg_com_reserve['visit_dow'] = air_hpg_com_reserve.visit_datetime.dt.dayofweek\nair_hpg_com_reserve['visit_date'] = air_hpg_com_reserve.visit_datetime.dt.date\n#air_hpg_com_reserve['visit_week'] = air_hpg_com_reserve.visit_datetime.dt.week # not this week we need week of month\nair_hpg_com_reserve['visit_mon'] = air_hpg_com_reserve.visit_datetime.dt.month\n\n\nair_hpg_com_reserve['reserve_datetime'] = pd.to_datetime(air_hpg_com_reserve['reserve_datetime'])\n#air_hpg_com_reserve['res_dow'] = air_hpg_com_reserve.reserve_datetime.dt.dayofweek\n#air_hpg_com_reserve['res_date'] = air_hpg_com_reserve.reserve_datetime.dt.date\n\n# This feature does not make sense for me \n#air_hpg_com_reserve['res_in_adv'] = air_hpg_com_reserve['visit_date']-air_hpg_com_reserve['res_date']\n#air_hpg_com_reserve['res_in_adv'] = air_hpg_com_reserve.res_in_adv.astype('str').apply(lambda x: int(x.split(' ')[0]))\n#air_hpg_com_reserve['res_in_adv'] = air_hpg_com_reserve.res_in_adv.apply(lambda x: 'v_early' if x > pd.Timedelta('100 Days') else 'late')\n\nair_hpg_com_reserve = air_hpg_com_reserve.drop(['area_name', 'visit_datetime'], axis=1) \nair_hpg_com_reserve.index = range(len(air_hpg_com_reserve))\nprint(air_hpg_com_reserve.head())\nprint(air_hpg_com_reserve.shape)","execution_count":8,"outputs":[]},{"metadata":{"scrolled":false,"_cell_guid":"c5c90ed2-88d9-4230-8d29-4bfe029479b5","_uuid":"356b64430e688d8e399f4a27bbbaf85f079a91d5","trusted":true},"cell_type":"code","source":"air_hpg_com_reduce = []\nn_cols = ['air_store_id', 'genre_name', 'latitude', 'longitude', 'reserve_datetime', 'visit_dow', 'visit_date', 'visit_mon']\nfor key, df in air_hpg_com_reserve.groupby(['air_store_id', 'visit_date']):\n    air_hpg_com_dict = {}\n    for cols in n_cols:\n        if len(df) > 1:\n            air_hpg_com_dict[cols] = df[cols].values[0]\n            air_hpg_com_dict['reserve_visitors'] = df['reserve_visitors'].sum()\n            air_hpg_com_dict['reserve_calls'] = len(df) \n        else:\n            air_hpg_com_dict[cols] = df[cols].values[0]\n            air_hpg_com_dict['reserve_visitors'] = df['reserve_visitors'].values[0]\n            air_hpg_com_dict['reserve_calls'] = 1\n    air_hpg_com_reduce.append(air_hpg_com_dict)    \n    #if len(df) > 10: break\nair_hpg_com_reduce_df = pd.DataFrame(air_hpg_com_reduce)\nprint(air_hpg_com_reduce_df.head())\nprint(air_hpg_com_reduce_df.shape)","execution_count":9,"outputs":[]},{"metadata":{"scrolled":false,"_cell_guid":"cbffd53c-f9bc-412a-ac0f-44dbecf47f44","_uuid":"f0c3687d125e076c993b41b58391b93f0993ea19","trusted":true},"cell_type":"code","source":"# Transforming air_visit_date['visit_date'] to datetime and then time\nair_visit_data['visit_date'] = pd.to_datetime(air_visit_data['visit_date'])\nair_visit_data['visit_date'] = air_visit_data.visit_date.dt.date\n\n# Combining air_hpg_com_reserve, air_visit_data\nair_hpg_com_reduce_df = pd.merge(air_hpg_com_reduce_df, air_visit_data, on=['air_store_id', 'visit_date'], how='inner')\nprint(air_hpg_com_reduce_df.head())\nprint(air_hpg_com_reduce_df.shape)","execution_count":10,"outputs":[]},{"metadata":{"scrolled":false,"_cell_guid":"0d65e3f9-91dc-4a8e-83c3-84b60a8189a7","_uuid":"3ca6ff2ff287c0a5982a288cafea2703b4f678e1","trusted":true},"cell_type":"code","source":"# Adding holiday information\n# Transforming date_info['calendar_date'] to datetime and then time\ndate_info = date_info.rename(columns={'calendar_date':'visit_date'})\ndate_info['visit_date'] = pd.to_datetime(date_info['visit_date'])\ndate_info['visit_date'] = date_info.visit_date.dt.date\n\nair_hpg_com_reduce_df = pd.merge(air_hpg_com_reduce_df, date_info, on='visit_date', how='inner')\nair_hpg_com_reduce_df = air_hpg_com_reduce_df.drop(['day_of_week', 'reserve_datetime'], axis=1)\nprint(air_hpg_com_reduce_df.head())\nprint(air_hpg_com_reduce_df.shape)","execution_count":11,"outputs":[]},{"metadata":{"_cell_guid":"47f64bf7-e4a3-400b-be2b-4b52bf509bac","_uuid":"07b903715e9463d32e4a51e974210e731919713f","trusted":true},"cell_type":"code","source":"# Some of the hotels have more than one genre associate to them\nhotels_genre = {}\nfor key, df in air_hpg_com_reserve.groupby('air_store_id'):\n    #if len(np.unique(df.genre_name.values)) > 1:\n    hotels_genre[key] = '_'.join(list(np.unique(df.genre_name.values)))\npprint.pprint(hotels_genre)\n\nhotels_genre_ids = {}\nfor ind, vals in enumerate(np.unique(list(hotels_genre.values()))):\n    hotels_genre_ids[vals] = ind\npprint.pprint(hotels_genre_ids)","execution_count":12,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9e4473ea77a7214a339ab4b7f16b11b732933306"},"cell_type":"code","source":"# Return genre from genre_ID\ndef get_genre_id2name(ID):\n    return [key for key, value in hotels_genre_ids.items() if value == ID][0]\n\n#get_genre_id2name(32)","execution_count":29,"outputs":[]},{"metadata":{"_uuid":"f4e6cd34c271766b4cddcc4c63f5554d75fb0267"},"cell_type":"markdown","source":"We are engineering two features here:\n- Geohash: it is a hash value for geographical coordinates so instead of Latitude and longitude we have one geohash \n- genre-id: we have resturents with more than one genre so we are combinig them and making new ids. "},{"metadata":{"_cell_guid":"80922076-869b-4a69-b805-10a3284e16f9","_uuid":"f7827dfce1217c4faa1d4db9f1b6aded1843fb65","trusted":true},"cell_type":"code","source":"air_hpg_com_reduce_df['geohash'] = 0 #genre_name\nair_hpg_com_reduce_df['genre_id'] = 0\nfor ind, row in air_hpg_com_reduce_df.iterrows():\n    air_hpg_com_reduce_df.loc[ind, 'geohash'] = gs.encode(row['latitude'], row['longitude'])\n    air_hpg_com_reduce_df.loc[ind, 'genre_id'] = hotels_genre_ids[hotels_genre[row['air_store_id']]]\n\nair_hpg_com_reduce_df = air_hpg_com_reduce_df.drop(['genre_name', 'latitude', 'longitude'], axis=1)\nprint(air_hpg_com_reduce_df.head())\nprint(air_hpg_com_reduce_df.shape)","execution_count":15,"outputs":[]},{"metadata":{"_uuid":"d785f8cc846053f81513ab709092ea0378fe6328"},"cell_type":"markdown","source":"**In the discussion I have seen people talking about holiday hack (reshifiting day of week depending on holidays) https://www.kaggle.com/c/recruit-restaurant-visitor-forecasting/discussion/49100.  We will do it later for checking its impact.**"},{"metadata":{"_uuid":"85771515c69b3d684456d4d583f0cd6a0ed48dfb"},"cell_type":"markdown","source":"#### Plot to visualize any pattern between features and visitor counts."},{"metadata":{"_cell_guid":"f36b9c10-1c61-41f6-9104-82bc9f5b6b63","_uuid":"c01b475e2b494d889721b08c16db660fdd40a16b","trusted":true},"cell_type":"code","source":"# Plotting latitude and longitude\nfig, ax = plt.subplots(3,2, figsize=(18,12))\nax[0,0].scatter(air_hpg_com_reduce_df.geohash, np.log2(air_hpg_com_reduce_df.visitors))\nax[0,0].set_title('geohash vs visitor')\nax[0,1].scatter(np.log2(air_hpg_com_reduce_df.reserve_calls), np.log2(air_hpg_com_reduce_df.visitors))\nax[0,1].set_title('reserve_calls vs visitor')\nax[1,0].scatter(np.log2(air_hpg_com_reduce_df.reserve_visitors), np.log2(air_hpg_com_reduce_df.visitors))\nax[1,0].set_title('reserve_visitors vs visitor')\nax[1,1].scatter(air_hpg_com_reduce_df.visit_mon, np.log2(air_hpg_com_reduce_df.visitors))\nax[1,1].set_title('visit_month vs visitor')\nax[2,0].scatter(air_hpg_com_reduce_df.genre_id, np.log2(air_hpg_com_reduce_df.visitors))\nax[2,0].set_title('genre vs visitor')","execution_count":19,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4c3a8cde767b46b3938a889a1156d3cba3c90c45"},"cell_type":"code","source":"# How genre_id correlate with visitors count depending on days between holidays and non-holidays\ngenre_id = 37\ntest = air_hpg_com_reduce_df[air_hpg_com_reduce_df['genre_id'] == genre_id]\nprint('Genre name:', get_genre_id2name(genre_id))\nprint(test.shape)\nprint(test.head(5))\n\nsns.set(style=\"ticks\")\n#sns.scatter(air_hpg_com_reduce_df.genre_id, np.log2(air_hpg_com_reduce_df.visitors))\nsns.boxplot(x=\"visit_dow\", y=\"visitors\", hue=\"holiday_flg\", data=test, palette=\"PRGn\")","execution_count":58,"outputs":[]},{"metadata":{"_uuid":"48af2baa57e8f6a263ad6e87ce9b6e0af8ba68e3"},"cell_type":"markdown","source":"Sooo, depending on the tip from Recruit Restaurant Visitor Forecasting holiday hack, we should treat holidays as Saturday and if there is a weekday before then that day as Friday and If the day after holiday is weekday ,treat the day after holiday as Monday. I will try it later because from the plots this does not feel correct.  "},{"metadata":{"_cell_guid":"47fa9c2b-df5b-4c1c-9f79-fb104e5c1654","_uuid":"36a28bc5af7365647f0c002d74bc9fae71326442","trusted":true},"cell_type":"code","source":"## Now we will start the data prepration for modelling\n### First we will convert some features to categorical features\n# get_dummie variables\nfeatures = pd.get_dummies(air_hpg_com_reduce_df)\n\n# Display the first 5 rows of the last 12 columns\nfeatures.head(5)","execution_count":63,"outputs":[]},{"metadata":{"_cell_guid":"f667b80a-be4c-4c58-9571-40023fb949d0","collapsed":true,"_uuid":"4f6a43053c65be3862318712e79d10cf007a55db","trusted":true},"cell_type":"code","source":"## Convert data to arrays\n# Labels are the values we want to predict\nlabels = np.array(features['visitors'])\n\n# Remove the labels from the features\n# axis 1 refers to the columns\nfeatures= features.drop('visitors', axis = 1)\n\n# Saving feature names for later use\nfeature_list = list(features.columns)\n\n# Convert to numpy array\nfeatures = np.array(features)","execution_count":64,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"43482cb5d06d93c76e496fa9a72596de16f06a13"},"cell_type":"code","source":"# Creatimg training and test set\n# Using Skicit-learn to split data into training and testing sets\nfrom sklearn.model_selection import train_test_split\n\n# Split the data into training and testing sets\ntrain_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size = 0.25, random_state = 42)","execution_count":71,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"68a5e9444c2d97ed2c90113c5e736784f81afde4"},"cell_type":"code","source":"print('Training Features Shape:', train_features.shape)\nprint('Training Labels Shape:', train_labels.shape)\nprint('Testing Features Shape:', test_features.shape)\nprint('Testing Labels Shape:', test_labels.shape)\n","execution_count":72,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0504b180149c25d97f3dae8f0865f94428c66c75"},"cell_type":"code","source":"# https://towardsdatascience.com/random-forest-in-python-24d0893d51c0\n# Import the model we are using\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Instantiate model with 1000 decision trees\nrf = RandomForestRegressor(n_estimators = 100, max_depth=3, random_state = 42)\n\n# Train the model on training data\nrf.fit(train_features, train_labels);","execution_count":73,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3074abfec2fa26b8bd561325a17c33b5938737c8"},"cell_type":"code","source":"# Use the forest's predict method on the test data\npredictions = rf.predict(test_features)\n\n# Calculate the absolute errors\nerrors = abs(predictions - test_labels)\n\n# Print out the mean absolute error (mae)\nprint('Mean Absolute Error:', round(np.mean(errors), 2), 'degrees.')","execution_count":74,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"08f65ae0f4fdbb55f0d59c39e381b91c90741dd8"},"cell_type":"code","source":"test_df = pd.DataFrame(test_features, columns=feature_list)\nprint(test_df.head())\ntest_values = pd.DataFrame({'Prediction':predictions,'Truth':test_labels})\nprint(test_values.head())","execution_count":84,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5a07d624c5f077c0121555ad52bd669fa51cd429"},"cell_type":"code","source":"res_test = pd.concat([test_df, test_values], axis=1)\nprint(res_test.head(5))","execution_count":86,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"93a79045944bad70721697222c7655ecf2ca9985"},"cell_type":"code","source":"from sklearn import *\ndef RMSLE(y, pred):\n    return metrics.mean_squared_error(y, pred)**0.5\n\ndef plot_actual_predicted(actual, predicted):\n    print('RMSE: ', RMSLE(actual, predicted))\n    tmp = pd.DataFrame({'actual': actual, 'predicted': predicted}).sort_values(['actual'])\n    plt.scatter(range(tmp.shape[0]), tmp['predicted'], color='green')\n    plt.scatter(range(tmp.shape[0]), tmp['actual'], color='blue')\n    plt.show()\n    del tmp\n\n","execution_count":93,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a8d192486529d273ac088e0f8bf59e39b1ef144b"},"cell_type":"code","source":"plot_actual_predicted(test_labels, predictions)","execution_count":94,"outputs":[]},{"metadata":{"_cell_guid":"afbcd712-3a7c-422f-a6f5-84b842dafdc1","collapsed":true,"_uuid":"00d43834f2fd8db804393013152a5533f7029c22","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}