{"metadata": {"language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "nbconvert_exporter": "python", "mimetype": "text/x-python", "pygments_lexer": "ipython3", "name": "python", "file_extension": ".py", "version": "3.6.1"}, "kernelspec": {"display_name": "Python 3", "name": "python3", "language": "python"}}, "nbformat": 4, "nbformat_minor": 0, "cells": [{"metadata": {"_uuid": "707305fa25dc8d7ffc14fbed1a0e90a0cdf35354", "collapsed": false, "_cell_guid": "2f307a38-9eb1-4754-921a-cc130fb5bf76", "_execution_state": "idle"}, "source": "Label the variability, or unforecastability\n---\ntaking the ratio var/mean  , and an efficiency ratio that is know when you pass 0.75 you have somewhere  bottlenecks in the production\n\nSecond find the X-labels explaining for each group the OLS\n----\nregressing each group apart , omitting irrelevant columns, you endup with a list of labels explaining or forecasting relatively perfect (LB 0.53) the testing time\n\nThe loss of information is due to the construction of the splitted train-test set. It was irrelevant for this question to make this splitted setup. It was enough imho to give the complete set and try to maximize the correlation. It would even helped since you could have added autoregressive effect, and that was something we couldn't test here...\n\nThird: OLS these relevant columns, you endup with LB 0.53\n----\nthis solution is very close to the 0.55, but it doesn't matter imho how high you get. Its the only solution describing exactly what columns have significant impact on the forecast of the test time. ", "outputs": [], "cell_type": "markdown", "execution_count": null}, {"metadata": {"_uuid": "eb5ac3c7bc5426d0cc485215501dc325f1c3f71c", "trusted": false, "_cell_guid": "01ad2fe9-92ab-46bb-a156-94ef4e793fc9", "_execution_state": "idle"}, "source": "import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport gc\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\npal = sns.color_palette()\n\ntrain = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\nprint(train.shape,test.shape)\ntrain['tt']=0\ntrain['y']=np.log(np.log(train['y']))\ntest['tt']=1\ntotal=train.append(test)\ntotal=total.sort_values('ID')\ntotal['group']=total['ID']/426\ntotal=total.set_index('ID')\n\ntotal['group']=total['group'].round(0)\ntotal=total.drop(['X214', 'X239', 'X53', 'X199', 'X134', 'X147', 'X222', 'X48', 'X119', 'X227', 'X146', 'X226', 'X326', 'X360', 'X382', 'X216', 'X62', 'X262', 'X67', 'X254', 'X279', 'X364', 'X71', 'X84', 'X385', 'X60', 'X293', 'X330', 'X296', 'X299', 'X44', 'X35', 'X37', 'X58', 'X39', 'X76'],axis=1)", "outputs": [], "cell_type": "code", "execution_count": 10}, {"metadata": {"_uuid": "efb1b07d5707eaf7920767df6f4cdfa256275991", "collapsed": false, "_cell_guid": "ea6e58b2-d76a-4358-a647-4bfd61672783", "_execution_state": "idle"}, "source": "Group data per 'hour'\n----\nwe estimate that there are 8000cars per day in a production so taking samples of 400 reflects a production of an hour", "outputs": [], "cell_type": "markdown", "execution_count": null}, {"metadata": {"_uuid": "70e5999193e6bdad812843f8d5e56703f8fe3272", "collapsed": false, "trusted": false, "_cell_guid": "3f42fc99-37ad-484a-b561-557d4dc67b67", "_execution_state": "idle"}, "source": "groepX0=total.groupby(['X0','group'])['y'].describe().fillna(method='bfill')\ngroepX0['eff']=groepX0['std']/groepX0['mean']\ngroepX0['eff2']=groepX0['eff']*groepX0['std']\n\n\ndef clust(x):\n    kl=0\n    if x<0.00024:   # low variability cluster\n        kl=1\n    if x>0.000239 and x<0.000438: # moderate variability cluster, process with short adjustments\n        kl=2\n    if x>0.0004379: # high variability class, process times with long outages, failures of tests\n        kl=4\n    return kl\ngroepX0['clust']=groepX0['eff2'].map(clust)\ngroepX0\n                                   ", "outputs": [], "cell_type": "code", "execution_count": 11}, {"metadata": {"_uuid": "ea8f5ca6019581f8365b70075d4ecee3bb88e3c5", "collapsed": false, "_cell_guid": "f046e604-d7e5-4689-a904-c12a4e3fb336", "_execution_state": "idle"}, "source": "Cluster 1 = narrow forecast error or small error group 2800 cars\n----\nCluster 2 = more variable\n----\nCluster 4 swinging group\n----\n10% of the cars causes the unpredictability\n\n", "outputs": [], "cell_type": "markdown", "execution_count": null}, {"metadata": {"_uuid": "7dd55f124529b38800a41bdaf93f1f76e5ff66e2", "collapsed": false, "trusted": false, "_cell_guid": "a3956992-bc28-4f1e-af99-14adaa97825a", "_execution_state": "idle"}, "source": "#compare cluster 4 with 1   \n#first merge data\ntotal=pd.merge(total,groepX0[['mean','min','25%','50%','75%','std','clust']], how='outer', left_on=['X0','group'],suffixes=('', '_X0'), right_index=True)\n\n\n", "outputs": [], "cell_type": "code", "execution_count": 12}, {"metadata": {"_uuid": "04dfd4d4041eeaac535abe8db945e4726880a16d", "collapsed": false, "_cell_guid": "15697d9e-c446-425c-afa0-1f79e0103d26", "_execution_state": "idle"}, "source": "what explains variance of cluster 4,2,1?\n-----\n\ntrying to forecast cluster 4", "outputs": [], "cell_type": "markdown", "execution_count": null}, {"metadata": {"_uuid": "33f98f265954f483ab266989730c847751b554cf", "collapsed": false, "trusted": false, "_cell_guid": "c53b1419-9fcb-48ec-95c0-55858e05ac6f", "_execution_state": "idle"}, "source": "from collections import Counter\ndef todrop_col(df,tohold):\n    # use todrop_col(dataframe,['listtohold'])\n    # Categorical features\n    df.replace([np.inf, -np.inf], np.nan).fillna(value=-1)\n    \n    cat_cols = []\n    for c in df.columns:\n        if df[c].dtype == 'object':\n            cat_cols.append(c)\n    #print('Categorical columns:', cat_cols)\n    \n    \n    # Constant columns\n    cols = df.columns.values    \n    const_cols = []\n    for c in cols:   \n        if len(df[c].unique()) == 1:\n            const_cols.append(c)\n    #print('Constant cols:', const_cols)\n    \n    \n    # Dublicate features\n    d = {}; done = []\n    cols = df.columns.values\n    for c in cols:\n        d[c]=[]\n    for i in range(len(cols)):\n        if i not in done:\n            for j in range(i+1, len(cols)):\n                if all(df[cols[i]] == df[cols[j]]):\n                    done.append(j)\n                    d[cols[i]].append(cols[j])\n    dub_cols = []\n    for k in d.keys():\n        if len(d[k]) > 0: \n            # print k, d[k]\n            dub_cols += d[k]        \n    #print('Dublicates:', dub_cols)\n    \n    kolom=list(set(dub_cols+const_cols+cat_cols))\n    kolom=[k for k in kolom if k not in tohold]\n    \n    return kolom\n\ndef tree_col(df,splitcol,splitval,groupcol):\n    #use tree_col(dataframe,column that splits,vale to split, column that groups)\n    #sklear feature selection\n    import sklearn    \n    from sklearn.svm import LinearSVC\n    from sklearn.feature_selection import SelectFromModel\n    from sklearn.ensemble import ExtraTreesClassifier\n    \n    tabel = df[df[splitcol]==splitval]\n    label = tabel[groupcol].round(0)\n    feat = df.columns  \n    clf = ExtraTreesClassifier()\n    clf = clf.fit(tabel[feat], label)\n    model = SelectFromModel(clf, prefit=True)\n    interesting_cols = model.transform(tabel[feat])\n    #print('Treeclassifier cols',interesting_cols.shape)\n    tabel2=pd.DataFrame(interesting_cols,index=tabel.index)\n    feat2=tabel2.columns\n    feat3=[]\n    for ci in feat:\n        for cj in feat2:\n            if all(tabel[ci] == tabel2[cj]):\n                feat3.append(ci) \n    #print('interesting Treecolumns',feat3)\n    return feat3\n\ndef corr_col(df,explvar):\n    \n    corr4=df.corr()\n    explstd=corr4[explvar]\n    absexplstd=explstd.abs()\n    expl_std4=[k for k in corr4.columns if absexplstd.loc[k]>0.1]\n    return expl_std4\n\ndef plotxy(titel,xlabel,ylabel,toty,y_pred,y_test,y_pred2,y_test2):\n    plt.figure(figsize=(20,5))\n\n    plt.subplot(1,5,1)\n\n    plt.title(titel+xlabel+ylabel)\n    plt.plot([1.4,1.8], [1.4,1.8], color='g', alpha=0.3)\n    plt.scatter(x=toty, y=y_pred, marker='.', alpha=0.5)\n    plt.scatter(x=toty, y=y_pred2, marker='.', alpha=0.5,color='g')\n    plt.scatter(x=[np.mean(toty)], y=[np.mean(y_pred)], marker='o', color='red')\n    plt.xlabel(xlabel); plt.ylabel(ylabel)\n    \n    plt.subplot(1,5,2)\n    sns.distplot(toty, kde=False, color='g')\n    sns.distplot(y_pred, kde=False, color='r')\n    plt.title('Distr.'+xlabel )\n\n    plt.subplot(1,5,3)\n    sns.distplot(toty, kde=False, color='g')\n    sns.distplot(y_test, kde=False, color='b')\n    plt.title('Distr'+ylabel)\n\n    plt.subplot(1,5,4)\n    sns.distplot(toty, kde=False, color='g')\n    sns.distplot(y_pred2, kde=False, color='r')\n    plt.title(' Distr. 2'+xlabel)\n\n    plt.subplot(1,5,5)\n    sns.distplot(toty, kde=False, color='g')\n    sns.distplot(y_test2, kde=False, color='b')\n    plt.title('Distr2'+ylabel)\n    \n\n", "outputs": [], "cell_type": "code", "execution_count": 22}, {"metadata": {"_uuid": "0c91a88ac3e134b878fc781b7e7544e46134f4b9", "collapsed": false, "trusted": false, "_cell_guid": "f34dea77-3677-47ac-8f09-5ea93464a2d1", "_execution_state": "idle"}, "source": "y_pred_tot=pd.DataFrame()\n#select cluster 4  merge ,split, search constant columns\n# select columns correlating with variability\nfor clustval in [4,2,1]:\n    total4=total[total['clust']==clustval]\n\n    dropcol=todrop_col(total4,['clust','tt'])\n    total4=total4.drop(dropcol,axis=1)\n    total4['intercept']=1\n    total41=total4[total4['tt']==1]\n    total40=total4[total4['tt']==0] \n\n    #tree classifier columns tree related with group\n    treeko4=tree_col(total4,'tt',0,'group')\n\n    #columns correlated with standarddeviation\n    expl_std4=corr_col(total40,'std')\n\n    expl_std = list(set(expl_std4+treeko4))\n    expl_std=[k for k in expl_std if k not in ['y','mean','min','std','25%','50%','75%']]\n    import statsmodels.formula.api as sm\n    #ols\n    \n    res = sm.OLS(total40.y,total40[expl_std]).fit()\n    print('Columns explaining variance ',clustval)\n    tval=pd.DataFrame(res.tvalues)\n    ttval=tval[tval[0]>2].append(tval[tval[0]<-2])\n    print('signific columns',[kt for kt in ttval.index])\n    y_pred = res.predict(total40[expl_std])\n    y_test = res.predict(total41[expl_std])\n    #print(pd.DataFrame(y_test ).isnull())\n    #print(y_test)\n    if clustval==4:\n        submis=pd.DataFrame(y_test)\n        y_pred_tot=y_pred_tot.append(submis)\n    if clustval==2:\n        submis=pd.DataFrame(y_test)\n        y_pred_tot=y_pred_tot.append(submis)\n    if clustval==1:\n        submis=pd.DataFrame(y_test)\n        y_pred_tot2=y_pred_tot.append(submis)\n        #print(y_pred_tot)\n        y_pred_tot2=np.exp(np.exp(y_pred_tot2))\n        y_pred_tot2.to_csv('stacked-models2.csv')    #0.48\n    res2 = sm.OLS(total40.y,total40[['intercept','min','std'] ]).fit()\n    print(res2.summary())\n    \n    y_pred2 = res2.predict(total40[['intercept','min','std']])\n    y_test2 = res2.predict(total41[['intercept','min','std']])\n    if clustval==1:\n        submis=pd.DataFrame(y_test2)\n        y_pred_tot=y_pred_tot.append(submis)\n        #print(y_pred_tot)\n        y_pred_tot=np.exp(np.exp(y_pred_tot))        \n        y_pred_tot.to_csv('stacked-models1.csv')  #0.39 the mix is worse althoug graph is better\n\n\n\n    plotxy(' fit ',' train predicted',' test predicted',total40.y,y_pred,y_test,y_pred2,y_test2)\n\n", "outputs": [], "cell_type": "code", "execution_count": 24}, {"metadata": {"_uuid": "88103522eb504954c87a885f70266308a79002c1", "collapsed": false, "trusted": false, "_cell_guid": "3c795feb-b148-4edf-b106-31d9691b20cd", "_execution_state": "idle"}, "source": "#print(y_pred_tot)", "outputs": [], "cell_type": "code", "execution_count": 15}, {"metadata": {"_uuid": "659baab3357249910714e0d11dcf62a0e01f38e8", "collapsed": false, "trusted": false, "_cell_guid": "b38df0a7-09db-48b1-8363-2b23e57838b0", "_execution_state": "idle"}, "source": "import statsmodels.formula.api as sm\n\n#split data again\ntotal_0=total[total['tt']==0] # train data\ntotal_1=total[total['tt']==1] #test data\ntotal_0['tt']=1\n\n#res = sm.ols(formula=\"y ~ X115+X116+X144+X157+X220+X27+X301+X313+X315+X334+min+std+clust\",data=total).fit()\nres = sm.ols(formula=\"y ~ X186+X187+X204+X205+X142+X263+X156+X157+X158+X51+X168+X171+X136\",data=total_0).fit()\nprint(res.summary())\n  \n\ny_pred = res.predict(total_0)\nprint('check size pred',y_pred.shape,total_0.shape)\n\ny_pred1 = res.predict(total_1)\nprint('chekc size pred1',y_pred1.shape,total_1.shape)\nprint(y_pred1.head())\n\nsub = pd.DataFrame()\nsub['ID'] = y_pred1.index\nsub['y'] = y_pred1\nsub.to_csv('submission.csv', index=False)\n\n\nplt.figure(figsize=(16,4))\n\n\n\nplt.subplot(1,4,2)\nsns.distplot(total_0.y, kde=False, color='g')\nsns.distplot(y_pred, kde=False, color='r')\nplt.title('Distr. of train and pred. train')\n\nplt.subplot(1,4,3)\nsns.distplot(total_0.y, kde=False, color='g')\nsns.distplot(y_pred1, kde=False, color='b')\nplt.title('Distr. of train and pred. test')\n\n#kolom4=['X215', 'X187', 'X205', 'X186', 'X118', 'X157', 'X156', 'X275', 'X204', 'X51']\n#kolom1=['tt','X313', 'X157', 'X316', 'X156', 'X301', 'X158', 'X286', 'X118', 'X142', 'X263', 'X54', 'X315', 'X18', 'X314', 'X29', 'X125', 'X351', 'group', 'X275', 'X232']\n#kolom2=['X187', 'X186', 'X118', 'X272', 'X171', 'X314', 'X194', 'X276', 'X232', 'X311', 'X157', 'X136', 'X168', 'X156', 'X54', 'X29', 'X162', 'X313', 'X127', 'X352', 'X148', 'X261', 'X316']\nkolom4=['X168', 'X205', 'X171', 'X204', 'X130', 'X18', 'X275', 'X156', 'X128', 'X157', 'X229']\nkolom2=['X358', 'X187', 'X186', 'X194', 'X127', 'X313', 'X316']\nkolom1=['X156', 'X313', 'X314', 'X118', 'X316', 'X136', 'X315', 'X157', 'X142', 'X18', 'X158', 'X54', 'X301', 'group', 'X125', 'X351', 'X225']\nkolom=list(set(kolom1+kolom2+kolom4))\nres4_mod=sm.OLS(total_0.y,total_0[kolom],1).fit()\nprint(res4_mod.summary())\ny_pred2 = res4_mod.predict(total_1[kolom])\ny_pred2t = res4_mod.predict(total_0[kolom])\n\nprint(y_pred2.head())\n#prediction\n\nplt.subplot(1,4,4)\nsns.distplot(total_0.y, kde=False, color='g')\nsns.distplot(y_pred2, kde=False, color='b')\nplt.title('Distr. of train and pred. test')\n\nplt.subplot(1,4,1)\nplt.title('True vs. Pred. train')\nplt.plot([80,265], [80,265], color='g', alpha=0.3)\nplt.scatter(x=total_0.y, y=y_pred2t, marker='.', alpha=0.5)\nplt.scatter(x=[np.mean(train.y)], y=[np.mean(y_pred2t)], marker='o', color='red')\nplt.xlabel('Real train'); plt.ylabel('Pred. train')\n\n\nprint(total_1.shape)\n\n\nplt.figure(figsize=(18,1))\nplt.plot( total_0.y[:200], color='r', linewidth=0.7)\nplt.plot(y_pred[:200], color='g', linewidth=0.7)\nplt.title('First 200 true and pred. trains')\n\nfrom sklearn.metrics import r2_score, mean_squared_error\nprint('Mean error =', np.mean(total_0.y - y_pred1))\nprint('Train r2 =', r2_score(total_0.y, y_pred1))\nprint('Train r2 =', r2_score(total_0.y, y_pred2))\nprint('Mean error =', np.mean(total_0.y - y_pred2))\ny_pred2.to_csv('stacked-models3.csv')\n\n#res = sm.ols(formula=\"y ~ X3+X6+mean\", data=total0).fit()  188 R2=1\n#res = sm.ols(formula=\"y ~ X3+X6+mean\", data=total1).fit()  2000 R2=0.79  X3,X6 twee groepjes relevant\n#res = sm.ols(formula=\"y ~ X3+X6+mean\", data=total2).fit()  639 R2=0.495\n#res = sm.ols(formula=\"y ~ X3+X6+mean\", data=total4).fit()  400 R2=0.29\n#print(res.summary())\n", "outputs": [], "cell_type": "code", "execution_count": 16}, {"metadata": {"_uuid": "08490c075d70bfb4a1ed23c4b1770863364e76c5", "collapsed": false, "trusted": false, "_cell_guid": "f0193f81-9eab-4cf0-95c3-8e038a2a215c", "_execution_state": "idle"}, "source": "y_pred2.columns=['y']\ny_pred2=np.exp(np.exp(y_pred2))\ny_pred2", "outputs": [], "cell_type": "code", "execution_count": 25}, {"metadata": {"_uuid": "e7887a56a84af52b2c1497f7d4b284f3aaa13c29", "collapsed": false, "trusted": false, "_cell_guid": "dd002a03-1f5c-443d-b394-33ab46ba2d78", "_execution_state": "idle"}, "source": "y_pred2.to_csv('stacked-models3.csv')", "outputs": [], "cell_type": "code", "execution_count": 26}, {"metadata": {"_uuid": "d3c0d3a0527d8c2e431316bf9a4fe3906a9ae0fc", "collapsed": false, "trusted": false, "_cell_guid": "20da8cfe-e533-443e-b61f-9c0ee25d0807", "_execution_state": "idle"}, "source": "\n#analyse table\nfrom collections import Counter\n\ndef detect_outliers(df,n,features):\n    # Categorical features\n    cat_cols = []\n    for c in df.columns:\n        if df[c].dtype == 'object':\n            cat_cols.append(c)\n    print('Categorical columns:', cat_cols)\n    \n    \n    # Constant columns\n    cols = df.columns.values    \n    const_cols = []\n    for c in cols:\n        if len(df[c].unique()) == 1:\n            const_cols.append(c)\n    print('Constant cols:', const_cols)\n    \n    \n    # Dublicate features\n    d = {}; done = []\n    cols = df.columns.values\n    for c in cols:\n        d[c]=[]\n    for i in range(len(cols)):\n        if i not in done:\n            for j in range(i+1, len(cols)):\n                if all(df[cols[i]] == df[cols[j]]):\n                    done.append(j)\n                    d[cols[i]].append(cols[j])\n    dub_cols = []\n    for k in d.keys():\n        if len(d[k]) > 0: \n            # print k, d[k]\n            dub_cols += d[k]        \n    print('Dublicates:', dub_cols)\n\n    #sklear feature selection\n    import sklearn    \n    from sklearn.svm import LinearSVC\n    from sklearn.feature_selection import SelectFromModel\n    from sklearn.ensemble import ExtraTreesClassifier\n\n    feat=[x for x in features if x not in cat_cols]\n    feat=[x for x in feat if x not in const_cols]\n    feat=[x for x in feat if x not in dub_cols]\n\n    print(feat)\n    \n    \n    tabel = df[df['tt']==0]\n    label = tabel['group'].round(0)\n    clf = ExtraTreesClassifier()\n    clf = clf.fit(tabel[feat], label)\n    model = SelectFromModel(clf, prefit=True)\n    interesting_cols = model.transform(tabel[feat])\n    print('Treeclassifier cols',interesting_cols.shape)\n    tabel2=pd.DataFrame(interesting_cols,index=tabel.index)\n    feat2=tabel2.columns\n    feat3=[]\n    for ci in feat:\n        for cj in feat2:\n            if all(tabel[ci] == tabel2[cj]):\n                feat3.append(ci) \n    print('interesting Treecolumns',feat3)\n    \n    lsvc = LinearSVC(C=0.01, penalty=\"l1\", dual=False).fit(tabel[feat],label)\n    model_2 = SelectFromModel(lsvc, prefit=True)\n    interesting_cols = model_2.transform(tabel[feat])\n    print('LinearSVC cols',interesting_cols.shape)\n    tabel2=pd.DataFrame(interesting_cols,index=tabel.index)\n    feat2=tabel2.columns\n    feat4=[]\n    for ci in feat:\n        for cj in feat2:\n            if all(tabel[ci] == tabel2[cj]):\n                feat4.append(ci)        \n    print('interesting SVCcolumns',feat4)\n    print('mixed',list(set(feat3+feat4)))    \n\n\n    # Outlier detection     \n    \"\"\"\n    Takes a dataframe df of features and returns a list of the indices\n    corresponding to the observations containing more than n outliers according\n    to the Tukey method.\n    \"\"\"\n    outlier_indices = []\n    \n    # iterate over features(columns)\n    for col in feat:\n        # 1st quartile (25%)\n        Q1 = np.percentile(df[col], 25)\n        # 3rd quartile (75%)\n        Q3 = np.percentile(df[col],75)\n        # Interquartile range (IQR)\n        IQR = Q3 - Q1\n        \n        # outlier step\n        outlier_step = 1.5 * IQR\n        \n        # Determine a list of indices of outliers for feature col\n        outlier_list_col = df[(df[col] < Q1 - outlier_step) | (df[col] > Q3 + outlier_step )].index\n        \n        # append the found outlier indices for col to the list of outlier indices \n        outlier_indices.extend(outlier_list_col)\n        \n    # select observations containing more than 2 outliers\n    outlier_indices = Counter(outlier_indices)        \n    multiple_outliers = list( k for k, v in outlier_indices.items() if v > n )\n    \n    return multiple_outliers   \n\nkolom=[x for x in total.columns if x not in ['ID']]\n# detect outliers from Age, SibSp , Parch and Fare\n\n\n#Outliers = detect_outliers(total0.replace([np.inf, -np.inf], np.nan).fillna(value=-1),2,kolom)\nOutliers = detect_outliers(total1.replace([np.inf, -np.inf], np.nan).fillna(value=-1),2,kolom)\nOutliers = detect_outliers(total2.replace([np.inf, -np.inf], np.nan).fillna(value=-1),2,kolom)\nOutliers = detect_outliers(total4.replace([np.inf, -np.inf], np.nan).fillna(value=-1),2,kolom)\n\ntotal.loc[Outliers_to_drop] # Show the outliers rows\n\n", "outputs": [], "cell_type": "code", "execution_count": 19}]}