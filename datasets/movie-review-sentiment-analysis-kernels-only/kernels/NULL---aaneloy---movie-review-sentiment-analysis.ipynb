{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5005ad17491c2ebc23eef68220339fe02211199f"},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn import svm\nfrom scipy import sparse\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import accuracy_score,make_scorer\nfrom sklearn.model_selection import StratifiedShuffleSplit\n\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem.wordnet import WordNetLemmatizer\n\nfrom imblearn.over_sampling import RandomOverSampler\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.over_sampling import ADASYN","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"../input/movie-review-sentiment-analysis-kernels-only/train.tsv\", delimiter = '\\t')\ntest = pd.read_csv(\"../input/movie-review-sentiment-analysis-kernels-only/test.tsv\", delimiter = '\\t')\nsubmission = pd.read_csv(\"../input/movie-review-sentiment-analysis-kernels-only/sampleSubmission.csv\",sep='delimiter', header=None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"be6b11233dfd3bf9989aa8e549d9f2f71e8eabd7"},"cell_type":"code","source":"train.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f4790f76db72a55c8525be5029047b4fd26d6b9e"},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d098657a0586fcd95b709ab9b610296c6020ade6"},"cell_type":"code","source":"y_train = train['Sentiment']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a54c09426b51dfca880ac52108332d11ce04431c"},"cell_type":"code","source":"sns.countplot(y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ac7cf46817ac1aeaa13ade5bfbdfb494095ff985"},"cell_type":"code","source":"class LemmaTokenizer(object):\n    def __init__(self):\n        self.wnl = WordNetLemmatizer()\n    def __call__(self, doc):\n        return [WordNetLemmatizer().lemmatize(w) for w in word_tokenize(doc)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e2df2b09240d4e190e8c3858f0194f2f5ee8c8f5"},"cell_type":"code","source":"import nltk\nnltk.download('punkt')\nnltk.download('wordnet')\n\nfrom nltk import word_tokenize,sent_tokenize\nvectorizer_w = TfidfVectorizer(sublinear_tf = True, max_df = 0.5, stop_words = None,ngram_range = (1,3), analyzer = 'word', encoding = 'utf-8', tokenizer = LemmaTokenizer())\nvectorizer_c = TfidfVectorizer(sublinear_tf = True, max_df = 0.5, stop_words = None,ngram_range = (2,6), analyzer = 'char', encoding = 'utf-8', tokenizer = LemmaTokenizer())\nX_train_w = vectorizer_w.fit_transform(train['Phrase'])\nX_train_c = vectorizer_c.fit_transform(train['Phrase'])\nX_test_w = vectorizer_w.transform(test['Phrase'])\nX_test_c = vectorizer_c.transform(test['Phrase'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8d2eb552e8a3984b1627c70a6f8cdd333d722bff"},"cell_type":"code","source":"X_train = sparse.hstack([X_train_w, X_train_c])\nX_test = sparse.hstack([X_test_w, X_test_c])\n\n#Tried Oversampling methods using imbalanced-learn API(http://contrib.scikit-learn.org/imbalanced-learn/stable/api.html)\n#However Oversampling did not help\nros = RandomOverSampler(random_state=42)\nada = ADASYN(random_state=152)\n#X_train_ros, y_train_ros = ros.fit_sample(X_train, y_train)\n#X_train_ada, y_train_ada = ada.fit_sample(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5e686dffddd0e3029f2b8b1f9e420481f2d770fd"},"cell_type":"code","source":"print(\"Number of samples in Train dataset i.e. n_samples: %d, Number of features in Train dataset i.e. n_features: %d\" % X_train.shape)\nprint(\"Number of samples in Test dataset i.e. n_samples: %d, Number of features in Test dataset i.e. n_features: %d\" % X_test.shape)\nprint(\"\\n\")\n#print(\"Number of samples in Resample Train dataset(Ramdom Sampler) i.e. n_samples: %d, Number of features in Train dataset i.e. n_features: %d\" % X_train_ros.shape)\n#print(\"Number of samples in Resample Train dataset(ADASYN) i.e. n_samples: %d, Number of features in Train dataset i.e. n_features: %d\" % X_train_ada.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e33bcd82924cab6591f440cf6e7343ca9ec6f18f"},"cell_type":"code","source":"clf = MultinomialNB()\nclf.fit(X_train,y_train)\ny_pred_nb = clf.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d340518a363561cb639227a8933ca51b8e1b72c7"},"cell_type":"code","source":"submission.to_csv(\"sampleSubmission.csv\", index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"154002fae5711cf7b38ccd8d649ad73016517267"},"cell_type":"code","source":"lclf = LogisticRegression(solver = 'saga',multi_class = 'multinomial', max_iter = 500, \n                          C = 4, random_state = 42, verbose = 10, class_weight = 'balanced')\n\n#parameters = {'C':[2 , 4] }\n#scorer = make_scorer(accuracy_score)\n#cv = StratifiedShuffleSplit(2, random_state = 62)\n#grid_obj = GridSearchCV(lclf, param_grid=parameters, cv = cv, scoring=scorer, n_jobs=-1, verbose=10)\n#grid_fit = grid_obj.fit(X_train, y_train)\n#best_clf = grid_fit.best_estimator_\n\npredictions = (lclf.fit(X_train, y_train)).predict(X_test)\n#best_predictions = best_clf.predict(X_tes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cb929f63634bbec755f347e9d9b7c74a5c46b8b0"},"cell_type":"code","source":"#Standard NLP Pre-Processing\nX_train = train['Phrase']\nX_test = test['Phrase']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"84d5768f040152adc3f148d8274afcff0a0e8edd"},"cell_type":"code","source":"X_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d2fcf6139b0f857515b4b93b3af3af0b393cb066"},"cell_type":"code","source":"#NORMALIZATION - Converting to lower case\nX_train_l = X_train.str.lower()\nprint(X_train_l[0])\nX_train_l.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"92e838a5f26c80258b1b56c83b1a4448d855b813"},"cell_type":"code","source":"#NORMALIZATION - Removing Punctuation marks\nimport re\ndef punc_rem(y):\n    return re.sub(r\"[^a-zA-Z0-9]\", \" \", y)\nX_train_p = X_train_l.apply(lambda x: punc_rem(x))\nprint(X_train_p[0])\nX_train_p.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cf11df52502fba3300fe528cb2ec529d389342eb"},"cell_type":"code","source":"#TOKENIZATION - Word & Setence tokenizers\nfrom nltk.tokenize import word_tokenize\nfrom nltk.tokenize import sent_tokenize\n\nX_train_wt = X_train_p.apply(lambda x : word_tokenize(x))\nX_train_st = X_train_p.apply(lambda x : sent_tokenize(x))\n\nprint(X_train_wt[0])\nprint(X_train_wt.head())\nprint(X_train_st[0])\nprint(X_train_st.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7734807db308301cd90039f659e7733cfea46429"},"cell_type":"code","source":"#STOPWORDS Removal\n\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nprint(stopwords.words('english'))\ndef stop_words(x):\n    return [i for i in x if i not in stopwords.words('english')]\nX_train_sw = X_train_wt.apply(lambda x : stop_words(x))\nprint(X_train_sw[0])\nprint(X_train_sw[3])\nprint(X_train_sw.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"27cac04c108fa79c53676f4e9aa17ba4cb04bbee"},"cell_type":"code","source":"#POS (Parts Of Speech Tagging) & NER (Named Entity Recognition)\n\nimport nltk\nnltk.download('averaged_perceptron_tagger')\ndef postag(x):\n    return nltk.pos_tag(x)\nX_train_pos = X_train_wt.apply(lambda x: postag(x))\nprint(X_train_pos[0])\nprint(X_train_pos[3])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c9e63534ee7fbaad4ec4811b1e7a23c3aed74a81"},"cell_type":"code","source":"#nltk.help.upenn_tagset('CC')\nnltk.download('tagsets')\nfor i in X_train_pos[0]:\n    print(\"{}: \".format(i))\n    (nltk.help.upenn_tagset(i[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"48dcfc509b29f97bd649f6f60173492064038605"},"cell_type":"code","source":"nltk.corpus.stopwords.readme() #https://www.nltk.org/book/ch05.html","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cd7fd4171f298d05f3fa97a3035359a0f2a61215"},"cell_type":"code","source":"nltk.download('maxent_ne_chunker')\nnltk.download('words')\ndef ner(x):\n    return nltk.ne_chunk(x)\n\nX_train_ner = X_train_pos.apply(lambda x: ner(x))\n\nprint(X_train_ner[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a71a84126ca8d6a538a79a5a11594274861f823f"},"cell_type":"code","source":"print(nltk.ne_chunk(nltk.pos_tag(word_tokenize(\"Bangladesh is a great country\"))))\n#https://www.nltk.org/book/ch07.html","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f4af72f1281cc6f7695eb5ab57aabda203d563e5"},"cell_type":"code","source":"#CFG - Context Free Grammer\n\nprint(X_train[1])\nprint(X_train_wt[1])\nprint(X_train_pos[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5c2ed64fed89f32afe2097aaa13556a139e74b88"},"cell_type":"code","source":"nltk.help.upenn_tagset('JJ')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c4669e7aec01ecc69ef9d709b51714cf37120325"},"cell_type":"code","source":"custom_grammer = nltk.CFG.fromstring(\"\"\"\nS -> NP VP\nPP -> P NP\nNP -> Det N | Det N PP \nVP -> V NP | VP PP | JJ\nDet -> 'the'|'a'\nN -> 'series'|'escapades'|'adage'|'goose'\nV -> 'demonstrating'|'is'\nJJ -> 'good'\nP -> 'that'|'for'|'of'|'what'\n\"\"\")\n\ncustom_parser = nltk.ChartParser(custom_grammer)\nprint(custom_parser.parse(X_train_wt[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2506a478f0d967efc8079eb7e0e48c0285607ee0"},"cell_type":"code","source":"help(nltk.ChartParser)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6b2ded35928b8521cfbb9560967639f299070145"},"cell_type":"code","source":"for custom_tree in custom_parser.parse(X_train_wt[1]):\n    print(\"Bangladesh\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e380f97566ecf8947dccf65f6225c68b4fe31660"},"cell_type":"code","source":"# Define a custom grammar\nmy_grammar = nltk.CFG.fromstring(\"\"\"\nS -> NP VP\nPP -> P NP\nNP -> Det N | Det N PP | 'I'\nVP -> V NP | VP PP\nDet -> 'an' | 'my'\nN -> 'elephant' | 'pajamas'\nV -> 'shot'\nP -> 'in'\n\"\"\")\nparser = nltk.ChartParser(my_grammar)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"77f5bc06ae5b4b67853c1a76396551bf1f971bf2"},"cell_type":"code","source":"nltk.pos_tag(sentence)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2b9e382a0fd5be3b6aaa81632d2b08cf8c9b1bb7"},"cell_type":"code","source":"# Parse a sentence\nsentence = word_tokenize(\"I shot an elephant in my pajamas\")\nprint(type(sentence))\nnltk.pos_tag(sentence)\nprint(parser.parse(sentence))\nfor tree in parser.parse(sentence):\n    print(type(tree))\n    print(tree)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4f9abaf1d63430aac69e41cb5c0aa2e08b846653"},"cell_type":"code","source":"#STEMMING and LEMMATIZATION\nfrom nltk.stem import porter\nstemmer = porter.PorterStemmer()\ndef stmr(x):\n    return [stemmer.stem(i) for i in x]\nX_train_stm = X_train_wt.apply(lambda x: stmr(x))\nprint(X_train_wt[0])\nprint(X_train_stm[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"03ee7f75d68e79ad7bfbd4dee6d21ec42d64907b"},"cell_type":"code","source":"from nltk.stem.wordnet import WordNetLemmatizer\ndef lmtr(x):\n    return [WordNetLemmatizer().lemmatize(i) for i in x]\n\ndef lmtrv(x):\n    return [WordNetLemmatizer().lemmatize(i, pos = 'v') for i in x]\n\nX_train_lm = X_train_wt.apply(lambda x: lmtr(x))\nX_train_lmv = X_train_wt.apply(lambda x: lmtrv(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d8d55cd85d2af3b871e0ab87b156bd3579bf45c9"},"cell_type":"code","source":"print(X_train_wt[1])\nprint(X_train_lm[1])\nprint(X_train_lmv[1])","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}