{"cells":[{"metadata":{"_uuid":"0f5831f6cbc2cdb5352ddd59310d1eb90035e219"},"cell_type":"markdown","source":"# Home Credit Default Risk\n## Walk Through Complete Workflow of One Submission\n\n#### The idea of this kernel is show basics steps of competition process, and putting it all together from loading data all the way to producing submition\n#### The main model I will use here is keras Neural Networks\n\nI hope this kernel will be useful especially for people new to data challenges.\n\nThe data is taken from kaggle's challenge on Home Credit Default Risk.\nIt consists of eight dataframes and I will load and present them one by one before creating model."},{"metadata":{"trusted":true,"_uuid":"c73f3241db2d3c8c49d25edb60d568cc89b97ac7","collapsed":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nfrom keras.preprocessing.text import Tokenizer\nimport keras\nfrom keras.preprocessing.sequence import pad_sequences\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\nfrom keras.models import Model\nfrom keras import layers\nfrom keras import Input \nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import adjusted_rand_score\nimport random\nfrom keras import regularizers\nfrom pandas.plotting import andrews_curves\nfrom keras.models import Sequential\nfrom keras.layers import Embedding, Flatten, Dense, LSTM, GRU, Dropout\nfrom keras import regularizers\nfrom sklearn.ensemble import RandomForestClassifier\nimport seaborn as sns\nfrom pandas.plotting import parallel_coordinates\nfrom sklearn.model_selection import GridSearchCV, StratifiedShuffleSplit\nfrom sklearn.metrics import confusion_matrix as CM\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"c93c4275d4ffb0aa192e64e3d0f5b8a857a9fbb9"},"cell_type":"code","source":"from sklearn import preprocessing\nscaler = preprocessing.StandardScaler() # Here I initialize scaler in order to call it later\n# It is used to scale column values to Normal(0,1) distribution\nle = preprocessing.LabelEncoder() # Label encoder will convert string values to numbers","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0d65e0d682dc50c5d3c25954677fa17a8bf7764f"},"cell_type":"markdown","source":"## Helper functions"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"ddfe90bd376fd0854a08ba8943caff027fcccc43"},"cell_type":"code","source":"def plot_pie(table, column):\n    '''\n    Function that plots single pie chart\n    '''\n    labels = []\n    for (key, value) in table[column].value_counts().items():\n        labels.append(key)\n    #plt.figure(figsize = (6,5))\n    plt.pie(table[column].value_counts())\n    plt.legend(labels, loc = 'best', bbox_to_anchor=(0.1,0.9))\n    plt.title(str(column))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"3e0537a657c093214e5e9c8bb606ca8a5c9e669f"},"cell_type":"code","source":"def plotting_pies(table, columns):\n    '''\n    Plots multiple pie chart using the above function\n    '''\n    n = len(columns)\n    plt.figure(figsize = (7*2,7*int(n/2+1)))\n    for i in range(len(columns)):\n        plt.subplot(int(n/2+1), 2, i+1)\n        plot_pie(table, columns[i])\n    plt.show()\n    plt.close('All')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"60f154f1169e8ffd41ad94c8bb7a787766b01e47"},"cell_type":"code","source":"def plot_nans(table):\n    '''\n    Presents nan values in dataframe\n    '''\n    n = len(table.columns)\n    zeros = []\n    for i in table.columns:\n        zeros.append(table[i].isnull().sum())\n    zeros = np.array(zeros)\n    indi = np.argsort(zeros)[::-1]\n    plt.figure(figsize = (n/3,5))\n    plt.title(\"Nans Over Columns\")\n    plt.bar(range(n), zeros[indi],\n           color='teal', align=\"center\")\n    plt.xticks(np.arange(n), table.columns[indi], rotation='vertical')\n    plt.xlim([-1, n])\n    plt.show()\n    plt.close(\"all\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"29ff843748da14a09c8d92471c88faece4f74d89"},"cell_type":"code","source":"def Aggregation(table1, table2, ID, summing, averaging, counting, maximum, minimum, suff):\n    ''' \n    table1 and table2 are two dataframes we want to merge, ID is column according to which we perform aggregation,\n    and summing, averaging, counting, maximum and minimum are column on wich we plan to perform specified operation;\n    suff is string to be added to each newly created column's name\n    '''\n    dictionary = {}\n    for col in summing:\n        dictionary[col] = 'sum'\n    for col in averaging:\n        dictionary[col] = 'mean'\n    for col in counting:\n        dictionary[col] = 'count'\n    for col in maximum:\n        dictionary[col] = 'max'\n    for col in minimum:\n        dictionary[col] = 'min'\n    indexi = table1[ID].values\n    Aggr = np.zeros((len(indexi),len(dictionary)))\n    for i in range(len(indexi)):\n        Aggr[i] = table2[table2[ID] == indexi[i]].agg(dictionary).values\n    Aggr = pd.DataFrame(Aggr, columns = [i+suff for i in dictionary.keys()])\n    \n    return pd.concat([table1, Aggr], axis=1, join='inner') ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9d95c1e25eae7568d8425f1be2144d13a2a8c133"},"cell_type":"markdown","source":"For each table I will show it right after loading, present it with couple of plots, preprocess it, and show it after processing\n\n\n\n## Application train and test\n\nThese two are 'basic' dataframes in this problem. We could create simple model consisting only of these two tables, but of course, with pretty poor performance.\n'application_train' is table consisting ID of person applying for loan; TARGET column, with value 1 if loan is approwed and 0 if it is rejected; and number of other attributes.\n'application_test' has all the columns as 'application_train' except TARGET, that needs to be predicted.\n\nAfter processing these two dataframes, I will aggregate them with other dataframes based on SK_ID_CURR"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"e8cdfd4cae825ee73ca3c690d8af210cf2b9710b"},"cell_type":"code","source":"application_train = pd.read_csv('../input/application_train.csv') ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"2695883bfc58fb6ca57f7c752c99b88b3f346070"},"cell_type":"code","source":"application_test = pd.read_csv('../input/application_test.csv') ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fb3129681503a6c821e3d6adb598142e029444fe","collapsed":true},"cell_type":"code","source":"application_train.head() ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"eb1b9b43f531466035cd8659858b664a570ddb05"},"cell_type":"markdown","source":"#### Plots\n\nThere are a lot of nan values in application train that need to be filled"},{"metadata":{"trusted":true,"_uuid":"f8fcd584855f4c274e1560974c3cff78f4fbc2aa","collapsed":true},"cell_type":"code","source":"plot_nans(application_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"75672d15309653ca827b8c2f82dd980cc7dd8dbf"},"cell_type":"code","source":"strColumns = ['NAME_CONTRACT_TYPE', 'CODE_GENDER','FLAG_OWN_CAR','FLAG_OWN_REALTY','NAME_INCOME_TYPE','NAME_EDUCATION_TYPE','NAME_FAMILY_STATUS','NAME_HOUSING_TYPE','OCCUPATION_TYPE','WEEKDAY_APPR_PROCESS_START','ORGANIZATION_TYPE','FONDKAPREMONT_MODE','HOUSETYPE_MODE','WALLSMATERIAL_MODE','EMERGENCYSTATE_MODE','NAME_TYPE_SUITE']\nnumColumns = ['AMT_INCOME_TOTAL', 'AMT_CREDIT', 'AMT_ANNUITY', 'AMT_GOODS_PRICE', 'REGION_POPULATION_RELATIVE','DAYS_BIRTH', 'DAYS_EMPLOYED', 'DAYS_REGISTRATION', 'DAYS_ID_PUBLISH', 'OWN_CAR_AGE', 'HOUR_APPR_PROCESS_START', 'EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'APARTMENTS_AVG', 'BASEMENTAREA_AVG', 'YEARS_BEGINEXPLUATATION_AVG', 'YEARS_BUILD_AVG', 'COMMONAREA_AVG', 'ELEVATORS_AVG', 'ENTRANCES_AVG', 'FLOORSMAX_AVG', 'FLOORSMIN_AVG', 'LANDAREA_AVG', 'LIVINGAPARTMENTS_AVG', 'LIVINGAREA_AVG', 'NONLIVINGAPARTMENTS_AVG', 'NONLIVINGAREA_AVG', 'APARTMENTS_MODE', 'BASEMENTAREA_MODE', 'YEARS_BEGINEXPLUATATION_MODE', 'YEARS_BUILD_MODE', 'COMMONAREA_MODE', 'ELEVATORS_MODE', 'ENTRANCES_MODE', 'FLOORSMAX_MODE', 'FLOORSMIN_MODE', 'LANDAREA_MODE', 'LIVINGAPARTMENTS_MODE', 'LIVINGAREA_MODE', 'NONLIVINGAPARTMENTS_MODE', 'NONLIVINGAREA_MODE', 'APARTMENTS_MEDI', 'BASEMENTAREA_MEDI', 'YEARS_BEGINEXPLUATATION_MEDI', 'YEARS_BUILD_MEDI', 'COMMONAREA_MEDI', 'ELEVATORS_MEDI', 'ENTRANCES_MEDI', 'FLOORSMAX_MEDI', 'FLOORSMIN_MEDI', 'LANDAREA_MEDI', 'LIVINGAPARTMENTS_MEDI', 'LIVINGAREA_MEDI', 'NONLIVINGAPARTMENTS_MEDI', 'NONLIVINGAREA_MEDI', 'TOTALAREA_MODE', 'DAYS_LAST_PHONE_CHANGE' ]\nCategorical = ['FLAG_MOBIL','FLAG_EMP_PHONE','FLAG_WORK_PHONE','FLAG_CONT_MOBILE','FLAG_PHONE','FLAG_EMAIL','REG_REGION_NOT_LIVE_REGION','REG_REGION_NOT_WORK_REGION','LIVE_REGION_NOT_WORK_REGION','LIVE_CITY_NOT_WORK_CITY','FLAG_DOCUMENT_2','FLAG_DOCUMENT_3','FLAG_DOCUMENT_4','FLAG_DOCUMENT_5','FLAG_DOCUMENT_6','FLAG_DOCUMENT_7','FLAG_DOCUMENT_8','FLAG_DOCUMENT_9','FLAG_DOCUMENT_10','FLAG_DOCUMENT_11','FLAG_DOCUMENT_12','FLAG_DOCUMENT_13','FLAG_DOCUMENT_14','FLAG_DOCUMENT_15','FLAG_DOCUMENT_16','FLAG_DOCUMENT_17','FLAG_DOCUMENT_18','FLAG_DOCUMENT_19','FLAG_DOCUMENT_20','FLAG_DOCUMENT_21']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2aec18aa12ec58fb15ee662569e60ce1bdc7f19d"},"cell_type":"markdown","source":"I split data in tree classes: \n\n1.) strColumn - columns consisting of string values; these ususaly have more than two possible categories, and I will present them using pie charts. After that, they need to be processed with label encoder (invoked above) because we need numerical values for further work\n\n2.) numColumns - columns with numerical values, usually float, that need to be scaled to Normal(0, 1) distribution; the reason for this is that most ML algorithms perform better when values are small, and if all attributes use the same scale\n\n3.) Categorical - ussualy consisting of 'flag' columns; values are 0 and 1, and there is no real need to process these except for fealing with nan values"},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"a2fcce135b74f5f26d3dd7a26e00bc61b954a01e","collapsed":true},"cell_type":"code","source":"plotting_pies(application_train, strColumns)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f143260c8492508a9cfcd243984af1893173ce2a"},"cell_type":"markdown","source":"#### Preprocessing\n\nIn order to scale values, first I need to fill nan values, because sklearn can't work with nans.\nThere is number of ways to do that; here I will fill nans with mean value of corresponding column.\n\nWhen filling nans in 'application_test' it is important to use mean values from 'application_train', in order to avoid overfitting.\n\nAfter that is finished, I call scaler function."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"5891a0c1325864dc49f16df3460e1961a8c74ab7"},"cell_type":"code","source":"for col in numColumns:\n    application_train[col] = application_train[col].fillna(application_train[col].mean()) \n    application_test[col] = application_test[col].fillna(application_train[col].mean()) \n    scaler.fit(application_train[col].values.reshape(-1, 1))\n    application_train[col] = scaler.transform(application_train[col].values.reshape(-1, 1))\n    application_test[col] = scaler.transform(application_test[col].values.reshape(-1, 1))    ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c9bc2868302deeac087d521b9c08ed3fa5beee88"},"cell_type":"markdown","source":"In sting columns, I replace nan values vith string 'nan', and then call label encoder to convert str values into numbers.\n\nLike above, we fit function on 'application_train' and transform values in 'application_test' according to that function"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"a6827d8fd9fd47a521b403a065527761990b7201"},"cell_type":"code","source":"application_train[np.array(strColumns)] = application_train[np.array(strColumns)].fillna('nan') \napplication_test[np.array(strColumns)] = application_test[np.array(strColumns)].fillna('nan')\n\nfor col in strColumns:\n    le.fit(application_train[col])\n    application_train[col] = le.transform(application_train[col])\n    application_test[col] = le.transform(application_test[col])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"54427ed2c7320d539e9ade8d6d186918c22f53f8"},"cell_type":"markdown","source":"Finally, this is how 'application_train' looks like after processing"},{"metadata":{"trusted":true,"_uuid":"95a19407c0ad71883dd30c7ba851d284aca277d8","collapsed":true},"cell_type":"code","source":"application_train.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"32f06211573d1d31b2f2d1e1c48372fd90df5d15"},"cell_type":"markdown","source":" Unfortunately, I currently work with on pretty old laptop and it isn't able to carry out that much data. Therefore, I will have to take only the fraction of data (first 1000 rows from training set) and work with that.\n \n If you are facing similar problem you can do the same (but then you won't be able to build realistic model).\n \n Or if you have a friend studying abroad, you can send him your notebook and ask him to execute it on Soviet SuperComputer that his University have, and send you back processed data in csv format."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"1abe2d1015ebe30056a4567c67c6409771e5e53e"},"cell_type":"code","source":"application_train = application_train[:1000]\napplication_test = application_test[:100]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"01eaee83a3c283fe2159baead07cafdc14fcd1ac"},"cell_type":"markdown","source":"## Bureau\n\nNow I load next table.\nIt contains column SK_ID_CURR on wich I will aggreagate it with application_train/test;\nbut it also have a column SK_ID_BUREAU, that I will use to merge it with next table - bureau balance, before aggregatig it with application_train/test"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"84f0467680153d0ea206056bda672e764c57acec"},"cell_type":"code","source":"bureau = pd.read_csv('../input/bureau.csv')\nbureau.head()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false,"collapsed":true,"_uuid":"0dc9163cfa1f7ab0b6d86b3d5bfbb6f098621d9b"},"cell_type":"code","source":"# pd.get_dummies(bureau)\n# moglo je i ovako, ali nema veze","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"84398e66b820d6e8f9cf7009b41735086e2623c1"},"cell_type":"markdown","source":"#### Plots"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"ac0dcd72b7a5de19253eb5515d82057a923c981f"},"cell_type":"code","source":"plot_nans(bureau)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"8c8a55af1e57f7c6ab3fc4682a8d641775028c30"},"cell_type":"code","source":"burNumeric = ['DAYS_CREDIT', 'CREDIT_DAY_OVERDUE', 'DAYS_CREDIT_ENDDATE','DAYS_ENDDATE_FACT','AMT_CREDIT_MAX_OVERDUE', 'CNT_CREDIT_PROLONG','AMT_CREDIT_SUM', 'AMT_CREDIT_SUM_DEBT','AMT_CREDIT_SUM_LIMIT', 'AMT_CREDIT_SUM_OVERDUE', 'DAYS_CREDIT_UPDATE', 'AMT_ANNUITY'] \nburCateg = ['CREDIT_ACTIVE', 'CREDIT_CURRENCY','CREDIT_TYPE']","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"3d5051a9b19be3cb9fbfaa4bd5819ca69d07f642"},"cell_type":"code","source":"plotting_pies(bureau, burCateg)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ce34f2b74adc31fa10727b7f2d7fce951336eb2d"},"cell_type":"markdown","source":"By looking at pie charts, I realized that it could be useful to mark users that had bad debt or sold credit, so I made columns for that"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"7c5e851b8b5bac10ad023c8b25460f1a6e14078c"},"cell_type":"code","source":"bureau['marker'] = np.ones(len(bureau)) # I added this column so that I could later count number of occurances for each user\n\nBad_Debt = np.zeros(len(bureau)) \nBad_Debt[np.where(bureau.CREDIT_ACTIVE == 'Bad debt')[0]] = 1\nSold = np.zeros(len(bureau))\nSold[np.where(bureau.CREDIT_ACTIVE == 'Sold')[0]] = 1\nbureau['Bad_debt'] = Bad_Debt\nbureau['Sold'] = Sold\ndel Bad_Debt\ndel Sold","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"94e0434dbf20030684892e7e4bd254f4ac7b48c8"},"cell_type":"code","source":"# Again fill nans in numerical columns with mean value and in categorical with string 'nan'\nfor col in burNumeric: \n    bureau[col] = scaler.fit_transform(bureau[col].fillna(bureau[col].mean()).values.reshape(-1, 1))\nfor col in burCateg:\n    bureau[col] = le.fit_transform(bureau[col].fillna('nan'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"35fc4e64663b4f9f2ff4c98a86cc365e714b5ad5"},"cell_type":"code","source":"# dataframe after processing\nbureau.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"d9b6557d6e396b1c37561b9cbcc4ee67f80ca190"},"cell_type":"code","source":"# As I mentione abowe, becouse of computer capacity I will take only fraction of data\nbureau = bureau[:1000]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ec6addab4cf8d4578adf6842419940d672cf2eac"},"cell_type":"markdown","source":"## Bureau balance\n\nThis one I need to merge with bureau first"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"d8c41091e2bc442545e599f20a916130a17d80da"},"cell_type":"code","source":"bureau_balance = pd.read_csv('../input/bureau_balance.csv')\nbureau_balance.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c6304a6da7decbcd1c117464c925f2f9c271c9a3"},"cell_type":"markdown","source":"#### Plot\n\nNo NaN values here"},{"metadata":{"scrolled":false,"trusted":false,"collapsed":true,"_uuid":"394d8e20efa416e8ab115546a1c9149be0a6406e"},"cell_type":"code","source":"plotting_pies(bureau_balance, ['STATUS'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"fe5a9e2f1c731131b95bf4de2ee9891c020f3875"},"cell_type":"code","source":"# Here I divided column MONTHS_BALANCE to scale it on 0-1 interval\nbureau_balance.MONTHS_BALANCE /= -1*bureau_balance.MONTHS_BALANCE.min() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"0d73014f0266fbf335d2e0b0c585fe6eba8dc7c0"},"cell_type":"code","source":"# get_dummies makes 'flag' columns for each STATUS value, which is good practice when working with categorical data\nbureau_balance = pd.get_dummies(bureau_balance)\nbureau_balance.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"d36e967be8a6766dfa3a0855a90719d45ee96462"},"cell_type":"code","source":"bureau_balance = bureau_balance[:1000]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"af5957e9533ad842847a595ad4d9391c83aca1e5"},"cell_type":"markdown","source":"#### Aggregating Bureau and Bureau Balance"},{"metadata":{"_uuid":"e9a3f59f958b676640d269308784eca7ab1bf5a0"},"cell_type":"markdown","source":"In order to aggregate bureau and bureau_balance I decided for each ID in bureau balance to take minimum value of MONTHS_BALANCE, becouse it indicates when user first applied.\n\nAt the begining of kernel, I introduced function Aggregation. As a input I need to provide lists of columns, telling where to look for minimum, where to average or something else (sum, count, max). Now, it's time to use it."},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"96c5bd3b4e8061f00b5bc48e328841ba1908db9f"},"cell_type":"code","source":"minimum = ['MONTHS_BALANCE']\naveraging = [bureau_balance.columns[i] for i in range(2, len(bureau_balance.columns))]","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false,"collapsed":true,"_uuid":"c37f69762f6ed14c42e2ec9039283bb664a88c68"},"cell_type":"code","source":"bureau = Aggregation(bureau, bureau_balance, 'SK_ID_BUREAU', summing= [], averaging = averaging, counting =[], maximum=[], minimum =minimum, suff = '_b_b')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"ee5aee9850e5d73b87abfcfd3e992e1004b9de71"},"cell_type":"code","source":"del bureau_balance # I don't need this table any more, so I'll delete it to save some working memory","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"cc19ba063816b6665e229901d12bce5548aaa83d"},"cell_type":"code","source":"bureau.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"756b1c5558ae9b1d5da819fb78e20ee8a3f114ac"},"cell_type":"markdown","source":"#### Aggregating Application train/test and Bureau \n\nI decided first to aggregate only those values in 'bureau' where status is ACTIVE, and then when it is not.\nThis way I will gate twice as more columns, but I hope it will also bring more information.\nAgain, I invoke function Aggregation, with specified columns"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"9c07911b904f661d4df55489604d169ab360f277"},"cell_type":"code","source":"minimum = ['AMT_CREDIT_MAX_OVERDUE']\nmaximum = ['DAYS_CREDIT', 'CREDIT_DAY_OVERDUE', 'DAYS_CREDIT_ENDDATE', 'CNT_CREDIT_PROLONG', 'DAYS_CREDIT_UPDATE']\naveraging = ['AMT_CREDIT_SUM', 'AMT_CREDIT_SUM_DEBT', 'AMT_CREDIT_SUM_LIMIT', 'AMT_CREDIT_SUM_OVERDUE', 'AMT_ANNUITY']\nsumming = ['marker']","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"ad83c385155c7e444551b1673c13f8f50300f229"},"cell_type":"code","source":"TRAIN = Aggregation(application_train, bureau[bureau.CREDIT_ACTIVE == 0], 'SK_ID_CURR', summing= summing, averaging = averaging, counting =[], maximum=maximum, minimum =minimum, suff = '_b_b_b')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"85d8dd85ce95016557df3a4a856bb469b0504bdd"},"cell_type":"code","source":"del application_train # delete to release space","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"11b767cf320d16d8262d4c040e8285777898986e"},"cell_type":"code","source":"TEST = Aggregation(application_test, bureau[bureau.CREDIT_ACTIVE == 0], 'SK_ID_CURR', summing= summing, averaging = averaging, counting =[], maximum=maximum, minimum =minimum, suff = '_b_Act')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"20c169eaba07518811d41b997ab8a046e1667266"},"cell_type":"code","source":"del application_test # delete to release space","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"aac0d6b923a814b980580540c2b8261e2e9f7e81"},"cell_type":"markdown","source":"And now for other satuses as well, but with a little different columns.\n\nIn list 'maximum' I add 'DAYS_ENDDATE_FACT', column with values only for closed credits; and of course, list 'summing' is different"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"99556d5d6d5b87c5150b56e08cdd3433897dd9a1"},"cell_type":"code","source":"minimum = ['AMT_CREDIT_MAX_OVERDUE']\nmaximum = ['DAYS_CREDIT', 'CREDIT_DAY_OVERDUE', 'DAYS_CREDIT_ENDDATE', 'CNT_CREDIT_PROLONG', 'DAYS_CREDIT_UPDATE','DAYS_ENDDATE_FACT']\naveraging = ['AMT_CREDIT_SUM', 'AMT_CREDIT_SUM_DEBT', 'AMT_CREDIT_SUM_LIMIT', 'AMT_CREDIT_SUM_OVERDUE', 'AMT_ANNUITY']\nsumming = ['marker', 'Bad_debt', 'Sold']","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"8b63f12062ef0406692c528f39da22220837d46b"},"cell_type":"code","source":"TRAIN = Aggregation(TRAIN, bureau[bureau.CREDIT_ACTIVE == 0], 'SK_ID_CURR', summing= summing, averaging = averaging, counting =[], maximum=maximum, minimum =minimum, suff = '_b_NAct')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"57f16a4b1917f2709858034bf6dbd9007a1a2e63"},"cell_type":"code","source":"TEST = Aggregation(TEST, bureau[bureau.CREDIT_ACTIVE != 0], 'SK_ID_CURR', summing= summing, averaging = averaging, counting =[], maximum=maximum, minimum =minimum, suff = '_b_NAct')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"8bbe7cc7364cd88979fac16434bf94ed4d6b058f"},"cell_type":"code","source":"TRAIN.head() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"4d9d420fa888405e7b15bcf05f8d715603ce047b"},"cell_type":"code","source":"del bureau","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4760cebdbe273a2d8e3dcfbd3910800a6d1f9272"},"cell_type":"markdown","source":"Now, it is pretty similar with every other table. I read and process them one by one, and aggregate it with TRAIN and TEST, so I wont go much into details.\n\n\n## POS CASH"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"11095d0bdf1b3b79f69f4b92011466e3557a2477"},"cell_type":"code","source":"POS_CASH_balance = pd.read_csv('../input/POS_CASH_balance.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d0536baccd430794e4e6f403e4944dce38edc10e"},"cell_type":"markdown","source":"### Plots"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"71ad768b0ff84ee991bc1ca875b023cb5b1836e3"},"cell_type":"code","source":"plot_nans(POS_CASH_balance)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"91f7fa96c5162ef74e9c9ee9be1bb0b79a13b79f"},"cell_type":"code","source":"POS_CASH_balance.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"f916551dac306b25f13bb46bd7726ec0cbd99fa3"},"cell_type":"code","source":"plotting_pies(POS_CASH_balance, ['NAME_CONTRACT_STATUS'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"68d290b90535178701a3690d0eb41277507dc7dc"},"cell_type":"markdown","source":"### Preprocessing"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"59afb3ab115f93f78d49f23e7cc71499b8c61223"},"cell_type":"code","source":"for col in ['CNT_INSTALMENT_FUTURE', 'CNT_INSTALMENT', 'MONTHS_BALANCE']:\n    POS_CASH_balance[col] = scaler.fit_transform(POS_CASH_balance[col].fillna(POS_CASH_balance[col].mean()).values.reshape(-1, 1))\nPOS_CASH_balance.SK_DPD /= POS_CASH_balance.SK_DPD.max()\nPOS_CASH_balance.SK_DPD_DEF /= POS_CASH_balance.SK_DPD_DEF.max()\nPOS_CASH_balance =pd.get_dummies(POS_CASH_balance)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"c726ad7bca301b5cf2f0ca91c90605426fbee52a"},"cell_type":"code","source":"POS_CASH_balance.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ce16addfaa79ba10ba7d640c3eea46187d04ad37"},"cell_type":"markdown","source":"#### Aggregating TRAIN and POS CASH "},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"c49ec8b8230daa7c0b5b8b2f20c363b4de7fd649"},"cell_type":"code","source":"maximum = ['MONTHS_BALANCE']\naveraging = ['CNT_INSTALMENT', 'CNT_INSTALMENT_FUTURE', 'SK_DPD', 'SK_DPD_DEF']\nsumming = POS_CASH_balance.columns[7:].tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"57eff522961ebf1b7ee2bcb49c382826d0ee6f94"},"cell_type":"code","source":"TRAIN = Aggregation(TRAIN, POS_CASH_balance, 'SK_ID_CURR', summing= summing, averaging = averaging, counting =[], maximum=maximum, minimum =[], suff = '_PCb')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"1692228aab014141beacacf482894d81737a377e"},"cell_type":"code","source":"TEST = Aggregation(TEST, POS_CASH_balance, 'SK_ID_CURR', summing= summing, averaging = averaging, counting =[], maximum=maximum, minimum =[], suff = '_PCb')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"c3e34946a7677e4726bb10b51a9a96d4dc5786a6"},"cell_type":"code","source":"del POS_CASH_balance","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"be30ec8ae24ee074ea0fe3d3e1ab39a7f109c8a6"},"cell_type":"markdown","source":"### Credit card balance"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"ac0e72f18ce306d5cf79504a5441c7ba2cbdb775"},"cell_type":"code","source":"credit_card_balance = pd.read_csv('../input/credit_card_balance.csv')\ncredit_card_balance.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b0f3798e0b9ebd1ed5ee4b07acde14d59c172aa5"},"cell_type":"markdown","source":"#### Plots"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"25014801d07aa45e6f355ffd4d3c0a593711cf45"},"cell_type":"code","source":"plot_nans(credit_card_balance)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"8a04a746df0e3413c9732938cbb89ec83a7acdd1"},"cell_type":"code","source":"plotting_pies(credit_card_balance, ['NAME_CONTRACT_STATUS'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"07876188f80e345a1d4001fe2e96be8156ead4c1"},"cell_type":"markdown","source":"#### Preprocessing"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"13852f22e925fdfbfd4039e0cf8963966924343e"},"cell_type":"code","source":"credit_card_balance = pd.get_dummies(credit_card_balance)\nfor col in credit_card_balance.columns[2:15]:\n    credit_card_balance[col] = scaler.fit_transform(credit_card_balance[col].fillna(credit_card_balance[col].mean()).values.reshape(-1, 1))\nfor col in credit_card_balance.columns[15:22]:\n    credit_card_balance[col] /= credit_card_balance[col].max()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"df140eb5956b65fadb4671ae6ce8ad6539e9daa6"},"cell_type":"code","source":"credit_card_balance.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1da9cce46b04b74c9273f7d35a886dc3436a8ee4"},"cell_type":"markdown","source":"#### Aggregation"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"30d063c57af34d9f39306817ed01844306def9e4"},"cell_type":"code","source":"averaging = credit_card_balance.columns[3:22].tolist()\nmaximum = ['MONTHS_BALANCE']\nsumming = credit_card_balance.columns[22:].tolist()\ncounting = ['NAME_CONTRACT_STATUS_Active']","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"ed465947f095ba941a1a5f35a1674e7e72a6087b"},"cell_type":"code","source":"TRAIN = Aggregation(TRAIN, credit_card_balance, 'SK_ID_CURR', summing= summing, averaging = averaging, counting =counting, maximum=maximum, minimum =[], suff = '_ccb')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"a2ab76ff706558b02d421a4793dfa98fbb87d298"},"cell_type":"code","source":"TEST = Aggregation(TEST, credit_card_balance, 'SK_ID_CURR', summing= summing, averaging = averaging, counting =counting, maximum=maximum, minimum =[], suff = '_ccb')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"7dae26e0b3fca48dda5956bb1da8f2da13c8fb7b"},"cell_type":"code","source":"del credit_card_balance","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"872a21af15de3c57b60dbaf0002eb0d8aba240c6"},"cell_type":"markdown","source":"### Previous application\n"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"1e8ce5d7b69e72100e6eb5cbb94d09e23ce52a74"},"cell_type":"code","source":"previous_application = pd.read_csv('../input/previous_application.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"8b4f25b1091dc52debaf72eab3fed2e6d00f7973"},"cell_type":"code","source":"previous_application.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"2f69ea3c03b8e005b24b938bbd195c9d7ec365ec"},"cell_type":"code","source":"plot_nans(previous_application)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"1b058bd6c328227f9ec9e10b8ca3506ff1c44fc5"},"cell_type":"code","source":"strColumns = ['NAME_CONTRACT_TYPE', 'WEEKDAY_APPR_PROCESS_START', 'FLAG_LAST_APPL_PER_CONTRACT', 'NFLAG_LAST_APPL_IN_DAY', 'NAME_CASH_LOAN_PURPOSE', 'NAME_CONTRACT_STATUS', 'NAME_PAYMENT_TYPE', 'CODE_REJECT_REASON', 'PRODUCT_COMBINATION', 'NAME_TYPE_SUITE', 'NAME_CLIENT_TYPE', 'NAME_PORTFOLIO', 'NAME_PRODUCT_TYPE', 'CHANNEL_TYPE', 'NAME_SELLER_INDUSTRY', 'NAME_YIELD_GROUP', 'PRODUCT_COMBINATION', 'NAME_GOODS_CATEGORY']\nnumColumns = ['AMT_ANNUITY','AMT_APPLICATION', 'AMT_CREDIT', 'AMT_DOWN_PAYMENT', 'AMT_GOODS_PRICE','RATE_DOWN_PAYMENT', 'RATE_INTEREST_PRIMARY', 'RATE_INTEREST_PRIVILEGED', 'DAYS_DECISION', 'SELLERPLACE_AREA', 'DAYS_FIRST_DRAWING', 'DAYS_FIRST_DUE', 'DAYS_LAST_DUE_1ST_VERSION', 'DAYS_LAST_DUE','DAYS_TERMINATION', 'SELLERPLACE_AREA', 'DAYS_FIRST_DRAWING', 'DAYS_FIRST_DUE', 'DAYS_LAST_DUE_1ST_VERSION', 'DAYS_LAST_DUE', 'DAYS_TERMINATION']","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false,"collapsed":true,"_uuid":"d188a7c429443a5507b5b9923a72dbec59183657"},"cell_type":"code","source":"plotting_pies(previous_application, strColumns)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dd71dedd34da829038cf9ba45f7d0ef2d461beae"},"cell_type":"markdown","source":"#### Preporcessing"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"250d9e0d2836029024d8a8a0753a71027f4bab53"},"cell_type":"code","source":"previous_application=pd.get_dummies(previous_application)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"38fa5c63664d2dd270e66362e6489b9a67cf4754"},"cell_type":"code","source":"for col in numColumns:\n    previous_application[col] = scaler.fit_transform(previous_application[col].fillna(previous_application[col].mean()).values.reshape(-1, 1))\nfor col in  ['HOUR_APPR_PROCESS_START', 'CNT_PAYMENT']:\n    previous_application[col] /= previous_application[col].max()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"5b3af6a8994c44a99a72fc1bb6b78eeee1daf5b8"},"cell_type":"code","source":"previous_application.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5971c8cfcbefeae45c7ca39453ad023146b40024"},"cell_type":"markdown","source":"#### Aggregation"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"1c3403bcac15fb987a2c50c8535d47590716da12"},"cell_type":"code","source":"averaging = [col for col in previous_application.columns[2:] if col in numColumns or col in ['HOUR_APPR_PROCESS_START', 'CNT_PAYMENT']]\nsumming = [col for col in previous_application.columns[2:] if col not in averaging]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"72f460781c99aa2338199f0b33979aa0687a993f"},"cell_type":"code","source":"TRAIN = Aggregation(TRAIN, previous_application, 'SK_ID_CURR', summing= summing, averaging = averaging, counting =[], maximum=[], minimum =[], suff = '_pa')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"d779fdcd8d7eb259d25d35a8ae67e0e5c7a05b1e"},"cell_type":"code","source":"TEST = Aggregation(TEST, previous_application, 'SK_ID_CURR', summing= summing, averaging = averaging, counting =[], maximum=[], minimum =[], suff = '_pa')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"6416097b5c59a57f2bd0b232a44a9cb06eae569e"},"cell_type":"code","source":"del previous_application","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"49cb629379af29f317167a8a48ed28c05b6cf248"},"cell_type":"markdown","source":"## Installments payments"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"0bf82ad7c8a6837cf971448f0518a9221d86b3a9"},"cell_type":"code","source":"installments_payments = pd.read_csv('../input/installments_payments.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"1a4edba5ef228ebd0f48adce145e088a89a02279"},"cell_type":"code","source":"installments_payments.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3f91466521c45f733493ccb3b8c906eee8e65ca6"},"cell_type":"markdown","source":"#### Plot"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"29cba4beaaa182a2ae607a079e820f878aac2bdc"},"cell_type":"code","source":"plot_nans(installments_payments)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"e12d8c570443803ecd83cad0a1924f13ed09e108"},"cell_type":"code","source":"for col in ['NUM_INSTALMENT_VERSION', 'NUM_INSTALMENT_NUMBER']:\n    installments_payments[col] /= installments_payments[col].max()\nfor col in installments_payments.columns[4:]:\n    installments_payments[col] = scaler.fit_transform(installments_payments[col].fillna(installments_payments[col].mean()).values.reshape(-1, 1))  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"368b59815e413c9a5d70c792ee7daf803afaf1f7"},"cell_type":"code","source":"installments_payments.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6863dcd26d4283f7781a79d544fb4a42d0a036c0"},"cell_type":"markdown","source":"#### Aggregation"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"a39bac2328f1f15c2aa3498270b0d24c7073dc1f"},"cell_type":"code","source":"averaging = [col for col in installments_payments.columns[2:]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"c61cf9719b130611785ab57d3f8512046549eb9b"},"cell_type":"code","source":"TRAIN = Aggregation(TRAIN, installments_payments, 'SK_ID_CURR', summing= [], averaging = averaging, counting =[], maximum=[], minimum =[], suff = '_ip')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"38bb84b3eb08228b22db6126182270d7357f0316"},"cell_type":"code","source":"TEST = Aggregation(TEST, installments_payments, 'SK_ID_CURR', summing= [], averaging = averaging, counting =[], maximum=[], minimum =[], suff = '_ip')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"c4f2f1ecc27bf3a516bfe04c64bf50bcf3a06853"},"cell_type":"code","source":"del installments_payments","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"90d324b1553a15306323759b35c171141cef5772"},"cell_type":"markdown","source":"##### Now, we have completed data preprocessing and aggregation, and here is how our data looks like now"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"3076f3cd4b742dddaac0fae9373cf173cc8fc998"},"cell_type":"code","source":"TRAIN.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6f655b388ed22b69c6537abfb8ee863903bb657b"},"cell_type":"markdown","source":"## Model\n\n#### When I  have prepared data, it is time to build model and made prediction\n\n\n\n\nBecouse of croping data earlier, this dataframe I am left with is not really representative, and probably contains a lot of missing values.\n"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"e563db99d04727685d7a24ecd1e3c6d403cb4fe9"},"cell_type":"code","source":"TRAIN = TRAIN.fillna(0) # in case that some nan values still remained","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"29cb0daac621bb431ff39751a46e26aab3774563"},"cell_type":"code","source":"TEST = TEST.fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"de68f8a8c804d63639db924ea2302c13267a7e16"},"cell_type":"code","source":"TRAIN.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"fffb761ce202b3a0ea4c685a173297b8f3c5417a"},"cell_type":"code","source":"TRAIN.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"d072da26ae0f9de6932de93e55ce686e2261292b"},"cell_type":"code","source":"TEST.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"e2d7cc7910e36136befc8ffb8338ce45f7b3c500"},"cell_type":"code","source":"TEST.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c9a7f6a851a333d682f499e71cedd8f4df50b38b"},"cell_type":"markdown","source":"As I sad earlier, I will made a neural network model. But now, there is another problem. Our data is not balanced. The ratio between rejected and accepted loans is about 92% : 8%. That means that I can make a model that does apsolutely nothing and achieves accuracy 92. \n\nSo, it isn't good idea to make model based only on accuracy score.\n\nBut, what ever metric, model won't fit well if data is as imbalanced as it is here. There are two simple solusions here. Oversampling and undersampling. In undersampling, you simply drop points from majority class until you get fairly even distribution of classes. Oversampling works the other way. You artificialy create more instances of minority class, usualy by taking linear combinations of existing points, until you got equal number in both classes. \n\nI would reccomand you to use oversampling, in order not to loose valuable information, but now I will stick to undersampling, couse it is less time and memory consuming. "},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"cc29d78a8d55eeb1116432a6e567601d15c56a72"},"cell_type":"code","source":"marker = [] # list that will keep track of rows I wont to keep\nfor i in range(len(TRAIN)):\n    if TRAIN.TARGET[i] == 0: # if application is rejected\n        if np.random.rand()<=0.1: # take it with probability 10 % (so the ratio between positive and negative targets is close to 1)\n            marker.append(i)\n    else:\n        marker.append(i) # keep all approved loans","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"033f376c76aa2ffa0eaea5658eda19efa2a1a4b0"},"cell_type":"markdown","source":"The size of original dataset was around 307k, and after undersampling, I am left with nearly 50k. \n\nLet's split dataframe to data and labels, and also to trainign and validation sets."},{"metadata":{"scrolled":true,"trusted":false,"collapsed":true,"_uuid":"e904ade2d0cf2cc4cc53f6cb800510826a30b48c"},"cell_type":"code","source":"X = TRAIN.iloc[np.array(marker),3:].values\nY = TRAIN.iloc[np.array(marker)].TARGET.values\nY = keras.utils.to_categorical(Y, 2) # for neural networks, it is better to encode labels (from 0, 1 to [1,0], [0,1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"0a7642710a1296ef0a31a69e0184d93ca32056cd"},"cell_type":"code","source":"limit = int(0.8*len(X)) # for training take 80% of data, and 20 for validation\ntrainX, valX, trainY, valY = X[:limit], X[limit:], Y[:limit], Y[limit:]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5f79acb509b8225b3985e03933f67b265510e721"},"cell_type":"markdown","source":"### Building model\n\nModel is very simple, with stack of dense layers and dropout layers in between, to avoid overfitting. I tried some other architectures as well, but the performance didn't chage significantly, so I picked this, as simple and fast model.\n\nConsidering the nature of dataset, I wouldn't benefit from reccurent or convolutional layers, so there is no need to complicate."},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"addf33aff00e9a0f59e594c48b3c422f4278c5d6"},"cell_type":"code","source":"model = Sequential()\nmodel.add(Dense(1024, input_shape=(trainX.shape[1],), activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(512, activation='relu',kernel_regularizer=regularizers.l2(0.01)))\nmodel.add(Dropout(0.4))\nmodel.add(Dense(512, activation='relu',kernel_regularizer=regularizers.l2(0.01)))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(256, activation='relu',kernel_regularizer=regularizers.l2(0.01)))\nmodel.add(Dense(2, activation='softmax'))\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"d992ec2c5c07f5d45e0c4bbb888c453bcf38996c"},"cell_type":"code","source":"model.compile(optimizer='rmsprop',\n              loss='binary_crossentropy',\n              metrics=['acc'])\nhistory = model.fit(trainX, trainY,\n                    epochs=3, # it should be way more epochs, so if your computer is able to carry this out, increase it\n                    batch_size=8,validation_data=(valX, valY))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2068aa75b0b884f7df7888ebbb36e00347eb142c"},"cell_type":"markdown","source":"### Model Evaluation\n\nTo see how model performed, it is best to plot confusion matrix"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"ae4b3c86b6bbb6cf2640082383f9f636bc29dd1c"},"cell_type":"code","source":"sns.heatmap(CM(np.argmax(model.predict(valX), axis=1), np.argmax(valY, axis = 1)), annot = True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"56874a9c679c63e3a01d9881f49293c2eb3d3153"},"cell_type":"markdown","source":"If you are satisfied with performance, it is time to train the same model on whole training set and make a submussion. If not, then fine tune model until you get better results. \n\n#### Training model on whole dataset"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"ce0813b2d6ed3f11be06601c7063774252be91e7"},"cell_type":"code","source":"model.compile(optimizer='rmsprop',\n              loss='binary_crossentropy',\n              metrics=['acc'])\nhistory = model.fit(X, Y,\n                    epochs=3, \n                    batch_size=8)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"aceba1bab4b89ef95a71c70d59fee78b385a692c"},"cell_type":"markdown","source":"#### Making sumbission\n\nThere is explicit ecplanation how solution should look like, so let's fit into that form"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"be0cf798209a48d12a984bac21a7bd7497683f35"},"cell_type":"code","source":"Solution = pd.DataFrame(TEST.SK_ID_CURR)\nSolution['TARGET'] = np.argmax(model.predict(TEST.iloc[:,2:].values).tolist(), axis = 1)\n# the next line saves solution in CSV format, and it is ready for submitting\n# Solution.set_index('SK_ID_CURR').to_csv('solution.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b1d0d2902023a06aba99b4b0fd6bb34c7a957434"},"cell_type":"markdown","source":"##### That's all folks.\n\n###### I hope this helped you. \n###### The project is very simple, but goes through all necessary steps. Results probably aren't satisfying, but it can improve with more detailed feature engineering and hyperparameter tuning. Anyway this should be good skeleton for making good prediction.\n\n"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"164a5ce927cc0aa4dc5725f0b439613d5e9ec64f"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}