{"metadata": {"language_info": {"version": "3.6.3", "pygments_lexer": "ipython3", "name": "python", "codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "nbconvert_exporter": "python", "mimetype": "text/x-python"}, "kernelspec": {"display_name": "Python 3", "name": "python3", "language": "python"}}, "nbformat": 4, "nbformat_minor": 1, "cells": [{"metadata": {"_cell_guid": "5e2f73c7-b9c2-4361-8c12-a3743aacc992", "_uuid": "56e5448cf75bce64f2a5c6bca1469323d30d4503"}, "cell_type": "markdown", "source": ["This is a simple stacking notebook to get you started with stacking keras's take on fastText + a classic BOW sklearn model.\n", "\n", "Based on: https://www.kaggle.com/sterby/fasttext-like-baseline-with-keras-lb-0-257 , https://www.kaggle.com/yekenot/toxic-regression"]}, {"metadata": {"_cell_guid": "08af5ead-6614-4f92-a4e3-093697cd4630", "collapsed": true, "_uuid": "9f8d6f83925faf36ff75acefedbb423e6fe76918"}, "cell_type": "code", "outputs": [], "execution_count": null, "source": ["import pandas as pd\n", "import numpy as np\n", "from sklearn.ensemble import GradientBoostingClassifier\n", "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n", "from sklearn.feature_extraction.text import CountVectorizer"]}, {"metadata": {"_cell_guid": "e78abc52-c2fb-48e5-80e2-a5538f9ef421", "_uuid": "622dd3eda5f1a4e20ca0a11ddc88e6cf1823bbf8"}, "cell_type": "code", "outputs": [], "execution_count": null, "source": ["from keras.preprocessing import sequence\n", "from keras.models import Model, Input\n", "from keras.layers import Dense, Embedding, GlobalAveragePooling1D, Dropout, SpatialDropout1D\n", "from keras.preprocessing.text import Tokenizer"]}, {"metadata": {"_cell_guid": "fb04a7a2-f1e9-44ad-a320-c8feda8a2a61", "_uuid": "1aee7e6435f922592df409c705751cc4a716ebc8"}, "cell_type": "markdown", "source": ["# Load the data"]}, {"metadata": {"_cell_guid": "09bf6043-2919-4aac-9b85-aa2dc71554d0", "collapsed": true, "_uuid": "849d6b0c78be1391c6148869118c87c3d3712544"}, "cell_type": "code", "outputs": [], "execution_count": null, "source": ["train_df = pd.read_csv(\"../input/train.csv\")\n", "test_df = pd.read_csv(\"../input/test.csv\")"]}, {"metadata": {"_cell_guid": "78f87293-8016-49dd-a63c-151779956a06", "collapsed": true, "_uuid": "5b0b36db084f6cd8d08f4b8135ea4b43ea36c38e"}, "cell_type": "code", "outputs": [], "execution_count": null, "source": ["df = pd.concat([train_df['comment_text'], test_df['comment_text']], axis=0).fillna(\"BLANK\")  # concat data for \"cheating\" in vectorizing"]}, {"metadata": {"_cell_guid": "6d95839a-7b1a-49db-89c2-37fe29d6bdfa", "collapsed": true, "_uuid": "4406ce81bc1da62436aa8d849426bde899ff1cfc", "scrolled": true}, "cell_type": "code", "outputs": [], "execution_count": null, "source": ["train_df.head()"]}, {"metadata": {"_cell_guid": "e0767151-80de-4109-8775-f986e685a505", "_uuid": "68b69fedacc0b693853926df2e42b71bd816b72f"}, "cell_type": "markdown", "source": ["## A little EDA: Is this multiclass or multilabel? \n", "* Looks like it can be multilabel :(\n", "* Might be reversable with : https://stackoverflow.com/questions/44464280/mapping-one-hot-encoded-target-values-to-proper-label-names"]}, {"metadata": {"_cell_guid": "2ebf324c-9be5-4f13-955b-66be73b3d38d", "collapsed": true, "_uuid": "5bb32bce59fc3f1888379d4b1c9f83f296134ef4"}, "cell_type": "code", "outputs": [], "execution_count": null, "source": ["(train_df.iloc[:,2:].apply(sum,axis=1)>1).sum()"]}, {"metadata": {"_cell_guid": "aea018f1-c215-468d-98c3-e0a68e4a3078", "collapsed": true, "_uuid": "856455e4d300957ee15ad757d06faf4ba8e5eb51"}, "cell_type": "code", "outputs": [], "execution_count": null, "source": ["print(train_df.comment_text.str.len().describe())"]}, {"metadata": {"_cell_guid": "6c5916e5-78c9-402b-b5bb-7c1d77380440", "collapsed": true, "_uuid": "f6caafa577523b187af482ee58b7681edd2a33d5", "scrolled": false}, "cell_type": "code", "outputs": [], "execution_count": null, "source": ["print(train_df.comment_text.str.split().str.len().describe())"]}, {"metadata": {"_cell_guid": "31bd24ee-4af6-458e-98f0-89de251908f1", "collapsed": true, "_uuid": "231cd713a15baddc391eecce4ad0923ae9ec1baa"}, "cell_type": "code", "outputs": [], "execution_count": null, "source": ["print(test_df.comment_text.str.split().str.len().describe())"]}, {"metadata": {"_cell_guid": "7cde6c71-6ff7-4af6-9615-ba45171df768", "_uuid": "bd46cf991dcb8f9236682180dcfcb994c79aac5f"}, "cell_type": "markdown", "source": ["* It looks like we have less than a hundred words, and a few hundred chars per sentence. \n", "* This will help us design our max len, as well as giving us insight into there being many short words/characters"]}, {"metadata": {"_cell_guid": "b3575b93-1b4b-4dd5-a624-816d87fa6724", "collapsed": true, "_uuid": "837178d74233a88f98b48b7e66e55771f0093fb5"}, "cell_type": "code", "outputs": [], "execution_count": null, "source": ["X_train = train_df[\"comment_text\"].fillna(\"BLANK\").values\n", "y_train = train_df[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]].values\n", "X_test = test_df[\"comment_text\"].fillna(\"BLANK\").values"]}, {"metadata": {"_cell_guid": "bc678041-4cc1-4ac3-8725-b82e64df2306", "collapsed": true, "_uuid": "4732035b6722f3edb52eb6b7d793018186eefb00"}, "cell_type": "code", "outputs": [], "execution_count": null, "source": ["i = 0\n", "print(\"Comment: {}\".format(X_train[i]))\n", "print(\"Label: {}\".format(y_train[i]))"]}, {"metadata": {"_cell_guid": "d779dd97-8872-4142-b4a0-ec567d22c1db", "_uuid": "53f203285fae275a29a91f7ef41ef164f867ab66"}, "cell_type": "markdown", "source": ["# Use simple fasttext-like model"]}, {"metadata": {"_cell_guid": "b291a985-4b50-4a1f-b1d8-80772a4f7992", "collapsed": true, "_uuid": "f14a84b803adc74b14b11e871ef87d9361f29d8a"}, "cell_type": "code", "outputs": [], "execution_count": null, "source": ["# Set parameters:\n", "max_features = 95000\n", "maxlen = 84\n", "batch_size = 32\n", "embedding_dims = 60 #50\n", "epochs = 3"]}, {"metadata": {"_cell_guid": "6c66a34e-f161-43c6-9516-da1c4529df2d", "collapsed": true, "_uuid": "dd03a24e101354dd3e42a7c56ffa4299e300e3d0"}, "cell_type": "code", "outputs": [], "execution_count": null, "source": ["print('Tokenizing data...')\n", "tok = Tokenizer(num_words=max_features)\n", "tok.fit_on_texts(list(X_train) + list(X_test))\n", "x_train = tok.texts_to_sequences(X_train)\n", "x_test = tok.texts_to_sequences(X_test)\n", "print(len(x_train), 'train sequences')\n", "print(len(x_test), 'test sequences')\n", "print('Average train sequence length: {}'.format(np.mean(list(map(len, x_train)), dtype=int)))\n", "print('Average test sequence length: {}'.format(np.mean(list(map(len, x_test)), dtype=int)))"]}, {"metadata": {"_cell_guid": "0cf4fbaa-0127-4074-8294-b9c735f39680", "collapsed": true, "_uuid": "6ec8bdd68059159a542cab8c9ff32d31d26dc373"}, "cell_type": "code", "outputs": [], "execution_count": null, "source": ["print('Pad sequences (samples x time)')\n", "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n", "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n", "print('x_train shape:', x_train.shape)\n", "print('x_test shape:', x_test.shape)"]}, {"metadata": {"_cell_guid": "38454ada-8e95-4add-a039-345bc6856666", "collapsed": true, "_uuid": "2fdfd068dd38b910f0b3605b863bf65d4e2d34c7"}, "cell_type": "code", "outputs": [], "execution_count": null, "source": ["print('Build model...')\n", "comment_input = Input((maxlen,))\n", "\n", "# we start off with an  embedding layer\n", "comment_emb = Embedding(max_features, embedding_dims, input_length=maxlen)(comment_input)\n", "# We see that we overfit straight away, so dropout may be useful\n", "drp = SpatialDropout1D(0.1)(comment_emb)\n", "# we add a GlobalAveragePooling1D, which will average the embeddings\n", "# of all words in the document\n", "main = GlobalAveragePooling1D()(drp)\n", "\n", "# We project onto a single unit output layer, and squash it with a sigmoid:\n", "output = Dense(6, activation='softmax')(main)\n", "\n", "model = Model(inputs=comment_input, outputs=output)\n", "\n", "model.compile(loss='categorical_crossentropy',\n", "              optimizer='adam',\n", "              metrics=['accuracy'])"]}, {"metadata": {"_cell_guid": "6e64b88a-81aa-455d-af55-c7b54625ddd0", "collapsed": true, "_uuid": "78d7759e7b282f535198186cb864c5deece6ee73"}, "cell_type": "code", "outputs": [], "execution_count": null, "source": ["hist = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)"]}, {"metadata": {"_cell_guid": "f9201487-a13d-4b63-b0da-fb2773507358", "_uuid": "907ff8f48c6785640f433c3978b44c43f2f8720d"}, "cell_type": "markdown", "source": []}, {"metadata": {"_cell_guid": "ef4c41c8-22ad-40fe-8c75-11a10622e5ea", "collapsed": true, "_uuid": "ab8dfc297cc8a7e451498662eb576de8b39a262f"}, "cell_type": "code", "outputs": [], "execution_count": null, "source": ["# print('Build model...')\n", "# comment_input = Input((maxlen,))\n", "\n", "# # we start off with an  embedding layer\n", "# comment_emb = Embedding(max_features, embedding_dims, input_length=maxlen)(comment_input)\n", "# # We see that we overfit straight away, so dropout may be useful\n", "# drp = Dropout(0.15)(comment_emb)\n", "# # we add a GlobalAveragePooling1D, which will average the embeddings\n", "# # of all words in the document\n", "# main = GlobalAveragePooling1D()(drp)\n", "\n", "# drp2 =  Dropout(0.25)(main)\n", "# # We project onto a single unit output layer, and squash it with a sigmoid:\n", "# output = Dense(6, activation='softmax')(drp2)\n", "\n", "# model2 = Model(inputs=comment_input, outputs=output)\n", "\n", "# model2.compile(loss='categorical_crossentropy',\n", "#               optimizer='adam',\n", "#               metrics=['accuracy'])\n", "\n", "# hist2 = model2.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)"]}, {"metadata": {"_cell_guid": "735185a5-81fd-4e03-bac6-1f76ddc85e21", "_uuid": "1495d6641a66944a593598739803e3ff72646c5c"}, "cell_type": "markdown", "source": ["#### Playing with dropout doesn't move the FastText needle (results are the same with/without dropout(s)). Not very surprising as it's just a linear embedding.\n", "* Final output is still \"loss: 0.2863 - acc: 0.9890 - val_loss: 0.3014 - val_acc: 0.9892\"\n", "\n", "* Note that spatial dropout has a much bigger effect! "]}, {"metadata": {"_cell_guid": "9d6db9c8-3e58-4cf9-a826-4e1754c90447", "_uuid": "91bc07c33b1bcd8a9baea93c79be0e501373edf8"}, "cell_type": "markdown", "source": ["## Ensemble!\n", "* Let's add the output from another model"]}, {"metadata": {"_cell_guid": "b9370bee-ded0-40ab-b6e0-9138c0312e3c", "collapsed": true, "_uuid": "58328c6812b6211bbe6ee5e4bf7ad4a751ec70ec", "scrolled": true}, "cell_type": "code", "outputs": [], "execution_count": null, "source": ["nrow_train = train_df.shape[0]\n", "\n", "vectorizer = CountVectorizer(stop_words='english',min_df=3, max_df=0.97,max_features = 40000)\n", "data = vectorizer.fit_transform(df)\n", "\n", "col = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n", "\n", "lr_preds = np.zeros((test_df.shape[0], len(col)))\n", "\n", "X_train = data[:nrow_train]\n", "X_test = data[nrow_train:]\n", "\n", "for i, j in enumerate(col):\n", "    print('fit '+j)\n", "    lr_model = LogisticRegression(C=0.1, dual=True)\n", "    lr_model.fit(X_train, train_df[j])\n", "    lr_preds[:,i] = lr_model.predict_proba(X_test)[:,1]\n", "print(\"done\")"]}, {"metadata": {"_cell_guid": "170ff5e4-9371-4ec3-bba1-37e7b3490a04", "_uuid": "f84bdf717fb9cf6c63060b49868999ccb8afde00"}, "cell_type": "markdown", "source": ["## Quick sanity check. compare out predicted outputs"]}, {"metadata": {"_cell_guid": "59d227dd-d38c-4b60-a156-331ca1f1c665", "collapsed": true, "_uuid": "86fe1c1262d97862000669d2f3129e393141c473"}, "cell_type": "code", "outputs": [], "execution_count": null, "source": ["for i, j in enumerate(col):\n", "    print(j,lr_preds[:,i].mean())"]}, {"metadata": {"_cell_guid": "f0399a20-e8a8-466e-a768-3ee22245a661", "collapsed": true, "_uuid": "be8514eefbd5a3fe95600d710cbb2d24580b10b4"}, "cell_type": "code", "outputs": [], "execution_count": null, "source": ["# Get predictions from our keras/fasttext model\n", "ft_pred = model.predict(x_test)"]}, {"metadata": {"_cell_guid": "e00bf5fc-ac8a-491f-b1f1-25b3e153f846", "_uuid": "61aba7e0f1a356b78fbd8db340c7560fe35bbe59"}, "cell_type": "markdown", "source": ["# submit"]}, {"metadata": {"_cell_guid": "4472f2e8-8c4a-4a36-97ec-050e6f952839", "collapsed": true, "_uuid": "787560f67d2f6be8ee2acd997171fa3e0b72dc89"}, "cell_type": "code", "outputs": [], "execution_count": null, "source": ["# get mean of both submissions\n", "\n", "y_pred = lr_preds+ft_pred\n", "y_pred = y_pred/2.0\n"]}, {"metadata": {"_cell_guid": "20edfc41-2e40-4dcb-8c93-a628878d21d7", "collapsed": true, "_uuid": "bcd379c47dc1770cb595720cf47a21d0bb289a18"}, "cell_type": "code", "outputs": [], "execution_count": null, "source": []}, {"metadata": {"_cell_guid": "e4dabe11-4cf0-4feb-af9b-e5074233511f", "collapsed": true, "_uuid": "7d98f168948c9240b680808ccf306220b045a876"}, "cell_type": "code", "outputs": [], "execution_count": null, "source": ["submission = pd.read_csv(\"../input/sample_submission.csv\")"]}, {"metadata": {"_cell_guid": "5491387a-4e8e-4080-b4cb-d549c3372f6b", "collapsed": true, "_uuid": "c5aaed3d9c600e4a674959902f9552a3112d47e8"}, "cell_type": "code", "outputs": [], "execution_count": null, "source": ["submission[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]] = y_pred"]}, {"metadata": {"_cell_guid": "3342bf21-3402-44f1-b601-51b6a0684515", "collapsed": true, "_uuid": "e335a3a0af28935e41aa08a140129033ef298658"}, "cell_type": "code", "outputs": [], "execution_count": null, "source": ["submission.head()"]}, {"metadata": {"_cell_guid": "50495ec2-e5d8-41cd-8b3a-c2779ef7b61c", "collapsed": true, "_uuid": "46dd6762d02e4de6c0fbef22f3257bc740f306b5"}, "cell_type": "code", "outputs": [], "execution_count": null, "source": ["submission.to_csv(\"submission_fasttext_1.csv\", index=False)"]}, {"metadata": {"_cell_guid": "e8549abb-9cda-44be-9d0f-abe49050474a", "collapsed": true, "_uuid": "c33847d50412c864b7ab099fea086405d9cbe193"}, "cell_type": "code", "outputs": [], "execution_count": null, "source": []}]}