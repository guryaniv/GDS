{"cells":[{"metadata":{"_uuid":"44bbae45bfef6ef3547b66e18166d9b99a2f4462"},"cell_type":"markdown","source":"# Introduction: Manual Feature Engineering (part two)\n\nIn this notebook we will expand on the [Introduction to Manual Feature Engineering](https://www.kaggle.com/willkoehrsen/introduction-to-manual-feature-engineering/output) notebook. We will use the aggregation and value counting functions developed in that notebook in order to incorporate information from the `previous_application`, `POS_CASH_balance`, `installments_payments`, and `credit_card_balance` data files. We already used the information from the `bureau` and `bureau_balance` in the previous notebook and were able to improve our competition score compared to using only the `application` data. After running a model with the features included here, performance does increase, but we run into issues with an explosion in the number of features! I'm working on a notebook of feature selection, but for this notebook we will continue building up a rich set of data for our model. \n\nThe definitions of the four additional data files are:\n\n* previous_application (called `previous`): previous applications for loans at Home Credit of clients who have loans in the application data. Each current loan in the application data can have multiple previous loans. Each previous application has one row and is identified by the feature SK_ID_PREV.\n* POS_CASH_BALANCE (called `cash`): monthly data about previous point of sale or cash loans clients have had with Home Credit. Each row is one month of a previous point of sale or cash loan, and a single previous loan can have many rows.\n* credit_card_balance (called `credit`): monthly data about previous credit cards clients have had with Home Credit. Each row is one month of a credit card balance, and a single credit card can have many rows.\n* installments_payment (called `installments`): payment history for previous loans at Home Credit. There is one row for every made payment and one row for every missed payment."},{"metadata":{"_uuid":"04c8826306314ba274e409667aa58a806c6f462c"},"cell_type":"markdown","source":"# Functions \n\nWe spent quite a bit of time developing two functions in the previous notebook:\n\n* `agg_numeric`: calculate aggregation statistics (`mean`, `count`, `max`, `min`) for numeric variables.\n* `count_categorical`: compute counts and normalized counts of each category in a categorical variable.\n\nTogether, these two functions can extract information about both the numeric and categorical data in a dataframe. Our general approach will be to apply both of these functions to the dataframes, grouping by the client id, `SK_ID_CURR`. For the `POS_CASH_balance`, `credit_card_balance`, and `installment_payments`, we can first group by the `SK_ID_PREV`, the unique id for the previous loan. Then we will group the resulting dataframe by the `SK_ID_CURR` to calculate the aggregation statistics for each client across all of their previous loans. If that's a little confusing, I'd suggest heading back to the [first feature engineering notebook](https://www.kaggle.com/willkoehrsen/introduction-to-manual-feature-engineering/output).**"},{"metadata":{"_uuid":"119c68a15375b625ee35d87cf46f91472aea7f32"},"cell_type":"markdown","source":"## Function to Aggregate Numeric Data\n\nThis groups by "},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"9b3c521b3cb7916e7473668356a6348d272c0655"},"cell_type":"code","source":"# pandas and numpy for data manipulation\nimport pandas as pd\nimport numpy as np\n\n# matplotlib and seaborn for plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Suppress warnings from pandas\nimport warnings\nwarnings.filterwarnings('ignore')\n\nplt.style.use('fivethirtyeight')\n\n# Memory management\nimport gc ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"c5a7850897a9f7ea20be066f379b29d46e670584"},"cell_type":"code","source":"def agg_numeric(df, group_var, df_name):\n    \"\"\"Aggregates the numeric values in a dataframe. This can\n    be used to create features for each instance of the grouping variable.\n    \n    Parameters\n    --------\n        df (dataframe): \n            the dataframe to calculate the statistics on\n        group_var (string): \n            the variable by which to group df\n        df_name (string): \n            the variable used to rename the columns\n        \n    Return\n    --------\n        agg (dataframe): \n            a dataframe with the statistics aggregated for \n            all numeric columns. Each instance of the grouping variable will have \n            the statistics (mean, min, max, sum; currently supported) calculated. \n            The columns are also renamed to keep track of features created.\n    \n    \"\"\"\n    \n    # First calculate counts\n    counts = pd.DataFrame(df.groupby(group_var, as_index = False)[df.columns[1]].count()).rename(columns = {df.columns[1]: '%s_counts' % df_name})\n    \n    # Group by the specified variable and calculate the statistics\n    agg = df.groupby(group_var).agg(['mean', 'max', 'min', 'sum']).reset_index()\n    \n    # Need to create new column names\n    columns = [group_var]\n    \n    # Iterate through the variables names\n    for var in agg.columns.levels[0]:\n        # Skip the grouping variable\n        if var != group_var:\n            # Iterate through the stat names\n            for stat in agg.columns.levels[1][:-1]:\n                # Make a new column name for the variable and stat\n                columns.append('%s_%s_%s' % (df_name, var, stat))\n              \n    #  Rename the columns\n    agg.columns = columns\n    \n    # Merge with the counts\n    agg = agg.merge(counts, on = group_var, how = 'left')\n    \n    return agg","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c99bb22a98377fda42944fbd04ad688cb53e3177"},"cell_type":"markdown","source":"### Function to Calculate Categorical Counts"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"6899ea01794c3c3e44c7bbc6bde3613202d545d4"},"cell_type":"code","source":"def count_categorical(df, group_var, df_name):\n    \"\"\"Computes counts and normalized counts for each observation\n    of `group_var` of each unique category in every categorical variable\n    \n    Parameters\n    --------\n    df : dataframe \n        The dataframe to calculate the value counts for.\n        \n    group_var : string\n        The variable by which to group the dataframe. For each unique\n        value of this variable, the final dataframe will have one row\n        \n    df_name : string\n        Variable added to the front of column names to keep track of columns\n\n    \n    Return\n    --------\n    categorical : dataframe\n        A dataframe with counts and normalized counts of each unique category in every categorical variable\n        with one row for every unique value of the `group_var`.\n        \n    \"\"\"\n    \n    # Select the categorical columns\n    categorical = pd.get_dummies(df.select_dtypes('object'))\n\n    # Make sure to put the identifying id on the column\n    categorical[group_var] = df[group_var]\n\n    # Groupby the group var and calculate the sum and mean\n    categorical = categorical.groupby(group_var).agg(['sum', 'mean'])\n    \n    column_names = []\n    \n    # Iterate through the columns in level 0\n    for var in categorical.columns.levels[0]:\n        # Iterate through the stats in level 1\n        for stat in ['count', 'count_norm']:\n            # Make a new column name\n            column_names.append('%s_%s_%s' % (df_name, var, stat))\n    \n    categorical.columns = column_names\n    \n    return categorical","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"eda8a6b2429cde1fcbaf5f5beec0707f739b9e73"},"cell_type":"markdown","source":"### Function for KDE Plots of Variable\n\nWe also made a function that plots the distribution of variable colored by the value of `TARGET` (either 1 for did not repay the loan or 0 for did repay the loan). We can use this function to visually examine any new variables we create. This also calculates the correlation cofficient of the variable with the target which can be used as an approximation of whether or not the created variable will be useful. "},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"557b945ebd7663bee123b9074389141d38c18b6b"},"cell_type":"code","source":"# Plots the disribution of a variable colored by value of the target\ndef kde_target(var_name, df):\n    \n    # Calculate the correlation coefficient between the new variable and the target\n    corr = df['TARGET'].corr(df[var_name])\n    \n    # Calculate medians for repaid vs not repaid\n    avg_repaid = df.ix[df['TARGET'] == 0, var_name].median()\n    avg_not_repaid = df.ix[df['TARGET'] == 1, var_name].median()\n    \n    plt.figure(figsize = (12, 6))\n    \n    # Plot the distribution for target == 0 and target == 1\n    sns.kdeplot(df.ix[df['TARGET'] == 0, var_name], label = 'TARGET == 0')\n    sns.kdeplot(df.ix[df['TARGET'] == 1, var_name], label = 'TARGET == 1')\n    \n    # label the plot\n    plt.xlabel(var_name); plt.ylabel('Density'); plt.title('%s Distribution' % var_name)\n    plt.legend();\n    \n    # print out the correlation\n    print('The correlation between %s and the TARGET is %0.4f' % (var_name, corr))\n    # Print out average values\n    print('Median value for loan that was not repaid = %0.4f' % avg_not_repaid)\n    print('Median value for loan that was repaid =     %0.4f' % avg_repaid)\n    ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0ed0bafec723c5206f130895838245fcc8d91b26"},"cell_type":"markdown","source":"Let's deal with one dataframe at a time. First up is the `previous_applications`. This has one row for every previous loan a client had at Home Credit. A client can have multiple previous loans which is why we need to aggregate statistics for each client."},{"metadata":{"_uuid":"063a93d01f0afaebd41f256c13cbecd67d177f19"},"cell_type":"markdown","source":"### `previous_application`"},{"metadata":{"trusted":true,"_uuid":"4f81a6dce526a8627039e58d1f7025941fde2798"},"cell_type":"code","source":"previous = pd.read_csv('../input/previous_application.csv')\nprevious.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f70f561a3075d5c0f68c31cf33449d9db8583ae9"},"cell_type":"code","source":"# Calculate aggregate statistics for each numeric column\nprevious_agg = agg_numeric(previous.drop(columns = ['SK_ID_PREV']), group_var = 'SK_ID_CURR', df_name = 'previous_loans')\nprevious_agg.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"267a4f0a8097c4b13bbcb67faa25fafb7dee0af5"},"cell_type":"code","source":"# Calculate value counts for each categorical column\nprevious_counts = count_categorical(previous, group_var = 'SK_ID_CURR', df_name = 'previous_loans')\nprevious_counts.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"39ccf84d56774e155a8cf62cb1c516d7dc97c6c8"},"cell_type":"code","source":"print('Previous aggregated shape: ', previous_agg.shape)\nprint('Previous categorical counts shape: ', previous_counts.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cc0032e7c0f6f591854d31d484bd14f709f4ff13"},"cell_type":"markdown","source":"We can join the calculated dataframe to the main training dataframe using a merge. Then we should delete the calculated dataframes to avoid using too much of the kernel memory."},{"metadata":{"trusted":true,"_uuid":"7b7651f8ad471a23e8dac04c5ee8ee3c42b214ea"},"cell_type":"code","source":"train = pd.read_csv('../input/application_train.csv')\ntest = pd.read_csv('../input/application_test.csv')\n\n# Merge in the previous information\ntrain = train.merge(previous_counts, on ='SK_ID_CURR', how = 'left')\ntrain = train.merge(previous_agg, on = 'SK_ID_CURR', how = 'left')\n\ntest = test.merge(previous_counts, on ='SK_ID_CURR', how = 'left')\ntest = test.merge(previous_agg, on = 'SK_ID_CURR', how = 'left')\n\n# Remove variables to free memory\ngc.enable()\ndel previous, previous_agg, previous_counts\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cec09d0de0aeaed65b63ed59889b3c121cade929"},"cell_type":"markdown","source":"We are going to have to be careful about calculating too many features. We don't want to overwhelm the model with too many irrelevant features or features with too many missing values. In the previous notebook, we removed any features with more than 75% missing values. To be consistent, we will apply that same logic here. "},{"metadata":{"_uuid":"d4c38cd12402823c091e81dbcf42ae56e108c055"},"cell_type":"markdown","source":"## Function to Calculate Missing Values"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"836b21cd4edfa344efc9bed804bfb889fc33b5ff"},"cell_type":"code","source":"# Function to calculate missing values by column# Funct \ndef missing_values_table(df, print_info = False):\n        # Total missing values\n        mis_val = df.isnull().sum()\n        \n        # Percentage of missing values\n        mis_val_percent = 100 * df.isnull().sum() / len(df)\n        \n        # Make a table with the results\n        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n        \n        # Rename the columns\n        mis_val_table_ren_columns = mis_val_table.rename(\n        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n        \n        # Sort the table by percentage of missing descending\n        mis_val_table_ren_columns = mis_val_table_ren_columns[\n            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n        '% of Total Values', ascending=False).round(1)\n        \n        if print_info:\n            # Print some summary information\n            print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n                \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n                  \" columns that have missing values.\")\n        \n        # Return the dataframe with missing information\n        return mis_val_table_ren_columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"d6a1c60994bd2be69b842c5393dbe2799c4ff74c"},"cell_type":"code","source":"def remove_missing_columns(train, test, threshold = 90):\n    # Calculate missing stats for train and test (remember to calculate a percent!)\n    train_miss = pd.DataFrame(train.isnull().sum())\n    train_miss['percent'] = 100 * train_miss[0] / len(train)\n    \n    test_miss = pd.DataFrame(test.isnull().sum())\n    test_miss['percent'] = 100 * test_miss[0] / len(test)\n    \n    # list of missing columns for train and test\n    missing_train_columns = list(train_miss.index[train_miss['percent'] > threshold])\n    missing_test_columns = list(test_miss.index[test_miss['percent'] > threshold])\n    \n    # Combine the two lists together\n    missing_columns = list(set(missing_train_columns + missing_test_columns))\n    \n    # Print information\n    print('There are %d columns with greater than %d%% missing values.' % (len(missing_columns), threshold))\n    \n    # Drop the missing columns and return\n    train = train.drop(columns = missing_columns)\n    test = test.drop(columns = missing_columns)\n    \n    return train, test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6d90c583647b95c11fa72273931ef51d584e9795"},"cell_type":"code","source":"train, test = remove_missing_columns_columns(train, test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d8b6b99fcfc81e3965213e2e98480365a9c25db1"},"cell_type":"markdown","source":"# Applying to More Data"},{"metadata":{"_uuid":"c9ff11c7e3fbd97f328c40217a902f2fb3dcad73"},"cell_type":"markdown","source":"### Function to Aggregate Stats at the Client Level"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"2041ca7741921a3508258b616db39101e1e4d722"},"cell_type":"code","source":"def aggregate_client(df, group_vars, df_names):\n    \"\"\"Aggregate a dataframe with data at the loan level \n    at the client level\n    \n    Args:\n        df (dataframe): data at the loan level\n        group_vars (list of two strings): grouping variables for the loan \n        and then the client (example ['SK_ID_PREV', 'SK_ID_CURR'])\n        names (list of two strings): names to call the resulting columns\n        (example ['cash', 'client'])\n        \n    Returns:\n        df_client (dataframe): aggregated numeric stats at the client level. \n        Each client will have a single row with all the numeric data aggregated\n    \"\"\"\n    \n    # Aggregate the numeric columns\n    df_agg = agg_numeric(df, group_var = group_vars[0], df_name = df_names[0])\n    \n    # If there are categorical variables\n    if any(df.dtypes == 'object'):\n    \n        # Count the categorical columns\n        df_counts = count_categorical(df, group_var = group_vars[0], df_name = df_names[0])\n\n        # Merge the numeric and categorical\n        df_by_loan = df_counts.merge(df_agg, on = group_vars[0], how = 'outer')\n\n        gc.enable()\n        del df_agg, df_counts\n        gc.collect()\n\n        # Merge to get the client id in dataframe\n        df_by_loan = df_by_loan.merge(df[[group_vars[0], group_vars[1]]], on = group_vars[0], how = 'left')\n\n        # Remove the loan id\n        df_by_loan = df_by_loan.drop(columns = [group_vars[0]])\n\n        # Aggregate numeric stats by column\n        df_by_client = agg_numeric(df_by_loan, group_var = group_vars[1], df_name = df_names[1])\n\n        \n    # No categorical variables\n    else:\n        # Merge to get the client id in dataframe\n        df_by_loan = df_agg.merge(df[[group_vars[0], group_vars[1]]], on = group_vars[0], how = 'left')\n        \n        gc.enable()\n        del df_agg\n        gc.collect()\n        \n        # Remove the loan id\n        df_by_loan = df_by_loan.drop(columns = [group_vars[0]])\n        \n        # Aggregate numeric stats by column\n        df_by_client = agg_numeric(df_by_loan, group_var = group_vars[1], df_name = df_names[1])\n        \n    # Memory management\n    gc.enable()\n    del df, df_by_loan\n    gc.collect()\n\n    return df_by_client","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"65c0196b37488c09ec7205edcb6726d365b5a96a"},"cell_type":"markdown","source":"## Monthly Cash Data"},{"metadata":{"trusted":true,"_uuid":"e2e13040abf2af99ef9dcaea9b29f8128d2e4f07"},"cell_type":"code","source":"cash = pd.read_csv('../input/POS_CASH_balance.csv')\ncash.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e261b4a55167cf017eb5a190b8c83571b5e1398d"},"cell_type":"code","source":"cash_by_client = aggregate_client(cash, group_vars = ['SK_ID_PREV', 'SK_ID_CURR'], df_names = ['cash', 'client'])\ncash_by_client.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"aa1c17e2ea0742c9bb3acdf7e7e218b0a8fc4fea"},"cell_type":"code","source":"print('Cash by Client Shape: ', cash_by_client.shape)\ntrain = train.merge(cash_by_client, on = 'SK_ID_CURR', how = 'left')\ntest = test.merge(cash_by_client, on = 'SK_ID_CURR', how = 'left')\n\ngc.enable()\ndel cash, cash_by_client\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"03b30744261f48c0515e1d1601654a780eaf1d51"},"cell_type":"code","source":"train, test = remove_missing_columns(train, test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f15d0b5811e29892393f643cba60f407a24084a4"},"cell_type":"markdown","source":"## Monthly Credit Data"},{"metadata":{"trusted":true,"_uuid":"0bb1f02376642b2bb50ff35620a482312ecbf258"},"cell_type":"code","source":"credit = pd.read_csv('../input/credit_card_balance.csv')\ncredit.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f0a8e270271e7037d893386550a091e24f10a5cd"},"cell_type":"code","source":"credit_by_client = aggregate_client(credit, group_vars = ['SK_ID_PREV', 'SK_ID_CURR'], df_names = ['credit', 'client'])\ncredit_by_client.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9a6501414046b9c6c1b7b72c13d3227efde93435"},"cell_type":"code","source":"print('Credit by client shape: ', credit_by_client.shape)\n\ntrain = train.merge(credit_by_client, on = 'SK_ID_CURR', how = 'left')\ntest = test.merge(credit_by_client, on = 'SK_ID_CURR', how = 'left')\n\ngc.enable()\ndel credit, credit_by_client\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7c4d9012f8c6d426a40949c2f96883480193d680"},"cell_type":"code","source":"train, test = remove_missing_columns(train, test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"13a46dfc971d3010b340636879697b5bba0f2255"},"cell_type":"markdown","source":"### Installment Payments"},{"metadata":{"trusted":true,"_uuid":"fe3bef4aaa73791c63e02ec2435a884984408ace"},"cell_type":"code","source":"installments = pd.read_csv('../input/installments_payments.csv')\ninstallments.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e68d671fc3fe08df21b81404ee1c3c4030ebafff"},"cell_type":"code","source":"installments_by_client = aggregate_client(installments, group_vars = ['SK_ID_PREV', 'SK_ID_CURR'], df_names = ['installments', 'client'])\ninstallments_by_client.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e70b057d5f1ec2ba66738ec1ef260e02133a5de9"},"cell_type":"code","source":"print('Installments by client shape: ', installments_by_client.shape)\n\ntrain = train.merge(installments_by_client, on = 'SK_ID_CURR', how = 'left')\ntest = test.merge(installments_by_client, on = 'SK_ID_CURR', how = 'left')\n\ngc.enable()\ndel installments, installments_by_client\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"478cdd9f3387297c88cca15fe725425d4910b776"},"cell_type":"code","source":"train, test = remove_missing_columns(train, test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9bfb6d762bbdbfadf2ac7aac8da7d835e53f5679"},"cell_type":"code","source":"print('Final Training Shape: ', train.shape)\nprint('Final Testing Shape: ', test.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b91556063fd2fc8b53ed8b7e674822f96e65d6cd"},"cell_type":"markdown","source":" #### Save All Newly Calculated Features\n \n Unfortunately, saving all the created features does not work in a Kaggle notebook. You will have to run the code on your personal machine. I have run the code and uploaded the [entire datasets here](https://www.kaggle.com/willkoehrsen/home-credit-manual-engineered-features). I plan on doing some feature selection and uploading reduced versions of the datasets. Right now, they are slightly to big to handle in Kaggle notebooks or scripts. ."},{"metadata":{"trusted":true,"_uuid":"33e37b48ab79b6773cbbb4331aedc6117fecef23"},"cell_type":"code","source":" #train.to_csv('train_previous_raw.csv', index = False, chunksize = 500)\n #test.to_csv('test_previous_raw.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"7bd7f24dabe54d068e48b90ac1e42291bed6bf4d"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}