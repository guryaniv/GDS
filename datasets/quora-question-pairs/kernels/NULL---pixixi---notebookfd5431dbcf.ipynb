{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "4c1a45fc-c393-7703-bd8e-dd08a9ecbeda"
      },
      "source": [
        "test 001"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "5cb7fb56-16c3-7e6d-8346-174830209d92"
      },
      "outputs": [],
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load in \n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the \"../input/\" directory.\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
        "\n",
        "from subprocess import check_output\n",
        "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n",
        "\n",
        "# Any results you write to the current directory are saved as output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "05fcbe9e-eafd-7666-1e3b-fd98148b20bf"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Wed Apr 26 09:11:12 2017\n",
        "\n",
        "@author: pixixi\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from collections import Counter\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from scipy.optimize import minimize\n",
        "stops = set(stopwords.words(\"english\"))\n",
        "import xgboost as xgb\n",
        "from sklearn.cross_validation import train_test_split\n",
        "import multiprocessing\n",
        "import difflib\n",
        "\n",
        "\n",
        "train = pd.read_csv('/Volumes/Luna/Data/quora/train.csv')\n",
        "test = pd.read_csv('/Volumes/Luna/Data/quora/test.csv')\n",
        "\n",
        "tfidf = TfidfVectorizer(stop_words='english', ngram_range=(1, 1))\n",
        "#cvect = CountVectorizer(stop_words='english', ngram_range=(1, 1))\n",
        "\n",
        "tfidf_txt = pd.Series(train['question1'].tolist() + train['question2'].tolist() + test['question1'].tolist() + test['question2'].tolist()).astype(str)\n",
        "tfidf.fit_transform(tfidf_txt)\n",
        "#cvect.fit_transform(tfidf_txt)\n",
        "\n",
        "def diff_ratios(st1, st2):\n",
        "    seq = difflib.SequenceMatcher()\n",
        "    seq.set_seqs(str(st1).lower(), str(st2).lower())\n",
        "    return seq.ratio()\n",
        "\n",
        "def word_match_share(row):\n",
        "    q1words = {}\n",
        "    q2words = {}\n",
        "    for word in str(row['question1']).lower().split():\n",
        "        if word not in stops:\n",
        "            q1words[word] = 1\n",
        "    for word in str(row['question2']).lower().split():\n",
        "        if word not in stops:\n",
        "            q2words[word] = 1\n",
        "    if len(q1words) == 0 or len(q2words) == 0:\n",
        "        return 0\n",
        "    shared_words_in_q1 = [w for w in q1words.keys() if w in q2words]\n",
        "    shared_words_in_q2 = [w for w in q2words.keys() if w in q1words]\n",
        "    R = (len(shared_words_in_q1) + len(shared_words_in_q2))/(len(q1words) + len(q2words))\n",
        "    return R\n",
        "\n",
        "def get_features(df_features):\n",
        "    print('nouns...')\n",
        "    df_features['question1_nouns'] = df_features.question1.map(lambda x: [w for w, t in nltk.pos_tag(nltk.word_tokenize(str(x).lower())) if t[:1] in ['N']])\n",
        "    df_features['question2_nouns'] = df_features.question2.map(lambda x: [w for w, t in nltk.pos_tag(nltk.word_tokenize(str(x).lower())) if t[:1] in ['N']])\n",
        "    df_features['z_noun_match'] = df_features.apply(lambda r: sum([1 for w in r.question1_nouns if w in r.question2_nouns]), axis=1)  #takes long\n",
        "    print('lengths...')\n",
        "    df_features['z_len1'] = df_features.question1.map(lambda x: len(str(x)))\n",
        "    df_features['z_len2'] = df_features.question2.map(lambda x: len(str(x)))\n",
        "    df_features['z_word_len1'] = df_features.question1.map(lambda x: len(str(x).split()))\n",
        "    df_features['z_word_len2'] = df_features.question2.map(lambda x: len(str(x).split()))\n",
        "    print('india/indian')\n",
        "    df_features['question1_india'] = df_features.question1.map(lambda x: 'india' in str(x))\n",
        "    df_features['question2_india'] = df_features.question2.map(lambda x: 'india' in str(x))\n",
        "    df_features['indiacator'] = df_features['question1_india'] + df_features['question2_india']\n",
        "    print('difflib...')\n",
        "    df_features['z_match_ratio'] = df_features.apply(lambda r: diff_ratios(r.question1, r.question2), axis=1)  #takes long\n",
        "    print('word match...')\n",
        "    df_features['z_word_match'] = df_features.apply(word_match_share, axis=1, raw=True)\n",
        "    print('tfidf...')\n",
        "    df_features['z_tfidf_sum1'] = df_features.question1.map(lambda x: np.sum(tfidf.transform([str(x)]).data))\n",
        "    df_features['z_tfidf_sum2'] = df_features.question2.map(lambda x: np.sum(tfidf.transform([str(x)]).data))\n",
        "    df_features['z_tfidf_mean1'] = df_features.question1.map(lambda x: np.mean(tfidf.transform([str(x)]).data))\n",
        "    df_features['z_tfidf_mean2'] = df_features.question2.map(lambda x: np.mean(tfidf.transform([str(x)]).data))\n",
        "    df_features['z_tfidf_len1'] = df_features.question1.map(lambda x: len(tfidf.transform([str(x)]).data))\n",
        "    df_features['z_tfidf_len2'] = df_features.question2.map(lambda x: len(tfidf.transform([str(x)]).data))\n",
        "    return df_features.fillna(0.0)\n",
        "\n",
        "train = get_features(train)\n",
        "#train.to_csv('train.csv', index=False)\n",
        "\n",
        "col = [c for c in train.columns if c[:1]=='z']\n",
        "\n",
        "pos_train = train[train['is_duplicate'] == 1]\n",
        "neg_train = train[train['is_duplicate'] == 0]\n",
        "p = 0.165\n",
        "scale = ((len(pos_train) / (len(pos_train) + len(neg_train))) / p) - 1\n",
        "while scale > 1:\n",
        "    neg_train = pd.concat([neg_train, neg_train])\n",
        "    scale -=1\n",
        "neg_train = pd.concat([neg_train, neg_train[:int(scale * len(neg_train))]])\n",
        "train = pd.concat([pos_train, neg_train])\n",
        "\n",
        "x_train, x_valid, y_train, y_valid = train_test_split(train[col], train['is_duplicate'], test_size=0.2, random_state=0)\n",
        "\n",
        "params = {}\n",
        "params[\"objective\"] = \"binary:logistic\"\n",
        "params['eval_metric'] = 'logloss'\n",
        "params[\"eta\"] = 0.02\n",
        "params[\"subsample\"] = 0.7\n",
        "params[\"min_child_weight\"] = 1\n",
        "params[\"colsample_bytree\"] = 0.7\n",
        "params[\"max_depth\"] = 4\n",
        "params[\"silent\"] = 1\n",
        "params[\"seed\"] = 1632\n",
        "\n",
        "d_train = xgb.DMatrix(x_train, label=y_train)\n",
        "d_valid = xgb.DMatrix(x_valid, label=y_valid)\n",
        "watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n",
        "bst = xgb.train(params, d_train, 500, watchlist, early_stopping_rounds=50, verbose_eval=100) #change to higher #s\n",
        "print(log_loss(train.is_duplicate, bst.predict(xgb.DMatrix(train[col]))))\n",
        "\n",
        "test = get_features(test)\n",
        "#test.to_csv('test.csv', index=False)\n",
        "\n",
        "sub = pd.DataFrame()\n",
        "sub['test_id'] = test['test_id']\n",
        "sub['is_duplicate'] = bst.predict(xgb.DMatrix(test[col]))\n",
        "\n",
        "sub.to_csv('z08_submission_xgb_01.csv', index=False)"
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}