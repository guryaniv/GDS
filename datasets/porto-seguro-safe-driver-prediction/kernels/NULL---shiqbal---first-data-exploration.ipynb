{"nbformat_minor": 1, "cells": [{"cell_type": "markdown", "metadata": {"_uuid": "5d2a1ec40b001f66bf449c8b58529b880e6cc0c7", "_cell_guid": "d4ef498c-a8a0-4b16-a07b-d876e3193642"}, "source": ["This is my very first completely self-written Kernel. I have stolen ideas from all over the place and just applied to this dataset to see to what extent I understand (Visualization most notably from @Anisotropic). I am at this point in reinforcing my fundamentals, specially the ones I have picked up reading a number of ML books.\n", "\n", "There is some work to be done (add more key points I have picked up and the reasoning behind those) and this Kernel is not final since I want to do more (may be I will do that in a second part of this). To do (apart from the longer list I have below) for this Kernel involves:\n", "\n", "1. Discretizing certain continuous variables\n", "2. Outlier cleaning for important features\n", "3. Re-looking NaNs with a reasonable assumption that NaNs are generally systematic rather than random (and comparing NaNs with test)\n", "4. Looking at more feature transformations for important features\n", "5. Do I really understand what I am doing or am I doing it while others do it? (*Most Important*!)\n", "\n", "\n", "**I would love some feedback. Thank you in advance!**"]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "127ebb601d5cc36ab5158038dd0e00071324c56f", "_cell_guid": "11b3f50b-77fe-40e2-b2a6-6907db2e7a23"}, "outputs": [], "source": ["import pandas as pd\n", "import numpy as np\n", "from scipy.stats import pearsonr, pointbiserialr\n", "from sklearn.metrics import matthews_corrcoef, classification_report, confusion_matrix\n", "from sklearn.ensemble import RandomForestClassifier\n", "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n", "from sklearn.preprocessing import StandardScaler\n", "from sklearn.base import clone\n", "import seaborn as sns\n", "import matplotlib.pyplot as plt\n", "%matplotlib inline\n", "from IPython.core.display import display, HTML\n", "import missingno as msno\n", "from itertools import combinations_with_replacement, combinations\n", "from collections import defaultdict\n", "import warnings\n", "warnings.filterwarnings('ignore')\n", "\n", "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "61d7d7cecda8ebb56a3c57e3bace19d0bc5e031f", "_cell_guid": "df8578a6-a497-48c9-83c1-321a23563053", "collapsed": true}, "outputs": [], "source": ["#train = pd.read_csv(\"D:/Kaggle_Data/Safe Driver/train.csv\")  # could have used the na_values=-1 argument for automatic replacement of -1 with NaNs\n", "#test = pd.read_csv(\"D:/Kaggle_Data/Safe Driver/test.csv\")\n", "train = pd.read_csv(\"../input/train.csv\")\n", "test = pd.read_csv(\"../input/test.csv\")"]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "f1bc1432a42fc0f0873565e10156ead25051a0c5", "_cell_guid": "e13fd4e6-3ec2-4440-9cb1-2556f2bc86f6"}, "outputs": [], "source": ["train.shape"]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "3a58eca6596bc39b65ab5f904cbc8fb769ddbac2", "_cell_guid": "831e17e4-b991-48e5-904c-2b52ea231a79"}, "outputs": [], "source": ["train.target.value_counts(normalize=True)*100"]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "c9eef9d97de4bfff4671f17cb14b5a341577a88b", "_cell_guid": "adb2cbe5-e567-4b84-acdd-85577474c02c"}, "outputs": [], "source": ["plt.figure(figsize=(4,4))\n", "ax = sns.countplot(train.target)\n", "ax.set_facecolor('white')"]}, {"cell_type": "markdown", "metadata": {"_uuid": "3692bba8be5ada09ab8bce728251392be31a1ee5", "_cell_guid": "047db2f6-a5f3-457a-a23b-63b62fd9b640"}, "source": ["Now that's one ugly unbalanced dataset. Gotta learn to live with it."]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "854811224d737e8065237540f6c4f952a6ec77fd", "_cell_guid": "83165119-50f7-4b4d-af84-298eb7bcf9a0"}, "outputs": [], "source": ["plt.figure(figsize=(4,4))\n", "ax = sns.countplot(train.dtypes)\n", "ax.set_facecolor('white')"]}, {"cell_type": "markdown", "metadata": {"_uuid": "3fea33e8d26f66dac1bae22b40a27f77cb0b445b", "_cell_guid": "51447538-062b-42a4-bcf6-be005c6fd51f"}, "source": ["In the train and test data, features that belong to similar groupings are tagged as such in the feature names (e.g., __ind, reg, car, calc__). In addition, feature names include the postfix __bin__ to indicate binary features and __cat__ to indicate categorical features. __Features without these designations are either continuous or ordinal__. Values of __-1__ indicate that the feature was missing from the observation. The target columns signifies whether or not a claim was filed for that policy holder."]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "a49a509aa61cdbdda9f6aa14d9b6cd48866360ca", "_cell_guid": "79ce8875-c741-4a74-a7de-300e2bd04395"}, "outputs": [], "source": ["pd.Series(train.columns)[:10]  # a trivial conversion to Series to see the names without the ugly single quotes"]}, {"cell_type": "markdown", "metadata": {"_uuid": "116519325f2b8da521f750fc4f22eb3df21e43da", "_cell_guid": "42925ae5-4f69-4a52-9dc3-0ef5b0666d42"}, "source": ["Steps:\n", "\n", "1. Get rid of the unnecessary \"ps_\" prefix in all columns since it just makes it hard to see the names\n", "2. Create name clusters to see how strongly the similarly grouped features are correlated\n", "3. Since no.2 would automatically include a few columns in each group that belong to the binary variable, we will leave them alone in the first step"]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "8801ee77246100fe45f849aae7998341ea68064d", "_cell_guid": "2014ce88-51af-48b5-a386-2730c1c5d043", "collapsed": true}, "outputs": [], "source": ["# Find the first underscore from the left and keep the remaining characters of the col.name\n", "new_col_names = [s[s.find(\"_\")+1:] for s in train.columns]"]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "05679446d76a700a1c83290ebf2a8242c0f6ea2d", "_cell_guid": "c6dfd484-467f-466b-abab-07d70e51fe94", "collapsed": true}, "outputs": [], "source": ["test_new_col_names = new_col_names[:]\n", "test_new_col_names.remove('target')"]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "7c808a56c208229cf2c4f5b366a7e924003d7c62", "_cell_guid": "2954627d-430c-4752-b735-b91a2766bac1"}, "outputs": [], "source": ["pd.DataFrame.from_dict({\"New Names\":new_col_names, \"Old Names\":list(train.columns)})"]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "77f1f5074e383bc138a2d3701f2bdace1c99f4db", "_cell_guid": "a79cfa55-9e11-4faf-b464-ad4c10a81cf2", "collapsed": true}, "outputs": [], "source": ["train.columns = new_col_names\n", "test.columns = test_new_col_names"]}, {"cell_type": "markdown", "metadata": {"_uuid": "dfd6850bdc6726cf0be979fd2dc7c4ee32143f63", "_cell_guid": "29c8d427-accc-43f1-b024-c619ddbb17ee"}, "source": ["Now we'll try to group the column names depending on their prefix. We will use the default dictionary for this purpose to store the prefix as the key and the list of columns in the value."]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "2bc9f00e5c17527441c44299ce0ec042df76b4ef", "scrolled": true, "_cell_guid": "6945cdbf-ac4a-4e78-b1e5-3c57e8ce5061"}, "outputs": [], "source": ["# checking how many prefixes exist\n", "prefixes=  set([s[:s.find(\"_\")] for s in train.columns if \"_\" in s])\n", "print(prefixes)"]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "b34a96e251d8dc7ca49a7cabb1b0f9a2542bd972", "_cell_guid": "1ba56ec9-d221-4136-b4f1-2960621dcf42", "collapsed": true}, "outputs": [], "source": ["grouped_cols = defaultdict(list)\n", "\n", "for prefix in prefixes:\n", "    grouped_cols[prefix]=[col for col in train.columns if prefix in col]"]}, {"cell_type": "markdown", "metadata": {"_uuid": "daa794c2078285ac23dc70715abc859be683e797", "_cell_guid": "5e6c3508-9e05-4914-9560-2e420dd52b59"}, "source": ["Checking if the dictionary is indeed working"]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "1085ee21de5a38afeb664bd0177359f31f08325d", "_cell_guid": "4f694bdb-c19f-40cd-b624-81bb4cf987fc"}, "outputs": [], "source": ["grouped_cols['reg']"]}, {"cell_type": "markdown", "metadata": {"_uuid": "74272e6fce150f4c703591193f969339c67ff99f", "_cell_guid": "7085ea50-c0b6-434b-bc2a-141cb8b2a07d"}, "source": ["### The nightmare of NaNs"]}, {"cell_type": "markdown", "metadata": {"_uuid": "c5bba65bd53554b554d83357ccd4f4761d34fb2a", "_cell_guid": "0affdfea-193a-483d-a2de-d5020df5325a"}, "source": ["First have to see what kind of missing value counts are we facing since correlations etc. only make sense if the data has atleast a semblance of completion. Since NaNs have been replaced with -1, we take it back so we can visualize it for our ease better. Here we go!"]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "0a02394f8ad998c0ce330de342b862515cc2d64e", "_cell_guid": "e13e1f20-cd75-416c-93de-43ebd4f2df3d", "collapsed": true}, "outputs": [], "source": ["train.replace(-1,np.nan, inplace=True)\n", "test.replace(-1,np.nan, inplace=True)"]}, {"cell_type": "markdown", "metadata": {"_uuid": "73007b4aefe85c912614f3371a56ab07a18727f4", "_cell_guid": "e5096a20-25ac-43f8-b407-41bea9916871"}, "source": ["Below we see the absolute count and more importantly their percentage value as a number of entire column length"]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "e442263710af698cb1358f566939118781a86fbb", "_cell_guid": "dd48bed9-deeb-44c8-964d-98a36507c018", "collapsed": true}, "outputs": [], "source": ["def display_nans(df):\n", "    '''\n", "    returns a dataframe with Number of NaNs in each column and also as a percentage of all rows in that column\n", "\n", "    :param df: DataFrame containing NaNs. Type: pandas.DataFrame\n", "    :return: DataFrame with indices as column names and columns as no. of NaN values and their percentage of # of rows.\n", "    '''\n", "    nans = pd.concat([df.isnull().sum(), (df.isnull().sum() / df.shape[0]) * 100], axis=1,\n", "                     keys=['Num_NaN', 'NaN_Percent'])\n", "    return nans[nans.Num_NaN > 0]"]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "71c1f49c41706b0ac0cd55f712da3ed1926d08a3", "_cell_guid": "49019079-3669-4b41-9344-b65dfcb592c6"}, "outputs": [], "source": ["# Train NaNs\n", "display_nans(train)"]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "c676c386e34262e1c99dde49705a32e813b555fe", "_cell_guid": "4bf45166-ec3c-41b3-94be-df457ebb9143"}, "outputs": [], "source": ["# Test NaNs\n", "display_nans(test)"]}, {"cell_type": "markdown", "metadata": {"_uuid": "9bdd487cceac77092e1cc822a5f0d3830aef57fc", "_cell_guid": "839809a7-9e35-499f-ae48-5e4f8540ce57"}, "source": ["Before we do something about the NaNs, it is useful to see if their missing has some kind of correlation. That might help us do an advanced version of imputation."]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "f0e6e2b5e0bb4aa3fc60a06ccb12e49a50ba5951", "_cell_guid": "49f3a87e-0c62-4789-9e5c-ba060678eab1"}, "outputs": [], "source": ["nans = pd.concat([train.isnull().sum(), (train.isnull().sum() / train.shape[0]) * 100], axis=1, keys=['Num_NaN', 'NaN_Percent'])\n", "cols_with_nans = nans[nans.Num_NaN > 0].index\n", "msno.matrix(df=train.loc[:,cols_with_nans], figsize=(20, 20), color=(0.24, 0.77, 0.77))"]}, {"cell_type": "markdown", "metadata": {"_uuid": "9cb77bfe77923785c1cca0de578435e79fbcae71", "_cell_guid": "a68b9ec0-45ff-42d0-8319-ef7be14c2a50"}, "source": ["Although there does seem to be some kind of correlation for example between all the ind NaNs, we will use the prefixes (since they allude to similar groupings) to look for correlation and fill up the columns. For the sake of simplicity I will be comparing them in pairs.\n", "\n", "Note that a few columns like car_03_cat and car_05_cat have a lot more nans so filling them is optional. One approach could be to get rid of them. Yet another approach is to see if they are nan in case of any particular value or set of values in other columns and impute them accordingly. I don't know how I could do that very neatly but hell, I'll give it a try later at that too (Perhaps treating it as a sub-ML problem)"]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "d51044fef8a4f5e7153801a3f8d0616353704427", "_cell_guid": "7130a7e1-b898-43b7-bf86-9f1c183d7625"}, "outputs": [], "source": ["cols_with_nans_ind = [col for col in cols_with_nans if \"ind\" in col]\n", "\n", "# ind_04_cat has a lot of target 1.0s than one might expect when its value is Null. One option \n", "# could be to create a new category \"2\" or something.\n", "\n", "for col1, col2 in combinations(cols_with_nans_ind, 2):\n", "    print(col1,col2, \":\", end=\" \")\n", "    count_of_both_nans = len(train[train[col1].isnull()].index & train[train[col2].isnull()].index)\n", "    print(count_of_both_nans, 'common indices')"]}, {"cell_type": "markdown", "metadata": {"_uuid": "4ea8d770ab555584e1796672ae41c4426f04e3e1", "_cell_guid": "7e707a06-e2c3-40a0-938b-972d1709b0a2"}, "source": ["The above result shows that when ind_04_cat is NULL then the other two columns namely ind_02_cat and ind_05_cat are also NULL. This means when filling in these NaN values, we ought to maintain this consistency and use ind_04_cat as the base. \n", "\n", "What about the other nans that only exist in only the other two columns i.e. 02 and 05 and in fact are the majority? Well again for the simplicity, I will fill them too with the same value since we'd be filling them up with the mode of those columns. (They could have been filled in some other fashion like one-way ANOVA by checking the correlation between a continuous variable and categorical variable but lets not sweat too much over a relatively small number of NaN values).\n", "\n", "Side Note: On the other hand, the NaNs of ind_02_cat and ind_05_cat do not seem to be correlated to one another as only two extra instances of these are together null apart from the 79 that they share in common with ind_04_cat."]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "ef2ebab542b19c43b2bc22d5588b7f5bca20f1ec", "_cell_guid": "4b940384-fedb-45ad-9caf-d9f559783986"}, "outputs": [], "source": ["train['ind_04_cat'].value_counts(dropna=False)"]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "9f1e5c2d751de3eaebbc28d22c3e47a22bc55315", "_cell_guid": "e1709ae9-eb11-4b7f-af5e-8d4396604b8b"}, "outputs": [], "source": ["# Since 0 occurs more often and the values are categorical, we will impute it with the mode\n", "train['ind_04_cat'].fillna(value=train['ind_04_cat'].mode()[0], inplace=True)  # dont forget the damn [0]\n", "test['ind_04_cat'].fillna(value=train['ind_04_cat'].mode()[0], inplace=True)  # Test NaNs are filled with Train mode values\n", "\n", "pd.DataFrame({'Train':train['ind_04_cat'].value_counts(dropna=False), 'Test':test['ind_04_cat'].value_counts(dropna=False)})"]}, {"cell_type": "markdown", "metadata": {"_uuid": "e90a823bd573fa2c2440abb8c174669d633e4714", "_cell_guid": "9bd08943-f7c0-418d-8a08-c92958025869"}, "source": ["Now we need to check what values of ind_02_cat and ind_05_cat occurs most often for the 0.0 of ind_04_cat"]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "79ca8df8bc60f764a14adc3c663b5365a40cdae0", "_cell_guid": "c5d319cf-5f3f-4b5d-9119-a9e7bbfe15c5"}, "outputs": [], "source": ["print('For ind_02_cat:', '\\n', train.loc[train.ind_04_cat==0.0,'ind_02_cat'].value_counts())\n", "print(\"*\"*30)\n", "print('For ind_05_cat:', '\\n', train.loc[train.ind_04_cat==0.0,'ind_05_cat'].value_counts())"]}, {"cell_type": "markdown", "metadata": {"_uuid": "3874efc52b5371034873a205ee907810a40ddb61", "_cell_guid": "09160953-2b0f-4f72-84e6-857975355c4b"}, "source": ["As can be seen above, after filling up the ind_04_cat, the overall mode holds it's value when conditionally checked against only the 0.0 value of ind_04_cat. Hence we're not doing anything crazy by filling up those NaNs with the mode of that column."]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "cad8a451a9e7ebc54f288b03ac1080adfd87c93e", "_cell_guid": "b6c56f1c-4edc-4226-9f89-a5360d87c9ff", "collapsed": true}, "outputs": [], "source": ["# So for both the mode value holds.\n", "train.ind_02_cat.fillna(train.ind_02_cat.mode()[0], inplace=True)\n", "train.ind_05_cat.fillna(train.ind_05_cat.mode()[0], inplace=True)\n", "\n", "test.ind_02_cat.fillna(train.ind_02_cat.mode()[0], inplace=True)\n", "test.ind_05_cat.fillna(train.ind_05_cat.mode()[0], inplace=True)"]}, {"cell_type": "markdown", "metadata": {"_uuid": "5254f04b09db511e83e351d83c76695f50eb9958", "_cell_guid": "57c4c7af-c187-43ee-aeed-876a09808905"}, "source": ["Dropping the overly NaN-ed columns"]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "fab1faf83e8429d4e7eff7cd9db0c8f4924757a2", "_cell_guid": "d12305e5-c38a-499b-b243-1162c5cfe9f5", "collapsed": true}, "outputs": [], "source": ["train.drop(['car_03_cat','car_05_cat'], axis=1, inplace=True)\n", "test.drop(['car_03_cat','car_05_cat'], axis=1, inplace=True)"]}, {"cell_type": "markdown", "metadata": {"_uuid": "67c3d166cc795c945e91e36dec3bed5bf36466af", "_cell_guid": "19383a5d-fed1-464d-b736-bc09f6e6a392"}, "source": ["Rechecking the NaN situation"]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "6a4ea88bee4295f3390c8d9b1d2bdc52600b777a", "_cell_guid": "f53bf492-dd27-4bbc-b9b7-dbe3787685c8"}, "outputs": [], "source": ["#Train\n", "display_nans(train)"]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "0c71c91520bde2a263f0067762f0bc12de2f78bf", "_cell_guid": "f6a5afeb-09ef-48be-9688-e74ddb887976"}, "outputs": [], "source": ["#Test\n", "display_nans(test)"]}, {"cell_type": "markdown", "metadata": {"_uuid": "86839f7712148538e0725f5105b2475f79a83157", "_cell_guid": "98cebcef-5c79-4210-addc-bca7a8c5b76b"}, "source": ["Since we'd already seen before that the other NaNs do not seem to be closely correlated, we'd just go ahead with imputing them column by column."]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "e3ae65b00c99ec3c91fbd34f483b9041ca7e518b", "_cell_guid": "173c6d12-8e40-4424-8079-5ae39b6d5ba2"}, "outputs": [], "source": ["# starting with the easiest ones i.e. with the fewest values and seem to be ordinal\n", "#train.car_01_cat.value_counts(dropna=False)  # uncheck one at a time to see the value counts\n", "#train.car_02_cat.value_counts(dropna=False)\n", "train.car_11.value_counts(dropna=False)"]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "ff3b9135c6da75f5f5a19ec8217d552ab245b273", "_cell_guid": "299e176f-91a9-416a-8ed9-4b0edaf173e2", "collapsed": true}, "outputs": [], "source": ["train.car_01_cat.fillna(train.car_01_cat.mode()[0], inplace=True)\n", "test.car_01_cat.fillna(train.car_01_cat.mode()[0], inplace=True)\n", "\n", "train.car_02_cat.fillna(train.car_02_cat.mode()[0], inplace=True)\n", "test.car_02_cat.fillna(train.car_02_cat.mode()[0], inplace=True)\n", "\n", "train.car_11.fillna(train.car_11.mode()[0], inplace=True)  # assuming car_11_cat column has nothing to do with this one\n", "test.car_11.fillna(train.car_11.mode()[0], inplace=True)  # assuming car_11_cat column has nothing to do with this one\n", "\n", "train.car_12.fillna(train.car_12.mean(), inplace=True)  # mean since car_12 is continuous\n", "test.car_12.fillna(train.car_12.mean(), inplace=True)  # mean since car_12 is continuous"]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "c359171ba36b34cb1d3038095505e53b02a4f722", "_cell_guid": "1c2a21a3-ae3f-4034-9d0b-4b193900dcee"}, "outputs": [], "source": ["f, axarr = plt.subplots(1,4, figsize=(16,5))\n", "train.plot(x=\"target\", y=\"reg_03\", ax=axarr[0], kind=\"scatter\");\n", "train.plot(x=\"target\", y=\"car_09_cat\",ax=axarr[1], kind=\"scatter\");\n", "train.plot(x=\"target\", y=\"car_07_cat\",ax=axarr[2], kind=\"scatter\");\n", "train.plot(x=\"target\", y=\"car_14\",ax=axarr[3], kind=\"scatter\");"]}, {"cell_type": "markdown", "metadata": {"_uuid": "2ac98b9fd34c542d7bf1f21ddcdbc58c4268f00c", "_cell_guid": "5cc64130-9c62-49aa-bba8-79ff1636b423"}, "source": ["As can be seen above, the data for reg_03 can basically take the same values for both target boolean values. Hence will probably be of limited significance. Same problem for car_09_cat, car_07_cat and car_14."]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "48456e5c80ba8196d9df5fed1510014886da22e6", "_cell_guid": "43f70d76-83a4-4bd5-8f42-9398c70f6441", "collapsed": true}, "outputs": [], "source": ["train.reg_03.fillna(train.reg_03.mean(),inplace=True)\n", "test.reg_03.fillna(train.reg_03.mean(),inplace=True)\n", "\n", "train.car_09_cat.fillna(train.car_09_cat.mode()[0],inplace=True)\n", "test.car_09_cat.fillna(train.car_09_cat.mode()[0],inplace=True)\n", "\n", "train.car_07_cat.fillna(train.car_07_cat.mode()[0], inplace=True)\n", "test.car_07_cat.fillna(train.car_07_cat.mode()[0], inplace=True)\n", "\n", "train.car_14.fillna(train.car_14.mode()[0], inplace=True)\n", "test.car_14.fillna(train.car_14.mode()[0], inplace=True)"]}, {"cell_type": "markdown", "metadata": {"_uuid": "c2b3eadc8f3e80cfa9e84de927c07e0a7ff01826", "_cell_guid": "35d10f8a-d102-485a-b361-bb7a5fd87dea"}, "source": ["Final check that we have indeed gotten rid of all the damn NaNs :D"]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "4fc38bc1c1ff8de5ecfd6b0fee0cc35f9eaddf78", "_cell_guid": "fc81169a-7df9-43dd-ae43-d10d74ff119c"}, "outputs": [], "source": ["# Train\n", "display_nans(train)"]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "96fc34a3b924546e6876133c0d7fc3b04bd8823d", "_cell_guid": "c3fc7b44-c480-4421-baee-96cd62742ac3"}, "outputs": [], "source": ["# Test\n", "display_nans(test)"]}, {"cell_type": "markdown", "metadata": {"_uuid": "08014a92a015aaa81a702576d7592672b5d18614", "_cell_guid": "34cc308e-7e8b-4261-9e4a-c6d0d1488c78"}, "source": ["### Heatmaps for correlations between continuous variables of the same groupings (and also with target)"]}, {"cell_type": "markdown", "metadata": {"_uuid": "a48f2808571cb04766389669d9994fc73a8efb13", "_cell_guid": "9fe168e1-8e96-4516-94e1-d310bc9920a6"}, "source": ["Before looking for correlations, it is very helpful to remind ourselves that all the many kinds of \"patterns\" return a zero for linear correlation:"]}, {"cell_type": "markdown", "metadata": {"_uuid": "d870aaf552fa066f8fec8d28ab3cb65418083aa2", "_cell_guid": "5559898f-e8b0-4604-ba23-aa240d8a8593"}, "source": ["![](http://cdn-ak.f.st-hatena.com/images/fotolife/h/hsameshima/20130703/20130703153559.png)"]}, {"cell_type": "markdown", "metadata": {"_uuid": "ec16d0e6a992d49fe3d8a97eed565676141adbca", "_cell_guid": "22aa8ecf-7930-4041-a85a-95392a5dadf3"}, "source": ["We will start by drawing a heatmap of all sets but ensuring that no categorical or binary variables are selected since they need a different kind of treatment"]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "0478de03cbbc210c378c315f032f1503e66128cb", "_cell_guid": "95712c7e-780f-4c8e-8e73-53397a007b82", "collapsed": true}, "outputs": [], "source": ["def draw_heatmap(filtered_cols, train, fmt='.1f', calc_corr=True):\n", "    sub_train = train.loc[:,filtered_cols]\n", "    f,ax = plt.subplots(figsize=(len(filtered_cols),len(filtered_cols)))\n", "    if calc_corr:\n", "        sns.heatmap(sub_train.corr(), annot=True, fmt= '.1f',ax=ax, vmin=0, vmax=1);\n", "    else:\n", "        sns.heatmap(train, annot=True, fmt=fmt,ax=ax);"]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "ac12260c9f313fa7d09cf84927faa6f6fbb47159", "_cell_guid": "db7f41e1-6b88-42e5-b215-0dd2ca2503a1"}, "outputs": [], "source": ["prefix='calc'\n", "filtered_cols = [col for col in grouped_cols[prefix] if ('bin' not in col) and ('cat' not in col)] + ['target']\n", "draw_heatmap(filtered_cols, train)"]}, {"cell_type": "markdown", "metadata": {"_uuid": "5d9f3607d3c163dceac847f5dffda81aff6b8a8d", "_cell_guid": "437d55af-6b82-455c-8913-e05ebb0c1591"}, "source": ["So it seems that the \"calc.\" fields seem that they are at least linearly independent."]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "e55a478965f7d1457bf65468e3f00c4ff3cc7dfc", "_cell_guid": "07e88b28-6a07-4c51-9fbe-5ab566c23da0"}, "outputs": [], "source": ["prefix='reg'\n", "filtered_cols = [col for col in grouped_cols[prefix] if ('bin' not in col) and ('cat' not in col)] + ['target']\n", "draw_heatmap(filtered_cols, train)"]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "f5240dab9862eb623e927253ae3de3fa078a2657", "_cell_guid": "c788de13-d9f1-4dd5-b581-7bed5bc05953"}, "outputs": [], "source": ["prefix='ind'\n", "filtered_cols = [col for col in grouped_cols[prefix] if ('bin' not in col) and ('cat' not in col)] + ['target']\n", "draw_heatmap(filtered_cols, train)"]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "b1c9d965310efd40ddbd095a750e22346ff3ad35", "_cell_guid": "ec441b6d-8a2b-42c8-886b-0b864449264f"}, "outputs": [], "source": ["prefix='ind'\n", "filtered_cols = [col for col in grouped_cols[prefix] if ('bin' not in col) and ('cat' not in col)] + [\"target\"]\n", "sub_train = train.loc[:,filtered_cols]\n", "sns.pairplot(sub_train,size=2.5,hue=\"target\");"]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "e585940a0a87cf954ca4f51bece7d42bb655972d", "_cell_guid": "0ee53eb5-5078-4da0-acb3-875b425816dd"}, "outputs": [], "source": ["prefix='car'\n", "filtered_cols = [col for col in grouped_cols[prefix] if ('bin' not in col) and ('cat' not in col)] + ['target']\n", "draw_heatmap(filtered_cols, train)"]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "260757f9d7ac5716b95ed35e3243fcb4be2eb47a", "scrolled": false, "_cell_guid": "1d94a8db-531e-4362-b31c-94f89aa3e768"}, "outputs": [], "source": ["prefix='car'\n", "filtered_cols = [col for col in grouped_cols[prefix] if ('bin' not in col) and ('cat' not in col)] + [\"target\"]\n", "sub_train = train.loc[:,filtered_cols]\n", "sns.pairplot(train,size=2.5, vars=filtered_cols,hue=\"target\", plot_kws={'alpha':0.3});"]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "590fe2269650e812287ac359e4995842b9dfdb33", "_cell_guid": "5b64eee6-d066-4664-aa49-21a9d9902e1d", "collapsed": true}, "outputs": [], "source": ["sns.lmplot(x=\"car_13\", y=\"car_15\", hue=\"target\", data=train,scatter_kws={'alpha':0.3});"]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "9a3fd795714cc5fa1789231fb007d4ed5c59b26d", "_cell_guid": "5d6f78a6-8350-4f02-acf9-a68a35261a96", "collapsed": true}, "outputs": [], "source": ["sns.lmplot(x=\"car_12\", y=\"car_15\", hue=\"target\", data=train,scatter_kws={'alpha':0.7});"]}, {"cell_type": "markdown", "metadata": {"_uuid": "3b7b9d225b357b7d0c00e0404a19145cae1103cb", "_cell_guid": "eacf7d8b-046c-463a-9320-7dd28fac3444"}, "source": ["This was an attempt to see closer if there is any hope of seperation using two variables but boy they're damn well sandiwched together"]}, {"cell_type": "markdown", "metadata": {"_uuid": "2177f73ef64e3f6c3a402724e285f0b77c432d53", "_cell_guid": "6738891b-cc60-432a-9be4-032377441c4c", "collapsed": true}, "source": ["### Comparing binary variables with the continuous features (within the same groupings) and target"]}, {"cell_type": "markdown", "metadata": {"_uuid": "aa3059fef5a89e516ed76d3295f6b13e7c99b6e9", "_cell_guid": "7dcdae11-373a-4690-9a62-6d4e1edbe29c"}, "source": ["Checking which columns are binary..."]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "e21075e566ed6a66f9b275e8b40aa759d1740d5f", "_cell_guid": "a645fc11-e1f0-46a2-abb0-c79b292c44cb", "collapsed": true}, "outputs": [], "source": ["[s for s in train.columns if \"_bin\" in s]"]}, {"cell_type": "markdown", "metadata": {"_uuid": "0c862b04253a712bea91fb245c1e404d6b247661", "_cell_guid": "ef6faacf-b9e9-40b7-9d4b-4b20ec9f4814"}, "source": ["Converting the columns to the right data type"]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "f389221f59fda8b455ebdfe28b4497b950691f23", "_cell_guid": "9a0f85ab-6dc6-47e1-b430-ada95c4e31f7", "collapsed": true}, "outputs": [], "source": ["for column in [col for col in train.columns if \"bin\" in col]:\n", "    train[column] = train[column].astype(bool)\n", "    test[column] = test[column].astype(bool)\n", "\n", "train['target'] = train['target'].astype(bool)"]}, {"cell_type": "markdown", "metadata": {"_uuid": "d01a6265ec0a914055398e6d5e7e18fac51e62ea", "_cell_guid": "25115c37-6f53-441d-9cbc-b7a7468a9d25"}, "source": ["Pearson's R correlation only works when both variables are continuous. Hence,\n", "\n", "1. ~~For comparing the binary variables to binary variables, we will calculate the phi coefficient.~~ \n", " I am using row-wise comparison since since phi coefficient calculation is returning false values due to a problem for which I have posted a question <a href= https://www.kaggle.com/questions-and-answers/41464> here </a>\n", "2. For comparing the binary to continuous variables, we will calculate the point bi-serial.\n", "\n", "__Starting with the prefix \"ind\"__"]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "31abb6d33e3221a0fd0332e8adefc0e0c0795136", "_cell_guid": "700d00ea-b73f-4f27-befb-c9fc1a29f1da", "collapsed": true}, "outputs": [], "source": ["#defining a correlation dataframe maker\n", "\n", "def correl_df_maker(filtered_cols, train, round_to=2):\n", "\n", "    coeff_df = pd.DataFrame(columns=filtered_cols,index=filtered_cols)\n", "    for idx,col in combinations_with_replacement(filtered_cols,2):\n", "\n", "        if train[idx].dtype == bool and train[col].dtype == bool:\n", "            coeff_df.loc[idx,col] = np.round_(np.sum(train[idx]==train[col])/train.shape[0],round_to)\n", "            coeff_df.loc[col,idx] = coeff_df.loc[idx,col]\n", "        elif train[idx].dtype == bool:\n", "            coeff_df.loc[idx,col] = np.round_(pointbiserialr(train[idx].values, train[col].values)[0],round_to)\n", "            coeff_df.loc[col,idx] = coeff_df.loc[idx,col]\n", "        elif train[col].dtype == bool:\n", "            coeff_df.loc[idx,col] = np.round_(pointbiserialr(train[col].values, train[idx].values)[0],round_to)\n", "            coeff_df.loc[col,idx] = coeff_df.loc[idx,col]\n", "        else:\n", "            coeff_df.loc[idx,col] = np.round_(pearsonr(train[idx].values, train[col].values)[0],round_to)\n", "            coeff_df.loc[col,idx] = coeff_df.loc[idx,col]\n", "            \n", "    return coeff_df.astype(float)"]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "d6840b75048b9c37a780c680c1a542025074b495", "_cell_guid": "78bebdf9-8930-4f18-a8bf-b46b22788535", "collapsed": true}, "outputs": [], "source": ["prefix='ind'\n", "\n", "# first comparing all binary variables with one another\n", "filtered_cols = [col for col in grouped_cols[prefix] if ('cat' not in col)] + ['target']\n", "coeff_ind_df = correl_df_maker(filtered_cols, train)\n", "coeff_ind_df"]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "b797ddf7517aecb5446b4628df942ef29a26feea", "scrolled": false, "_cell_guid": "c0fa8cf4-f787-4897-8130-a60d44590e32", "collapsed": true}, "outputs": [], "source": ["draw_heatmap(filtered_cols, coeff_ind_df, fmt='.2f', calc_corr=False)"]}, {"cell_type": "markdown", "metadata": {"_uuid": "2b36ba25d59e63d954e7b94607a1031e70ef866e", "_cell_guid": "20cc2239-058a-41b6-8a53-049f39a5072d"}, "source": ["Next we will do the same with the only other prefix with binary variables: __\"calc\"__"]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "eabb2c77baff8b811e6c02f2f24d9c6dd7cbb8b4", "scrolled": false, "_cell_guid": "b3f32981-d446-4317-be7c-6e19717a5957", "collapsed": true}, "outputs": [], "source": ["prefix='calc'\n", "\n", "# first comparing all binary variables with one another\n", "filtered_cols = [col for col in grouped_cols[prefix] if ('cat' not in col)] + ['target']\n", "correl_car_df = correl_df_maker(filtered_cols,train)\n", "correl_car_df"]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "921f21c9adf362e8a8d5182366e16d59c3e84548", "_cell_guid": "44c0029f-0392-4a41-95e8-68c35dad980c", "collapsed": true}, "outputs": [], "source": ["draw_heatmap(filtered_cols, correl_car_df, fmt='.2f', calc_corr=False)"]}, {"cell_type": "markdown", "metadata": {"_uuid": "859a4872d1686da72f66d0d0c597ed93e9be8d15", "_cell_guid": "0682b829-c1fe-4cc3-a904-501a44fbe25e"}, "source": ["What's next?\n", "\n", "So I have the correlations between binary variables and also between continuous and binary variables. The question is... what now. It seems that there are a bunch of variables that give a seemingly high correlation with target but that's because of the class imbalance in the output. One could always say False and still have more than 96 percent match. Among these, I need to see which one give the highest f1_score and keep that and remove the rest perhaps?\n", "\n", "Furthermore, I tried training the RandomForestClassifier (RFC) and it gives less than ideal results. Categorical Variables are basically binary variables also after OHE (One-Hot Encoding) so I could do that and check the correlation again perhaps or run RFC/Catboost etc. on it to see again how well I do.\n", "\n", "A further option would be to standardize the fields using StandardScaler and even eliminate or shorten the long tails of continuous variables (should they be normally distributed) before doing that.\n", "\n", "Final options, run LDA, PCA, try different methods, try stacking with different methods, imbalance learn and finally the two beasts feature engineering and deep learning (with Keras preferably but TF too).\n", "\n", "To-Do:\n", "\n", "1. Get rid of highly correlated features\n", "2. Do OHE for the categorical features\n", "3. Try Random Forest\n", "4. Check the distribution of ordinal and continuous variables\n", "5. If they have long tails, bring them to the center by taking the log\n", "6. Standardize them using StandardScaler\n", "7. Run the LDA and PCA on the model and draw the necessary conclusions\n", "9. Try Linear Model (perhaps a regularized version like ElasticNet), try the Support Vector Machines, try k-Nearest Neighbors.\n", "10. See if a simple ensemble for these performs better\n", "11. Try dealing with the imbalance of classes somehow (SMOTE etc.) and retry the models. Check if the outputs improved\n", "12. Try the models and ensemble again\n", "13. Try some of the feature reduction techniques to see if the result improves\n", "14. Try to make sense of the features even though they are anonymized\n", "15. Try a deep learning model with Keras\n", "15. Try adverserial validation"]}, {"cell_type": "markdown", "metadata": {"_uuid": "3c2e569488dbee76b6fc7b5133e7f8a76745549f", "_cell_guid": "7b902ec4-b5cd-460a-a120-49c41e010793"}, "source": ["### Remove highly correlated features"]}, {"cell_type": "markdown", "metadata": {"_uuid": "824418573e4c534415cdef3eac9d997c6a8fb03e", "_cell_guid": "e0a3fe15-ec0e-47f3-8a57-03e6eb30f4d8", "collapsed": true}, "source": ["So let's begin. In the next step, I will get rid of strongly inter-correlated features:"]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "8b1c87a2855d58b68350cd57ec51543e111b3086", "_cell_guid": "09318cbb-6e48-4047-b918-1a3d60c37244", "collapsed": true}, "outputs": [], "source": ["for column in ['ind_10_bin', 'ind_11_bin', 'ind_12_bin', 'ind_13_bin']:\n", "\n", "    print(\"*\"*15,column,\"*\"*15)\n", "    print(classification_report(train.target.values, train[column].values))"]}, {"cell_type": "markdown", "metadata": {"_uuid": "67e6f9055c72474833cff19993f0e5a82ca7134e", "_cell_guid": "9764fb4b-b9c5-4d56-96f9-d7299230cbd5"}, "source": ["So I will go with ind_12_bin and get rid of others since it has a beter better recall and f1-score than others."]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "d2b6576b0b6ef0d52ff0dc49e804d4db9d0aa5ec", "_cell_guid": "4a9ce501-15d7-4eea-8689-d12daa51a7c7", "collapsed": true}, "outputs": [], "source": ["train.drop(['ind_10_bin', 'ind_11_bin', 'ind_13_bin'], axis=1, inplace=True)\n", "test.drop(['ind_10_bin', 'ind_11_bin', 'ind_13_bin'], axis=1, inplace=True)"]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "866a49ac970f6c6e0ba9da9db2481c3cf69fb196", "_cell_guid": "de09d827-b49e-4fde-ab31-6a642272e75a", "collapsed": true}, "outputs": [], "source": ["for column in ['calc_18_bin', 'calc_20_bin']:\n", "\n", "    print(\"*\"*15,column,\"*\"*15)\n", "    print(classification_report(train.target.astype(int).values, train[column].astype(int).values))"]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "828f41413f9acb4233adca0b61ad3ddab19a47b8", "_cell_guid": "fe715799-c8a2-4679-a08f-4092de97d4f0", "collapsed": true}, "outputs": [], "source": ["train.drop(['calc_18_bin'], axis=1, inplace=True)  # since its recall of 1s is better even though it has an overall worse f1-score\n", "test.drop(['calc_18_bin'], axis=1, inplace=True)"]}, {"cell_type": "markdown", "metadata": {"_uuid": "e67326568db7bdda22d393abc98dae746112424d", "_cell_guid": "9818e0cd-4102-4be8-8978-d1399d46087a"}, "source": ["<p><font color=\"green\"> Before I close this conversation, I would like to reflect on what Pearson's R really means. I mean if a value is anything below 1.0, where do stop dropping the features if they are highly correlated. How can \"high\" be mathematically defined? I thought about it and came to the conclusion that beyond the extreme values of 0 and 1, what I am missing is a picture that usually one associates to a concept. Yes I know the higher the number the \"more obvious\" the correlation.</font></p>\n", "\n", "<p><font color=\"green\">So what does it say for example a value between two pairs of features one with a correlation of 0.6 vs. 0.7? Should one be dropped or neither? So far I am going with the understanding that unless they are almost completely correlated and any lack of perfect correlation is due to noise that is unavoidable in real world, there are real influences that can sort of lead to divergence and that divergence may capture information.</font></p>\n", "\n", "<p><font color=\"green\">For example, as a rule you always reach work in 15 minute after you leave your house door. So the leaving time and arriving time have a perfect correlation but the traffic introduces variance i.e. the noise.</font></p>\n", "\n", "<p><font color=\"green\">On  the other hand, if you go drop your kid to school twice per week, which takes you 10 more minutes, the divergence is not noise and will bring the correlation down. Such a model could only be modeled if the data was chronological in which the data points would show a pattern else a second feature \"go to school\" would be needed to model it using a linear relationship (arrival time = 15x1 + 10x2)</font></p>\n", "\n", "_<p><font color=\"green\">So the question is always, to what extent is the noise bringing the correlation down and to what extent is it the work of other factors that need to be paid attention to.</font></p>_\n", "\n", "\n", "<p> Get rid of highly correlated features &#10004;</p>"]}, {"cell_type": "markdown", "metadata": {"_uuid": "1ba130e72cfff442c45d2b16d87d74bdb4d9c3e0", "_cell_guid": "e25474c6-c78e-48bb-966c-82e6e0d47389"}, "source": ["### One-Hot Encoding"]}, {"cell_type": "markdown", "metadata": {"_uuid": "d34d67b79469f1698a36beacf500bf038bd1aaa4", "_cell_guid": "f280aac7-dfef-4460-8ae3-16970314056b"}, "source": ["This is the standard process of converting categories (numerical or strings) into binary format."]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "7630ffdfc01137c4913198af2c21b538a0b03e7b", "_cell_guid": "7efe78df-fd07-49ac-8e8b-65b2992b6bab", "collapsed": true}, "outputs": [], "source": ["for col in [col for col in train.columns if \"cat\" in col]:\n", "    print(col, end=\"|\")  \n", "    df = pd.get_dummies(train[col],prefix=col).astype(bool)\n", "    train.drop([col],axis=1,inplace=True)  # dropping the original columns\n", "    train = pd.concat([train, df], axis=1)\n", "    \n", "    df = pd.get_dummies(test[col],prefix=col).astype(bool)\n", "    test.drop([col],axis=1,inplace=True)  # dropping the original columns\n", "    test = pd.concat([test, df], axis=1)"]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "d7e5afbbd6fe944382fef66d4252cd12059fc5bd", "scrolled": false, "_cell_guid": "a1f9df12-6db7-4e12-bebc-6434a88412b0", "collapsed": true}, "outputs": [], "source": ["train.shape, test.shape  # train has target so all is well!"]}, {"cell_type": "markdown", "metadata": {"_uuid": "5af3accf45f517df9636595b1863ae7bb0c20bbe", "_cell_guid": "fe675320-0e6d-49b0-a888-f173da0448ea", "collapsed": true}, "source": ["<p> Do OHE for the categorical features &#10004;</p>"]}, {"cell_type": "markdown", "metadata": {"_uuid": "410e0ed13ec373faa259b5e970b26d3fc075085a", "_cell_guid": "9892fc23-ef46-491a-9705-568e66d24328"}, "source": ["### Random Forest Classification"]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "7a7adbab9dce2eeb4b94921fb5ae79e9d0333c12", "_cell_guid": "568c4060-ff29-44d4-bcc7-4af56e61d45c", "collapsed": true}, "outputs": [], "source": ["X_train, X_test, y_train, y_test = train_test_split(train.iloc[:,2:].values, train.iloc[:,1].values, random_state=42)"]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "b69ebc9350afcea9dae6176b48eb2c1ad61926ad", "_cell_guid": "5e2ab169-05db-42d1-80e7-7f3221e2b08d", "collapsed": true}, "outputs": [], "source": ["rf_clf = RandomForestClassifier(n_estimators=100, min_samples_leaf=50, class_weight=\"balanced\", random_state=42)"]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "76afbc768689a465db125e3ea40192bd759edb10", "_cell_guid": "13cafdb5-e795-49d0-8d93-316935bfca7a", "collapsed": true}, "outputs": [], "source": ["rf_clf.fit(X_train, y_train)"]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "b9040c7106417d92d11899cff0ab2dd54774c3fb", "_cell_guid": "9001a653-b7d0-4cbb-ad61-7bb2cc03d427", "collapsed": true}, "outputs": [], "source": ["y_train_pred = rf_clf.predict(X_train)\n", "confusion_matrix(y_train,y_train_pred)"]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "8f2025483bcce3858b521b5604462e3377fa1a5d", "_cell_guid": "cfc41110-802c-42cd-8daf-dcc903508a75", "collapsed": true}, "outputs": [], "source": ["y_test_pred = rf_clf.predict(X_test)\n", "confusion_matrix(y_test, y_test_pred)"]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "94a9b42554c0ef3716e657bd4ca254d2ab29ac01", "_cell_guid": "17c6780d-8864-47e4-ae99-1062b8fe3799"}, "outputs": [], "source": ["dict(zip(train.columns[2:], np.round(rf_clf.feature_importances_*100,3)))"]}, {"cell_type": "markdown", "metadata": {"_uuid": "7ba1b50758ae8d9bcb31589bec6ded9f6efeed80", "_cell_guid": "a33b7e80-12f7-4d39-9057-a5cdf37395ed"}, "source": ["<p> Try Random Forest  &#10004;</p>"]}, {"cell_type": "markdown", "metadata": {"_uuid": "0b30c3280e85b79b212d0082b03b5eb609c55380", "_cell_guid": "2a75e53d-3300-4e35-85bf-ab49ed6e77cf"}, "source": ["### Violin plots for distribution comparison"]}, {"cell_type": "markdown", "metadata": {"_uuid": "2d1cf3929218564f8bff345d3535046e19501ed5", "_cell_guid": "9819bf26-fee3-475a-9235-2f15d72cf663"}, "source": ["First we will check how many variables do we have ordinal and continuous variables"]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "06b5a3afa68e176a8e62e6c507d3ab1f2c6d83c6", "_cell_guid": "f7bc525d-4607-4dd7-b8a5-b6c7172162ff", "collapsed": true}, "outputs": [], "source": ["train.select_dtypes(include=[int,float]).describe()"]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "5f9b68694df8972b71ad88cf23a06fada2f8984d", "scrolled": false, "_cell_guid": "53776a48-9aa9-4286-996d-0f36f9610719", "collapsed": true}, "outputs": [], "source": ["columns_for_violin = list(train.iloc[:,1:].select_dtypes(include=[int,float]).columns) + ['target']  # target for hue\n", "data = train[columns_for_violin]\n", "data = pd.melt(data, id_vars='target', var_name='feature', value_name=\"value\")\n", "plt.figure(figsize=(len(columns_for_violin), len(columns_for_violin)))\n", "sns.violinplot(x=\"feature\", y=\"value\", hue=\"target\", data=data,split=True,inner=\"quart\")\n", "plt.xticks(rotation=90);"]}, {"cell_type": "markdown", "metadata": {"_uuid": "e2e2d935b201d762093b1f14e80dac0cc25b02fc", "_cell_guid": "cf1ea70f-860c-463a-bce5-30f0797c7142"}, "source": ["<font color=\"red\">What may seem like different data distribution in calc_01/02/03 is actually nothing more than class imbalance (i.e. difference in the number of samples of each class) as can be seen by the numbers below.</font>"]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "851c8230248c2823b95a30dfcf284e1c8a344bd1", "scrolled": true, "_cell_guid": "0d425300-acae-4759-9164-30fba8fc0c62", "collapsed": true}, "outputs": [], "source": ["calc01_dist_df = pd.concat([train[train.target==True].calc_01.value_counts(),train[train.target==False].calc_01.value_counts()], axis=1)\n", "calc01_dist_df.columns = ['True','False']\n", "calc01_dist_df"]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "902aefe7e4d8f7ff44d4c6b02866891db22dd0ad", "_cell_guid": "840c5689-fdae-445e-b3f5-1afb85573e5f", "collapsed": true}, "outputs": [], "source": ["f, axarr = plt.subplots(3,2, figsize=(16,5))\n", "sns.distplot(train.car_13,ax=axarr[0,1])\n", "sns.distplot(train.car_11,ax=axarr[1,1])\n", "sns.distplot(train.car_15,ax=axarr[2,1])\n", "sns.distplot(train.reg_02,ax=axarr[0,0])\n", "sns.distplot(train.calc_01,ax=axarr[1,0])\n", "sns.distplot(train.car_12,ax=axarr[2,0]);"]}, {"cell_type": "markdown", "metadata": {"_uuid": "887c9279382063ace9defdc1fb53371b5182fa85", "_cell_guid": "a036ff7f-48ba-41ec-9ee2-a695fec85f33"}, "source": ["So much of the data, in my opinion based on the graphs above,  is ordinal. This can be seen given the frequency peaks at certain points that are otherwise zero."]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "d0d5944878db69313a4d65d5caf5b7bfbce629be", "_cell_guid": "4c54040e-d2d9-484e-9a1f-5ffc7c9c0945", "collapsed": true}, "outputs": [], "source": ["sns.distplot(train.car_13);  # a long tail"]}, {"cell_type": "markdown", "metadata": {"_uuid": "5ddde3f296a0a84ca37c230b4120e54211640e3d", "_cell_guid": "a34d3f24-adde-40ea-ad98-41ad3bb77ca4"}, "source": ["First thoughts: ~~Before applying the log function, I will try standardizing it and see if that helps.~~\n", "\n", "I can't take the log after standardization because then all values will no longer be positive."]}, {"cell_type": "markdown", "metadata": {"_uuid": "8c08a0755a915b569ae61381c4b7c9e6ae30f237", "_cell_guid": "1fbf2944-87f1-4c69-9780-0223efdeb36a"}, "source": ["### Standardization"]}, {"cell_type": "markdown", "metadata": {"_uuid": "7299a47c8d4182a728c4f6070d82a8d9bebf6af6", "_cell_guid": "3ff9b7da-ff59-4b3c-83da-65f9c9bd4d13"}, "source": ["<p><font color=\"green\">An important point regarding standardization is that the data needs to be split beforehand. This is because, whatever values the model learns during the fit() call, need to be extracted/calculated from the training set __only__. Ofcourse we can't know the data we are going to test our model on and calling fit over the entire data set will introduce a form of leakage.</font><p>"]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "fcb1d21bd74a3b106a74b44beff909a72788a13a", "_cell_guid": "7ae68691-de65-40cd-a735-7be43be7138d", "collapsed": true}, "outputs": [], "source": ["train_df, val_df = train_test_split(train, test_size=0.2, random_state=42)"]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "af7300935fb97bfc2b2330bbba6d0eaba4509d12", "_cell_guid": "a6277077-2c21-4979-9ef9-f9f519b233f3", "collapsed": true}, "outputs": [], "source": ["# To preserve the original train DataFrame, I will apply log on the split ones\n", "\n", "for df in [train_df, val_df]:\n", "    \n", "    df.car_13 = df.car_13.apply(np.log2)"]}, {"cell_type": "markdown", "metadata": {"_uuid": "799458f6dc0b53c2e26292a28a34145738bb53cb", "_cell_guid": "517eb0b2-3b21-474d-ba96-c75ebb62aca3"}, "source": ["Only the continous and ordinal variables ought to be normalized. The code below is ugly but due to NaNs creeping up for me for an unknown number of reasons, I need to write tests to narrow down when exactly that happens when if I am unsure why exactly that happens."]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "04c0dd0ac117b1a7ebbfeb4e52d72eea5c669dcd", "scrolled": false, "_cell_guid": "861180cd-6877-43d5-85d0-684e12507275", "collapsed": true}, "outputs": [], "source": ["std_scaler = StandardScaler()\n", "\n", "for col in train.select_dtypes(include=[\"int64\",\"float64\"]).columns:\n", "    \n", "    clone_scalr = clone(std_scaler)\n", "    print(col, end=' | ')\n", "    \n", "    np_data_train = train_df[col].astype(np.float32).values.reshape(-1, 1)\n", "    assert np.sum(np.isnan(np_data_train)) == 0, 'NaNs exist in Series converted to ndarray for train'\n", "    np_data_val = val_df[col].astype(np.float32).values.reshape(-1, 1)\n", "    assert np.sum(np.isnan(np_data_val)) == 0, 'NaNs exist in Series converted to ndarray for validation'\n", "\n", "    np_data_train_t = np.round(clone_scalr.fit_transform(np_data_train).ravel(),4)\n", "    assert np.sum(np.isnan(np_data_train_t)) == 0, 'NaNs exist in transformed ndarray for train'\n", "    np_data_val_t = np.round(clone_scalr.transform(np_data_val).ravel(),4)\n", "    assert np.sum(np.isnan(np_data_val_t)) == 0, 'NaNs exist in transformed ndarray for validation'\n", "    \n", "    train_df[col] = pd.Series(np_data_train_t, name=col, index=train_df.index)\n", "    assert not train_df[col].isnull().any(), \"NaNs exist in conversion of transformed ndarray to Series for train\"\n", "    val_df[col] = pd.Series(np_data_val_t, name=col, index=val_df.index)\n", "    assert not val_df[col].isnull().any(), \"NaNs exist in conversion of transformed ndarray to Series for validation\""]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "d2390290e22221b0fc16494d01ca418b877142e6", "_cell_guid": "5eafec36-9054-4242-96a5-ad013faf80d4", "collapsed": true}, "outputs": [], "source": ["sns.distplot(train_df.car_13);  # looks good!"]}, {"execution_count": null, "cell_type": "code", "metadata": {"_uuid": "914abd1ebc89119f665f15f025df9bc2de4463c9", "scrolled": false, "_cell_guid": "21877737-99eb-45c8-84a0-7cd066a9fdaa", "collapsed": true}, "outputs": [], "source": ["columns_for_violin = list(train_df.select_dtypes(include=[\"float32\"]).columns) +['target'] # target for hue\n", "data = train_df[columns_for_violin]\n", "data = pd.melt(data, id_vars='target', var_name='feature', value_name=\"value\")\n", "plt.figure(figsize=(len(columns_for_violin), len(columns_for_violin)))\n", "sns.violinplot(x=\"feature\", y=\"value\", hue=\"target\", data=data,split=True,inner=\"quart\")\n", "plt.xticks(rotation=90);"]}], "nbformat": 4, "metadata": {"language_info": {"nbconvert_exporter": "python", "version": "3.6.3", "name": "python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "file_extension": ".py", "mimetype": "text/x-python"}, "kernelspec": {"language": "python", "display_name": "Python 3", "name": "python3"}}}