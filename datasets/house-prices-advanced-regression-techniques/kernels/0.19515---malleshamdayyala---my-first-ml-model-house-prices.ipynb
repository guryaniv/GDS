{"cells":[{"metadata":{"_uuid":"b6269c0e8f417f82daf093dda8fa0da6d2c57d86","_cell_guid":"e81ee64d-e474-4662-9036-ce23df615199"},"cell_type":"markdown","source":"# Introduction\n**This will be your workspace for Kaggle's Machine Learning education track.**\n\nYou will build and continually improve a model to predict housing prices as you work through each tutorial.  Fork this notebook and write your code in it.\n\nThe data from the tutorial, the Melbourne data, is not available in this workspace.  You will need to translate the concepts to work with the data in this notebook, the Iowa data.\n\nCome to the [Learn Discussion](https://www.kaggle.com/learn-forum) forum for any questions or comments. \n\n# Write Your Code Below\n\n"},{"metadata":{"_uuid":"1c728098629e1301643443b1341556a15c089b2b","_cell_guid":"86b26423-563a-4fa1-a595-89e25ff93089","trusted":true,"collapsed":true},"cell_type":"code","source":"import pandas as pd\n\nmain_file_path = '../input/train.csv'\ndata = pd.read_csv(main_file_path)\nprint('hello world')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c49f596876290a8ca32943711703abdf9dbf15a8","collapsed":true},"cell_type":"code","source":"print(data.describe())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8ca954f027d079c44430bc1dddc138b9cb3adcb6","collapsed":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5839e6f91ceca6ba00476256eb2a2ffccd13ff1f","collapsed":true},"cell_type":"code","source":"data.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d8912626552f35e67c63fe4265ddbc08dd46ced6","collapsed":true},"cell_type":"code","source":"print(data.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"453e2ef564f97ffb881d25f60757b67fa882d330","collapsed":true},"cell_type":"code","source":"data.PoolArea.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9c0bca016b5820304a014f58998afe60586d3846","collapsed":true},"cell_type":"code","source":"mycolumns = ['Id','SalePrice']\ndata[mycolumns].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2ac718f2cdf9baf1896c288df62df22d5055e146","collapsed":true},"cell_type":"code","source":"data[mycolumns].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e633e0e97e2e2901c5099035d700a90acb76f4c5","collapsed":true},"cell_type":"code","source":"y = data['SalePrice']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"536429dae44ac154ad67be24265f947f5e05ab70"},"cell_type":"code","source":"pred = ['LotArea','YearBuilt','YearBuilt','1stFlrSF', '2ndFlrSF', 'FullBath', 'BedroomAbvGr', 'TotRmsAbvGrd']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"9afa864757db0e00d83f6a795d2bf15f483d8511"},"cell_type":"code","source":"X = data[pred]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"044da0c5360ebc9aab9870fbd682c4444a4031a8","collapsed":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor\n# Define model\nmy_model = DecisionTreeRegressor()\n\n# Fit model\nmy_model.fit(X,y)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1c7b224ec61340826be0686c68a978725ce94626","collapsed":true},"cell_type":"code","source":"print(\"Making predictions for the following 5 houses:\")\nprint(X.head())\nprint(\"The predictions are\")\nprint(my_model.predict(X.head()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f40041efa3ceed9bc0d8c95d541b34ae5accc00d","collapsed":true},"cell_type":"code","source":"from sklearn.metrics import mean_absolute_error\nmy_pred = my_model.predict(X)\n#print(my_pred)\nmean_absolute_error(y,my_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"de499aaf2a0ce5bd028f3aa12b94ffb3047680af","collapsed":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ntrain_X, val_X, train_y, val_y = train_test_split(X,y,random_state = 0)\n# Define model\nmy_model = DecisionTreeRegressor()\n# Fit model\nmy_model.fit(train_X, train_y)\n# get predicted prices on validation data\nval_predictions = my_model.predict(val_X)\nprint(mean_absolute_error(val_y, val_predictions))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"08d07d57568968467712ac3c0bd40d11ed94fd4f","collapsed":true},"cell_type":"code","source":"from sklearn.metrics import mean_absolute_error\nfrom sklearn.tree import DecisionTreeRegressor\n\ndef get_mea(max_leaf_nodes, predictors_train,predictors_val,targ_train, targ_val):\n    model = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes,random_state=0)\n    model.fit(predictors_train,targ_train)\n    pred_val = model.predict(predictors_val)\n    mea = mean_absolute_error(targ_val,pred_val)\n    return(mea)\n    \n    \n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3a864c5bba4b8645af2a85c164e9cbac17a92a45","collapsed":true},"cell_type":"code","source":"# compare MAE with differing values of max_leaf_nodes\nfor max_leaf_nodes in [10,100,1000,5000]:\n    my_mea = get_mea(max_leaf_nodes,train_X,val_X,train_y,val_y)\n    print(\"Max Leaf Nodes: %d \\t\\t  Mean Absolute Error: %d \" %(max_leaf_nodes,my_mea))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a71d9c01b793e505b3ad985e0404c6a1814a5055","collapsed":true},"cell_type":"code","source":"##Random Forests\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\n\nforest_model = RandomForestRegressor()\nforest_model.fit(train_X,train_y)\niowa_pred = forest_model.predict(val_X)\nprint(mean_absolute_error(val_y, iowa_pred))\n\n# improvement found in 'Random Forest' over 'Decision Tree' ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"05cdea0e37d69ae6cc0d7f9d4f1a4a81b3212887","collapsed":true},"cell_type":"code","source":"##Submitting From A Kernel\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Read the Data\ntrain = pd.read_csv('../input/train.csv')\n\n# pull data into target (y) and predictors (X)\ntrain_y = train.SalePrice\npredictor_columns = ['LotArea', 'OverallQual', 'YearBuilt', 'TotRmsAbvGrd']\n\n# Create training predictors data\ntrain_X = train[predictor_columns]\n\nmy_model = RandomForestRegressor()\nmy_model.fit(train_X,train_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d28402db59d068373e34b9fd3b2b35b10b2a0241","collapsed":true},"cell_type":"code","source":"# Read the test data\ntest = pd.read_csv('../input/test.csv')\n# Treat the test data in the same way as training data. In this case, pull same columns.\ntest_X = test[predictor_columns]\n# Use the model to make predictions\npredicted_price = my_model.predict(test_X)\n# We will look at the predicted prices to ensure we have something sensible.\nprint(predicted_price)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6a9310536709ba07eae6ee4b1a3739d1c2356de9","collapsed":true},"cell_type":"code","source":"#Prepare Submission File\nmy_submission = pd.DataFrame({'Id': test.Id, 'SalePrice': predicted_price})\n# File Name: MD1stSubmission.csv\nmy_submission.to_csv('MD1stSubmission.csv', index=False)\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"55fc91c61c1618e45b633120dd45509a7c86de57","collapsed":true},"cell_type":"code","source":"##Handling Missing Values\nprint(data.isnull().sum())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"44e18ec340c69d520dc8866e5bbfc20eb9ff4b68","collapsed":true},"cell_type":"code","source":"#Handling Missing Values\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"4a96c31aefadd0a57864ece5449e326e6b444e53"},"cell_type":"code","source":"main_file_path = '../input/train.csv'\niowa_data = pd.read_csv(main_file_path)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"92730525c6573a34f953a5ca808883f4dae771ad","collapsed":true},"cell_type":"code","source":"iowa_data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"418984d15fe45d331f37cad124124dbe5d62c1cc","collapsed":true},"cell_type":"code","source":"iowa_data.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"8fcadb4e5163c832f945993f5696d11d6bebc617","collapsed":true},"cell_type":"code","source":"iowa_data.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"e1d58c60b8ad6183e2f608f67d0218fa3558407a","collapsed":true},"cell_type":"code","source":"print(iowa_data.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f3ad57569152fe163fdde76d6c9a8d1e1f4cad78","collapsed":true},"cell_type":"code","source":"iowa_target = iowa_data.SalePrice\niowa_predictors = iowa_data.drop(['SalePrice'],axis=1)\n# For the sake of keeping the example simple, we'll use only numeric predictors.\niowa_numeric_predictors = iowa_predictors.select_dtypes(exclude=['object'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"17f4f7d4566e637c154d34c279faf0be4c1b726c","collapsed":true},"cell_type":"code","source":"#from sklearn.preprocessing import Imputer\nfrom sklearn.impute import SimpleImputer \nmy_imputer = SimpleImputer()\ndata_with_imputed_values  = my_imputer.fit_transform(iowa_numeric_predictors)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"769334eaa5d50070a22280a4e87c7c2da9cf06d9","collapsed":true},"cell_type":"code","source":"#data_with_imputed_values is became array , so changing to DataFrame and adding columns back as original DF\ndata_with_imputed_values = pd.DataFrame(my_imputer.fit_transform(iowa_numeric_predictors))\ndata_with_imputed_values.columns = iowa_numeric_predictors.columns\ndata_with_imputed_values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"97f3389ef6dbf9169ff6247b11acf1c846771a3a","collapsed":true},"cell_type":"code","source":"# pull data into target (y) and predictors (X)\ny_n = iowa_target\nnew_predictor_columns = ['LotArea', 'OverallQual', 'YearBuilt', 'TotRmsAbvGrd','LotFrontage']\n\n# Create training predictors data\nX_n = data_with_imputed_values[new_predictor_columns]\n\ntrain_X_n, test_X_n, train_y_n, test_y_n = train_test_split(X_n,y_n,train_size=0.7,test_size=0.3,random_state = 0)\n\n# fit, predict, mea in same def\ndef score_dataset(train_X_n, test_X_n, train_y_n, test_y_n):\n    my_iowa_model = RandomForestRegressor()\n    my_iowa_model.fit(train_X_n, train_y_n)\n    preds_n = my_iowa_model.predict(test_X_n)\n    return mean_absolute_error(test_y_n, preds_n)\n\n#my_iowa_model = RandomForestRegressor()\n#my_iowa_model.fit(train_X_n,train_y_n)\n# Use the model to make predictions\n#predicted_price_n = my_iowa_model.predict(test_X_n)\n# We will look at the predicted prices to ensure we have something sensible.\n#print(predicted_price_n)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"191f52510d4c52daff010a82a57726ff2ac4774c","collapsed":true},"cell_type":"code","source":"print(\"Mean Absolute Error from Imputation while Track What Was Imputed:\")\nprint(score_dataset(train_X_n, test_X_n, train_y_n, test_y_n))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"e870619c4707d7adf898f5a54646eac6d5b52655","collapsed":true},"cell_type":"code","source":"#Get Model Score from Dropping Columns with Missing Values\n\ncols_with_missing = [col for col in train_X_n.columns \n                                 if train_X_n[col].isnull().any()]\nreduced_train_X_n = train_X_n.drop(cols_with_missing,axis=1) \nreduced_test_X_n = test_X_n.drop(cols_with_missing,axis=1) \nprint(\"Mean Absolute Error from dropping columns with Missing Values:\")\nscore_dataset(reduced_train_X_n,reduced_test_X_n,train_y_n,test_y_n)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0f4b309204237bbfd56e6c74053acc19d4e7944f","collapsed":true},"cell_type":"code","source":"#Get Score from Imputation with Extra Columns Showing What Was Imputed\n\nimputed_train_X_plus = train_X_n.copy()\nimputed_test_X_plus =  test_X_n.copy()\n\ncols_with_missing = (col for col in train_X_n.columns \n                                 if train_X_n[col].isnull().any())\nfor col in cols_with_missing:\n        imputed_X_train_plus[col + '_was_missing'] = imputed_X_train_plus[col].isnull()\n        imputed_X_test_plus[col + '_was_missing'] = imputed_X_test_plus[col].isnull()\n\n# Imputation\nmy_imputer = SimpleImputer()\nimputed_X_train_plus = my_imputer.fit_transform(imputed_train_X_plus)\nimputed_X_test_plus = my_imputer.transform(imputed_test_X_plus)\n\nprint(\"Mean Absolute Error from Imputation while Track What Was Imputed:\")\nprint(score_dataset(imputed_X_train_plus,imputed_X_test_plus,train_y_n,test_y_n ))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d9c4c3faae7bd76f356cf95f3b537fbbf26c3114","collapsed":true},"cell_type":"code","source":"##Using Categorical Data with One Hot Encoding\nimport pandas as pd\ntrain_data = pd.read_csv('../input/train.csv')\ntest_data = pd.read_csv('../input/test.csv')\n\n# Drop houses where the target is missing\ntrain_data.dropna(subset=['SalePrice'],axis=0,inplace= True)\n\ntarget = train_data.SalePrice","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e3a0d853ef3d8cc6697f47ab2348685b0a165d6d","collapsed":true},"cell_type":"code","source":"train_data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a5d7f211e2990c607e452410312b5b2ca1ef8899","collapsed":true},"cell_type":"code","source":"test_data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"0b7dfab0eeb294c27e52e9929ab7c2450dfe5bab"},"cell_type":"code","source":"# Since missing values isn't the focus of this tutorial, we use the simplest\n# possible approach, which drops these columns. \n# For more detail (and a better approach) to missing values, see\n# https://www.kaggle.com/dansbecker/handling-missing-values\ncols_with_missing = [col for col in train_data.columns \n                                 if train_data[col].isnull().any()] \ncandidate_train_predictors = train_data.drop(['Id', 'SalePrice'] + cols_with_missing, axis=1)\ncandidate_test_predictors = test_data.drop(['Id'] + cols_with_missing, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"dddc0d99f76565bdee72feac861614564241d522"},"cell_type":"code","source":"# \"cardinality\" means the number of unique values in a column.\n# We use it as our only way to select categorical columns here. This is convenient, though\n# a little arbitrary.\nlow_cardinality_cols = [cname for cname in candidate_train_predictors.columns if \n                                candidate_train_predictors[cname].nunique() < 10 and\n                                candidate_train_predictors[cname].dtype == \"object\"]\n\nnumeric_cols = [cname for cname in candidate_train_predictors.columns if \n                                candidate_train_predictors[cname].dtype in ['int64', 'float64']]\n\nmy_cols = low_cardinality_cols + numeric_cols\n\ntrain_predictors = candidate_train_predictors[my_cols]\ntest_predictors = candidate_test_predictors[my_cols]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"61041f204ba7436ec6cd520f7a509f46fa31cb34","collapsed":true},"cell_type":"code","source":"train_predictors.dtypes.sample(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"aed8f1d9a62caa868c8b78129913590f31399658","collapsed":true},"cell_type":"code","source":"test_predictors.dtypes.sample(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"f924e0d257d682097132b5563930723d6908f08b"},"cell_type":"code","source":"one_hot_encoded_training_predictors = pd.get_dummies(train_predictors)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1cec9b8bb4f1cf7d25a4797cba7360cb690e7b10","collapsed":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestRegressor\n\ndef get_mea(X,y):\n    # multiple by -1 to make positive MAE score instead of neg value returned as sklearn convention\n    return -1 * cross_val_score(RandomForestRegressor(50), X, y, scoring = 'neg_mean_absolute_error').mean()\n\npredictors_without_categoricals = train_predictors.select_dtypes(exclude=['object'])\nmae_without_categoricals = get_mea(predictors_without_categoricals, target)\n\nmae_one_hot_encoded = get_mea(one_hot_encoded_training_predictors, target)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a054d7d53fcd71a015d3225c88c855009059652a","collapsed":true},"cell_type":"code","source":"print('Mean Absolute Error when Dropping Categoricals: ' + str(int(mae_without_categoricals)))\nprint('Mean Abslute Error with One-Hot Encoding: ' + str(int(mae_one_hot_encoded)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9327ee67dc98c850e6055784bb958c63117fe0f8","collapsed":true},"cell_type":"code","source":"#Applying to Multiple Files\n\n#The align command makes sure the columns show up in the same order in both datasets (it uses column names to identify which columns line up in each dataset.) \n#The argument join='left' specifies that we will do the equivalent of SQL's left join. That means, \n#if there are ever columns that show up in one dataset and not the other, we will keep exactly the columns from our training data. \n#The argument join='inner' would do what SQL databases call an inner join, keeping only the columns showing up in both datasets. That's also a sensible choice.\n\none_hot_encoded_training_predictors = pd.get_dummies(train_predictors)\none_hot_encoded_testing_predictors = pd.get_dummies(test_predictors)\n\nfinal_train, final_test = one_hot_encoded_training_predictors.align(one_hot_encoded_testing_predictors, join = 'left', axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"69e412854ae7a286b5d9693cf2fbc8008ca53d58","collapsed":true},"cell_type":"code","source":"#Learning to Use XGBoost\n\nxg_data = pd.read_csv('../input/train.csv')\nxg_data.dropna(subset=['SalePrice'],axis=0,inplace=True)\ny = xg_data.SalePrice\nX = xg_data.drop(['SalePrice'] ,axis=1).select_dtypes(exclude=['object'])\n#train_X, test_X, train_y, test_y = train_test_split(X.as_matrix(), y.as_matrix(), test_size=0.25)\n#NOTE: .as_matrix is removed from future versions\nxg_train_X, xg_test_X, xg_train_y, xg_test_y = train_test_split(X.values, y.values, test_size=0.25)\n\nxg_imputer = SimpleImputer()\nxg_train_X = xg_imputer.fit_transform(train_X)\nxg_test_X = xg_imputer.transform(test_X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4bffa37bd0d8eec5b1129c7b65190b2e03797b94","collapsed":true},"cell_type":"code","source":"#We build and fit a model just as we would in scikit-learn.\n#from xgboost import XGBRegressor\n#xg_model = XGBRegressor()\n# Add silent=True to avoid printing out updates with each cycle\n#xg_model.fit(xg_train_X, xg_train_y)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"88b033373ace674dbea2d2e0508dd2831c92764f","collapsed":true},"cell_type":"code","source":"#We similarly evaluate a model and make predictions as we would do in scikit-learn.\n# make predictions\n\n#xg_predictions = xg_model.predict(xg_test_X)\n#print('Mean Absolute Error :' + str(mean_absolute_error(xg_predictions, xg_test_y)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"87d7aff4fddf77afc7ceab81ffb2838cf98eb437","collapsed":true},"cell_type":"code","source":"#Model Tuning\n#xg_model = XGBRegressor(n_estimators=1000)\n#xg_model.fit(xg_train_X, xg_train_y,early_stopping_rounds = 5, eval_set=[(xg_test_X, xg_test_y)], verbose = False)\n\n#xg_predictions = xg_model.predict(xg_test_X)\n#print('Mean Absolute Error :' + str(mean_absolute_error(xg_predictions, xg_test_y)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"13f44c0ee6309828c6cdcf6003df6a353e6370d2","collapsed":true},"cell_type":"code","source":"#learning_rate\n#xg_model = XGBRegressor(n_estimators=1000,learning_rate=0.05)\n#xg_model.fit(xg_train_X, xg_train_y,early_stopping_rounds = 5, eval_set=[(xg_test_X, xg_test_y)], verbose = False)\n\n#xg_predictions = xg_model.predict(xg_test_X)\n#print('Mean Absolute Error :' + str(mean_absolute_error(xg_predictions, xg_test_y)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eda80a883bb54894fbd9e97b53fe022f6b3f800b","collapsed":true},"cell_type":"code","source":"##Partial Dependence Plots\nimport pandas as pd\nfrom sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier\nfrom sklearn.ensemble.partial_dependence import partial_dependence, plot_partial_dependence\nfrom sklearn.impute import SimpleImputer \n\ncols_to_use = ['LotArea', 'OverallQual', 'YearBuilt']\n\ndef get_some_data():\n    data = pd.read_csv('../input/train.csv')\n    y = data.SalePrice\n    X = data[cols_to_use]\n    my_imputer = SimpleImputer()\n    imputed_X = my_imputer.fit_transform(X)\n    return imputed_X, y\n\nX, y = get_some_data() \nmy_model = GradientBoostingRegressor()\nmy_model.fit(X,y)\n\nmy_plots = plot_partial_dependence(my_model,features=[0,2],X=X,feature_names = cols_to_use, grid_resolution=10)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"fe24c055f5704a6a5ff84ccc1d65c3c634494969"},"cell_type":"code","source":"#Pipelines\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# Read Data\ndata = pd.read_csv('../input/train.csv')\ncols_to_use = ['LotArea', 'OverallQual', 'YearBuilt']\nX = data[cols_to_use]\ny = data.SalePrice\n\ntrain_X, train_y, test_X, train_y = train_test_split(X,y)\n    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ff9b58fc2b750df6ad23ce9ed7766493371cd024","collapsed":true},"cell_type":"code","source":"#You have a modeling process that uses an Imputer to fill in missing values, followed by a RandomForestRegressor to make predictions. \n#These can be bundled together with the make_pipeline function as shown below.\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.impute import SimpleImputer\n\nmy_pipeline = make_pipeline(SimpleImputer(), RandomForestRegressor())\n\n#You can now fit and predict using this pipeline as a fused whole.\n#my_pipeline.fit(train_X, train_y)\n#predictions = my_pipeline.predict(test_X)\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"37b3d178fad597a89e69436418cce483ace3ca14","collapsed":true},"cell_type":"code","source":"#For comparison, here is the code to do the same thing without pipelines\n\nmy_imputer = SimpleImputer()\nmy_model = RandomForestRegressor()\n\n#imputed_train_X = my_imputer.fit_transform(train_X)\n#imputed_test_X = my_imputer.transform(test_X)\n#my_model.fit(imputed_train_X, train_y)\n#predictions = my_model.predict(imputed_test_X)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"a82367719bf097bb21317873fc16fddd7c975d2e"},"cell_type":"code","source":"#Cross-Validation\n\nimport pandas as pd\ndata = pd.read_csv('../input/train.csv')\ncols_to_use = ['LotArea', 'OverallQual', 'YearBuilt']\nX = data[cols_to_use]\ny = data.SalePrice\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"99a4148a6aa1260012008f3971dafe95383d3178","collapsed":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.impute import SimpleImputer\n\nmy_pipeline = make_pipeline(SimpleImputer(),RandomForestRegressor())\n\nfrom sklearn.model_selection import cross_val_score\nscores = cross_val_score(my_pipeline,X,y,scoring='neg_mean_absolute_error')\nprint(scores)\nprint('Mean Absolute Error %2f' %(-1 * scores.mean()))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"2296ff4acd84e2681b51660edb28a8c5399e7de3"},"cell_type":"code","source":"# Data Leakage    DO IT LATER o IOWA DATASET\n#Good Luck for your DS , ML Learning\n ","execution_count":null,"outputs":[]}],"metadata":{"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}