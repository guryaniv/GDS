{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "55c9cb9e-1bd5-3ec8-c2a3-b775f230e803"
      },
      "source": [
        "Just a simple linear regression with scikit.\n",
        "\n",
        "ok, not so simple"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d646cb46-fa5a-858d-fe06-13fa055d847c"
      },
      "outputs": [],
      "source": [
        "import os,sys,time,random,math\n",
        "import tarfile, zipfile\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "from sklearn.cross_validation import train_test_split, StratifiedShuffleSplit\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn import decomposition, datasets, ensemble\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.grid_search import GridSearchCV\n",
        "from sklearn.metrics import make_scorer,precision_score, recall_score, f1_score, average_precision_score, accuracy_score\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.ensemble import ExtraTreesRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display, Image\n",
        "\n",
        "import xgboost as xgb\n",
        "\n",
        "\n",
        "from subprocess import check_output\n",
        "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "9519871b-63c0-8b97-9edb-8ec804e86360"
      },
      "outputs": [],
      "source": [
        "def loadData(datadir,filename):\n",
        "    # Load the wholesale customers dataset\n",
        "    #data = pd.read_csv(filename)\n",
        "    data = ''\n",
        "    print (\"loading: \"+datadir+filename)\n",
        "    try:\n",
        "        if zipfile.is_zipfile(datadir+filename):\n",
        "            z = zipfile.ZipFile(datadir+filename)\n",
        "            filename = z.open(filename[:-4])\n",
        "        else:\n",
        "            filename=datadir+filename\n",
        "        data = pd.read_csv(filename, parse_dates=True)  \n",
        "        print (\"Dataset has {} samples with {} features each.\".format(*data.shape))\n",
        "    except Exception as e:\n",
        "        print (\"Dataset could not be loaded. Is the dataset missing?\")\n",
        "        print(e)\n",
        "    return data\n",
        "\n",
        "def writeData(data,filename):\n",
        "    # Load the wholesale customers dataset\n",
        "    try:\n",
        "        data.to_csv(filename, index=False)\n",
        "    except Exception as e:\n",
        "        print (\"Dataset could not be written.\")\n",
        "        print(e)\n",
        "    verify=[]\n",
        "    try:\n",
        "        with open(filename, 'r') as f:\n",
        "            for line in f:\n",
        "                verify.append(line)\n",
        "        f.closed\n",
        "        return verify[:5]\n",
        "    except IOError:\n",
        "        sys.std"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ca71246a-a227-cb6d-c9de-e037ad24bdc2"
      },
      "outputs": [],
      "source": [
        "datadir=\"../input/\"\n",
        "data = loadData(datadir,'train.csv')\n",
        "display(data.info())\n",
        "display(data.head(5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "2ea93962-5cdb-5ee2-5f49-6cd5ae069d38"
      },
      "outputs": [],
      "source": [
        "features = data.columns\n",
        "cats = [feat for feat in features if 'cat' in feat]\n",
        "for feat in cats:\n",
        "    data[feat] = pd.factorize(data[feat], sort=True)[0]\n",
        "\n",
        "display(data.info())\n",
        "display(data.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "a0acb611-176a-89e0-e636-d1e473747098"
      },
      "outputs": [],
      "source": [
        "\n",
        "x=data.drop(['id','loss'],1).fillna(value=0)\n",
        "y=data['loss']\n",
        "\n",
        "display(x.head(5))\n",
        "display(y.head(5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c7b9ddb2-6cbf-a8e2-e6bd-80e3d5abe67c"
      },
      "outputs": [],
      "source": [
        "#  train/validation split\n",
        "X_train, X_test, y_train, y_test = train_test_split( x.values, y.values, test_size=0.25, random_state=42)\n",
        "\n",
        "dataSize=X_train.shape[0]\n",
        "print (\"size of train data\",dataSize, )\n",
        "test_sizes=[50]\n",
        "for i in range(5):\n",
        "    test_sizes.append(int(round(dataSize*(i+1)*.2)))\n",
        "\n",
        "#test_sizes=[63,630,6300,31500]\n",
        "#test_sizes=[50, 38108, 76217, 114325, 152434, 190542]\n",
        "print (\"run tests of size\",test_sizes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ac6ef20d-b30c-3dce-58a9-e21ae2594af1"
      },
      "outputs": [],
      "source": [
        "#regr = LinearRegression()\n",
        "\n",
        "# regr = ensemble.AdaBoostRegressor()  ## The mean squared error\n",
        "\n",
        "#regr = xgb.XGBClassifier(max_depth=6, learning_rate=0.075, n_estimators=15,\n",
        "#                                objective=\"reg:linear\", subsample=0.7,\n",
        "#                                colsample_bytree=0.7, seed=42)\n",
        "#regr = ExtraTreesRegressor()\n",
        "regr = RandomForestRegressor()\n",
        "\n",
        "#pca = decomposition.PCA(n_components = 100)\n",
        "#regr = Pipeline(steps=[('pca', pca), ('classifier', regr )]) # set up the clf as a pipeline so we can do randomized PCA\n",
        "\n",
        "regr.fit(X_train ,y_train )\n",
        "\n",
        "#params=dict(fit_intercept=[True,False], normalize  = [True,False])\n",
        "#grid_search = GridSearchCV(regr, param_grid= params, n_jobs= 1, scoring=make_scorer(f1_score)) \n",
        "#grid_search.fit(X_train,y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b290904e-3de5-0b1a-dcec-11e48e06ae02"
      },
      "outputs": [],
      "source": [
        "# The mean squared error\n",
        "print(\"Mean abs error: {:.2f}\".format(np.mean(abs(regr.predict(X_test) - y_test))))\n",
        "# Explained variance score: 1 is perfect prediction\n",
        "print(\"Score: {:.2f}\".format(regr.score(X_test, y_test)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "287c09b4-b9fc-c123-ee9c-0f36d18b03d3"
      },
      "outputs": [],
      "source": [
        "regr.fit(x,y)\n",
        "# The mean squared error\n",
        "print(\"Mean abs error: {:.2f}\".format(np.mean(abs(regr.predict(X_test) - y_test))))\n",
        "# Explained variance score: 1 is perfect prediction\n",
        "print(\"Score: {:.2f}\".format(regr.score(X_test, y_test)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d95eef17-1bf4-be59-0f9a-bc0b5d7ba223"
      },
      "outputs": [],
      "source": [
        "test_data= loadData(datadir,'test.csv')\n",
        "display(test_data.info())\n",
        "display(test_data.head(5))\n",
        "\n",
        "kfeatures = test_data.columns\n",
        "cats = [feat for feat in features if 'cat' in feat]\n",
        "for feat in cats:\n",
        "    test_data[feat] = pd.factorize(test_data[feat], sort=True)[0]\n",
        "\n",
        "test_X=test_data.drop(['id'],1).fillna(value=0)\n",
        "test_data['loss']=regr.predict(test_X)\n",
        "\n",
        "display(test_data.info())\n",
        "display(test_data.head())\n",
        "\n",
        "result=test_data[['id','loss']]\n",
        "display(result.info())\n",
        "display(result.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "4f2d7563-1a33-d1ab-6d1f-4b74b2853fd8"
      },
      "outputs": [],
      "source": [
        "output_fname=\"result_submission.csv\"\n",
        "writeData(result,output_fname)"
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}