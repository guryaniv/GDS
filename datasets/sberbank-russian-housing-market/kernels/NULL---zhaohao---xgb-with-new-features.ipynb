{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "a8c44092-3791-5953-541c-e2156fdd9c39"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "%matplotlib inline\n",
        "from sklearn import model_selection, preprocessing\n",
        "import xgboost as xgb\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "6e54d007-266f-4971-937f-decf5de2b833"
      },
      "outputs": [],
      "source": [
        "total = pd.read_csv('../input/train.csv')\n",
        "test = pd.read_csv('../input/test.csv')\n",
        "macro = pd.read_csv('../input/macro.csv')\n",
        "\n",
        "df_total = pd.merge(total, macro, on='timestamp', how='left')\n",
        "df_total.drop('id', axis = 1, inplace = True)\n",
        "df_total['price_doc'] = np.log(df_total['price_doc'])\n",
        "Ytotal = df_total['price_doc']\n",
        "\n",
        "df_test = pd.merge(test, macro, on='timestamp', how='left')\n",
        "df_test.drop('id', axis = 1, inplace = True)\n",
        "df_all = pd.concat([df_total,df_test], keys = ['total','test'])\n",
        "\n",
        "print ('total: ', df_total.shape)\n",
        "print ('test: ', df_test.shape)\n",
        "print ('macro: ', macro.shape)\n",
        "print ('all: ', df_all.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d89f29fc-3479-5119-14c3-40688749d75a"
      },
      "outputs": [],
      "source": [
        "def missingPattern(df):\n",
        "    numGroup = list(df._get_numeric_data().columns)\n",
        "    catGroup = list(set(df.columns) - set(numGroup))\n",
        "    print ('Total categorical/numerical variables are %s/%s' % (len(catGroup), len(numGroup)))\n",
        "    \n",
        "    #missing data\n",
        "    n = df.shape[0]\n",
        "    count = df.isnull().sum()\n",
        "    percent = 1.0 * count / n\n",
        "    dtype = df.dtypes\n",
        "    # correlation\n",
        "    missing_data = pd.concat([count, percent,dtype], axis=1, keys=['Count', 'Percent', 'Type'])\n",
        "    missing_data.sort_values('Count', ascending = False, inplace = True)\n",
        "    missing_data = missing_data[missing_data['Count'] > 0]\n",
        "    print ('Total missing columns is %s' % len(missing_data))\n",
        "\n",
        "    return numGroup, catGroup, missing_data\n",
        "\n",
        "numGroup, catGroup, missing_data = missingPattern(df_all)\n",
        "# missing_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "be39d4e6-8ba9-8e58-1b29-e06cecf0724a"
      },
      "outputs": [],
      "source": [
        "import operator\n",
        "def getCorr(df, numGroup, eps, *verbose):\n",
        "    corr = df[numGroup].corr()\n",
        "#     plt.figure(figsize=(8, 6))\n",
        "#     plt.pcolor(corr, cmap=plt.cm.Blues)\n",
        "#     plt.show()\n",
        "    corr.sort_values([\"price_doc\"], ascending = False, inplace = True)\n",
        "    highCorrList = list(corr.price_doc[abs(corr.price_doc)>eps].index)\n",
        "    if verbose:\n",
        "        print (\"Find most important features relative to target\")\n",
        "        print (corr.price_doc[abs(corr.price_doc)>eps])\n",
        "    return corr, highCorrList\n",
        "corr, highCorrList = getCorr(df_all.ix['total',:], numGroup, 0.4, True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "8e74d0b4-7138-3b9a-7ed4-35ed528dc4c6"
      },
      "outputs": [],
      "source": [
        "# for numerical variable, draw scatter plot(x vs y) and histogram plot(total vs test)    \n",
        "def scatterplotNum(df, varNum, ax):\n",
        "    plt.scatter(df[varNum], df['price_doc'])\n",
        "    plt.xlabel(varNum)\n",
        "    plt.ylabel('Price_doc')\n",
        "\n",
        "def hishplotNum(df, varNum, ax):\n",
        "    plt.hist(df.ix['total',varNum], bins = 50, alpha = 0.4)\n",
        "    plt.hist(df.ix['test',varNum], bins = 50, color = 'r', alpha = 0.4)\n",
        "    plt.xlabel(varNum)\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.legend(('total','test'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "57d9e3be-25f5-3a61-eb4d-b11794cb248a"
      },
      "outputs": [],
      "source": [
        "high_missing_data = missing_data[missing_data['Percent'] > 0.5]\n",
        "# print high_missing_data.index\n",
        "XYcorr = corr['price_doc'].to_dict()\n",
        "\n",
        "for i in XYcorr:\n",
        "    if i != 'price_doc' and XYcorr[i] > -1 and i in high_missing_data.index:\n",
        "        fig = plt.figure(i)\n",
        "        ax1 = fig.add_subplot(1,1,1)\n",
        "        scatterplotNum(df_all.ix['total'], i, ax1)\n",
        "        plt.title('correlation is %.4f' %(XYcorr[i]))\n",
        "\n",
        "#         ax2 = fig.add_subplot(1,2,2)\n",
        "#         hishplotNum(pd.concat([total,test],keys = ['total','test']), i, ax2)\n",
        "#         plt.title('correlation is %.4f' %(XYcorr[i]))\n",
        "        plt.gcf().set_size_inches(6, 4)\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b7a2b7ae-c79f-5f59-17cf-e2f8470cfc3c"
      },
      "outputs": [],
      "source": [
        "# remove the heavy missing features\n",
        "for i in high_missing_data.index:\n",
        "    df_all.drop(i, axis = 1, inplace = True)\n",
        "\n",
        "print ('all: ', df_all.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "5c11f91a-8cab-3f60-5f43-4815c8fc3467"
      },
      "outputs": [],
      "source": [
        "# total missing\n",
        "# macro missing\n",
        "basic_missing = list((set(missing_data.index) - set(high_missing_data.index)) & set(total.columns))\n",
        "macro_missing = list((set(missing_data.index) - set(high_missing_data.index)) & set(macro.columns))\n",
        "print ('missing in basic: ', len(basic_missing))\n",
        "print ('missing in macro: ', len(macro_missing))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "045bdd17-e47a-4a24-6671-78c7fad5fa4f"
      },
      "outputs": [],
      "source": [
        "### for macro info, look at the missing value info(mean, std) groupby yr, year_month\n",
        "df_all['timestamp'] = pd.to_datetime(df_all['timestamp'])\n",
        "df_all['year'] = df_all.timestamp.dt.year\n",
        "df_all['year_month'] = df_all.timestamp.dt.month + df_all.timestamp.dt.year * 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "2d3bd204-1f83-baa7-6f6a-a109828c251c"
      },
      "outputs": [],
      "source": [
        "# life_sq and full_sq are highly related to price_doc\n",
        "# life_sq <= full_sq and full_sq has no missing value\n",
        "\n",
        "# life_sq or full_sq <= 5\n",
        "df_all['life_sq'][df_all['life_sq']<=5] = df_all['full_sq'][df_all['life_sq']<=5]\n",
        "df_all['full_sq'][df_all['full_sq']<=5] = df_all['life_sq'][df_all['full_sq']<=5]\n",
        "\n",
        "\n",
        "# # life_sq or full_sq > 200 \n",
        "df_all['life_sq'].ix['total'][1084] = 28.1\n",
        "df_all['life_sq'].ix['total'][4385] = 42.6\n",
        "df_all['life_sq'].ix['total'][9237] = 30.1\n",
        "df_all['life_sq'].ix['total'][9256] = 45.8\n",
        "df_all['life_sq'].ix['total'][9646] = 80.2\n",
        "df_all['life_sq'].ix['total'][13546] = 74.78\n",
        "df_all['life_sq'].ix['total'][13629] = 25.9\n",
        "df_all['life_sq'].ix['total'][21080] = 34.9\n",
        "df_all['life_sq'].ix['total'][26342] = 43.5\n",
        "\n",
        "df_all['life_sq'].ix['test'][601] = 74.2\n",
        "df_all['life_sq'].ix['test'][1896] = 36.1\n",
        "df_all['life_sq'].ix['test'][2031] = 23.7\n",
        "df_all['life_sq'].ix['test'][2791] = 86.9\n",
        "df_all['life_sq'].ix['test'][5187] = 28.3\n",
        "\n",
        "df_all['full_sq'].ix['total'][1478] = 35.3\n",
        "df_all['full_sq'].ix['total'][1610] = 39.4\n",
        "df_all['full_sq'].ix['total'][2425] = 41.2\n",
        "df_all['full_sq'].ix['total'][2780] = 72.9\n",
        "df_all['full_sq'].ix['total'][3527] = 53.3\n",
        "df_all['full_sq'].ix['total'][5944] = 63.4\n",
        "df_all['full_sq'].ix['total'][7207] = 46.1\n",
        "\n",
        "\n",
        "# life_sq > full_sq\n",
        "df_all['life_sq'][df_all.life_sq > df_all.full_sq] = df_all['full_sq'][df_all.life_sq > df_all.full_sq]\n",
        "\n",
        "# kitch_sq > full_sq\n",
        "\n",
        "df_all['kitch_sq'][df_all.kitch_sq > df_all.full_sq] = \\\n",
        "            df_all['full_sq'][df_all.kitch_sq > df_all.full_sq] - df_all['life_sq'][df_all.kitch_sq > df_all.full_sq]\n",
        "\n",
        "\n",
        "# else\n",
        "# floor > max_floor\n",
        "df_all['max_floor'][df_all.floor > df_all.max_floor] = \\\n",
        "        df_all['floor'][df_all.floor > df_all.max_floor] + df_all['max_floor'][df_all.floor > df_all.max_floor]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "628ed064-4ac8-c116-c1aa-20e4647d010e"
      },
      "outputs": [],
      "source": [
        "# fill the missing value in train and test\n",
        "def basicmissingFill(df):\n",
        "    # num variables\n",
        "    # pre-processing\n",
        "    n = df.shape[0]\n",
        "    \n",
        "    \n",
        "    df_all['life_sq'][df_all.life_sq.isnull()] = df_all['full_sq'][df_all.life_sq.isnull()]\n",
        "\n",
        "\n",
        "    df['state'] = df['state'].replace({33:3})\n",
        "    df['build_year'][df['build_year'] == 20052009] = 2005\n",
        "    df['build_year'][df['build_year'] == 4965] = float('nan')\n",
        "    df['build_year'][df['build_year'] == 0] = float('nan')\n",
        "    df['build_year'][df['build_year'] == 1] = float('nan')\n",
        "    df['build_year'][df['build_year'] == 3] = float('nan')\n",
        "    df['build_year'][df['build_year'] == 71] = float('nan')\n",
        "    df['build_year'][df['build_year'] == 20] = 2000\n",
        "    df['build_year'][df['build_year'] == 215] = 2015\n",
        "    df['build_year'].ix['total'][13117] = 1970\n",
        "\n",
        "\n",
        "    \n",
        "    # zero-filling count feature \n",
        "    zero_fil = ['build_count_brick','build_count_block','build_count_mix','build_count_before_1920',\\\n",
        "               'build_count_1921-1945','build_count_1946-1970','build_count_1971-1995','build_count_after_1995',\\\n",
        "               'build_count_monolith','build_count_slag','build_count_wood','build_count_panel','build_count_frame',\\\n",
        "               'build_count_foam','preschool_quota']\n",
        "    for i in zero_fil:\n",
        "        df[i] = df[i].fillna(0)\n",
        "    \n",
        "    # mode-filling: count feature and ID\n",
        "    mode_fil = ['state','ID_railroad_station_walk','build_year','material','num_room']\n",
        "    for i in mode_fil:\n",
        "        df[i] = df[i].fillna(df[i].mode()[0]) \n",
        "\n",
        "    # mean-filling\n",
        "    mean_fil = ['cafe_avg_price_500','cafe_avg_price_1000','cafe_avg_price_1500','cafe_avg_price_2000',\\\n",
        "               'cafe_avg_price_3000','cafe_avg_price_5000','cafe_sum_500_max_price_avg','cafe_sum_500_min_price_avg',\\\n",
        "               'cafe_sum_1000_max_price_avg','cafe_sum_1000_min_price_avg','cafe_sum_1500_max_price_avg',\\\n",
        "               'cafe_sum_1500_min_price_avg','cafe_sum_2000_max_price_avg','cafe_sum_2000_min_price_avg',\\\n",
        "               'cafe_sum_3000_max_price_avg','cafe_sum_3000_min_price_avg','cafe_sum_5000_max_price_avg',\\\n",
        "               'cafe_sum_5000_min_price_avg','railroad_station_walk_min','railroad_station_walk_km',\\\n",
        "               'school_quota','raion_build_count_with_material_info','prom_part_5000',\\\n",
        "               'raion_build_count_with_builddate_info','green_part_2000','metro_km_walk','metro_min_walk',\\\n",
        "               'hospital_beds_raion']\n",
        "    for i in mean_fil:\n",
        "        grouped = df[['year',i]].groupby('year')\n",
        "        df[i] = grouped.transform(lambda x: x.fillna(x.mean()))\n",
        "        \n",
        "    # exception: 'kitch_sq','floor','max_floor'\n",
        "    df['kitch_sq'][df.kitch_sq.isnull()] = df['full_sq'][df.kitch_sq.isnull()] - df['life_sq'][df.kitch_sq.isnull()]\n",
        "    df['floor'] = df['floor'].fillna(df['floor'].mean())\n",
        "    df['max_floor'][df.max_floor.isnull()] = df['floor'][df.max_floor.isnull()]\n",
        "    \n",
        "    #================\n",
        "    # Cat. variables\n",
        "    df['product_type'] = df['product_type'].fillna(df['product_type'].mode()[0])\n",
        "    \n",
        "    return df\n",
        "\n",
        "df_all = basicmissingFill(df_all)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "95f5ebfd-7d80-7c66-701f-1fd34435a9d5"
      },
      "outputs": [],
      "source": [
        "print ('basic_missing filling finished: ', df_all[basic_missing].isnull().sum().sum() == 7662)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "30e2c947-cccc-bf0c-52c2-95534a1650da"
      },
      "outputs": [],
      "source": [
        "# for Cat features in macro_missing\n",
        "macro_missing_obj = []\n",
        "for i in macro_missing:\n",
        "    if df_all[i].dtype == object:\n",
        "        grouped = df_all[['year',i]].groupby(['year',i])\n",
        "        # print grouped.agg(len)\n",
        "        macro_missing_obj.append(i)\n",
        "        # print missing_data.ix[i]\n",
        "        # print '\\n'\n",
        "# consider to drop macro_missing_obj\n",
        "for i in macro_missing_obj:\n",
        "    df_all.drop(i, axis = 1, inplace = True)\n",
        "    macro_missing.remove(i)\n",
        "\n",
        "print ('macro missing features count: ', len(macro_missing))\n",
        "print ('df_all shape: ', df_all.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "8b34708b-7c94-92ec-4038-a978196fbc15"
      },
      "outputs": [],
      "source": [
        "# for num features in macro_missing\n",
        "# filling strategy: for each feature->if 2015 is not null: fillna the mean(2015) else: fillna the mean(2014)\n",
        "def macromissingFill(df):\n",
        "    for i in macro_missing:\n",
        "        fill2014 = np.nanmean(df[i][df['year']==2014])\n",
        "        fill2015 = np.nanmean(df[i][df['year']==2015])\n",
        "        # income_per_cap: the only macro_missing feature which is not agg by year\n",
        "        if ~np.isnan(fill2015):\n",
        "            df[i] = df[i].fillna(fill2015)\n",
        "        else:\n",
        "            df[i] = df[i].fillna(fill2014)\n",
        "\n",
        "    return df\n",
        "\n",
        "df_all = macromissingFill(df_all)\n",
        "print ('macro_missing filling finished: ', df_all[macro_missing].isnull().sum().sum() == 0)\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d61119e5-29f9-c1f6-df25-67a08b3eba84"
      },
      "outputs": [],
      "source": [
        "# running mean price vs timestamp\n",
        "# len(df_all.timestamp.unique()) -> total 1435 timestamp\n",
        "\n",
        "def running_mean(df, x, y, agg_list, k, *condition): \n",
        "    if condition:\n",
        "        if condition[0] == 'Xless':\n",
        "            grouped = df[[x,y]][df[x] <= condition[1]].ix['total'].groupby(x)\n",
        "        elif condition[0] == 'Xlarger':\n",
        "            grouped = df[[x,y]][df[x] >= condition[1]].ix['total'].groupby(x)\n",
        "        elif condition[0] == 'Yless':\n",
        "            grouped = df[[x,y]][df[y] <= condition[1]].ix['total'].groupby(x)\n",
        "        elif condition[0] == 'Ylarger':\n",
        "            grouped = df[[x,y]][df[y] >= condition[1]].ix['total'].groupby(x)  \n",
        "        else: \n",
        "            assert 'Wrong conditions!'\n",
        "    else:\n",
        "        grouped = df[[x, y]].ix['total'].groupby(x)\n",
        "    agg_y = grouped.agg({y: agg_list})[y]\n",
        "\n",
        "    m, n = agg_y.shape\n",
        "    px = list(agg_y.index)[:m+1-k]\n",
        "    py = []\n",
        "    for i in range(n):\n",
        "        temp = []\n",
        "        for j in range(m+1-k):\n",
        "            temp.append(np.mean(agg_y.iloc[j:j+k,i]))\n",
        "        py.append(temp)\n",
        "\n",
        "    # plot\n",
        "    colors = ['r','g','b','y']\n",
        "    plt.figure()\n",
        "    for i in range(len(agg_list)):\n",
        "        if k == 1:\n",
        "            plt.scatter(px, py[i], color = colors[i])\n",
        "        else:\n",
        "            plt.plot(px, py[i], color = colors[i])\n",
        "    \n",
        "    plt.xlabel(x)\n",
        "    plt.ylabel(y)\n",
        "    plt.legend(agg_list)\n",
        "    plt.title('running mean: ' + str(k))\n",
        "    plt.gcf().set_size_inches(10, 6)\n",
        "    plt.show()        \n",
        "    return px, py\n",
        "\n",
        "x = 'timestamp'\n",
        "y = 'price_doc'\n",
        "agg_list = ['mean','median']\n",
        "k = 60\n",
        "px, py = running_mean(df_all, x, y, agg_list, k)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d09b9291-699c-1a8c-76cb-e90b8ac2d8a4"
      },
      "outputs": [],
      "source": [
        "# year and housing sq\n",
        "df_all['used_yr'] = df_all['year'] - df_all['build_year']\n",
        "df_all['used_yr'][df_all['used_yr'] < 0] = 0\n",
        "df_all['rel_floor'] = df_all['floor'] / df_all['max_floor'].astype(float)\n",
        "df_all['rel_life_sq'] = df_all['life_sq'] / df_all['full_sq'].astype(float)\n",
        "df_all['rel_kitch_sq'] = df_all['kitch_sq'] / df_all['full_sq'].astype(float)\n",
        "# fillna\n",
        "df_all['rel_floor'] = df_all['rel_floor'].fillna(df_all['rel_floor'].mean())\n",
        "df_all['rel_life_sq'] = df_all['rel_life_sq'].fillna(df_all['rel_life_sq'].mean())\n",
        "df_all['rel_kitch_sq'] = df_all['rel_kitch_sq'].fillna(df_all['rel_kitch_sq'].mean())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "06b2b5d2-9682-183a-05bf-191024703741"
      },
      "outputs": [],
      "source": [
        "numGroup = list(df_all._get_numeric_data().columns)\n",
        "corr, highCorrList = getCorr(df_all.ix['total',:], numGroup, 0.15)\n",
        "\n",
        "cntGroup = [i for i in numGroup if re.match(r'\\w+_count+',i)]\n",
        "raionGroup = [i for i in numGroup if re.match(r'\\w+_raion+',i)]\n",
        "kmGroup = [i for i in numGroup if re.match(r'\\w+_km+',i)]\n",
        "minGroup = ['metro_min_avto', 'metro_min_walk', 'public_transport_station_min_walk', 'railroad_station_avto_min',\\\n",
        "            'railroad_station_walk_min']\n",
        "print ('cntGroup:', len(cntGroup))\n",
        "print ('raionGroup: ', len(raionGroup))\n",
        "print ('kmGroup: ', len(kmGroup))\n",
        "print ('minGroup: ', len(minGroup))\n",
        "\n",
        "# for i in kmGroup+minGroup+cntGroup:\n",
        "#     if sum(df_all[i] < 0) > 0:\n",
        "#         print i\n",
        "#     else:\n",
        "#         print 'positive'\n",
        "# all positive values in kmGroup, minGroup, cntGroup "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "96b50f68-bc97-0408-5af2-35573d3ce705"
      },
      "outputs": [],
      "source": [
        "# km and minutes to neighborhood feats transformation\n",
        "# house full_sq and distance to neighborhood combination\n",
        "\n",
        "def min_km_trans(df, group):\n",
        "    group = list(group)\n",
        "    newFeats = []\n",
        "    for i in group:\n",
        "        df['fsq_'+i+'_inv1'] = df['full_sq'] / (df[i] + 0.1)\n",
        "        df['fsq_'+i+'_inv5'] = df['full_sq'] / (df[i] + 0.5)\n",
        "        df['fsq_'+i+'_inv10'] = df['full_sq'] / (df[i] + 1.0)\n",
        "        df['fsq_'+i+'_invlg1'] = df['full_sq'] / (np.log1p(df[i]) + 0.1)\n",
        "        df['fsq_'+i+'_invlg5'] = df['full_sq'] / (np.log1p(df[i]) + 0.5)\n",
        "        df['fsq_'+i+'_invlg10'] = df['full_sq'] / (np.log1p(df[i]) + 1.0)\n",
        "        # df['full_sq_'+i] = df['full_sq'] / (np.log1p(df[i]) + 0.1)\n",
        "        newFeats += ['fsq_'+i+'_inv1','fsq_'+i+'_inv5','fsq_'+i+'_inv10','fsq_'+i+'_invlg1',\\\n",
        "                     'fsq_'+i+'_invlg5','fsq_'+i+'_invlg10']\n",
        "    group.extend(newFeats)\n",
        "    return df, group\n",
        "\n",
        "df_all, kmFeats = min_km_trans(df_all, kmGroup)\n",
        "df_all, minFeats = min_km_trans(df_all, minGroup)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "457074e8-a388-7468-2f41-110bab410b4c"
      },
      "outputs": [],
      "source": [
        "cntFeats = list(set(cntGroup) & set(highCorrList))\n",
        "extFeats = ['full_sq','life_sq','kitch_sq','floor','max_floor','num_room','build_year',\\\n",
        "            'used_yr','rel_life_sq','rel_kitch_sq','rel_floor',\\\n",
        "           'ppi','cpi','price_doc']\n",
        "print (len(highCorrList),len(cntFeats),len(kmFeats)/7,len(minFeats)/7,len(extFeats))\n",
        "# print highCorrList\n",
        "numFeats = cntFeats + kmFeats + minFeats + extFeats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "61c8c070-43e6-31ac-2156-c09b2a2eaaf5"
      },
      "outputs": [],
      "source": [
        "testId = list(test['id'])\n",
        "drop_list = ['timestamp','year','year_month','price_doc']\n",
        "for i in drop_list:\n",
        "    df_all.drop(i, axis = 1, inplace = True)\n",
        "\n",
        "numGroup,catGroup,_ = missingPattern(df_all)\n",
        "\n",
        "# self-define numGroup\n",
        "# numGroup = numFeats\n",
        "\n",
        "df_total_num = df_all.ix['total',numGroup]\n",
        "df_test_num = df_all.ix['test',numGroup]\n",
        "df_total_cat = df_all.ix['total',catGroup]\n",
        "df_test_cat = df_all.ix['test',catGroup]\n",
        "print ('Current training numerical variables count is %d '  %(df_total_num.shape[1]))\n",
        "print ('Current training categorical variables count is %d '  %(df_total_cat.shape[1]))\n",
        "print ('Current test numerical variables count is %d '  %(df_test_num.shape[1]))\n",
        "print ('Current test categorical variables count is %d '  %(df_test_cat.shape[1]))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "2d35aa35-b9bc-98d4-c91c-543fc47476ed"
      },
      "outputs": [],
      "source": [
        "# one-hot encoding for categorical variables\n",
        "df_concat_cat = pd.concat([df_total_cat,df_test_cat],keys = ['total','test'])\n",
        "df_total_cat = pd.get_dummies(df_concat_cat).ix['total',:]\n",
        "df_test_cat = pd.get_dummies(df_concat_cat).ix['test',:]\n",
        "print ('After one-hot encoding, total training cat variables are %d' %(df_total_cat.shape[1]))\n",
        "print ('After one-hot encoding, total test cat variables are %d' %(df_test_cat.shape[1]))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "f690a8df-c73e-8150-2eca-67e3a689145a"
      },
      "outputs": [],
      "source": [
        "#Xtotal = pd.concat([df_total_num,df_total_cat], axis = 1)\n",
        "#Xtest = pd.concat([df_test_num,df_test_cat], axis = 1)\n",
        "Xtotal = df_total_num\n",
        "Xtest = df_test_num\n",
        "dtrain = xgb.DMatrix(Xtotal, Ytotal)\n",
        "dtest = xgb.DMatrix(Xtest)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "443fc813-9907-b334-745a-05b658786daf"
      },
      "source": [
        "### Modeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "89747693-764c-cd5a-c64d-ef5b62bde3c0"
      },
      "outputs": [],
      "source": [
        "xgb_params = {\n",
        "    'eta': 0.05,\n",
        "    'max_depth': 3,\n",
        "    'subsample': 0.8,\n",
        "    'colsample_bytree': 0.7,\n",
        "    'lambda': 0.2,\n",
        "    'alpha': 0.2,\n",
        "    'objective': 'reg:linear',\n",
        "    'eval_metric': 'rmse',\n",
        "    'silent': 1,\n",
        "    'scale_pos_weight': 1,\n",
        "}\n",
        "cv_output = xgb.cv(xgb_params, dtrain, num_boost_round=1000, early_stopping_rounds=50,\n",
        "    verbose_eval=50, show_stdv=False)\n",
        "cv_output[['train-rmse-mean', 'test-rmse-mean']].plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "3d9bacab-2a91-70b1-c9b7-416ace5224df"
      },
      "outputs": [],
      "source": [
        "num_boost_rounds = len(cv_output)\n",
        "model = xgb.train(dict(xgb_params, silent=0), dtrain, num_boost_round= num_boost_rounds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c81699ed-0437-6f2f-0554-4d8ad4af8135"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1, 1, figsize=(8, 12))\n",
        "xgb.plot_importance(model, max_num_features=50, height=0.5, ax=ax)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "9837d559-a897-0c38-6a31-705b4324c675"
      },
      "source": [
        "### Prediction on test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "09205e39-8545-9a7a-0095-eeb03aa4fc74"
      },
      "outputs": [],
      "source": [
        "Ypred = model.predict(dtest)\n",
        "Ypred = np.expm1(Ypred)\n",
        "output = pd.DataFrame({\"id\": testId, \"price_doc\": Ypred})\n",
        "output.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c1d8487e-cefa-bb1b-77ae-bab241b2f851"
      },
      "outputs": [],
      "source": [
        "output.to_csv('xgb_submission.csv',index=False)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}