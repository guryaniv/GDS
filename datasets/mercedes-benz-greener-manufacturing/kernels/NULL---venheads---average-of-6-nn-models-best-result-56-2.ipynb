{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "935b4ed8-9402-0e5e-2f71-aa93918087a0"
      },
      "source": [
        "Hi\n",
        "\n",
        " You can find below a simple net with 1 hidden layers and averaging of 6. It's perfomance changes with different types of activation Units, batch size, number of epochs and the data you put it (mostly) Play around with them\n",
        "\n",
        "The basic feature generation has been made\n",
        "\n",
        "There is also a checkpoint but Umberto has already published a net with that  (https://www.kaggle.com/umbertogriffo/deep-learning-0-55326-for-now/comments#latest-189753)\n",
        "\n",
        "The best result from net was about 56,2 - but i think it can achieve a little more\n",
        "\n",
        "Try it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "bfa853b1-55e3-a3b6-5382-43e3239f83cc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.cross_validation import train_test_split\n",
        "import pandas as pd\n",
        "import subprocess\n",
        "from scipy.sparse import csr_matrix, hstack\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cross_validation import KFold\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation\n",
        "from keras.layers.advanced_activations import PReLU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b255ba5d-b447-ebd6-5db5-faaf86dc62ac"
      },
      "outputs": [],
      "source": [
        "test = pd.read_csv('../input/test.csv')\n",
        "train = pd.read_csv('../input/train.csv')\n",
        "sub = pd.read_csv('../input/sample_submission.csv')\n",
        "y = train['y']\n",
        "ID = test['ID']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "1d0a403a-99e7-9eb7-50d4-ecf4723fab5c"
      },
      "outputs": [],
      "source": [
        "train.drop(['y','ID'],axis=1,inplace= True)\n",
        "test.drop(['ID'],axis=1,inplace= True)\n",
        "ntrain = train.shape[0]\n",
        "xtr_te = pd.concat((train, test), axis = 0)\n",
        "sparse_data = []\n",
        "categories = ['X0', 'X1', 'X2', 'X3', 'X4','X5', 'X6', 'X8']\n",
        "for f in categories:\n",
        "    dummy = pd.get_dummies(xtr_te[f].astype('category'))\n",
        "    tmp = csr_matrix(dummy)\n",
        "    sparse_data.append(tmp)\n",
        "xtr_te_cat = hstack(sparse_data, format = 'csr')\n",
        "xtr_te_cat = xtr_te_cat.todense()\n",
        "drop = []\n",
        "for i in train.columns:\n",
        "    if len(train[i].unique()) <2:\n",
        "        print(len(train[i].unique())), i\n",
        "        drop.append(i)\n",
        "xtr_te.drop(categories,axis=1,inplace=True)\n",
        "xtr_te.drop(drop,axis=1,inplace=True)\n",
        "xtr_te = np.array(xtr_te)\n",
        "xtr_te = np.concatenate((xtr_te_cat,xtr_te),axis =1)\n",
        "xtrain = xtr_te[:ntrain,:]\n",
        "xtest = xtr_te[ntrain:,:]\n",
        "from sklearn import preprocessing\n",
        "\n",
        "xtr_te = np.array(xtr_te)\n",
        "xtr_te = preprocessing.minmax_scale(xtr_te)\n",
        "xtrain = xtr_te[:ntrain,:]\n",
        "xtest = xtr_te[ntrain:,:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d09f9d48-33c7-0570-da6e-e50e7548848b"
      },
      "outputs": [],
      "source": [
        "X_train_1,X_test_1,y_train_1,y_test_1 = train_test_split(xtrain,y,test_size=0.4)\n",
        "X_train_2,X_test_2,y_train_2,y_test_2 = train_test_split(xtrain,y,test_size=0.3)\n",
        "X_train_3,X_test_3,y_train_3,y_test_3 = train_test_split(xtrain,y,test_size=0.3)\n",
        "X_train_4,X_test_4,y_train_4,y_test_4 = train_test_split(xtrain,y,test_size=0.3)\n",
        "X_train_5,X_test_5,y_train_5,y_test_5 = train_test_split(xtrain,y,test_size=0.3)\n",
        "X_train_6,X_test_6,y_train_6,y_test_6 = train_test_split(xtrain,y,test_size=0.3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "2aa11dbd-8dc0-d1e4-4837-0cf68dc9a7a8"
      },
      "outputs": [],
      "source": [
        "# play around with parameters\n",
        "batch__size = 128 # try 32,64,16\n",
        "number_of_epochs = 10 #change it to 10000 to find out the best set of weights\n",
        "den = 125 # try different one"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b1279cd7-e6bb-9cda-b065-6b982c2edd99"
      },
      "outputs": [],
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "import h5py\n",
        "checkpointer = ModelCheckpoint(filepath=\"weights_model_1.hdf5\", verbose=0, save_best_only=True,monitor='val_loss')\n",
        "def nn_model():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(den, input_dim = xtrain.shape[1], init = 'he_normal',activation='tanh'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(1, init = 'he_normal'))\n",
        "    model.compile(loss = 'mse', optimizer = 'adadelta')\n",
        "    return(model)\n",
        "model_1 = nn_model()\n",
        "model_1.fit(X_train_1, y_train_1, nb_epoch=number_of_epochs,verbose=0,batch_size=batch__size,\n",
        "                 callbacks=[checkpointer],validation_data=(X_test_1,y_test_1))\n",
        "\n",
        "v1 = model_1.evaluate(X_test_1,y_test_1)\n",
        "print (v1,v1**0.5)\n",
        "\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import h5py\n",
        "checkpointer = ModelCheckpoint(filepath=\"weights_model_2.hdf5\", verbose=0, save_best_only=True,monitor='val_loss')\n",
        "def nn_model():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(den, input_dim = xtrain.shape[1], init = 'he_normal',activation='tanh'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(1, init = 'he_normal'))\n",
        "    model.compile(loss = 'mse', optimizer = 'adadelta')\n",
        "    return(model)\n",
        "model_2 = nn_model()\n",
        "model_2.fit(X_train_2, y_train_2, nb_epoch=number_of_epochs,verbose=0,batch_size=batch__size,\n",
        "                 callbacks=[checkpointer],validation_data=(X_test_2,y_test_2))\n",
        "\n",
        "v2 = model_2.evaluate(X_test_2,y_test_2)\n",
        "\n",
        "print (v2,v2**0.5)\n",
        "\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import h5py\n",
        "checkpointer = ModelCheckpoint(filepath=\"weights_model_3.hdf5\", verbose=0, save_best_only=True,monitor='val_loss')\n",
        "def nn_model():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(den, input_dim = xtrain.shape[1], init = 'he_normal',activation='tanh'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(1, init = 'he_normal'))\n",
        "    model.compile(loss = 'mse', optimizer = 'adadelta')\n",
        "    return(model)\n",
        "model_3 = nn_model()\n",
        "model_3.fit(X_train_3, y_train_3, nb_epoch=number_of_epochs,verbose=0,batch_size=batch__size,\n",
        "                 callbacks=[checkpointer],validation_data=(X_test_3,y_test_3))\n",
        "\n",
        "v3 = model_3.evaluate(X_test_3,y_test_3)\n",
        "\n",
        "print (v3,v3**0.5)\n",
        "\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import h5py\n",
        "checkpointer = ModelCheckpoint(filepath=\"weights_model_4.hdf5\", verbose=0, save_best_only=True,monitor='val_loss')\n",
        "def nn_model():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(den, input_dim = xtrain.shape[1], init = 'he_normal',activation='tanh'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(1, init = 'he_normal'))\n",
        "    model.compile(loss = 'mse', optimizer = 'adadelta')\n",
        "    return(model)\n",
        "model_4 = nn_model()\n",
        "model_4.fit(X_train_4, y_train_4, nb_epoch=number_of_epochs,verbose=0,batch_size=batch__size,\n",
        "                 callbacks=[checkpointer],validation_data=(X_test_4,y_test_4))\n",
        "\n",
        "v4 = model_4.evaluate(X_test_4,y_test_4)\n",
        "print (v4,v4**0.5)\n",
        "\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import h5py\n",
        "checkpointer = ModelCheckpoint(filepath=\"weights_model_5.hdf5\", verbose=0, save_best_only=True,monitor='val_loss')\n",
        "def nn_model():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(den, input_dim = xtrain.shape[1], init = 'he_normal',activation='tanh'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(1, init = 'he_normal'))\n",
        "    model.compile(loss = 'mse', optimizer = 'adadelta')\n",
        "    return(model)\n",
        "model_5 = nn_model()\n",
        "model_5.fit(X_train_5, y_train_5, nb_epoch=number_of_epochs,verbose=0,batch_size=batch__size,\n",
        "                 callbacks=[checkpointer],validation_data=(X_test_5,y_test_5))\n",
        "\n",
        "v5 = model_5.evaluate(X_test_5,y_test_5)\n",
        "print (v5,v5**0.5)\n",
        "\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import h5py\n",
        "checkpointer = ModelCheckpoint(filepath=\"weights_model_6.hdf5\", verbose=0, save_best_only=True,monitor='val_loss')\n",
        "def nn_model():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(den, input_dim = xtrain.shape[1], init = 'he_normal',activation='tanh'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(1, init = 'he_normal'))\n",
        "    model.compile(loss = 'mse', optimizer = 'adadelta')\n",
        "    return(model)\n",
        "model_6 = nn_model()\n",
        "model_6.fit(X_train_6, y_train_6, nb_epoch=number_of_epochs,verbose=0,batch_size=batch__size,\n",
        "                 callbacks=[checkpointer],validation_data=(X_test_6,y_test_6))\n",
        "\n",
        "v6 = model_6.evaluate(X_test_6,y_test_6)\n",
        "print (v6,v6**0.5)\n",
        "\n",
        "print ((v1+v2+v3+v4+v5+v6)/6, 'mse final',(v1**0.5+v2**0.5+v3**0.5+v4**0.5+v5**0.5+v6**0.5)/6,'as rmse final')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "f96cd14d-ba26-37b2-bf2b-a565e176cd89"
      },
      "outputs": [],
      "source": [
        "model_1.load_weights(\"weights_model_1.hdf5\")\n",
        "model_2.load_weights(\"weights_model_2.hdf5\")\n",
        "model_3.load_weights(\"weights_model_3.hdf5\")\n",
        "model_4.load_weights(\"weights_model_4.hdf5\")\n",
        "model_5.load_weights(\"weights_model_5.hdf5\")\n",
        "model_6.load_weights(\"weights_model_6.hdf5\")\n",
        "\n",
        "r1 = r2_score(y_test_1,model_1.predict(X_test_1))\n",
        "r2 = r2_score(y_test_2,model_2.predict(X_test_2))\n",
        "r3 = r2_score(y_test_3,model_3.predict(X_test_3))\n",
        "r4 = r2_score(y_test_4,model_4.predict(X_test_4))\n",
        "r5 = r2_score(y_test_5,model_5.predict(X_test_5))\n",
        "r6 = r2_score(y_test_6,model_6.predict(X_test_6))\n",
        "\n",
        "print (r1,r2,r3,r4,r5,r6)\n",
        "print ((r1+r2+r3+r4+r5+r6)/6.0)\n",
        "\n",
        "\n",
        "\n",
        "pred1 = model_1.predict(xtest)\n",
        "pred2 = model_2.predict(xtest)\n",
        "pred3 = model_3.predict(xtest)\n",
        "pred4 = model_4.predict(xtest)\n",
        "pred5 = model_5.predict(xtest)\n",
        "pred6 = model_6.predict(xtest)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "170369fc-409d-4590-c72d-ca8e1cf22aa2"
      },
      "outputs": [],
      "source": [
        "pred_final2 = (pred1+pred2+pred3+pred4+pred5+pred6)/6.0\n",
        "sub['y'] = pred_final2\n",
        "sub.to_csv('output.csv',index=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "618b8802-092f-88cc-a530-edc47526beff"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}