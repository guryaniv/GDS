{"cells":[{"metadata":{"_cell_guid":"a239913a-b27f-15e7-8911-53b62bf41c4e","_uuid":"d5622be7b8726d3477b072ff9652c71c0431bdaa"},"cell_type":"markdown","source":"# Addressing class imbalance \n\nThis notebook will examine a Random Forest approach on skewed data. The point is to explore how class inbalance leads to indesirable classification perfomance and how we can use non-unit weighting to adress this.\n\n### The fraudulent transaction dataset and costs\n\nThe dataset used here is highly unbalanced dataset where the relevant class is whether the transaction is fraudulent (class 1) or not (class 0). In particular in the setting of fraud, each undetected fraudulent transaction carries an average cost, e.g., due direct monetary loss or an indirect reputation loss. On the other hand, a cost may also be associated a false positives, due to e.g., that a human must manually examine this transaction.\n\nFor this reason, it does not make sense to just accept an arbitrary tradeoff dictated by a classifier. An acceptable tradeoff would balance expectations about costs of both false positives and false negatives.\n\n### String a balance between false negatives and false positives\n\nThis notebook will examine a Random Forest approach on skewed data. The point is to explore how class inbalance leads to indesirable classification perfomance, in particular favouritism of the majority class and whether we (indirectly) can adapt the loss function to create a more appropriate balance between the false positive rate and false negative, rather than just minimizing the total number of misclassifications.\n\nWhile Random Forests do not have explicit loss function, we  implicitly have one due to the way we split nodes in each decision tree.  To simply the argument, let's consider a single decision tree where the Gini impurity is a splitting criterion (this is default is scikit-learn) and that we have only two classes: \n \n $$I_{G} = 1 - (p_0^2  + p_1^2)$$ \n \n With weights, we essentially end up with something like\n\n  $$I_{G} = 1 - (w_0 p_0^2  + w_1 p_1^2)$$ \n  \n (possibly with some some normalization of the weights to make Gini more interpretable)\n\n \n "},{"metadata":{"_cell_guid":"029ecde6-086d-7a8e-de44-363a7a23dbd8","_uuid":"9753184729d3ae8f94c222e3bbb6c2cb97eaea76","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport sklearn as sk\n\n%matplotlib inline\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6260e0c9d90a176a2726f1d664d148a4de25741e"},"cell_type":"markdown","source":"### Load the data"},{"metadata":{"_cell_guid":"7e5ca1e3-3597-19d2-b4be-dffd335df630","_uuid":"100266119b228d1f3f8fe22fcd4101f53e6a1a67","trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"../input/creditcard.csv\")\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"6268bbd8-6de5-2389-5693-ecd9a14872d4","_uuid":"3c2cafdaf4435aa37419eca793d7293331273e15"},"cell_type":"markdown","source":"### Checking the target classes"},{"metadata":{"_cell_guid":"3f6e6674-12e9-6983-5788-5755f80c7ec2","_uuid":"2e8050e85056962a0e1443623d68ffeb8553a050","trusted":true},"cell_type":"code","source":"count_classes = pd.value_counts(data['Class'], sort = True).sort_index()\ncount_classes.plot(kind = 'bar')\nplt.title(\"Fraud class histogram\")\nplt.xlabel(\"Class\")\nplt.ylabel(\"Frequency\")\nplt.yscale(\"log\")","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"801dd843-a90b-bb5f-97e7-2da84cc6cd6a","_uuid":"ba077233ea8fbf1168bad3304781e3870ece3e94"},"cell_type":"markdown","source":"### Clearly the data is totally unbalanced!!  \n\nSince the data is so unbalanced, using a typical accuracy score to evaluate our classification algorithm would be misleading. For example, if we just used a majority class to assign values to all records, we will still be having a high accuracy, but we would be classifying all ones incorrectly. "},{"metadata":{"_cell_guid":"6b74ba73-82a8-3585-b790-44fe486ff19d","_uuid":"4a2af122155f70d980a84bcf2be52cfd7a2158bc"},"cell_type":"markdown","source":"### Splitting data into train and test set. \n\nTo keep this simple, we use a traditional training/test split."},{"metadata":{"_cell_guid":"4a725b16-c14a-2be8-8240-617b7b2ed8cd","_uuid":"73c34e308338a870d6044651d7098b76cab33ea0","trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX = data.iloc[:,1:data.shape[1]-1]\ny = data.iloc[:,data.shape[1]-1]\n\n# Whole dataset\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.3, random_state = 0)\n\nprint(\"Number transactions train dataset: \", len(X_train))\nprint(\"Number transactions test dataset: \", len(X_test))\nprint(\"Total number of transactions: \", len(X_train)+len(X_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"22b0db6f2fa64ab3a85bec2be39d8ec8f5f07848"},"cell_type":"code","source":"def confusion(classifier, X_test, y_test):\n    y_pred  = classifier.predict(X_test)\n    return confusion_matrix(y_test, y_pred).ravel()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b3e17445866791c66ea8e051395e02e596515740"},"cell_type":"code","source":"def show(tn,fp,fn,tp):\n    print(\"TN:\" + str(tn) + \" FP:\" + str(fp) + \" FN:\" + str(fn) + \" TP:\" + str(tp) + \n          \" FNR=\" + str(fn/(fn+tp)) + \" FPR=\" + str(fp/(fp+tn)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5830552a41e235f19c7384c761373e0aba60cbf7"},"cell_type":"markdown","source":"### Using ***unbalanced*** RandomForestClassifier approach:\n\nThis is usual Random forest classifier which is the default in scikit-learn. All transactions regardless of label are weighted the same. "},{"metadata":{"trusted":true,"_uuid":"8611a869ee92ca32994aa0e9d845f54374be623d"},"cell_type":"code","source":"show(*confusion(RandomForestClassifier(random_state=0, n_jobs=-1, n_estimators=10).fit(X_train,y_train),X_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c3ef44ed134e826891118f87733d25da08152fec"},"cell_type":"markdown","source":"Clearly, the false negative rate is much higher than the false positive rate. \nThis demonstrates the inherent  _bias_ toward classification of the majority class in this unbalanced data set."},{"metadata":{"_uuid":"5008dc58914e47c376258500f82faeb52104103c"},"cell_type":"markdown","source":"#### Using ***balanced*** RandomForestClassifier approach:\n\n\nWeights are inversely proportional with the frequency of class observaton: $$w_j = \\frac{n_j}{k n_j}$$\n\nwhere $w_j$ is the weight to class $j$, $n$ is the number of observations, $n_j$  is the number of observations in class $j$, and $k$  is the total number of classes. In our case, that indicates that the minority class label (fraud) should be weighted higher. "},{"metadata":{"trusted":true,"_uuid":"608d967c5479dd14ef0400a7340b49ae1a81943d"},"cell_type":"code","source":"show(*confusion(RandomForestClassifier(random_state=0, n_jobs=-1, n_estimators=10, class_weight=\"balanced\").fit(X_train,y_train),X_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c6d374b48f3b1aa3a485249c02c1613d7c8f71b6"},"cell_type":"markdown","source":"This makes only a small difference, and again, the tradeoff between the false positive rate and the false negative rate is an arbitrary one.  We still have quite the  _bias_ toward classification of the majority class."},{"metadata":{"_uuid":"255482dc2547fab4d286fadcdacbc765ecd2733e"},"cell_type":"markdown","source":"#### Using balanced RandomForestClassifier in`balanced_subsample` mode.\nThis is the same as “balanced” except that weights are computed based on the bootstrap sample for every tree grown."},{"metadata":{"trusted":true,"_uuid":"d3dbb1697fc1b271891d717ae126de51f26de842"},"cell_type":"code","source":"show(*confusion(RandomForestClassifier(random_state=0, n_jobs=-1, n_estimators=10, class_weight=\"balanced_subsample\").fit(X_train,y_train),X_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c76130ef8cdefe3e0e5760e0ca7ab162c5ba4c50"},"cell_type":"markdown","source":"#### Custom weighting\n\nPositive samples are given exponentially increasingly higher weight $w_{pos}$ relative to negative samples which have constant but very small weights $w_{neg} = 10^{-4}$ . The intention is that we can find a better for the classifier with fewer false negatives at the cost of accepting a large (but not unreasonable) amount of false positives. \n\nThe weights $w_{neg}$ and  $w_{pos}$ become hyperparameters that we can optimize. For the sake of simplicity we evaluate these on the test set here, but we stress that in we should evalaute these hyperparameters using CV in training set alone to avoid leakage to test set.\n\nThe balance tips within a rather small numeric range of $\\frac{w_{pos}}{w_{neg}}$ so we search for $w_{pos}$ in exponential increments. A form of quisence search around the bend, might find a better tradeoff."},{"metadata":{"trusted":true,"_uuid":"a391deb87f497ab57abe166b12ea2d0bf478aab7"},"cell_type":"code","source":"w_neg = 10**-4\nw_pos_range = np.exp(np.arange(np.log(1), np.log(10**9)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"64a02cc618a3e9a177401b2950256e736d827e3d"},"cell_type":"code","source":"for w_pos in w_pos_range:\n    print(\"w_pos: \" + str(w_pos))\n    show(*confusion(RandomForestClassifier(random_state=0, n_jobs=-1, n_estimators=10, class_weight={0: w_neg, 1: w_pos}).fit(X_train,y_train),X_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3d1d08399b5a13e910442c8aea7712b0537aa303"},"cell_type":"markdown","source":"Note for instance, \n```\nw_pos: 59874.14171519782\nTN:84457 FP:839 FN:29 TP:118 FNR=0.19727891156462585 FPR=0.009836334646407803\n```\n\nHere we have less than 30 false negatives which is about 25% less than for an unweighted or even balanced weighted approach, but at the cost of having 8 times as many false positives. However,  in a setting where false negatives are way more expensive than false positives, this may be a more acceptable balance."},{"metadata":{"_uuid":"667507d4289f94f1b21bfaf5a16c2ec54f5f5ca9"},"cell_type":"markdown","source":"#### Conclusion\nIt is feasible to strike a balance between false negatives and false positives using a class weight hyper parameter. The optimal setting of this hyperparameter depends not only on the cost of false negatives and the cost of false positives, but also on the class imbalance. Similarly to the process of finding the expected cost association to class weights, it makes sense to bootstrap the data to estimate the variance around this expectation."},{"metadata":{"_cell_guid":"96c3a75c-0333-e22f-958e-036c0e0d337d","_uuid":"cc43c77cf0c311f83699afdeb5384d627bc9096d"},"cell_type":"markdown","source":"### References:\n\n- Good blog article on different methods handling class imbalance:\nhttps://www.svds.com/learning-imbalanced-classes/\n- A Survey of Predictive Modelling under Imbalanced Distributions: https://arxiv.org/pdf/1505.01658.pdf\n- Short blog post about scikit-learn RandomForestClassifier balanced mode: https://chrisalbon.com/machine_learning/trees_and_forests/handle_imbalanced_classes_in_random_forests/\n- The mechanism of doing the _weighted_ node-split in the tree in scikit learn: https://github.com/scikit-learn/scikit-learn/blob/70f170dedf2927c2d805144425522459d92700a7/sklearn/tree/_criterion.pyx#L635\n- Paper that introduces \"Weighted Random Forests\" as a method to deal with class imbalance: https://statistics.berkeley.edu/sites/default/files/tech-reports/666.pdf\n- This paper frames the concepts around \"cost sensitive learning\" , i.e., minizing cost rather than just misclassification loss: https://cling.csd.uwo.ca/papers/cost_sensitive.pdf"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}