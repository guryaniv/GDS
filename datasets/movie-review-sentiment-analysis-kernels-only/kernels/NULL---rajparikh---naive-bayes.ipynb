{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import pandas as pd\nframe1=pd.read_csv('../input/train.tsv',sep='\\t')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2904bc101f602dd11bde2e8ff61c1c28a0c444a7"},"cell_type":"code","source":"import nltk\nimport string\nimport os\n\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom nltk.stem.porter import PorterStemmer\n\npath = '/opt/datacourse/data/parts'\ntoken_dict = {}\nstemmer = PorterStemmer()\nwordnet_lemmatizer = WordNetLemmatizer()\n\ndef stem_tokens(tokens, stemmer):\n    stemmed = []\n    for item in tokens:\n        stemmed.append(wordnet_lemmatizer.lemmatize(item))\n    return stemmed\n\ndef tokenize(text):\n    tokens = nltk.word_tokenize(text)\n    stems = stem_tokens(tokens, stemmer)\n    return stems\ni=0\nfor line in frame1['Phrase']:\n    \n    lowers = line.lower()\n    no_punctuation = lowers.translate(str.maketrans('','',string.punctuation))\n    token_dict[i] = no_punctuation\n    i=i+1    \n#this can take some time\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a7375f76bc3c08869ac98161b74b32f22c533724"},"cell_type":"code","source":"senti=list(frame1['Sentiment'])\nvalues=list(token_dict.values())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"19c64d1bd0dafb825df46f218e1ae4291f8d6985"},"cell_type":"code","source":"frame2=pd.DataFrame({'Phrases':values,'Sentiment':senti})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d7abade40923009abccef6a116f8e069c2dc2720"},"cell_type":"code","source":"tfidf = TfidfVectorizer(tokenizer=tokenize, stop_words='english')\ntfs = tfidf.fit_transform(token_dict.values())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"084adf1e60e269c9a91bedb07fe2b03825d68d3a"},"cell_type":"code","source":"feature_names = tfidf.get_feature_names()\npx=[]\nb=dict()\nfor x in tfs:\n    a=[]\n    for col in x.nonzero()[1]:\n        if(x[0,col]>0.12):\n            #print(col)\n            #print (feature_names[col], ' - ', x[0, col])\n            a.append(feature_names[col])\n        else:\n            if feature_names[col] not in b:\n                b[feature_names[col]]=x[0,col]\n            b[feature_names[col]]=min(x[0,col],b[feature_names[col]])\n    px.append(a)\nb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"37efa64f404cace57b02d2646210157b74f14d50"},"cell_type":"code","source":"from nltk.corpus import stopwords \nstop_words = set(stopwords.words('english'))\n\nfor i in range(len(px)):\n    temp=px[i]\n    px[i]=[]\n    for w in temp:\n        if w not in stop_words:\n            px[i].append(w)\n\nfor i in range(len(px)):\n    px[i]=' '.join(px[i])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cba02cf836d45a0296f4cf75bedd26da5e7388a9"},"cell_type":"code","source":"frame3=pd.DataFrame(columns=['phrase','sentiment'])\nframe2=pd.DataFrame({'phrase':px,'sentiment':frame2['Sentiment']})\nk=0\n\nl1=len(frame2)\nfor i in range(l1):\n    if frame2.loc[i,'phrase']!='':\n        frame3.loc[k]=[str(frame2.loc[i,'phrase']),frame2.loc[i,'sentiment']]\n        k=k+1\n        print(k)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"96bae1e75249f491e275b1b8f51bc9e520f4a77e"},"cell_type":"code","source":"vocab_size=len(feature_names)   \nmax_words=196","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"14734c89e362ff50cb7666d58a4f6351312ba701"},"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\nt=Tokenizer()\nt.fit_on_texts(px)\ntrain_X=t.texts_to_sequences(px)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"65a8020e5e9369668048996875f3e8acdaa09bed"},"cell_type":"code","source":"from keras.preprocessing.sequence import pad_sequences\ntrain_X= np.array(pad_sequences(train_X,maxlen=max_words,padding='post'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fde9b1b44b0f8e582e2a550e99647d47d21b7a5b"},"cell_type":"code","source":"y_train=frame1['Sentiment']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"05cb3f67cf756e32ae9e1f0644aa1146819d9719"},"cell_type":"code","source":"import numpy as np\ntrain_X=np.array(train_X)\ny_train=np.array(y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"227cf2fc20cc2844043d42c73e37e3382788db1a"},"cell_type":"code","source":"from sklearn.utils import shuffle\nbatch_size=1000\ntrain_X=np.array(train_X)\ntrain_X,y_train=shuffle(train_X,y_train)\n#y_train=to_categorical(y_train)\nX_valid, y_valid = train_X[:batch_size], y_train[:batch_size]\nX_train2, y_train2 = train_X[batch_size:], y_train[batch_size:]\nprint(type(X_train2),type(y_train2),type(X_valid),type(y_valid))\n\n\nratio={0:12000,1:30000,2:79087,3:32927,4:15000}\nfrom imblearn.over_sampling import SMOTE \nsm = SMOTE(random_state=42,ratio=ratio)\ntrain_X,y_train = sm.fit_sample(X_train2,y_train2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f00256df8b2e1dce2ea4938c6f1709c277a5d035"},"cell_type":"code","source":"import numpy as np\n\nfrom sklearn.naive_bayes import MultinomialNB\nclf = MultinomialNB()\nclf.fit(train_X,y_train)\n\na=clf.predict(X_valid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0b84336ceba67a356f236dc7795900da7c0bb79e"},"cell_type":"code","source":"frame_test=pd.read_csv('../input/test.tsv',sep='\\t')\ni=0\ntoken_dict1={}\nfor line in frame_test['Phrase']:\n    \n    lowers = line.lower()\n    no_punctuation = lowers.translate(str.maketrans('','',string.punctuation))\n    token_dict1[i] = no_punctuation\n    i=i+1\nvalues=list(token_dict1.values())\n\nframe_test=pd.DataFrame({'phrase':values})\nlen(frame_test)\n\npx=values\npx1=[]\ntfs = tfidf.fit_transform(px)\nfor x in tfs:\n    a=[]\n    for col in x.nonzero()[1]:\n        a.append(feature_names[col])\n    px1.append(' '.join(a))\n\ntest_X=t.texts_to_sequences(px1)\ntest_X= np.array(pad_sequences(test_X,maxlen=max_words,padding='post'))\ntest_X\n\na=clf.predict(test_X)\n\nframe=pd.read_csv('../input/test.tsv',sep='\\t')\nframe_final=pd.DataFrame({'PhraseId':frame['PhraseId'],'Sentiment':a})\n\nframe_final.to_csv('../input/final_submission1.csv',index=False)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"baacef4f1314b4100d983450e898db4be627104c"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}