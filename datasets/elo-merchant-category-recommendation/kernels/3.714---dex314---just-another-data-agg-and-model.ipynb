{"cells":[{"metadata":{"_uuid":"ee3360b57a2b06b0ac8adb91358c13e41ebed194"},"cell_type":"raw","source":""},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\nimport gc\n\nimport datetime\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"The idea behind this notebook is to have a little fun using pandas data aggregation and light gbm.\n\nWe are predicting these **loyalty scores** so we will be focusing on RMSE.\n\nWe will move through each of the data csv's listed above and attempt to aggegate and merge them into one data set."},{"metadata":{"_uuid":"2e116120453c11d1fc6482f23d1bf6e6a7f393f7"},"cell_type":"markdown","source":"This memory reducer is from:\n- https://www.kaggle.com/ashishpatel26/lightgbm-gbdt-dart-baysian-ridge-reg-lb-3-61\n"},{"metadata":{"trusted":true,"_uuid":"bad2471d73d1f0f505677f79751331b2ed5cccbb"},"cell_type":"code","source":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"31781c8e0ab1eedced9a342a4da45c9f181af549"},"cell_type":"markdown","source":"## Bring in Train and Test and do a quick check..."},{"metadata":{"trusted":true,"_uuid":"afa420e8150d1731b8758b44997aebde28280905"},"cell_type":"code","source":"train = reduce_mem_usage(pd.read_csv('../input/train.csv'))\ntest = reduce_mem_usage(pd.read_csv('../input/test.csv'))\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f103fa3c821668525ff0ab22e778c517340f3162"},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b0936db8469df16b3933dee73815e1fbb6368225"},"cell_type":"markdown","source":"So our training data set has a target and for the purposes of a predicting index, we would be using the **CARD ID**."},{"metadata":{"trusted":true,"_uuid":"66a7b6f24ddbc3f9a47e39ef17eaa958b6f56665"},"cell_type":"code","source":"test.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c90856be086bd07ab883376432d294e0b53074d3"},"cell_type":"markdown","source":"## Bring in the merchant data, merge, clean up and aggergate...."},{"metadata":{"trusted":true,"_uuid":"6144203a0a91349d345dbe78988bc72bf5c62186"},"cell_type":"code","source":"nm_df = reduce_mem_usage(pd.read_csv('../input/new_merchant_transactions.csv'))\nprint(nm_df.shape)\nnm_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fdc14ee4214608701e21bbc389571aa283f4ed10"},"cell_type":"code","source":"nm_df['purchase_date'] = pd.to_datetime(nm_df.purchase_date)\n\nnm_df['purch_year'] = nm_df.purchase_date.dt.year\nnm_df['purch_mon'] = nm_df.purchase_date.dt.month\nnm_df['purch_dow'] = nm_df.purchase_date.dt.dayofweek\nnm_df['purch_wk'] = nm_df.purchase_date.dt.week\nnm_df['purch_day'] = nm_df.purchase_date.dt.dayofyear\n\n## adding in a few more features - 2.6.19\n## source: https://www.kaggle.com/chauhuynh/my-first-kernel-3-699\nnm_df['purch_woy'] = nm_df.purchase_date.dt.weekofyear\nnm_df['purch_wknd'] = (nm_df.purchase_date.dt.weekday >=5).astype(int)\nnm_df['purch_hr'] = nm_df.purchase_date.dt.hour\n#https://www.kaggle.com/c/elo-merchant-category-recommendation/discussion/73244\nnm_df['purch_month_diff'] = ((datetime.datetime.today() - nm_df.purchase_date).dt.days)//30\nnm_df['purch_month_diff'] += nm_df['month_lag']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"67250fabe3d8051e70395d5432ea4316e4bea11f"},"cell_type":"code","source":"nm_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8b0d4fffbe77f50a8f396d8b801407fa200d7182"},"cell_type":"code","source":"m_df = reduce_mem_usage(pd.read_csv('../input/merchants.csv'))\nprint(m_df.shape)\nm_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e3422eec8942628f276e64ea91c9309ec7bf4287"},"cell_type":"code","source":"nmm_df = nm_df.merge(m_df, on='merchant_id', how='outer')\nnmm_df.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1dc58a7c05f3c26ebe8c336f2b3d2468c2c9786c"},"cell_type":"markdown","source":"Check some data types and unique values..."},{"metadata":{"trusted":true,"_uuid":"ae3f5a5b86c274be5e29767d7d6e3f009f5240ce"},"cell_type":"code","source":"for c in nmm_df.columns:\n    if nmm_df[c].dtype == 'object':\n        vcs = nmm_df[c].value_counts()\n        if len(vcs) < 20: ## checking if theres a lot of unique values\n            print(vcs)\n#     print(c,'->',nmm_df[c].dtype)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"590d9d038da4760024c0f78b65a30cccf1a0568a"},"cell_type":"markdown","source":"Clean up...."},{"metadata":{"trusted":true,"_uuid":"f173a8b44c39ad6bcb1e61e3baeb88a159d3927f"},"cell_type":"code","source":"nmm_df['authorized_flag'] = (nmm_df.authorized_flag == 'Y').astype(int)\nnmm_df['category_1_x'] = (nmm_df.category_1_x == 'Y').astype(int)\nnmm_df['purchase_date'] = pd.to_datetime(nmm_df.purchase_date)\nnmm_df['category_1_y'] = (nmm_df.category_1_y == 'Y').astype(int)\nnmm_df['category_4'] = (nmm_df.category_4 == 'Y').astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d4ea2edcd3b291eaa4f8d91663c3f40ce16c7030"},"cell_type":"code","source":"mrpr = pd.get_dummies(nmm_df.most_recent_purchases_range)\nmrsr = pd.get_dummies(nmm_df.most_recent_sales_range)\ncat3 = pd.get_dummies(nmm_df.category_3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0472f3679ce66112a7a53ab5a7df470ce6713f0f"},"cell_type":"code","source":"for d in [mrpr, mrsr, cat3]:\n    for c in d.columns:\n        nmm_df[c] = d[c]\n        \nnmm_df.drop(['most_recent_purchases_range','most_recent_sales_range','category_3'], axis=1, inplace=True)\n\ntry:\n    del mrpr, mrsr, cat3, nm_df, m_df\nexcept:\n    pass\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9ff6f96912318192b9b4e14d96f876a39c2363ea"},"cell_type":"code","source":"nmm_df.isnull().sum()/len(nmm_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"39808ea6b375a7f22356ba8402497c94730d4203"},"cell_type":"code","source":"## fill NA with most common value\nfor c in nmm_df.columns:\n    if nmm_df[c].isnull().sum()/len(nmm_df[c]) > 0:\n        nmm_df[c] = nmm_df[c].fillna(nmm_df[c].value_counts().index[0])\n        \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1876de2c0b168a1b74dcd3ff6ee23d04ee02c265"},"cell_type":"code","source":"# nmm_df.isnull().sum()/len(nmm_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ed078443f0f59f3c58bbad7b0f581dbc22f053d9"},"cell_type":"code","source":"## factorize merchant id\n\nnmm_df['merchant_id'] = pd.factorize(nmm_df['merchant_id'])[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"035e1b0aa3cb2830baac6d317893cedfa7d118f0"},"cell_type":"code","source":"nmm_agg = nmm_df.groupby('card_id').agg(['sum','mean','max','min','var',\n                                        'median','count','skew','nunique',#'mode'\n                                       ])\nprint(nmm_agg.shape)\n\ntry:\n    del nmm_df\nexcept:\n    pass\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"927608b758612cb7ac58d7b824ee0bfa37c75560"},"cell_type":"markdown","source":"## Bring in the Historical data, clean up and aggregate...."},{"metadata":{"trusted":true,"_uuid":"4ce54d0b41dbce5776534481d3b1b792329b82d0"},"cell_type":"code","source":"hist = reduce_mem_usage(pd.read_csv('../input/historical_transactions.csv'))\nhist.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cb2b31f92fa0226b2ea4bbaedd21626de753a369"},"cell_type":"markdown","source":"Clean up..."},{"metadata":{"trusted":true,"_uuid":"f0f7f321f6294605a95afa2801e159ba90dff44d"},"cell_type":"code","source":"hist['authorized_flag'] = (hist.authorized_flag == 'Y').astype(int)\nhist['category_1'] = (hist.category_1 == 'Y').astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0af58613d82d0ac657413e1ef3e3b78ba0ccc342"},"cell_type":"code","source":"cat3 = pd.get_dummies(hist.category_3)\n\nfor c in cat3.columns:\n    hist[c] = cat3[c]\nhist.drop('category_3',axis=1, inplace=True)\n\ndel cat3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e5afd7a445f0f7c319f05ba74e1b4ecf7cb8fd83"},"cell_type":"code","source":"hist['purchase_date'] = pd.to_datetime(hist.purchase_date)\n\nhist['histpurch_year'] = hist.purchase_date.dt.year\nhist['histpurch_mon'] = hist.purchase_date.dt.month\nhist['histpurch_dow'] = hist.purchase_date.dt.dayofweek\nhist['histpurch_wk'] = hist.purchase_date.dt.week\nhist['histpurch_day'] = hist.purchase_date.dt.dayofyear\n\n## adding in a few more features - 2.6.19\n## source: https://www.kaggle.com/chauhuynh/my-first-kernel-3-699\nhist['histpurch_woy'] = hist.purchase_date.dt.weekofyear\nhist['histpurch_wknd'] = (hist.purchase_date.dt.weekday >=5).astype(int)\nhist['histpurch_hr'] = hist.purchase_date.dt.hour\n#https://www.kaggle.com/c/elo-merchant-category-recommendation/discussion/73244\nhist['histpurch_month_diff'] = ((datetime.datetime.today() - hist.purchase_date).dt.days)//30\nhist['histpurch_month_diff'] += hist['month_lag']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4ef7944530627057e5fbe3a8104bd14a285131ba"},"cell_type":"code","source":"print(hist.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d229a5dba10c95ca45d0835ea3aa79fb3b897783"},"cell_type":"code","source":"hist.isnull().sum()/len(hist)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"90f9d28a4332a50d0aa060c2304a4b885c3334f7"},"cell_type":"code","source":"## fill NA with most common value\nfor c in hist.columns:\n    if hist[c].isnull().sum()/len(hist[c]) > 0:\n        hist[c] = hist[c].fillna(hist[c].value_counts().index[0])\n        \n\n#factorize merchant id\nhist['merchant_id'] = pd.factorize(hist['merchant_id'])[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2757c0c16f47a9eb73617084bc14d9f534662c8b"},"cell_type":"code","source":"hist_agg = hist.groupby('card_id').agg(['sum','mean','max','min','var',\n                                        'median','count','skew','nunique',#'mode'\n                                       ])\nprint(hist_agg.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"549bc54ec2fa971ccf35712a88e8b542aa31e560"},"cell_type":"code","source":"try:\n    del hist\nexcept:\n    pass\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"adbb1c3e7040b252c1caf3a1dd8c85d957498aeb"},"cell_type":"markdown","source":"## Merge the train and test dataframes and aggergated mercant and historical data together..."},{"metadata":{"trusted":true,"_uuid":"f67358dc9a62ad3bded5d584a2bc90d12a3ae179"},"cell_type":"code","source":"train_agg = train.set_index('card_id').join(hist_agg, how='left').join(nmm_agg, how='left', rsuffix='_nmm').fillna(0)\nprint(train_agg.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"32d05a1c78e9c511ca13ece76155e078cac6cfa5"},"cell_type":"code","source":"test_agg = test.set_index('card_id').join(hist_agg, how='left').join(nmm_agg, how='left', rsuffix='_nmm').fillna(0)\nprint(test_agg.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"202802e7ce495d5a35f6781f2c02d2c7eb1f282c"},"cell_type":"markdown","source":"## Clean up the column names some...."},{"metadata":{"trusted":true,"_uuid":"c6c97734866a97e03988c010dfe416341b513aa6"},"cell_type":"code","source":"train_agg.columns = [''.join(col).strip() for col in train_agg.columns.values]\ntest_agg.columns = [''.join(col).strip() for col in test_agg.columns.values]\ntrain_agg.columns = train_agg.columns.str.replace(' ','')\ntest_agg.columns = test_agg.columns.str.replace(' ','')\ntrain_agg.head(3)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f903ef9d05d788fc5ba36a6bba19f3be69c7aa40"},"cell_type":"markdown","source":"## A little more memory clean up and some other data clean up...."},{"metadata":{"trusted":true,"_uuid":"5bbe9eed5b1f9792a1c5bc9aaae078786c6fa302"},"cell_type":"code","source":"# train_agg = train_agg.reset_index()\ntry:\n    del train, test\nexcept:\n    pass\n\ngc.collect()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"10747e49a05c81e2cb67d51d95b4f6c1ea12a273"},"cell_type":"code","source":"train_agg['month'] = train_agg['first_active_month'].str[-2:]\ntrain_agg['year'] = train_agg['first_active_month'].str[:-3]\ntrain_agg.drop(['first_active_month'],axis=1,inplace=True)\ntrain_agg['month'] = train_agg.month.astype(int)\ntrain_agg['year'] = train_agg.year.astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"881911b9946213a882f730bfa0e1ea91e97d328e"},"cell_type":"code","source":"test_agg['month'] = test_agg['first_active_month'].str[-2:]\ntest_agg['year'] = test_agg['first_active_month'].str[:-3]\ntest_agg.drop(['first_active_month'],axis=1,inplace=True)\ntest_agg['month'] = test_agg.month.fillna(0).astype(int)\ntest_agg['year'] = test_agg.year.fillna(0).astype(int)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a53729514a4f7748104a2bed5f05cc9050132a28"},"cell_type":"markdown","source":"__Remove Infs?__"},{"metadata":{"trusted":true,"_uuid":"243a34979739003229573f3dc58cde9c90e71a6e"},"cell_type":"code","source":"for c in train_agg.columns:\n    if np.isinf(train_agg[c]).sum()/len(train_agg[c]) > 0:\n        print(c)\n        train_agg[c] = train_agg[c].replace([np.inf, -np.inf], train_agg[c].value_counts().index[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"429c618295f5968dec9c61fa14f5d35989a830a9"},"cell_type":"code","source":"for c in test_agg.columns:\n    if np.isinf(test_agg[c]).sum()/len(test_agg[c]) > 0:\n        print(c)\n        test_agg[c] = test_agg[c].replace([np.inf, -np.inf], test_agg[c].value_counts().index[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3941fb189b347693e38bce267710c79eab7d8345"},"cell_type":"code","source":"for c in train_agg.columns:\n    if train_agg[c].isnull().sum()/len(train_agg[c]) > 0:\n        print(c)\n        train_agg[c] = train_agg[c].fillna(train_agg[c].value_counts().index[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6802f33d918626268a53b8630453f5482a8d7b12"},"cell_type":"code","source":"for c in test_agg.columns:\n    if test_agg[c].isnull().sum()/len(test_agg[c]) > 0:\n        print(c)\n        test_agg[c] = test_agg[c].fillna(test_agg[c].value_counts().index[0])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7dc1826a905d4422a6c40b37f837faaf561f42f2"},"cell_type":"markdown","source":"## Light GBM model with a Linear Estimator for the outliers?"},{"metadata":{"trusted":true,"_uuid":"ae4e57c9878587a5eba0eb948c93c027e3464291"},"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold, RepeatedKFold\nimport lightgbm as lgb\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.linear_model import HuberRegressor, LassoLars, RANSACRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"485dbed740aa26d1589d171dd338bf5d97cd1d9e"},"cell_type":"code","source":"## trying out various methods to grrab the outliers\n## The Huber Regressor errored out. Lars didnt improve anything nor did RANSAC.\n## Also trying a gradient boosting decision stump with huber loss\n##   GBT stumps tend to fit to outliers more as well\n##      On v18, we will do a little bigger than a stump w maxdepth of 2\n\n## Nothing seemed to make improvements. See comments below.\n\n# llars = LassoLars(alpha=0.9, fit_intercept=True, normalize=True, \n#                   precompute=\"auto\", max_iter=500)\n\n# rr = RANSACRegressor(#llars,\n#                      random_state=123)\n\n# gbr = GradientBoostingRegressor(loss='huber',\n#                                 learning_rate=0.01, #0.02,\n#                                 n_estimators=1000, #500,\n#                                 subsample=0.8, \n#                                 max_depth=2, #1, \n#                                 random_state=123, max_features=None, alpha=0.9,\n#                                 validation_fraction=0.1)\n\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b095de5a27d0c9e37accd3febc85ee05752fb59d"},"cell_type":"code","source":"# del x0, y, trn_data, val_data\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"460bfdfb2687c7354b4f862e4bc4b153d0bb31d0"},"cell_type":"code","source":"folds = StratifiedKFold(n_splits = 5, shuffle = True)\ntrain_predictions = np.zeros(len(train_agg))\ntest_predictions = np.zeros(len(test_agg))\nlin_preds = np.zeros(len(test_agg))\nn_fold = 0\nfor train_index, test_index in folds.split(train_agg, train_agg['feature_1']):\n    ## from https://www.kaggle.com/mfjwr1/simple-lightgbm-without-blending\n    n_fold += 1\n    param ={\n        'task': 'train',\n        'boosting': 'goss',\n        'objective': 'regression',\n        'metric': 'rmse',\n        'learning_rate': 0.01,\n        'subsample': 0.9855232997390695,\n        'max_depth': 7,\n        'top_rate': 0.9064148448434349,\n        'num_leaves': 63,\n        'min_child_weight': 41.9612869171337,\n        'other_rate': 0.0721768246018207,\n        'reg_alpha': 9.677537745007898,\n        'colsample_bytree': 0.5665320670155495,\n        'min_split_gain': 9.820197773625843,\n        'reg_lambda': 8.2532317400459,\n        'min_data_in_leaf': 21,\n        'verbose': -1,\n        'seed':int(2**n_fold),\n        'bagging_seed':int(2**n_fold),\n        'drop_seed':int(2**n_fold)\n        }\n    \n    y = train_agg['target']\n    x0 = train_agg.drop('target', axis = 1)\n    \n    trn_data = lgb.Dataset(x0.iloc[train_index], label=y.iloc[train_index])\n    val_data = lgb.Dataset(x0.iloc[test_index], label=y.iloc[test_index])\n    \n    num_round = 10000\n    clf = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=-1, early_stopping_rounds = 200)\n    train_predictions[test_index] = clf.predict(x0.iloc[test_index], num_iteration=clf.best_iteration)\n\n    test_predictions += clf.predict(test_agg, num_iteration=clf.best_iteration) / folds.n_splits\n    \n##     llars.fit(x0.iloc[train_index], y.iloc[train_index])\n##     rr.fit(x0.iloc[train_index], y.iloc[train_index])\n#     gbr.fit(x0.iloc[train_index], y.iloc[train_index]) ## Decision Stump on Huber to Grab Outliers?\n#     lin_preds += gbr.predict(test_agg)/folds.n_splits\n\nprint(\"LGB Train Error:\",np.sqrt(mean_squared_error(train_predictions, y)))\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0ebbd6617a1890d04d9eea515bb405fb721d3f82"},"cell_type":"code","source":"# pd.concat([pd.DataFrame(test_predictions[:10], columns=['lgb']),\n#            pd.DataFrame(lin_preds[:10], columns=['lin'])], axis=1, join='outer')\n\nprint(test_predictions[:10])\n# print('/n')\n# print(lin_preds[:10])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6b494ccb3639c89978c788a8be4efc2eae7b19c8"},"cell_type":"code","source":"# combined_preds = 0.9*test_predictions + 0.1*lin_preds\n# combined_preds","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2ca5b457c25bf5572666c2e528bae8ad9ac8b362"},"cell_type":"markdown","source":"__Update 2.7.19__\n\n__I kept trying to improve the model by using some method to grab the outliers but nothing seemed to do better than Light GBM by iteself.__"},{"metadata":{"trusted":true,"_uuid":"b7b8e3ca3f04dd44f7ff1493282a7736da00ee8f"},"cell_type":"code","source":"predictions = pd.DataFrame(\n    data = {\n        'card_id' : test_agg.index, #test_agg['card_id'],\n        ## I kept trying to improve the model by using a linear method to grab the outliers\n        'target' : test_predictions, #combined_preds, \n    }\n)\npredictions.to_csv('submit.csv', index = False) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5bfb35b546139252ced485fd3af6ac784109486c"},"cell_type":"code","source":"predictions.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fcd54305f91421751f01fdb726cbfdfc3273d7ea"},"cell_type":"code","source":"len(predictions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c66b2d49a1bec45b2426affd9c6d5db4fd38f221"},"cell_type":"code","source":"import matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dd108a637546ba7717cd29ded40a8ebf72932fee"},"cell_type":"code","source":"fig,ax=plt.subplots(1,1,figsize=(14,12))\nlgb.plot_importance(clf, max_num_features=50, ax=ax);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"beac28038f34c894e3eea238f41e7b2f0b5a1731"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"240f6d91e3551ffdbb52f35e596c8e5f50983401"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}