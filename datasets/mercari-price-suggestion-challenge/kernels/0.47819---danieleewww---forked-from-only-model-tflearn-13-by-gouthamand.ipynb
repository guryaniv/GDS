{"cells":[{"metadata":{"_cell_guid":"a1de6fa3-e1cb-48f1-9273-743bbceae0d9","_uuid":"bf32162bae739b0324bf462b8be650b7ba1b3e39","collapsed":true,"trusted":true},"cell_type":"code","source":"# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# This notebook was copied from https://www.kaggle.com/gouthamand/only-model-tflearn-13\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4416fe3914a212f2e767f976091a4bc722c65b71"},"cell_type":"code","source":"import pandas as pd \nimport numpy as np\nimport re\n    \nimport sklearn\nfrom sklearn.model_selection import train_test_split \nfrom sklearn.preprocessing import LabelEncoder\nfrom scipy.sparse import csr_matrix\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nimport nltk \nfrom nltk.corpus import stopwords\nfrom gensim.models import word2vec\nimport collections \nimport logging\n\nimport tensorflow as tf\nimport tflearn\nfrom tflearn.layers.core import input_data, dropout, fully_connected, flatten\nfrom tflearn.layers.embedding_ops import embedding \nfrom tflearn.layers.merge_ops import merge\nfrom tflearn.data_utils import pad_sequences\nfrom tflearn.models.dnn import DNN\nfrom tflearn.layers.estimator import regression\nimport tflearn.summaries as summary\n\nimport gc\nimport csv\nimport string \nimport time\nimport pickle\n\nprint ( 'Finished importing....')\nmodeling_start_time = time.time()\nprint ('******Modeling start time******:' , \n       (time.strftime(\"%Hhrs:%Mm:%Ss\", time.gmtime((modeling_start_time)))) )\nprint ( 'Loading data....')\nprint ( 'Setting opition pd.set_option display.max_colwidth to avoid truncation of columns....')\npd.set_option('display.max_colwidth', -1)\n# Read the file into dataframe\n#origtrain = pd.read_csv('../input/train.tsv', sep='\\t', encoding='utf-8' )\n#origtest  = pd.read_csv('../input/test.tsv',  sep='\\t', encoding='utf-8' )\norigtrain = pd.read_table( '../input/train.tsv',\n                        converters = {'brand_name': np.str , \n                               'category_name':  np.str,\n                               'item_condition_id': np.uint8,\n                               'item_description':  np.str,\n                               'name': np.str,\n                               'price': np.float64,\n                               'shipping': np.uint8,\n                               'train_id' :  np.uint64\n                               })\norigtest = pd.read_table('../input/test.tsv',\n                        converters = {'brand_name': np.str , \n                               'category_name':  np.str,\n                               'item_condition_id': np.uint8,\n                               'item_description':  np.str,\n                               'name': np.str,\n                               'price': np.float64,\n                               'shipping': np.uint8,\n                               'train_id' :  np.uint64\n                               })\n                                           \n                        \n# The rows in train and test data \nprint (\"   The number of rows in train\", origtrain.shape)\nprint (\"   The number of rows in test\", origtest.shape)\n\n# See if there are any price columns with zero value\nprint (\"   Number of rows with price zero : \" ,origtrain.price[origtrain.price == 0.0].count())\nprint (\"   Number of rows :  \", origtrain.shape[0])\n#df= df[df[\"score\"] > 50]\norigtrain = origtrain[origtrain.price > 0.0]\nnrow_train = origtrain.shape[0]\nprint (\"   Number of rows aferdrop :  \", nrow_train)\ntest_id = origtest.test_id.values \n\n\n# Move the data into working dataframe  \ncombdata = pd.concat([origtrain , origtest],0 )           \ncombdata = combdata.reindex()\n#prepare for ntlk\n#porter = PorterStemmer()\nstop_words = set(stopwords.words('english'))\nprint ( 'Finished loading data....')\n\nprint\ndel origtrain\ndel origtest\ngc.collect()\n#review the memory used by the dataset \n#print('Memory review :',combdata.info(memory_usage= 'deep' )) \n#!free -h\n\n\n#review the memory used by the dataset \n#print('Memory review :',combdata.info(memory_usage= 'deep' )) \n#!free -h\n","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"e2010689c4c2f8214443eb18d55854f7711f7d49"},"cell_type":"code","source":"def removeSpecial (text):\n    text = re.sub('\\W+',' ', text.lower())\n    return(text)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"f82a12b2dfb264cc483ddb67739619acb6291143"},"cell_type":"code","source":"# Preprocssing of the of text columns \n# replace null with a '0000 aaaa'\n# remove special character from the columns\n# create  for new columns for each of the text columns \n# they are newName/description/newBrand/newCategory \n\nstart_time = time.time()\nprint ('Starting pre-processing of text columns at start time is:' , \n       (time.strftime(\"%Hhrs:%Mm:%Ss\", time.gmtime((start_time)))) )\n\n#brand name\nprint ('   pre-processing brand name...')\ncombdata.brand_name = combdata.brand_name.apply(removeSpecial) \ncombdata.loc[combdata['brand_name'].values == '', 'brand_name'] = '0000aaaa'\ncombdata['newBrand'] = combdata['brand_name'].str.strip()\n\n#Price\nprint ('   pre-processing price...')\ncombdata.price = combdata.price.fillna(0) \ncombdata.price = combdata.price.astype('float16') \n\n#shipping\nprint ('   pre-processing shipping...')\ncombdata.shipping =  combdata.shipping.astype('uint8')\n\n#item condition\nprint ('   pre-processing item condition...')\ncombdata.item_condition_id =  combdata.item_condition_id.astype('uint8')\n\n#category name\nprint ('   pre-processing category name...')\ncombdata.loc[combdata['category_name'].values == '', 'category_name'] = '0000aaaa'\ncombdata['newCategory'] =  combdata.category_name.apply(removeSpecial)\n\n#test_id\nprint ('   pre-processing test_id...')\ncombdata.test_id = combdata.test_id.fillna(9999999) \ncombdata.test_id = combdata.test_id.astype('uint32') \n\n#train_id\nprint ('   pre-processing train id...')\ncombdata.train_id = combdata.train_id.fillna(9999999) \ncombdata.train_id = combdata.train_id.astype('uint32') \n\n#name \nprint ( '   filling the null with 0000aaaa...')\ncombdata.loc[combdata['name'].values == '', 'name'] = '0000aaaa'\ncombdata.loc[combdata['item_description'].values == '', 'item_description'] = '0000aaaa'\n\nprint ( '   creating new columns newName and description... ')\ncombdata['newName'] =  combdata.name.values\ncombdata['description'] =  combdata.item_description.values\n\nprint ( '   removing special character from newName... ')\ncombdata['newName'] =  combdata.newName.apply(removeSpecial)\n\nprint ( '   removing special character from description... ')\ncombdata['description'] =  combdata.description.apply(removeSpecial)\n\n#combdata.reindex()\n\n# Arrange the data sorted by brand name\n#print ( '   Rearraging the dataframe by brand name...')\n#combdata = combdata.sort_values(by = ['brand_name'])     \n#combdata = combdata.sort_values(by = ['newBrand'])     \n#combdata.reindex()\n#print ( '   Rearraged the dataframe by brand name...')\n\nend_time = end_time = time.time()\nprint ('Completed pre-processing of text columns and time taken:', \n       (time.strftime(\"%Hhrs:%Mm:%Ss\",( time.gmtime((end_time - start_time))))))\ngc.collect()\n#review the memory used by the dataset \n#print('Memory review :',combdata.info(memory_usage= 'deep' )) \n#!free -h ","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"3a0afef610f5b30329615dee4ec44208e7c0918c"},"cell_type":"code","source":"# Creating a raw text for the vocab using newNew and description columns\n# this would enable to build a vocab\n# sentences = [['first', 'sentence'], ['second', 'sentence']]\nstart_time = time.time()\nprint ('Building sentneces start time is:' , \n       (time.strftime(\"%Hhrs:%Mm:%Ss\", time.gmtime((start_time)))) )\nsentences = []\nmask =    (combdata['description'].values != 'no description yet') & (combdata['description'].values != '0000 aaaa')\ncmask = (combdata['newCategory'].values != '0000aaaa')\nbmask =    (combdata['newBrand'].values != '0000aaaa')\nvtext = [ [x] for x in combdata.description[mask].values]\nvtext.extend([ [x] for x in combdata.newCategory[cmask].values])\nvtext.extend([ [x] for x in combdata.newBrand[bmask].values ])\nvtext.extend([ [x] for x in combdata.newName.values ])\nfor sent in vtext:\n    for words in sent :\n        w  = [ word  for word in words.split(' ') if ((len(word) > 1)  & ( word not in stop_words))  ]\n        sentences.append(w)\n\n#raw_text = [[ word for word in words.split(' ')] for words in vtext] \nprint (' The raw_text type : %s and no of words : %d \\n sample : %s'\n             %(type(sentences), len(sentences) , sentences[0:1]) )\n\nend_time = time.time()\nprint ('Completed building sentneces and time taken::', \n       (time.strftime(\"%Hhrs:%Mm:%Ss\",( time.gmtime((end_time - start_time))))))\ndel vtext\ngc.collect()\n#!free -h","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"5fa5bf489a1d1e9da2326eb6f4cfe5b678fa7ce3"},"cell_type":"code","source":"#word2vec.Word2Vec\n#Three such matrices are held in RAM (work is underway to reduce that number to two, or even one). \n#So if your input contains 100,000 unique words, and you asked for layer size=200, \n#the model will require approx. 100,000*200*4*3 bytes = ~229MB.\n\n#logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n\nnoOfOccurance = 10 \nwordVecSize = 200\nwindowSize = 5 \nnoOfWorkers = 8 \nvocab_size = len(sentences)\n                 \nstart_time = time.time()\nprint ('Building the Word2Vec model at start time is:' , \n       (time.strftime(\"%Hhrs:%Mm:%Ss\", time.gmtime((start_time)))) )\n\nword2vecModel = word2vec.Word2Vec(sentences, size=wordVecSize, window=windowSize,\n                                  min_count=noOfOccurance, workers=noOfWorkers)\n# trim unneeded model memory = use (much) less RAM\nword2vecModel.init_sims(replace=True)\n\n#print( '  Saving the model')\n#word2vecModel.save('data/word2vecModel')\n\ngc.collect()\nend_time = time.time()\nprint ('Completed building word2vec word2vecModel and time taken::', \n       (time.strftime(\"%Hhrs:%Mm:%Ss\",( time.gmtime((end_time - start_time))))))","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"142e2e7d4f4f9c0f77052fabe52ae8c2cc1667ca"},"cell_type":"code","source":"# We will create a tfid scores matrix for every word in vocab\n# this will a score for individual words\n# we will us this score along with word2vec matrix to get a average score for each word \n# in the description and name columns\n\nstart_time = time.time()\nprint ('Building tfidf score matrix for the vocab and and start time is:' , \n       (time.strftime(\"%Hhrs:%Mm:%Ss\", time.gmtime((start_time)))) )\n\nnoOfocurance = 10 \nvectorizer = TfidfVectorizer(analyzer=lambda x: x, min_df=noOfocurance)\nmatrix = vectorizer.fit_transform([x  for x in sentences])\ntfidf = dict(zip(vectorizer.get_feature_names(), vectorizer.idf_))\nprint ('   vocab size(%d) ...' %( len(tfidf)))\n\n\n# Store data (serialize)\n#with open('data/sentences.pickle', 'wb') as fp:\n#    pickle.dump(sentences, fp, protocol=pickle.HIGHEST_PROTOCOL)\n\n# Load data (deserialize)\n#with open('data/sentences.pickle', 'rb') as fp:\n#    unserialized_data = pickle.load(fp)\n\n    \nend_time = time.time()\nprint ('Completed tfidf score matrix for the vocab and time taken::', \n       (time.strftime(\"%Hhrs:%Mm:%Ss\",( time.gmtime((end_time - start_time))))))","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"2db9ae7d09697880b2485fcdb523889b341d5af4"},"cell_type":"code","source":"# We build a two dictionary \n# a > brandCounts : unique brand names and there count\n# b > brandNames : brand name as key and value use that to search the name column\n# c > brandMultiNames : multi names  (multi-gram) brand names as key and value use that to search the name column\n# d > strbrandNames : brand names as string use that to search the name column\n# e > strbrandMultiNames : multi names  (multi-gram) brand names as string use use that to search the name column\n#\nstart_time = time.time()\nprint ('Building dictionary to replace null brand and start time is:' , \n       (time.strftime(\"%Hhrs:%Mm:%Ss\", time.gmtime((start_time)))) )\n\nnoOfocurance = 50 \nbrandCounts = dict()\nbrandNames = dict()\nbrandMultiNames = dict()\nbrandCount = collections.Counter(combdata[\"newBrand\"].values).most_common()\n\nfor  brand,  count in brandCount:\n    if ((count >= noOfocurance) &  (len(brand) > 1)):\n        brandCounts[brand]=  count\n    \n#brandNames = dict(zip(brandCounts.keys(),brandCounts.keys()))\nbrandNames['ipad'] = 'apple'\nbrandNames ['iphone'] ='apple'\nbrandNames['galaxy'] = 'samsung'\n\nfor value in  brandCounts.keys():\n    if (len(value.split(' ')) > 1):\n        brandMultiNames[value] = value\n    else:\n        brandNames[value] = value\nbrandNames.pop('0000aaaa')\n       \nstrbrandNames = ' '.join(brandNames.keys())\nstrbrandMultiNames = ' '.join(brandMultiNames.keys())\n\nend_time = time.time()\nprint ('Completed  dictionary to replace null brand and time taken:', \n       (time.strftime(\"%Hhrs:%Mm:%Ss\",( time.gmtime((end_time - start_time))))))\n\n#print('UNI: =============================================================')\n#for value in  brandNames.values(): \n#    print('UNI:(%d) %s %d' % (len(brandNames),value, len(value.split(' '))))\n#print('MULTI: =============================================================')\n#for value in  brandMultiNames.values(): \n#    print('MULTI:(%d) %s %d' % (len(brandMultiNames),value, len(value.split(' '))))\n#print('UNI: The type (%s) and value:\\n %s' %(type(strbrandMultiNames), strbrandMultiNames))","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"c266ecfbf92271918cdaa793085430ce6b062b09"},"cell_type":"code","source":"# searching for the brand in name \n# Use the brand dic's to find the brand name in name, return appropriate brand name \n# if brand name (partial word)exists in name, then return apporiate brand name from the dict\n# if the brand name happens to be multigram, then return appropriate multi gram brand name from the dict\n\n# Please note that we are using two string nameString & descString. These are newName and description\n# column's text. These correspondse to a row where brand is '0000aaaa'\n\ndef getBrand(text):\n    global globvar\n    global nameString\n    global nameDesc \n    quit = False\n\n    if (nameDesc):\n        nameText = nameString[globvar]\n    else:\n        nameText = descString[globvar]\n        \n    ### since we are using apply function on Brand text passed is always'0000aaaa'. We use\n    #### *****globvar*** as variable that iterates through nameString & descString for corresponding \n    ###  to Brand column value in search of brand value.\n    ### This variable keeps brand in-line with corresponding newName and description column values\n    \n    globvar += 1  \n        \n    words = nameText.split()\n    #print('getBrand : ' ,text, words)\n    rtext ='0000aaaa'\n    for word in words:\n        brandFound =  brandNames.get(word, 'NO')\n        if brandFound != 'NO':\n            rtext = brandFound\n            quit = True\n            break\n            \n    if not quit:\n        for brand in brandMultiNames.keys():\n            result = ''\n            result = re.search('\\\\b'+brand+'\\\\b', nameText)\n            if (result):\n                rtext = brand\n                quit = True\n                break\n\n    #if ((globvar%100000) == 0 ):\n       # print ('   Are we using nameString : ', nameDesc)\n       # print ('   Iteration : ', globvar)\n        #print ('type (%s) and name string (%s) : '%(type(nameText),nameText))\n        #print()        \n        #print('The brand returned:' , rtext)        \n        #print()        \n        \n    return (rtext)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"970a03bb8ebbec42232563d0d748adfffa303044"},"cell_type":"code","source":"# Search name and description fileds for brand name \n# first we will search the name column and then we wiil go for the \n# description. \nstart_time = time.time()\nprint ('Starting replacing null brands and start time is:' ,  \n       (time.strftime(\"%Hhrs:%Mm:%Ss\",( time.gmtime((start_time))))))\nglobvar = 0\nnameString = []\ndescString = []\nnameDesc = True\n\nmask =    (combdata['newBrand'].values == '0000aaaa') \nnoOfNull =  combdata.newBrand[mask].values\nnameString =  combdata.newName[mask].values\n\nprint ('   The count of brands with 0000aaaa are %d Before searching through newName'  %(len(noOfNull))  )\n\ncombdata['Brand'] = combdata['newBrand'].apply(lambda x: x if x != '0000aaaa' else getBrand(x) )\nmask = combdata['Brand'].values == '0000aaaa'\nnoOfNull = combdata.newBrand[mask]\nprint ('   The count of brands with 0000aaaa are %d After searching through newName'  %(len(noOfNull))  )\n\n# Now serach the scription column\nglobvar = 0\ndescString = []\nnameDesc = False\n\nmask =  ((combdata['Brand'].values == '0000aaaa') )\nbtext =  combdata.Brand[mask].values\ndescString =  combdata.description[mask].values\nprint ('   The count of brands with 0000aaaa are %d Before searching through description'  %(len(btext))  )\ncombdata['Brand'] = combdata['Brand'].apply(lambda x: x if x != '0000aaaa' else getBrand(x) )\nmask = combdata['Brand'].values == '0000aaaa'\nx = combdata.newBrand[mask]\nprint ('   The count of brands with 0000aaaa are %d After searching through description'  %(len(x))  )\n\ndel nameString\ndel descString \ndel btext\ndel mask\ngc.collect()\nend_time = time.time()\nprint ('Completed replacing null brands and time taken to process:', \n       (time.strftime(\"%Hhrs:%Mm:%Ss\",( time.gmtime((end_time - start_time))))))\n","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"3d070cb0627e380d783bfd38981a565f4585bed4"},"cell_type":"code","source":"# creating two new features lenght for newName and descritpion columns\n# a > len_description: length of description  \n# b > len_name: length of newName column\n\nstart_time = time.time()\nprint ('Creating new feature len_description and len_name and start time is:' ,  \n       (time.strftime(\"%Hhrs:%Mm:%Ss\",( time.gmtime((start_time))))))\nprint('   Creating new feature len_description for column description...')\ntry:\n    combdata['len_description'] = combdata['description'].map(len)\nexcept TypeError  as e:\n    print(e)\nx = np.max(combdata['len_description'])\n\nprint('   Creating new feature len_name for column newName...')\ntry:\n    combdata['len_name'] = combdata['newName'].map(len)\nexcept TypeError  as e:\n    print(e)\ny = np.max(combdata['len_name'])\nprint('   max length of column newName is (%d) and column description is (%d)e...' %(y,x))\n\nend_time = end_time = time.time()\nprint ('Completed creating new feature len_description and len_name and time taken::', \n       (time.strftime(\"%Hhrs:%Mm:%Ss\",( time.gmtime((end_time - start_time))))))","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"d27237d21f783233043bed1a6ac23d019c7ea0aa"},"cell_type":"code","source":"# Creating a new feature using one hot encoding of shipping and item_condition columns\n# hot_shp_item_cnd : one hot encoding of shipping and item_condition columns\nstart_time = time.time()\nprint ('Encoding of item_condition_id and shipping and start time is:' ,  \n       (time.strftime(\"%Hhrs:%Mm:%Ss\",( time.gmtime((start_time))))))\n\ncombdata['hot_shp_item_cnd'] = csr_matrix(pd.get_dummies(combdata[[\"item_condition_id\", \"shipping\" ]], sparse=True).values)\nprint ('   Shape of X_shp_item_cnd after encoding %s ...'%(str(combdata['hot_shp_item_cnd'].shape)))\nend_time = end_time = time.time()\nprint ('Completed encoding of item_condition_id and shipping and time taken::', \n       (time.strftime(\"%Hhrs:%Mm:%Ss\",( time.gmtime((end_time - start_time))))))","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"73ab5941c88b4144ec6f330f7091cd05f3258d0d"},"cell_type":"code","source":"# LabelEncoder on brand and newCategory\n# creating two new features lenght for newName and descritpion columns\n# a > lbl_category: LabelEncoder of newCategory column\n# b > lbl_brand_name: LabelEncoder of Brand column\n\nstart_time = time.time()\nprint ('Starting encoding Brand and newCategory and start time is:' ,  \n       (time.strftime(\"%Hhrs:%Mm:%Ss\",( time.gmtime((start_time))))))\n\nle = LabelEncoder()\n\nle.fit(np.hstack([combdata.newCategory]))\ncombdata['lbl_category'] = le.transform(combdata.newCategory)\n\nle.fit(np.hstack([combdata.Brand]))\ncombdata['lbl_brand_name'] = le.transform(combdata.Brand)\n\ndel le\ngc.collect()\n\nend_time = time.time()\nprint ('Completed encoding Brand and newCategory and time taken to process:', \n       (time.strftime(\"%Hhrs:%Mm:%Ss\",( time.gmtime((end_time - start_time))))))","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"13a0e75a18b2aec6ba09d4f98521cd63673a8be1"},"cell_type":"code","source":"# we will creating a average score for each token \n# using tfid score and word2vec matrix to calculate \n# token = vector multiplicaiton of word2vec matrix (200) and tfidf score (1)\n# this would then be divided by the no of words\n \ndef buildWordVector(tokens, size, vec):\n    #size = 200\n    #print ('The tokens:',type(tokens), tokens, vec)\n    #vec = np.zeros(size).reshape((1, size))\n    count = 0.\n    for words in tokens.split(' '):\n        #print( 'The type (words) : ', (words))\n        try:\n            #print('The word :', word)\n            vec = vec + word2vecModel.wv.get_vector(words).reshape((1, size)) * tfidf[words]\n            count += 1.\n        except KeyError: # handling the case where the token is not\n                         # in the corpus. useful for testing.\n                #print ('Unknow word:',word,KeyError)\n                continue\n                \n    if count != 0:\n        vec /= count\n    #finalVec = vec\n    #print(vec)\n    return vec","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"30e25f0edc82686f6c3f5ead07fd165223127080"},"cell_type":"code","source":"# Let us clean up our data and delete some of the columns that we do not require\n# this free up some memry \nstart_time = time.time()\nprint ('Deleting unwanted data and freeing memory and start time is:' ,  \n       (time.strftime(\"%Hhrs:%Mm:%Ss\",( time.gmtime((start_time))))))\ncols = combdata.columns\nprint ('   We have following columns....\\n   ', cols)\n\nrcols = ('brand_name', 'category_name', 'item_condition_id', 'item_description',\n       'name', 'shipping', 'Brand','newBrand', 'newCategory') \n\nprint()\nprint ('   We will remove following columns...\\n   ', rcols)\nprint()\ncombdata = combdata.filter(['train_id','test_id','price', 'newName', 'description', 'shipping',\n                        'hot_shp_item_cnd','lbl_category','item_condition_id',\n                        'lbl_brand_name', 'len_description', 'len_name' ], axis= 1) \ncols = combdata.columns\nprint()\nprint ('   We have following columns after removing unwanted columns....\\n   ', cols)\n\n# The processed data to file \n#print ('   Saving the dataframe to disk (processdata.pkl) for future use...')\n#combdata.to_pickle('data/processdata.pkl')    #to save the dataframe, df to 123.pkl\n#combdata = pd.read_pickle('123.pkl')   #to load 123.pkl back to the dataframe df\n\n#del combdata\ngc.collect()\n\nend_time = time.time()\nprint ('Completed deleting unwanted data and freeing memory and time taken to process:', \n       (time.strftime(\"%Hhrs:%Mm:%Ss\",( time.gmtime((end_time - start_time))))))","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"9e2729f98c648aebdc5678d1f1eac5e21efbcb0a"},"cell_type":"code","source":"# We will be splitting the trainning and test data\n# we would split the trainning data in to train and validation data \n\nstart_time = time.time()\nprint ('Spliting train and test data and start time is:' , \n       (time.strftime(\"%Hhrs:%Mm:%Ss\", time.gmtime((start_time)))) )\n\nprint('The original train no of rows(%d)'  %(nrow_train))\n#Separate the train and test data \norg_train = combdata[:nrow_train]\nX_test = combdata[nrow_train:]\nprint('   org_train shape ', org_train.shape)\nprint('   X_test shape ',  X_test.shape)\n\nprint ('   Spliting training data into train and validation set...')\nX_splt_train, X_splt_val  = train_test_split(org_train, random_state=42, test_size=0.10)\ny_train = X_splt_train['price'].values \ny_train = y_train.reshape(len(y_train),1)\ny_val = X_splt_val['price'].values \ny_val = y_val.reshape(len(y_val),1)\n\nX_TRAIN_SHAPE = X_splt_train.shape[1]\nY_TRAIN_SHAPE = y_train.shape[1]\nX_VAL_SHAPE = X_splt_val.shape[1]\nY_VAL_SHAPE = y_val.shape[1]\n\nprint('   X_splt_train shape ', X_splt_train.shape)\nprint('   X_splt_val shape ', X_splt_val.shape)\nprint('   y_train shape ', y_train.shape)\nprint('   y_val shape ', y_val.shape)\ndel combdata\ngc.collect()\n\nend_time = time.time()\nprint ('Completed spliting train and test data and time taken::', \n       (time.strftime(\"%Hhrs:%Mm:%Ss\",( time.gmtime((end_time - start_time))))))","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"0dc6a1838d1a84921c5ba14be935792f5c9f90bf"},"cell_type":"code","source":"#Tflearn DATA DEFINITION\n# Creating a two dictionary \n# x with all the data \n# shape ditionary with each of the columns shape[1] to be used as input for algo\n\ndef get_tflearn_data(dataset, scaler):\n\n    print ('     Creating data dictionary...')\n    \n    wordVecSize = 200\n    #The global variable\n    zeroVec = np.zeros(wordVecSize, dtype=np.float32).reshape((1, wordVecSize))\n    print ('      Creating name feature...')\n    name = np.concatenate([buildWordVector(z, wordVecSize, zeroVec)  for z in dataset.newName], axis=0)\n    print ('      The shape of namefeature... ', name.shape)\n\n    print ('      Creating description feature...')\n    description = np.concatenate([buildWordVector(z, wordVecSize, zeroVec)  for z in dataset.description], axis=0)\n    print ('      The shape of description feature...', description.shape)\n    \n    \n    brand_name =  np.array(dataset.lbl_brand_name.values)\n    brand_name = brand_name.reshape(len(brand_name),1)\n    \n    category =  np.array(dataset.lbl_category.values)\n    category = category.reshape(len(category),1)\n    \n    hot_shp_item_cnd = np.array(dataset.item_condition_id.values)\n    #hot_shp_item_cnd = hot_shp_item_cnd.reshape(len(hot_shp_item_cnd), 1)\n    item_condition = np.array(dataset.item_condition_id.values)\n    item_condition = item_condition.reshape(len(item_condition), 1)\n    \n    shipping = np.array(dataset.shipping.values)\n    shipping = shipping.reshape(len(shipping), 1)\n    \n    print ('      Scaling inputs features...')\n    name = scaler.fit_transform(name)\n    description =  scaler.fit_transform(description)\n    brand_name =  scaler.fit_transform(brand_name)\n    category = scaler.fit_transform(category) \n    item_condition = scaler.fit_transform(item_condition)\n    shipping = scaler.fit_transform(shipping)\n    print ('      Finished scaling inputs features...')\n\n    X =  {\n        'name': name,\n        'item_desc' : description,\n        'brand_name' : brand_name,\n        'category' : category, #'hot_shp_item_cnd':hot_shp_item_cnd\n        'item_condition' : item_condition, \n        'shipping' : shipping  \n        }\n\n    print ('     Finished data dictionary...')\n       \n    return (X)\n","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"e0fd616c9b79e56407ae9268e4163c48e8bd1de6"},"cell_type":"code","source":"# Build the model using our train dictionary data set\n\ndef getModel(dataset):\n    print('Preparing the model...')\n    tf.reset_default_graph()\n    \n    wordVecSize = 200\n\n    name = input_data (shape= [None, wordVecSize], name=\"name\" )\n    item_desc = input_data(shape= [None, wordVecSize] , name=\"item_desc\" ) \n    brand_name = input_data(shape= [None, 1], name=\"brand_name\" ) \n    category = input_data(shape= [None,1],  name=\"category\" ) \n    item_condition = input_data(shape= [None, 1], name=\"item_condition\" ) \n    shipping = input_data(shape= [None, 1], name=\"shipping\" ) \n\n    \n    flat_name = flatten(name)\n    flat_item_desc =  flatten(item_desc)\n    \n    net = merge ([ flat_name, \n                  flat_item_desc,\n                  brand_name,\n                  category,\n                  item_condition,\n                  shipping],  \n                 mode = 'concat', axis = 1)\n    \n    X_TRAIN_SHAPE = net.shape[1]\n    print('   Buildig the model...')\n    print('   Shape of the final merge input dataset:',X_TRAIN_SHAPE )\n    \n    # create model\n    #print('      The graph :' , tf.Graph.version)\n    print('      Creating input layer...')\n    net1 = fully_connected(net, X_TRAIN_SHAPE, activation='ReLU')\n    net2 = fully_connected(net1,  X_TRAIN_SHAPE, activation='ReLU')\n    net3 = fully_connected(net2, 256, activation='ReLU')\n    net4 = fully_connected(net3, 1, activation='linear')\n    net5 = regression(net4, optimizer='Adam', loss='mean_square',  metric='R2')\n\n    model = DNN(net5)\n    print('  Finished building model...')\n\n    print('Finished preparing the model...')\n\n    return(model)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"f3cd10fdab62300ab343f965142c6b7e6c5ebd50"},"cell_type":"code","source":"# prepare the data defination for the model\n# for train/validation set and test set\n\n#tf.reset_default_graph()\n\nstart_time = time.time()\nprint ('Preparing data definations and start time is:' , \n       (time.strftime(\"%Hhrs:%Mm:%Ss\", time.gmtime((start_time)))) )\n\n\nscaler = sklearn.preprocessing.RobustScaler()\n\nprint ('   Preparing Train data set...')\nX_train_dict = get_tflearn_data(X_splt_train,scaler )\n\nprint ('   Preparing Validation data set...')\nX_val_dict = get_tflearn_data(X_splt_val,scaler)\n\nprint ('   Preparing Test data set...')\nX_test_dict = get_tflearn_data(X_test, scaler)\n\n# convert the price into log1 value for scaling \nprint ('   Scaling price feature through log...')\ny_train = np.log1p(y_train)\ny_val = np.log1p(y_val)\n\n# drop the word2vec and tfid model to recover memory\n#print ('   Droping word2vec and tfid...')\n#with open('vectorizer.pk', 'wb') as tfid:\n#    pickle.dump(vectorizer, tfid)\ndel word2vecModel\ndel tfidf \ndel sentences \ndel X_splt_train\ndel X_splt_val\ndel X_test\ngc.collect()\n\n#print ('   Preparing train array set...')\n#X_train = get_dict_array(X_train_dict)\n\n#print ('   Preparing validation array set...')\n#X_val = get_dict_array(X_val_dict)\n\n#print ('   Preparing test array set...')\n#X_test = get_dict_array(X_test_dict)\n\n#print('   X_train shape ', X_train.shape)\n#print('   X_test shape ', X_test.shape)\n#print('   X_val shape ', X_val.shape)\n\nprint ('Finished data definations...')\nend_time = time.time()\nprint ('Completed preparing data definations and time taken::', \n       (time.strftime(\"%Hhrs:%Mm:%Ss\",( time.gmtime((end_time - start_time))))))\n#print (end running)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"4bb29519bb3d47ef3e6cc21023bf6652a2ecde8e"},"cell_type":"code","source":"#Compute the Root Mean Squared Log Error for hypthesis h and targets y\n#\n#Args:\n#y_pred - numpy array containing predictions with shape (n_samples, n_targets)\n#y_actual - numpy array containing targets with shape (n_samples, n_targets)\n#A function to calculate Root Mean Squared Logarithmic Error (RMSLE)\ndef rmsle(y_pred, y_actual): \n    return np.sqrt(np.square(np.log(y_pred + 1) - np.log(y_actual + 1)).mean())","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"e2eff375979b944a5867640247f87a106e2ff6a3"},"cell_type":"code","source":"# We would be building the model \n# training the model \nstart_time = time.time()\nprint ('Building and training  the modeland start time is:' , \n       (time.strftime(\"%Hhrs:%Mm:%Ss\", time.gmtime((start_time)))) )\n\n\nmodelR = getModel(X_train_dict)\nprint('Training the model...')\n\nmodelR.fit( X_train_dict,  y_train, n_epoch=5, \n           batch_size=256, snapshot_step= 1000, # validation_set=[ X_val_dict, y_val],           \n           show_metric=True,run_id='tflMP')\n\n# Target label used for training\n#labels = np.array(data[label], dtype=np.float32)\n# Reshape target label from (6605,) to (6605, 1)\n#labels =np.reshape(y_train,(-1,1)) #makesure the labels has the shape of (?,1)\n\nend_time = time.time()\nprint ('Completed building and training  the model and time taken::', \n       (time.strftime(\"%Hhrs:%Mm:%Ss\",( time.gmtime((end_time - start_time))))))","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"f7607a440d08315a0370e3a93780b01434ddc6ae"},"cell_type":"code","source":"#let us evluate the model\nstart_time = time.time()\nprint ('Evaluating the model  the modeland start time is:' , \n       (time.strftime(\"%Hhrs:%Mm:%Ss\", time.gmtime((start_time)))) )\n\nscore = modelR.evaluate(X_val_dict, y_val, batch_size=256 )\nprint('   The validation data set accuracy is:', score)\n\n# calculate the rmsle for val  data set     \nval_rmsle = 0\ny_val_pred = modelR.predict(X_val_dict)\nval_rmsle = rmsle(y_val_pred, y_val)\nprint ('   The validation data set rmsle is (%f) '%(val_rmsle))\n\nend_time = time.time()\nprint ('Completed evaluation of the model and time taken::', \n       (time.strftime(\"%Hhrs:%Mm:%Ss\",( time.gmtime((end_time - start_time))))))","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"342576c25c3e1dd322e79d4571f24a36dd7e95aa"},"cell_type":"code","source":"# We are ready to predict price for test features\n\nstart_time = time.time()\nprint ('Predicting and submitting results and start time is:' , \n       (time.strftime(\"%Hhrs:%Mm:%Ss\", time.gmtime((start_time)))) )\n\n\ny_pred = np.zeros(X_test_dict['name'].shape[0])\n\ny_pred = modelR.predict(X_test_dict)\ny_pred = np.expm1(y_pred)\n\nprint('   preparing for submission...')\n\npricePred = pd.DataFrame()\npricePred['test_id'] = test_id\npricePred['price'] = np.round(y_pred,2)\npricePred.to_csv('Sample_sub.csv',index=False)\n\n\nprint('   submitted...')\n\nend_time = time.time()\nprint ('******Price recommended in ******:' , \n       (time.strftime(\"%Hhrs:%Mm:%Ss\", time.gmtime((end_time - modeling_start_time)))) )","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}