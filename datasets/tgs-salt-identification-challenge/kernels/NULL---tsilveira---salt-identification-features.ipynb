{"cells":[{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"# TGS - Salt identification challenge\nFirst exploratory data analysis.  \n2018.07.20  "},{"metadata":{"_uuid":"1aa1492a9c67312650a5d48eb94e5869eae3f577"},"cell_type":"markdown","source":"This kernel proposal is to carry out the initial analysis of the images made available by TGS.  \n\n## Summary:\n* [A. Initial statements](#secA)  \n\n* [B. Understanding the data](#secB)\n  * [B.1. Background](#secB.1)\n  * [B.2. Available data](#secB.2)  \n  * [B.3. Loading the data](#secB.3)\n  * [B.4. Choosing a random sample](#secB.4)\n  \n* [C. Feature extraction](#secC)\n  * [C.1 Creating a feature dataset](#secC.1)\n  * [C.2 Taking a look at depth](#secC.2)\n  * [C.3 Target defining](#secC.3)"},{"metadata":{"_uuid":"bfb84f565f93707226fdfd0b314a55b60e63b6de"},"cell_type":"markdown","source":"<a name='secA'></a>\n## A. Initial statements"},{"metadata":{"_kg_hide-input":false,"_kg_hide-output":true,"trusted":true,"collapsed":true,"_uuid":"5d1951662dfc86c8e6c2919d5059f07fc51d5049"},"cell_type":"code","source":"## This kernel must be run on Python=3.6\nimport numpy as np \nimport pandas as pd  #Python Data Analysis Library\nimport random\n\nimport scipy.ndimage as scipyImg\nimport scipy.misc as misc\n\nimport matplotlib.pyplot as plt \nimport seaborn as sns\n\nimport os\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true,"collapsed":true,"_uuid":"4eec52b7e2ded96b0156d3450d37748e9ec01d68"},"cell_type":"code","source":"## Disabling filter warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"810c604a22790f8da30d081cb781711fd6a8efaa"},"cell_type":"code","source":"## Defining basic functions\ndef basic_readImg(directory, filename):\n    '''Reading an RGB image through the scipy library. Provides an array.\n    Sintaxe: basic_readImg(directory, filename).'''\n    sample = scipyImg.imread(directory + filename, mode='RGB')\n    if sample.shape[2] != 3:\n        return 'The input must be an RGB image.'\n    return sample\n\ndef basic_showImg(img, size=4):\n    ''' Displays the image at the chosen size. The image (img) should be read through basic_readImg().\n    Sintaxe: basic_showImg(img, size=4).'''\n    plt.figure(figsize=(size,size))\n    plt.imshow(img)\n    plt.show()\n    \ndef basic_writeImg(directory, filename, img):\n    misc.imsave(directory+filename, img)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1209dcce0d251d4ec101252cc9e8cb5046450e5d"},"cell_type":"markdown","source":"<a name='secB'></a>\n## B. Understanding the data\n<a name='secB.1'></a>\n### B.1. Background  \n*The information in this section is from [TGS Salt Challenge page](https://www.kaggle.com/c/tgs-salt-identification-challenge/data)*.  \n\nSeismic data is collected using reflection seismology, or seismic reflection. The method requires a controlled seismic source of energy, such as compressed air or a seismic vibrator, and sensors record the reflection from rock interfaces within the subsurface. The recorded data is then processed to create a 3D view of earth’s interior. Reflection seismology is similar to X-ray, sonar and echolocation.\n\nA seismic image is produced from imaging the reflection coming from rock boundaries. The seismic image shows the boundaries between different rock types. In theory, the strength of reflection is directly proportional to the difference in the physical properties on either sides of the interface. While seismic images show rock boundaries, they don't say much about the rock themselves; some rocks are easy to identify while some are difficult.\n\nThere are several areas of the world where there are vast quantities of salt in the subsurface. One of the challenges of seismic imaging is to identify the part of subsurface which is salt. Salt has characteristics that makes it both simple and hard to identify. Salt density is usually 2.14 g/cc which is lower than most surrounding rocks. The seismic velocity of salt is 4.5 km/sec, which is usually faster than its surrounding rocks. This difference creates a sharp reflection at the salt-sediment interface. Usually salt is an amorphous rock without much internal structure. This means that there is typically not much reflectivity inside the salt, unless there are sediments trapped inside it. The unusually high seismic velocity of salt can create problems with seismic imaging.\n"},{"metadata":{"_uuid":"5f8df67e37dfdc149e3e945cd493c5c7dc97e85c"},"cell_type":"markdown","source":"<a name='secB.2'></a>\n### B.2. Available data\nThe data is a set of images chosen at various locations chosen at random in the subsurface. The images are 101 x 101 pixels and each pixel is classified as either salt or sediment. In addition to the seismic images, the depth of the imaged location is provided for each image. The goal of the competition is to segment regions that contain salt."},{"metadata":{"_uuid":"e360b35aa22e5f917e42352c883dc5bb4fe9f8f0"},"cell_type":"markdown","source":"<a name='secB.3'></a>\n### B.3. Loading the data\n#### Depths database reading  \nThe depth underground (in feet) of each image is available on the *depths.csv* file, where the attribute *id* is the unique image identifier and *z* is its depth in feet (1 feet equals 0.3048 meters)."},{"metadata":{"trusted":true,"_uuid":"1db65a355e9a3419cd28497d79d32f534b787fda","collapsed":true},"cell_type":"code","source":"## Loading the dataset and showing the first rows:\ndepths = pd.read_csv('../input/depths.csv')\ndepths.head(2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"967a6d0d4da0b661d862ffd356d187717d9a948a"},"cell_type":"markdown","source":"#### Training set masks in RLE\nTGS has also made available some [Run-length encoding (RLE)](https://en.wikipedia.org/wiki/Run-length_encoding) masks with the salt portion of the images already identified."},{"metadata":{"trusted":true,"_uuid":"0f4622b9204a553e930a14ec2e81a846e1f400e1","collapsed":true},"cell_type":"code","source":"train_masks = pd.read_csv('../input/train.csv')\ntrain_masks.head(2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"df2bef0b94b1962566f22b9c2a9c0f3b7d8cd5f0"},"cell_type":"markdown","source":"In order to read such encoded masks, I will make use of [the function created by Robert](https://www.kaggle.com/robertkag/rle-to-mask-converter) at Kaggle, which reads the RLE string and generates an image array corresponding to the mask:  "},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"9378224a9b335c90e3207ce4d5dfadcc0b22ca3c"},"cell_type":"code","source":"def rleToMask(rleString,height,width):\n    rows,cols = height,width\n    try:\n        rleNumbers = [int(numstring) for numstring in rleString.split(' ')]\n        rlePairs = np.array(rleNumbers).reshape(-1,2)\n        img = np.zeros(rows*cols,dtype=np.uint8)\n        for index,length in rlePairs:\n            index -= 1\n            img[index:index+length] = 255\n        img = img.reshape(cols,rows)\n        img = img.T\n    except:\n        img = np.zeros((cols,rows))\n    return img","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"453b82eb1799a328547dbee5dc89af07f8de3540"},"cell_type":"markdown","source":"Regarding the training data, for each subsurface image there is its corresponding mask, listed below:"},{"metadata":{"trusted":true,"_uuid":"69827fa9337b81253da088d929fb4a557beb343b","collapsed":true},"cell_type":"code","source":"file_imgs = os.listdir(path='../input/train/images/')\nfile_masks = os.listdir(path='../input/train/masks/')\nprint('Images found: {0}\\nCorresponding masks: {1}'.format(len(file_imgs), len(file_masks)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e1bf77f86a427caa496042e34b3150440e17ea12"},"cell_type":"markdown","source":"<a name='secB.4'></a>\n### B.4. Choosing a random sample"},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true,"_uuid":"493eb61d948ccd42db1158187266ff70a7a9f54d","collapsed":true},"cell_type":"code","source":"## Defining a function since there's sample without valid RLE.\ndef choose_sample(data=train_masks):\n    ## Choosing a random image from train dataset:\n    sample = random.choice(range(len(data)))\n\n    ## Parsing the sample information:\n    sample_id = data['id'][sample]\n    sample_depth = depths[depths['id'] == sample_id]['z'].values[0]\n    sample_RLEstring = data['rle_mask'][sample]\n    try: \n        sample_RLE = rleToMask(sample_RLEstring, 101,101)\n    except: \n        sample_RLE = np.zeros((101,101))\n    file_name = sample_id + '.png'\n    sample_img = basic_readImg('../input/train/images/',file_name)\n    sample_mask = basic_readImg('../input/train/masks/',file_name)\n    \n    fig1, axes = plt.subplots(1,3, figsize=(10,4))\n    axes[0].imshow(sample_img)\n    axes[0].set_xlabel('Subsurface image')\n    axes[1].imshow(sample_mask)\n    axes[1].set_xlabel('Provided mask')\n    axes[2].imshow(sample_RLE)\n    axes[2].set_xlabel('Decoded RLE mask')\n    fig1.suptitle('Image ID = {0}\\nDepth = {1} ft.'.format(sample_id, sample_depth));\n    return","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"a7ca584b51a4489a62160e7979fba8adfe377e65","collapsed":true},"cell_type":"code","source":"choose_sample()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a84cdc7eadffa610f80d9507bf49b58be936566b"},"cell_type":"markdown","source":"<a name='secC'></a>\n## C. Exploratory Data Analysis\n<a name='secC.1'></a>\n### C.1. Creating a feature dataset\nPutting together all the training information we have.\n"},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"4f110a1b1bdead7f2120b18c0850201ac38e8c9a","collapsed":true},"cell_type":"code","source":"df1 = depths.set_index('id')\ndf2= train_masks.set_index('id')\ndataset = pd.concat([df1, df2], axis=1, join='inner')\ndataset = dataset.reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"0fb36f9c70d48c796285af5a6c3cee9d69d19516","collapsed":true},"cell_type":"code","source":"dataset['mask'] = dataset['rle_mask'].apply(lambda x: rleToMask(x, 101,101))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"935fa398b13a14c0d5a3e2180cb6e215eeeadf0e"},"cell_type":"code","source":"def salt_proportion(imgArray):\n    try: \n        unique, counts = np.unique(imgArray, return_counts=True)\n        ## The total number of pixels is 101*101 = 10,201\n        return counts[1]/10201.\n    except: \n        return 0.0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ebc941a6d2859d3b3b0008ce849d5e73919a37e7","collapsed":true},"cell_type":"code","source":"dataset['salt_proportion'] = dataset['mask'].apply(lambda x: salt_proportion(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2c1540f748180da8d2ab3b94414fd805c2420be3","collapsed":true},"cell_type":"code","source":"dataset.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c38578bbd6136970baf5aaedac533aafc1171b86"},"cell_type":"markdown","source":"<a name='secC.2'></a>\n### C.2. Taking a look at depth\nHow is depth distributed along the dataset?\n"},{"metadata":{"trusted":true,"_uuid":"ff9f2d5133ee3b36aa9dd690584e0e972725aafd","scrolled":true,"collapsed":true},"cell_type":"code","source":"sns.set();\nsns.distplot(dataset['z'], bins=20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2685eec4db97d10ae9ba174c2eed29e77412826f","collapsed":true},"cell_type":"code","source":"sns.pairplot(dataset, vars=['z', 'salt_proportion'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"abf16343b6a49578b5ca4e50d10f01967bd36a11"},"cell_type":"markdown","source":"The pairplot above results are expected, since the salt proportion in each image is related to the region from which the data were colected. Next steps now is to extract some features from the *salty* regions in order to correlate it with depth as well as other attributes."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"4dcbf00c3b1ad0ab8241071064b43cfb68da1cc3"},"cell_type":"markdown","source":"<a name='secC.3'></a>\n### C.3. Target defining\nIn this first approach I will consider the salt proportion as a target feature. In this way, I'll divide the dataset into four salty categories:  \ni. No salt [0%]  \nii. Very low [0.01% - 10%]  \niii. Low [10.01% - 40%]  \niv. Medium [40.01% - 60%]  \nv. High [60.01% - 90%]  \nvi. Very high [> 90%]"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"b9fce34586e96088c4a71038bd47e06185c753e0"},"cell_type":"code","source":"dataset['target'] = pd.cut(dataset['salt_proportion'], bins=[0, 0.001, 0.1, 0.4, 0.6, 0.9, 1.0], \n       include_lowest=True, labels=['No salt', 'Very low', 'Low', 'Medium', 'High', 'Very high'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"10e14a7fdd29b121ca835841a7a58b4055987a70","collapsed":true},"cell_type":"code","source":"dataset.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"7154284022769def222d2281be71a5af2b224ef2"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}