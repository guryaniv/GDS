{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n    \nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"from collections import Counter\nfrom sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.linear_model import LinearRegression, ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder, RobustScaler\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nimport lightgbm as lgb\n#import xgboost as xgb\n\nfrom datetime import tzinfo, timedelta, datetime\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n%matplotlib inline\n\nfrom scipy import stats\nfrom scipy.stats import norm, skew #for some statistics\n\nfrom sklearn import model_selection, preprocessing, metrics","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1b58fad6dcad62c4fdcf75da4be650d292a64c64"},"cell_type":"code","source":"#load all files\ndf_train = pd.read_csv('../input/train.csv')\ndf_test = pd.read_csv('../input/test.csv')\ndf_hist = pd.read_csv('../input/historical_transactions.csv')\ndf_new = pd.read_csv('../input/new_merchant_transactions.csv')\ndf_merch = pd.read_csv('../input/merchants.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"33058f9adf1ae0ae050e95b60d05ef1dbd1c4a30"},"cell_type":"code","source":"#drop duplicated from the merchent list\ndf_merch = df_merch.drop_duplicates(['merchant_id'],keep='first')\ndf_merch.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f42cda90a2b9814237c3916a1a9cd37b2b91f6f7"},"cell_type":"code","source":"# I am interested in 2 featres from the merchant data\ndf_merch = df_merch[['merchant_id','numerical_1', 'numerical_2']]\n#Combine the data with the card details\ndf_hist = pd.merge(df_hist,df_merch, on='merchant_id',how='left',sort=False)\ndf_new = pd.merge(df_new,df_merch, on='merchant_id',how='left',sort=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"843249def287687240d14a4bee8b3eb67ddba836"},"cell_type":"code","source":"df_merch = pd.DataFrame()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fc35b4ff4567973408afe5367e02fae8e81c1fe9"},"cell_type":"code","source":"#save the card details for future use\ndy_train = df_train['target']\ndf_train = df_train.drop(['target'],axis=1)\ntest_card_id = df_test['card_id']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bbc6d77b7354447f3255c34ae2696bfa90d1f770"},"cell_type":"code","source":"df_merge = pd.concat([df_train,df_test],sort=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4dfdc0cd59a9472f20432e6ddcdd1d5413659907"},"cell_type":"code","source":"hist_df = pd.concat([df_hist,df_new],sort=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a8a9c23856c693e3f9714d0c7ccb47080cc1dd50"},"cell_type":"code","source":"df_hist = hist_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1cff04b108fa5c2ac613d38eaaef8dfd786014f0"},"cell_type":"code","source":"#def aggregate - authorized_flag = Y\nxin = df_hist[df_hist['authorized_flag'] == 'Y'].groupby(['card_id'])\n\nxon = xin.agg({'purchase_amount':['min','max','mean','sum']})\nxon.columns = [\"_y_\".join(x) for x in xon.columns.ravel()]\ndf_merge = pd.merge(df_merge,xon, on='card_id',how='left',sort=False)\n\nxon = xin.agg({'authorized_flag':['count']})\nxon.columns = [\"_y_\".join(x) for x in xon.columns.ravel()]\ndf_merge = pd.merge(df_merge,xon, on='card_id',how='left',sort=False)\n\n\nxon = xin.agg({'numerical_1':['sum']})\nxon.columns = [\"_y_\".join(x) for x in xon.columns.ravel()]\ndf_merge = pd.merge(df_merge,xon, on='card_id',how='left',sort=False)\n\nxon = xin.agg({'numerical_2':['sum']})\nxon.columns = [\"_y_\".join(x) for x in xon.columns.ravel()]\ndf_merge = pd.merge(df_merge,xon, on='card_id',how='left',sort=False)\n\n\n#def aggregate - authorized_flag != Y\nxin = df_hist[df_hist['authorized_flag'] != 'Y'].groupby(['card_id'])\nxon = xin.agg({'authorized_flag':['count']})\nxon.columns = [\"_n_\".join(x) for x in xon.columns.ravel()]\ndf_merge = pd.merge(df_merge,xon, on='card_id',how='left',sort=False)\n\nxon = xin.agg({'purchase_amount':['min','max','mean','sum']})\nxon.columns = [\"_n_\".join(x) for x in xon.columns.ravel()]\ndf_merge = pd.merge(df_merge,xon, on='card_id',how='left',sort=False)\n\nxon = xin.agg({'numerical_1':['sum']})\nxon.columns = [\"_n_\".join(x) for x in xon.columns.ravel()]\ndf_merge = pd.merge(df_merge,xon, on='card_id',how='left',sort=False)\n\nxon = xin.agg({'numerical_2':['sum']})\nxon.columns = [\"_n_\".join(x) for x in xon.columns.ravel()]\ndf_merge = pd.merge(df_merge,xon, on='card_id',how='left',sort=False)\n\n\nxon = df_hist[df_hist['category_1'] == 'Y'].groupby(['card_id']).agg({'category_1':['count']})\nxon.columns = [\"_y_\".join(x) for x in xon.columns.ravel()]\ndf_merge = pd.merge(df_merge,xon, on='card_id',how='left',sort=False)\n\nxon = df_hist[df_hist['category_1'] != 'Y'].groupby(['card_id']).agg({'category_1':['count']})\nxon.columns = [\"_n_\".join(x) for x in xon.columns.ravel()]\ndf_merge = pd.merge(df_merge,xon, on='card_id',how='left',sort=False)\n\nxon = df_hist[df_hist['category_2'] == 1].groupby(['card_id']).agg({'category_2':['count']})\nxon.columns = [\"_1_\".join(x) for x in xon.columns.ravel()]\ndf_merge = pd.merge(df_merge,xon, on='card_id',how='left',sort=False)\n\nxon = df_hist[df_hist['category_2'] == 2].groupby(['card_id']).agg({'category_2':['count']})\nxon.columns = [\"_2_\".join(x) for x in xon.columns.ravel()]\ndf_merge = pd.merge(df_merge,xon, on='card_id',how='left',sort=False)\n\nxon = df_hist[df_hist['category_2'] == 3].groupby(['card_id']).agg({'category_2':['count']})\nxon.columns = [\"_3_\".join(x) for x in xon.columns.ravel()]\ndf_merge = pd.merge(df_merge,xon, on='card_id',how='left',sort=False)\n\nxon = df_hist[df_hist['category_2'] == 4].groupby(['card_id']).agg({'category_2':['count']})\nxon.columns = [\"_4_\".join(x) for x in xon.columns.ravel()]\ndf_merge = pd.merge(df_merge,xon, on='card_id',how='left',sort=False)\n\nxon = df_hist[df_hist['category_2'] == 5].groupby(['card_id']).agg({'category_2':['count']})\nxon.columns = [\"_5_\".join(x) for x in xon.columns.ravel()]\ndf_merge = pd.merge(df_merge,xon, on='card_id',how='left',sort=False)\n\nxon = df_hist[df_hist['category_3'] == 'A'].groupby(['card_id']).agg({'category_3':['count']})\nxon.columns = [\"_A_\".join(x) for x in xon.columns.ravel()]\ndf_merge = pd.merge(df_merge,xon, on='card_id',how='left',sort=False)\n\nxon = df_hist[df_hist['category_3'] == 'B'].groupby(['card_id']).agg({'category_3':['count']})\nxon.columns = [\"_B_\".join(x) for x in xon.columns.ravel()]\ndf_merge = pd.merge(df_merge,xon, on='card_id',how='left',sort=False)\n\nxon = df_hist[df_hist['category_3'] == 'C'].groupby(['card_id']).agg({'category_3':['count']})\nxon.columns = [\"_C_\".join(x) for x in xon.columns.ravel()]\ndf_merge = pd.merge(df_merge,xon, on='card_id',how='left',sort=False)\n\nxin = df_hist.groupby(['card_id'])\nxon = xin.agg({'merchant_id':['nunique']})\nxon.columns = [\"_\".join(x) for x in xon.columns.ravel()]\ndf_merge = pd.merge(df_merge,xon, on='card_id',how='left',sort=False)\n\nxon = xin.agg({'subsector_id':['nunique']})\nxon.columns = [\"_\".join(x) for x in xon.columns.ravel()]\ndf_merge = pd.merge(df_merge,xon, on='card_id',how='left',sort=False)\n\nxon = xin.agg({'state_id':['nunique']})\nxon.columns = [\"_\".join(x) for x in xon.columns.ravel()]\ndf_merge = pd.merge(df_merge,xon, on='card_id',how='left',sort=False)\n\nxon = xin.agg({'installments':['sum']})\nxon.columns = [\"_\".join(x) for x in xon.columns.ravel()]\ndf_merge = pd.merge(df_merge,xon, on='card_id',how='left',sort=False)\n\nxon = xin.agg({'purchase_amount':['min','max','sum','mean']})\nxon.columns = [\"_\".join(x) for x in xon.columns.ravel()]\ndf_merge = pd.merge(df_merge,xon, on='card_id',how='left',sort=False)\n\nxon = xin.agg({'numerical_1':['min','max','sum','mean']}) \nxon.columns = [\"_\".join(x) for x in xon.columns.ravel()]\ndf_merge = pd.merge(df_merge,xon, on='card_id',how='left',sort=False)\n\nxon = xin.agg({'numerical_2':['min','max','sum','mean']}) \nxon.columns = [\"_\".join(x) for x in xon.columns.ravel()]\ndf_merge = pd.merge(df_merge,xon, on='card_id',how='left',sort=False)\n\nxon = xin.agg({'purchase_date':['min','max']}) \nxon.columns = [\"_\".join(x) for x in xon.columns.ravel()]\ndf_merge = pd.merge(df_merge,xon, on='card_id',how='left',sort=False)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e30e73f88a0b2a7aea795ad6d14ed54ea11cc236"},"cell_type":"code","source":"#Saving for future need - instead of processing all again\n#df_merge.to_csv('Merged_all.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bfc9455d292f60ec65635269635956f84059b75a"},"cell_type":"code","source":"df_merge.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e08789dd8d3ea5d9315cb1c349a1d127318a52bd"},"cell_type":"code","source":"df_merge['first_active_month'] = df_merge['first_active_month'].fillna(df_merge['first_active_month'].mode()[0])\ndf_merge.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9d66715aa9c85361d2a0faa5cbd81b8f7baabe1c"},"cell_type":"code","source":"df_merge = df_merge.fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"666437563117d170e51ad619e00a956cf8468d75"},"cell_type":"code","source":"#df_merge['TotalFeatures'] = df_merge['feature_1'] + df_merge['feature_2'] + df_merge['feature_3']\ndf_merge['auth'] = df_merge['authorized_flag_y_count'] + df_merge['authorized_flag_n_count']\ndf_merge['AuthRateY'] = (df_merge['purchase_amount_y_sum'] /df_merge['auth'])\ndf_merge['AuthRateN'] = (df_merge['purchase_amount_n_sum'] /df_merge['auth'])\ndf_merge['AvgAuthPurY'] = df_merge.apply(lambda row: row['purchase_amount_sum']/row['authorized_flag_y_count'] if(row['authorized_flag_y_count']>0) else 0, axis=1)\ndf_merge['AvgAuthPurN'] = df_merge.apply(lambda row: row['purchase_amount_sum']/row['authorized_flag_n_count'] if(row['authorized_flag_n_count']>0) else 0, axis=1)\n\ndf_merge['cat1'] = df_merge['category_1_y_count'] + df_merge['category_1_n_count']\ndf_merge['RatioCat1'] = df_merge['category_1_y_count'] / df_merge['cat1']\ndf_merge['cat2'] = df_merge['category_2_1_count'] + df_merge['category_2_2_count'] + df_merge['category_2_3_count'] + df_merge['category_2_4_count'] + df_merge['category_2_5_count']\ndf_merge['AvgCat2'] = df_merge['cat2'] / 5 \ndf_merge['cat3'] = df_merge['category_3_A_count'] + df_merge['category_3_B_count'] + df_merge['category_3_C_count']\ndf_merge['AvgCat3'] = df_merge['cat3'] / 3\n\ndf_merge['AvgInstAmt'] =  df_merge.apply(lambda row: row['purchase_amount_sum']/row['installments_sum'] if(row['installments_sum']>0) else row['purchase_amount_sum'], axis=1)\ndf_merge['AvgPurchAmt'] =  df_merge.apply(lambda row: row['purchase_amount_sum']/row['auth'], axis=1)\ndf_merge['AvgMerchant'] =  df_merge.apply(lambda row: row['purchase_amount_sum']/row['merchant_id_nunique'], axis=1)\ndf_merge['AvgSubsector'] =  df_merge.apply(lambda row: row['purchase_amount_sum']/row['subsector_id_nunique'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"660f529bc5ff70117ba397d66fdcccc14a930eca"},"cell_type":"code","source":"#find the number of month over\ndf_merge['YearElapsed'] = df_merge['first_active_month'].apply(lambda x: (2019- int(str(x).split('-')[0])))\ndf_merge['MonthElapsed'] = df_merge['first_active_month'].apply(lambda x: (12-(int(str(x).split('-')[1]))))\n#find the number of month over\ndf_merge['YearActive'] = df_merge['first_active_month'].apply(lambda x: (int(str(x).split('-')[0])))\ndf_merge['MonthActive'] = df_merge['first_active_month'].apply(lambda x: ((int(str(x).split('-')[1]))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6f68be9290c6efddedcf02425bf99ad64339af12"},"cell_type":"code","source":"df_merge.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fb91fd2faf63e41166f3334e164588f7ede73247"},"cell_type":"code","source":"df_merge['purchase_date_min'] = pd.to_datetime(df_merge['purchase_date_min'])\ndf_merge['purchase_date_max'] = pd.to_datetime(df_merge['purchase_date_max'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8c3b6258a7811db9e1c5da9cddd9cd66cfd3650b"},"cell_type":"code","source":"#'purchase_date'\ndf_merge['days_from_purchase_first'] = (datetime.today() - df_merge['purchase_date_min']).dt.days\ndf_merge['days_from_purchase_last'] = (datetime.today() - df_merge['purchase_date_max']).dt.days\n\ndf_merge['first_purchase_year'] = df_merge['purchase_date_min'].dt.year\ndf_merge['first_purchase_month'] = df_merge['purchase_date_min'].dt.month\ndf_merge['first_purchase_year'] = df_merge['purchase_date_min'].dt.day\ndf_merge['first_purchase_wkofyear'] = df_merge['purchase_date_min'].dt.weekofyear\ndf_merge['first_purchase_hour'] = df_merge['purchase_date_min'].dt.hour\n\ndf_merge['last_purchase_year'] = df_merge['purchase_date_max'].dt.year\ndf_merge['last_purchase_month'] = df_merge['purchase_date_max'].dt.month\ndf_merge['last_purchase_year'] = df_merge['purchase_date_max'].dt.day\ndf_merge['last_purchase_wkofyear'] = df_merge['purchase_date_max'].dt.weekofyear\ndf_merge['last_purchase_hour'] = df_merge['purchase_date_max'].dt.hour","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9a76cfa376791e0a2f2fa2ff434f421a6e106e36"},"cell_type":"code","source":"df_train = df_merge.iloc[:len(df_train), :]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7682c7281c13d8fb7a7300d505265de0986a5e9a"},"cell_type":"code","source":"#Find the mean of the train data to the whole of the merge data\ndf_merge['purchase_amount_y_meanDiff'] = df_merge['purchase_amount_y_mean']- df_train['purchase_amount_y_mean'].mean()\ndf_merge['purchase_amount_y_sumDiff'] = df_merge['purchase_amount_y_sum']- df_train['purchase_amount_y_sum'].mean()\ndf_merge['purchase_amount_y_minDiff'] = df_merge['purchase_amount_y_min']- df_train['purchase_amount_y_min'].mean()\ndf_merge['purchase_amount_y_maxDiff'] = df_merge['purchase_amount_y_max']- df_train['purchase_amount_y_max'].mean()\n\ndf_merge['purchase_amount_minDiff'] = df_merge['purchase_amount_min'] - df_train['purchase_amount_min'].mean()\ndf_merge['purchase_amount_maxDiff'] = df_merge['purchase_amount_max'] - df_train['purchase_amount_max'].mean()\ndf_merge['purchase_amount_sumDiff'] = df_merge['purchase_amount_sum'] - df_train['purchase_amount_sum'].mean()\ndf_merge['purchase_amount_meanDiff'] = df_merge['purchase_amount_mean'] - df_train['purchase_amount_mean'].mean()\n#Purchase amount to 'purchase_amount_sum' 'merchant_id_nunique', 'subsector_id_nunique', 'state_id_nunique'\ndf_merge['merchant_purchase_mean'] = df_merge['purchase_amount_sum'] / df_merge['merchant_id_nunique']\ndf_merge['subsector_purchase_mean'] = df_merge['purchase_amount_sum'] / df_merge['subsector_id_nunique']\ndf_merge['state_purchase_mean'] = df_merge['purchase_amount_sum'] / df_merge['state_id_nunique']\ndf_merge['anual_purchase_mean'] = df_merge['purchase_amount_sum'] / df_merge['YearElapsed']\ndf_merge['month_purchase_mean'] = df_merge['purchase_amount_sum'] / (df_merge['YearElapsed']*12 + df_merge['MonthElapsed'])\n\n#Purchase amout and numerical 1 & 2 value\ndf_merge['purchase_numerical1_rate'] = df_merge['purchase_amount_sum'] / df_merge['numerical_1_sum']\ndf_merge['purchase_numerical2_rate'] = df_merge['purchase_amount_sum'] / df_merge['numerical_2_sum']\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b8fe19d396d7661f4c782e402e80730445cc0522"},"cell_type":"code","source":"droplist = ['purchase_date_min','purchase_date_max']#,'auth','auth_n','AuthRateY', 'AuthRateN', 'cat1', 'RatioCat1', 'cat2', 'AvgCat2',\n#       'cat3', 'AvgCat3', 'AvgPurchAmt','merchant_count']\ndf_merge = df_merge.drop(droplist,axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ca42af6ee93f52cca43987d99cdbd2c2e3da27a0"},"cell_type":"code","source":"numeric_feats = df_merge.dtypes[df_merge.dtypes != \"object\"].index\n# Check the skew of all numerical features\nskewed_feats = df_merge[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nprint(\"\\nSkew in numerical features: \\n\")\nskewness = pd.DataFrame({'Skew' :skewed_feats})\nskewness = skewness[abs(skewness.Skew) > 0.8]\nskewness.head(25)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8dab216afd312d5696ae05baa343740714dc5551"},"cell_type":"code","source":"skewness = skewness[abs(skewness.Skew) > 1.0]\nprint(\"There are {} skewed numerical features to Box Cox transform\".format(skewness.shape[0]))\n\nfrom scipy.special import boxcox1p\nskewed_features = skewness.index\nlam = 0.15\nfor feat in skewed_features:\n    #all_data[feat] += 1\n    df_merge[feat] = boxcox1p(df_merge[feat], lam)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b4c3fbab2c62cc5d955a7400be571804ad6e477d"},"cell_type":"code","source":"df_merge.isnull().sum()[df_merge.isnull().sum() > 0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4f8b208ede8eb97db72fafa2e783a48cf7f84fbd"},"cell_type":"code","source":"#Fill the null values after the boxcoxing \ndef fillNa(lst,val):\n    for x in list(lst):\n        df_merge[x] = df_merge[x].fillna(0)\n    \n        \nlst =['numerical_1_y_sum','numerical_2_y_sum','purchase_amount_n_sum','numerical_1_n_sum','numerical_2_n_sum','numerical_1_sum',\n      'numerical_2_sum','installments_sum','AvgAuthPurY','AvgAuthPurN','AvgInstAmt','AvgMerchant','AvgSubsector','purchase_amount_y_maxDiff',\n     'purchase_amount_sumDiff','purchase_amount_meanDiff','purchase_amount_maxDiff','purchase_amount_sum','merchant_purchase_mean',\n     'subsector_purchase_mean','state_purchase_mean','anual_purchase_mean','month_purchase_mean','purchase_numerical1_rate','purchase_numerical2_rate']\n        \nfillNa(lst,0)\n#Drop the features having high amout of null values\nto_drop = ['purchase_amount_n_sum','purchase_amount_sum',\n      'AvgAuthPurN','AvgInstAmt','AvgMerchant','AvgSubsector','purchase_amount_y_maxDiff','purchase_amount_maxDiff',\n      'purchase_amount_sumDiff','purchase_amount_meanDiff','merchant_purchase_mean','subsector_purchase_mean',\n      'state_purchase_mean','anual_purchase_mean','month_purchase_mean','purchase_numerical1_rate','purchase_numerical2_rate']\ndf_merge = df_merge.drop(to_drop,axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dcee42aae187bcef6e6afccc0955ffc1725126fc"},"cell_type":"code","source":"df_merge.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7f252fa92505ea16f8260a2b6fb05e10c6cc2426"},"cell_type":"code","source":"#convert float to numbers whereever not float\nfeat = ['authorized_flag_y_count','authorized_flag_n_count', 'category_1_y_count', 'category_1_n_count', \n        'category_2_1_count','category_2_2_count', 'category_2_3_count', 'category_2_4_count',\n       'category_2_5_count', 'category_3_A_count', 'category_3_B_count','category_3_C_count', \n        'merchant_id_nunique', 'subsector_id_nunique','state_id_nunique','YearElapsed', 'MonthElapsed']\nfor z in feat:\n    if df_merge[z].dtype =='float':\n        df_merge[z] = df_merge[z].astype(int)\n        \nfeat_cat = ['feature_1', 'feature_2', 'feature_3']\nfor z in feat_cat:\n    df_merge[z] = df_merge[z].astype('category')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e0933ccfbf2086ef650bab7a94dbb5045da63055"},"cell_type":"code","source":"df_cat = df_merge.select_dtypes(include=['category'])\ndf_num = df_merge.select_dtypes(exclude=['category'])\n#apply label encoder for categorical columns \nle = LabelEncoder()\ndf_cat = df_cat.apply(le.fit_transform)\n#perform one hot encoding on categorical columns \n#df_cat_dum = pd.get_dummies(df_cat.astype(str))\n#Concatenate dummy columns, numeric and the target (dependent variable) as a final dataframe\ndf_merge = pd.concat([df_cat,df_num],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"542e45ef4f3076226ff5fc5d6e7f3f91c5f6e756"},"cell_type":"code","source":"def score_model(model):\n    model.fit(Xtrain,ytrain)\n    ypred = model.predict(Xtest)\n    return mean_squared_error(ytest,ypred)\n\ndef pred_model(model):\n    model.fit(df_train,dy_train)\n    ypred = model.predict(df_test)\n    return ypred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f5f02184bfdbe12365205a1bc341da2c5139a5a6"},"cell_type":"code","source":"droplist = ['first_active_month','card_id']#,'auth','auth_n','AuthRateY', 'AuthRateN', 'cat1', 'RatioCat1', 'cat2', 'AvgCat2',\n#       'cat3', 'AvgCat3', 'AvgPurchAmt','merchant_count']\ndf_merged = df_merge.drop(droplist,axis=1)\ndf_train = df_merged.iloc[:len(df_train), :]\ndf_test  = df_merged.iloc[len(df_train):, :]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5e6c9b5c4c4647b7fd2603909ebbd708c813146d"},"cell_type":"code","source":"Xtrain,Xtest,ytrain,ytest = train_test_split(df_train,dy_train,test_size=0.2,random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fcda7ef2de5d9b6241b676140df7c87539545845"},"cell_type":"code","source":"LinReg = LinearRegression(n_jobs=6)\nnp.sqrt(score_model(LinReg))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7ae9c91a9b19d3f905d6cd1cca191642effe19e5"},"cell_type":"code","source":"BasRig = BayesianRidge()\nnp.sqrt(score_model(BasRig))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"22c01e3998cb8c5a9cdbc05eb9eea6c7a820a06e"},"cell_type":"code","source":"ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.5, random_state=3))\nnp.sqrt(score_model(ENet))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b4e2d1418e69843e60b1f2a7f833fde4549d303b"},"cell_type":"code","source":"lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))\nnp.sqrt(score_model(lasso))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6b96bf485690da3ba42728fc3f626bacafd743b2"},"cell_type":"code","source":"df_train.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fd1ef9ae4a80e5f9867ae9506522934cd6a20d21"},"cell_type":"code","source":"cols_to_use =['feature_1', 'feature_2', 'feature_3', 'purchase_amount_y_min',\n       'purchase_amount_y_max', 'purchase_amount_y_mean',\n       'purchase_amount_y_sum', 'authorized_flag_y_count', 'numerical_1_y_sum',\n       'numerical_2_y_sum', 'authorized_flag_n_count', 'purchase_amount_n_min',\n       'purchase_amount_n_max', 'purchase_amount_n_mean', 'numerical_1_n_sum',\n       'numerical_2_n_sum', 'category_1_y_count', 'category_1_n_count',\n       'category_2_1_count', 'category_2_2_count', 'category_2_3_count',\n       'category_2_4_count', 'category_2_5_count', 'category_3_A_count',\n       'category_3_B_count', 'category_3_C_count', 'merchant_id_nunique',\n       'subsector_id_nunique', 'state_id_nunique', 'installments_sum',\n       'purchase_amount_min', 'purchase_amount_max', 'purchase_amount_mean',\n       'numerical_1_min', 'numerical_1_max', 'numerical_1_sum',\n       'numerical_1_mean', 'numerical_2_min', 'numerical_2_max',\n       'numerical_2_sum', 'numerical_2_mean', 'auth', 'AuthRateY', 'AuthRateN',\n       'AvgAuthPurY', 'cat1', 'RatioCat1', 'cat2', 'AvgCat2', 'cat3',\n       'AvgCat3', 'AvgPurchAmt', 'YearElapsed', 'MonthElapsed', 'YearActive',\n       'MonthActive', 'days_from_purchase_first', 'days_from_purchase_last',\n       'first_purchase_year', 'first_purchase_month',\n       'first_purchase_wkofyear', 'first_purchase_hour', 'last_purchase_year',\n       'last_purchase_month', 'last_purchase_wkofyear', 'last_purchase_hour',\n       'purchase_amount_y_meanDiff', 'purchase_amount_y_sumDiff',\n       'purchase_amount_y_minDiff', 'purchase_amount_minDiff']\n\ndef run_lgb(train_X, train_y, val_X, val_y, test_X):\n    params = {'num_leaves': 31,\n         'min_data_in_leaf': 30, \n         'objective':'regression',\n         'max_depth': -1,\n         'learning_rate': 0.01,\n         \"min_child_samples\": 20,\n         \"boosting\": \"gbdt\",\n         \"feature_fraction\": 0.9,\n         \"bagging_freq\": 1,\n         \"bagging_fraction\": 0.9 ,\n         \"bagging_seed\": 11,\n         \"metric\": 'rmse',\n         \"lambda_l1\": 0.1,\n         \"verbosity\": -1,\n         \"nthread\": 4,\n         \"random_state\": 2018}\n    \n    lgtrain = lgb.Dataset(train_X, label=train_y)\n    lgval = lgb.Dataset(val_X, label=val_y)\n    evals_result = {}\n    model = lgb.train(params, lgtrain, 1000, valid_sets=[lgval], early_stopping_rounds=100, verbose_eval=100, evals_result=evals_result)\n    \n    pred_test_y = model.predict(test_X, num_iteration=model.best_iteration)\n    return pred_test_y, model, evals_result\n\ntrain_X = df_train[cols_to_use]\ntest_X = df_test[cols_to_use]\ntrain_y = dy_train.values\n\npred_test = 0\nmodval = 4.0\nkf = model_selection.KFold(n_splits=5, random_state=2018, shuffle=True)\nfor dev_index, val_index in kf.split(df_train):\n    dev_X, val_X = train_X.loc[dev_index,:], train_X.loc[val_index,:]\n    dev_y, val_y = train_y[dev_index], train_y[val_index]\n    \n    pred_test_tmp, model, evals_result = run_lgb(dev_X, dev_y, val_X, val_y, test_X)\n    m = float(str(model.best_score).split(':')[2].split('}')[0])\n    print(m)\n    if (m < modval):\n        pred_test = pred_test_tmp\n        modval = m\n    \n#pred_test /= 5.\n#np.sqrt(score_model(GBoost))\nprint(\"model value :\",modval)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6e673d1e5cab09e9d5f9f3305b8461dfaf4d1bad"},"cell_type":"code","source":"pred_df = pd.DataFrame({'card_id':test_card_id})\npred_df['target'] = pred_test #pred_model(BasRig)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a9c2f5c6b57064e8d29b457ca03293a335c6e8e3"},"cell_type":"code","source":"pred_df.to_csv('prediction001.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8e1e9077f7a5e31428fe8f4187d472f89716624a"},"cell_type":"code","source":"pred_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3f08c30bc37eaf26ce87bd064d77bdc9c94e90bf"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}