{"cells":[{"metadata":{"_uuid":"f05215689bbfb841106e90a86124aea095a4f64b","trusted":false,"collapsed":true},"cell_type":"code","source":"# Import all required libraries\nimport pandas\nfrom pandas import *\nimport numpy\nfrom datetime import datetime\n\n# Initialize values\n\nenv = None\nsample_size = None\n# Set env, if env = test, will only be run locally and display the result\nenv = \"prod\"\n#env = \"test\"\n\nenable_scaler=True\nnb_models = 10\n\nnb_year_min = 1\nnb_year_max = 0\n\nnb_week_min = 15\nnb_week_max = 0\n# Number of value on which to train, if null, train on all value\n#sample_size = 120000","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2eea0a1844c03de4f333e052968dd08d9fbbaae9","trusted":false,"collapsed":true},"cell_type":"code","source":"# Read training data + test data\ndf_data = pandas.read_csv(\"../input/train.csv\")\ndf_test = pandas.read_csv(\"../input/test.csv\")\n\n\n# Display basic information\ndisplay(df_data.head(5))\nprint(df_data.describe())\ndf_data.columns\n\ndf_test.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"523a14194b4587841a86b7e0ca3cf8a18df647df","trusted":false,"collapsed":true},"cell_type":"code","source":"if env == \"test\":\n    # Take the last 3 months of 2017 as testing data\n    df_test = df_data[df_data.date >= '2017-10-01']\n    # Remove the last 3 months, as it would not be fair to train on those\n    df_train = df_data[df_data.date < '2017-10-01']\n    if nb_week_max > 0 or nb_year_max >0:\n        df_train = df_tain[df_train.date >= '2015-01-01']\n    df_train = df_train[df_train.date >= '2016-01-01']\nelse:\n    if nb_week_max > 0 or nb_year_max >0:\n        df_train = df_data[df_data.date >= '2015-01-01']\n    df_test['sales'] = 0\n\n# Only select a small sample, faster local testing\nif sample_size is not None and sample_size > 0:\n    df_train = df_train.sample(sample_size)\n    \ndisplay(df_train.head(5))\ndf_test.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c90f4fb02a65187a6ae4010b30852025139e6322","trusted":false,"collapsed":true},"cell_type":"code","source":"# Add time to df_data\nfrom datetime import timedelta\nfrom dateutil.relativedelta import relativedelta\n\n# This can be quite slow, but is important for next steps\n\nif nb_week_max > 0 or nb_year_max >0:\n    df_data = df_data.assign(time = df_data.date.apply(lambda x: datetime.strptime(x, \"%Y-%m-%d\")))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9b13df3e18b3c9818fa8a30c9f4b0d1a0217b198","trusted":false,"collapsed":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0b629ec37c5b0212453c33c77c98c4009c331178","trusted":false,"collapsed":true},"cell_type":"code","source":"# Let's add the value for week 15 to 20\nfor i in range(nb_week_min, nb_week_min+nb_week_max):\n    print(\"Adding weeks\")\n    df_data['weekplus{0}'.format(i)] = (df_data['time'] + timedelta(days=i*7)).astype(str)\n\nfor i in range(nb_year_min, nb_year_min+nb_year_max):\n    print(\"Adding years\")\n    df_data['yearplus{0}'.format(i)] = (df_data['time'] + pandas.DateOffset(years=i)).astype(str)\n\ndf_data.head(5)\ndf_test.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"29ef26673c0804f713fdd409e52bf0b776af0388","trusted":false,"collapsed":true},"cell_type":"code","source":"# First preparation step, adding some values to both dataframe\ndef prepare_initial(df, df_data, nb_year_max = 2, nb_week_max = 5):\n    for i in range(nb_year_min, nb_year_min+nb_year_max):\n        df = df.merge(df_data[['yearplus{0}'.format(i), 'store', 'item', 'sales']], how='inner', \n                           left_on=['date', 'store', 'item'],\n                           right_on=['yearplus{0}'.format(i), 'store', 'item'],\n                           suffixes=('', '_saleminus{0}year'.format(i)))\n        \n        df['sales_saleminus{0}year'.format(i)] = df['sales_saleminus{0}year'.format(i)].fillna(0)\n\n        df = df.drop(columns=['yearplus{0}'.format(i)])\n\n\n    for i in range(nb_week_min, nb_week_min+nb_week_max):\n        df = df.merge(df_data[['weekplus{0}'.format(i), 'store', 'item', 'sales']], how='inner', \n                           left_on=['date', 'store', 'item'],\n                           right_on=['weekplus{0}'.format(i), 'store', 'item'],\n                           suffixes=('', '_saleminus{0}week'.format(i)))\n        \n        df['sales_saleminus{0}week'.format(i)] = df['sales_saleminus{0}week'.format(i)].fillna(0)\n        \n        df = df.drop(columns=['weekplus{0}'.format(i)])\n\n    print(\"Start\")\n    return df\n   \n\ndf_train = prepare_initial(df_train, df_data, nb_year_max, nb_week_max)\ndf_test = prepare_initial(df_test, df_data, nb_year_max, nb_week_max)\n\nif env != \"test\":\n    # Sometimes, we have duplicates, so they should be removed:\n    df_test = df_test.drop_duplicates(subset='id', keep=\"last\")\ndf_train.sample(5)\ndf_test.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"762762a6551f3deb6cca76ae205068e144cea85b","trusted":false,"collapsed":true},"cell_type":"code","source":"# Second preparation step\n# We need to add all might be useful information from df_test and df_train\n# Extracting some variable from date\n\ndef prepare_data(df, df_data):\n    # Add time column, easier for later step\n    df = df.assign(time = df.date.apply(lambda x: datetime.strptime(x, \"%Y-%m-%d\")))\n    \n    # nb days since the beginning of the data, as traffic grows by store\n    df = df.assign(days = df.time.apply(lambda x: (x - datetime(2012,12,31)).days))\n    \n    # Week day should be used \n    df = df.assign(weekday = df.time.apply(lambda x: x.weekday()))\n    \n    df = df.assign(dom = df.time.apply(lambda x: x.day))\n    \n    df = df.assign(cw = df.time.apply(lambda x: x.isocalendar()[1]))\n\n    df = df.assign(month = df.time.apply(lambda x: x.month))\n    \n    df = pandas.get_dummies(df, prefix=['store', 'item', 'dom', 'cw', 'weekday', 'month'], \n                            columns=['store', 'item', 'dom', 'cw', 'weekday', 'month'])\n    \n    # Should we give more information? Like nb sales previous X weeks?\n    # Previous sales of last 2/3 year at the same date (bank holiday, black friday, etc.)\n    # Probably yes as this information is available, would be a good try\n    return df\n\ndf_train = prepare_data(df_train, df_data)\ndf_test = prepare_data(df_test, df_data)\n\n# Initialize column that do not exist in test with value 0, to avoid dummy not creating enough columns\n# For instance, non existing month like month_5 will not exist in test set\nfor train_col in df_train.columns.values:\n    if train_col not in df_test.columns.values:\n        df_test[train_col] = 0\n\n# Also train the other way around\nfor test_col in df_test.columns.values:\n    if test_col not in df_train.columns.values:\n        df_train[test_col] = 0\n        \ndisplay(df_train.head(5))\ndisplay(df_test.head(5))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1320e2724e73ad72b485a4b2d861275bfdf8306c","trusted":false,"collapsed":true},"cell_type":"code","source":"# Generate our training/validation datasets\nfrom sklearn import model_selection\n\n# Name of the result column\nresult_cols = ['sales']\nresult_excl_cols = 'sales_'\n# Removing input_cols = ['store', 'item',\n# dom, cw, \n\n# best model contained:\n# days, store, item, weekday, month, cw, dom\ninput_cols = [\n#    'sales_',\n    'store_',\n    'item_',\n    #'day', always out\n    #'day_',\n    #'weekday', always out\n    'dom_',\n    'cw_',\n    'weekday_',\n    #'month', always out\n    'month_',\n    \n    'days'\n]\n\n# Get the final values\ndef get_values(df, cols=[], excl_cols = \"doqwidjoqwidjqwoidjqwoidjqwodijqw\"):\n    columns = df.columns.values\n    # Remove all columns that are not inside the list\n    for column in columns:\n        find = False\n        if column.startswith(excl_cols):\n            print(\"Ignoring {0}\".format(column))\n        else:\n            for col in cols:\n                if column.startswith(col):\n                    find = True\n        if not find:\n            df = df.drop(columns=[column])\n    new_order = sorted(df.columns.values)\n    print(new_order)\n    # Same order for both training and testing set\n    df = df[new_order]\n    return df.values\n\nX_train = get_values(df_train, input_cols)\nY_train = get_values(df_train, result_cols, result_excl_cols).ravel()\nX_test = get_values(df_test, input_cols)\n\n# In test env, we calculate it for the test only\nif env == \"test\":\n    Y_test = get_values(df_test, result_cols, result_excl_cols).ravel()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2c8a28e85b62c20287ed57b8f2e6e392986fe6d5","trusted":false,"collapsed":true},"cell_type":"code","source":"df_test.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"77d430fb964ba8c4ec83a8c7793b25713016c0ba","trusted":false,"collapsed":true},"cell_type":"code","source":"# Normalize the data\n\n\nX_all = [x + y for x, y in zip(X_train, X_test)]\n#print(len(X_all))\nfrom sklearn.preprocessing import StandardScaler  \nscaler = StandardScaler() \n\n# Don't cheat - fit only on training data\n# Def adding x_train + X_test + X_validation to fit all of them\n\nif enable_scaler:\n    scaler.fit(X_train)  \n\n    X_train = scaler.transform(X_train) \n    X_test = scaler.transform(X_test) ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"568b66f82315df3c1fb72c70b6e4611722534f22","trusted":false,"collapsed":true},"cell_type":"code","source":"# Custom function to calculate the SMAPE\ndef get_smape(Y_validation, Y_validation_predict):\n    result = 0\n    for i in range(0, len(Y_validation)):\n        result += (abs(Y_validation[i] - Y_validation_predict[i]))/(abs(Y_validation[i])+abs(Y_validation_predict[i]))\n    return result / len(Y_validation) * 200","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4c846a7f3498e2d1f8bf4b44d3c1cdea782521a9","trusted":false,"collapsed":true},"cell_type":"code","source":"# Import algorithm\nfrom sklearn import model_selection\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import *\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.neural_network import MLPRegressor\n\nmodels = []\n\nfor i in range(5, 5 + nb_models):\n    models.append(('MLPRegressor_adam_{0}'.format(i), MLPRegressor(hidden_layer_sizes=(8,),  activation='relu', solver='adam',\n                                                                   alpha=0.0001, batch_size='auto',\n    learning_rate='constant', learning_rate_init=0.001, power_t=0.5, max_iter=1000, shuffle=False,\n    random_state=i, tol=0.0001, verbose=False, warm_start=False, nesterovs_momentum=True,\n    beta_1=0.9, beta_2=0.999, epsilon=1e-08)))\n    \n    #models.append(('MLPRegressor_adam_{0}'.format(i), MLPRegressor(hidden_layer_sizes=(8,),  activation='relu', solver='adam',\n    #                                                               alpha=0.001, batch_size='auto',\n    #learning_rate='constant', learning_rate_init=0.01, power_t=0.5, max_iter=1000, shuffle=True,\n    #random_state=i, tol=0.0001, verbose=False, warm_start=False, momentum=0.9, nesterovs_momentum=True,\n    #early_stopping=False, validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08)))\n    \n\n# High value until first model get solved\nmax_score = 10000\nbest_model = \"UNKNOWN\"\n\nres = []\n# Testing all models, one by one\nfor name, model in models:\n    print(\"Executing for model {0}\".format(name))\n    time_start = datetime.now()\n\n    # Training the model\n    model.fit(X_train, Y_train)\n    \n    print(\"Finish fit for {0}\".format(name))\n\n    Y_test_result = model.predict(X_test)\n    res.append(Y_test_result)\n    if env == \"test\":\n        # We can calculate the avg error\n        score = get_smape(Y_test, Y_test_result)\n        print(\"Model {0} got score of {1}, time: {2}\".format(name, score, datetime.now() - time_start))\n    #else:\n        # Let's write an output file, with the name of the model\n        #print(\"Writing output file {0}.csv for model {0}\".format(name))\n        \n        #df_test['sales'] = Y_test_result\n        #result_df = df_test[['id', 'sales']]\n        #result_df['sales'] = Y_test_result\n        \n        #result_df.to_csv(\"{0}.csv\".format(name), index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6719e24c241741dd88ff2c199e592fcf0fe565e6","trusted":false,"collapsed":true},"cell_type":"code","source":"# For all result in res, if test, display the result, if not, write it to a file\nfinal_res = []\nnb_variable = len(res[0])\nfor variable in range(0, nb_variable):\n    final_res.append(0.0)\n    for i in range(0, len(res)):\n        final_res[variable] += res[i][variable]\n    final_res[variable] = final_res[variable] / len(res)\n\nif env == \"test\":\n    # We can calculate the avg error\n    score = get_smape(Y_test, final_res)\n    print(\"avg model got score of {0}\".format(score))\nelse:\n    print(\"Writing output file merged.csv\".format(name))\n\n    df_test['sales'] = final_res\n    result_df = df_test[['id', 'sales']]\n    result_df['sales'] = final_res\n\n    result_df.to_csv(\"merged.csv\".format(name), index=False)\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"nbformat":4,"nbformat_minor":1}