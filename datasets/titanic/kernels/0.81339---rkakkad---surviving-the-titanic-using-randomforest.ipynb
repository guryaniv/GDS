{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "55869677-54ce-c21c-4aa9-8bddb2f7dd2f"
      },
      "source": [
        "# The Problem\n",
        "The sinking of the RMS Titanic after colliding with an icerberg was one of the most tragic incidents in modern history. It killed 1502 of its 2224 passengers and crew. \n",
        "\n",
        "In this problem we have been provided data about each of the passengers and whether they survived or not. Our problem is to analyze the data and build a model to predict which type of passengers will survive the incident. The accuracy of the model will be measured based on the accuracy of the predicted result of passengers relative to the truth. \n",
        "\n",
        "This is a  classification problem in which we need to build a model to classify a passenger as likely surviver or not. We will be building a randomforrest model. We will broadly following the following steps\n",
        "1. Set Up: Import the libraries & Load the datasets\n",
        "2. Data Exploration: Analyze the fields in the dataset and build hypothesis around how they can be used\n",
        "3. Feature Engineering: Setting up our features in line with the data exploration results\n",
        "4. Set the hyperparameters\n",
        "5. Build the model and the prediction\n",
        "Let's get started! "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "af38527c-b3b2-6c32-3780-8a2954cc6aab"
      },
      "outputs": [],
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load in \n",
        "\n",
        "# Importing the libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import csv\n",
        "import collections\n",
        "import os\n",
        "import seaborn as sns\n",
        "\n",
        "# Input data files are available in the \"../input/\" directory.\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
        "\n",
        "from subprocess import check_output\n",
        "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n",
        "\n",
        "# Any results you write to the current directory are saved as output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "3b8b1c83-f036-769e-2e6a-199c15785aa7"
      },
      "source": [
        "# Set up the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "2e30ba9e-c763-ad86-2fa5-4dc462d48e55"
      },
      "outputs": [],
      "source": [
        "# Importing the dataset\n",
        "dataset_train = pd.read_csv('../input/train.csv')\n",
        "dataset_test = pd.read_csv('../input/test.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "b8a36089-5000-5a90-6d86-bbb15fd94277"
      },
      "source": [
        "# Data Exploration\n",
        "We will understand the overall structure of our dataset and the dive in to each of the key parameters. At a broad level we try to answer key questions about each parameter like\n",
        "\n",
        "- Is this likely to be an important predictor?\n",
        "- What is the coverage of this data point (ie how many values are populated). How should we populate blank values?\n",
        "- What is the quality of the field - is it a clean field or a dirty field & if dirty - what cleaning can be performed?\n",
        "- Can any parameters be derived from this field that may be good predictors.\n",
        "- How is our dataset distributed across the fiedl. \n",
        "\n",
        "As a process we would like to explore all our data fields first before manipulating our data. Let's get started."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "9bc92449-f3dc-be38-52df-31b8b1e7fb24"
      },
      "outputs": [],
      "source": [
        "# Get info on training dataset\n",
        "dataset_train.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "e2677b68-e652-8431-02aa-449e92b8ba84"
      },
      "source": [
        "Training Data Set: We have 891 observations in total with 12 columns. Our target variable is \"Survived\". Of these the Age parameter has only 714 entries, Cabin has only 204 &  Embarked has 889. We will need to explore filling the missing values in these parameters. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ec029f9c-011f-13c1-3c0c-08a6980d86a6"
      },
      "outputs": [],
      "source": [
        "# Get info on testing dataset\n",
        "dataset_test.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "9587cc72-89ed-279e-58f9-6aa5c56d47d1"
      },
      "source": [
        "Testing Data Set: The testing data set has 418 entries, i.e. testing data as a % of the total sample is 418/(418 + 891) = 32%. This is on the higher side, as we usually use about 20-25% of a sample for testing, however in this case we take this a given and do not change the datasets. The testing data set has gaps in Age, Cabin and Fare fields. Let's move on..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b1707d41-b32a-5973-346a-c60a31ce8534"
      },
      "outputs": [],
      "source": [
        "# checking the column names & sample data for dataset_train\n",
        "dataset_train.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "7246c808-3ba0-edf6-efef-97f5dbd78665"
      },
      "outputs": [],
      "source": [
        "# Passenger ID \n",
        "dataset_train.PassengerId.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "c0d878bc-cd70-cfa9-bc48-7e9e0c56765a"
      },
      "source": [
        "This is an index that increments by 1 for every passenger. Since we do not know how this field has been derived, we cannot formulate hypotheses for how it may impact Survival. However, if this field is related to some real world phenomenon such as if it is provided in order of making a reservation then it would mean that the lower indexes are for the earliest booked passengers - which may be correlated with passengers being more planned in their lives v/s the passengers with later indexes which were booked late or last minute. Since we are not sure if this will impact us - I tried leaving it in the base model - but it did not show as a important factor - hence decided to drop it. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "36b86b35-a4b8-be7b-a896-5011a4ac71e7"
      },
      "outputs": [],
      "source": [
        "# Data Exploration - Survived\n",
        "dataset_train.Survived.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "a6488974-4b23-245d-284b-7e4b1f3b36b3"
      },
      "outputs": [],
      "source": [
        "dataset_train[\"Survived\"].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "062996f4-3ed9-ac07-2024-5d1a0897baac"
      },
      "outputs": [],
      "source": [
        "dataset_train[\"Survived\"].value_counts(normalize = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "4858d201-24f0-8632-2093-bca297f5dc9f"
      },
      "source": [
        "Survived is our target variable. It is a binary field with 0 - indicating did not survive and 1 - indicating survive. We can see from our data that ~62% people died & 38% survived. These are good benchmarks to keep in mind going forward. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b1877092-a53a-8297-8ab1-9853110e6199"
      },
      "outputs": [],
      "source": [
        "# Data Exploration - Pclass\n",
        "dataset_train[\"Pclass\"].value_counts(normalize = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "a59b9617-24aa-eddf-6ab9-f7c4154294db"
      },
      "source": [
        "Passenger class shows that 24% passengers are in first class, 21% second class and 55% in 3rd class. Lets see how survival is correlated with these. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "3f02addb-794e-231f-6f43-473bbb640999"
      },
      "outputs": [],
      "source": [
        "dataset_train[\"Survived\"].groupby(dataset_train[\"Pclass\"]).mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "ea394c6c-1e99-e11d-a36d-3edb6e7a8ce1"
      },
      "source": [
        "This is interesting. We can see that Passengers in 1st class have a 62% survival rate compared to 38% benchmark. Similarly 3rd class passengers have only a 24% survival rate. This is a good variable to keep in our model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "77868fbf-5561-9e8e-cac1-7e3d3733fa59"
      },
      "outputs": [],
      "source": [
        "# Data exploration - Name\n",
        "dataset_train[\"Name\"].describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "efc78c91-91ea-fd97-ed51-f2d6a82d4490"
      },
      "source": [
        "The field Name has 891 unique values. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "5e8ed665-b746-2314-6866-fb95d9085483"
      },
      "outputs": [],
      "source": [
        "dataset_train[\"Name\"].head(20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "cc122644-3874-f334-daca-6a668593bd48"
      },
      "source": [
        "Name is a dirty field (as is most often the case) and is unusable as-is, but we can glean potentially useful information from the titles embedded in the name. We see that every name  starts with surname followed by comma and the title which ends in a dot. So we can split the name to get a new column called title - which could impact survival rates.\n",
        "\n",
        "Upon analyzing the data we also find that there are some titles with lots of occurences like Mr., Mrs. etc and there are lots of titles with single occurences. During the feature engineering - we will club the singleton titles in to 1 group."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "3927774d-737b-b080-01bd-92731a5cc4b2"
      },
      "outputs": [],
      "source": [
        "# Data exploration - Sex\n",
        "dataset_train[\"Survived\"].groupby(dataset_train[\"Sex\"]).mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "6ed5ff17-9b10-953a-5b95-63688df74007"
      },
      "source": [
        "We see a higher survival rate among women, we can keep this field as is."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "5777c914-9ded-8aac-6293-3f0cde13b8d2"
      },
      "outputs": [],
      "source": [
        "# Data exploration Age\n",
        "dataset_train[\"Survived\"].groupby(dataset_train[\"Age\"].isnull()).count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "23a15091-b794-e21d-69ec-9f65cfd6a9f0"
      },
      "source": [
        "About 177 passengers do not have an age. Let's see if this impacts survival rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "a8ddc299-0063-a91e-0cb1-506b0b87c614"
      },
      "outputs": [],
      "source": [
        "# Check if null age impacts survival rate\n",
        "dataset_train[\"Survived\"].groupby(dataset_train[\"Age\"].isnull()).mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "7840718b-fe18-bc5b-2146-fbc1f97a2a58"
      },
      "source": [
        "Passengers with null age have only 29% survival rate. We can impute the age by taking the mean of age for the group of passengers having the same class and title as the passenger with a null age. This would give us a reasonable approximation. Note as is standard practice - we take the averages of values from the training data set for the testing dataset too."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "65996670-0223-25b4-9cd8-91e53d609c9b"
      },
      "outputs": [],
      "source": [
        "# Data exploration - SibSp\n",
        "dataset_train[\"SibSp\"].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "1bd87454-a227-9834-0b2e-3febcb73b98c"
      },
      "outputs": [],
      "source": [
        "dataset_train[\"Survived\"].groupby(dataset_train[\"SibSp\"]).mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "2e0b7cbe-15f0-c8d4-8ecf-3b08060e8da8"
      },
      "source": [
        "Having siblings or spouse on board could improve ones chances of survival. We see that people with greater than 2 siblings or spouses have a significantly lower rate of survival.\n",
        "Let's move on"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "a29f4aae-3e0a-0547-b5f5-4fd2bebe4cde"
      },
      "outputs": [],
      "source": [
        "# Data exploration - Parch\n",
        "dataset_train[\"Parch\"].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "9dcbdede-3dc7-0221-5ad5-05c5922f3c0f"
      },
      "outputs": [],
      "source": [
        "dataset_train[\"Survived\"].groupby(dataset_train[\"Parch\"]).mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "dca75a51-ea41-2790-249f-eb5686712635"
      },
      "source": [
        "Data gets really thin in the Parch > 2 region...so we should not read too much in to variations in that range. For now since it stands to reason that having parents or children on board will impact ones survival rate ... we will keep it in the mix. \n",
        "\n",
        "In initial versions of my model I had kept the SibSp and Parch fields as separate predictors. However, they were both very weak hence I combined the two in to a single predictor called family size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "4f0c92a3-3b3c-a999-919d-6dd2c1918d73"
      },
      "outputs": [],
      "source": [
        "# Data exploration - Ticket\n",
        "dataset_train[\"Ticket\"].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "f1491765-06a1-cef5-7fd5-177a84b5a1ac"
      },
      "source": [
        "While the ticket field is dirty as provided - an interesting observation is that there are group tickets and individual tickets...ie some tickets have as many as 7 passengers. This would indiciate a group traveling together which may impact survival rates. We can append the group ticke flag and see if it has predictive value. Further, ticket letters and numbers could be indicative of the location on the ship - however we have not used those in this model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "00a2f30e-6030-0891-61cb-abdbd8aa7f7d"
      },
      "outputs": [],
      "source": [
        "# Data exploration - Fare\n",
        "dataset_train.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "bd9d6330-52e6-97c9-386c-e6b214cac85f"
      },
      "source": [
        "Fare should be a factor in survival as it will likely determine the location the seat of the passenger, the place they were when the iceberg struck, etc...This should also be highly correlated with the passenger class and may be even correlated with the port of embarkment. Let's take a look."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "cf3f4fb2-d800-3382-f879-84624e83a356"
      },
      "outputs": [],
      "source": [
        "# Fare pentiles\n",
        "pd.qcut(dataset_train[\"Fare\"], 5).value_counts(sort = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "97ed1a79-1b02-dd24-b51c-554a895c3404"
      },
      "outputs": [],
      "source": [
        "# Checking fare correlation with passenger class\n",
        "pd.crosstab(pd.qcut(dataset_train[\"Fare\"], 5), columns = dataset_train[\"Pclass\"] )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "302d627d-fa87-6e1e-9ee5-1fd2957eaa92"
      },
      "outputs": [],
      "source": [
        "# Checking fare correlation with port of embarkment\n",
        "pd.crosstab(pd.qcut(dataset_train[\"Fare\"], 4), columns = dataset_train[\"Embarked\"]).apply(lambda r: r/r.sum(), axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "c967c0aa-2b52-51e0-13c4-c54c09157044"
      },
      "source": [
        "While there is a correlation between ticket price and port of embarkment, across the board most passengers have boarded from port S. Let's see the correlation of ticket fare with survival rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "9685bdf5-e79e-7e2e-c725-91cf0b7a14a5"
      },
      "outputs": [],
      "source": [
        "# Correlation of ticket fare with survival rate\n",
        "dataset_train[\"Survived\"].groupby(pd.qcut(dataset_train[\"Fare\"], 5)).mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "21c6bd8e-dc58-3e6a-1726-66c5e3d066ee"
      },
      "source": [
        "We see a strong correlation between fare and survival rate so we will keep this predictor. Let's move to the next variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "3484eef9-5c69-1c1f-76ca-551a9ca16d1e"
      },
      "outputs": [],
      "source": [
        "# Data exploration - Cabin\n",
        "dataset_train[\"Cabin\"].describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "dd319e14-de8c-5892-5b46-f391578964dc"
      },
      "outputs": [],
      "source": [
        "dataset_train[\"Cabin\"].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "583dcf77-956e-ea09-ecb6-0f270846abd4"
      },
      "outputs": [],
      "source": [
        "# Frequency count of Cabin values\n",
        "dataset_train[\"Cabin\"].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d532eec8-df24-e6f5-5e9c-87f0b9ca0c5b"
      },
      "outputs": [],
      "source": [
        "# Checking if null cabin impacts survival\n",
        "dataset_train[\"Survived\"].groupby(dataset_train[\"Cabin\"].isnull()).mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "97a39400-fc25-7a1c-ab73-59b6af5b9ac2"
      },
      "source": [
        "So we see that a passenger with a null cabin has a far lower chance of survival. Also, similar to ticket, some cabins have many passengers which could impact the rate of survival. So let's add the 3 group categories - Group Cabin, single cabin or \"Not present\" in the feature engineering stage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "3c02d1ff-c524-917f-49e8-e384b9929e7e"
      },
      "outputs": [],
      "source": [
        "# Data exploration - Embarked\n",
        "dataset_train[\"Embarked\"].describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "d9d9c308-3bdc-ac13-c2af-98d2c84acf7b"
      },
      "source": [
        "Embarked is the port from where the passenger has embarked. It has 2 missing values. While S is the most common port we should check other values ot see if there's a better way to update the port of embarkment. Let's check the fare of these tickets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "fea2d463-9e16-8277-e80d-454e791a3364"
      },
      "outputs": [],
      "source": [
        "# Checking the ticket of passengers with empty embarkment\n",
        "print(dataset_train.loc[dataset_train[\"Embarked\"].isnull()])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "1781fa9a-e16e-0199-5e2f-b2f718de66ca"
      },
      "source": [
        "These passengers are traveling together on a ticket worth $80. They are also first class passengers - lets check teh correlation between class and port. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "e2bf4dee-df7a-39c1-fe3c-f4363163e0af"
      },
      "outputs": [],
      "source": [
        "# checking correlation between class and port\n",
        "pd.crosstab(dataset_train[\"Pclass\"], columns = dataset_train[\"Embarked\"]).apply(lambda r: r/r.sum(), axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "76da666f-2ff3-4d21-2e05-5333af9ca44b"
      },
      "source": [
        "Passengers of Class 1 mostly came from port S so we will add that to the embarked port for these passengers. Ok lets get started with feature Engineering."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "001ccc5b-2a45-61b2-3408-572657bad2aa"
      },
      "source": [
        "# Feature Engineering - Function Definition\n",
        "We will create a series of functions to modify our data as discussed in the data exploration stage. This will allow us to modify our training and test datasets simultaneously and in the same mannner. It is risky to separate the manipulation of testing and training datasets as there is chance of errors creeping in."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ae87b9ba-d73b-ce64-6163-0c2116811578"
      },
      "outputs": [],
      "source": [
        "# Deleting the passengerID\n",
        "del dataset_train[\"PassengerId\"]\n",
        "del dataset_test[\"PassengerId\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "bf6de378-78b9-15f8-156f-283a58021cbd"
      },
      "outputs": [],
      "source": [
        "# Function for extracting titles and removing the Name Column\n",
        "def titles(dataset_train, dataset_test):\n",
        "    for i in [dataset_train, dataset_test]:\n",
        "        i[\"Title\"] = i[\"Name\"].apply(lambda x: x.split(',')[1]).apply(lambda x: x.split()[0])\n",
        "        del i[\"Name\"]\n",
        "    return dataset_train, dataset_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ae336a21-4979-67bb-9e4d-39091140b6b2"
      },
      "outputs": [],
      "source": [
        "# Function for removing the low incidence titles and bucketing them in to others\n",
        "def titleGroups(dataset_train, dataset_test):\n",
        "    for i in [dataset_train, dataset_test]:\n",
        "        i.loc[i[\"Title\"] == \"Col.\",[\"Title\"]] = \"Other\" \n",
        "        i.loc[i[\"Title\"] == \"Major.\",[\"Title\"]] = \"Other\" \n",
        "        i.loc[i[\"Title\"] == \"Mlle.\",[\"Title\"]] = \"Other\" \n",
        "        i.loc[i[\"Title\"] == \"Ms.\",[\"Title\"]] = \"Miss.\" \n",
        "        i.loc[i[\"Title\"] == \"Sir.\",[\"Title\"]] = \"Mr.\" \n",
        "        i.loc[i[\"Title\"] == \"Capt.\",[\"Title\"]] = \"Other\" \n",
        "        i.loc[i[\"Title\"] == \"Lady.\",[\"Title\"]] = \"Mrs.\" \n",
        "        i.loc[i[\"Title\"] == \"Don.\",[\"Title\"]] = \"Other\" \n",
        "        i.loc[i[\"Title\"] == \"the\",[\"Title\"]] = \"Other\" \n",
        "        i.loc[i[\"Title\"] == \"Mme.\",[\"Title\"]] = \"Other\" \n",
        "        i.loc[i[\"Title\"] == \"Jonkheer.\",[\"Title\"]] = \"Other\" \n",
        "    return dataset_train, dataset_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "90c2f1e4-43d5-d380-a7ce-5e56c07a7f7f"
      },
      "outputs": [],
      "source": [
        "# Function to fill missing age values in the dataset\n",
        "def fillAges(dataset_train, dataset_test):\n",
        "    for i in [dataset_train, dataset_test]:\n",
        "        data = dataset_train.groupby(['Title', 'Pclass'])['Age']\n",
        "        i['Age'] = data.transform(lambda x: x.fillna(x.mean()))\n",
        "    return dataset_train, dataset_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "52a171b0-eab6-7c8d-e8a2-39084b83711a"
      },
      "outputs": [],
      "source": [
        "# Function to convert siblings and parch to family size\n",
        "def familySize(dataset_train, dataset_test):\n",
        "    for i in [dataset_train, dataset_test]:\n",
        "        i[\"FamilySize\"] = np.where((i[\"SibSp\"]+i[\"Parch\"]) == 0 , \"Single\", np.where((i[\"SibSp\"]+i[\"Parch\"]) <= 3,\"Small\", \"Big\"))\n",
        "        del i[\"SibSp\"]\n",
        "        del i[\"Parch\"]\n",
        "    return dataset_train, dataset_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "94ec29b7-afb7-c7bd-89cb-aaaa5d5f3440"
      },
      "outputs": [],
      "source": [
        "# Function to append ticketCounts to dataset & delete ticket\n",
        "def ticketCounts(dataset_train, dataset_test):\n",
        "    for i in [dataset_train, dataset_test]:\n",
        "        i[\"TicketCount\"] = i.groupby([\"Ticket\"])[\"Title\"].transform(\"count\")\n",
        "        del i[\"Ticket\"]\n",
        "    return dataset_train, dataset_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c10dd75b-3d25-9f6f-0b1c-2734abcbc3e8"
      },
      "outputs": [],
      "source": [
        "# Fill the na Fares with mean of fares from the set.\n",
        "dataset_train['Fare'].fillna(dataset_train['Fare'].mean(), inplace = True)\n",
        "dataset_test['Fare'].fillna(dataset_train['Fare'].mean(), inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "36fdcd32-34ea-a5b5-4562-151002f9c8b1"
      },
      "outputs": [],
      "source": [
        "# Function to add Cabin count flag\n",
        "def cabinCount(dataset_train, dataset_test):\n",
        "    for i in [dataset_train, dataset_test]:\n",
        "        i[\"CabinCount\"] = i.groupby([\"Cabin\"])[\"Title\"].transform(\"count\")\n",
        "        del i[\"Cabin\"]\n",
        "    return dataset_train, dataset_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "3992b8b3-12be-f82f-6e12-6d77cd438dd0"
      },
      "outputs": [],
      "source": [
        "# Function to convert cabinCount to cabinType Flag\n",
        "def cabinCountFlag(dataset_train, dataset_test):\n",
        "    for i in [dataset_train, dataset_test]:\n",
        "        i[\"CabinType\"] = \"Missing\"\n",
        "        i.loc[i[\"CabinCount\"] == 1,[\"CabinType\"]] = \"Single\" \n",
        "        i.loc[i[\"CabinCount\"] == 2,[\"CabinType\"]] = \"Double\" \n",
        "        i.loc[i[\"CabinCount\"] >= 3,[\"CabinType\"]] = \"ThreePlus\" \n",
        "        del i[\"CabinCount\"]\n",
        "    return dataset_train, dataset_test "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "00ae6467-1411-62bf-2473-0e85b1ee4c20"
      },
      "outputs": [],
      "source": [
        "# Function to fill the missing values of Embarked\n",
        "def fillEmbarked(dataset_train, dataset_test):\n",
        "    for i in [dataset_train, dataset_test]:\n",
        "        i[\"Embarked\"] = i[\"Embarked\"].fillna(\"S\")\n",
        "    return dataset_train, dataset_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "cc45072e-4329-34cd-a544-297ccc706480"
      },
      "outputs": [],
      "source": [
        "# Encoding our categorical variables as dummy variables to ensure scikit learn works\n",
        "def dummies(dataset_train, dataset_test, columns = [\"Pclass\", \"Sex\", \"Embarked\",\"Title\",\"TicketCount\",\"CabinType\",\"FamilySize\"]):\n",
        "    for column in columns:\n",
        "        dataset_train[column] = dataset_train[column].apply(lambda x: str(x))\n",
        "        dataset_test[column] = dataset_test[column].apply(lambda x: str(x))\n",
        "        good_cols = [column+'_'+i for i in dataset_train[column].unique() if i in dataset_test[column].unique()]\n",
        "        dataset_train = pd.concat((dataset_train, pd.get_dummies(dataset_train[column], prefix = column)[good_cols]), axis = 1)\n",
        "        dataset_test = pd.concat((dataset_test, pd.get_dummies(dataset_test[column], prefix = column)[good_cols]), axis = 1)\n",
        "        del dataset_train[column]\n",
        "        del dataset_test[column]\n",
        "    return dataset_train, dataset_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "d6548e53-bd37-b2ca-3af4-9106e36dbe1d"
      },
      "source": [
        "# Feature engineering - Running the functions\n",
        "Now that our functions are set up - let's run our datasets through them and get the final datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "6f5673ac-fbcd-02a0-e51b-6dc838a82a3e"
      },
      "outputs": [],
      "source": [
        "dataset_train, dataset_test = titles(dataset_train, dataset_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "167718f2-ec6b-0f3c-7eff-f9c992c9aadb"
      },
      "outputs": [],
      "source": [
        "dataset_train, dataset_test = titleGroups(dataset_train, dataset_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b2e5b1d5-0cfd-49e2-d4b2-ae436d917557"
      },
      "outputs": [],
      "source": [
        "dataset_train, dataset_test = fillAges(dataset_train, dataset_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "2af76491-3479-6254-7eb0-4639aafeb571"
      },
      "outputs": [],
      "source": [
        "dataset_train, dataset_test = familySize(dataset_train, dataset_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "698604e4-307b-8dc8-72b4-f5522154887a"
      },
      "outputs": [],
      "source": [
        "dataset_train, dataset_test = ticketCounts(dataset_train, dataset_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "336a04a3-4e7b-ee07-d481-8aa040d4d27e"
      },
      "outputs": [],
      "source": [
        "dataset_train, dataset_test = cabinCount(dataset_train, dataset_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "263c62eb-3cb0-243e-5f0b-5d19b313d5f2"
      },
      "outputs": [],
      "source": [
        "dataset_train, dataset_test = cabinCountFlag(dataset_train, dataset_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "0bed5a9a-ebb9-f3c5-eaeb-535a66b91c1c"
      },
      "outputs": [],
      "source": [
        "dataset_train, dataset_test = fillEmbarked(dataset_train, dataset_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "5e136761-691a-cbc1-0d4b-c1b0500e83cf"
      },
      "outputs": [],
      "source": [
        "dataset_train, dataset_test = dummies(dataset_train, dataset_test,columns = [\"Pclass\", \"Sex\", \"Embarked\",\"Title\",\"TicketCount\",\"CabinType\", \"FamilySize\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "c89fe078-dc7c-6ec0-1b80-04ab92e9a56e"
      },
      "source": [
        "# Checking the final datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "6cc96c7f-3d2e-076f-8de6-aab8e9023617"
      },
      "outputs": [],
      "source": [
        "dataset_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "1ceaf31a-5ceb-7df2-8434-a5924ac8f608"
      },
      "outputs": [],
      "source": [
        "dataset_train.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "85968c01-7438-9b63-42aa-65b2a08207ab"
      },
      "outputs": [],
      "source": [
        "dataset_test.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "586f189a-4ecc-6c1b-c96d-59c1954962c4"
      },
      "outputs": [],
      "source": [
        "dataset_test.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "6e0c8933-bfea-f890-3d02-350519ef9f33"
      },
      "source": [
        "# Building the model\n",
        "We will start building our random forrest model by setting up our optimal hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "4ab9ee4d-ac01-0098-005e-cfe2ac68b581"
      },
      "outputs": [],
      "source": [
        "# Fitting Random Forest Classification to the Training set\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "classifier = RandomForestClassifier(max_features='auto', \n",
        "                                oob_score=True,\n",
        "                                random_state=1,\n",
        "                                n_jobs=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "818bf3db-db66-59d1-dd6b-d8f7aa6cdca8"
      },
      "outputs": [],
      "source": [
        "# Creating the Grid Search Parameter list\n",
        "parameters = { \"criterion\"   : [\"gini\", \"entropy\"],\n",
        "             \"min_samples_leaf\" : [1, 5, 10],\n",
        "             \"min_samples_split\" : [12, 16, 20, 24],\n",
        "             \"n_estimators\": [100, 400, 700]}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "8732c83c-4aea-0793-e566-d6e7aef9586a"
      },
      "outputs": [],
      "source": [
        "# Setting up the gridSearch to find the optimal parameters\n",
        "gridSearch = GridSearchCV(estimator=classifier,\n",
        "                  param_grid=parameters,\n",
        "                  scoring='accuracy',\n",
        "                  cv=10,\n",
        "                  n_jobs=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "077b21be-5fb6-69ef-1d5f-1dacbcb686e4"
      },
      "outputs": [],
      "source": [
        "# Getting the optimal grid search parameters\n",
        "gridSearch = gridSearch.fit(dataset_train.iloc[:, 1:], dataset_train.iloc[:, 0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "50d3db20-8715-f41a-ae4c-5c11146fc468"
      },
      "outputs": [],
      "source": [
        "# Printing the out of bag score and the best parameters values\n",
        "print(gridSearch.best_score_)\n",
        "print(gridSearch.best_params_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "fd407ab3-e4ca-f9ae-a25b-82c7e645738a"
      },
      "outputs": [],
      "source": [
        "# building the random forrest classifier\n",
        "classifier = RandomForestClassifier(criterion='entropy', \n",
        "                             n_estimators=100,\n",
        "                             min_samples_split=16,\n",
        "                             min_samples_leaf=1,\n",
        "                             max_features='auto',\n",
        "                             oob_score=True,\n",
        "                             random_state=1,\n",
        "                             n_jobs=-1)\n",
        "classifier.fit(dataset_train.iloc[:, 1:], dataset_train.iloc[:, 0])\n",
        "print(\"%.5f\" % classifier.oob_score_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "bdef5633-2c2f-df65-6021-86b9fd944bd6"
      },
      "outputs": [],
      "source": [
        "# Creating the list of important features\n",
        "pd.concat((pd.DataFrame(dataset_train.iloc[:, 1:].columns, columns = ['variable']), \n",
        "           pd.DataFrame(classifier.feature_importances_, columns = ['importance'])), \n",
        "          axis = 1).sort_values(by='importance', ascending = False)[:20]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "d455623d-36ee-d0cb-a725-6a471a260863"
      },
      "source": [
        "From the factor importance we can see that a number of the factors we built have become important predictors, most notable Title Mr. is the 3rd most important factor, CabinType_Missing & FamilySize both turned out important as well!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "2ba9d2d2-bf61-7fab-290f-d2cdb0d99c70"
      },
      "outputs": [],
      "source": [
        "# Making the predictions on the test set\n",
        "predictions = classifier.predict(dataset_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ca9748a0-8f76-901b-661b-8ffc044c8707"
      },
      "outputs": [],
      "source": [
        "# Making the predictions file for submission\n",
        "predictions = pd.DataFrame(predictions, columns=['Survived'])\n",
        "passengerIds = pd.read_csv('../input/test.csv')\n",
        "predictions = pd.concat((passengerIds.iloc[:, 0], predictions), axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "935dd5a7-845a-0d7a-5f84-3ac4d3dc88a8"
      },
      "outputs": [],
      "source": [
        "# To save our results to a csv locally\n",
        "predictions.to_csv('predictions.csv', index = False)"
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}