{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\n\n\nfrom keras.datasets import mnist\nfrom keras.datasets import cifar10\nfrom keras.layers import Input, Dense, Reshape, Flatten, Dropout\nfrom keras.layers import BatchNormalization, Activation, ZeroPadding2D\nfrom keras.layers.advanced_activations import LeakyReLU\nfrom keras.layers.convolutional import UpSampling2D, Conv2D\nfrom keras.models import Sequential, Model\nfrom keras.optimizers import Adam\nfrom keras import initializers\nimport os\nimport skimage.transform\nfrom skimage import data, io, filters\nimport numpy as np\nfrom numpy import array\nfrom skimage.transform import rescale, resize\nfrom keras.layers.convolutional import Conv2D, Conv2DTranspose\nprint(os.listdir(\"../\"))\n\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1d1416c23e422b37fb57d76b36f492f407cf93f6"},"cell_type":"code","source":"print(os.mkdir(\"../output\"))\nprint(os.listdir(\"../\"))\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"def load_path(path):\n    directories = []\n    for elem in os.listdir(path):\n        if os.path.isdir(os.path.join(path,elem)):\n            directories = directories + load_path(os.path.join(path,elem))\n            directories.append(os.path.join(path,elem))\n    return directories","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6ddea98eaa46ef644d21b5424a2d0708ff4e72db"},"cell_type":"code","source":"np.random.seed(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c25aa0465e27541ca936866fe292acf7fd806046"},"cell_type":"code","source":"def load_data(dirs, ext):\n    files = []\n    file_names = []\n    count = 0\n    for d in dirs:\n        for f in os.listdir(d): \n            if f.endswith(ext):\n                files.append(data.imread(os.path.join(d,f)))\n                file_names.append(os.path.join(d,f))\n                count = count + 1\n    return files\n            ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d6f57dc125f739af20bcd36fe28b8ba076281e82"},"cell_type":"code","source":"files = load_data(load_path(\"../input/anime-dataset/faces/\"), \".jpg\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ee2009a8af2efd358f729963bb929ebe43fe317a"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c7e2854c211613171fc35c75068c266f8c9f6727"},"cell_type":"code","source":"\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fe446c121642f27b1fc9b8bba092792bcd357ac1"},"cell_type":"code","source":"def res_block(model, kernal_size, filters, strides):\n    \n    gen = model\n    \n    model = Conv2D(filters = filters, kernel_size = kernal_size, strides = strides, padding = \"same\")(model)\n    model = BatchNormalization(momentum = 0.5)(model)\n    model = LeakyReLU(0.2)(model)\n    model = Conv2D(filters = filters, kernel_size = kernal_size, strides = strides, padding = \"same\")(model)\n    model = BatchNormalization(momentum = 0.5)(model)\n        \n    model = keras.layers.add([gen, model])\n    \n    return model\n    \n    \ndef block(model, kernal_size, filters, strides):\n    model = Conv2D(filters = filters, kernel_size = kernal_size, strides = strides, padding = \"same\")(model)\n    #shape = [model.shape[0].value, model.shape[1].value, model.shape[2].value, model.shape[3].value]\n    #model = Utills.SubpixelConv2D(shape, 2)(model)\n    model = BatchNormalization(momentum = 0.5)(model)\n    model = LeakyReLU(alpha = 0.2)(model)\n    \n    return model\n\n\ndef discriminator_block(model, filters, kernel_size, strides):\n    \n    model = Conv2D(filters = filters, kernel_size = kernel_size, strides = strides, padding = \"same\")(model)\n    model = BatchNormalization(momentum = 0.5)(model)\n    model = LeakyReLU(alpha = 0.2)(model)\n    \n    return model\n\n\nclass Generator(object):\n\n    def __init__(self, noise_shape):\n        self.noise_shape = noise_shape\n\n    def generator(self):\n\t    gen_input = Input(shape = self.noise_shape)\n\t    \n\t    model = Conv2DTranspose(filters = 512, kernel_size = 4, strides = 1, padding = \"same\")(gen_input)\n\t    model = BatchNormalization(momentum = 0.5)(model)\n\t    model = LeakyReLU(alpha = 0.2)(model)\n\t    \n\t    model = Conv2DTranspose(filters = 256, kernel_size = 4, strides = 2, padding = \"same\")(model)\n\t    model = BatchNormalization(momentum = 0.5)(model)\n\t    model = LeakyReLU(alpha = 0.2)(model)\n\t    \n\t    model = Conv2DTranspose(filters = 128, kernel_size = 4, strides = 2, padding = \"same\")(model)\n\t    model = BatchNormalization(momentum = 0.5)(model)\n\t    model = LeakyReLU(alpha = 0.2)(model)\n\t    \n\t    model = Conv2DTranspose(filters = 64, kernel_size = 4, strides = 2, padding = \"same\")(model)\n\t    model = BatchNormalization(momentum = 0.5)(model)\n\t    model = LeakyReLU(alpha = 0.2)(model)\n\t    \n\t    \n\t    model = Conv2DTranspose(filters = 64, kernel_size = 3, strides = 1, padding = \"same\")(model)\n\t    model = BatchNormalization(momentum = 0.5)(model)\n\t    model = LeakyReLU(alpha = 0.2)(model)\n\t    \n\t    model = Conv2DTranspose(filters = 3, kernel_size = 4, strides = 2, padding = \"same\")(model)\n\t    \n\t    model = Activation('tanh')(model)\n\t    \n\t    generator_model = Model(inputs = gen_input, outputs = model)\n\t    \n\t    return generator_model\n\n\nclass Discriminator(object):\n\n    def __init__(self, image_shape):\n        self.image_shape = image_shape\n    \n    def discriminator(self):\n        dis_input = Input(shape = self.image_shape)\n        \n        model = discriminator_block(dis_input, 64, 4, 2)\n        \n        model = discriminator_block(model, 128, 4, 2)\n        \n        model = discriminator_block(model, 256, 4, 2)\n\n        model = discriminator_block(model, 512, 4, 2)\n       \n        model = Flatten()(model)\n        model = Dense(1)(model)\n        model = Activation('sigmoid')(model) \n        \n        generator_model = Model(inputs = dis_input, outputs = model)\n        \n        return generator_model\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2dacda46756506cfb522581af0da28c289426b7c"},"cell_type":"code","source":"def SubpixelConv2D(input_shape, scale=4):\n    def subpixel_shape(input_shape):\n        dims = [input_shape[0],input_shape[1] * scale,input_shape[2] * scale,int(input_shape[3] / (scale ** 2))]\n        output_shape = tuple(dims)\n        return output_shape\n    \n    def subpixel(x):\n        return tf.depth_to_space(x, scale)\n        \n    return Lambda(subpixel, output_shape=subpixel_shape)\n    \ndef load_path(path):\n    directories = []\n    for elem in os.listdir(path):\n        if os.path.isdir(os.path.join(path,elem)):\n            directories = directories + load_path(os.path.join(path,elem))\n            directories.append(os.path.join(path,elem))\n    return directories\n    \ndef load_data_from_dirs(dirs, ext):\n    files = []\n    file_names = []\n    count = 0\n    for d in dirs:\n        for f in os.listdir(d): \n            if f.endswith(ext):\n                files.append(data.imread(os.path.join(d,f)))\n                file_names.append(os.path.join(d,f))\n                count = count + 1\n    return files     \n            \ndef load_data_from_dirs_resize(dirs, ext, size):\n    files = []\n    file_names = []\n    count = 0\n    for d in dirs:\n        for f in os.listdir(d): \n            if f.endswith(ext):\n                files.append(resize(data.imread(os.path.join(d,f)), size))\n                file_names.append(os.path.join(d,f))\n                count = count + 1\n    return files     \n                        \n          \ndef load_data(directory, ext):\n\n    files = load_data_from_dirs(load_path(directory), ext)\n    return files\n    \ndef load_data_as_array(directory, ext):\n\n    files = load_data_from_dirs(load_path(directory), ext)\n    files = array(files)\n    return files\n    \ndef load_data_resize(directory, ext, size):\n\n    files = load_data_from_dirs_resize(load_path(directory), ext, size)\n    return files\n    \ndef load_data_as_array_resize(directory, ext, size):\n\n    files = load_data_from_dirs_resize(load_path(directory), ext, size)\n    files = array(files)\n    return files\n\ndef normalize(input_data):\n\n    input_data = (input_data.astype(np.float32) - 127.5)/127.5\n    return input_data\n    \ndef denormalize(input_data):\n\n    input_data = (input_data + 1) * 127.5\n    return input_data.astype(np.uint8) \n    \n    \n\n\t","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"539aef8f55a2340aee8469b0e856d5c49d7fd072"},"cell_type":"code","source":"image_shape = (96,96,3)\nnoise_shape = (6,6,100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ad2db551296bdc1d92560d9a4c3313f76b00f261"},"cell_type":"code","source":"\n\nx_train = files[:8000]\nx_train = array(x_train)\nx_train = normalize(x_train)\nplt.imshow(x_train[134])\nx_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c8a60ac2c6705cba595e348df1e3bcaf8ec205ac"},"cell_type":"code","source":"Gen = Generator(noise_shape) \ngenerator = Gen.generator()\ngenerator.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f62989b57470700f90f367f1a88dcb9e8220cf12"},"cell_type":"code","source":"def get_gan_network(discriminator, shape, generator, optimizer):\n    discriminator.trainable = False\n    gan_input = Input(shape=shape)\n    x = generator(gan_input)\n    gan_output = discriminator(x)\n    gan = Model(inputs=gan_input, outputs=gan_output)\n    gan.compile(loss='binary_crossentropy', optimizer=optimizer)\n    return gan","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8cde6fc781a7d250604ceca1af2fbc113ec0a133"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e6d262caed041df9e45cd8d7414a4ce218ec9757"},"cell_type":"code","source":"def plot_generated_images(epoch, generator, examples=25, dim=(5, 5), figsize=(10, 10)):\n    noise = np.random.normal(0, 1, size=[examples,  6, 6, 100])\n    generated_images = generator.predict(noise)\n    generated_images = generated_images.reshape(examples, 96, 96, 3)\n\n    plt.figure(figsize=figsize)\n    for i in range(generated_images.shape[0]):\n        plt.subplot(dim[0], dim[1], i+1)\n        plt.imshow(generated_images[i])\n        plt.axis('off')\n    plt.tight_layout()\n    plt.show()\n\n    plt.savefig('../output/gan_generated_image_epoch_%d.png' % epoch)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8571f589b755a5477041871cdf6eb59b982e492f"},"cell_type":"code","source":"def train(epochs=4000, batch_size=16):\n    \n    batch_count = int(x_train.shape[0] / batch_size)\n    \n    adam = Adam(lr=0.0002, beta_1=0.5)\n    \n    Dis = Discriminator(image_shape)\n    discriminator = Dis.discriminator()\n    discriminator.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n\n    Gen = Generator(noise_shape) \n    generator = Gen.generator()\n    generator.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n    \n    gan = get_gan_network(discriminator, noise_shape, generator, adam)\n\n    for e in range(1, epochs+1):\n        print ('-'*15, 'Epoch %d' % e, '-'*15)\n        for _ in range(batch_count):\n            noise = np.random.normal(0, 1, size=[batch_size, 6, 6, 100])\n            image_batch = x_train[np.random.randint(0, x_train.shape[0], size=batch_size)]\n\n            generated_images = generator.predict(noise)\n            #X = np.concatenate([image_batch, generated_images])\n\n            #y_dis = np.zeros(2*batch_size)\n            #y_dis[:batch_size] = 1\n            \n            real_data_Y = np.ones(batch_size) - np.random.random_sample(batch_size)*0.2\n            fake_data_Y = np.random.random_sample(batch_size)*0.2\n\n            discriminator.trainable = True\n            #discriminator.train_on_batch(X, y_dis)\n            discriminator.train_on_batch(image_batch, real_data_Y)\n            discriminator.train_on_batch(generated_images, fake_data_Y)\n            \n\n            noise = np.random.normal(0, 1, size=[batch_size,  6, 6, 100])\n            #y_gen = np.ones(batch_size)\n            gan_Y = np.ones(batch_size) - np.random.random_sample(batch_size)*0.2\n            discriminator.trainable = False\n            #gan.train_on_batch(noise, y_gen)\n            gan.train_on_batch(noise, gan_Y)\n            \n\n        if e == 1 or e % 5 == 0:\n            plot_generated_images(e, generator)\n        \n    generator.save('../output/gen_model%d.h5' % e) \n    discriminator.save('../output/dis_model%d.h5' % e) \n    gan.save('../output/gan_model%d.h5' % e) \n    \n    return generator, discriminator, gan\n    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4b0a71c8fa549c9ea31edd289ea0381480cfe159"},"cell_type":"code","source":"generator, discriminator, gan = train(300,64)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"db0d4dc58a4d3ace8c0cd8f46f4654bd64ddfc44"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"43d80ed1b50e71d751c0148c8548d5148684185c"},"cell_type":"code","source":"\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"925d129cab150e37da25a817d6871308c4c6aede"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":false,"trusted":true,"_uuid":"f10fdfc318a6290304cde18e55dec1836368deb6"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"931eeff2f38b2c5f4cec3829fb821af70554bc17"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}