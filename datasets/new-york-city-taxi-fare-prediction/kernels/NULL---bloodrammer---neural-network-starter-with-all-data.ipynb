{"cells":[{"metadata":{"_uuid":"4f7d253239fcee8be6d4237301db7fec68dc2f84"},"cell_type":"markdown","source":"# Starter with Neural Networks\nThere is almost no work with features, I only split the datetime column into 6 columns, one-hot encoded 'passenger_count', extracted order ID from 'key' and used two features from the baseline kernel. The model is flawed and not tuned at all, its only purpose was to make sure that loss goes down no matter what, hence dropout+L2+BN. I almost purposefully made a bunch of mistakes in hope that somebody publicly corrects them.\n\nDespite all that, I achieved 3.95 MSE with 10M samples and 3.83 MSE with all data. There is plenty of work ahead, though."},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"9aaef520e80325dcd8a8b722d57be357fe3d2794"},"cell_type":"code","source":"# Initial Python environment setup...\nimport numpy as np # linear algebra\nimport pandas as pd # CSV file I/O (e.g. pd.read_csv)\nimport os # reading the input files we have access to\n\nfrom tqdm import tqdm\n\nfrom sklearn.preprocessing import StandardScaler\nfrom keras_tqdm import TQDMNotebookCallback","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"382537c3ee86798718d397559af241512b19dd9e"},"cell_type":"code","source":"#features from basic linear model kernel\ndef add_travel_vector_features(df):\n    df['abs_diff_longitude'] = (df.dropoff_longitude - df.pickup_longitude).abs()\n    df['abs_diff_latitude'] = (df.dropoff_latitude - df.pickup_latitude).abs()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d3de21aba974460653a58e3fb389873ff76eef02"},"cell_type":"markdown","source":"# Loading and preprocessing data in its entirety\nI managed to load and preprocess the whole dataset with pandas, but it took ~20 minutes. Again, I'm uploading it so that somebody shows how to do it correctly with, I dunno, Dask. "},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"6a3e28f02f27b273ebe97450fb1986a136388637"},"cell_type":"code","source":"filename = 'train.csv'\ndfs = []\nchunksize = 10 ** 6\nfor chunk in tqdm(pd.read_csv(filename, chunksize=chunksize)):\n    #preprocessing section\n    add_travel_vector_features(chunk)\n    chunk = chunk.dropna(how = 'any', axis = 'rows')\n    chunk = chunk[(chunk.abs_diff_longitude < 5.0) & (chunk.abs_diff_latitude < 5.0)]\n    chunk = chunk[(chunk.passenger_count > 0) & (chunk.passenger_count <= 6)]\n    chunk[['date','time','timezone']] = chunk['pickup_datetime'].str.split(expand=True)\n    chunk[['year','month','day']] = chunk['date'].str.split('-',expand=True).astype('int64')\n    chunk[['hour','minute','second']] = chunk['time'].str.split(':',expand=True).astype('int64')\n    chunk['year_after_0'] = chunk['year'] - np.min(chunk['year'])\n    chunk[['trash', 'order_no']] = chunk['key'].str.split('.',expand=True)\n    chunk['order_no'] = chunk['order_no'].astype('int64')\n    chunk = pd.concat([chunk,pd.get_dummies(chunk['passenger_count'],prefix='pass')], axis =1)\n    chunk = chunk.drop(['timezone','date','time', 'pickup_datetime','trash','key','passenger_count'], axis = 1)\n    #append chunk to the list\n    dfs.append(chunk)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"7c47ee53805734eb29d4eec0f3aaa78f900c4a55"},"cell_type":"code","source":"%%time\n#concatenate all chunk in one big-ass DataFrame\ntrain_df = pd.concat(dfs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"8dad9fccf9d07e689324e3718c281defabbbb830"},"cell_type":"code","source":"#delete the chunks as I only have 16 GB RAM\ndel dfs","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"55663585a864273d0216eded72168eef66076127"},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"0cd6c16d2549289f2962023dcc2521bf5be889ac"},"cell_type":"code","source":"train_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"05a992e50f8af153ef5bfd3f70b5d344f19c178e"},"cell_type":"code","source":"X_train = train_df.drop(['fare_amount'],axis=1)\nY_train = train_df['fare_amount']","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"1022206873eff972325bd13f6dfdd02e2791b35a"},"cell_type":"code","source":"del train_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"a1a9b27b1905447d9cf6ae7719352fa6bef1a54c"},"cell_type":"code","source":"scaler = StandardScaler()\ny_scaler = StandardScaler()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"334167c6ec185a9923afa8712b36d35c90bc8f54"},"cell_type":"code","source":"#scale the data so that columns have zero mean and unit variance\ntrain = scaler.fit_transform(X_train.values)\ny_train =  y_scaler.fit_transform(Y_train.values.reshape(-1,1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"c7ff6e7bfb2818cd8d3154029b1d34e1df4f777d"},"cell_type":"code","source":"del X_train\ndel Y_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"f0bb3eeb22360a83d7f08fd3ae94e82981d48edd"},"cell_type":"code","source":"import keras\nimport tensorflow as tf","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"c78b7cee111fb0fb27ec95d1ab2ea2ac5d24fd64"},"cell_type":"code","source":"#some imports are unnecessary\nfrom keras import layers\nfrom keras.layers import Input, Dropout,Dense, Activation, BatchNormalization\nfrom keras.models import Model, load_model\nfrom keras.initializers import glorot_uniform\nfrom keras.callbacks import ModelCheckpoint,  ReduceLROnPlateau\nfrom keras.regularizers import l2\nfrom keras.optimizers import Adam","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"903817b654aad0aa7ae432f20d39d36219810b73"},"cell_type":"markdown","source":"# Model"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"24e03c4adcb0230e095dd1c292f71499ce0370d3"},"cell_type":"code","source":"model = keras.Sequential([\n    keras.layers.Dense(1024,kernel_initializer = glorot_uniform(),\n              kernel_regularizer = l2(1e-2)),\n    keras.layers.BatchNormalization(),\n    keras.layers.Activation(tf.nn.leaky_relu),\n    keras.layers.Dropout(0.5),\n    keras.layers.Dense(1024,kernel_initializer = glorot_uniform(),\n              kernel_regularizer = l2(1e-2)),\n    keras.layers.BatchNormalization(),\n    keras.layers.Activation(tf.nn.leaky_relu),\n    keras.layers.Dropout(0.5),\n    keras.layers.Dense(1024,kernel_initializer = glorot_uniform(),\n              kernel_regularizer = l2(1e-2)),\n    keras.layers.BatchNormalization(),\n    keras.layers.Activation(tf.nn.leaky_relu),\n    keras.layers.Dropout(0.5),\n    keras.layers.Dense(1024,kernel_initializer = glorot_uniform(),\n              kernel_regularizer = l2(1e-2)),\n    keras.layers.BatchNormalization(),\n    keras.layers.Activation(tf.nn.leaky_relu),\n    keras.layers.Dense(1, activation=tf.nn.leaky_relu)\n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"ac21ef3ce5aeca5b35a9cc68e1306114b74dbac4"},"cell_type":"code","source":"model.compile(optimizer=Adam(5e-4), \n              loss='mean_squared_error')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"57a95c9078d54a9f6bf954940a105243d2cc81af"},"cell_type":"markdown","source":"# Callbacks"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"20ff141bcc075e70500d6beb2e3d5fda71eebece"},"cell_type":"code","source":"filepath = './model_weights/weights-improvement-55M-{epoch:02d}-{val_loss:.4f}.hdf5'\nbest_callback = ModelCheckpoint(filepath, \n                                save_best_only=True)\nlr_sched = ReduceLROnPlateau(monitor='val_loss', factor = 0.2, patience = 5, verbose = 1)\ntqdm_callback = TQDMNotebookCallback(leave_inner=True,metric_format=\"{name}: {value:0.5f}\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"999d867960b261f84aa319814c55dc3ea9c058b2"},"cell_type":"markdown","source":"# Training"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"95da09eeac11c214dfe3a482441b7128650628d8"},"cell_type":"code","source":"history = model.fit(train, y_train, \n          epochs=20,\n          verbose=0,\n          batch_size=2048,\n          validation_split=0.0002,\n          callbacks=[tqdm_callback,best_callback, lr_sched])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d9d494754f297775ebd484ed4be7b7d7183c1496"},"cell_type":"markdown","source":"# Load best result"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"739228764cfe67b53041533f57be28919e5f0d3f"},"cell_type":"code","source":"model.load_weights('./model_weights/weights-improvement-55M-19-0.0471.hdf5')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"511ef0b8909f5364215529fc5c46817f9ab3728f"},"cell_type":"markdown","source":"# Load and preprocess test data"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"ac25bde9f00a5917b666434c269504b02df39d59"},"cell_type":"code","source":"test_df = pd.read_csv('test.csv')\ntest_df.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"a600ad8a0d88e68a638b9cfd20998d42c8b236b9"},"cell_type":"code","source":"key = test_df.key\nadd_travel_vector_features(test_df)\ntest_df[['date','time','timezone']] = test_df['pickup_datetime'].str.split(expand=True)\ntest_df[['year','month','day']] = test_df['date'].str.split('-',expand=True).astype('int64')\ntest_df[['hour','minute','second']] = test_df['time'].str.split(':',expand=True).astype('int64')\ntest_df['year_after_0'] = test_df['year'] - np.min(test_df['year'])\ntest_df[['trash', 'order_no']] = test_df['key'].str.split('.',expand=True)\ntest_df['order_no'] = test_df['order_no'].astype('int64')\ntest_df = pd.concat([test_df,pd.get_dummies(test_df['passenger_count'],prefix='pass')], axis =1)\ntest_df = test_df.drop(['timezone','date','time', 'pickup_datetime','trash','key','passenger_count'], axis = 1)\n# Predict fare_amount on the test set using our model (w) tested on the testing set.\ntest_df.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7c0c4005d5c823dca05f7ae2eef701e004f41736"},"cell_type":"markdown","source":"# Inference and submission"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"055a9b1581bdff1b6bcfcc7493b872e73a9bb416"},"cell_type":"code","source":"test = scaler.transform(test_df.values)\ny_test = model.predict(test)\ny_test = y_scaler.inverse_transform(y_test).reshape(-1)\n# Write the predictions to a CSV file which we can submit to the competition.\nsubmission = pd.DataFrame(\n    {'key': key, 'fare_amount': y_test},\n    columns = ['key', 'fare_amount'])\nsubmission.to_csv('submission_100.csv', index = False)\n\nprint(os.listdir('.'))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f0940b90544b1233d07e762569f6ea92cfbf611c"},"cell_type":"markdown","source":"# What's next\n1. Extract better features.\n2. Choose a better architecture.\n3. Tune the hyperparameters.\n4. Forget all that and resort to XGBoost and ensembling."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"33228044841d46a5311307af13dd5c6e4d5ddc95"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}