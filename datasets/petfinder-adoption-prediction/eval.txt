As we will be switching out test data to re-evaluate kernels on stage 2 data
to populate the private leaderboard, submissions must be named submission.csv
Submissions are scored based on the quadratic weighted kappa, which measures
the agreement between two ratings. This metric typically varies from 0 (random
agreement between raters) to 1 (complete agreement between raters). In the
event that there is less agreement between the raters than expected by chance,
the metric may go below 0. The quadratic weighted kappa is calculated between
the scores which are expected/known and the predicted scores. Results have 5
possible ratings, 0,1,2,3,4. The quadratic weighted kappa is calculated as
follows. First, an N x N histogram matrix O is constructed, such that O
corresponds to the number of adoption records that have a rating of i (actual)
and received a predicted rating j. An N-by-N matrix of weights, w, is
calculated based on the difference between actual and predicted rating scores:
An N-by-N histogram matrix of expected ratings, E, is calculated, assuming
that there is no correlation between rating scores. This is calculated as the
outer product between the actual rating's histogram vector of ratings and the
predicted rating's histogram vector of ratings, normalized such that E and O
have the same sum. From these three matrices, the quadratic weighted kappa is
calculated as: PetID,AdoptionSpeed 378fcc4fc,3 73c10e136,2 72000c4c5,1
e147a4b9f,4 etc..

