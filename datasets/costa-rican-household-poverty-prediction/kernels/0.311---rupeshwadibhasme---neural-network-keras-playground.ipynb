{"cells":[{"metadata":{"_uuid":"6b8014f7d96d0aa4e4da3fcbe61e0cc96d6aa4a6"},"cell_type":"markdown","source":"This is pure keras based implementation of Neural network. Added most popular hyperparameters options available with keras to optimize the Neural Network. There is minimalistic preprocessing is used (Except Filling null values as keras is not happy if input has null values) hence lot many options to imporove."},{"metadata":{"trusted":false,"_uuid":"8dfab591b0e4ac0f40fea9da53535c3335d82abd"},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport sys,os\nimport numpy as np\nfrom sklearn import datasets, preprocessing, metrics, cross_validation\nfrom sklearn.decomposition import PCA\nimport pandas as pd\nfrom keras.models import Sequential\nfrom keras.layers.core import Dense, Activation, Dropout\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom keras.utils import np_utils\nfrom sklearn.preprocessing import LabelEncoder\nfrom keras.callbacks import ModelCheckpoint\nfrom sklearn.pipeline import Pipeline\nfrom keras.callbacks import EarlyStopping \nfrom keras.layers.advanced_activations import PReLU\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.optimizers import SGD,Adam,Adamax,Nadam,Adadelta\nfrom keras.callbacks import ReduceLROnPlateau\nfrom sklearn.preprocessing import MinMaxScaler\n#from keras.regularizers import l1l2\nfrom sklearn.preprocessing import LabelEncoder\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"1e539f5b128753d3404ea9589bfa0a6d7067c4cc"},"cell_type":"code","source":"train_df = pd.read_csv('../input/train.csv')\ntrain_df.fillna(0,axis=1,inplace=True)\ntrain_df.shape\n\n#df1=train_df.loc[train_df['Target'] ==1]\n#df2=train_df.loc[train_df['Target'] ==2]\n#df3=train_df.loc[train_df['Target'] ==3]\n#df4=train_df.loc[train_df['Target'] ==4]    \n\n#rows=int(df1.shape[0]+df2.shape[0]+df3.shape[0])/3\n\n#df4=df4.sample(rows+1000, replace=True)\n\n\n#train_df=pd.concat([df1,df3,df2,df4],axis=0)\ntrain_df = shuffle(train_df)\nprint (train_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"8c34323a3fcc4d562f9c3effeb87ca4d8c79747f"},"cell_type":"code","source":"test_df = pd.read_csv('../input/test.csv')\ntest_df.fillna(0,axis=1,inplace=True)\ntest_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"2e18143ee232634882dafb8f1f2ff00289786d47"},"cell_type":"code","source":"plt.figure(figsize=(12, 5))\nplt.hist(train_df.Target.values, bins=4)\nplt.title('Histogram target counts')\nplt.xlabel('Count')\nplt.ylabel('Target')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"7a518e0fab8f983ca104d4f24f8423ded45ab483"},"cell_type":"code","source":"columns_to_use = train_df.columns[1:-1]\ny = train_df['Target'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"fb08cc1521d2a23a0981c3d5c90cf83129833a87"},"cell_type":"code","source":"train_test_df = pd.concat([train_df[columns_to_use], test_df[columns_to_use]], axis=0)\ntrain_test_df.fillna(0,axis=1,inplace=True)\ncols = [f_ for f_ in train_test_df.columns if train_test_df[f_].dtype == 'object']\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"913bad8288053ed790e2aa650f798ace34d9e1bf"},"cell_type":"code","source":"for col in cols:\n    le = LabelEncoder()\n    le.fit(train_test_df[col].astype(str))\n    train_df[col] = le.transform(train_df[col].astype(str))\n    test_df[col] = le.transform(test_df[col].astype(str))\ndel le\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"cd14ea30c91229700d5ed318633d70884fd50dfa"},"cell_type":"code","source":"train_df.drop(['Id','Target'],axis=1,inplace=True)\ntest_df.drop(['Id'],axis=1,inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"4792a0b51fbfb4ae2359b7f48ce7a0dc1bc5d07a"},"cell_type":"code","source":"scaler = MinMaxScaler()\n\ndf_train_scaled = pd.DataFrame(scaler.fit_transform(train_df), columns=train_df.columns)\ndf_test_scaled = pd.DataFrame(scaler.fit_transform(test_df), columns=test_df.columns)\nprint (df_train_scaled.shape)\nprint (df_test_scaled.shape)\ndf_test_scaled=df_test_scaled.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"2d40b8015d5fa5bd750ec0287a51eed9b0301c06"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_valid, y_train, y_valid = train_test_split(\n       train_df, y, test_size=0.20, random_state=42)\ny_train = np_utils.to_categorical(y_train)  \ny_valid = np_utils.to_categorical(y_valid)  \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"1f4560529858cc625520d3ae1bf0196cebe34e6a"},"cell_type":"code","source":"#Build the model Here , Following is open playground to play on \n\nmodel = Sequential()\n#Base Model\nmodel.add(Dense(64, input_dim=X_train.shape[1], init='uniform', activation='relu'))\nmodel.add(Dropout(0.20))\nmodel.add(BatchNormalization(moving_mean_initializer='zeros',momentum=0.9))\nmodel.add(Dense(32 ,init='uniform', activation='relu'))\nmodel.add(BatchNormalization(moving_mean_initializer='zeros',momentum=0.9))\nmodel.add(Dropout(0.20))\n#model.add(Dense(128, init='uniform', activation='relu'))\n#model.add(BatchNormalization(moving_mean_initializer='zeros',momentum=0.9))\n#model.add(Dropout(0.20))\nmodel.add(Dense(y_train.shape[1], init='uniform', activation='softmax'))\nadam=Adam(lr=1e-3, decay=1e-6)\n\nmodel.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n\nfilepath=\"weights.best.hdf5\"\ncheckpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=True, mode='max')\ncallbacks = [EarlyStopping(monitor='val_loss', patience=3, verbose=0),checkpoint]   \n\nhistory=model.fit(X_train, y_train,batch_size=100,validation_data=(X_valid,y_valid), epochs=50,shuffle=True,callbacks=callbacks)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"bbfe90d1d9f9da50b74db6b142a1f2d5416cfc49"},"cell_type":"code","source":"output=model.predict_proba(df_test_scaled)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"b18a8762b0a1086b758beaa38f506ac6b60b13d1"},"cell_type":"code","source":"predicted_probs = np.argmax(output,axis=1)\npreds=pd.Series(predicted_probs)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"0bcbd1b8363d808fb06acfd30293d941962bfc85"},"cell_type":"code","source":"#Some visualization to see model has any overfitting (Though it cant as this got handled by early stopping criteria)\n\nplt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()\n\n\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('loss')\nplt.ylabel('val_loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"b1c8c454ec43f2feff945d0c3ccb9a65f2fab5d3"},"cell_type":"code","source":"sample_submission = pd.read_csv(\"../input/sample_submission.csv\")\nsample_submission['Target'] = preds\nsample_submission.to_csv('DNN_submission.csv', index=False)\nsample_submission.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}