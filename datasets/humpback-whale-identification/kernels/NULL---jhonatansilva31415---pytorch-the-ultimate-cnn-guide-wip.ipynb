{"cells":[{"metadata":{"_uuid":"83d18201d5182bc88aceb85906c519767390564e"},"cell_type":"markdown","source":"### Creating a CNN with PyTorch\nHere you'll be creating a dataloader with PyTorch and then you'll be making a CNN\n\n#### Some usefull links\n\n* [DATA LOADING AND PROCESSING TUTORIAL](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html)\n* [A detailed example of how to generate your data in parallel with PyTorch](https://stanford.edu/~shervine/blog/pytorch-how-to-generate-data-parallel)\n* [Detailed explanation of the dataloader](https://www.youtube.com/watch?v=myYMrZXpn6U)\n\n*** There are video explanations of this Kernel in my YouTube channel ***\n* [Data Science related content](https://youtube.com/jhonatandasilva?sub_confirmation=1)\n\n\n### Series of videos\n\n1. [Introduction](https://youtu.be/j3n2m61Fxhk)\n2. [How to create the Class](https://www.youtube.com/watch?v=adgKmNlwcdw)\n\nDon't forget to ****[Subscribe](https://youtube.com/jhonatandasilva?sub_confirmation=1)**** to the next video of the series coming next week :D"},{"metadata":{"_uuid":"9bdd0dd4e863476e57cb97c8a998715a2103d1af"},"cell_type":"markdown","source":"### Importing the Libraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"from __future__ import print_function, division\nimport os\nimport pandas as pd\nfrom skimage import io, transform\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\n\n# PyTorch related \nimport torch\nimport torch.nn as nn\nimport torchvision.transforms as transforms, utils\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\n# Ignore warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nplt.ion()   # interactive mode","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8f0cb9248a5817555cb59dfecd3b5d0bb6383562"},"cell_type":"code","source":"df = pd.read_csv('../input/train.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c419295fbeafa5b7846040a17a4f5760462c0e76"},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"111cbc44d4193f7205b76fac0f38076cb89ea40d"},"cell_type":"code","source":"whales_label = np.array(df['Id'])\nwhales_label[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ef7396307930191a5321cea22d12017d6775fbc1"},"cell_type":"code","source":"label_encoder = LabelEncoder()\ninteger_encoded = label_encoder.fit_transform(whales_label)\n\nonehot_encoder = OneHotEncoder(sparse=False)\ninteger_encoded = integer_encoded.reshape(len(integer_encoded), 1)\nonehot_encoded = onehot_encoder.fit_transform(integer_encoded)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bf6e6f7c121b1eee272964a5b742439064fab7c7"},"cell_type":"code","source":"whales_label[:5],onehot_encoded[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c617b0c22a99b3507040118ec109aa9fd8157ba2"},"cell_type":"code","source":"df.iloc[0,1]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"class WhalesDS(Dataset):\n    \"\"\" Humpback Whale Identification Challenge dataset. \"\"\"\n    def __init__(self, csv_file, root_dir, transform=None,test=False):\n        \"\"\"\n        Args:\n            csv_file (string): Path to the csv file with annotations.\n            root_dir (string): Directory with all the images.\n            transform (callable, optional): Optional transform to be applied\n                on a sample.\n        \"\"\"\n        self.labels = []\n        if type(test) == pd.core.frame.DataFrame:\n            self.whales_frame = test\n            labels = np.zeros((5005,))\n        else:\n            self.whales_frame,self.labels = self.one_hot_encoder()\n        self.root_dir = root_dir\n        self.transform = transform\n        \n    def __len__(self):\n        \"\"\" Returns the length of the dataset \"\"\"\n        return len(self.whales_frame)\n\n    def __getitem__(self, idx):\n        \"\"\" Get one record from the dataset \"\"\"\n        img_name = os.path.join(self.root_dir,\n                                self.whales_frame.iloc[idx, 0])\n        image = io.imread(img_name)\n        label = self.labels[idx]\n        sample = {'image': image, 'label': label}\n\n        if self.transform:\n            sample = self.transform(sample)\n\n        return sample\n\n    def one_hot_encoder(self):\n        \"\"\" \n            Got this function from this Kernel, https://www.kaggle.com/pestipeti/keras-cnn-starter\n            chaged a little bit, but the essence is the same \n            from the kernel linked, amazing keras kernel btw if you are reading this :D \n        \"\"\"\n        df = pd.read_csv('../input/train.csv')\n        whales_label = np.array(df['Id'])\n        label_encoder = LabelEncoder()\n        integer_encoded = label_encoder.fit_transform(whales_label)\n\n        onehot_encoder = OneHotEncoder(sparse=False)\n        integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n        onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n        return df,onehot_encoded\n\n    def encode(self):\n        \"\"\" One of the ways to make the encoding \"\"\"\n        df = pd.read_csv('../input/train.csv')\n        unique_classes = pd.unique(df['Id'])\n        encoding = dict(enumerate(unique_classes))\n        encoding = {value: key for key, value in encoding.items()}\n        df = df.replace(encoding)\n        return df ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5e5e9b600dd07feabde046adcbf444dfd37ffba1"},"cell_type":"code","source":"dataset = WhalesDS(csv_file='../input/train.csv',\n                               root_dir='../input/train/',\n                               test=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6f62a2dcb51f4ad7242ee2297227f4313bda6132"},"cell_type":"code","source":"plt.imshow(dataset[100]['image'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2e0cd3f1de82fddbff6efd8efc588d1d263e24ed"},"cell_type":"code","source":"# batch_size, epoch and iteration\nbatch_size = 4\nnum_epochs = 5\nimage_size = 32\nchannels = 3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"790af088acbed805c4a57f49a07498a7b60a8fbd"},"cell_type":"code","source":"class Rescale(object):\n    \"\"\"Rescale the image in a sample to a given size.\n    Args:\n        output_size (tuple or int): Desired output size. If tuple, output is\n            matched to output_size. If int, smaller of image edges is matched\n            to output_size keeping aspect ratio the same.\n    \"\"\"\n\n    def __init__(self, output_size):\n        assert isinstance(output_size, (int, tuple))\n        self.output_size = output_size\n\n    def __call__(self, sample):\n        image, label = sample['image'], sample['label']\n\n        h, w = image.shape[:2]\n        if isinstance(self.output_size, int):\n            if h > w:\n                new_h, new_w = self.output_size * h / w, self.output_size\n            else:\n                new_h, new_w = self.output_size, self.output_size * w / h\n        else:\n            new_h, new_w = self.output_size\n\n        new_h, new_w = int(new_h), int(new_w)\n        img = transform.resize(image, (new_h, new_w))\n\n        return {'image': img, 'label': label}\n\n\nclass RandomCrop(object):\n    \"\"\"Crop randomly the image in a sample.\n\n    Args:\n        output_size (tuple or int): Desired output size. If int, square crop\n            is made.\n    \"\"\"\n\n    def __init__(self, output_size):\n        assert isinstance(output_size, (int, tuple))\n        if isinstance(output_size, int):\n            self.output_size = (output_size, output_size)\n        else:\n            assert len(output_size) == 2\n            self.output_size = output_size\n\n    def __call__(self, sample):\n        image, label = sample['image'], sample['label']\n\n        h, w = image.shape[:2]\n        new_h, new_w = self.output_size\n\n        top = np.random.randint(0, h - new_h)\n        left = np.random.randint(0, w - new_w)\n\n        image = image[top: top + new_h,\n                      left: left + new_w]\n\n        return {'image': image, 'label': label}\n\n\nclass ToTensor(object):\n    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n\n    def __call__(self, sample):\n        image, label = sample['image'], sample['label']\n\n        # swap color axis because\n        # numpy image: H x W x C\n        # torch image: C X H X W\n        \"\"\" The original code didn't expect gray scale images \"\"\"\n        gray_scale_image = torch.zeros([image_size,image_size]).shape == image.shape\n        if gray_scale_image:\n            image = np.stack((image,)*3, axis=-1)\n        image = image.transpose((2, 0, 1))\n        return {'image': torch.from_numpy(image).double(),\n                'label': torch.tensor(label).double()}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1a13e81080b0d0263ccef9d7a3b6a3d8104a77f0"},"cell_type":"code","source":"transformed_dataset = WhalesDS(csv_file='../input/train.csv',\n                                           root_dir='../input/train/',\n                                           transform=transforms.Compose([\n                                               Rescale(int(image_size*1.25)),\n                                               RandomCrop(image_size),\n                                               ToTensor()\n                                           ]),\n                              test=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c1f25cf33d0381808a7cbcdd9adb781b72fdcf92"},"cell_type":"code","source":"transformed_dataset[0]['image'].shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b7c083e8ed1ee1772c3dbf22d603219e5ab2f0ff"},"cell_type":"code","source":"transformed_dataset[0]['label'].shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5d1184f1757b1e7a7c1df2b69477bb19594d331b"},"cell_type":"code","source":"dataloader = DataLoader(transformed_dataset, batch_size=4,\n                        shuffle=True, num_workers=4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ab89f51e561b4664bfa6f365945e0b2b517e8399","scrolled":true},"cell_type":"code","source":"class Whales_CNN(nn.Module):\n    def __init__(self):\n        super(Whales_CNN, self).__init__()\n        # Default stride = 1, padding = 0\n        self.pool = nn.MaxPool2d(2,2)\n        self.dropout = nn.Dropout(0.25)\n        # (In_Channels,Out_channels,Kernel_size) \n        # 3x32x32\n        self.conv1 = nn.Conv2d(3,16,3,padding=1)\n        # 16x16x16\n        self.conv2 = nn.Conv2d(16,32,3,padding=1)\n        # 32x8x8\n        self.conv3 = nn.Conv2d(32,64,3,padding=1)\n        # Dense layer\n        self.fc1 = nn.Linear(64 * 4 * 4, 5005) \n    \n    def forward(self, out):\n        # Conv 1 \n        out = self.pool(F.relu(self.conv1(out)))\n        out = self.pool(F.relu(self.conv2(out)))\n        out = self.pool(F.relu(self.conv3(out)))\n        \n        # Dense Layer \n        out = out.view(out.size(0), -1)\n        out = self.dropout(out)\n        out = self.fc1(out)\n        \n        return out\n\nmodel = Whales_CNN()\n\nerror = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=0.1)\nmodel","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a1bc3e1a01ef5ade7776da0caaf6400000c4919c"},"cell_type":"code","source":"temp = 0\ntotal_iterations = len(dataloader)\nfor epoch in range(3):\n    for i_batch, sample_batched in enumerate(dataloader):\n        images = sample_batched['image']\n        labels = sample_batched['label']\n        train = Variable(images.view(images.shape[0],channels,image_size,image_size)).float()\n        labels = Variable(labels).type(torch.LongTensor)\n        optimizer.zero_grad()\n        outputs = model(train)\n        loss = error(outputs, torch.max(labels, 1)[1])\n        loss.backward()\n        optimizer.step()\n        if temp % 500 == 0:\n            print('Iter {} out of {}'.format(i_batch,total_iterations))\n            temp = 0\n        temp += 1\n    print('Iter {} out of {}'.format(i_batch,total_iterations))\n    print('Loss: {} '.format(loss))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a70c6fabcc386be42b30f9e7639d593c37ca117a"},"cell_type":"markdown","source":"I'm making adjustments to the evaluation part, to make the submission :D "}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}