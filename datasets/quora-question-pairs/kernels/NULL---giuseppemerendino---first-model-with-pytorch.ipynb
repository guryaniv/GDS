{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "da55614b-6dc7-6c53-5b3e-0c7a238c6c6a"
      },
      "source": [
        "First model with pytorch, some ideas taken from https://www.kaggle.com/lystdo/quora-question-pairs/lstm-with-word2vec-embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "fc49a165-8182-f828-d182-a517f846b0ed"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import csv\n",
        "import codecs\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer\n",
        "from string import punctuation\n",
        "\n",
        "import torch\n",
        "import torch.autograd as autograd\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "# Input data files are available in the \"../input/\" directory.\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
        "\n",
        "from subprocess import check_output\n",
        "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n",
        "\n",
        "# Any results you write to the current directory are saved as output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "9c94cb98-6096-9f2c-4eed-42b8f831c32d"
      },
      "outputs": [],
      "source": [
        "# The function \"text_to_wordlist\" is from\n",
        "# https://www.kaggle.com/currie32/quora-question-pairs/the-importance-of-cleaning-text\n",
        "def text_to_wordlist(text, remove_stopwords=False, stem_words=False):\n",
        "    # Clean the text, with the option to remove stopwords and to stem words.\n",
        "    \n",
        "    # Convert words to lower case and split them\n",
        "    text = text.lower().split()\n",
        "\n",
        "    # Optionally, remove stop words\n",
        "    if remove_stopwords:\n",
        "        stops = set(stopwords.words(\"english\"))\n",
        "        text = [w for w in text if not w in stops]\n",
        "    \n",
        "    text = \" \".join(text)\n",
        "\n",
        "    # Clean the text\n",
        "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
        "    text = re.sub(r\"what's\", \"what is \", text)\n",
        "    text = re.sub(r\"\\'s\", \" \", text)\n",
        "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
        "    text = re.sub(r\"can't\", \"cannot \", text)\n",
        "    text = re.sub(r\"n't\", \" not \", text)\n",
        "    text = re.sub(r\"i'm\", \"i am \", text)\n",
        "    text = re.sub(r\"\\'re\", \" are \", text)\n",
        "    text = re.sub(r\"\\'d\", \" would \", text)\n",
        "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
        "    text = re.sub(r\",\", \" \", text)\n",
        "    text = re.sub(r\"\\.\", \" \", text)\n",
        "    text = re.sub(r\"!\", \" ! \", text)\n",
        "    text = re.sub(r\"\\/\", \" \", text)\n",
        "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
        "    text = re.sub(r\"\\+\", \" + \", text)\n",
        "    text = re.sub(r\"\\-\", \" - \", text)\n",
        "    text = re.sub(r\"\\=\", \" = \", text)\n",
        "    text = re.sub(r\"'\", \" \", text)\n",
        "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
        "    text = re.sub(r\":\", \" : \", text)\n",
        "    text = re.sub(r\" e g \", \" eg \", text)\n",
        "    text = re.sub(r\" b g \", \" bg \", text)\n",
        "    text = re.sub(r\" u s \", \" american \", text)\n",
        "    text = re.sub(r\"\\0s\", \"0\", text)\n",
        "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
        "    text = re.sub(r\"e - mail\", \"email\", text)\n",
        "    text = re.sub(r\"j k\", \"jk\", text)\n",
        "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
        "    \n",
        "    # Optionally, shorten words to their stems\n",
        "    if stem_words:\n",
        "        text = text.split()\n",
        "        stemmer = SnowballStemmer('english')\n",
        "        stemmed_words = [stemmer.stem(word) for word in text]\n",
        "        text = \" \".join(stemmed_words)\n",
        "    \n",
        "    # Return a list of words\n",
        "    return(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b2e73d88-5650-8cd6-6d84-5dc9e2168a73"
      },
      "outputs": [],
      "source": [
        "BASE_DIR = '../input/'\n",
        "TRAIN_DATA_FILE = BASE_DIR + 'train.csv'\n",
        "TEST_DATA_FILE = BASE_DIR + 'test.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b05617da-2bd5-4164-3dd3-821312f42d6b"
      },
      "outputs": [],
      "source": [
        "texts_1 = [] \n",
        "texts_2 = []\n",
        "labels = []\n",
        "with codecs.open(TRAIN_DATA_FILE, encoding='utf-8') as f:\n",
        "    reader = csv.reader(f, delimiter=',')\n",
        "    header = next(reader)\n",
        "    for values in reader:\n",
        "        texts_1.append(text_to_wordlist(values[3]))\n",
        "        texts_2.append(text_to_wordlist(values[4]))\n",
        "        labels.append(int(values[5]))\n",
        "print('Found %s texts in train.csv' % len(texts_1))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "e0fcc1b0-be50-fe7b-de05-2026977fa210"
      },
      "outputs": [],
      "source": [
        "texts_1[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "eff538b4-8fad-4f76-a2ee-820712a7d4b0"
      },
      "outputs": [],
      "source": [
        "test_texts_1 = []\n",
        "test_texts_2 = []\n",
        "test_ids = []\n",
        "with codecs.open(TEST_DATA_FILE, encoding='utf-8') as f:\n",
        "    reader = csv.reader(f, delimiter=',')\n",
        "    header = next(reader)\n",
        "    for values in reader:\n",
        "        test_texts_1.append(text_to_wordlist(values[1]))\n",
        "        test_texts_2.append(text_to_wordlist(values[2]))\n",
        "        test_ids.append(values[0])\n",
        "print('Found %s texts in test.csv' % len(test_texts_1))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "f3ee0f2a-c30a-d42b-d30e-065fb819f374"
      },
      "outputs": [],
      "source": [
        "test_texts_1[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "2a2ee6dd-3da2-7d7d-4219-b933fa515fbc"
      },
      "outputs": [],
      "source": [
        "class Dictionary(object):\n",
        "    def __init__(self):\n",
        "        self.word2idx = {}\n",
        "        self.idx2word = []\n",
        "\n",
        "    def add_word(self, word):\n",
        "        if word not in self.word2idx:\n",
        "            self.idx2word.append(word)\n",
        "            self.word2idx[word] = len(self.idx2word) - 1\n",
        "        return self.word2idx[word]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.idx2word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "5ac3a326-5d4e-bdb3-4ecd-4c152481ce64"
      },
      "outputs": [],
      "source": [
        "class Corpus(object):\n",
        "    def __init__(self):\n",
        "        self.dictionary = Dictionary()\n",
        "\n",
        "    def tokenize(self, data):\n",
        "        tokens = 0\n",
        "        for line in data:\n",
        "            words = line.split()\n",
        "            tokens += len(words)\n",
        "            for word in words:\n",
        "                self.dictionary.add_word(word)\n",
        "        print (str(tokens))\n",
        "        ids = torch.LongTensor(tokens)\n",
        "        token = 0\n",
        "        for line in data:\n",
        "            words = line.split()\n",
        "            for word in words:\n",
        "                ids[token] = self.dictionary.word2idx[word]\n",
        "                token += 1\n",
        "\n",
        "        return ids\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "9a24bd2e-0d69-c926-abef-2a4d9867ece1"
      },
      "outputs": [],
      "source": [
        "corpus = Corpus()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "23abdd3f-e5ce-69e8-6e2c-b96a083133c5"
      },
      "outputs": [],
      "source": [
        "token=corpus.tokenize(texts_1 + texts_2 + test_texts_1 + test_texts_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "e5fb4561-084f-6de1-c796-5a852b8515d1"
      },
      "outputs": [],
      "source": [
        "ntokens = len(corpus.dictionary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "14e26f56-575e-c19b-4d85-11ed29576900"
      },
      "outputs": [],
      "source": [
        "ntokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "059d3838-9972-c2ea-e0b7-d146cf18d2ec"
      },
      "outputs": [],
      "source": ""
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}