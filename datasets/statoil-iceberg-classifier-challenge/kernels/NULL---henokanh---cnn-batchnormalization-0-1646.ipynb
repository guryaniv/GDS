{"nbformat_minor": 1, "cells": [{"metadata": {"collapsed": true}, "outputs": [], "execution_count": null, "source": ["import numpy as np\n", "import pandas as pd\n", "import keras as k\n", "from keras.layers import Merge\n", "from keras.layers.normalization import BatchNormalization\n", "from keras.callbacks import ModelCheckpoint,EarlyStopping,ReduceLROnPlateau\n", "from keras.callbacks import History\n", "from keras.layers import Activation\n", "from keras.models import model_from_json\n", "from keras.optimizers import Adam\n", "from matplotlib import pyplot as plt\n", "from scipy.ndimage import rotate as rot\n", "np.random.seed(100)"], "cell_type": "code"}, {"metadata": {"collapsed": true}, "outputs": [], "execution_count": null, "source": ["file_path = '/Users/henok.s.mengistu/Documents/Henok\\'s/Iceberg_challenge/data/processed/train.json'"], "cell_type": "code"}, {"metadata": {"collapsed": true}, "outputs": [], "execution_count": null, "source": ["train = pd.read_json(file_path)"], "cell_type": "code"}, {"metadata": {}, "outputs": [], "execution_count": null, "source": ["print(train.head())\n", "train.shape"], "cell_type": "code"}, {"metadata": {}, "outputs": [], "execution_count": null, "source": ["train[train['inc_angle'] == 'na'].count()"], "cell_type": "code"}, {"metadata": {"collapsed": true}, "outputs": [], "execution_count": null, "source": ["train.inc_angle = train.inc_angle.map(lambda x: 0.0 if x == 'na' else x)"], "cell_type": "code"}, {"metadata": {"collapsed": true}, "outputs": [], "execution_count": null, "source": ["def transform (df):\n", "    images = []\n", "    for i, row in df.iterrows():\n", "        band_1 = np.array(row['band_1']).reshape(75,75)\n", "        band_2 = np.array(row['band_2']).reshape(75,75)\n", "        band_3 = band_1 + band_2\n", "        \n", "        band_1_norm = (band_1 - band_1.mean()) / (band_1.max() - band_1.min())\n", "        band_2_norm = (band_2 - band_2. mean()) / (band_2.max() - band_2.min())\n", "        band_3_norm = (band_3 - band_3.mean()) / (band_3.max() - band_3.min())\n", "        \n", "        images.append(np.dstack((band_1_norm, band_2_norm, band_3_norm)))\n", "    \n", "    return np.array(images)"], "cell_type": "code"}, {"metadata": {"collapsed": true}, "outputs": [], "execution_count": null, "source": ["def augment(images):\n", "    image_mirror_lr = []\n", "    image_mirror_ud = []\n", "    image_rotate = []\n", "    for i in range(0,images.shape[0]):\n", "        band_1 = images[i,:,:,0]\n", "        band_2 = images[i,:,:,1]\n", "        band_3 = images[i,:,:,2]\n", "            \n", "        # mirror left-right\n", "        band_1_mirror_lr = np.flip(band_1, 0)\n", "        band_2_mirror_lr = np.flip(band_2, 0)\n", "        band_3_mirror_lr = np.flip(band_3, 0)\n", "        image_mirror_lr.append(np.dstack((band_1_mirror_lr, band_2_mirror_lr, band_3_mirror_lr)))\n", "        \n", "        # mirror up-down\n", "        band_1_mirror_ud = np.flip(band_1, 1)\n", "        band_2_mirror_ud = np.flip(band_2, 1)\n", "        band_3_mirror_ud = np.flip(band_3, 1)\n", "        image_mirror_ud.append(np.dstack((band_1_mirror_ud, band_2_mirror_ud, band_3_mirror_ud)))\n", "        \n", "        #rotate \n", "        band_1_rotate = rot(band_1, 30, reshape=False)\n", "        band_2_rotate = rot(band_2, 30, reshape=False)\n", "        band_3_rotate = rot(band_3, 30, reshape=False)\n", "        image_rotate.append(np.dstack((band_1_rotate, band_2_rotate, band_3_rotate)))\n", "        \n", "    mirrorlr = np.array(image_mirror_lr)\n", "    mirrorud = np.array(image_mirror_ud)\n", "    rotated = np.array(image_rotate)\n", "    images = np.concatenate((images, mirrorlr, mirrorud, rotated))\n", "    return images"], "cell_type": "code"}, {"metadata": {"collapsed": true}, "outputs": [], "execution_count": null, "source": ["train_X = transform(train)\n", "train_y = np.array(train ['is_iceberg'])\n", "\n", "indx_tr = np.where(train.inc_angle > 0)\n", "print (indx_tr[0].shape)\n", "\n", "train_y = train_y[indx_tr[0]]\n", "train_X = train_X[indx_tr[0], ...]\n", "\n", "train_X = augment(train_X)\n", "train_y = np.concatenate((train_y,train_y, train_y, train_y))\n", "\n", "print (train_X.shape)\n", "print (train_y.shape)"], "cell_type": "code"}, {"metadata": {"collapsed": true}, "outputs": [], "execution_count": null, "source": ["model = k.models.Sequential()\n", "\n", "model.add(k.layers.convolutional.Conv2D(64, kernel_size=(3,3), input_shape=(75,75,3)))\n", "model.add(Activation('relu'))\n", "model.add(BatchNormalization())\n", "model.add(k.layers.convolutional.MaxPooling2D(pool_size=(3,3), strides=(2,2)))\n", "model.add(k.layers.Dropout(0.2))\n", "\n", "model.add(k.layers.convolutional.Conv2D(128, kernel_size=(3, 3)))\n", "model.add(Activation('relu'))\n", "model.add(BatchNormalization())\n", "model.add(k.layers.convolutional.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n", "model.add(k.layers.Dropout(0.2))\n", "\n", "model.add(k.layers.convolutional.Conv2D(128, kernel_size=(3, 3)))\n", "model.add(Activation('relu'))\n", "model.add(BatchNormalization())\n", "model.add(k.layers.convolutional.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n", "model.add(k.layers.Dropout(0.3))\n", "\n", "model.add(k.layers.convolutional.Conv2D(64, kernel_size=(3, 3)))\n", "model.add(Activation('relu'))\n", "model.add(BatchNormalization())\n", "model.add(k.layers.convolutional.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n", "model.add(k.layers.Dropout(0.3))\n", "\n", "model.add(k.layers.Flatten())\n", "\n", "model.add(k.layers.Dense(512))\n", "model.add(Activation('relu'))\n", "model.add(BatchNormalization())\n", "model.add(k.layers.Dropout(0.2))\n", "\n", "model.add(k.layers.Dense(256))\n", "model.add(Activation('relu'))\n", "model.add(BatchNormalization())\n", "model.add(k.layers.Dropout(0.2))\n", "\n", "\n", "model.add(k.layers.Dense(1))\n", "model.add(Activation('sigmoid'))\n", "\n", "mypotim=Adam(lr=0.01, decay=0.0)\n", "model.compile(loss='binary_crossentropy', optimizer = mypotim, metrics=['accuracy'])\n", "\n", "model.summary()"], "cell_type": "code"}, {"metadata": {"collapsed": true}, "outputs": [], "execution_count": null, "source": ["batch_size = 64\n", "early_stopping = EarlyStopping(monitor = 'val_loss', patience = 10, verbose = 0, mode= 'min')\n", "reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor = 0.1, patience = 7, verbose =1, \n", "                                   epsilon = 1e-4, mode='min', min_lr = 0.0001)\n", "model_filepath='/Users/henok.s.mengistu/Documents/Henok\\'s/Iceberg_challenge/weights.best.hdf5'\n", "checkpoint = ModelCheckpoint(model_filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n", "callbacks_list = [early_stopping, checkpoint]"], "cell_type": "code"}, {"metadata": {"scrolled": false, "collapsed": true}, "outputs": [], "execution_count": null, "source": ["history = model.fit(train_X, train_y, batch_size = batch_size, epochs =20, verbose =1, validation_split = 0.1, \n", "          callbacks=callbacks_list)"], "cell_type": "code"}, {"metadata": {}, "outputs": [], "execution_count": null, "source": ["print (history.history.keys())\n", "fig = plt.figure()\n", "plt.plot(history.history['acc'])\n", "plt.plot(history.history['val_acc'])\n", "plt.title('model accuracy')\n", "plt.ylabel('accuracy')\n", "plt.xlabel('epoch')\n", "plt.legend(['train','test'],loc='upper left')\n", "plt.show()"], "cell_type": "code"}, {"metadata": {}, "outputs": [], "execution_count": null, "source": ["plt.plot(history.history['loss'])\n", "plt.plot(history.history['val_loss'])\n", "plt.title('model loss')\n", "plt.ylabel('loss')\n", "plt.xlabel('epoch')\n", "plt.legend(['train', 'test'], loc='upper right')\n", "plt.show()"], "cell_type": "code"}, {"metadata": {"collapsed": true}, "outputs": [], "execution_count": null, "source": ["model_json = model.to_json()\n", "with open(\"/Users/henok.s.mengistu/Documents/Henok\\'s/Iceberg_challenge/model.json\", \"w\") as json_file:\n", "    json_file.write(model_json)"], "cell_type": "code"}, {"metadata": {"collapsed": true}, "outputs": [], "execution_count": null, "source": ["# load json and create model\n", "json_file = open('/Users/henok.s.mengistu/Documents/Henok\\'s/Iceberg_challenge/model.json', 'r')\n", "loaded_model_json = json_file.read()\n", "json_file.close()\n", "loaded_model = model_from_json(loaded_model_json)\n", "# load weights into new model\n", "loaded_model.load_weights('/Users/henok.s.mengistu/Documents/Henok\\'s/Iceberg_challenge/weights.best.hdf5')\n", "print(\"Loaded model from disk\")\n", "loaded_model.compile(loss='binary_crossentropy', optimizer = mypotim, metrics=['accuracy'])"], "cell_type": "code"}, {"metadata": {"collapsed": true}, "outputs": [], "execution_count": null, "source": ["test_file = '/Users/henok.s.mengistu/Documents/Henok\\'s/Iceberg_challenge/data-1/processed/test.json'\n", "test = pd.read_json(test_file)\n", "test.inc_angle = test.inc_angle.replace('na',0)\n", "test_X = transform(test)\n", "print (test_X.shape)"], "cell_type": "code"}, {"metadata": {"collapsed": true}, "outputs": [], "execution_count": null, "source": ["pred_test = loaded_model.predict(test_X, verbose=1)\n", "submission = pd.DataFrame({'id': test[\"id\"], 'is_iceberg': pred_test.reshape((pred_test.shape[0]))})\n", "submission.to_csv('/Users/henok.s.mengistu/Documents/Henok\\'s/Iceberg_challenge/submission.csv', index=False)"], "cell_type": "code"}, {"metadata": {"collapsed": true}, "outputs": [], "execution_count": null, "source": [], "cell_type": "code"}], "metadata": {"language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "name": "python", "pygments_lexer": "ipython3", "version": "3.6.3", "codemirror_mode": {"version": 3, "name": "ipython"}, "file_extension": ".py"}, "kernelspec": {"name": "python3", "language": "python", "display_name": "Python 3"}}, "nbformat": 4}