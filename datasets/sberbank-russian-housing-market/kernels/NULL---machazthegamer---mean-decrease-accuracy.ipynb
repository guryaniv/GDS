{"cells": [{"outputs": [], "source": "getting mean accuracy decrease for features using variety of regressors", "cell_type": "markdown", "execution_count": null, "metadata": {"_uuid": "b4972cffa2d478196d66bfdf4b2576a0ed8a7281", "_execution_state": "idle", "_cell_guid": "7284af26-2931-4da9-a207-52cbd36d3f0f", "collapsed": false}}, {"outputs": [], "source": "import pandas as pd\nimport numpy as np", "cell_type": "code", "execution_count": null, "metadata": {"_uuid": "371ec8fa1edf20cbb8d6fd6d6d0a6c19f64a2845", "_execution_state": "idle", "_cell_guid": "f04dc8bc-336a-4907-b804-aefd439ce532", "trusted": false, "collapsed": false}}, {"outputs": [], "source": "df = pd.read_csv(\"../input/train.csv\")\ndf_x = df.drop(labels=[\"price_doc\"], axis=1)\ntest = pd.read_csv(\"../input/test.csv\")\n\ntrain = df_x\n#data cleaning\nbad_index = train[train.life_sq > train.full_sq].index\ntrain.ix[bad_index, \"life_sq\"] = np.NaN\nequal_index = [601,1896,2791]\ntest.ix[equal_index, \"life_sq\"] = test.ix[equal_index, \"full_sq\"]\nbad_index = test[test.life_sq > test.full_sq].index\ntest.ix[bad_index, \"life_sq\"] = np.NaN\nbad_index = train[train.life_sq < 5].index\ntrain.ix[bad_index, \"life_sq\"] = np.NaN\nbad_index = test[test.life_sq < 5].index\ntest.ix[bad_index, \"life_sq\"] = np.NaN\nbad_index = train[train.full_sq < 5].index\ntrain.ix[bad_index, \"full_sq\"] = np.NaN\nbad_index = test[test.full_sq < 5].index\ntest.ix[bad_index, \"full_sq\"] = np.NaN\nkitch_is_build_year = [13117]\ntrain.ix[kitch_is_build_year, \"build_year\"] = train.ix[kitch_is_build_year, \"kitch_sq\"]\nbad_index = train[train.kitch_sq >= train.life_sq].index\ntrain.ix[bad_index, \"kitch_sq\"] = np.NaN\nbad_index = test[test.kitch_sq >= test.life_sq].index\ntest.ix[bad_index, \"kitch_sq\"] = np.NaN\nbad_index = train[(train.kitch_sq == 0).values + (train.kitch_sq == 1).values].index\ntrain.ix[bad_index, \"kitch_sq\"] = np.NaN\nbad_index = test[(test.kitch_sq == 0).values + (test.kitch_sq == 1).values].index\ntest.ix[bad_index, \"kitch_sq\"] = np.NaN\nbad_index = train[(train.full_sq > 210) & (train.life_sq / train.full_sq < 0.3)].index\ntrain.ix[bad_index, \"full_sq\"] = np.NaN\nbad_index = test[(test.full_sq > 150) & (test.life_sq / test.full_sq < 0.3)].index\ntest.ix[bad_index, \"full_sq\"] = np.NaN\nbad_index = train[train.life_sq > 300].index\ntrain.ix[bad_index, [\"life_sq\", \"full_sq\"]] = np.NaN\nbad_index = test[test.life_sq > 200].index\ntest.ix[bad_index, [\"life_sq\", \"full_sq\"]] = np.NaN\ntrain.product_type.value_counts(normalize= True)\ntest.product_type.value_counts(normalize= True)\nbad_index = train[train.build_year < 1500].index\ntrain.ix[bad_index, \"build_year\"] = np.NaN\nbad_index = test[test.build_year < 1500].index\ntest.ix[bad_index, \"build_year\"] = np.NaN\nbad_index = train[train.num_room == 0].index \ntrain.ix[bad_index, \"num_room\"] = np.NaN\nbad_index = test[test.num_room == 0].index \ntest.ix[bad_index, \"num_room\"] = np.NaN\nbad_index = [10076, 11621, 17764, 19390, 24007, 26713, 29172]\ntrain.ix[bad_index, \"num_room\"] = np.NaN\nbad_index = [3174, 7313]\ntest.ix[bad_index, \"num_room\"] = np.NaN\nbad_index = train[(train.floor == 0).values * (train.max_floor == 0).values].index\ntrain.ix[bad_index, [\"max_floor\", \"floor\"]] = np.NaN\nbad_index = train[train.floor == 0].index\ntrain.ix[bad_index, \"floor\"] = np.NaN\nbad_index = train[train.max_floor == 0].index\ntrain.ix[bad_index, \"max_floor\"] = np.NaN\nbad_index = test[test.max_floor == 0].index\ntest.ix[bad_index, \"max_floor\"] = np.NaN\nbad_index = train[train.floor > train.max_floor].index\ntrain.ix[bad_index, \"max_floor\"] = np.NaN\nbad_index = test[test.floor > test.max_floor].index\ntest.ix[bad_index, \"max_floor\"] = np.NaN\ntrain.floor.describe(percentiles= [0.9999])\nbad_index = [23584]\ntrain.ix[bad_index, \"floor\"] = np.NaN\ntrain.material.value_counts()\ntest.material.value_counts()\ntrain.state.value_counts()\nbad_index = train[train.state == 33].index\ntrain.ix[bad_index, \"state\"] = np.NaN\ntest.state.value_counts()\n\n\ndf_x = train \ncombined = pd.concat([df_x,test], ignore_index=True, axis=0)\nobj_col = combined.select_dtypes(include=[object]).columns\n\nfrom sklearn.preprocessing import LabelEncoder\nfor name in obj_col:\n    if name != \"timestamp\" and name != \"product_type\":\n        print(name)\n        encoder = LabelEncoder()\n        combined[name] = encoder.fit_transform(combined[name].fillna(value=99).values)\n        \n#investment is 1 while occupier 0        \ncombined[\"product_type\"] = combined.product_type.map({\"Investment\":1, \n                                                      \"OwnerOccupier\":0, np.nan:99}).values\n\nfilled= combined.groupby(by=\"sub_area\").fillna(99)\ncombined = filled.merge(combined[[\"id\", \"sub_area\"]], on=\"id\")\n\n#add ratio and age vars\ncombined[\"age\"] = pd.to_datetime(combined[\"timestamp\"]).dt.year - combined.build_year\ncombined[\"ratio\"] = (combined[\"full_sq\"]/combined[\"life_sq\"]).fillna(value=99)\ncombined = combined.replace([-np.inf, np.inf], 99)\n\n\n\n\n\n# add month, monthyear, ratio of green to industry, ofice count over leisure, \n#muslim to christ, ratio of close to far big chuches, mean distance to the art attractions\n#increase in buildings from old_times, ratio of children to youngens, and ratio of young male to female \ncombined[\"timestamp\"] = pd.to_datetime(combined.timestamp)\ncombined = combined.assign(month=combined.timestamp.dt.month)\ncombined = combined.assign(month_year=combined.timestamp.dt.year.astype(str) + combined.month.astype(str))\\\n           .assign(green_industry=combined[\"green_part_3000\"]/combined[\"prom_part_3000\"])\\\n           .assign(work_or_play=combined[\"office_count_1500\"]/(combined[\"sport_count_1500\"]+combined[\"leisure_count_1500\"]))\\\n           .assign(islam_or_christ=combined[\"mosque_count_500\"]/ combined[\"church_count_500\"])\\\n           .assign(church_appeal=combined[\"big_church_count_500\"]/combined[\"big_church_count_1500\"])\\\n           .assign(mean_km_art=combined[[\"museum_km\", \"exhibition_km\", \"catering_km\", \"theater_km\", \"park_km\"]].mean(axis=1))\\\n           .assign(new_to_old_count=combined[\"build_count_after_1995\"]/combined[\"build_count_1971-1995\"])\\\n           .assign(new_to_older_count=combined[\"build_count_after_1995\"]/combined[\"build_count_1946-1970\"])\\\n        .assign(young_to_old=combined[\"0_13_all\"]/combined[\"16_29_all\"])\\\n        .assign(male_to_femal_young=combined[\"young_male\"]/combined[\"young_female\"])\n        \ncombined[\"month_year\"] = combined.month_year.astype(np.float64)\ncombined = combined.replace([-np.inf, np.inf], 99)", "cell_type": "code", "execution_count": null, "metadata": {"_uuid": "63e6c56e9f52d7535e80a58ec5eca5e611d128c3", "_execution_state": "idle", "_cell_guid": "e7bc31ff-5ff6-4c72-9757-dac949e52eb3", "trusted": false, "collapsed": false}}, {"outputs": [], "source": "combined = combined.drop(labels=[\"id\", \"timestamp\"], axis=1, errors=\"ignore\").fillna(value=1)\nx_traindf = combined[:30471].copy()\nx_testdf = combined[30471:]\nprice =  df.price_doc.values", "cell_type": "code", "execution_count": null, "metadata": {"_uuid": "611af084ca918025c7066fa308edb98d7b1ab987", "_execution_state": "idle", "_cell_guid": "7ef1a9c1-d42d-4124-bd64-3f83a0c122fd", "trusted": false, "collapsed": false}}, {"outputs": [], "source": "from sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(x_traindf.values, price, train_size=0.7, test_size=0.3)", "cell_type": "code", "execution_count": null, "metadata": {"_uuid": "e2874d43c341899dfd40db5eb2bc6194a24ce0ac", "_execution_state": "idle", "_cell_guid": "4f7f4ad0-c80d-4c44-a20d-66c80d83decd", "trusted": false, "collapsed": false}}, {"outputs": [], "source": "from sklearn.model_selection import ShuffleSplit\nfrom sklearn.linear_model import LinearRegression, SGDRegressor, BayesianRidge, \\\nPassiveAggressiveRegressor\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom collections import defaultdict\n\nregressors = {\"linreg\" : LinearRegression(), \n              \"sdg\" : SGDRegressor(l1_ratio=0, alpha=0.01), \n              \"bayes\" : BayesianRidge(), \n              \"pasagg\" : PassiveAggressiveRegressor(), \n              \"neural\": MLPRegressor(), \n             \"gtr\":GradientBoostingRegressor()}\n\n\ncolumns = x_traindf.columns\n ", "cell_type": "code", "execution_count": null, "metadata": {"_uuid": "0fa8ec5dd176b295fe1c3c089de773632397b542", "_execution_state": "idle", "_cell_guid": "169b6b3d-1e5c-4260-943a-22ec5b11c332", "trusted": false, "collapsed": false}}, {"outputs": [], "source": "splitter = ShuffleSplit(test_size=0.2, n_splits=10)\ndef mean_accuracy(estimator, train, target, reg=None):\n    \"\"\"\n    train and targets should be numpy arrays\n    \"\"\"\n    scores = []\n    decreases = defaultdict(list)\n    for train_index, test_index in splitter.split(X=train, y=target):\n        train_x, train_y = train[train_index, :], target[train_index]\n        test_x, test_y = train[test_index, :], target[test_index]\n    \n        estimator.fit(X=train_x, y=train_y)\n        score = estimator.score(X=test_x, y=test_y)\n        scores.append(score)\n        for i, name in enumerate(columns):\n            test_copy = np.copy(test_x)\n            np.random.shuffle(test_copy[:, i])\n            try:\n                test_copy = estimator.transform(test_copy)\n            except AttributeError:\n                pass\n            score_i = estimator.score(X=test_copy, y=price[test_index])\n            decrease = (score_i - score)/score\n            decreases[name].append(decrease)\n    print(reg)\n    print(np.median(scores))\n    return pd.DataFrame(data=decreases).mean()\n\n", "cell_type": "code", "execution_count": null, "metadata": {"_uuid": "28d94e1c7935b2418c9454d1a9cbea7881fadfa2", "_execution_state": "idle", "_cell_guid": "a394e3f0-55fd-439c-b474-7b710c40c013", "trusted": false, "collapsed": false}}, {"outputs": [], "source": "gtb_imp = mean_accuracy(regressors[\"gtr\"], X_train, Y_train, reg=\"gtr\").nlargest(100).index.to_list()", "cell_type": "code", "execution_count": null, "metadata": {"_uuid": "fc9b989cfe6f66394e4e2e045309d8946d54a218", "_execution_state": "idle", "trusted": false, "collapsed": false}}, {"outputs": [], "source": "#important = {}\n#for k in regressors.keys():\n#    important[k] = mean_accuracy(regressors[k], X_train, Y_train, reg=k).nlargest(100).index.to_list()", "cell_type": "code", "execution_count": null, "metadata": {"_uuid": "b0824b87abb8eae8c9faaf07c99e681a0458de86", "_execution_state": "idle", "_cell_guid": "f6ae8cf0-4e15-4c63-b6c2-2600e0510b9c", "trusted": false, "collapsed": false}}, {"outputs": [], "source": "from sklearn.decomposition import KernelPCA\ndecomposer = KernelPCA(n_components=20, kernel=\"poly\", copy_X=False)\ncol_indices = pd.Series(data=range(len(columns)), index=columns)\n#performance = {}\ndef validate(reg, col_imp):\n    \"\"\"giv fit regressor and its important columns\"\"\"\n    col_index = col_indices.loc[col_imp].values\n    for_pca = set(columns).difference(col_imp)\n    pca_index = col_indices.loc[for_pca].values\n    X_train_copy = X_train.copy()\n    X_test_copy = X_test.copy()\n    decomposer.fit_transform(X_train_copy[:, pca_index])\n    X_test_copy[:, ] = decomposer.transform(X_test_copy[:, pca_index])\n    reg.fit(X=X_train_copy, y=Y_train)\n    score = regressors[k].score(X=X_test_copy, y=Y_test)  \n    return score", "cell_type": "code", "execution_count": null, "metadata": {"_uuid": "804c31ea4b190f48840f2565871776f4682155e2", "_execution_state": "idle", "_cell_guid": "4d0c70c3-2361-44b5-b970-e9df3c8b1eae", "trusted": false, "collapsed": false}}, {"outputs": [], "source": "validate(regressors[\"gtr\"], gtb_imp)", "cell_type": "code", "execution_count": null, "metadata": {"_uuid": "b9a8aea5fd04d1feaf7cbb670b68f5dfbd59ff7a", "_execution_state": "idle", "_cell_guid": "829658d6-b2d1-45e8-96e0-f4212f196032", "trusted": false, "collapsed": false}}, {"outputs": [], "source": "", "cell_type": "code", "execution_count": null, "metadata": {"_uuid": "d8129049ae720f5a4b1a159badcdc0ed06e29e1d", "_execution_state": "idle", "_cell_guid": "87f5e58c-fc85-4267-8cb3-4b1e5bd8bdcc", "trusted": false, "collapsed": false}}], "nbformat": 4, "nbformat_minor": 0, "metadata": {"_change_revision": 0, "language_info": {"file_extension": ".py", "version": "3.6.1", "codemirror_mode": {"version": 3, "name": "ipython"}, "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "mimetype": "text/x-python", "name": "python"}, "kernelspec": {"language": "python", "display_name": "Python 3", "name": "python3"}, "_is_fork": false}}