{"cells":[{"metadata":{"_uuid":"798691cee2df34e413b561afe591fd8114977e8c"},"cell_type":"markdown","source":"# Toxic Comment Classification"},{"metadata":{"_uuid":"7d668362f81dd7def2fbcbf83d5f4980f17806d0"},"cell_type":"markdown","source":"This notebook aims at classifying comments into 6 categories using **Keras**. We will be using Sequential model to classify comments by **feature extraction** using keras preprocessing functionality. We will be using **Tokenizer** to convert the text (comments) into word vectors using term frequency inverse document frequency **(TFIDF)**."},{"metadata":{"trusted":true,"_uuid":"d9038ad7d22cff1a3761f829c57a51b6745b528c"},"cell_type":"code","source":"# Importing important libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9b0d07bef9bfbffb41a7eaac4c00aca9840b3db3"},"cell_type":"markdown","source":"We will first begin with importing essential elements from keras.\nSequential model can stack up different layers in it. We will be using Dense layer followed by drop out, which is used to randomly droping out features while training them. "},{"metadata":{"trusted":true,"_uuid":"583effde1e833208a2824f048f6a04951bea1366"},"cell_type":"code","source":"import keras\nfrom keras.models import Sequential\nfrom keras.preprocessing import text\nfrom keras.layers import Dense,Dropout","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4a07c91a040f3f599f9315a1c5a538309a8731d4"},"cell_type":"markdown","source":"We can use pandas read-csv module to read training and test data from the csv file. These files are formatted using Dataframes and function differently from numpy matrices.You can read more about Data frames here https://pandas.pydata.org/pandas-docs/version/0.23.4/generated/pandas.DataFrame.html"},{"metadata":{"trusted":true,"_uuid":"18a5c014ee6efd958b2123290da66a071863fa12"},"cell_type":"code","source":"# Reading datasets from csv file\ntrain_set = pd.read_csv('../input/train.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2458e171c753c13cc15accc7a916cb50277dda16"},"cell_type":"code","source":"train_set.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"50137861bdd3d570326de8d4030e762a5d6ad7db"},"cell_type":"markdown","source":"We will now separate out **comments** and their **labels** from the dataframes (train_set). Dataframes are sort of dictionary so we can use column heading to select columns."},{"metadata":{"trusted":true,"_uuid":"ea4213f7226beaa973054122c0f86375ce730ae6"},"cell_type":"code","source":"# Separating Comments and Labels\ncomments,labels= train_set['comment_text'],train_set[['toxic','severe_toxic','obscene','threat','insult','identity_hate']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e426789d2e3ce8410323e281ffb59ccbdbb502f0"},"cell_type":"code","source":"comments.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9d83b39b9108726a6d8dc1d2651d9e43af6b9d4a"},"cell_type":"markdown","source":"### Preprocessing Comments"},{"metadata":{"_uuid":"16a83837d48647867135271bf7235c3ef961ea5a"},"cell_type":"markdown","source":"We will be taking help of keras in built text processing  function Tokenizer. \nThis class allows to vectorize a text corpus, by turning each text into either a sequence of integers (each integer being the index of a token in a dictionary) or into a vector where the coefficient for each token could be binary, based on word count, based on tf-idf. In short and simple, it will convert text into a word sequence which can be used to extract features from sequence. We will be using TFIDF, you can use word count as well."},{"metadata":{"_uuid":"62773e033d79f8881c96833e0e68b3ed80ff0a61"},"cell_type":"markdown","source":"##### A brief about TFIDF"},{"metadata":{"_uuid":"7575fd7a6eeff04143868b2f05d9fc17424fcef8"},"cell_type":"markdown","source":"Typically, the tf-idf weight is composed by two terms: the first computes the normalized Term Frequency (TF), aka. the number of times a word appears in a document, divided by the total number of words in that document; the second term is the Inverse Document Frequency (IDF), computed as the logarithm of the number of the documents in the corpus divided by the number of documents where the specific term appears.\n\nTF: Term Frequency, which measures how frequently a term occurs in a document. Since every document is different in length, it is possible that a term would appear much more times in long documents than shorter ones. Thus, the term frequency is often divided by the document length (aka. the total number of terms in the document) as a way of normalization: \n\nTF(t) = (Number of times term t appears in a document) / (Total number of terms in the document).\n\nIDF: Inverse Document Frequency, which measures how important a term is. While computing TF, all terms are considered equally important. However it is known that certain terms, such as \"is\", \"of\", and \"that\", may appear a lot of times but have little importance. Thus we need to weigh down the frequent terms while scale up the rare ones, by computing the following: \n\nIDF(t) = log_e(Total number of documents / Number of documents with term t in it)."},{"metadata":{"trusted":true,"_uuid":"9759cb729c0557a8fe113edbeacbf1df6da31693"},"cell_type":"code","source":"# Use of tokenizer to convert texts into word array\nnum_words=3000\ntokenizer = text.Tokenizer(lower=True,num_words=num_words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"79437369042661fa2444d315402f37b8862f69fe"},"cell_type":"code","source":"tokenizer.fit_on_texts(comments)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"473c664cf4444f43783898500ee4992d4d0f0088"},"cell_type":"code","source":"encoded_text = tokenizer.texts_to_matrix(comments,mode='tfidf')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ed938422317571cd4efb8222c3a099d33901f283"},"cell_type":"code","source":"encoded_text.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8c14642c269d90377fcbddf1eb56c908f98d5c98"},"cell_type":"markdown","source":"## Defining model"},{"metadata":{"_uuid":"bae54326c7d5780451ddbab801d8a0bc2bb7b2c0"},"cell_type":"markdown","source":"Our model will consist of three fully connected hidden layers followed by output layer. "},{"metadata":{"trusted":true,"_uuid":"49a698cc009ac834d7de0fb5d6379bbcf7f5da2d"},"cell_type":"code","source":"def Model():\n    model = Sequential()\n    model.add(Dense(1024,input_shape=(num_words,),activation='relu'))\n    model.add(Dropout(0.4))\n    \n    model.add(Dense(128,activation='relu'))\n    model.add(Dropout(0.2))\n    \n    model.add(Dense(64,activation='relu'))\n    model.add(Dense(6,activation='sigmoid'))\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8ea5bdd5a8e64a3e69625a2fb42087c443a8e365"},"cell_type":"code","source":"model = Model()\nmodel.compile(loss=keras.losses.binary_crossentropy,optimizer='adam',metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9d6c0d8504f4dbdc0c2596d1f5590d16197fd0e8"},"cell_type":"markdown","source":"##### Training on dataset"},{"metadata":{"_uuid":"d47a1be374d86190cd8ba336fef7c142ba248112"},"cell_type":"markdown","source":"Split dataset, to use a part of it for validation data to ensure the correctness of our model. We will use 2 epochs for training our model."},{"metadata":{"trusted":true,"_uuid":"6f4443c1c079affc65e80db5a943edd462e6a8f1"},"cell_type":"code","source":"history = model.fit(encoded_text,labels,verbose=1,epochs=2,validation_split=0.3)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ace0ff0f9a76671edcce65202a7a1a170c88dd92"},"cell_type":"markdown","source":"Evaluate the model using test set. First convert the test data using same tokensizer and same mode tfidf"},{"metadata":{"_uuid":"8363fe0cb566172b3c061730ca69dab8ca6eb178"},"cell_type":"markdown","source":"### Visualising the results"},{"metadata":{"trusted":true,"_uuid":"fdfc6913e71b2efca527af642604939efa2e5647"},"cell_type":"code","source":"# Plot training & validation loss values\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":2},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython2","version":"2.7.15"}},"nbformat":4,"nbformat_minor":1}