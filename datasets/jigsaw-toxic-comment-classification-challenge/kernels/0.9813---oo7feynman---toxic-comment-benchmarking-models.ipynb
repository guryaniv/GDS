{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true,"collapsed":true},"cell_type":"code","source":"# Udacity Machine Learning Capstone\n\nimport time\n#Data Importing Modules\nimport pandas as pd\nimport numpy as np\nnp.random.seed(42)\nimport string\nimport re\nfrom collections import Counter\nimport pickle\nimport tensorflow as tf\n\n#Selective Sklearn Libraries\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\nfrom keras.models import Model\nfrom keras.layers import Input, Dense, Dropout, Conv1D, Embedding, SpatialDropout1D, concatenate\nfrom keras.layers import GRU, LSTM,Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D\nfrom keras.layers import CuDNNLSTM, CuDNNGRU\nfrom keras.preprocessing import text, sequence\nfrom keras.callbacks import Callback\nfrom keras import optimizers\nfrom keras.layers import Lambda\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom nltk.corpus import stopwords\nimport os\nos.environ['OMP_NUM_THREADS'] = '4'\nimport gc\nfrom keras import backend as K\nfrom sklearn.model_selection import KFold\n\n#Text Cleaning Module\nfrom unidecode import unidecode\neng_stopwords = set(stopwords.words(\"english\"))\n\n#Visualization Libraries\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\nimport matplotlib.pyplot as plt\nfrom matplotlib_venn import venn2\nfrom matplotlib_venn import venn3\n\nfrom wordcloud import WordCloud","execution_count":1,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c05027fc73cf23140c06ec881a37d6a75948f617","collapsed":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense, Activation","execution_count":2,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"301730c61ffd52812db0cb0a29b5c65c31378826","collapsed":true},"cell_type":"code","source":"#reading all input files\n\n# train\nprint(\"reading train files\")\ntrain = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/train.csv',encoding='utf-8')\n\n#files will have to cleaned- contains non utf-8 characters\ntrain = train.replace(r'\\n',' ', regex=True)\ntrain = train.replace(r'\\\\',' ', regex=True)\nprint (train.head())\n\nprint (\"Now reading test files\")\ntest=pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/test.csv',encoding='utf-8')\n\n#Filters out punctuation (filters=’!”#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n’).\n\n#special_character_removal = re.compile(r'[!\"#$%&()*,-./:;<=>?@[\\\\]^_`{|}~\\t\\n]',re.IGNORECASE)\nspecial_character_removal = re.compile(r'[^A-Za-z\\.\\-\\?\\!\\,\\#\\@\\% ]',re.IGNORECASE)\ndef clean_text(x):\n    x_ascii = unidecode(x)\n    x_clean = special_character_removal.sub('',x_ascii)\n    return x_clean\n\ntrain['clean_text'] = train['comment_text'].apply(lambda x: clean_text(str(x)))\ntest['clean_text'] = test['comment_text'].apply(lambda x: clean_text(str(x)))\n\nX_train = train['clean_text'].fillna(\"something\").values\ny_train = train[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]].values\nX_test = test['clean_text'].fillna(\"something\").values\n","execution_count":3,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f002dd61973077089e4f3603e61cd267b011455f","collapsed":true},"cell_type":"code","source":"#adding features\ndef add_features(df):\n    \n    df['comment_text'] = df['comment_text'].apply(lambda x:str(x))\n    df['total_length'] = df['comment_text'].apply(len)\n    df['capitals'] = df['comment_text'].apply(lambda comment: sum(1 for c in comment if c.isupper()))\n    df['caps_vs_length'] = df.apply(lambda row: float(row['capitals'])/float(row['total_length']),\n                                axis=1)\n    df['num_words'] = df.comment_text.str.count('\\S+')\n    df['num_unique_words'] = df['comment_text'].apply(lambda comment: len(set(w for w in comment.split())))\n    df['words_vs_unique'] = df['num_unique_words'] / df['num_words']  \n\n    return df\n\ntrain = add_features(train)\ntest = add_features(test)\n\nfeatures = train[['caps_vs_length', 'words_vs_unique']].fillna(0)\ntest_features = test[['caps_vs_length', 'words_vs_unique']].fillna(0)\n\n#Using Standard Scaler to get z score\nfrom sklearn.preprocessing import StandardScaler\n\nss = StandardScaler()\nss.fit(np.vstack((features, test_features)))\nfeatures = ss.transform(features)\ntest_features = ss.transform(test_features)","execution_count":7,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"23d27970a361517a451dd745013afc7a65941c06","collapsed":true},"cell_type":"code","source":"#Basic EDA\nCOLUMNS = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n\n#Creating a copy for wordcloud\nCATEGORIES = COLUMNS.copy()\n\ntrain_distribution = train[COLUMNS].sum()\\\n                            .to_frame()\\\n                            .rename(columns={0: 'count'})\\\n                            .sort_values('count')\n\ntrain_distribution.sort_values('count', ascending=False)    ","execution_count":8,"outputs":[]},{"metadata":{"_uuid":"8ff7527629dc50586eb667fe3fbd8a0e9d764b7d"},"cell_type":"markdown","source":"From the data, we see that  the three major labels are :\ntoxic, obscene and insult\nOther labels have a very small quantum"},{"metadata":{"trusted":true,"_uuid":"70cd20b824f080de669078b67d1bd7ce2213659f","collapsed":true},"cell_type":"code","source":"train_comb = train.groupby(COLUMNS)\\\n                    .size()\\\n                    .sort_values(ascending=False)\\\n                    .reset_index()\\\n                    .rename(columns={0: 'count'})\ntrain_comb.head(n=10)","execution_count":9,"outputs":[]},{"metadata":{"_uuid":"7d5275fc9241c31dea8ea5dadc5d93e0b2a9fbc1"},"cell_type":"markdown","source":"Here, we are checking for class imbalances. From this table, we know that some comments can fall in multiple categories."},{"metadata":{"trusted":true,"_uuid":"338bb6629dac7f1f3d1a3cead4d29ea5180f94d2","collapsed":true},"cell_type":"code","source":"#Correlation Matrix\nf, ax = plt.subplots(figsize=(9, 6))\nf.suptitle('Correlation matrix for categories')\nsns.heatmap(train[COLUMNS].corr(), annot=True, linewidths=.5, ax=ax)","execution_count":10,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"160ae2f4e1ec0109a1a973dcd1ecd668aff17b61","collapsed":true},"cell_type":"code","source":"train[COLUMNS].corr().abs().unstack().sort_values(ascending=False)","execution_count":5,"outputs":[]},{"metadata":{"_uuid":"3e8c637a3d69a4766524b1744eb1156c050c9bca"},"cell_type":"markdown","source":"insult, obscene and toxic are highly correlated to each other."},{"metadata":{"trusted":true,"_uuid":"aa5609c4097506f635f20f9d7bac878c965d1fa2","collapsed":true},"cell_type":"code","source":"# Correlation matrix of added features\n\nCOLUMNS=COLUMNS+['total_length','caps_vs_length', 'num_words','num_unique_words','words_vs_unique']\nf, ax = plt.subplots(figsize=(20, 20))\nf.suptitle('Correlation matrix for categories and features')\nsns.heatmap(train[COLUMNS].corr(), annot=True, linewidths=.5, ax=ax)","execution_count":11,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"5434ffb0c5d85004be5aeafdb161a51ed8eb796f","collapsed":true},"cell_type":"code","source":"#Creating word cloud \nword_counter = {}\ndef clean_text(text):\n    text = re.sub('[{}]'.format(string.punctuation), ' ', text.lower())\n    return ' '.join([word for word in text.split() if word not in (eng_stopwords)])\n\nfor categ in CATEGORIES:\n    d = Counter()\n    train[train[categ] == 1]['comment_text'].apply(lambda t: d.update(clean_text(t).split()))\n    word_counter[categ] = pd.DataFrame.from_dict(d, orient='index')\\\n                                        .rename(columns={0: 'count'})\\\n                                        .sort_values('count', ascending=False)\nfor w in word_counter:\n    wc = word_counter[w]\n\n    wordcloud = WordCloud(\n          background_color='black',\n          max_words=200,\n          max_font_size=100, \n          random_state=4561\n         ).generate_from_frequencies(wc.to_dict()['count'])\n\n    fig = plt.figure(figsize=(12, 8))\n    plt.title(w)\n    plt.imshow(wordcloud)\n    plt.axis('off')\n\n    plt.show()            ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"65e8f89b7fc16d06a6effbb2300b7a20f87b37f7","collapsed":true},"cell_type":"code","source":"from keras.preprocessing import text, sequence\n\n#https://machinelearningmastery.com/prepare-text-data-deep-learning-keras/\n#Keras provides the text_to_word_sequence() function that you can use to split text into a list of words.\nmax_features=20000\nmaxlen = 50\n\ntokenizer = text.Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(X_train) + list(X_test))\n\nX_train_sequence = tokenizer.texts_to_sequences(X_train)\nX_test_sequence = tokenizer.texts_to_sequences(X_test)\n\nx_train = sequence.pad_sequences(X_train_sequence, maxlen=maxlen)\nx_test = sequence.pad_sequences(X_test_sequence, maxlen=maxlen)\n#print(tokenizer.word_index)\n\n\n#word index created, each word will have an index\n# pass words through embedding to get corresponding values\n","execution_count":14,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ea62ba3811c99fb371abc388646948d18125ae53","collapsed":true},"cell_type":"code","source":"max_features=20000\nmaxlen = 50","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e176a548ace8eea2530b36b2640b3c8582561930","collapsed":true},"cell_type":"code","source":"import csv\n\n#Reference: https://machinelearningmastery.com/develop-word-embedding-model-predicting-movie-review-sentiment/\n# load embedding as a dict\nword_index = tokenizer.word_index\nnb_words = min(max_features, len(word_index))\nembedding_vectors = np.zeros((nb_words,501))\n\ndef load_embedding(filename):\n    file = open(filename,'r')\n    lines = file.readlines()\n    file.close()\n    embedding = dict()\n    for line in lines:\n        parts = line.split()\n        embedding[parts[0]] = np.asarray(parts[1:], dtype='float32')\n    return embedding\n\ndef get_weight_matrix(embedding, vocab):\n# total vocabulary size plus 0 for unknown words\n    #controlling vocab size using max features\n    vocab_size = len(vocab) + 1\n    # define weight matrix dimensions with all 0\n    weight_matrix = np.zeros((vocab_size, 200))\n    # step vocab, store vectors using the Tokenizer's integer mapping\n    for word, i in vocab.items():\n        vector = embedding.get(word)\n        if vector is not None:\n            weight_matrix[i] = vector\n    return weight_matrix\n\nraw_embedding = load_embedding('../input/glove-twitter-27b-200d-txt/glove.twitter.27B.200d.txt')\n\n# get vectors in the right order #taking only count till nb_words\nembedding_vectors = get_weight_matrix(raw_embedding, tokenizer.word_index)[:nb_words,:] #Will be used to create embedding layer","execution_count":15,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3949d2da1b15190a7ae3b400e020fc95fa64568f","collapsed":true},"cell_type":"code","source":"#testing embedding data\nprint(list(tokenizer.word_counts.keys())[:2])\nprint(list(tokenizer.word_index.keys())[:2]) # emdding vector will align to word index and now to word doc or word counts\nprint(list(tokenizer.word_docs.keys())[:2])","execution_count":16,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"98b0cfac34500d1f0c7eab67ad025fe2ba345552","collapsed":true},"cell_type":"code","source":"#sum(raw_embedding['explanation']==embedding_vectors[1] returns zero\n#sum(raw_embedding['the']==embedding_vectors[1]) #returns 200 - same as dimension of glove data 200d\n#sum(raw_embedding['after']==embedding_vectors[1]) returns zero\n\nraw_embedding['hate']","execution_count":17,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"711baffce97205e9795b0530b59054f04b5dc77d","collapsed":true},"cell_type":"code","source":"#Base Accuracy - Predicting all labels as non toxic\n# using train_comb\nprint(\"Base Accuracy - Predicting all labels as non toxic \")\n(1-train_comb[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].mean())*100.0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"ff221d771d5878567cb9f14e390b8164e8fd7656"},"cell_type":"markdown","source":"For an unbalanced classification problem, marking everything as non toxic will give the above accuracies. Our neural model needs to work better than the base accuracy"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"1cb60dba26e46c6daed2a1eeabe887c42506a2eb"},"cell_type":"code","source":"# ROC - Boiler Plate Code\n\nclass RocAucEvaluation(Callback):\n    def __init__(self, validation_data=(), interval=1):\n        super(Callback, self).__init__()\n\n        self.interval = interval\n        self.X_val, self.y_val = validation_data\n        self.max_score = 0\n        self.not_better_count = 0\n\n    def on_epoch_end(self, epoch, logs={}):\n        if epoch % self.interval == 0:\n            y_pred = self.model.predict(self.X_val, verbose=1)\n            score = roc_auc_score(self.y_val, y_pred)\n            print(\"\\n ROC-AUC - epoch: %d - score: %.6f \\n\" % (epoch+1, score))\n            if (score > self.max_score):\n                print(\"*** New High Score (previous: %.6f) \\n\" % self.max_score)\n                model.save_weights(\"best_weights.h5\")\n                self.max_score=score\n                self.not_better_count = 0\n            else:\n                self.not_better_count += 1\n                if self.not_better_count > 3:\n                    print(\"Epoch %05d: early stopping, high score = %.6f\" % (epoch,self.max_score))\n                    self.model.stop_training = True","execution_count":17,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3a26d0a783621105a0aa66fc69681342bf004f14","collapsed":true},"cell_type":"code","source":"def get_model(features,clipvalue=1.,num_filters=40,dropout=0.5,embed_size=200):\n    features_input = Input(shape=(features.shape[1],))\n    inp = Input(shape=(maxlen, ))    \n    x = Embedding(max_features, embed_size, weights=[embedding_vectors], trainable=False,name='EmbeddingLayer')(inp)\n    #x = SpatialDropout1D(dropout)(x)\n    #x = Bidirectional(LSTM(num_filters, return_sequences=True),name='BidirectionalLSTM')(x)\n    x, x_h, x_c = Bidirectional(GRU(num_filters, return_sequences=True, return_state = True),name='BidirectionalGRU')(x)  \n    avg_pool = GlobalAveragePooling1D()(x)\n    max_pool = GlobalMaxPooling1D()(x)    \n    x = concatenate([avg_pool, x_h, max_pool,features_input])\n    outp = Dense(6, activation=\"sigmoid\")(x)\n    model = Model(inputs=[inp,features_input], outputs=outp)\n    adam = optimizers.adam(clipvalue=clipvalue)\n    model.compile(loss='binary_crossentropy',\n                  optimizer=adam,\n                  metrics=['accuracy'])\n    return model","execution_count":18,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5c51fab9e075c95d9f7b6084be7d514ab0c7276c","collapsed":true},"cell_type":"code","source":"get_model(features).summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"356095a2247590bccefb2d12bbec284ae2da268c","collapsed":true},"cell_type":"code","source":"model = get_model(features)\nbatch_size = 32\nepochs = 5\ngc.collect()\nK.clear_session()\nnum_folds = 5 \npredict = np.zeros((test.shape[0],6))\nscores = []\noof_predict = np.zeros((train.shape[0],6))\nkf = KFold(n_splits=num_folds, shuffle=True, random_state=239)\nfor train_index, test_index in kf.split(x_train):\n    kfold_y_train,kfold_y_test = y_train[train_index], y_train[test_index]\n    kfold_X_train = x_train[train_index]\n    kfold_X_features = features[train_index]\n    kfold_X_valid = x_train[test_index]\n    kfold_X_valid_features = features[test_index] \n    gc.collect()\n    K.clear_session()\n    model = get_model(features)\n    ra_val = RocAucEvaluation(validation_data=([kfold_X_valid,kfold_X_valid_features], kfold_y_test), interval = 1)\n    model.fit([kfold_X_train,kfold_X_features], kfold_y_train, batch_size=batch_size, epochs=epochs, verbose=1,\n             callbacks = [ra_val])\n    gc.collect()\n    model.load_weights(\"best_weights.h5\")\n    predict += model.predict([x_test,test_features], batch_size=batch_size,verbose=1) / num_folds\n    gc.collect() #- Running out of kaggle memory\n    oof_predict[test_index] = model.predict([kfold_X_valid, kfold_X_valid_features],batch_size=batch_size, verbose=1)\n    cv_score = roc_auc_score(kfold_y_test, oof_predict[test_index])\n    scores.append(cv_score)\n    print('Cross Validation Score: ',cv_score)\n\nprint(\"Model Completion for Keras DL\")\nprint('Total CV score is {}'.format(np.mean(scores)))   \nprint (\"Saving Predictions for offline upload\")\nsample_submission = pd.read_csv(\"../input/jigsaw-toxic-comment-classification-challenge/sample_submission.csv\")\nclass_names = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\nsample_submission[class_names] = predict\nsample_submission.to_csv('UdactiyAssignment_v1.csv',index=False)\n","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"52b9b444-4d2a-4507-9965-224ae88d52fb","_uuid":"f269d6206590f6f40b6a6524d411b5b579546f97","trusted":true},"cell_type":"code","source":"print (\"Code Run Completed\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}