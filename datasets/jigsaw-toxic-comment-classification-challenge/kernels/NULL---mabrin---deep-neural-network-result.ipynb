{"cells":[{"metadata":{"_cell_guid":"0cb3e036-7ff4-4e1f-94c3-82335eebedf8","_uuid":"095d45953e791275bbc47f711bb0c00d3682dc5d","collapsed":true,"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"bf7961a5-b9e0-471b-8a3a-ff5e177b0026","_uuid":"a4c30f65ee8c7005656cf3e15390e58c76eefc2b","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\nfrom scipy.sparse import hstack\nfrom scipy.special import logit, expit\nfrom tqdm import tqdm\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\nclass_names = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n\ntrain = pd.read_csv('../input/train.csv').fillna(' ')\ntest = pd.read_csv('../input/test.csv').fillna(' ')\n# train = train[0:10000]\n# test = test[0:10000]\nprint(train)\ntrain_text = train['comment_text']\ntest_text = test['comment_text']\n\n# train_text = train_text[0:100]\n# test_text = test_text[0:100]\ndef correlation(a , b ,unwanted='*'):\n\ta = list(a.lower())\n\tb = list(b.lower())\n\t\n\tcount = 0 \n\tflag =0\n\tfor i , j in zip(a,b) :\n\t\tif (not flag) and j not in unwanted:\n\t\t\tflag =1\n\t\tif i == j or j in unwanted :\n\t\t\tcount = count + 1\n\t\t\tcontinue\n\n\t\treturn 0\n\treturn count >= len(a)/2 and flag\nprint(train.head())","execution_count":2,"outputs":[]},{"metadata":{"_cell_guid":"87a45d32-83e4-4dbc-91f4-b1e2f437f111","_uuid":"0d8044f3407e43aa573e87121dd7461bf5f430e4","collapsed":true,"scrolled":true,"trusted":true},"cell_type":"code","source":"\ndef remove_unwanted_char(arr):\n    badWords = [\"fuck\" ,\"cock\" ,\"suck\" ,\"piss\" ,\"bullshit\" ,\"ass\" ,\"asshole\" ,\"dick\",\"shit\" ,\"motherfuck\"]\n    for k in range(len(arr)) :\n        tmp = arr[k].split()\n        flag = 0\n        for j in badWords:\n            for i in range(len(tmp)):\n                if correlation(j ,tmp[i]) :\n                    tmp[i]= j\n                    flag =1\n                    \n        if flag :\n            arr[k] = \" \".join(tmp)\n    return arr\n#train_text = remove_unwanted_char(train_text)\n#test_text = remove_unwanted_char(test_text)","execution_count":3,"outputs":[]},{"metadata":{"_cell_guid":"efb11137-545b-456a-917b-96175150a956","_uuid":"7610c24f0f603aa8831fb002a11f84ce261188cf","trusted":true,"collapsed":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense, Activation\nfrom keras.layers import Dense, Dropout, Activation ,LSTM\n\ndef new_model(  drop):\n    model = Sequential()\n\n    # Add an input layer \n    model.add(Dense(300, activation='relu', input_shape=(3000,)))\n    model.add(Dropout(drop))\n    model.add(Dense(100, activation='relu'))\n    model.add(Dropout(drop))\n    model.add(Dense(1, activation='sigmoid'))\n    model.compile(loss='binary_crossentropy',\n                  optimizer='adam',\n                  metrics=['accuracy'])\n    return model","execution_count":7,"outputs":[]},{"metadata":{"_cell_guid":"b3fcd781-baa7-40c0-ab77-cd1dd8d5af1c","_uuid":"576b17c11bcb060a9ccf4e6a7dd77b4fce14ab54","trusted":true,"collapsed":true},"cell_type":"code","source":"\n\nall_text = pd.concat([train_text])\n \n\n\n\n\nlosses = []\n\nword_vectorizer = TfidfVectorizer(\n    sublinear_tf=True,\n    strip_accents='unicode',\n    analyzer='word',\n    token_pattern=r'\\w{1,}.',\n    ngram_range=(1, 1),\n    max_features=3000,\n    max_df =.4 ,\n    min_df    = .000001\n)\nword_vectorizer.fit(all_text)\ntrain_word_features = word_vectorizer.transform(train_text)\n# test_word_features = word_vectorizer.transform(test_text)\n# train_features = hstack([train_word_features])\n# test_features = hstack([test_char_features,test_word_features])\n","execution_count":5,"outputs":[]},{"metadata":{"_cell_guid":"45e45260-6471-4455-8df8-72bd1d991246","_uuid":"b4d35aa85e442b3fcf36abcefe1f21ec0bbf18ff","trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense, Activation\nfrom keras.layers import Dense, Dropout, Activation ,LSTM\n\n\ntrain_label = train[\"toxic\"]\nsplit_point = int((2/3)*len(train_label))\n\nprint(train_word_features.shape)\nx_train = (train_word_features.toarray())[:split_point]\nx_test = (train_word_features.toarray())[split_point:]\n\ny_train = train_label[:split_point]\ny_test = train_label[split_point:]\n","execution_count":6,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6df42384a2b06ed730d89cec0e077ef218b2d48a"},"cell_type":"code","source":"model = new_model(.5)\nmodel.fit(x_train,y_train,epochs=100,batch_size=10)\ntestPredict = model.predict_classes(x_test)\n","execution_count":17,"outputs":[]},{"metadata":{"_cell_guid":"26e4b7c9-7c44-4e95-ba7a-4cd709c11192","_uuid":"122b6af661c4a89685c0a68ca106c9233e14f538","trusted":true},"cell_type":"code","source":"count =0\ny_test = np.asarray(y_test)\n\nfor i in range(len(testPredict)):\n    if(testPredict[i]>=0.5 and y_test[i]==1 or testPredict[i]<=0.5 and y_test[i]==0 ):\n        count=count+1\nprint(count/len(y_test))","execution_count":19,"outputs":[]},{"metadata":{"_cell_guid":"f3937c73-9177-4702-bc97-74843a4f2049","_uuid":"016c1e5363b01acc6a3c391539304ece54a82d74","collapsed":true,"trusted":true},"cell_type":"code","source":"# from keras.models import Sequential\n# from keras.layers import Dense, Activation\n# from keras.layers import Dense, Dropout, Activation ,LSTM\n\n# def new_model(  drop):\n \n#     model = Sequential()  \n#     model.add(LSTM(100, input_shape=(400, 100),return_sequences=True))\n#     model.add(Dense(100))\n#     model.compile(loss='mean_absolute_error', optimizer='adam',metrics=['accuracy'])\n\n#     return model\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"9f3bd816-d8fa-4881-9ac0-c484ca2f1a22","_uuid":"e6d4ced19a72edad4237306241ce0cf4a95bac6f","collapsed":true,"trusted":true},"cell_type":"code","source":"# train_features = [train_features.toarray().T]\n\n# train_labels = train[\"\"]\npredictions = {'id': test['id']}\nfor class_name in (class_names):\n#     if class_name == \"threat\" :\n        \n#     train_target = [[list(train[class_name])]]\n    train_target = train[class_name]\n    model = new_model(.5)\n   \n    #classifier = LogisticRegression(solver='sag',multi_class=\"ovr\",tol=1e-10)\n#     classifier =LinearDiscriminantAnalysis(solver=’svd’, shrinkage=None, priors=None, n_components=None, store_covariance=False, tol=0.0001)\n#     cv_loss = np.mean(cross_val_score(classifier,train_features, train_target, cv=3, scoring='roc_auc'))\n#     losses.append(cv_loss)\n#     print('CV score for class {} is {}'.format(class_name, cv_loss))\n    print(np.shape(train_features) ,np.shape(train_target))\n    model.fit(train_features, train_target,epochs=7, batch_size=50, verbose=1)\n    predictions[class_name] = model.predict( test_features )\n#     classifier.fit(train_features, train_target)\n#     predictions[class_name] = classifier.predict_proba(test_features)[:, 1]\n    \nprint('Total CV score is {}'.format(np.mean(losses)))\nsubmission = pd.DataFrame.from_dict(predictions)\nsubmission.to_csv('submission.csv', index=False)\n\n ","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"5597d841-d70d-4e2f-a006-30857a10570a","_uuid":"8fad495da0cf5dfbe5408e0a6dbaabce1f9b582e","collapsed":true,"scrolled":true,"trusted":true},"cell_type":"code","source":"print(np.shape(train_features))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e5634c32-7cbb-449e-bc7c-5b4d70df614a","_uuid":"0a6233c82678f928c3a64ec16c9a8ae350a2168b","collapsed":true,"trusted":true},"cell_type":"code","source":"# from scipy import sparse\n# sparse.save_npz('train_featureMatrix.npz', train_features)\n# sparse.save_npz('test_featureMatrix.npz', test_features)\n\n# #Load\n# #data = sparse.load_npz(\"data_sparse.npz\")\n# np.savetxt('train_featureMatrix.csv',train_features,delimiter=',' , fmt='%.10f' )\n# np.savetxt('test_featureMatrix.csv',test_features,delimiter=',' , fmt='%.10f' )","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"7037c91f-1594-41aa-963c-7d90b5547fcc","_uuid":"65beb85d4f8c37f362c591ae888e2e908c225740","collapsed":true,"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"24efe316-3107-4e1e-b4e7-2eb87d12f93e","_uuid":"f0f9fd11a4fec4176fbaa0a4746d850320ba0916","collapsed":true,"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"4f9b2074-1cdd-4091-bb93-45a1b6c32898","_uuid":"361d061758157898aeeae3d35a4cb40df043d659","collapsed":true,"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}