{"cells":[{"metadata":{"_uuid":"36a5c5c1189db70c8221012745ce35f46dcb1926"},"cell_type":"markdown","source":"I tried to make spaCy work for this task, I am still convinced that there is a way to do it but it seemed like too much work. Eventually gave up and tried to see if potentially just picking the closest word everytime would be a decent baseline, but it turns out it is worse than the all .33 predictions. "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":false,"_kg_hide-output":false},"cell_type":"code","source":"import numpy as np\nimport pandas as pd","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4ebeab2a0cdc00be7506a87dfe51030e3f7cfae4"},"cell_type":"markdown","source":"## Load GAP Coreference Data\n\n"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"gap_train = pd.read_csv(\"https://raw.githubusercontent.com/google-research-datasets/gap-coreference/master/gap-development.tsv\", delimiter='\\t')\ngap_test = pd.read_csv(\"https://raw.githubusercontent.com/google-research-datasets/gap-coreference/master/gap-test.tsv\", delimiter='\\t')\ngap_valid = pd.read_csv(\"https://raw.githubusercontent.com/google-research-datasets/gap-coreference/master/gap-validation.tsv\", delimiter='\\t')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"001ec7244eb860be3df2df6683d453a6200593d7"},"cell_type":"markdown","source":"## Load Competition Data"},{"metadata":{"trusted":true,"_uuid":"68cb3c8c773606946ad213fb303636070a9ddc93"},"cell_type":"code","source":"test_stage_1 = pd.read_csv('../input/test_stage_1.tsv', delimiter='\\t')\nsub = pd.read_csv('../input/sample_submission_stage_1.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"59266ed45d7784e2b98823c389c8503936e39fb7"},"cell_type":"markdown","source":"In order to make the coreference model load correctly we need to install the correct specific version of the neuralcoref model, cymem and spacy. Order is also important. If you install neuralcoref after it will not work. "},{"metadata":{"trusted":true,"_uuid":"b0badb97c38f4bfba4ccd42b81292a1963206521"},"cell_type":"code","source":"# !pip install https://github.com/huggingface/neuralcoref-models/releases/download/en_coref_md-3.0.0/en_coref_md-3.0.0.tar.gz\n# !pip install cymem==1.31.2 spacy==2.0.12","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a69febc79c842ab295eca0e27df267729979e52a"},"cell_type":"code","source":"# import en_coref_md\n# from spacy.tokens import Doc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"557936bc630f831eea32cb89f45921ea4664f6e2"},"cell_type":"code","source":"# nlp = en_coref_md.load()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0ef0e0ecb03d1285226c7b010b4cdaedb91a943c"},"cell_type":"markdown","source":"Writing a custom tokenizer to replace SpaCy's. This one will simply split on spaces. SpaCy's is good, but it makes it much more complicated if the tokenizer is changing the character lengths because that is part of the information we have for knowing where the pronouns and referenced terms are. "},{"metadata":{"trusted":true,"_uuid":"f8b6b4557d47705f31f62667043fa554d4d9f3ab"},"cell_type":"code","source":"# class WhitespaceTokenizer(object):\n#     def __init__(self, vocab):\n#         self.vocab = vocab\n#     def __call__(self, text):\n#         words = text.split(' ')\n#         words = [word for word in words]\n#         spaces = [True] * len(words)\n#         return Doc(self.vocab, words=words, spaces=spaces)\n# nlp.tokenizer = WhitespaceTokenizer(nlp.vocab)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0cb88ab5d61e26c744c6c096a275fcd182a7a131"},"cell_type":"code","source":"# gap_train.iloc[0:2, :]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"47e6026bc8774f82b37d9307cfbd530be211fcb0"},"cell_type":"markdown","source":"Wrote a ridiculous function to try to line up the character offset to actually match the words we are looking for. It resolves the correct words in all but 12 pronouns. Have not spent much time trying to figure out why these few do not resolve correctly, but my hypothesis is places where there are double spaces. "},{"metadata":{"trusted":true,"_uuid":"8391ac724041caf5761b82676c7f21df581f9f94","_kg_hide-input":false,"_kg_hide-output":false},"cell_type":"code","source":"# def check_coref(row):\n#     text = row[\"Text\"]\n#     words = text.split()\n\n#     pronoun = row[\"Pronoun\"]\n#     pronoun_off = row[\"Pronoun-offset\"]\n#     A = row[\"A\"]\n#     len_a = len(A.split())\n#     A_off = row[\"A-offset\"]\n#     B = row[\"B\"]\n#     len_b = len(B.split())\n#     B_off = row[\"B-offset\"]\n#     position = 0\n#     for i, word in enumerate(words):\n#         if position == pronoun_off:\n#             pronoun_word_index = i\n#         if position == A_off:\n#             A_off_word_index = (i, i+len_a)\n#         if position == B_off:\n#             B_off_word_index = (i, i+len_b)\n#         position += len(word) + 1\n#     #print(A_off_word_index, B_off_word_index, pronoun_word_index)\n#     doc = nlp(text)\n#     token = None\n#     try:\n#         token = doc[pronoun_word_index]\n#     except:\n#         print(pronoun, pronoun_off)\n#     try:\n#         print(token, A, B, token._.coref_clusters)\n#     except:\n#         return [0, 0, 1]\n    \n# gap_train.apply(check_coref, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f7930eec7c6071f2ff8e31bb54e96428bc24081d"},"cell_type":"code","source":"# test_stage_1","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f2a00cc0a712acada771d45fa6ce65b19bcd77a4"},"cell_type":"markdown","source":"After giving up on that I tried a much simpler approach of just weighting the decisions based on how far they are from the pronouns. \n\nI.e (pronoun at 271, A at 298, B at 341)\n\nA_dist = 298 - 271\n\nB_dist = 341 - 271\n\ntotal dist = A_dist(27) +B_dist( 70)\n\na_val = A_dist(27)/total_dist(97)\n\nb_val = B_dist(70)/total_dist(97)\n\n\nThis will calculate weights to give A and B. I weighted neither to simply always be .5 so that it balances with the other two since they will always add up to 1. In this competition we dont need the probability to sum to 1 since it is renormalized on a per row basis according to the rule section. \n"},{"metadata":{"trusted":true,"_uuid":"fbef09f082bd30f5731832b1d6dc5bee64ab3968"},"cell_type":"code","source":"def measure_dist(row):\n    pro_off = row[\"Pronoun-offset\"]\n    a_off = row[\"A-offset\"]\n    b_off = row[\"B-offset\"]\n    a_dist = np.abs(pro_off - a_off)\n    b_dist = np.abs(pro_off - b_off)\n    dist_tot = a_dist + b_dist\n    a_val = a_dist/dist_tot\n    b_val = b_dist/dist_tot\n    neither = .5\n    return [a_val, b_val, neither]\ntest_stage_1[\"preds\"] = test_stage_1.apply(measure_dist, axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"25a43df236006d1c46a41834dea2cea6305c1a5c"},"cell_type":"code","source":"test = test_stage_1.preds.apply(pd.Series)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f36061c2e1ee8030bd647af8b04609d57ed62697"},"cell_type":"code","source":"sub[\"A\"] = test[0]\nsub[\"B\"] = test[1]\nsub[\"NEITHER\"] = test[2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"849762e6b8286f4294b10caa9276e697b7af43cf"},"cell_type":"code","source":"sub[['ID', 'A', 'B', 'NEITHER']].to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5026e8d498c30d1d8bdd25a6175a1489d044197c"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}