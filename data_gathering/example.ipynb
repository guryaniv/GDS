{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned in the readme file, our first task is to create a repository of reproducible data science code-snippets. \n",
    "We'de like to methodically download datasets, jupyter notebooks, and all relevant metadata from Kaggle.com (online community of data scientists). <br>\n",
    "Then, we'de like to parse this repository into tsv files that will be used as input to our models in the following stages. <br>\n",
    "This notebook's purpose is to demonstrate the data gathering process - downloading the notebooks and data, and parsing it into usable files.\n",
    "\n",
    "Keep in mind that it's better to use the relevant functions from the python files directly.\n",
    "\n",
    "All of the functions that are used in this notebook are well-documented inside the relevant python files. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Create a list of relevant Kaggle datasets and competitions\n",
    "\n",
    "First we need to decide which datasets we would like to download from Kaggle.com. <br>\n",
    "The datasets in Kaggle are tagged with related subjects.\n",
    "We set the wanted tags in ``search_terms`` (as a string where the tags are seperated by spaces).\n",
    "The tags from ``search_terms`` will be searched via Kaggle API to collect all relevant datasets (that are relevant to all of the tags).\n",
    "\n",
    "Using ```get_kaggle_metadata``` we create 2 txt files for later use:\n",
    "* **consts.NOT_VALID_DATA_LINKS_NAME** - list of non-relevant datasets, so we won't search for them again\n",
    "* **consts.DATA_LINKS_NAME** - list of relevant datasets that will be downloaded using \"download.py\"\n",
    "\n",
    "You may change the paths to those files (and others) in [consts.py](https://github.com/TAU-DB/guided-ds/blob/master/data_gathering/consts.py).\n",
    "\n",
    "Our files are currently located in the [assets](https://github.com/TAU-DB/guided-ds/tree/master/data_gathering/assests) folder.\n",
    "\n",
    "We can decide whether to get info on regular datasets, competitions, or both, by setting ```comp``` and ```ds``` parameters.<br>\n",
    "We can also decide what is the minimum number of notebooks per dataset that we want to consider by setting ```num```.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "consts imported\n",
      "collecting relevant datasets links...\n",
      "***************************\n",
      "Found 12 competitions\n",
      "V - 0: two-sigma-financial-news\n",
      "V - 1: santander-value-prediction-challenge\n",
      "V - 2: two-sigma-financial-modeling\n",
      "X - 3: Not enough notebooks\n",
      "V - 4: donorschoose-application-screening\n",
      "V - 5: elo-merchant-category-recommendation\n",
      "V - 6: santander-customer-transaction-prediction\n",
      "V - 7: bnp-paribas-cardif-claims-management\n",
      "V - 8: santander-customer-satisfaction\n",
      "V - 9: santander-product-recommendation\n",
      "V - 10: home-credit-default-risk\n",
      "V - 11: sberbank-russian-housing-market\n",
      "Finished collecting links\n"
     ]
    }
   ],
   "source": [
    "import kaggleapi\n",
    "search_terms = \"finance\"    # example for multiple tags: \"finance money\"\n",
    "print(\"collecting relevant datasets links...\")\n",
    "print(\"***************************\")\n",
    "kaggleapi.get_kaggle_metadata(search_terms, comp=True, ds=False, num=10)   # we only want to get competitions (to make example shorter)\n",
    "print(\"Finished collecting links\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check to see some datasets that were collected to the data_links file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/c/santander-value-prediction-challenge/\n",
      "\n",
      "/c/two-sigma-financial-modeling/\n",
      "\n",
      "/c/donorschoose-application-screening/\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import consts\n",
    "\n",
    "with open(consts.DATA_LINKS_NAME, \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "print(lines[1])\n",
    "print(lines[2])\n",
    "print(lines[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Download datasets, notebooks and metadata\n",
    "\n",
    "Using the lists that we created, we get the download links using ```read_links()```. <br>\n",
    "Then, in ```download_data_and_kernels_link``` we use WebDriver to scrape Kaggle website, download the datasets and relevant metadata, and get links for all of the jupyter notebooks that are associated with the datasets.\n",
    "Finally, in ```pull_kernels()``` we use kaggle API to pull (download) all of the notebooks (kernels).\n",
    "\n",
    "<em>**Important Note** - You need to setup your Kaggle username and password in [consts.py](https://github.com/TAU-DB/guided-ds/blob/master/data_gathering/consts.py) as ```KAGGLE_USER``` and ```KAGGLE_PW```</em>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "consts imported\n",
      "Datasets links were read...\n",
      "***************************\n",
      "Downloading datasets and metadata...\n",
      "***************************\n",
      "Got total of 11 datasets\n",
      "Starting with dataset 1\n",
      "Dataset name is two-sigma-financial-news\n",
      "Getting data from https://www.kaggle.com/c/two-sigma-financial-news\n",
      "Chrome will save to - C:\\Workspace\\guided-ds\\Example_datasets\\two-sigma-financial-news\\input\n",
      "Opening https://www.kaggle.com/c/two-sigma-financial-news\n",
      "Browsing again to requested link (After login may site may redirect)\n",
      "['finance', 'time series', 'money', 'news agencies']\n",
      "Got competition description\n",
      "Got competition evaluation\n",
      "Chrome will save to - C:\\Workspace\\guided-ds\\Example_datasets\\two-sigma-financial-news\\input\n",
      "Opening https://www.kaggle.com/c/two-sigma-financial-news/data\n",
      "Browsing again to requested link (After login may site may redirect)\n",
      "Downloading two-sigma-financial-news - Browser will remain open\n",
      "Chrome will save to - C:\\Workspace\\guided-ds\\Example_datasets\\two-sigma-financial-news\n",
      "Opening https://www.kaggle.com/c/two-sigma-financial-news/kernels?sortBy=scoreDescending&group=everyone&pageSize=20&language=Python&kernelType=Notebook\n",
      "Browsing again to requested link (After login may site may redirect)\n",
      "Found total of 418 kernel links\n",
      "Done with dataset 1\n",
      "So far total notebooks is: 418\n",
      "- - - - - - - - - -\n",
      "\n",
      "\n",
      "***************************\n",
      "Downloading notebooks\n",
      "***************************\n",
      "1 - Done with notebook: lb-4-modified-reciprocal-of-r\n",
      "2 - Done with notebook: env-var07\n",
      "3 - Done with notebook: lstm-model-with-market-and-news-data\n",
      "4 - Done with notebook: lightgbm-with-online-training\n",
      "5 - Done with notebook: very-simple-nn-model-market-data-only\n",
      "6 - Done with notebook: reciprocal-of-previous-10-day-benchmark\n",
      "7 - Done with notebook: sma-only-v2\n",
      "8 - Done with notebook: iterative-approach\n",
      "9 - Done with notebook: lb-0-6326-tuned-xgboost-baseline\n",
      "10 - Done with notebook: xgboost-2-market-news\n",
      "***************************\n",
      "Finished example downloading\n"
     ]
    }
   ],
   "source": [
    "from download import read_links\n",
    "from download import download_data_and_kernels_link\n",
    "from download import pull_kernels\n",
    "\n",
    "# we set earlystop for the datasets and notebooks downloading functions\n",
    "# otherwise it will be a very long run\n",
    "\n",
    "link_list = read_links()\n",
    "print(\"Datasets links were read...\")\n",
    "print(\"***************************\")\n",
    "print(\"Downloading datasets and metadata...\")\n",
    "print(\"***************************\")\n",
    "download_data_and_kernels_link(link_list, stop_nb=False, earlystop=1)    # we only download all data for 1 dataset (to make example shorter)\n",
    "print(\"***************************\")\n",
    "print(\"Downloading notebooks\")\n",
    "print(\"***************************\") \n",
    "pull_kernels(earlystop=10)                         # we only download 10 notebooks for this dataset (to make example shorter)\n",
    "print(\"***************************\")\n",
    "print(\"Finished example downloading\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our downloaded datasets and notebooks, along with the relevant metadata, are stored in the [Datasets](https://github.com/TAU-DB/guided-ds/tree/master/datasets) folder. The notebooks are located within the 'kernels' folder inside the folder of the relevant dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Parse the downloaded data into usable tsv files\n",
    "\n",
    "Now, we would like to generate usable tsv files to train our models (for the [workflow stage classifier](https://github.com/TAU-DB/guided-ds/tree/master/Classification)). We'll create three files:\n",
    "* **datasets.tsv** - contains for each dataset: its name, description, evaluation method (if exists), tags\n",
    "* **notebooks.tsv** - contains for each notebook: its name, username (of author), the relevant dataset name, score (if exists)\n",
    "* **cells.tsv** - contains for each cell: unique cell id, relevant notebook, username (of author), cell's source code, output (initially empty, needs to be executed), execeution count.\n",
    "\n",
    "To train our models we mainly used cells.tsv.\n",
    "We later add more data to this file such as the code's AST and masked representation (see [Documentation](https://github.com/TAU-DB/guided-ds/tree/master/Documentation))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "consts imported\n",
      "consts imported\n",
      "***************************\n",
      "Creating datasets.tsv...\n",
      "***************************\n",
      "Done, Saved at C:\\Workspace\\guided-ds\\Example_Data\\datasets.tsv\n",
      "***************************\n",
      "Creating notebooks.tsv , cells.tsv ...\n",
      "***************************\n",
      "Loading datasets from:  C:\\Workspace\\guided-ds\\Example_datasets\n",
      "0 --- Start with C:\\Workspace\\guided-ds\\Example_datasets\\two-sigma-financial-news \n",
      "\n",
      "Starting notebook 0 2.7857---oriormeir---xgboost-2-market-news.ipynb\n",
      "Found 10 cells\n",
      "Finished notebook number 0  - with total cells of 10\n",
      "So far gathered 10 of cells\n",
      "Starting notebook 1 2.89408---alluxia---lb-0-6326-tuned-xgboost-baseline.ipynb\n",
      "Found 25 cells\n",
      "Finished notebook number 1  - with total cells of 21\n",
      "So far gathered 31 of cells\n",
      "Starting notebook 2 3.01095---charleslandau---iterative-approach.ipynb\n",
      "Found 16 cells\n",
      "Finished notebook number 2  - with total cells of 15\n",
      "***************************\n",
      "Finished creating example tsvs\n"
     ]
    }
   ],
   "source": [
    "from ds_csv_generator import generate_ds_tsv\n",
    "from nb_csv_generator import generate_nb_tsv\n",
    "\n",
    "print(\"***************************\")\n",
    "print(\"Creating datasets.tsv...\")\n",
    "print(\"***************************\")\n",
    "generate_ds_tsv()\n",
    "print(\"***************************\")\n",
    "print(\"Creating notebooks.tsv , cells.tsv ...\")\n",
    "print(\"***************************\")\n",
    "\n",
    "# again, we set earlystop for the datasets and notebooks tsvs generator\n",
    "# otherwise it will be a very long run\n",
    "\n",
    "generate_nb_tsv(earlystop=3)\n",
    "print(\"***************************\")\n",
    "print(\"Finished creating example tsvs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see how the parsed files look like in the [Data](https://github.com/TAU-DB/guided-ds/tree/master/Data) folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 - Workflow stage classification\n",
    "\n",
    "In the next stage of our project, we used the generated cells.tsv file to train a workflow stage classifier.\n",
    "See the [Classification](https://github.com/TAU-DB/guided-ds/tree/master/Classification) folder for more details.\n",
    "\n",
    "Using the classifier we classified our cells to the relevant data science workflow stage, adding a 'Label' for each cell in the cells.tsv file.\n",
    "\n",
    "For future use, you may use the pretrained classifier model to tag new cells [Here](https://github.com/TAU-DB/guided-ds/blob/master/Classification/Classification.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 - Create chatbot's input files\n",
    "\n",
    "The final (and main) stage of our project is to create a chatbot that will generate next-line recommendations for data scientists.\n",
    "In order to train a chatbot model we need an input file of input and output pairs (seperated by tab).\n",
    "During our research we used many different representations as input and output (see [Documentation](https://github.com/TAU-DB/guided-ds/tree/master/Documentation)). \n",
    "\n",
    "Here, we create the following files:\n",
    "* **Source code pairs** - input cell source code \\t output cell source code \\n\n",
    "2. **AST pairs** - input cell code's AST \\t output cell code's AST \\n\n",
    "3. **Masked pairs** - input cell masked representation /t output cell masked representation \\n\n",
    "\n",
    "For our final recommendation engine we created a file of: 3-lines masked representation \\t next-line masked representation \\n <br>using ```cells_to_masked_lines```.\n",
    "\n",
    "You may create any input and output pairs, seperated by tabs, and try to train our chatbot using it [here](https://github.com/TAU-DB/guided-ds/blob/master/Chatbot/Jupyter_Cells_Chatbot_Model.ipynb).\n",
    "\n",
    "<em> Note: execution for all notebooks and cells that were downloaded may take a while</em> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "consts imported\n",
      "***************************\n",
      "Creating Source code pairs...\n",
      "***************************\n",
      "Cells number before cleaning: 43\n",
      "Cells number after cleaning: 38\n",
      "Found diffrent sources of: 0\n",
      "Done. Kept: 0.0 %\n",
      "***************************\n",
      "Creating AST pairs...\n",
      "***************************\n",
      "Cells number before cleaning: 43\n",
      "Cells number after cleaning: 40\n",
      "Found diffrent sources of: 0\n",
      "Done. Kept: 0.0 %\n",
      "***************************\n",
      "Creating Masked pairs...\n",
      "***************************\n",
      "Starts with 43\n",
      "After initial cleaning 43\n",
      "Cells number before cleaning: 43\n",
      "Before filtering by label - total cells was 15\n",
      "Cells number after cleaning: 3\n",
      "***************************\n",
      "Creating Masked line pairs...\n",
      "***************************\n",
      "Saving notebook and labels lists...\n",
      "Notebooks size is 3\n",
      "Labels list size is 3\n",
      "Total 3 notebooks\n",
      "Starts with notebook number -  1\n",
      "Starts with notebook number -  2\n",
      "Starts with notebook number -  3\n",
      "Clearing any remains inside buffer -Prep. Remaining length is955\n",
      "Clearing any remains inside buffer -Train. Remaining length is793\n",
      "Clearing any remains inside buffer -Eval. Remaining length is126\n",
      "Clearing any remains inside buffer -Explore. Remaining length is437\n",
      "***************************\n",
      "Finished creating chatbot input example files\n"
     ]
    }
   ],
   "source": [
    "from convert_to_chatbot_input import cells_to_masked_pairs\n",
    "from convert_to_chatbot_input import cells_to_ast_pairs\n",
    "from convert_to_chatbot_input import cells_to_source_pairs\n",
    "from convert_to_chatbot_input import cells_to_masked_lines\n",
    "\n",
    "print(\"***************************\")\n",
    "print(\"Creating Source code pairs...\")\n",
    "print(\"***************************\")\n",
    "cells_to_source_pairs()\n",
    "print(\"***************************\")\n",
    "print(\"Creating AST pairs...\")\n",
    "print(\"***************************\")\n",
    "cells_to_ast_pairs()\n",
    "\n",
    "print(\"***************************\")\n",
    "print(\"Creating Masked pairs...\")\n",
    "print(\"***************************\")\n",
    "\n",
    "# we create the masked pairs file here for input cells with 'Explore' label, and we don't keep just unique pairs\n",
    "cells_to_masked_pairs(label='Explore', unique=False, ptg=False) \n",
    "\n",
    "print(\"***************************\")\n",
    "print(\"Creating Masked line pairs...\")\n",
    "print(\"***************************\")\n",
    "\n",
    "# again, we set earlystop for the line pairs generator\n",
    "# otherwise it will be a very long run\n",
    "cells_to_masked_lines(load=False, earlystop=3, ptg=False)\n",
    "print(\"***************************\")\n",
    "print(\"Finished creating chatbot input example files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, you can see how the parsed files should look like in the [Data](https://github.com/TAU-DB/guided-ds/tree/master/Data) folder. <br>\n",
    "\n",
    "The files are saved to the paths defined in consts.\n",
    "specifically the files that we used are generated by the ```cells_to_masked_lines``` functions and will be stored in ```consts.LOAD_TSV```, ```consts.PREP_TSV```, etc. (for each stage)\n",
    "\n",
    "More explanations about the content of each file is in the [project report](https://github.com/TAU-DB/guided-ds/blob/master/Documentation/ProjectReport_18-1-1-1638.pdf) or in the [chatbot training notebook](https://github.com/TAU-DB/guided-ds/blob/master/Chatbot/Jupyter_Cells_Chatbot_Model.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Step - Execute notebooks to get output\n",
    "\n",
    "We didn't use the cell's output to train our models, but it could be useful as an additional feature for future purposes.\n",
    "The following script automatically executes each notebook, The script was tested on a Windows 10 machine (Unfortunaly no guarantee for Linux\\Unix machines).\n",
    "\n",
    "<em>**Important note** - the first argument of the function is the URL link to an activate conda localhost server. Run \"jupyter notebook\" command from the main folder of the project.</em>\n",
    "\n",
    "We set ```to_execute=1```, meaning this will execute all of the notebooks for 1 dataset.\n",
    "Selenium will control your browser and run the notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from execute_nb import execute_notebooks\n",
    "\n",
    "execute_notebooks(\"http://localhost:8889/?token=f3dec8fa35423b846ef3ecccfb6d584ac1ef844974af0c65\", to_execute=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_env",
   "language": "python",
   "name": "tensorflow_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
