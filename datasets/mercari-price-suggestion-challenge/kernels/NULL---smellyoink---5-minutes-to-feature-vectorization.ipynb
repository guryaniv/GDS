{"nbformat_minor": 1, "cells": [{"cell_type": "markdown", "metadata": {"_uuid": "8713596eb698e398be13ba08d5e4dca74cbbd17c"}, "source": ["I'm new to kaggle, but a major part of this challenge seems to be efficiently processing data to leave time for supervised learning algorithms.  Below is my attempt to make this efficient.  I would appreciate any comments/improvements/bug calls!"]}, {"cell_type": "code", "metadata": {"_uuid": "104eda7a5f9c19286fb7a17453bbbd22ea476cb1", "_cell_guid": "6936436a-cbd4-4fde-8fb2-b4f96af54f18", "collapsed": true}, "execution_count": null, "source": ["import time\n", "import numpy as np\n", "import pandas as pd\n", "import gc"], "outputs": []}, {"cell_type": "code", "metadata": {"_uuid": "4132552bbff7f67d064306b58fe40d896fbb6dc7", "_cell_guid": "ce3e2939-f65c-4065-83ef-35e423593ace", "collapsed": true}, "execution_count": null, "source": ["def split_cat(text):\n", "    try:\n", "        return text.split(\"/\")\n", "    except:\n", "        return (\"No Label\", \"No Label\", \"No Label\")"], "outputs": []}, {"cell_type": "code", "metadata": {"_uuid": "b80a17442ee595b6c17ac02980bd8832d4602bd2", "_cell_guid": "14a21f6a-6ffb-4c4b-ba96-f0407fef3223"}, "execution_count": null, "source": ["%%time\n", "\n", "start_time = time.time()\n", "    \n", "\n", "train = pd.read_table('../input/train.tsv', engine='c', \n", "                      dtype={'item_condition_id': 'category',\n", "                             'shipping': 'category',\n", "                            }, \n", "                     converters={'category_name': split_cat})\n", "test = pd.read_table('../input/test.tsv', engine='c', \n", "                      dtype={'item_condition_id': 'category',\n", "                             'shipping': 'category',\n", "                            },\n", "                    converters={'category_name': split_cat})\n", "print('[{}] Finished load data'.format(time.time() - start_time))\n", "\n", "nrow_test = train.shape[0]\n", "dftt = train[(train.price < 1.0)]\n", "train = train.drop(train[(train.price < 1.0)].index)\n", "del dftt['price']\n", "nrow_train = train.shape[0]\n", "y = np.log1p(train['price'])\n", "merge = pd.concat([train, dftt, test])\n", "submission: pd.DataFrame = test[['test_id']]\n", "\n", "del train, test\n", "gc.collect()\n", "\n", "merge['gencat_name'] = merge['category_name'].str.get(0).replace('', 'missing').astype('category')\n", "merge['subcat1_name'] = merge['category_name'].str.get(1).fillna('missing').astype('category')\n", "merge['subcat2_name'] = merge['category_name'].str.get(2).fillna('missing').astype('category')\n", "merge.drop('category_name', axis=1, inplace=True)\n", "print('[{}] Split categories completed.'.format(time.time() - start_time))\n", "\n", "merge['item_condition_id'] = merge['item_condition_id'].cat.add_categories(['missing']).fillna('missing')\n", "merge['shipping'] = merge['shipping'].cat.add_categories(['missing']).fillna('missing')\n", "merge['item_description'].fillna('missing', inplace=True)\n", "merge['brand_name'] = merge['brand_name'].fillna('missing').astype('category')\n", "print('[{}] Handle missing completed.'.format(time.time() - start_time))"], "outputs": []}, {"cell_type": "markdown", "metadata": {"_uuid": "f439a33691a5e105d128cfd070e8ca1458b90011"}, "source": ["Below I subclass sklearn's CountVectorizer and make it multiprocessing, adding in some efficiencies to the original code.  Thanks to Keras and sklearn code for some ideas. Help wanted on making the pickling for multiprocessing more efficient.\n", "\n", "This class can be chained with sklearn's TfidfTransformer as seen in a few code blocks."]}, {"cell_type": "code", "metadata": {"_uuid": "1564f464320c0a179a36d6ce04eb276df73a81c9", "_cell_guid": "20afb268-4fa5-4363-9ea7-000802c10d5a", "collapsed": true}, "execution_count": null, "source": ["from scipy import sparse as sp\n", "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n", "from multiprocessing import Pool\n", "from collections import Counter\n", "from contextlib import closing\n", "import array\n", "import dill\n", "dill.settings['byref'] = True\n", "from operator import itemgetter\n", "from numbers import Integral\n", "from six import string_types\n", "from bisect import bisect_left, bisect_right\n", "\n", "def apply_packed_function_for_map(x,):\n", "    \"\"\"\n", "    https://stackoverflow.com/questions/8804830/python-multiprocessing-pickling-error\n", "    Unpack dumped function as target function and call it with arguments.\n", "\n", "    :param (dumped_function, item, args, kwargs):\n", "        a tuple of dumped function and its arguments\n", "    :return:\n", "        result of target function\n", "    \"\"\"\n", "    dumped_function, item = x\n", "    target_function = dill.loads(dumped_function)\n", "    res = target_function(item)\n", "    return res\n", "\n", "\n", "def pack_function_for_map(target_function, items):\n", "    \"\"\"\n", "    https://stackoverflow.com/questions/8804830/python-multiprocessing-pickling-error\n", "    Pack function and arguments to object that can be sent from one\n", "    multiprocessing.Process to another. The main problem is:\n", "        \u00abmultiprocessing.Pool.map*\u00bb or \u00abapply*\u00bb\n", "        cannot use class methods or closures.\n", "    It solves this problem with \u00abdill\u00bb.\n", "    It works with target function as argument, dumps it (\u00abwith dill\u00bb)\n", "    and returns dumped function with arguments of target function.\n", "    For more performance we dump only target function itself\n", "    and don't dump its arguments.\n", "    How to use (pseudo-code):\n", "\n", "        ~>>> import multiprocessing\n", "        ~>>> images = [...]\n", "        ~>>> pool = multiprocessing.Pool(100500)\n", "        ~>>> features = pool.map(\n", "        ~...     *pack_function_for_map(\n", "        ~...         super(Extractor, self).extract_features,\n", "        ~...         images,\n", "        ~...         type='png'\n", "        ~...         **options,\n", "        ~...     )\n", "        ~... )\n", "        ~>>>\n", "\n", "    :param target_function:\n", "        function, that you want to execute like  target_function(item, *args, **kwargs).\n", "    :param items:\n", "        list of items for map\n", "    :param args:\n", "        positional arguments for target_function(item, *args, **kwargs)\n", "    :param kwargs:\n", "        named arguments for target_function(item, *args, **kwargs)\n", "    :return: tuple(function_wrapper, dumped_items)\n", "        It returs a tuple with\n", "            * function wrapper, that unpack and call target function;\n", "            * list of packed target function and its' arguments.\n", "    \"\"\"\n", "    dumped_function = dill.dumps(target_function)\n", "    dumped_items = list(zip([dumped_function] * len(items), items))\n", "#     print('done pickling')\n", "    return apply_packed_function_for_map, dumped_items\n", "\n", "class MPCountVectorizer(CountVectorizer):\n", "    \"\"\"Subclass CountVectorizer and make multiprocessing \"\"\"\n", "    def __init__(self, n_jobs, chunk_size, save_vocab, input='content', encoding='utf-8',\n", "                 decode_error='strict', strip_accents=None,\n", "                 lowercase=True, preprocessor=None, tokenizer=None,\n", "                 stop_words=None, token_pattern=r\"(?u)\\b\\w\\w+\\b\",\n", "                 ngram_range=(1, 1), analyzer='word',\n", "                 max_df=1.0, min_df=1, max_features=None,\n", "                 vocabulary=None, binary=False, dtype=np.int64):\n", "\n", "        self.n_jobs = n_jobs\n", "        self.chunk_size = chunk_size\n", "        self.save_vocab = save_vocab\n", "        \n", "        super().__init__(input=input, encoding=encoding,\n", "                 decode_error=decode_error, strip_accents=strip_accents,\n", "                 lowercase=lowercase, preprocessor=preprocessor, tokenizer=tokenizer,\n", "                 stop_words=stop_words, token_pattern=token_pattern,\n", "                 ngram_range=ngram_range, analyzer=analyzer,\n", "                 max_df=max_df, min_df=min_df, max_features=max_features,\n", "                 vocabulary=vocabulary, binary=binary, dtype=dtype)\n", "        \n", "    def _task_multiprocess(self, task, args, vocabulary, analyzer):\n", "        def init(x, y):\n", "            global vocabulary_, analyzer_\n", "            vocabulary_ = x\n", "            analyzer_ = y\n", "            \n", "        with closing(Pool(self.n_jobs, \n", "                          initializer=init, \n", "                          initargs=(vocabulary, analyzer),\n", "                          maxtasksperchild=2)) as pool:\n", "            results = pool.map_async(*pack_function_for_map(task, args))\n", "            results.wait(timeout=600)\n", "            if results.ready():  \n", "                results = results.get()\n", "        return results\n", "    \n", "    def _chunker(self, l, n):\n", "            \"\"\"Yield successive n-sized chunks from l.\"\"\"\n", "            for i in range(0, len(l), n):\n", "                yield l.iloc[i:i + n]\n", "    \n", "    def _feat_vector_task(self, raw_documents):\n", "        \"\"\" Subprocess to create sparse feature matrix\n", "        \"\"\"\n", "        \n", "        j_indices = []\n", "        values = array.array(\"i\")\n", "        indptr = array.array(\"i\")\n", "        indptr.append(0)\n", "        \n", "        for doc in raw_documents:\n", "            feature_counter = {}\n", "            for feature in analyzer_(doc):\n", "                try:\n", "                    feature_idx = vocabulary_[feature]\n", "                    if feature not in feature_counter:\n", "                        feature_counter[feature_idx] = 1\n", "                    else:\n", "                        feature_counter[feature_idx] += 1\n", "                except KeyError:\n", "                    continue\n", "            j_indices += feature_counter.keys()\n", "            values.extend(feature_counter.values())\n", "            indptr.append(len(j_indices))\n", "        \n", "        return sp.csr_matrix((values, j_indices, indptr),\n", "                             shape=(len(indptr) - 1, len(vocabulary_)),\n", "                             dtype=self.dtype)\n", "\n", "    def _word_count_task(self, raw_documents):\n", "\n", "        word_counts = {}\n", "        for doc in raw_documents:\n", "            for tok in analyzer_(doc):\n", "                if tok in word_counts:\n", "                    word_counts[tok] += 1\n", "                else:\n", "                    word_counts[tok] = 1\n", "\n", "        return word_counts\n", "    \n", "    def _count_vocab(self, raw_documents, fixed_vocab):\n", "        \"\"\"Create sparse feature matrix, and vocabulary where fixed_vocab=False\n", "        \"\"\"\n", "\n", "#         print('create chunks')\n", "        chunks = list(self._chunker(raw_documents, self.chunk_size))\n", "        analyzer = self.build_analyzer()\n", "\n", "        if fixed_vocab:\n", "            vocabulary = self.vocabulary_\n", "\n", "        else:\n", "            max_df = self.max_df\n", "            min_df = self.min_df\n", "\n", "#             print('build vocabulary')\n", "            self.vocabulary_ = None\n", "            \n", "            vocabulary = {}\n", "            partial_counts = self._task_multiprocess(\n", "                self._word_count_task,\n", "                chunks,\n", "                None,\n", "                analyzer\n", "            )\n", "#             print('merge vocabulary')\n", "            word_counts = Counter(partial_counts[0])\n", "            for count in partial_counts[1:]:\n", "                word_counts.update(count)\n", "            word_counts = dict(word_counts)\n", "        \n", "#             print('filter vocabulary')\n", "            n_doc = len(raw_documents)\n", "            max_features = self.max_features\n", "            max_doc_count = (max_df\n", "                                 if isinstance(max_df, Integral)\n", "                                 else max_df * n_doc)\n", "            min_doc_count = (min_df\n", "                             if isinstance(min_df, Integral)\n", "                             else min_df * n_doc)\n", "\n", "            if max_doc_count < min_doc_count:\n", "                raise ValueError(\"max_df corresponds to < documents than min_df\")\n", "\n", "#             removed_terms = []\n", "            word_counts = sorted(word_counts.items(), key=itemgetter(1))\n", "            keys, vals = list(zip(*word_counts))\n", "            left_index = bisect_left(vals, min_doc_count)\n", "            right_index = bisect_right(vals, max_doc_count)\n", "            \n", "#             removed_terms += keys[:left_index]\n", "#             removed_terms += keys[right_index:]\n", "            word_counts = word_counts[left_index:right_index]\n", "            if max_features:\n", "#                 removed_terms += keys[:-max_features]\n", "                word_counts = word_counts[-max_features:]\n", "            vocabulary = dict(zip([kv[0] for kv in word_counts], \n", "                                   range(len(word_counts))))\n", "                     \n", "#             self.stop_words_ = removed_terms\n", "\n", "            if not vocabulary:\n", "                raise ValueError(\"empty vocabulary; perhaps the documents only\"\n", "                                 \" contain stop words or min_df, max_df too stringent\")\n", "        \n", "#         self.vocabulary_ = word_counts\n", "        \n", "#         print('create counts')\n", "        partial_X = self._task_multiprocess(\n", "            self._feat_vector_task,\n", "            chunks,\n", "            vocabulary,\n", "            analyzer\n", "        )\n", "                        \n", "#         print('merge counts')\n", "        X = sp.vstack(partial_X)\n", "        X.sort_indices()\n", "        \n", "#         print('done')\n", "        \n", "        return word_counts, X\n", "    \n", "    def fit_transform(self, raw_documents, y=None):\n", "        \"\"\"Learn the vocabulary dictionary and return term-document matrix.\n", "        This is equivalent to fit followed by transform, but more efficiently\n", "        implemented.\n", "        Parameters\n", "        ----------\n", "        raw_documents : iterable\n", "            An iterable which yields either str, unicode or file objects.\n", "        Returns\n", "        -------\n", "        X : array, [n_samples, n_features]\n", "            Document-term matrix.\n", "        \"\"\"\n", "        # We intentionally don't call the transform method to make\n", "        # fit_transform overridable without unwanted side effects in\n", "        # TfidfVectorizer.\n", "        if isinstance(raw_documents, string_types):\n", "            raise ValueError(\n", "                \"Iterable over raw text documents expected, \"\n", "                \"string object received.\")\n", "\n", "        self._validate_vocabulary()\n", "        max_df = self.max_df\n", "        min_df = self.min_df\n", "        max_features = self.max_features\n", "\n", "        vocabulary, X = self._count_vocab(raw_documents,\n", "                                          self.fixed_vocabulary_)\n", "\n", "        if self.binary:\n", "            X.data.fill(1)\n", "            \n", "        if not self.fixed_vocabulary_ and self.save_vocab:\n", "            self.vocabulary_ = vocabulary\n", "\n", "        return X\n"], "outputs": []}, {"cell_type": "markdown", "metadata": {"_uuid": "5ce7bcb31d0efefcfe029007e26a9634ac9e897d"}, "source": ["Below is a selector class to easily feed pandas data to sklearn.  Categorical data can be simply accessed using df.cat.codes, rather than using sklearn's LabelEncoder as seen in some other public kernels."]}, {"cell_type": "code", "metadata": {"_uuid": "207585819cdb4577193a14ae810cdab87e29eaa1", "_cell_guid": "5d54666b-0074-444c-9a8e-df3f094fa825", "collapsed": true}, "execution_count": null, "source": ["from sklearn.base import BaseEstimator, TransformerMixin\n", "\n", "class ItemSelector(BaseEstimator, TransformerMixin):\n", "\n", "    def __init__(self, field, dtype=None):\n", "        self.field = field\n", "        self.dtype = dtype\n", "\n", "    def fit(self, x, y=None):\n", "        return self\n", "\n", "    def transform(self, dataframe):\n", "        if self.dtype == 'category':\n", "            return dataframe[self.field].cat.codes[:, None]\n", "        elif self.dtype == 'numeric':\n", "            return dataframe[self.field][:, None]\n", "        else:\n", "            return dataframe[self.field]"], "outputs": []}, {"cell_type": "code", "metadata": {"_uuid": "343c76325d6d68c71e687b8443a9fa8b1f4bcf9d", "_cell_guid": "c951bc68-0498-450f-add6-2d8cf7368c0e"}, "execution_count": null, "source": ["%%time\n", "from sklearn.pipeline import FeatureUnion, Pipeline\n", "from sklearn.preprocessing import OneHotEncoder\n", "\n", "vectorizer_cat = FeatureUnion([\n", "    ('item_condition_id', Pipeline([\n", "        ('selector', ItemSelector(field='item_condition_id', dtype='category')),\n", "        ('ohe', OneHotEncoder())\n", "    ])),\n", "    ('shipping', Pipeline([\n", "        ('selector', ItemSelector(field='shipping', dtype='category')),\n", "        ('ohe', OneHotEncoder())\n", "    ])),\n", "    ('gencat_name', Pipeline([\n", "        ('selector', ItemSelector(field='gencat_name', dtype='category')),\n", "        ('ohe', OneHotEncoder())\n", "    ])),\n", "    ('subcat1_name', Pipeline([\n", "        ('selector', ItemSelector(field='subcat1_name', dtype='category')),\n", "        ('ohe', OneHotEncoder())\n", "    ])),\n", "    ('subcat2_name', Pipeline([\n", "        ('selector', ItemSelector(field='subcat2_name', dtype='category')),\n", "        ('ohe', OneHotEncoder())\n", "    ])),\n", "    ('brand_name', Pipeline([\n", "        ('selector', ItemSelector(field='brand_name', dtype='category')),\n", "        ('ohe', OneHotEncoder())\n", "    ])),\n", "    ('name', Pipeline([\n", "        ('selector', ItemSelector(field='name')),\n", "        ('cv', MPCountVectorizer(\n", "            n_jobs=4,\n", "            chunk_size=50000,\n", "            save_vocab=False,\n", "            ngram_range=(1, 2),\n", "            stop_words='english',\n", "            min_df=10\n", "        )),\n", "    ])),\n", "    ('item_description', Pipeline([\n", "        ('selector', ItemSelector(field='item_description')),\n", "        ('cv', MPCountVectorizer(\n", "            n_jobs=4,\n", "            chunk_size=50000,\n", "            save_vocab=False,\n", "            ngram_range=(1, 3),\n", "            stop_words='english',\n", "            max_features=1500000\n", "        )),\n", "        ('tfidf', TfidfTransformer()\n", "        )\n", "    ]))\n", "], n_jobs=1)\n", "\n", "X_train = vectorizer_cat.fit_transform(merge)\n", "\n", "print('[{}] Data vectorization completed'.format(time.time() - start_time))"], "outputs": []}, {"cell_type": "code", "metadata": {"_uuid": "9323f3b32f8186389fbe4a64ff179f5c0c1aa07f"}, "execution_count": null, "source": ["print('Total examples: {}, total features: {}'.format(X_train.shape[0], X_train.shape[1]))"], "outputs": []}, {"cell_type": "markdown", "metadata": {"_uuid": "2614e73f6c0ba17c12563c5a773fed2d81baa3b4", "_cell_guid": "6f4260c8-48b9-400f-8fe5-5a1641d7426c", "collapsed": true}, "source": ["About 4 minutes on this run from load to vectorization for ~1.6m features, not bad compared to what I was getting before.\n", "\n", "Lets test out the features with a simple model:"]}, {"cell_type": "code", "metadata": {"_uuid": "ed3f14929562343099d44c8c6e0ac2b1743ca0a0"}, "execution_count": null, "source": ["from sklearn.linear_model import Ridge\n", "from sklearn.model_selection import train_test_split\n", "train_X, valid_X, train_y, valid_y = train_test_split(X_train[:nrow_train], y, test_size = 0.1, random_state = 144)\n", "\n", "model = Ridge(alpha=.5, copy_X=True, fit_intercept=True, max_iter=100,\n", "      normalize=False, random_state=101, solver='auto', tol=0.01)\n", "model.fit(train_X, train_y)\n", "print('[{}] Train ridge completed'.format(time.time() - start_time))\n", "\n", "# valid_X = lsa.transform(valid_X)\n", "predsR = model.predict(valid_X)\n", "print('[{}] Predict ridge completed'.format(time.time() - start_time))\n", "\n", "def rmsle(y, y0):\n", "    assert len(y) == len(y0)\n", "    return np.sqrt(np.mean(np.power(np.log1p(y)-np.log1p(y0), 2)))\n", "print('valid rmsle is', rmsle(np.expm1(predsR), np.expm1(valid_y)))"], "outputs": []}, {"cell_type": "markdown", "metadata": {"_uuid": "716ce41e8b178db750b347ea71fba121bf805a67"}, "source": ["About 6.5 minutes total on this (inconsistent) kernel, down from >15 minutes for me before.\n", "\n", "And yeah better features are out there ;-)!"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"file_extension": ".py", "codemirror_mode": {"version": 3, "name": "ipython"}, "mimetype": "text/x-python", "nbconvert_exporter": "python", "version": "3.6.4", "name": "python", "pygments_lexer": "ipython3"}}, "nbformat": 4}