{"nbformat_minor": 1, "cells": [{"execution_count": null, "source": ["import numpy as np # linear algebra\n", "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n", "import nltk\n", "from nltk.corpus import stopwords\n", "import string\n", "\n", "eng_stopwords = set(stopwords.words(\"english\"))"], "cell_type": "code", "outputs": [], "metadata": {"_uuid": "22307f6388aeabe2580713a1257f7181cdce932e", "_cell_guid": "18b00353-fd83-47b7-8355-340b1c8b2998", "collapsed": true}}, {"execution_count": null, "source": ["train_df = pd.read_csv('../input/spooky-author-identification/train.csv')\n", "test_df = pd.read_csv('../input/spooky-author-identification/test.csv')\n", "sample = pd.read_csv('../input/spooky-author-identification/sample_submission.csv')"], "cell_type": "code", "outputs": [], "metadata": {"_uuid": "15632aeae44d1805ed65d930694a01cc49f9291a", "_cell_guid": "034371d6-81f7-44d6-bd3c-c20f398df1b3", "collapsed": true}}, {"execution_count": null, "source": ["train_df['num_words'] = train_df['text'].apply(lambda x: len(str(x).split()))\n", "test_df['num_words'] = test_df['text'].apply(lambda x: len(str(x).split()))\n"], "cell_type": "code", "outputs": [], "metadata": {"_uuid": "31365d2449f114412b09bbfb3e2d35f8c4e4c832", "_cell_guid": "b8627659-8afc-4c19-9ccb-93bd0b61e17c", "collapsed": true}}, {"execution_count": null, "source": ["train_df['num_unique_words'] = train_df['text'].apply(lambda x: len(set(str(x).split())))\n", "test_df['num_unique_words'] = test_df['text'].apply(lambda x: len(set(str(x).split())))"], "cell_type": "code", "outputs": [], "metadata": {"_uuid": "10df68dcf016d3599fd6b74f2cc5549dd7a2db6b", "_cell_guid": "03567576-8016-48bf-b35a-63aa8fc3f6ff", "collapsed": true}}, {"execution_count": null, "source": ["train_df[\"num_chars\"] = train_df[\"text\"].apply(lambda x: len(str(x)))\n", "test_df[\"num_chars\"] = test_df[\"text\"].apply(lambda x: len(str(x)))\n", "\n", "## Number of stopwords in the text ##\n", "train_df[\"num_stopwords\"] = train_df[\"text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n", "test_df[\"num_stopwords\"] = test_df[\"text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n", "\n", "## Number of punctuations in the text ##\n", "train_df[\"num_punctuations\"] =train_df['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n", "test_df[\"num_punctuations\"] =test_df['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n", "\n", "## Number of title case words in the text ##\n", "train_df[\"num_words_upper\"] = train_df[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n", "test_df[\"num_words_upper\"] = test_df[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n", "\n", "## Number of title case words in the text ##\n", "train_df[\"num_words_title\"] = train_df[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n", "test_df[\"num_words_title\"] = test_df[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n", "\n", "## Average length of the words in the text ##\n", "train_df[\"mean_word_len\"] = train_df[\"text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n", "test_df[\"mean_word_len\"] = test_df[\"text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))"], "cell_type": "code", "outputs": [], "metadata": {"_uuid": "75a138775bf04e3f7d035c8f14b31cf5c0f6a0c6", "_cell_guid": "9851fb02-264c-4ebd-8ec2-dd6587b20644", "collapsed": true}}, {"execution_count": null, "source": ["author_mapping_dict = {'EAP':0, 'HPL':1, 'MWS':2}\n", "train_y = train_df['author'].map(author_mapping_dict)\n", "train_id = train_df['id'].values\n", "test_id = test_df['id'].values"], "cell_type": "code", "outputs": [], "metadata": {"_uuid": "a098d230b75d17384800a9ed1c9fb87afb34c432", "_cell_guid": "097138ec-8702-43b3-a036-cf3fbf47b6b0", "collapsed": true}}, {"execution_count": null, "source": ["cols_to_drop = ['id','text']\n", "train_X = train_df.drop(cols_to_drop+['author'],axis=1)\n", "test_X = test_df.drop(cols_to_drop,axis=1)"], "cell_type": "code", "outputs": [], "metadata": {"_uuid": "53069ea8830700f9ee555d79d822e431614cd9ec", "_cell_guid": "ee4455e3-baef-4524-a9e0-809d7f57537f", "collapsed": true}}, {"execution_count": null, "source": ["train_X.head()"], "cell_type": "code", "outputs": [], "metadata": {"_uuid": "5fb959807ddc9ac721bf2c7c080fc6f298cf5318", "_cell_guid": "8acd31c3-6b0e-4ca9-9b8f-f3c06cfae567", "collapsed": true}}, {"execution_count": null, "source": ["from sklearn.linear_model import LogisticRegression\n", "from sklearn import ensemble, metrics, model_selection, naive_bayes\n", "def runLR(train_X,train_y,test_X,test_y=None,test_X2=None):\n", "    model = LogisticRegression()\n", "    model.fit(train_X,train_y)\n", "    return model.predict_proba(test_X),model.predict_proba(test_X2),model\n", "def runMNB(train_X, train_y, test_X, test_y, test_X2):\n", "    model = naive_bayes.MultinomialNB()\n", "    model.fit(train_X, train_y)\n", "    pred_test_y = model.predict_proba(test_X)\n", "    pred_test_y2 = model.predict_proba(test_X2)\n", "    return pred_test_y, pred_test_y2, model"], "cell_type": "code", "outputs": [], "metadata": {"_uuid": "a77a05792f59f81580dd5f8ce034c39865ca697d", "_cell_guid": "01ce210e-e997-4fb3-bbc0-87fe3cee5dec", "collapsed": true}}, {"execution_count": null, "source": ["kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\n", "cv_scores = []\n", "pred_full_test = 0\n", "pred_train = np.zeros([train_df.shape[0], 3])\n", "for dev_index, val_index in kf.split(train_X):\n", "    dev_X, val_X = train_X.loc[dev_index], train_X.loc[val_index]\n", "    dev_y, val_y = train_y[dev_index], train_y[val_index]\n", "    pred_val_y, pred_test_y, model = runLR(dev_X, dev_y, val_X, val_y, test_X)\n", "    pred_full_test = pred_full_test + pred_test_y\n", "    pred_train[val_index,:] = pred_val_y\n", "    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n", "    \n", "print(\"cv scores : \", cv_scores)"], "cell_type": "code", "outputs": [], "metadata": {"_uuid": "c05892ed7e02ee309d9ddfa6e44e6b5b6e7348c6", "scrolled": true, "_cell_guid": "d90a864e-aa27-4047-a5fc-1656a2801d6a"}}, {"execution_count": null, "source": ["from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n", "from sklearn.decomposition import TruncatedSVD\n", "tfidf_vec = TfidfVectorizer(stop_words='english',ngram_range=(1,3))\n", "full_tfidf = tfidf_vec.fit_transform(train_df['text'].values.tolist()+test_df['text'].values.tolist())\n", "train_tfidf = tfidf_vec.transform(train_df['text'].values.tolist())\n", "test_tfidf = tfidf_vec.transform(test_df['text'].values.tolist())"], "cell_type": "code", "outputs": [], "metadata": {"_uuid": "65857d2cff4847986a823c70b3f4c995140f5b5e", "_cell_guid": "14bb9409-3e5c-47fb-8478-41a5e9402fe2", "collapsed": true}}, {"execution_count": null, "source": ["kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\n", "cv_scores = []\n", "pred_full_test = 0\n", "pred_train = np.zeros([train_df.shape[0], 3])\n", "for dev_index, val_index in kf.split(train_X):\n", "    dev_X, val_X = train_tfidf[dev_index], train_tfidf[val_index]\n", "    dev_y, val_y = train_y[dev_index], train_y[val_index]\n", "    pred_val_y, pred_test_y, model = runMNB(dev_X, dev_y, val_X, val_y, test_tfidf)\n", "    pred_full_test = pred_full_test + pred_test_y\n", "    pred_train[val_index,:] = pred_val_y\n", "    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n", "    \n", "print(\"cv scores : \", cv_scores)"], "cell_type": "code", "outputs": [], "metadata": {"_uuid": "d708f8bdb7e09c304093aeb15bddc5e4a4cbb73e", "_cell_guid": "f5c3aef6-233d-49f4-8f65-4cdb05c90550"}}, {"execution_count": null, "source": ["n_comp =20\n", "svd_tf = TruncatedSVD(n_components=20,algorithm='arpack')\n", "svd_tf.fit(full_tfidf)\n", "train_svd = pd.DataFrame(svd_tf.transform(train_tfidf))\n", "test_svd = pd.DataFrame(svd_tf.transform(test_tfidf))\n", "\n", "train_svd_cols = ['svd_word_'+str(i) for i in range(n_comp)]\n", "test_svd_cols = ['svd_word_'+str(i) for i in range(n_comp)]\n", "\n", "train_df = pd.concat([train_df,train_svd],axis=1)\n", "test_df = pd.concat([test_df,test_svd],axis=1)\n", "train_df.head()"], "cell_type": "code", "outputs": [], "metadata": {"_uuid": "1af107d02b818ac4ee4f418b00c26b3ffce247e4", "_cell_guid": "c58bfe44-354c-4f0c-acf4-178ac80443df"}}, {"execution_count": null, "source": ["cols_to_drop = ['id','text']\n", "train_X = train_df.drop(cols_to_drop+['author'],axis=1)\n", "test_X = test_df.drop(cols_to_drop,axis=1)\n", "kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\n", "cv_scores = []\n", "pred_full_test = 0\n", "pred_train = np.zeros([train_df.shape[0], 3])\n", "for dev_index, val_index in kf.split(train_df):\n", "    dev_X, val_X = train_X.loc[dev_index], train_X.loc[val_index]\n", "    dev_y, val_y = train_y[dev_index], train_y[val_index]\n", "    pred_val_y, pred_test_y, model = runLR(dev_X, dev_y, val_X, val_y, test_X)\n", "    pred_full_test = pred_full_test + pred_test_y\n", "    pred_train[val_index,:] = pred_val_y\n", "    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n", "    \n", "print(\"cv scores : \", cv_scores)"], "cell_type": "code", "outputs": [], "metadata": {"_uuid": "5be6cf1bb8cf165aa763e235b09dc8ab8227326f", "_cell_guid": "a289de63-d894-409f-b0d4-c39b276154a7"}}, {"execution_count": null, "source": ["tfidf_vec = CountVectorizer(stop_words='english', ngram_range=(1,2))\n", "tfidf_vec.fit(train_df['text'].values.tolist() + test_df['text'].values.tolist())\n", "train_tfidf = tfidf_vec.transform(train_df['text'].values.tolist())\n", "test_tfidf = tfidf_vec.transform(test_df['text'].values.tolist())"], "cell_type": "code", "outputs": [], "metadata": {"_uuid": "9b57b0688e7a7569e5c1dd467dfbf13d7f2a0c95", "_cell_guid": "262d59fd-b43e-4ecb-b14d-26c6780f7ba2", "collapsed": true}}, {"execution_count": null, "source": ["cv_scores = []\n", "pred_full_test = 0\n", "pred_train = np.zeros([train_df.shape[0], 3])\n", "kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\n", "for dev_index, val_index in kf.split(train_X):\n", "    dev_X, val_X = train_tfidf[dev_index], train_tfidf[val_index]\n", "    dev_y, val_y = train_y[dev_index], train_y[val_index]\n", "    pred_val_y, pred_test_y, model = runMNB(dev_X, dev_y, val_X, val_y, test_tfidf)\n", "    pred_full_test = pred_full_test + pred_test_y\n", "    pred_train[val_index,:] = pred_val_y\n", "    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n", "print(\"Mean cv score : \", np.mean(cv_scores))\n", "pred_full_test = pred_full_test / 5.\n", "print(\"cv scores : \", cv_scores)\n", "\n", "train_df[\"nb_cvec_eap\"] = pred_train[:,0]\n", "train_df[\"nb_cvec_hpl\"] = pred_train[:,1]\n", "train_df[\"nb_cvec_mws\"] = pred_train[:,2]\n", "test_df[\"nb_cvec_eap\"] = pred_full_test[:,0]\n", "test_df[\"nb_cvec_hpl\"] = pred_full_test[:,1]\n", "test_df[\"nb_cvec_mws\"] = pred_full_test[:,2]\n"], "cell_type": "code", "outputs": [], "metadata": {"_uuid": "dc3aa6f9367b91ff78fbdd27dc9aa885693a153d", "_cell_guid": "225637fd-761b-4202-b27a-0eba3770e8f1"}}, {"execution_count": null, "source": ["cols_to_drop = ['id','text']\n", "train_X = train_df.drop(cols_to_drop+['author'],axis=1)\n", "test_X = test_df.drop(cols_to_drop,axis=1)\n", "kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\n", "cv_scores = []\n", "pred_full_test = 0\n", "pred_train = np.zeros([train_df.shape[0], 3])\n", "for dev_index, val_index in kf.split(train_df):\n", "    dev_X, val_X = train_X.loc[dev_index], train_X.loc[val_index]\n", "    dev_y, val_y = train_y[dev_index], train_y[val_index]\n", "    pred_val_y, pred_test_y, model = runLR(dev_X, dev_y, val_X, val_y, test_X)\n", "    pred_full_test = pred_full_test + pred_test_y\n", "    pred_train[val_index,:] = pred_val_y\n", "    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n", "print(\"cv scores : \", cv_scores)\n"], "cell_type": "code", "outputs": [], "metadata": {"_uuid": "e4d69272b4bd35779eb9d152f3ab46bb0c49ef84", "_cell_guid": "9110ad80-0e30-4e35-85b7-9412396ee87d"}}, {"execution_count": null, "source": ["wv = \"../input/glove-global-vectors-for-word-representation/glove.6B.100d.txt\"\n", "from tqdm import tqdm\n", "from nltk import word_tokenize\n", "def loadWordVecs():\n", "    embeddings_index = {}\n", "    f = open(wv)\n", "    for line in f:\n", "        values = line.split()\n", "        word = values[0]\n", "        coefs = np.asarray(values[1:], dtype='float32')\n", "        embeddings_index[word] = coefs\n", "    f.close()\n", "    print('Found %s word vectors.' % len(embeddings_index))\n", "    return embeddings_index\n", "\n", "def sent2vec(embeddings_index,s): # this function creates a normalized vector for the whole sentence\n", "    words = str(s).lower()\n", "    words = word_tokenize(words)\n", "    words = [w for w in words if not w in stopwords.words('english')]\n", "    words = [w for w in words if w.isalpha()]\n", "    M = []\n", "    for w in words:\n", "        try:\n", "            M.append(embeddings_index[w])\n", "        except:\n", "            continue\n", "    M = np.array(M)\n", "    if M.shape[0] != 0:\n", "        v = M.min(axis=0)\n", "        w = M.max(axis=0)\n", "        return np.concatenate((v,w),axis=0)\n", "    else:\n", "        return np.zeros(200)\n", "\n", "def doGlove(x_train,x_test):\n", "    embeddings_index = loadWordVecs()\n", "    # create sentence vectors using the above function for training and validation set\n", "    xtrain_glove = [sent2vec(embeddings_index,x) for x in tqdm(x_train)]\n", "    xtest_glove = [sent2vec(embeddings_index,x) for x in tqdm(x_test)]\n", "    xtrain_glove = np.array(xtrain_glove)\n", "    xtest_glove = np.array(xtest_glove)\n", "    return xtrain_glove,xtest_glove,embeddings_index\n", "glove_vecs_train,glove_vecs_test,embeddings_index = doGlove(train_df['text'],test_df['text'])"], "cell_type": "code", "outputs": [], "metadata": {"_uuid": "4c93c1156f07290da9ff24e0b30b32da7635be76", "_cell_guid": "8a6be7cd-1b73-4ab8-aec7-0a1696a4a62b"}}, {"execution_count": null, "source": ["\n", "train_df[['sent_vec_'+str(i) for i in range(200)]] = pd.DataFrame(glove_vecs_train.tolist())\n", "test_df[['sent_vec_'+str(i) for i in range(200)]] = pd.DataFrame(glove_vecs_test.tolist())\n", "train_df.head()"], "cell_type": "code", "outputs": [], "metadata": {"_uuid": "0c2f22b4a83a06c41a23a2d982b548a18cf68e9a", "_cell_guid": "67c8f5ea-fa39-4761-861d-14b410903295"}}, {"execution_count": null, "source": ["import xgboost as xgb\n", "def runXGB(train_X, train_y, test_X, test_y=None, test_X2=None, seed_val=0, child=1, colsample=0.3):\n", "    param = {}\n", "    param['objective'] = 'multi:softprob'\n", "    param['eta'] = 0.1\n", "    param['max_depth'] = 3\n", "    param['silent'] = 1\n", "    param['num_class'] = 3\n", "    param['eval_metric'] = \"mlogloss\"\n", "    param['min_child_weight'] = child\n", "    param['subsample'] = 0.8\n", "    param['colsample_bytree'] = colsample\n", "    param['seed'] = seed_val\n", "    num_rounds = 2000\n", "\n", "    plst = list(param.items())\n", "    xgtrain = xgb.DMatrix(train_X, label=train_y)\n", "\n", "    if test_y is not None:\n", "        xgtest = xgb.DMatrix(test_X, label=test_y)\n", "        watchlist = [ (xgtrain,'train'), (xgtest, 'test') ]\n", "        model = xgb.train(plst, xgtrain, num_rounds, watchlist, early_stopping_rounds=50, verbose_eval=20)\n", "    else:\n", "        xgtest = xgb.DMatrix(test_X)\n", "        model = xgb.train(plst, xgtrain, num_rounds)\n", "\n", "    pred_test_y = model.predict(xgtest, ntree_limit = model.best_ntree_limit)\n", "    if test_X2 is not None:\n", "        xgtest2 = xgb.DMatrix(test_X2)\n", "        pred_test_y2 = model.predict(xgtest2, ntree_limit = model.best_ntree_limit)\n", "    return pred_test_y, pred_test_y2, model\n"], "cell_type": "code", "outputs": [], "metadata": {"_uuid": "b14091579444efc91d4fffa0086ae0b4118719a9", "_cell_guid": "6fcb07e7-bb53-4766-8512-ecbca495a7f7", "collapsed": true}}, {"execution_count": null, "source": ["\n", "cols_to_drop = ['id','text']\n", "train_X = train_df.drop(cols_to_drop+['author'],axis=1)\n", "test_X = test_df.drop(cols_to_drop,axis=1)\n", "kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\n", "cv_scores = []\n", "pred_full_test = 0\n", "pred_train = np.zeros([train_df.shape[0], 3])\n", "for dev_index, val_index in kf.split(train_df):\n", "    dev_X, val_X = train_X.loc[dev_index], train_X.loc[val_index]\n", "    dev_y, val_y = train_y[dev_index], train_y[val_index]\n", "    pred_val_y, pred_test_y, model = runXGB(dev_X, dev_y, val_X, val_y, test_X, seed_val=0, colsample=0.7)\n", "    pred_full_test = pred_full_test + pred_test_y\n", "    pred_train[val_index,:] = pred_val_y\n", "    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n", "print(\"cv scores : \", cv_scores)\n"], "cell_type": "code", "outputs": [], "metadata": {"_uuid": "63675a954264084a2bc9cd21a1b44a52d6330000", "_cell_guid": "abbfe337-9c30-4f1d-93c6-365cc1842e0e"}}, {"execution_count": null, "source": ["p = pred_full_test/5\n", "result = pd.DataFrame()\n", "result['id'] = test_id\n", "result['EAP'] = p[:,0]\n", "result['HPL'] = p[:,1]\n", "result['MWS'] = p[:,2]"], "cell_type": "code", "outputs": [], "metadata": {"_uuid": "3786bdab74a74b5d53c6fa1e6333c84ce914728d", "_cell_guid": "a486af79-dabf-471e-9b1d-708ccd60279e", "collapsed": true}}, {"execution_count": null, "source": ["result.head()\n", "result.to_csv(\"result.csv\", index=False)"], "cell_type": "code", "outputs": [], "metadata": {"_uuid": "416a6fcc69a626913fa33d2dc3c6dd2409c7a803", "_cell_guid": "7cf1c637-ef4b-416b-9298-9b6de17ed1e1", "collapsed": true}}, {"execution_count": null, "source": [], "cell_type": "code", "outputs": [], "metadata": {"_uuid": "a74d8f462c0424a154192899bfb78fd986ddd87a", "_cell_guid": "6930c8c9-abbb-4b31-af42-f46b5f3de6e3", "collapsed": true}}], "nbformat": 4, "metadata": {"language_info": {"codemirror_mode": {"version": 3, "name": "ipython"}, "file_extension": ".py", "mimetype": "text/x-python", "version": "3.6.3", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "name": "python"}, "kernelspec": {"language": "python", "display_name": "Python 3", "name": "python3"}}}