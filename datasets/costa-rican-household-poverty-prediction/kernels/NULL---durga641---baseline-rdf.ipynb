{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nfrom collections import Counter\nfrom sklearn import model_selection\nimport seaborn as sns\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\n\nprint(os.listdir(\"../input\"))\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import preprocessing\n# Any results you write to the current directory are saved as output.\n##os.chdir(\"D:\\Data Science\\costatica_poverty_class\")\n\nos.chdir(\"../input\")    ## if you using kaggle\ntrain=pd.read_csv('train.csv')\ntest=pd.read_csv(\"test.csv\")\n\n#### impute missing values\nmean_imputer = preprocessing.Imputer() #By defalut parameter is mean and let it use default one.\nmean_imputer.fit(train[['v2a1','meaneduc','SQBmeaned']]) \ntrain[['v2a1','meaneduc','SQBmeaned']] = mean_imputer.transform(train[['v2a1','meaneduc','SQBmeaned']])\n\n#### droping columns with object data type\nx_train=train.drop(columns=['dependency','edjefa','edjefe','idhogar','Id','rez_esc','v18q1'])\n\nx_train=x_train.drop_duplicates()\n\nmean_imputer = preprocessing.Imputer() #By defalut parameter is mean and let it use default one.\nmean_imputer.fit(test[['v2a1','meaneduc','SQBmeaned']]) \ntest[['v2a1','meaneduc','SQBmeaned']] = mean_imputer.transform(test[['v2a1','meaneduc','SQBmeaned']])\n\n\nx_test=test.drop(columns=['dependency','edjefa','edjefe','idhogar','Id','rez_esc','v18q1'])\n\n\n############################# DETECT OUTLIERS ############################\n\n\ndef detect_outliers(df,n,features):\n    \"\"\"\n    Takes a dataframe df of features and returns a list of the indices\n    corresponding to the observations containing more than n outliers according\n    to the Tukey method.\n    \"\"\"\n    outlier_indices = []\n    \n    # iterate over features(columns)\n    for col in features:\n        # 1st quartile (25%)\n        Q1 = np.percentile(df[col], 25)\n        # 3rd quartile (75%)\n        Q3 = np.percentile(df[col],75)\n        # Interquartile range (IQR)\n        IQR = Q3 - Q1\n        \n        # outlier step\n        outlier_step = 1.5 * IQR\n        \n        # Determine a list of indices of outliers for feature col\n        outlier_list_col = df[(df[col] < Q1 - outlier_step) | (df[col] > Q3 + outlier_step )].index\n        \n        # append the found outlier indices for col to the list of outlier indices \n        outlier_indices.extend(outlier_list_col)\n        \n    # select observations containing more than 2 outliers\n    outlier_indices = Counter(outlier_indices)        \n    multiple_outliers = list( k for k, v in outlier_indices.items() if v > n )\n    \n    return multiple_outliers   \n\n# detect outliers from all fields\nOutliers_to_drop = detect_outliers(x_train,12,list(x_train))\n\nx_train = x_train.drop(Outliers_to_drop, axis = 0).reset_index(drop=True)\ny_train=x_train['Target']\nx_train=x_train.drop(columns=['Target'])\n\n\n\n\nkfold=model_selection.StratifiedKFold(n_splits=10,random_state=105641)\n\n\n############ RANDOM FOREST###################3\nrdt=RandomForestClassifier()\ndt_grid = {'max_depth':list(range(3,8)), 'min_samples_split':[2,3,6,7,8], 'criterion':['gini','entropy']}\ngrid_tree_estimator = model_selection.GridSearchCV(rdt,dt_grid,cv=kfold)\ngrid_tree_estimator.fit(x_train, y_train)\n\n\nx_test['Target']=grid_tree_estimator.predict(x_test)\nx_test['Id']=test[['Id']]\noutput=x_test[['Id','Target']]\n\noutput.to_csv('submission_abc.csv',index=False)\n\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}