{"cells":[{"metadata":{"_uuid":"7df597f2ecb17e539e3b89092e17f7d8f606cf38"},"cell_type":"markdown","source":"# Rolling with the Features\n\nIn a [previous kernel](https://www.kaggle.com/donkeys/quest-for-market-data-outliers-meaning), I explored the outliers in the market data, and the meaning of all those Mktres columns. As I could not quite figure what these features mean, I wanted to try to calculate my own.\n\nA slightly more interesting twist is that not all data is accessible at once. Rather, the training data is given, but for the test data, one has to read a line at a time and any custom features need to be build based on having access to the new data one line at a time.\n\nHere we go."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"## Given description of the market data\n\nThe marketdata contains a variety of returns calculated over different timespans. All of the returns in this set of marketdata have these properties:\n\n* Returns are always calculated either open-to-open (from the opening time of one trading day to the open of another) or close-to-close (from the closing time of one trading day to the open of another).\n* Returns are either raw, meaning that the data is not adjusted against any benchmark, or market-residualized (Mktres), meaning that the movement of the market as a whole has been accounted for, leaving only movements inherent to the instrument.\n* Returns can be calculated over any arbitrary interval. Provided here are 1 day and 10 day horizons.\n* Returns are tagged with 'Prev' if they are backwards looking in time, or 'Next' if forwards looking.\n"},{"metadata":{"_uuid":"53037cc4a43837a485d2e8cacecf70ee39395a83"},"cell_type":"markdown","source":"## Market data columns\n\nWithin the marketdata, you will find the following columns:\n\n* __time(datetime64[ns, UTC])__ - the current time (in marketdata, all rows are taken at 22:00 UTC)\n* __assetCode(object)__ - a unique id of an asset\n* __assetName(category)__ - the name that corresponds to a group of assetCodes. These may be \"Unknown\" if the corresponding assetCode does not have any rows in the news data.\n* __universe(float64)__ - a boolean indicating whether or not the instrument on that day will be included in scoring. This value is not provided outside of the training data time period. The trading universe on a given date is the set of instruments that are avilable for trading (the scoring function will not consider instruments that are not in the trading universe). The trading universe changes daily.\n* __volume(float64)__ - trading volume in shares for the day\n* __close(float64)__ - the close price for the day (not adjusted for splits or dividends)\n* __open(float64)__ - the open price for the day (not adjusted for splits or dividends)\n* __returnsClosePrevRaw1(float64)__ - see returns explanation above\n* __returnsOpenPrevRaw1(float64)__ - see returns explanation above\n* __returnsClosePrevMktres1(float64)__ - see returns explanation above\n* __returnsOpenPrevMktres1(float64)__ - see returns explanation above\n* __returnsClosePrevRaw10(float64)__ - see returns explanation above\n* __returnsOpenPrevRaw10(float64)__ - see returns explanation above\n* __returnsClosePrevMktres10(float64)__ - see returns explanation above\n* __returnsOpenPrevMktres10(float64)__ - see returns explanation above\n* __returnsOpenNextMktres10(float64)__ - 10 day, market-residualized return. This is the target variable used in competition scoring. The market data has been filtered such that returnsOpenNextMktres10 is always not null."},{"metadata":{"_uuid":"75f1b045100f98f0eae89fc9ec3e37d7deb710c4"},"cell_type":"markdown","source":"# The code"},{"metadata":{"trusted":true,"_uuid":"79a672bac76ae0b3e59c9baa0d47ddcb6f2f20ea"},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5b820a2b2c6e67b1115c4a3139745d83f595d661"},"cell_type":"code","source":"from kaggle.competitions import twosigmanews\n#   You  can  only    call    make_env() once, so don't lose it!\nenv = twosigmanews.make_env()\nprint('Done!')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0fabe5685330018cf6be55c47fbb5a2da08578b2"},"cell_type":"code","source":"market_df, news_df = env.get_training_data()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0c06452b98db2ef3c47910aac35d562501e2ea0d"},"cell_type":"code","source":"market_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e530bbe8f18ff1c7bafea1af291d03b4fe376de9"},"cell_type":"code","source":"market_df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"db3de95fb949db549f10f5ca4373271e63692412"},"cell_type":"code","source":"market_df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9aba02bf94d8951430db4973b320d6b362ba2201"},"cell_type":"code","source":"market_df.nunique()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ab8de5b6e0527e4855295ee7f31d1b857be43dba"},"cell_type":"markdown","source":"The following drops the outliers identified in my [previous kernel](https://www.kaggle.com/donkeys/quest-for-market-data-outliers-meaning). But it only drops the outlier row, leaving all the tails it causes in market residual and raw columns that reflect back in time. Which is why I wanted to recalculate my own versions to get rid of all those outlier impacts."},{"metadata":{"trusted":true,"_uuid":"a0a08e7987afaff4bc95a320a6abfc674f587f62"},"cell_type":"code","source":"outlier_indices = [3845015, 3845309, 3845467, 3845835, 3846067, 3846276, 3846636, \n                   50031, 92477, 206676, 459234, 132779, 50374, 276388, 3845946, \n                   616236, 3846151, 49062, 588960, 165718, 25574, 555738, 56387, \n                   1127598, 49050, 50332, 49850, 49531, 627577, 503021, 520681, \n                   471405, 242868, 3264667, 120158, 617101, 133218, 132360, 132809, \n                   133089, 133602, 7273, 194569, 133345, 459489, 132386, 132342, \n                   551857, 133587, 132817, 88284, 133478, 549498, 132821, 193015, 177298]\nmarket_df.drop(outlier_indices, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2705dc4ba9e5e1a76b3b1714d04cede4369f5e03"},"cell_type":"markdown","source":"From all the data, just pick the columns needed to build my own features:"},{"metadata":{"trusted":true,"_uuid":"c53097e8dfab77d1f124a72c6b57e80af55b7a7c"},"cell_type":"code","source":"mt_df = market_df[[\"time\", \"assetCode\", \"assetName\", \"volume\", \"open\", \"close\"]]#, \"open_change_p1\"]]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bc4cfd015803b63b446f96ddfdc0b1ddc9c74376"},"cell_type":"markdown","source":"Pull out the prediction target so I can drop the rest of the original dataframe and save memory:"},{"metadata":{"trusted":true,"_uuid":"9c58b1402a9988b1264d66bd494a7ac8587b936f"},"cell_type":"code","source":"#possibly another target could be made from self-created features, but for now..\ny = market_df[\"returnsOpenNextMktres10\"]\nup = market_df[\"returnsOpenNextMktres10\"] > 0","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"906833861bd7548a51d8d176d60cab086fb2e0e4"},"cell_type":"markdown","source":"Now delete the original dataframe to save memory:"},{"metadata":{"trusted":true,"_uuid":"5b80e9ad9d82978b96c1c2679fdd4a8275b141b4"},"cell_type":"code","source":"del market_df","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1883940ee01004290f2fb972cf407ec1e4e47e97"},"cell_type":"markdown","source":"Verify it looks good:"},{"metadata":{"trusted":true,"_uuid":"0258b3f23244426e72fd3fa2da17755c4e4a4ca9"},"cell_type":"code","source":"mt_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"74e6a24341cee3f9a47cf4571c3c556155693d06"},"cell_type":"code","source":"mt_df.columns.values","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e1035468f98ea195243e5cb847dba25583146ebb"},"cell_type":"markdown","source":"Preprocess dataframes to add my own features for percent change in one day and percent change over 10 days. And 10 day exponential weighted moving average."},{"metadata":{"trusted":true,"_uuid":"bcd97a821b518114516139664ee4f9f5cc6fba7a"},"cell_type":"code","source":"def preprocess(df):\n    #quiet the setting with copy warning for this method or the log will be full of it\n    #since this is called for every row in test set\n    orig_setting = pd.options.mode.chained_assignment\n    \n    pd.options.mode.chained_assignment = None\n    #mt_df.sort_values(\"assetCode\").groupby('assetCode', as_index=False).pct_change().head(10)\n    #needs apply due to pandas bug in 23.0: https://github.com/pandas-dev/pandas/issues/21200\n    #percentage change over 1 day\n    df['pct_chg_open1'] = df.groupby('assetCode')['open'].apply(lambda x: x.pct_change())\n    df['pct_chg_close1'] = df.groupby('assetCode')['close'].apply(lambda x: x.pct_change())\n    #percentage change over 10 days\n    df['pct_chg_open10'] = df.groupby('assetCode')['open'].apply(lambda x: x.pct_change(periods=10))\n    df['pct_chg_close10'] = df.groupby('assetCode')['close'].apply(lambda x: x.pct_change(periods=10))\n    #https://stackoverflow.com/questions/37924377/does-pandas-calculate-ewm-wrong?noredirect=1&lq=1\n\n    #if running in rolling model, drop the old average columns to create new ones.\n    #otherwise it will cause naming issues with overlapping column names..\n    if 'avg_pct_open1' in df.columns:\n        #if there is one, there are all\n        df.drop('avg_pct_open1', axis=1, inplace=True)\n        df.drop('avg_pct_close1', axis=1, inplace=True)\n        df.drop('avg_pct_open10', axis=1, inplace=True)\n        df.drop('avg_pct_close10', axis=1, inplace=True)\n\n    #average changes over 1 and 10 days\n    avg_open1 = df.groupby('time')['pct_chg_open1'].mean().reset_index()\n    avg_open1.rename(columns={'pct_chg_open1': 'avg_pct_open1'}, inplace=True)\n    df = df.merge(avg_open1, how='left', on=['time'])\n\n    avg_close1 = df.groupby('time')['pct_chg_close1'].mean().reset_index()\n    avg_close1.rename(columns={'pct_chg_close1': 'avg_pct_close1'}, inplace=True)\n    df = df.merge(avg_close1, how='left', on=['time'])\n\n    avg_open10 = df.groupby('time')['pct_chg_open10'].mean().reset_index()\n    avg_open10.rename(columns={'pct_chg_open10': 'avg_pct_open10'}, inplace=True)\n    df = df.merge(avg_open10, how='left', on=['time'])\n    \n    avg_close10 = df.groupby('time')['pct_chg_close10'].mean().reset_index()\n    avg_close10.rename(columns={'pct_chg_close10': 'avg_pct_close10'}, inplace=True)\n    df = df.merge(avg_close10, how='left', on=['time'])\n\n    df[\"close_ewma_10\"] = df.groupby('assetName')['pct_chg_close1'].apply(lambda x: x.ewm(span=10).mean())\n    df[\"open_ewma_10\"] = df.groupby('assetName')['pct_chg_open1'].apply(lambda x: x.ewm(span=10).mean())\n    \n    pd.options.mode.chained_assignment = orig_setting\n\n    return df\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"07ac4bfcab93839d8ffa2aa86deef1715767484d"},"cell_type":"markdown","source":"Process the initial training data:"},{"metadata":{"trusted":true,"_uuid":"924fbc3bca97a330dcfd5c7563c2e38ea5a367a5","scrolled":false},"cell_type":"code","source":"mt_df = preprocess(mt_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5badad8bb278fd9cef0ba1e01a30177fbbcbfa1b"},"cell_type":"code","source":"mt_df[mt_df[\"assetCode\"]==\"A.N\"].head(11)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9afd311a03ce1700daaeccd983a7d3b1886653d8","scrolled":true},"cell_type":"code","source":"X_cols = [col for col in mt_df.columns if col not in [\"time\", \"assetCode\", \"assetName\"]]\nX_cols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cc9b95953214c375ea3e0ebb37f2d261a255060c"},"cell_type":"code","source":"X_cols.append(\"time\")\nX = mt_df[X_cols]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2f60e7bf495ca163e0d7586a712621e7883a5076"},"cell_type":"markdown","source":"The usual train-test split. Not sure if time should be counted or not. Since the features are on a single line, maybe not. For other types of models more likely so."},{"metadata":{"trusted":true,"_uuid":"f69999f9ecfbd64ce0687940d88da0826cac75cf"},"cell_type":"code","source":"from sklearn import model_selection\n\nX_train, X_test, y_train, y_test, u_train, u_test = model_selection.train_test_split(X, y, up, test_size=0.25, random_state=99)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6709f470d85775aea39cd7d5e5208602ad1430ef"},"cell_type":"code","source":"y.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e3e9e5da41dea44291a03169dc9b2025d1d19695"},"cell_type":"code","source":"y = (y > 0).astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b7ac0119e15f4cf7abd7c5962899d3f5e8f7006d"},"cell_type":"code","source":"y.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9ccebec4cfce597a1eed10be498d8b69b46949d4"},"cell_type":"code","source":"X_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bc72f03f10d2ffd92ccb065486591b8832280438"},"cell_type":"code","source":"X_train.drop(\"time\", axis=1, inplace=True)\nX_test.drop(\"time\", axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"73ef76101465020ce2963bbdd0603c5a2973bcb0"},"cell_type":"code","source":"print(X_train.shape, X_test.shape)\nprint(u_train.shape, u_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ece6b3ad7bca1f5be73c66b6294ec8b19a6d4c87"},"cell_type":"code","source":"X_cols = [col for col in X_cols if \"time\" not in col]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3708d1182887a1d2e3b00e8074ab2b91133dc777"},"cell_type":"markdown","source":"Just a simple classifier to see the rolling works:"},{"metadata":{"trusted":true,"_uuid":"06202d7720d28af8d4a19baa6d05e525c87947c1","scrolled":false},"cell_type":"code","source":"from catboost import CatBoostClassifier\nimport time\n\ncat_up = CatBoostClassifier(thread_count=4, \n                            #n_estimators=400, \n                            max_depth=10, \n                            eta=0.1, \n                            loss_function='Logloss', \n                            random_seed = 64738,\n                            iterations=1000,\n                            verbose=10)\n\nt = time.time()\nprint('Fitting Up')\ncat_up.fit(X_train, u_train)#, cat_features) \nprint(f'cat Done, time = {time.time() - t}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e8f91f027fbbd9dfcf336d28ba707525d2668276"},"cell_type":"markdown","source":"Find the last date in the dataset."},{"metadata":{"trusted":true,"_uuid":"a8b3d9e9ce732690306dd03ba9d69dd2382ef857"},"cell_type":"code","source":"mt_df.tail(1)[\"time\"][0]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"84f1fff8cded73004200d82021f27a0389313d3b"},"cell_type":"markdown","source":"Slice the dataframe to get the last 30 days of data.  Use this later as basis to build the custom features for test data."},{"metadata":{"trusted":true,"_uuid":"5c05f75ffa9b3370bbaf2a4209a90fecbf8e706a"},"cell_type":"code","source":"last_day = mt_df.tail(1)[\"time\"][0]\npast_offset = pd.Timedelta(30, unit='d')\nfuture_offset = pd.Timedelta(1, unit='d')\ndate1 = last_day - past_offset\ndate2 = last_day + future_offset\nroller = mt_df[mt_df['time'] > date1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"51d2e28a4f35c832515887495152d92940c603a2"},"cell_type":"code","source":"roller.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"000d6b6c7d3f914b1b7332f7ea40fe987594d83a"},"cell_type":"code","source":"roller.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bbab19d0cc87a46162ddd37940bb08686cdea7f5"},"cell_type":"markdown","source":"Now try the tricks on test data that is provided a day at a time:"},{"metadata":{"trusted":true,"_uuid":"4f436bf063f94cb3a58f5052e83a5f5cbd8ba324"},"cell_type":"code","source":"days = env.get_prediction_days()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"08a799ebe0a3484a9639e23a2fd9308527155b8a","scrolled":false},"cell_type":"code","source":"n_days = 0\nprep_time = 0\nprediction_time = 0\npackaging_time = 0\npredicted_confidences = np.array([])\nfor (market_obs_df, news_obs_df, predictions_template_df) in days:\n    n_days +=1\n    print(n_days,end=' ')\n    \n    drop_cols = [col for col in market_obs_df.columns if col not in roller.columns]\n    market_obs_df = market_obs_df.drop(drop_cols, axis=1)\n\n    #add the new rows to the current set of lines used to build the new features\n    roller3 = pd.concat([roller, market_obs_df]).reset_index()\n    roller3.drop(\"index\", axis=1, inplace=True)\n    roller3.columns\n    \n    #Create the custom features for the new data (using the old to calculate it)\n    roller4 = preprocess(roller3)\n    #now slice only the data for the new day/rows along with custom features just added\n    last_day = roller4.tail(1)[\"time\"][0]\n    roller5 = roller4[roller4['time'] >= last_day]\n    #and add it to the current set of rows as basis to calculate the next date in this loop\n    roller = pd.concat([roller, roller5])\n\n    #slice the roller to keep it at 30 days worth of data and save memory\n    past_offset = pd.Timedelta(30, unit='d')\n    date1 = last_day - past_offset\n    roller = roller[roller['time'] > date1]\n\n    t = time.time()\n    # discard assets that are not scored\n    roller5 = roller5[roller5.assetCode.isin(predictions_template_df.assetCode)]\n    X_market_obs = roller5[X_cols]\n    prep_time += time.time() - t\n    \n    t = time.time()\n    #make predictions for the new data received for the new date\n    market_prediction = cat_up.predict_proba(X_market_obs)[:,1]*2 -1\n    predicted_confidences = np.concatenate((predicted_confidences, market_prediction))\n    prediction_time += time.time() -t\n    \n    t = time.time()\n    preds = pd.DataFrame({'assetCode':roller5['assetCode'],'confidence':market_prediction})\n    # insert predictions to template\n    predictions_template_df = predictions_template_df.merge(preds,how='left').drop('confidenceValue',axis=1).fillna(0).rename(columns={'confidence':'confidenceValue'})\n    env.predict(predictions_template_df)\n    packaging_time += time.time() - t\n\nenv.write_submission_file()\ntotal = prep_time + prediction_time + packaging_time\nprint(f'Preparing Data: {prep_time:.2f}s')\nprint(f'Making Predictions: {prediction_time:.2f}s')\nprint(f'Packing: {packaging_time:.2f}s')\nprint(f'Total: {total:.2f}s')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"16cf7939431a0043c3cb5f72d308ec81d5a65cec"},"cell_type":"markdown","source":"Just print some final info:"},{"metadata":{"trusted":true,"_uuid":"19e67b7992fe60ff3d6f2c8720b5927fbc2cd34e"},"cell_type":"code","source":"roller.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a5f7a6aa75cd71b14717345401490438d8c28262"},"cell_type":"code","source":"df_feats = pd.DataFrame()\ndf_feats[\"names\"] = X_cols\ndf_feats[\"weights\"] = cat_up.feature_importances_\ndf_feats.sort_values(by=\"weights\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7ee5a7e2edbcd791f72782122618c5f60c1801e8"},"cell_type":"markdown","source":"That's all folks. I am not sure how to verify this all works as intended, and the custom features are correctly calculated for the new test data. But I think the general concept should work. \n\nThis version also does not seem to score all that well but then I could use it as a basis at least to try other customized versions. For example, calculate rise/fall of an asset in relation to the others in the same period. This should make it closer to the idea of the \"Mketres\" columns in the original dataset.\n\nIf you have some ideas, or otherwise improvement suggestions, or anything.."},{"metadata":{"trusted":true,"_uuid":"01efc27a8fb2bc8f0603e2e4ac9563fcffdb3527"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}