{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"c08382286cbab82b28e24b721f0881b2148868fd"},"cell_type":"code","source":"train = pd.read_csv(\"../input/train.csv\")\ntest = pd.read_csv(\"../input/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"collapsed":true},"cell_type":"code","source":"####################### Train data #############################################\ntrain['HF1'] = train['Horizontal_Distance_To_Hydrology']+train['Horizontal_Distance_To_Fire_Points']\ntrain['HF2'] = abs(train['Horizontal_Distance_To_Hydrology']-train['Horizontal_Distance_To_Fire_Points'])\ntrain['HR1'] = abs(train['Horizontal_Distance_To_Hydrology']+train['Horizontal_Distance_To_Roadways'])\ntrain['HR2'] = abs(train['Horizontal_Distance_To_Hydrology']-train['Horizontal_Distance_To_Roadways'])\ntrain['FR1'] = abs(train['Horizontal_Distance_To_Fire_Points']+train['Horizontal_Distance_To_Roadways'])\ntrain['FR2'] = abs(train['Horizontal_Distance_To_Fire_Points']-train['Horizontal_Distance_To_Roadways'])\ntrain['ele_vert'] = train.Elevation-train.Vertical_Distance_To_Hydrology\n\ntrain['slope_hyd'] = (train['Horizontal_Distance_To_Hydrology']**2+train['Vertical_Distance_To_Hydrology']**2)**0.5\ntrain.slope_hyd=train.slope_hyd.map(lambda x: 0 if np.isinf(x) else x) # remove infinite value if any\n\n#Mean distance to Amenities \ntrain['Mean_Amenities']=(train.Horizontal_Distance_To_Fire_Points + train.Horizontal_Distance_To_Hydrology + train.Horizontal_Distance_To_Roadways) / 3 \n#Mean Distance to Fire and Water \ntrain['Mean_Fire_Hyd']=(train.Horizontal_Distance_To_Fire_Points + train.Horizontal_Distance_To_Hydrology) / 2 \n\n####################### Test data #############################################\ntest['HF1'] = test['Horizontal_Distance_To_Hydrology']+test['Horizontal_Distance_To_Fire_Points']\ntest['HF2'] = abs(test['Horizontal_Distance_To_Hydrology']-test['Horizontal_Distance_To_Fire_Points'])\ntest['HR1'] = abs(test['Horizontal_Distance_To_Hydrology']+test['Horizontal_Distance_To_Roadways'])\ntest['HR2'] = abs(test['Horizontal_Distance_To_Hydrology']-test['Horizontal_Distance_To_Roadways'])\ntest['FR1'] = abs(test['Horizontal_Distance_To_Fire_Points']+test['Horizontal_Distance_To_Roadways'])\ntest['FR2'] = abs(test['Horizontal_Distance_To_Fire_Points']-test['Horizontal_Distance_To_Roadways'])\ntest['ele_vert'] = test.Elevation-test.Vertical_Distance_To_Hydrology\n\ntest['slope_hyd'] = (test['Horizontal_Distance_To_Hydrology']**2+test['Vertical_Distance_To_Hydrology']**2)**0.5\ntest.slope_hyd=test.slope_hyd.map(lambda x: 0 if np.isinf(x) else x) # remove infinite value if any\n\n#Mean distance to Amenities \ntest['Mean_Amenities']=(test.Horizontal_Distance_To_Fire_Points + test.Horizontal_Distance_To_Hydrology + test.Horizontal_Distance_To_Roadways) / 3 \n#Mean Distance to Fire and Water \ntest['Mean_Fire_Hyd']=(test.Horizontal_Distance_To_Fire_Points + test.Horizontal_Distance_To_Hydrology) / 2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b468fd782d12404cff4afa146b485e8dd379a00f","collapsed":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"15e598aff1349712fb5331f2873ac92f0f2ed6e0"},"cell_type":"code","source":"y = train.Cover_Type\nX = train.drop([\"Id\",\"Cover_Type\"],axis =1)\nX_test = test.iloc[:,1:]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"807fb81038f8421c95e22c21d4cade2a686ae744"},"cell_type":"markdown","source":" I calculated feature importance,  Sparse data had about 20% importance. So I divided features into 2 parts"},{"metadata":{"trusted":true,"_uuid":"3c5f4a1510171cf62782ab35b423ae873488645d","collapsed":true},"cell_type":"code","source":"important = list(X.iloc[:,:10].columns) + list(X.iloc[:,-10:].columns)\nX_imp = X[important]\nX_nimp = X.drop(important,axis = 1)\ntest_imp = X_test[important]\ntest_nimp = X_test.drop(important,axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"34508b929b676a5c52fc3c16a16a49eeb0839707","collapsed":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nsc.fit(X_imp)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"8743861c0dd0ebcd20116dea033630c5cec93a0d"},"cell_type":"code","source":"X_imp = sc.transform(X_imp)\ntest_imp = sc.transform(test_imp)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"ccf07b6771fd43b713d868dcbaa7d7e5717fc750"},"cell_type":"code","source":"# from sklearn import preprocessing\n# le = preprocessing.LabelEncoder()\n# y_train = le.fit_transform(y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"863d954f1e5f8f97d0f82d84a694ea224b4a7495","collapsed":true},"cell_type":"code","source":"import os\nimport sys\nimport matplotlib.pyplot as plt\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Input, GlobalMaxPooling1D, Dropout\nfrom keras.layers import Conv1D, MaxPooling1D, Embedding\nfrom keras.models import Model,Sequential\nfrom sklearn.metrics import roc_auc_score\nfrom keras.optimizers import Adam, RMSprop, Adagrad,Adadelta,SGD\nfrom keras.layers import Flatten\nfrom keras.layers.merge import concatenate\nfrom sklearn.model_selection import StratifiedKFold","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"c03eff99e0addb956b2866f0bb6738d44f4c1771"},"cell_type":"code","source":"VALIDATION_SPLIT = 0.10\nBATCH_SIZE = 64\nEPOCHS = 1000","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"a5d6a4fc8f0301111a34bbe897bcbdd08b006253"},"cell_type":"code","source":"# seed = 7\n# np.random.seed(seed)\n# kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n# cvscores = []","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"308e8036d852882a28e7559f9c5a0375f8ec3ded","collapsed":true},"cell_type":"code","source":"input1 = Input(shape=(20, ))\ndense11 = Dense(40, )(input1)\ndense12 = Dense(20, )(dense11)\n\ninput2 = Input(shape=(44, ))\ndense21 = Dense(44, )(input2)\ndense22 = Dense(30, )(dense21)\n#dense22 = Dense(2, )(dense21)\n\nmerged = concatenate([dense12, dense22])\ndense1 = Dense(600, activation='relu')(merged)\ndrop = Dropout(0.1)(dense1)\noutput = Dense(20, activation='softmax')(drop)\n\nmodel = Model([input1,input2], output)    \n\nadam = Adam(lr = 0.001)\nmodel.compile(\n    loss='sparse_categorical_crossentropy',\n    optimizer= \"Adagrad\",\n    metrics=['accuracy']\n)\nr = model.fit(\n    [X_imp,X_nimp],\n    y,\n    batch_size=BATCH_SIZE,\n    epochs=EPOCHS,\n    validation_split=VALIDATION_SPLIT,\n    verbose=0\n)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f8cd86378274455d3a5f61d0ffaa476f53fb982c","collapsed":true},"cell_type":"code","source":"plt.plot(r.history['loss'], label='loss')\nplt.plot(r.history['val_loss'], label='val_loss')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cfea53b3011576731d4ef4153074977da0d28b52","scrolled":true,"collapsed":true},"cell_type":"code","source":"plt.plot(r.history['acc'], label='acc')\nplt.plot(r.history['val_acc'], label='val_acc')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e3bff1bbcfab00808d96b3ec6eb5bc061d7793ed","collapsed":true},"cell_type":"code","source":"print(model.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"616a5f62a497fb7b9f94eba07482323a1a033664","collapsed":true},"cell_type":"code","source":"pred = model.predict([test_imp,test_nimp])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d53b59b9cba33e369e23284feab4d6a935aa1258","collapsed":true},"cell_type":"code","source":"#np.argmax(pred, axis=1)\nsamp_sub = pd.read_csv(\"../input/sample_submission.csv\")\nsamp_sub[\"Cover_Type\"] = np.argmax(pred, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4dc6de47b5e3b7747d811a5951050af35d4469c8","collapsed":true},"cell_type":"code","source":"samp_sub.to_csv(\"sub.csv\",index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"70d76bd39711e3823464db514101a8a55cc0e21c"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}