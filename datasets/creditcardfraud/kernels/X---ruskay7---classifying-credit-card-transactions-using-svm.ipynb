{"nbformat": 4, "cells": [{"execution_count": null, "cell_type": "markdown", "outputs": [], "source": "*********************************************************************************************************************\n**Classification of credit card transactions:**\n*********************************************************************************************************************\n\n \nThis project will try to classify credit card fraudulent/legitimate transactions.  I'm using SVM to build our model and will use cross-validation to check the fit of the model. After studying the dataset, it is clear that the data is skewed and will require proper resampling to train the model effectively.  I studied joparaga3' s model and referred his resampling technique to solve the skewed data problem.\n \nMy plan is to resample the data such that we have a balanced dataset, train our model and check the fit of our model for solving the problem and to choose the best parameters to use in our model. I have used Kaggle for studying many problems and this will be my first submission. Any advice or recommendation is appreciated. ", "metadata": {"_execution_state": "idle", "_uuid": "aa45fd141b871a03fdbe022a0de447605fa63558", "_cell_guid": "2222926c-8c00-48d2-a1d1-6a09acb9b5d3", "collapsed": false}}, {"execution_count": null, "cell_type": "code", "outputs": [], "source": "#importing the libraries\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\nfrom mlxtend.plotting import plot_confusion_matrix\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import GridSearchCV\n", "metadata": {"_execution_state": "idle", "_uuid": "eb5482b4f76150ea2fee34baea7954387cd159cc", "_cell_guid": "1bc3ed8a-9d60-4d56-93f1-69bb452d9d02", "collapsed": false, "trusted": false}}, {"execution_count": null, "cell_type": "markdown", "outputs": [], "source": "*********************************************************************************************************************\n**Loading the Dataset**\n*********************************************************************************************************************", "metadata": {"_execution_state": "idle", "_uuid": "efb3e24233af0ba4a377b3b67343672172cc3b52", "_cell_guid": "ce1ea15f-5d1f-48f6-8ebb-dbc0998e5416", "collapsed": false}}, {"execution_count": null, "cell_type": "code", "outputs": [], "source": "data= pd.read_csv(\"../input/creditcard.csv\")\nprint(data.head())", "metadata": {"_execution_state": "idle", "_uuid": "8e76847efbdfd95ed61ad481292e602d1223c182", "_cell_guid": "25f9e800-f701-40dc-816a-02f2910c0e7c", "trusted": false}}, {"execution_count": null, "cell_type": "markdown", "outputs": [], "source": " The dataset has 31 features. Most of the features are scaled and are\n   results from PCA.  \n\nThe Amount column needs to be scaled and the Class column will be used as our dependent variable.\n Now let's visualize the Class feature to check the balance of features.", "metadata": {"_execution_state": "idle", "_uuid": "d30c691c25b41e6e8963e9ff916b641bceb13a81", "_cell_guid": "1a6fb4c1-9105-431c-8696-e829424dbc49", "collapsed": false}}, {"execution_count": null, "cell_type": "markdown", "outputs": [], "source": "*********************************************************************************************************************\n**Visualization of the \"Class\" feature**\n*********************************************************************************************************************", "metadata": {"_execution_state": "idle", "_uuid": "c27f0e27a807f84304257765fdfcd43473361981", "_cell_guid": "53e3b50b-8d10-46d5-8c83-a4a58980745a", "collapsed": false}}, {"execution_count": null, "cell_type": "code", "outputs": [], "source": "count_Class=pd.value_counts(data[\"Class\"], sort= True)\ncount_Class.plot(kind= 'bar')", "metadata": {"_execution_state": "idle", "_uuid": "d3b8065673b57dd1be9ae168450e59437d8404cb", "_cell_guid": "c607a3bd-232a-4545-8ace-edcf8f31e574", "collapsed": false, "trusted": false}}, {"execution_count": null, "cell_type": "markdown", "outputs": [], "source": "The dataset is highly unbalanced and is understandable. The Class 0 represents the normal transactions while Class 1 represents the fraudulent transactions.", "metadata": {"_execution_state": "idle", "_uuid": "b1dec1277397988d0f218cc073a9d8a2cdb99966", "_cell_guid": "50d20836-631f-4499-ad70-2547330bf83e", "collapsed": false}}, {"execution_count": null, "cell_type": "markdown", "outputs": [], "source": "*********************************************************************************************************************\n**Let explore the \"Class\" feature to decide on the measure to solve this unbalance of data**\n*********************************************************************************************************************", "metadata": {"_execution_state": "idle", "_uuid": "904e1b878ae90fb9157ee2299c1901e68f79cfb4", "_cell_guid": "22d5d6fa-25b0-4c15-90ad-80237a2360b6", "collapsed": false}}, {"execution_count": null, "cell_type": "code", "outputs": [], "source": "No_of_frauds= len(data[data[\"Class\"]==1])\nNo_of_normals = len(data[data[\"Class\"]==0])\nprint(\"The number of fraudulent transactions( Class 1) are: \", No_of_frauds)\nprint(\"The number of normal transactions( Class 0) are: \", No_of_normals)\ntotal= No_of_frauds + No_of_normals\nFraud_percent= (No_of_frauds / total)*100\nNormal_percent= (No_of_normals / total)*100\nprint(\"Class 0 percentage = \", Normal_percent)\nprint(\"Class 1 percentage = \", Fraud_percent)", "metadata": {"_execution_state": "idle", "_uuid": "414154cc466518ab6ff9df70df1c72a2950b80c7", "_cell_guid": "95aa0c68-c357-4047-b662-af1f4b595a07", "collapsed": false, "trusted": false}}, {"execution_count": null, "cell_type": "markdown", "outputs": [], "source": " 1. It is clear that Class 1 is under-represented and accounts for only 0.17 % of the whole dataset. If we train our model using this dataset, the model will be inefficient and it will be trained to predict only Class 0 because it will not have sufficient training data. \n 2. One of the other problems of using this skewed dataset to train our model is that since Class 1 is under-represented, the model will assume that it is a rare case and will try to predict positive due to the lack of training data.\n 3. We may get a high accuracy when we test our model but we should not be confused by this because our dataset does not have a balanced test data. Hence, we have to rely on the recall which relies on TP and FP.\n 4. In cases where we have skewed data, adding additional data of the under-represented feature( over-sampling) is an option. Since we don't have that option we have to resort to under-sampling.\n 5. Under-sampling of the dataset involves keeping all our under-represented data( Class 1) while adding the same number of features of Class 0 to create a new dataset comprising of an equal representation from both classes.", "metadata": {"_execution_state": "idle", "_uuid": "0275a51b3724bceea77882b05f3d7b8eef91436a", "_cell_guid": "f85098a8-9eca-42ca-81fa-4bb2c79e1516", "collapsed": false}}, {"execution_count": null, "cell_type": "code", "outputs": [], "source": "#list of fraud indices\nfraud_index= np.array(data[data[\"Class\"]==1].index)\n\n#getting the list of normal indices from the full dataset\nnormal_index= data[data[\"Class\"]==0].index\n\n#choosing random normal indices equal to the number of fraudulent transactions\nrandom_normal_indices= np.random.choice(normal_index, No_of_frauds, replace= False)\nrandom_normal_indices= np.array(random_normal_indices)\n\n# concatenate fraud index and normal index to create a list of indices\nundersampled_indices= np.concatenate([fraud_index, random_normal_indices])\n\n#use the undersampled indices to build the undersampled_data dataframe\nundersampled_data= data.iloc[undersampled_indices, :]\n\nprint(undersampled_data.head())", "metadata": {"_execution_state": "idle", "_uuid": "23fdb24481ed04e25a9e4812eaafc2a2d9712b46", "_cell_guid": "f1a7d1e6-06cc-435d-8ff8-7d04573acfea", "collapsed": false, "trusted": false}}, {"execution_count": null, "cell_type": "markdown", "outputs": [], "source": "*********************************************************************************************************************\n**Let explore the Undersampled data**\n*********************************************************************************************************************", "metadata": {"_execution_state": "idle", "_uuid": "1748abeeee4a69a866e189babea0516598a92fe2", "_cell_guid": "af371b8a-ff79-4384-8d36-2463376bf7c7", "collapsed": false}}, {"execution_count": null, "cell_type": "code", "outputs": [], "source": "No_of_frauds_sampled= len(undersampled_data[undersampled_data[\"Class\"]== 1])\n\nNo_of_normals_sampled = len(undersampled_data[undersampled_data[\"Class\"]== 0])\n\nprint(\"The number of fraudulent transactions( Class 1) are: \", No_of_frauds_sampled)\nprint(\"The number of normal transactions( Class 0) are: \", No_of_normals_sampled)\ntotal_sampled= No_of_frauds_sampled + No_of_normals_sampled\nprint(\"The total number of rows of both classes are: \", total_sampled)\n\nFraud_percent_sampled= (No_of_frauds_sampled / total_sampled)*100\nNormal_percent_sampled= (No_of_normals_sampled / total_sampled)*100\nprint(\"Class 0 percentage = \", Normal_percent_sampled)\nprint(\"Class 1 percentage = \", Fraud_percent_sampled)\n\n#Check the data count now\ncount_sampled=pd.value_counts(undersampled_data[\"Class\"], sort= True)\ncount_sampled.plot(kind= 'bar')", "metadata": {"_execution_state": "idle", "_uuid": "a4f6bdb73eb848718bc9acfde102419d4e9fb031", "_cell_guid": "9101a7ce-b015-457b-9ef3-f58b93f53cc6", "collapsed": false, "trusted": false}}, {"execution_count": null, "cell_type": "markdown", "outputs": [], "source": "So, we have to train/test our model using 984 rows of data. It is not much but it will have to suffice. Let's find out if we can generate good predictions using this dataset.", "metadata": {"_execution_state": "idle", "_uuid": "972f09c9d78cad7bd213d967c8ef0803e6a2967d", "_cell_guid": "f2d4e0a1-a618-49a3-bd34-255795b71539", "collapsed": false}}, {"execution_count": null, "cell_type": "markdown", "outputs": [], "source": "*********************************************************************************************************************\n**Scaling the Undersampled_data:** \n*********************************************************************************************************************", "metadata": {"_execution_state": "idle", "_uuid": "65e50c1644c4eea021ecccbea70de403ba91cbb2", "_cell_guid": "2cd4733b-0d9e-455b-9a25-906cfbd50230", "collapsed": false}}, {"execution_count": null, "cell_type": "code", "outputs": [], "source": "#We have to scale the Amount feature before fitting our model to our dataset\n\nsc= StandardScaler()\nundersampled_data[\"scaled_Amount\"]=  sc.fit_transform(undersampled_data.iloc[:,29].values.reshape(-1,1))\n\n#dropping time and old amount column\nundersampled_data= undersampled_data.drop([\"Time\",\"Amount\"], axis= 1)\n\nprint(undersampled_data.head())", "metadata": {"_execution_state": "idle", "_uuid": "5791f05405858347452bb0efa39d27c2d6de8a53", "_cell_guid": "8eb071ab-d537-48ba-b26e-92293cc6a1fa", "collapsed": false, "trusted": false}}, {"execution_count": null, "cell_type": "markdown", "outputs": [], "source": "\n*********************************************************************************************************************\n**Separating the dependent and independent variables**\n*********************************************************************************************************************\n\n", "metadata": {"_execution_state": "idle", "_uuid": "cd44c113af6c903ef6068139635111a8dac3c599", "_cell_guid": "708ce0da-44bf-4af3-a121-da2505a0cd89", "collapsed": false}}, {"execution_count": null, "cell_type": "code", "outputs": [], "source": "X= undersampled_data.iloc[:, undersampled_data.columns != \"Class\"].values\n\ny= undersampled_data.iloc[:, undersampled_data.columns == \"Class\"].values\n\n", "metadata": {"_execution_state": "idle", "_uuid": "5324ab238e663b638aa7494952f5f1c9b96a19b3", "_cell_guid": "2e71a33b-53c7-464c-b037-125b5fa82f23", "collapsed": false, "trusted": false}}, {"execution_count": null, "cell_type": "markdown", "outputs": [], "source": "*********************************************************************************************************************\n**Splitting the undersampled data into training set and test set**\n*********************************************************************************************************************", "metadata": {"_execution_state": "idle", "_uuid": "92aa07ce6e23ae51e1a741aac37c05b23ce541eb", "_cell_guid": "620f660e-02dd-4a32-b9f0-ccfe225181ce", "collapsed": false}}, {"execution_count": null, "cell_type": "code", "outputs": [], "source": "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.25, random_state= 0)\nprint(\"The split of the under_sampled data is as follows\")\nprint(\"X_train: \", len(X_train))\nprint(\"X_test: \", len(X_test))\nprint(\"y_train: \", len(y_train))\nprint(\"y_test: \", len(y_test))", "metadata": {"_execution_state": "idle", "_uuid": "f46ee54536ce2f0c15cf5436b2a015ae33096cb7", "_cell_guid": "c0b37bce-0937-493b-a666-f5d92f62f49e", "collapsed": false, "trusted": false}}, {"execution_count": null, "cell_type": "markdown", "outputs": [], "source": "*********************************************************************************************************************\n**SVM- Fitting our dataset to the classifier**\n*********************************************************************************************************************", "metadata": {"_execution_state": "idle", "_uuid": "b040fbd529061c1e12a81317ba85ec0ce6998dfc", "_cell_guid": "c0667ffd-2d8e-4dd6-9c0d-101c9726cbce", "collapsed": false}}, {"execution_count": null, "cell_type": "code", "outputs": [], "source": "#Using the gaussian kernel to build the initail model. Let us see if this is the best parameter later\nclassifier= SVC(C= 1, kernel= 'rbf', random_state= 0)\nclassifier.fit(X_train, y_train.ravel())\n", "metadata": {"_execution_state": "idle", "_uuid": "751d2fb65af3d3e17e8dc9d563c2cc8a9ec8377e", "_cell_guid": "24ba6c31-8bc9-43b2-9b81-5c702cad73f8", "collapsed": false, "trusted": false}}, {"execution_count": null, "cell_type": "markdown", "outputs": [], "source": "*********************************************************************************************************************\n**Predicting the class by fitting X_test to our classifier**\n*********************************************************************************************************************", "metadata": {"_execution_state": "idle", "_uuid": "04f01ff0699c86f15259e17bf0badd0f31c23ee9", "_cell_guid": "ae9d88f8-68b0-45c3-a844-4ea378a5bbd7", "collapsed": false}}, {"execution_count": null, "cell_type": "code", "outputs": [], "source": "#Predict the class using X_test\ny_pred = classifier.predict(X_test)", "metadata": {"_execution_state": "idle", "_uuid": "6f2a4a07f93cfb68b518bd3c81faec4085cdfe8a", "_cell_guid": "e19880e1-bfec-433b-867d-87bd201ce5b9", "collapsed": false, "trusted": false}}, {"execution_count": null, "cell_type": "markdown", "outputs": [], "source": "*********************************************************************************************************************\n**The Confusion matrix**\n*********************************************************************************************************************", "metadata": {"_execution_state": "idle", "_uuid": "457fb31a770d26e2e23bce60235307dbe2c70337", "_cell_guid": "46f6e5bd-59a4-4663-806a-d768120868da", "collapsed": false}}, {"execution_count": null, "cell_type": "markdown", "outputs": [], "source": "The Confusion matrix can be used to calculate the accuracy, recall, and precision from our model.  Let us fit our confusion matrix class with y_pred(predicted results) and y_test( actual results)", "metadata": {"_execution_state": "idle", "_uuid": "3ee34b0abf73c6b3e6d3ce551ae7d70a8373f5de", "_cell_guid": "65de0686-281f-47c0-8713-03b05f4e8313", "collapsed": false}}, {"execution_count": null, "cell_type": "code", "outputs": [], "source": "#cm1 is the confusion matrix 1 which uses the undersampled dataset\ncm1 = confusion_matrix(y_test, y_pred)\n\n", "metadata": {"_execution_state": "idle", "_uuid": "28825fee80773202d918c6a31d9c00827a3a0882", "_cell_guid": "8687ae94-33aa-4250-b48a-b1f76ee05732", "collapsed": false, "trusted": false}}, {"execution_count": null, "cell_type": "markdown", "outputs": [], "source": "*********************************************************************************************************************\n**Visualizing the confusion matrix using mlxtend library**\n*********************************************************************************************************************", "metadata": {"_execution_state": "idle", "_uuid": "630a716859c0aa379208da96d0b23e90ced87495", "_cell_guid": "b6825ed0-ea92-467b-831f-36dad5097762", "collapsed": false}}, {"execution_count": null, "cell_type": "code", "outputs": [], "source": "def confusion_matrix_1(CM):\n    fig, ax = plot_confusion_matrix(conf_mat=CM)\n    plt.title(\"The Confusion Matrix 1 of Undersampled dataset\")\n    plt.ylabel(\"Actual\")\n    plt.xlabel(\"Predicted\")\n    plt.show()\n\n    print(\"The accuracy is \"+str((CM[1,1]+CM[0,0])/(CM[0,0] + CM[0,1]+CM[1,0] + CM[1,1])*100) + \" %\")\n    print(\"The recall from the confusion matrix is \"+ str(CM[1,1]/(CM[1,0] + CM[1,1])*100) +\" %\")\nconfusion_matrix_1(cm1)", "metadata": {"_execution_state": "idle", "_uuid": "ead931cc6f20b517f45b7cea033679622fea4f9a", "_cell_guid": "9273c16d-f266-4cc8-8866-4c0c9c512c0a", "collapsed": false, "trusted": false}}, {"execution_count": null, "cell_type": "markdown", "outputs": [], "source": "We have good accuracy and recall score so our model is giving good results when using the undersampled data. The real test will be when we apply the model to our whole dataset(skewed).", "metadata": {"_execution_state": "idle", "_uuid": "f8041846a6a4c55153b4412bc9a885c655820ef1", "_cell_guid": "4481f8e5-c4ae-4ec9-a823-21ba763b72c5", "collapsed": false}}, {"execution_count": null, "cell_type": "markdown", "outputs": [], "source": "*********************************************************************************************************************\n**Applying Cross Validation to determine accuracy of our model**\n*********************************************************************************************************************", "metadata": {"_execution_state": "idle", "_uuid": "f1b379729ad039374bd804f072a8aa52b295e86c", "_cell_guid": "9ca3a6d1-1959-44ba-aa40-1fad96f8bcc6", "collapsed": false}}, {"execution_count": null, "cell_type": "code", "outputs": [], "source": "#Applying 10 fold cross validation\naccuracies = cross_val_score(estimator = classifier, X=X_train, y = y_train.ravel(), cv = 10)\nmean_accuracy= accuracies.mean()*100\nstd_accuracy= accuracies.std()*100\nprint(\"The mean accuracy in %: \", accuracies.mean()*100)\nprint(\"The standard deviation in % \", accuracies.std()*100)\nprint(\"The accuracy of our model in % is betweeen {} and {}\".format(mean_accuracy-std_accuracy, mean_accuracy+std_accuracy))", "metadata": {"_execution_state": "idle", "_uuid": "d4f6622cff7ff5d318f3484f1efe2c58740b853b", "_cell_guid": "3ad30cf4-9d56-470b-947c-f64fb23a33d6", "collapsed": false, "trusted": false}}, {"execution_count": null, "cell_type": "markdown", "outputs": [], "source": "*********************************************************************************************************************\n**Applying GridSearch to find if our model is the best fit and also to determine the best parameters to train our model.** \n*********************************************************************************************************************", "metadata": {"_execution_state": "idle", "_uuid": "f1df0d65c9e60afe7a735904da6dcf7bdf4dc488", "_cell_guid": "e5bd27dd-5450-4026-8941-5165d19c06e8", "collapsed": false}}, {"execution_count": null, "cell_type": "code", "outputs": [], "source": "#applying gridsearchCV to our classifier\n#Specifying the parameters in dictionaries to try out different parameters.\n#The GridSearchCV will try all the parameters and give us the best parameters\n\nparameters = [{'C': [1, 10, 100, 1000], 'kernel': ['linear']},\n              {'C': [1, 10, 100, 1000], 'kernel': ['rbf'], 'gamma': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]}]\n\ngrid_search = GridSearchCV(estimator = classifier,\n                           param_grid = parameters,\n                           scoring = 'accuracy',\n                           cv = 10,\n                           n_jobs = -1)\ngrid_search = grid_search.fit(X_train, y_train.ravel())\nbest_accuracy = grid_search.best_score_\nprint(\"The best accuracy using gridSearch is\", best_accuracy)\n\nbest_parameters = grid_search.best_params_\nprint(\"The best parameters for using this model is\", best_parameters)", "metadata": {"_execution_state": "idle", "_uuid": "16467a678154cb21036d9a9b6390b6d144c37e1d", "_cell_guid": "59a0a93c-4dce-432b-bc1f-7f9b776498d7", "collapsed": false, "trusted": false}}, {"execution_count": null, "cell_type": "markdown", "outputs": [], "source": "*********************************************************************************************************************\n**Now we use the best parameters to train the SVM classifier:**\n*********************************************************************************************************************", "metadata": {"_execution_state": "idle", "_uuid": "9d1382b9091c440ccb891b81158badeb36dbd8e3", "_cell_guid": "696063ec-d809-46b5-a1d4-b93134434189", "collapsed": false}}, {"execution_count": null, "cell_type": "code", "outputs": [], "source": "#fitting the model with the best parameters\nclassifier_with_best_parameters =  SVC(C= best_parameters[\"C\"], kernel= best_parameters[\"kernel\"], random_state= 0)\nclassifier_with_best_parameters.fit(X_train, y_train.ravel())\n#predicting the Class \ny_pred_best_parameters = classifier_with_best_parameters.predict(X_test)\n#creating a confusion matrix\n#cm2 is the confusion matrix  which uses the best parameters\ncm2 = confusion_matrix(y_test, y_pred_best_parameters)\n#visualizing the confusion matrix\ndef confusion_matrix_2(CM):\n    fig, ax = plot_confusion_matrix(conf_mat= CM)\n    plt.title(\"The Confusion Matrix 2 of Undersampled dataset using best_parameters\")\n    plt.ylabel(\"Actual\")\n    plt.xlabel(\"Predicted\")\n    plt.show()\n    print(\"The accuracy is \"+str((CM[1,1]+CM[0,0])/(CM[0,0] + CM[0,1]+CM[1,0] + CM[1,1])*100) + \" %\")\n    print(\"The recall from the confusion matrix is \"+ str(CM[1,1]/(CM[1,0] + CM[1,1])*100) + \" %\")\nconfusion_matrix_2(cm2)\n#also printing the confusion matrix 1 for comparison\nconfusion_matrix_1(cm1)\n", "metadata": {"_execution_state": "idle", "_uuid": "f2cee14e492c104301051b88eba7aab328b11d91", "_cell_guid": "17ebb25d-8e84-4f4e-8c6f-77646a602138", "collapsed": false, "trusted": false}}, {"execution_count": null, "cell_type": "markdown", "outputs": [], "source": "The accuracy of our model has improved by using the best_parameters.  We were able to classify more samples in the True negative region. We can re-run the models by using different range values of C to improve the accuracy, but I'm quite happy with both the recall and the accuracy. Now its time to test our model with the whole dataset.", "metadata": {"_execution_state": "idle", "_uuid": "0a9208348691d920bc258cb8bad3c32d4909d5e8", "_cell_guid": "a879f34a-40ed-4973-9998-c5f868a05818", "collapsed": false}}, {"execution_count": null, "cell_type": "markdown", "outputs": [], "source": "*********************************************************************************************************************\n**Testing the model against the full dataset(skewed)**\n*********************************************************************************************************************", "metadata": {"_execution_state": "idle", "_uuid": "9df32dc639e60ff00637feb87baffc3cd02bd514", "_cell_guid": "a5a7042d-de4a-49a5-8c90-e1abf553c183", "collapsed": false}}, {"execution_count": null, "cell_type": "code", "outputs": [], "source": "#creating a new dataset to test our model\ndatanew= data.copy()\n\n#Now to test the model with the whole dataset\ndatanew[\"scaled_Amount\"]=  sc.fit_transform(datanew[\"Amount\"].values.reshape(-1,1))\n\n#dropping time and old amount column\ndatanew= datanew.drop([\"Time\",\"Amount\"], axis= 1)\n\n#separating the x and y variables to fit our model\nX_full= datanew.iloc[:, undersampled_data.columns != \"Class\"].values\n\ny_full= datanew.iloc[:, undersampled_data.columns == \"Class\"].values\n\n\n", "metadata": {"_execution_state": "idle", "_uuid": "456e322c461195be617e46914378a9cbe813ae73", "_cell_guid": "dc4dce1b-787b-45d3-8238-14ac47b778dd", "collapsed": false, "trusted": false}}, {"execution_count": null, "cell_type": "markdown", "outputs": [], "source": "*********************************************************************************************************************\n**Splitting the full dataset into training and test set**\n*********************************************************************************************************************\n", "metadata": {"_execution_state": "idle", "_uuid": "2438f47c26caf6f58c61a9549033a0e83bbad9e9", "_cell_guid": "4dcb14a0-c11a-4f95-9ab6-5b7ee7cdea04", "collapsed": false}}, {"execution_count": null, "cell_type": "code", "outputs": [], "source": "#splitting the full dataset into training and test set\nX_train_full, X_test_full, y_train_full, y_test_full = train_test_split(X_full, y_full, test_size= 0.25, random_state= 0)\n\nprint(\"The split of the full dataset is as follows\")\nprint(\"X_train_full: \", len(X_train_full))\nprint(\"X_test_full: \", len(X_test_full))\nprint(\"y_train_full: \", len(y_train_full))\nprint(\"y_test_full: \", len(y_test_full))", "metadata": {"_execution_state": "idle", "_uuid": "2dbf44983e98d03fb9e2b437b8ed5741d0c0cc5b", "_cell_guid": "7ccd7f13-29da-4790-99c5-5e7312d0215c", "collapsed": false, "trusted": false}}, {"execution_count": null, "cell_type": "markdown", "outputs": [], "source": "*********************************************************************************************************************\n**Predicting y_pred, ie. our class value using X_test_full**\n*********************************************************************************************************************", "metadata": {"_execution_state": "idle", "_uuid": "dcabbf71a04b4fefcaa15dae2497f50edf454ca5", "_cell_guid": "38c3e691-5415-49ac-9b19-c5c60e28960a", "collapsed": false}}, {"execution_count": null, "cell_type": "code", "outputs": [], "source": "#predicting y_pred_full_dataset\ny_pred_full_dataset= classifier_with_best_parameters.predict(X_test_full)\n\n#confusion matrix usign y_test_full and ypred_full\ncm3 = confusion_matrix(y_test_full, y_pred_full_dataset)", "metadata": {"_execution_state": "idle", "_uuid": "ea12ce9a827e306665ea28606dd0c767589f6b0e", "_cell_guid": "5ab963fe-a954-46ca-9105-64ddddadbc33", "collapsed": false, "trusted": false}}, {"execution_count": null, "cell_type": "markdown", "outputs": [], "source": "*********************************************************************************************************************\n**Plotting the Confusion matrix 3 and comparing it with Confusion matrix 2 ( the one with the best parameters)**\n*********************************************************************************************************************", "metadata": {"_execution_state": "idle", "_uuid": "a88d60dcb8db4f61e6b320aa8f18718d9c0b8ae2", "_cell_guid": "0068c78b-9058-45e6-bd9b-48f29f00c7b2", "collapsed": false}}, {"execution_count": null, "cell_type": "code", "outputs": [], "source": "def confusion_matrix_3(CM):\n    fig, ax = plot_confusion_matrix(conf_mat= CM)\n    plt.title(\"The Confusion Matrix 3 of full dataset using best_parameters\")\n    plt.ylabel(\"Actual\")\n    plt.xlabel(\"Predicted\")\n    plt.show()\n    print(\"The accuracy is \"+str((CM[1,1]+CM[0,0])/(CM[0,0] + CM[0,1]+CM[1,0] + CM[1,1])*100) + \" %\")\n    print(\"The recall from the confusion matrix is \"+ str(CM[1,1]/(CM[1,0] + CM[1,1])*100) +\" %\")\nconfusion_matrix_3(cm3)\n\n#printing confusion matrix 2 for comparison with training set results\nconfusion_matrix_2(cm2)", "metadata": {"_execution_state": "idle", "_uuid": "3d7e4d0d75e04909df7025edb53b7a04f923c4fd", "_cell_guid": "11419987-5cf3-4fbe-b8c8-7488a453098f", "collapsed": false, "trusted": false}}, {"execution_count": null, "cell_type": "markdown", "outputs": [], "source": "By fitting our model to the whole dataset, we have obtained similar accuracy and recall as our training set results. I'm quite happy with the results although I believe we can further improve the model by having more training data and also by either using a better algorithm or by modifying our parameters. I will try implementing the same using decision trees, ANN and compare the results. This is my first Kaggle submission and I'd appreciate any suggestions to improve my model. Thanks!", "metadata": {"_execution_state": "idle", "_uuid": "7333a6840b61e63d5d5e6d82eb1231ae8650f763", "_cell_guid": "43e97f05-639c-4dc8-9749-ae52fdffdd6d", "collapsed": false}}], "nbformat_minor": 0, "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3", "language": "python"}, "language_info": {"name": "python", "file_extension": ".py", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "version": "3.6.1", "pygments_lexer": "ipython3", "nbconvert_exporter": "python"}}}