{"cells":[{"metadata":{"_uuid":"6420567d7c9551d807c5216e2b9b04a12103b506"},"cell_type":"markdown","source":"In this kernel I will use 1D Convolutional Neural Network, perform in sentences which will be represented as a sequence of word vectors. The [word2vec](https://www.tensorflow.org/tutorials/word2vec)  method was utilized to encode words to vectors. One way to run `word2vec` inside python code is the [python wrapper of the original C++ source](https://github.com/danielfrg/word2vec). It can be installed by using the command `pip install word2vec`.   \nFor the overview, the dataset has been read. We also discover more information of the dataset, such as the ratio of classes labels."},{"metadata":{"trusted":true,"_uuid":"25f82f3c8cebc8b4d49f3d6dac6e3603957db282","collapsed":true},"cell_type":"code","source":"import pandas\nimport numpy\nimport matplotlib.pyplot as plt\nfrom matplotlib.gridspec import GridSpec\n\ndata = pandas.read_table(\"../input/movie-review-sentiment-analysis-kernels-only/train.tsv\")\n# display(data)\nlabels = [\"0\", \"1\", \"2\", \"3\", \"4\"]\n\nnumberPhrase = data.groupby(\"Sentiment\").count().PhraseId\nplt.pie(list(numberPhrase), labels=labels, autopct='%.2f%%', shadow=True)\n\nplt.title('Ratio diagram of Phrase')\nplt.show()\n\nnumberSentence = data.groupby(\"Sentiment\").SentenceId.nunique()\nplt.pie(list(numberSentence), labels=labels, autopct='%.2f%%', shadow=True)\nplt.title('Ratio diagram of Sentence')\nplt.show()\n\nnumberLengthText = [[0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0]]\nfor i in range(len(data.PhraseId)):\n    phrase = data.Phrase[i]\n    words = phrase.split(\" \")\n    numberLengthText[min(len(words)-1, 2)][int(data.Sentiment[i])] += 1\n\nnumberLengthText = numpy.array(numberLengthText, dtype=numpy.float32)\nfor i in range(3):\n    numberLengthText[i, :] /= numpy.sum(numberLengthText[i, :])\nx = [\"1\", \"2\", \">=3\"]\nx_ = numpy.arange(len(x))\nplt.bar(x_-0.2, numberLengthText[:, 0], 0.1, label=labels[0])\nplt.bar(x_-0.1, numberLengthText[:, 1], 0.1, label=labels[1])\nplt.bar(x_, numberLengthText[:, 2], 0.1, label=labels[2])\nplt.bar(x_+0.1, numberLengthText[:, 3], 0.1, label=labels[3])\nplt.bar(x_+0.2, numberLengthText[:, 4], 0.1, label=labels[4])\nplt.xticks(x_, x)\nplt.legend()\nplt.xlabel('Number of words in phrase')\nplt.ylabel('Percentage')\nplt.title('Ratio diagram of Phrase by word numbers')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"91afb69177c2c222651a477b386c479f272a507c"},"cell_type":"markdown","source":"As we can see in the diagram of Phrase, over 50% is labeled as `2`. That is an imbalanced dataset, and we can submit a result file which has all label `2` to get around 50% accuracy.  \nBy counting the ratio of sentiment classes along with number of words in a phrase, we can see that most of the phrases which have single (or couple) words are neutral (label as `2`). Looking in the dataset, there are many phrases contain only a dot or a comma. Perhaps I should process them separately but let me try the following way first.  \nThe pretrained model of `word2vec` encoding method was obtained by the original C++ source code. I just use `text8` dataset to train the the word embedding model, follow completely with the guideline of the original author. After training, the weights of that model was stored as `vectors.bin`."},{"metadata":{"trusted":true,"_uuid":"b8e9a87c037c73943c929c6b3ec1216f7130ddee","collapsed":true},"cell_type":"code","source":"import os\nimport word2vec\nimport numpy\n\nclass Preprocessor:\n    @staticmethod\n    def lemmatize(word):\n        if word[-4:] == \"sess\":\n            return word[:-4]+\"ss\"\n        if word[-3:] == \"ies\":\n            return word[:-3]+\"y\"\n        if word[-2:] == \"ss\":\n            return word\n        if word[-1:] == \"s\":\n            return word[:-1]\n        if word == \"'s\":\n            return \"be\"\n\n    @staticmethod\n    def tokenize(sentence):\n        return sentence.lower().split(\" \")\n\nclass W2VProcessor:\n    def __init__(self, originData=None, dataFolder=\"\", vectorSize=100):\n        self.__model = None\n        self.__vectorSize = vectorSize\n        if type(originData) is str:\n            word2vec.word2vec(\n                originData, \n                os.path.join(dataFolder, \"vec.w2v\"), \n                size=vectorSize, \n                verbose=True)\n            self.__model = word2vec.load(os.path.join(dataFolder, \"vec.w2v\"))\n\n    def load(self, wordVectorFile):\n        self.__model = word2vec.load(wordVectorFile)\n        self.__vectorSize = self.__model.vectors.shape[1]\n\n    def getVectorSize(self):\n        return self.__vectorSize\n\n    def process(self, sentence, length=None):\n        if self.__model is None:\n            print(\"Error: The model is None\")\n            return None\n\n        if not sentence:\n            print(\"Error: The sentence is None\")\n            return None\n        \n        sentence = Preprocessor.tokenize(sentence)\n\n        if length is None:\n            length = len(sentence)\n        sentence = sentence[:length]\n        tensor = []\n        for word in sentence:\n            try:\n                tensor.append(self.__model[word])\n            except:\n                try:\n                    tensor.append(self.__model[Preprocessor.lemmatize(word)])\n                except:    \n                    tensor.append(numpy.zeros((self.__vectorSize,)))\n        for i in range(length-len(sentence)):\n            tensor.append(numpy.zeros(tensor[0].shape))\n        tensor = numpy.concatenate(tensor).reshape((length, len(tensor[0])))\n\n        return tensor\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2e91a21ff1b06aee7b256bed1c397bc0cc025df9"},"cell_type":"markdown","source":"According to the class `W2VProcessor`, all phrases (which I called `sentence` in the source code) will be normalized as a fixed size (25 as default). It means that I will pad zeros vectors to the phrases sorter than 25 words, otherwise I cut it out.  \nNow I try to define a CNN model. This architecture was inspired by the paper [Yoon Kim, \"Convolutional Neural Networks for Sentence Classification\", 2014](https://arxiv.org/abs/1408.5882). The CNN model takes multiple kernel sizes with multiple convolutional layers parallely, performs global max pooling step over time, then a fully connected layer at final."},{"metadata":{"trusted":true,"_uuid":"74e20c48ad519098500245bfddd49cab23233149","collapsed":true},"cell_type":"code","source":"import numpy\nimport tensorflow\n\nclass CNN:\n    def __init__(self, wordVectSize=200, numOfWords=25, learningRate=0.1, numFilters=16):\n        ALPHA = 2\n        self.__numberOfWords = numOfWords\n        self.__sizeOfWordVectors = wordVectSize\n        self.__graph = tensorflow.Graph()\n        self.__globalStep = tensorflow.Variable(0, trainable=False)\n        self.__learningRate = tensorflow.train.exponential_decay(\n            learningRate, self.__globalStep, decay_steps=10, decay_rate=0.98, staircase=True)\n        \n        self.__input = tensorflow.placeholder(tensorflow.float32, shape=[None, numOfWords, wordVectSize])\n        self.__label = tensorflow.placeholder(tensorflow.float32, shape=[None])\n        self.__dropout = tensorflow.placeholder(tensorflow.float32, shape=[])\n        \n        conv1 = tensorflow.layers.conv1d(\n            self.__input, filters=numFilters, kernel_size=1, padding=\"valid\", activation=tensorflow.nn.relu)\n        conv3 = tensorflow.layers.conv1d(\n            self.__input, filters=numFilters, kernel_size=3, padding=\"valid\", activation=tensorflow.nn.relu)\n        conv5 = tensorflow.layers.conv1d(\n            self.__input, filters=numFilters, kernel_size=5, padding=\"valid\", activation=tensorflow.nn.relu)\n\n        pool1 = tensorflow.layers.max_pooling1d(conv1, pool_size=(numOfWords, ), strides=1)\n        pool3 = tensorflow.layers.max_pooling1d(conv3, pool_size=(numOfWords-2, ), strides=1)\n        pool5 = tensorflow.layers.max_pooling1d(conv5, pool_size=(numOfWords-4, ), strides=1)\n        \n        conca = tensorflow.concat([pool1, pool3, pool5], axis=2)\n        dropo = tensorflow.nn.dropout(conca, self.__dropout)\n        self.__logits = tensorflow.layers.dense(dropo, units=5, activation=tensorflow.nn.softmax)\n        self.__classes = tensorflow.reshape(tensorflow.argmax(self.__logits, axis=2), [-1])\n        \n        onehotLabels = tensorflow.one_hot(\n            indices=tensorflow.cast(self.__label, tensorflow.int32), depth=5)\n        onehotLabels = tensorflow.reshape(onehotLabels, [-1, 1, 5])\n        weights = tensorflow.reshape(ALPHA*(tensorflow.abs(self.__label-2)+1), [-1, 1])\n        # [0, 1, 2, 3, 4] -> [3, 2, 1, 2, 3]*ALPHA\n        self.__loss = tensorflow.losses.softmax_cross_entropy(\n            onehot_labels=onehotLabels, logits=self.__logits, weights=weights)\n\n        self.__trainOp = tensorflow.contrib.layers.optimize_loss(\n            loss=self.__loss,\n            global_step=self.__globalStep,\n            learning_rate=self.__learningRate,\n            optimizer=\"SGD\"\n        )\n\n        self.__session = tensorflow.Session()\n        self.__session.run(tensorflow.global_variables_initializer())\n        self.__saver = tensorflow.train.Saver()\n        \n    def fit_on_batch(self, trainData, label):\n        feedDict = {\n            self.__input: trainData,\n            self.__label: label,\n            self.__dropout: 0.6\n        }\n\n        _, loss = self.__session.run([self.__trainOp, self.__loss], feed_dict=feedDict)\n        return loss\n\n    def predict(self, testData):\n        return self.__session.run([self.__classes], feed_dict={self.__input: testData, self.__dropout: 1.0})[0]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b057704f84c7aa72164e7c79d22ee4cbb38bdb0b"},"cell_type":"markdown","source":"On training script, we can adjust the `trainRatio` to split the training set into training/validating sets. Currently I takes the whole training data to create the CNN model, due to the fact that I has run on 70% dataset before and get the accuracy ~0.493 on the other 30%. The result before 5 epoches is almost `neutral` (label `2`) and there is no concern to get **0.51789** on test set, means the model is underfitting.   \nMy next action is to try dropping all duplicated phrases of sentences, and only train the model with full sentences. In cross-validating step I still keep all phrases. Let see if the model which was fitted by full sentences can predict a part of a sentence.  Furthermore, I took a look at the length of all phrases, then I decided to choose 55, because the longest phrase of our data is 52 words.  The second submission got **0.53531**.  \nKeeping default sentences size 55, I then modify the loss function. I add some weights to the classes in order to balance the dataset. Now retry to train with the whole data (including both full sentences and phrases).\n"},{"metadata":{"trusted":true,"_uuid":"81bbba516269687eaf946e0bf7e0b9fee64c723c","collapsed":true},"cell_type":"code","source":"import random\nimport pandas\nimport numpy\n\nsentenceSize = 55\nprocessor = W2VProcessor()\nprocessor.load(\"../input/word2vec-model/vectors.bin\")\nclf = CNN(numOfWords=sentenceSize)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fd5c09dcd34be3150536e5d256138ccad0a1bd5c","collapsed":true},"cell_type":"code","source":"epochNum = 15\nbatchSize = 100\ntrainRatio = 1.0\n\ntrainingData = data[:int(trainRatio*len(data))]\n# trainingData = trainingData.drop_duplicates(subset=[\"SentenceId\"])\ntrainingLabel = trainingData.Sentiment\ntrainingData = trainingData.Phrase\n\ntestingData = data[int(trainRatio*len(data.Phrase)):]\ntestingLabel = testingData.Sentiment\ntestingData = testingData.Phrase\n\nfor epoch in range(epochNum):\n    indice = 0\n    totalLoss = 0\n    for i in range(0, len(trainingData), batchSize):\n        processedData = [processor.process(phrase, sentenceSize) for phrase in trainingData[i:i+batchSize]]\n        X = numpy.array(processedData)\n        Y = numpy.array(pandas.to_numeric(trainingLabel[i:i+batchSize]))\n        totalLoss += clf.fit_on_batch(X, Y)\n        indice += 1\n    print(\"epoch: \", epoch, \" loss: \", totalLoss/indice)\n\n    if len(testingData) <= 0:\n        continue\n\n    totalTrue = 0\n    for j in range(0, len(testingData), batchSize):\n        processedData = [processor.process(phrase, sentenceSize) for phrase in testingData[j:j+batchSize]]\n        x = numpy.array(processedData)\n        y = numpy.array(pandas.to_numeric(testingLabel[j:j+batchSize]))\n        z = clf.predict(x)\n        totalTrue += numpy.count_nonzero(y-z==0)\n\n    accuracy = 1.0*totalTrue/len(testingData)\n    print(\"cross-validation accuracy: \", accuracy)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"157858d6402ac16785184fa0efa9e236059339ce"},"cell_type":"markdown","source":"Now write some code lines to get the output file from `test.tsv`"},{"metadata":{"trusted":true,"_uuid":"8a6278d6a64a726f117fb7e701f54728aa1d5777","_kg_hide-output":false,"_kg_hide-input":false,"collapsed":true},"cell_type":"code","source":"blindData = pandas.read_table(\"../input/movie-review-sentiment-analysis-kernels-only/test.tsv\")\nwith open(\"submission.csv\", \"w\") as outFile:\n    outFile.write(\"PhraseId,Sentiment\\n\")\n    for i in range(0, len(blindData.PhraseId), batchSize):\n        processedBlindData = [processor.process(phrase, sentenceSize) for phrase in blindData.Phrase[i:i+batchSize]]\n        z = clf.predict(numpy.array(processedBlindData))\n        for j in range(len(z)):\n            outFile.write(str(blindData.PhraseId[i+j])+\",\"+str(int(z[j]))+\"\\n\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}