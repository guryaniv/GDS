{"cells":[{"metadata":{"_uuid":"1c48edb1eb51b5144f1439e3fa26ce4652809cf5"},"cell_type":"markdown","source":"# Baselines fully using a Scikit-Learn pipeline: Bow, Tf-idf, Logreg, SVM, NB, etc."},{"metadata":{"_uuid":"5c8d2bd5f4ef792f998522359934ebd49a478e9f"},"cell_type":"markdown","source":"The purpose of this notebook is to present a fully scikit-learn based pipeline for processing text data, training simple models, and submit the best results.\n\nWe try the following vectorizers:\n* Bag-of-Words (BoW)\n* Bag-of-Words + frequency–inverse document frequency (tfidf)\n\nWe try the following models:\n* Logistic Regression (Logreg)\n* Support Vector Machines (SVM)\n* Naive Bayes (NB)"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"print(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f3fc03823a07e97054e31c3920a2a818e70f7ba1"},"cell_type":"markdown","source":"## Preprocessing"},{"metadata":{"trusted":true,"_uuid":"69923c2cff8b62d721a3aa392d358cba8652a245"},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('../input/train.csv')\ntest_df = pd.read_csv('../input/test.csv')\ny_train = train_df['target'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7941c8aa6936de2f3a1bca2629714b7ddf82c7b1","scrolled":false},"cell_type":"code","source":"print(\"Train data:\", train_df.shape)\nprint(\"Test data:\", test_df.shape)\ntrain_df.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1c08601b9533af1ef8bda774761f4fc0f7776bfa"},"cell_type":"code","source":"train_df['target'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5fe89b3a88e203620454e3f53524c882bf331482"},"cell_type":"markdown","source":"We notice a very imbalanced dataset. This is a problem that can be addressed if you realize your submission is not getting the desired score. There is no way in this case to know if it's worth trying to balance the dataset, since the test set could also be imbalanced"},{"metadata":{"_uuid":"b569bc2ca339dbfd05c07d64308315970dc87309"},"cell_type":"markdown","source":"### Bag of Words (BOW)"},{"metadata":{"_uuid":"43c30fd890e98145fbb6d1fc2b308f39e4c3b0aa"},"cell_type":"markdown","source":"Here is the description of BoW on the [feature extraction user guide](https://scikit-learn.org/stable/modules/feature_extraction.html) in the scikit-learn documentation:\n> Text Analysis is a major application field for machine learning algorithms. However the raw data, a sequence of symbols cannot be fed directly to the algorithms themselves as most of them expect numerical feature vectors with a fixed size rather than the raw text documents with variable length.\n>\n> ...\n>\n> A corpus of documents can thus be represented by a matrix with one row per document and one column per token (e.g. word) occurring in the corpus.\n>\n> We call vectorization the general process of turning a collection of text documents into numerical feature vectors. This specific strategy (tokenization, counting and normalization) is called the Bag of Words or “Bag of n-grams” representation. Documents are described by word occurrences while completely ignoring the relative position information of the words in the document.\n\nWe will limit ourselves to the top 50,000 most frequent words for faster training. Try increasing dimensionality if you feel this is too small."},{"metadata":{"trusted":true,"_uuid":"21d70de38303430e7e6033961c2dc8dbf7bb796d"},"cell_type":"code","source":"count_vec = CountVectorizer(max_features=50000)\ntrain_bow = count_vec.fit_transform(train_df['question_text'])\ntest_bow = count_vec.transform(test_df['question_text'])\n\nprint(train_bow.shape)\nprint(test_bow.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"90fcd68c1276c1c61c13ec9fd5a814d86326b046"},"cell_type":"markdown","source":"### Tf-idf"},{"metadata":{"_uuid":"36e2fc4934eb805885a532a67ef7da16b81b3884"},"cell_type":"markdown","source":"[Tf-idf documentation](https://scikit-learn.org/stable/modules/feature_extraction.html#tfidf-term-weighting):\n> In a large text corpus, some words will be very present (e.g. “the”, “a”, “is” in English) hence carrying very little meaningful information about the actual contents of the document. If we were to feed the direct count data directly to a classifier those very frequent terms would shadow the frequencies of rarer yet more interesting terms.\n>\n> In order to re-weight the count features into floating point values suitable for usage by a classifier it is very common to use the tf–idf transform.\n>\n> Tf means term-frequency while tf–idf means term-frequency times inverse document-frequency: \n>\n> $\\text{tf-idf}(t,d) = \\text{tf}(t,d) \\times \\text{idf}(t)$\n>\n> Using the `TfidfTransformer`’s default settings, `TfidfTransformer(norm='l2', use_idf=True, smooth_idf=True, sublinear_tf=False)` the term frequency, the number of times a term occurs in a given document, is multiplied with idf component, which is computed as\n>\n> $idf(t) = log \\frac{1+n_d}{1+df(d,t)+1}$"},{"metadata":{"trusted":true,"_uuid":"19809b87ffbfb4ea4b0028dcf5864645dc096962"},"cell_type":"code","source":"tfidf = TfidfTransformer()\ntrain_tfidf = tfidf.fit_transform(train_bow)\ntest_tfidf = tfidf.transform(test_bow)\n\nprint(train_tfidf.shape)\nprint(test_tfidf.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"adbf0ecbfc629a3fbd9666c30f5c7355869cc132"},"cell_type":"markdown","source":"## Model Training and Evaluation"},{"metadata":{"trusted":true,"_uuid":"621e822bc173fe67fd7780b95fd97baa6b54bc59"},"cell_type":"code","source":"# Models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import LinearSVC\nfrom sklearn import naive_bayes\nfrom sklearn.metrics import f1_score, make_scorer\nfrom sklearn.model_selection import cross_val_score, GridSearchCV","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b30310839033f3e2b68985a817efe8e20efa6d52"},"cell_type":"code","source":"# Create a scorer object we will use to evaluate the methods.\nf1_scorer = make_scorer(f1_score)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"25ebe65b5510df1651f05464e65d7cb60524dab9"},"cell_type":"markdown","source":"### Logistic Regression on BoW"},{"metadata":{"_kg_hide-output":false,"trusted":true,"_uuid":"73087752a1bd1f64a11e269d39a5ea22dbbfe9a8"},"cell_type":"code","source":"logreg_bow = LogisticRegression(solver='lbfgs')\nlogreg_bow_score = cross_val_score(\n    estimator=logreg_bow,\n    X=train_bow, \n    y=y_train,\n    verbose=2,\n    scoring=f1_scorer,\n    cv=4, # Since kaggle CPUs have 4 cores\n    n_jobs=-1\n)\nprint(logreg_bow_score)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f227ea82245e2d953301c57e44ec83c849087cc8"},"cell_type":"markdown","source":"### Logistic Regression on Tfidf"},{"metadata":{"_kg_hide-output":false,"trusted":true,"_uuid":"459949ed18ec1ca7473226e1a99b40a9c5fb231b"},"cell_type":"code","source":"logreg_tfidf = LogisticRegression(solver='saga')\nlogreg_tfidf_score = cross_val_score(\n    estimator=logreg_tfidf,\n    X=train_tfidf, \n    y=y_train,\n    scoring=f1_scorer,\n    verbose=2,\n    cv=4, # Since kaggle CPUs have 4 cores\n    n_jobs=-1\n)\nprint(logreg_tfidf_score)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1f26339f6cadf9b2a6b6612dd805e2b20b4a3ed8"},"cell_type":"markdown","source":"### SVM on BoW"},{"metadata":{"_kg_hide-output":false,"trusted":true,"_uuid":"a95d3e451aa059a1f71f32458a231b3630542880"},"cell_type":"code","source":"# Use Dual=False since n_samples > n_features\nsvm_bow = LinearSVC(dual=False)\nsvm_bow_score = cross_val_score(\n    estimator=svm_bow,\n    X=train_bow, \n    y=y_train,\n    verbose=2,\n    scoring=f1_scorer,\n    cv=4, # Since kaggle CPUs have 4 cores\n    n_jobs=-1\n)\nprint(svm_bow_score)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"365c04b1981f7f11a186f87ab3ace31c3d247a77"},"cell_type":"markdown","source":"### SVM on Tfidf"},{"metadata":{"_kg_hide-output":false,"trusted":true,"_uuid":"cb91a9c3f8552aa8b0c60cd09255a3adef3b4c07"},"cell_type":"code","source":"svm_tfidf = LinearSVC(dual=False)\nsvm_tfidf_score = cross_val_score(\n    estimator=svm_tfidf,\n    X=train_tfidf, \n    y=y_train,\n    scoring=f1_scorer,\n    verbose=2,\n    cv=4, # Since kaggle CPUs have 4 cores\n    n_jobs=-1\n)\nprint(svm_tfidf_score)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0fd12be1383e99f2891b6c5000a55659575d4e36"},"cell_type":"markdown","source":"### Multinomial Naive Bayes on BoW"},{"metadata":{"_uuid":"603162bfbcfe9a4b61ce86388b38eb4cbeb532b4"},"cell_type":"markdown","source":"[Documentation](https://scikit-learn.org/stable/modules/naive_bayes.html#multinomial-naive-bayes):\n> MultinomialNB implements the naive Bayes algorithm for multinomially distributed data, and is one of the two classic naive Bayes variants used in text classification (where the data are typically represented as word vector counts, although tf-idf vectors are also known to work well in practice)."},{"metadata":{"_kg_hide-output":false,"trusted":true,"_uuid":"7cf3731904258c34e9d80eae2ddd2f653c9a3848"},"cell_type":"code","source":"# Use Dual=False since n_samples > n_features\nmnb_bow = naive_bayes.MultinomialNB()\nmnb_bow_score = cross_val_score(\n    estimator=mnb_bow,\n    X=train_bow, \n    y=y_train,\n    verbose=2,\n    scoring=f1_scorer,\n    cv=4, # Since kaggle CPUs have 4 cores\n    n_jobs=-1\n)\nprint(mnb_bow_score)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a2ef1871cfc6493a22801a8772d7b39c82bbef01"},"cell_type":"markdown","source":"### Complement Naive Bayes on Tfidf"},{"metadata":{"_kg_hide-output":false,"trusted":true,"_uuid":"bc95f00e6a76e5efd95bddd0ec8ed5d40352fb99"},"cell_type":"code","source":"mnb_tfidf = naive_bayes.MultinomialNB()\nmnb_tfidf_score = cross_val_score(\n    estimator=mnb_tfidf,\n    X=train_tfidf, \n    y=y_train,\n    scoring=f1_scorer,\n    verbose=2,\n    cv=4, # Since kaggle CPUs have 4 cores\n    n_jobs=-1\n)\nprint(mnb_tfidf_score)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4ef78a77533a2334c0556ae856d77a30cfbbd238"},"cell_type":"markdown","source":"### Complement Naive Bayes on BoW"},{"metadata":{"_kg_hide-output":false,"trusted":true,"_uuid":"5b32010a4f6b8b29c28480ddf5c3daf3d546ad52"},"cell_type":"code","source":"cnb_bow = naive_bayes.ComplementNB()\ncnb_bow_score = cross_val_score(\n    estimator=cnb_bow,\n    X=train_bow, \n    y=y_train,\n    verbose=2,\n    scoring=f1_scorer,\n    cv=4, # Since kaggle CPUs have 4 cores\n    n_jobs=-1\n)\nprint(cnb_bow_score)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"eb570237ffbd7172c41bb555ad3d7afc56aaecae"},"cell_type":"markdown","source":"### Complement Naive Bayes on Tfidf"},{"metadata":{"_kg_hide-output":false,"trusted":true,"_uuid":"8edaa02a05fcc8c03a8f26ff6feaeeefebafe81a"},"cell_type":"code","source":"cnb_tfidf = naive_bayes.ComplementNB()\ncnb_tfidf_score = cross_val_score(\n    estimator=cnb_tfidf,\n    X=train_tfidf, \n    y=y_train,\n    scoring=f1_scorer,\n    verbose=2,\n    cv=4, # Since kaggle CPUs have 4 cores\n    n_jobs=-1\n)\nprint(cnb_tfidf_score)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7f9dc2653e66c7e8f44f54f147b3c565bb60212f"},"cell_type":"markdown","source":"## Evaluating the models"},{"metadata":{"trusted":true,"_uuid":"0db94b79545506a8f44c4c06a72109700d0f47ec"},"cell_type":"code","source":"model_scores = dict(\n    logreg_bow=logreg_bow_score,\n    svm_bow=svm_bow_score,\n    mnb_bow=mnb_bow_score,\n    cnb_bow=cnb_bow_score,\n    logreg_tfidf=logreg_tfidf_score,\n    svm_tfidf=svm_tfidf_score,\n    mnb_tfidf=mnb_tfidf_score,\n    cnb_tfidf=cnb_tfidf_score,\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"28210e32d12ffda3c6ef705f5d41dd0ccd40e6c1"},"cell_type":"code","source":"model_scores_df = pd.DataFrame(model_scores)\nmodel_scores_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c7dcd2993ad64dd2ebdf3520af063fe3028a8815"},"cell_type":"code","source":"best_model_name = model_scores_df.mean().idxmax()\nprint(\"Best Model:\", best_model_name)\nmodel_scores_df.mean().plot(kind='bar', rot=45)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true,"_uuid":"6ceb8311fa7b781b41d82a43757f10792ef26ae5"},"cell_type":"code","source":"model = eval(best_model_name)\n\nif 'bow' in best_model_name:\n    X_train = train_bow\n    X_test = test_bow\nelse:\n    X_train = train_tfidf\n    X_test = test_tfidf\n\nmodel.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"aecb239a6341e8adf2ef693371af3cb07e4ac173"},"cell_type":"markdown","source":"## Submission"},{"metadata":{"trusted":true,"_uuid":"c7d54cd88afc6057626a273bac758f9b1c4e4bed"},"cell_type":"code","source":"submission_df = pd.read_csv('../input/sample_submission.csv')\nsubmission_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7b192fa48a83bce1516d08e18a6809e18656233e"},"cell_type":"code","source":"submission_df['prediction'] = model.predict(X_test)\nsubmission_df.to_csv('submission.csv', index=None)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}