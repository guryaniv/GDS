{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "2e162541-b438-1524-7925-b27b5d57b1fb"
      },
      "source": [
        "problems i have is\n",
        "\n",
        "-tuning ? Am i overfitting ? how do i know ?\n",
        "-how do i get the prediction ? see the error below ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "af1d2510-716c-7e91-5f81-1c79b72dda86"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "# timing function\n",
        "import time   \n",
        "start = time.clock() #_________________ measure efficiency timing\n",
        "\n",
        "input_folder='../input/'\n",
        "train = pd.read_csv(input_folder + 'train.csv',encoding='utf8')[:10000]\n",
        "test  = pd.read_csv(input_folder + 'test.csv',encoding='utf8')[:10000]\n",
        "\n",
        "# lege opvullen\n",
        "train.fillna(value='leeg',inplace=True)\n",
        "test.fillna(value='leeg',inplace=True)\n",
        "\n",
        "print(\"Original data: trainQ: {}, testQ: {}\".format(train.shape, test.shape) )\n",
        "end = time.clock()\n",
        "print('open:',end-start)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "3c5d2c42-5a87-99cd-c3b6-665abed8a30e"
      },
      "outputs": [],
      "source": [
        "def cleantxt(x):   \n",
        "    # Pad punctuation with spaces on both sides\n",
        "    for char in ['.', '\"', ',', '(', ')', '!', '?', ';', ':']:\n",
        "        x = x.replace(char, ' ' + char + ' ')\n",
        "    return x\n",
        "\n",
        "train['question1']=train['question1'].map(cleantxt)\n",
        "train['question2']=train['question2'].map(cleantxt)\n",
        "test['question1']=test['question1'].map(cleantxt)\n",
        "test['question2']=test['question2'].map(cleantxt)\n",
        "\n",
        "train_qs = pd.Series(train['question1'].tolist() + train['question2'].tolist())\n",
        "test_qs = pd.Series(test['question1'].tolist() + test['question2'].tolist())\n",
        "\n",
        "count_vectorizer = CountVectorizer(binary=True, ngram_range=(1, 2) )\n",
        "count_vectorizer.fit(train_qs.append(test_qs))  #Learn vocabulary and idf, return document freq list.\n",
        "print('lengt dictionary',len(count_vectorizer.vocabulary_))\n",
        "\n",
        "end = time.clock()\n",
        "print('clean and make freq word dict:',end-start)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "ad5ab207-75bd-db41-1d90-c0d543da2aac"
      },
      "source": [
        "here i look for equality, differences with tfidf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "727e3a1d-49cb-8e26-b374-7d90aa77a695"
      },
      "outputs": [],
      "source": [
        "def splitter(dfQ,Dict):\n",
        "    eq=[]\n",
        "    di1=[]\n",
        "    di2=[]\n",
        "    for xi in range(0,len(dfQ)):\n",
        "        q1words = dfQ.iloc[xi].question1.split()\n",
        "        q2words = dfQ.iloc[xi].question2.split()\n",
        "        equq1 = [w for w in q1words if w in q2words]\n",
        "        difq1 = [w for w in q1words if w not in q2words] \n",
        "        difq2 = [w for w in q2words if w not in q1words ]\n",
        "        eq.append(' '.join(equq1))\n",
        "        di1.append(' '.join(difq1))\n",
        "        di2.append(' '.join(difq2))\n",
        "    count1_vectorizer = CountVectorizer(vocabulary=Dict, ngram_range=(1, 2),binary=True, min_df=1)\n",
        "    count1_vectorizer.fit_transform(dfQ['question1'])  #Learn vocabulary and idf, return term-document matrix.\n",
        "    freq1_term_matrix = count_vectorizer.transform(dfQ['question1'])\n",
        "    count2_vectorizer = CountVectorizer(vocabulary=Dict, ngram_range=(1, 2),binary=True, min_df=1)\n",
        "    count2_vectorizer.fit_transform(dfQ['question2'])\n",
        "    freq2_term_matrix = count_vectorizer.transform(dfQ['question2']) #Transform documents to document-term matrix. Uses the vocabulary and document frequencies (df) learned by fit (or fit_transform) This is equivalent to fit followed by transform\n",
        "    count3_vectorizer = CountVectorizer(vocabulary=Dict, ngram_range=(1, 2),binary=True, min_df=1)\n",
        "    count3_vectorizer.fit_transform(di1)\n",
        "    freq3_term_matrix = count_vectorizer.transform(di1) #Transform documents to document-term matrix. Uses the vocabulary and document frequencies (df) learned by fit (or fit_transform) This is equivalent to fit followed by transform\n",
        "    count4_vectorizer = CountVectorizer(vocabulary=Dict, ngram_range=(1, 2),binary=True, min_df=1)\n",
        "    count4_vectorizer.fit_transform(di2)\n",
        "    freq4_term_matrix = count_vectorizer.transform(di2) #Transform documents to document-term matrix. Uses the vocabulary and document frequencies (df) learned by fit (or fit_transform) This is equivalent to fit followed by transform\n",
        "    count5_vectorizer = CountVectorizer(vocabulary=Dict, ngram_range=(1, 2),binary=True, min_df=1)\n",
        "    count5_vectorizer.fit_transform(eq)\n",
        "    freq5_term_matrix = count_vectorizer.transform(eq) #Transform documents to document-term matrix. Uses the vocabulary and document frequencies (df) learned by fit (or fit_transform) This is equivalent to fit followed by transform\n",
        "    tfidf1 = TfidfTransformer(norm=\"l2\")\n",
        "    tf1_idf_matrix = tfidf1.fit_transform(freq1_term_matrix)\n",
        "    tfidf2 = TfidfTransformer(norm=\"l2\")\n",
        "    tf2_idf_matrix = tfidf2.fit_transform(freq2_term_matrix)\n",
        "    tfidf3 = TfidfTransformer(norm=\"l2\")\n",
        "    tf3_idf_matrix = tfidf3.fit_transform(freq3_term_matrix)\n",
        "    tfidf4 = TfidfTransformer(norm=\"l2\")\n",
        "    tf4_idf_matrix = tfidf4.fit_transform(freq4_term_matrix)\n",
        "    tfidf5 = TfidfTransformer(norm=\"l2\")\n",
        "    tf5_idf_matrix = tfidf5.fit_transform(freq5_term_matrix)\n",
        "    corr1=tf1_idf_matrix[:].dot(tf2_idf_matrix[:].T).diagonal().round(2)\n",
        "    corr2=tf1_idf_matrix[:].dot(tf5_idf_matrix[:].T).diagonal().round(2)\n",
        "    corr3=tf2_idf_matrix[:].dot(tf5_idf_matrix[:].T).diagonal().round(2)\n",
        "    corr4=tf1_idf_matrix[:].dot(tf3_idf_matrix[:].T).diagonal().round(2)\n",
        "    corr5=tf2_idf_matrix[:].dot(tf4_idf_matrix[:].T).diagonal().round(2)\n",
        "    tf23e=corr2>corr3\n",
        "    tf2345=(corr2+corr3)>(corr4+corr5)\n",
        "    tf24=corr2>corr4\n",
        "    tf35=corr3>corr5\n",
        "    tf145=corr1>(corr4/2+corr5/2)\n",
        "    \n",
        "    return corr1,corr2,corr3,corr4,corr5,tf23e,tf2345,tf24,tf35,tf145\n",
        "\n",
        "\n",
        "train['corr1'],train['corr2'],train['corr3'],train['corr4'],train['corr5'],train['tf23e'],train['tf2345'],train['tf24'],train['tf35'],train['tf145']=splitter(train,count_vectorizer.vocabulary_)\n",
        "    \n",
        "print(train.head(10))\n",
        "\n",
        "test['corr1'],test['corr2'],test['corr3'],test['corr4'],test['corr5'],test['tf23e'],test['tf2345'],test['tf24'],test['tf35'],test['tf145']=splitter(test,count_vectorizer.vocabulary_)\n",
        "\n",
        "print(test.head(10))\n",
        "\n",
        "end = time.clock()\n",
        "print('tfidf - corr:',end-start)         \n",
        "\n",
        "    \n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "7125b66f-c218-7ad6-6a4a-8bc8856181c1"
      },
      "source": [
        "here i have the problem with xgboost optimalisation \n",
        "How do i get that prediction working ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "80b83d7a-7a33-e658-7cff-6140eb7089ba"
      },
      "outputs": [],
      "source": [
        "import xgboost as xgb\n",
        "from xgboost.sklearn import XGBClassifier\n",
        "from sklearn import cross_validation, metrics\n",
        "import matplotlib.pylab as plt\n",
        "%matplotlib inline\n",
        "from matplotlib.pylab import rcParams\n",
        "rcParams['figure.figsize'] = 12, 4\n",
        "\n",
        "def modelfit(alg, dtrain, predictors,predlabel,useTrainCV=True, cv_folds=5, early_stopping_rounds=50):\n",
        "    \n",
        "    if useTrainCV:\n",
        "        xgb_param = alg.get_xgb_params()\n",
        "        xgtrain = xgb.DMatrix(dtrain[predictors].values, label=dtrain[predlabel].values)\n",
        "        cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds,\n",
        "            metrics='auc', early_stopping_rounds=early_stopping_rounds)\n",
        "        alg.set_params(n_estimators=cvresult.shape[0])\n",
        "    \n",
        "    #Fit the algorithm on the data\n",
        "    alg.fit(dtrain[predictors], dtrain[predlabel],eval_metric='auc')\n",
        "        \n",
        "    #Predict training set:\n",
        "    dtrain_predictions = alg.predict(dtrain[predictors])\n",
        "    dtrain_predprob = alg.predict_proba(dtrain[predictors])[:,1]\n",
        "        \n",
        "    #Print model report:\n",
        "    print(\"\\nModel Report\" )\n",
        "    print(\"Accuracy : %.4g\" % metrics.accuracy_score(dtrain[predlabel].values, dtrain_predictions) )\n",
        "    print(\"AUC Score (Train): %f\" % metrics.roc_auc_score(dtrain[predlabel], dtrain_predprob) )\n",
        "                    \n",
        "    feat_imp = pd.Series(alg.booster().get_fscore()).sort_values(ascending=False)\n",
        "    feat_imp.plot(kind='bar', title='Feature Importances')\n",
        "    plt.ylabel('Feature Importance Score')\n",
        "    return alg\n",
        "\n",
        "predictors = [x for x in train.columns if x not in ['id','question1','question2','is_duplicate','qid1','qid2']]\n",
        "\n",
        "for di in range (5,20,3):\n",
        "    # Set our parameters for xgboost\n",
        "    for ci in range (1,2,2):\n",
        "    # Set our parameters for xgboost\n",
        "        print('maxdepth',di,'minchild',ci)\n",
        "        xgb1 = XGBClassifier(\n",
        " learning_rate =0.1, n_estimators=1000, max_depth=di, min_child_weight=ci,\n",
        " gamma=0, subsample=0.8, colsample_bytree=0.8, objective= 'binary:logistic',\n",
        " nthread=4, scale_pos_weight=1, seed=27)\n",
        "        xgbmodel=modelfit(xgb1,train,predictors,'is_duplicate')\n",
        "\n",
        "testcolumn = [x for x in test.columns if x not in ['id','question1','question2']]   \n",
        "#print(test[testcolumn])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "a5ac5fe0-7b7c-bbc4-9078-d7d805ba7d6c"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b4df90c6-527c-ed18-a975-c4d3acdb4d30"
      },
      "outputs": [],
      "source": [
        "corrcolumns = [x for x in train.columns if x not in ['question1','question2']]\n",
        "\n",
        "corr_mat=train[corrcolumns].corr()\n",
        "corr_mat.head(15)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}