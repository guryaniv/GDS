{"cells":[{"metadata":{"_uuid":"3a8b384b0dcc5daf801ad3861cc001dbe606278f"},"cell_type":"markdown","source":"# Random Forest Model interpretation\n#### This is an annotated copy of the Fast.ai Machine Learning Lesson 2  notebook. Notes and edits by Joseph Catanzarite"},{"metadata":{"trusted":false,"_uuid":"292c876926b45c74689da711301e080bbb5337c0"},"cell_type":"code","source":"%load_ext autoreload\n%autoreload 2","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"c3ef3a0c2bc83045b215274a034f6837032b8fdd"},"cell_type":"code","source":"%matplotlib inline\n\nfrom fastai.imports import *\nfrom fastai.structured import *\nfrom pandas_summary import DataFrameSummary\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\nfrom IPython.display import display\nfrom sklearn import metrics\nimport feather\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"6a9b3a0134f87d87b8bd73e182380b5423a53579"},"cell_type":"code","source":"set_plot_sizes(12,14,16)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dfb1fc25c5b704f947baba4bfd0a1b9bade06362"},"cell_type":"markdown","source":"## Load in our data from last lesson and run proc_df to preprocess it"},{"metadata":{"trusted":false,"_uuid":"67944f27347f7452304d7813a1f04c4e659ea7fb"},"cell_type":"code","source":"PATH = \"C:/Users/jcat/fastai/data/bulldozers/\"\n# df_raw = pd.read_feather('tmp/bulldozers-raw')\ndf_raw = feather.read_dataframe('tmp/bulldozers-raw')\ndf_trn, y_trn, nas = proc_df(df_raw, 'SalePrice')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"e62882acde7f4fa0d4d5c58f45cc947400288ff0"},"cell_type":"code","source":"# split data into training and validation parts\ndef split_vals(a,n): return a[:n], a[n:]\nn_valid = 12000\nn_trn = len(df_trn)-n_valid\nX_train, X_valid = split_vals(df_trn, n_trn)\ny_train, y_valid = split_vals(y_trn, n_trn)\nraw_train, raw_valid = split_vals(df_raw, n_trn)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"73667d77a255cb43fd4e7176aca7a79018e1640d"},"cell_type":"code","source":"# functions to define and print scores\ndef rmse(x,y): return math.sqrt(((x-y)**2).mean())\n\ndef print_score(m):\n    res = [rmse(m.predict(X_train), y_train), rmse(m.predict(X_valid), y_valid),\n                m.score(X_train, y_train), m.score(X_valid, y_valid)]\n    if hasattr(m, 'oob_score_'): res.append(m.oob_score_)\n    print(res)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"ae9bae62ef316743c19896adacb6f236b298d6f7"},"cell_type":"code","source":"df_raw","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e4592c3bc75a42d29fda78e56dbb6aa7acd9eb85"},"cell_type":"markdown","source":"# Confidence based on tree variance"},{"metadata":{"_uuid":"e4c352fd4ab368e5279ab45b1c37a8e918c0ef87"},"cell_type":"markdown","source":"For model interpretation, there's no need to use the full dataset on each tree - using a subset will be both faster, and also provide better interpretability (since an overfit model will not provide much variance across trees)."},{"metadata":{"trusted":false,"_uuid":"cc9a1cd5a05e6c752b2706c51b6ac310fc3ff730"},"cell_type":"code","source":"# use a subset of examples for each tree, \n#     instead of the full bootstrap sample \nset_rf_samples(50000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"caee5338d5546546d5af9ff4cdc8e23620b8edbc"},"cell_type":"code","source":"??set_rf_samples","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"e089577fe94dfe1c8c00791bc6c4414e8a4e9f2a"},"cell_type":"code","source":"# metric = 0.2509\nm = RandomForestRegressor(n_estimators=40, min_samples_leaf=3, max_features=0.5, n_jobs=-1, oob_score=True)\nm.fit(X_train, y_train)\nprint_score(m)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"a2d31e97944b41bfc76c30aebb0b7cdf0a155b6d"},"cell_type":"code","source":"# compare to full bootstrap sample\n# score = 0.2268, so it's better by 0.024\nreset_rf_samples()\nm = RandomForestRegressor(n_estimators=40, min_samples_leaf=3, max_features=0.5, n_jobs=-1, oob_score=True)\nm.fit(X_train, y_train)\nprint_score(m)\n\n# return to subsampling\nset_rf_samples(50000)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0bacf5b005718a4d38c6d12766bf7b0da8283101"},"cell_type":"markdown","source":"We saw how the model averages predictions across the trees to get an estimate - but how can we know the confidence of the estimate? One simple way is to use the standard deviation of predictions, instead of just the mean. This tells us the *relative* confidence of predictions - that is, for rows where the trees give very different results, you would want to be more cautious of using those results, compared to cases where they are more consistent. Using the same example as in the last lesson when we looked at bagging:"},{"metadata":{"trusted":false,"_uuid":"19ff8aca6104f51d1ad479d28db6056a3ae13280"},"cell_type":"code","source":"%time preds = np.stack([t.predict(X_valid) for t in m.estimators_])\nnp.mean(preds[:,0]), np.std(preds[:,0])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1aa0cb6cf691314f16b107710983e1cfa73a7180"},"cell_type":"markdown","source":"When we use python to loop through trees like this, we're calculating each in series, which is slow! We can use parallel processing to speed things up:"},{"metadata":{"_uuid":"bfeb0d03eb693ada488a350ef0af3fb4222380a9"},"cell_type":"markdown","source":"??parallel_trees\nSignature: parallel_trees(m, fn, n_jobs=8)\nDocstring: <no docstring>\nSource:   \ndef parallel_trees(m, fn, n_jobs=8):\n        return list(ProcessPoolExecutor(n_jobs).map(fn, m.estimators_))\nFile:      c:\\users\\jcat\\fastai\\courses\\ml1\\fastai\\structured.py\nType:      function\n"},{"metadata":{"trusted":false,"_uuid":"aa66df272b74238cedcfe8408aad75fd872f8b5c"},"cell_type":"code","source":"# problem with parallelization\ndef get_preds(t): return t.predict(X_valid)\n%time preds = np.stack(parallel_trees(m, get_preds))\nnp.mean(preds[:,0]), np.std(preds[:,0])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"728b9120554098fa2c9fd0c6d1d66a0549adcc62"},"cell_type":"markdown","source":"We can see that different trees are giving different estimates this this auction. In order to see how prediction confidence varies, we can add this into our dataset."},{"metadata":{"trusted":false,"_uuid":"08cbb0c0782d891348cc07cd0cdaf0cdac7e5b50"},"cell_type":"code","source":"preds.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"6caeaa1f2815b0aa1db928accb0b455de5c31b5c"},"cell_type":"code","source":"x = raw_valid.copy()\nx['pred_std'] = np.std(preds, axis=0)\nx['pred'] = np.mean(preds, axis=0)\nx.Enclosure.value_counts().plot.barh();","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"8185cf026017c6101eb4f1b7227d04bda63512dd"},"cell_type":"code","source":"flds = ['Enclosure', 'SalePrice', 'pred', 'pred_std']\nenc_summ = x[flds].groupby('Enclosure', as_index=False).mean()\nenc_summ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"1d5f49fea96d0b83e998d2c7e13ac49bf7b3dccc"},"cell_type":"code","source":"# plot sale price grouped by enclosure category\nenc_summ = enc_summ[~pd.isnull(enc_summ.SalePrice)]\nenc_summ.plot('Enclosure', 'SalePrice', 'barh', xlim=(0,11));","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"f194a6d2745d1cb89ed3abadd538ae2312da4c53"},"cell_type":"code","source":"# include error bars\nenc_summ.plot('Enclosure', 'pred', 'barh', xerr='pred_std', alpha=0.6, xlim=(0,11));","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"940fa2c7dca81a17f3207cccfcab224cc84e20b2"},"cell_type":"markdown","source":"*Question*: Why are the predictions nearly exactly right, but the error bars are quite wide?"},{"metadata":{"trusted":false,"_uuid":"aeb9301307f9163671e19648c7ad15e70764ffd7"},"cell_type":"code","source":"raw_valid.ProductSize.value_counts().plot.barh();","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"d119692e2e36dc6e9b6b7962fc94798ac7770ba5"},"cell_type":"code","source":"flds = ['ProductSize', 'SalePrice', 'pred', 'pred_std']\nsumm = x[flds].groupby(flds[0]).mean()\nsumm","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"63125f54b1f9a687f7ede3755e5050cb8303fd1f"},"cell_type":"code","source":"# fractional error in predicted price\n(summ.pred_std/summ.pred).sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"662ba8624075c080092d7c1dc1987156d3d5055b"},"cell_type":"markdown","source":"# Feature importance"},{"metadata":{"_uuid":"1134d8b420b7f63f58fcfe328aa82d7e7b21fbc3"},"cell_type":"markdown","source":"It's not normally enough to just to know that a model can make accurate predictions - we also want to know *how* it's making predictions. The most important way to see this is with *feature importance*."},{"metadata":{"trusted":false,"_uuid":"dfd0cfcc7cbc9320b077bbb8bc1f5c7c297be301"},"cell_type":"code","source":"fi = rf_feat_importance(m, df_trn); fi[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"ac637b4750614db3edf5b056d166b79c01c482a8"},"cell_type":"code","source":"fi.plot('cols', 'imp', figsize=(10,6), legend=False);","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"4ffbfd1f4bb8b4e3d090fa2f129f3e3c0dfbf787"},"cell_type":"code","source":"def plot_fi(fi): return fi.plot('cols', 'imp', 'barh', figsize=(12,7), legend=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"897c65d74f83a3025a8eca4bf25b72ef01711ca6"},"cell_type":"code","source":"plot_fi(fi[:30]);","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"84d43eec7fa401fcc707121119b9aff96b276164"},"cell_type":"code","source":"to_keep = fi[fi.imp>0.005].cols; len(to_keep)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"848a493305b8ddb22400acb7395ed612ab962a25"},"cell_type":"code","source":"# keep features with importance > 0.005\ndf_keep = df_trn[to_keep].copy()\nX_train, X_valid = split_vals(df_keep, n_trn)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"bc065a0476427ee3b8524bcb50ba208f196c0242"},"cell_type":"code","source":"# eliminating unimportant features improved score by 0.007\nm = RandomForestRegressor(n_estimators=40, min_samples_leaf=3, max_features=0.5,\n                          n_jobs=-1, oob_score=True)\nm.fit(X_train, y_train)\nprint_score(m)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"af71fac2d787fd4f448ff4c84143ed79e55a4b54"},"cell_type":"code","source":"# feature importances within reduced feature set \n#     vary a bit from previous order\nfi = rf_feat_importance(m, df_keep)\nplot_fi(fi);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fca482d7c338e8bef5741e8007bc7b8b80371bb1"},"cell_type":"markdown","source":"## One-hot encoding"},{"metadata":{"_uuid":"9f7cabb8a3d3bb94a5286a86ddc285564d4993ad"},"cell_type":"markdown","source":"proc_df's optional *max_n_cat* argument will turn some categorical variables into new columns.\n\nFor example, the column **ProductSize** which has 6 categories:\n\n* Large\n* Large / Medium\n* Medium\n* Compact\n* Small\n* Mini\n\ngets turned into 6 new columns:\n\n* ProductSize_Large\n* ProductSize_Large / Medium\n* ProductSize_Medium\n* ProductSize_Compact\n* ProductSize_Small\n* ProductSize_Mini\n\nand the column **ProductSize** gets removed.\n\nIt will only happen to columns whose number of categories is no bigger than the value of the *max_n_cat* argument.\n\nNow some of these new columns may prove to have more important features than in the earlier situation, where all categories were in one column."},{"metadata":{"trusted":false,"_uuid":"638866e7f2ae3b775533ea90b4b535927aaedb0e"},"cell_type":"code","source":"# one-hot-encoding made metric worse by 0.01!\ndf_trn2, y_trn, nas = proc_df(df_raw, 'SalePrice', max_n_cat=7)\nX_train, X_valid = split_vals(df_trn2, n_trn)\n\nm = RandomForestRegressor(n_estimators=40, min_samples_leaf=3, max_features=0.6, n_jobs=-1, oob_score=True)\nm.fit(X_train, y_train)\nprint_score(m)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"7eee3b89187f0ab85ae456530f277e48c47a0fd1"},"cell_type":"code","source":"# importance ordering is changed \nfi = rf_feat_importance(m, df_trn2)\nplot_fi(fi[:25]);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"68adc94c3fd583072bfb9eeec1e2779ebb9c7305"},"cell_type":"markdown","source":"#### Done with one-hot-encoding experiment!"},{"metadata":{"_uuid":"54e872ae751af137b675da92ae821e098ad6e292"},"cell_type":"markdown","source":"# Removing redundant features"},{"metadata":{"_uuid":"bac73eafea57e2ade44d721d2850d527d9a8cb86"},"cell_type":"markdown","source":"One thing that makes this harder to interpret is that there seem to be some variables with very similar meanings. Let's try to remove redundent features."},{"metadata":{"trusted":false,"_uuid":"c326315ed1ff4259cd6c55da85de858e14f8a32e"},"cell_type":"code","source":"from scipy.cluster import hierarchy as hc","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"d946a2b52c6c67c37523942ad3eba3e3449d23c4"},"cell_type":"code","source":"# spearman correlation\ncorr = np.round(scipy.stats.spearmanr(df_keep).correlation, 4)\ncorr_condensed = hc.distance.squareform(1-corr)\nz = hc.linkage(corr_condensed, method='average')\nfig = plt.figure(figsize=(16,10))\ndendrogram = hc.dendrogram(z, labels=df_keep.columns, orientation='left', leaf_font_size=16)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b03c6ee90770b522efa2b55a1c5f60d5d6c8a2cf"},"cell_type":"markdown","source":"Let's try removing some of these related features to see if the model can be simplified without impacting the accuracy."},{"metadata":{"trusted":false,"_uuid":"1e97a33a500da5872e7ee1dd8155b3a13b27a80d"},"cell_type":"code","source":"def get_oob(df):\n    # why vary parameters from original values?\n    #     n_estimators = 40\n    #     min_samples_leaf = 3\n    #     max_features = 0.5\n    # m = RandomForestRegressor(n_estimators=30, min_samples_leaf=5, max_features=0.6, n_jobs=-1, oob_score=True)\n    # original parameter values improve metric by 0.005\n    #     so let's keep them\n    m = RandomForestRegressor(n_estimators=40, min_samples_leaf=3, max_features=0.5, n_jobs=-1, oob_score=True)\n    x, _ = split_vals(df, n_trn)\n    m.fit(x, y_train)\n    return m.oob_score_","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"35e8f2d33d6d8850c1bd01a514f1ce30c7367707"},"cell_type":"markdown","source":"Here's our baseline."},{"metadata":{"trusted":false,"_uuid":"3df29bb77cec0d59c32d37467934507baee8538c"},"cell_type":"code","source":"# revert to df_keep\nget_oob(df_keep)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c609f77096e63283f6af03d6868295b4ee80ab82"},"cell_type":"markdown","source":"Now we try removing each variable one at a time."},{"metadata":{"trusted":false,"_uuid":"39412ff1c7f5a99f2a8f543a8bbc21cf19ff3125"},"cell_type":"code","source":"# removing these features has little effect on oob score\nfor c in ('saleYear', 'saleElapsed', 'fiModelDesc', 'fiBaseModel', 'Grouser_Tracks', 'Coupler_System'):\n    print(c, get_oob(df_keep.drop(c, axis=1)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fe34e5a3e6e2abba1fb111b27c6a5f08c41370b5"},"cell_type":"markdown","source":"It looks like we can try one from each group for removal. Let's see what that does."},{"metadata":{"trusted":false,"_uuid":"3d4974f05e8a0f7ca29c739760deffd276069685"},"cell_type":"code","source":"# metric is worse by 0.002\nto_drop = ['saleYear', 'fiBaseModel', 'Grouser_Tracks']\nget_oob(df_keep.drop(to_drop, axis=1))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2df5af23c8c107c6e4aa66c1db986a239d5b2e5d"},"cell_type":"markdown","source":"Looking good! Let's use this dataframe from here. We'll save the list of columns so we can reuse it later."},{"metadata":{"trusted":false,"_uuid":"acb4fbbe2628390d62fbfdb367332d78b155dc27"},"cell_type":"code","source":"df_keep.drop(to_drop, axis=1, inplace=True)\nX_train, X_valid = split_vals(df_keep, n_trn)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"df9336f88f2a34921b61ca231afd09747fe31df4"},"cell_type":"code","source":"# save list of columns to keep\nnp.save('tmp/keep_cols.npy', np.array(df_keep.columns))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"812e7e5d5bb9050b32bd56f51f45231edab23a01"},"cell_type":"code","source":"# retrieve list of columns to keep\nkeep_cols = np.load('tmp/keep_cols.npy')\ndf_keep = df_trn[keep_cols]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a1a9e401aec4add7cf9334683a470b6f445d8f36"},"cell_type":"markdown","source":"And let's see how this model looks on the full dataset."},{"metadata":{"trusted":false,"_uuid":"3ab4a0b8f3fc23c85edd67d54b8bf844568c109c"},"cell_type":"code","source":"# revert to full bootstrap sample\nreset_rf_samples()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"692e2e8fa7cd4b386612aad4a98b081531904373"},"cell_type":"code","source":"# metric improved to 0.227 using full bootstrap, \n#     which is what we had before\nm = RandomForestRegressor(n_estimators=40, min_samples_leaf=3, max_features=0.5, n_jobs=-1, oob_score=True)\nm.fit(X_train, y_train)\nprint_score(m)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"71425ab909113de27495fbadb6f93b1fcf0b2de7"},"cell_type":"markdown","source":"### Conclusion: OHE, and eliminating unimportant and/or redundant features do _not_ improve the metric\n"},{"metadata":{"_uuid":"a21a7a166e91913c0532800db0d3505e1efe02d0"},"cell_type":"markdown","source":"# Partial dependence"},{"metadata":{"trusted":false,"_uuid":"ac6e00111ddc8be72f7c8f25774321806084f37a"},"cell_type":"code","source":"# first, install pdpbox and plotnine\n# pip install pdpbox\n# conda install -c conda-forge plotnine\nfrom pdpbox import pdp\nfrom plotnine import *","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"58ed987afa36cdd377aa3fbfcf21cb0147175988"},"cell_type":"code","source":"set_rf_samples(50000)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1253770db24bb3661647564e3f1934a16d42f133"},"cell_type":"markdown","source":"This next analysis will be a little easier if we use the 1-hot encoded categorical variables, so let's load them up again."},{"metadata":{"trusted":false,"_uuid":"8f7abf75db1dee0415bfb0206a0523948936b9a2"},"cell_type":"code","source":"# start with metric 0.253\ndf_trn2, y_trn, nas = proc_df(df_raw, 'SalePrice', max_n_cat=7)\nX_train, X_valid = split_vals(df_trn2, n_trn)\nm = RandomForestRegressor(n_estimators=40, min_samples_leaf=3, max_features=0.6, n_jobs=-1)\nm.fit(X_train, y_train);\nprint_score(m)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"861e98d3e42297a9fe35ed790c62a872570b3d09"},"cell_type":"code","source":"plot_fi(rf_feat_importance(m, df_trn2)[:10]);","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"0e54b69e1dccdb17240bfa6b53edaf777936ffd8"},"cell_type":"code","source":"df_raw.plot('YearMade', 'saleElapsed', 'scatter', alpha=0.01, figsize=(10,8));","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"e556e0793b5e15482b49aaf89f46b487790551ab"},"cell_type":"code","source":"x_all = get_sample(df_raw[df_raw.YearMade>1930], 500)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"5d33bf487308173e47e1e3e8dd012a2d6a94cef7"},"cell_type":"code","source":"# first install scikit-misc\n# pip install scikit-misc\nggplot(x_all, aes('YearMade', 'SalePrice'))+stat_smooth(se=True, method='loess')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"7ff8b6aee2ad4b1defec523b133495e9aab0b716"},"cell_type":"code","source":"x = get_sample(X_train[X_train.YearMade>1930], 500)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"2e93cbd0a8197b59586e18ca3375c157dd4b17ea"},"cell_type":"code","source":"def plot_pdp(feat_name, clusters=None):\n    p = pdp.pdp_isolate(m, x, feature=feat_name, model_features=x.columns)\n    return pdp.pdp_plot(p, feat_name, plot_lines=True, \n                        cluster=clusters is not None, n_cluster_centers=clusters)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"d8d1395c55da87f9a8a846956214b56a482fb430"},"cell_type":"code","source":"plot_pdp('YearMade')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"0e95fdf619f61526d84b5a3ce2c7df6cf728eb74"},"cell_type":"code","source":"plot_pdp('YearMade', clusters=5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"7b242f3033f9229060eb9a3ce36b6a838419d173"},"cell_type":"code","source":"feats = ['saleElapsed', 'YearMade']\np = pdp.pdp_interact(m, x, feats)\npdp.pdp_interact_plot(p, feats)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"5d0c943d31cc52e747a06dc8155b13fc39ba0092"},"cell_type":"code","source":"plot_pdp(['Enclosure_EROPS w AC', 'Enclosure_EROPS', 'Enclosure_OROPS'], 5)#, 'Enclosure')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"b2ca25d542457fbcc068cdc43125a769a69e37a6"},"cell_type":"code","source":"# define engineered feature 'age'\ndf_raw.YearMade[df_raw.YearMade<1950] = 1950\ndf_keep['age'] = df_raw['age'] = df_raw.saleYear-df_raw.YearMade","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"8748ea7bc9dedcd6a9830dd39256564655446219"},"cell_type":"code","source":"# age becomes the most important feature!\nX_train, X_valid = split_vals(df_keep, n_trn)\nm = RandomForestRegressor(n_estimators=40, min_samples_leaf=3, max_features=0.6, n_jobs=-1)\nm.fit(X_train, y_train)\nplot_fi(rf_feat_importance(m, df_keep));","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"78b8492e503a62b9215ee3bc970fc428bf106db4"},"cell_type":"markdown","source":"# Tree interpreter\n#### not sure what this section demonstrates?"},{"metadata":{"trusted":false,"_uuid":"a24e645f1446ad9530329cd5d1a0395b51934658"},"cell_type":"code","source":"# install treeinterpreter\n# pip install treeinterpreter\nfrom treeinterpreter import treeinterpreter as ti","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"de5d1cff0bf5134fb4aca45e7ae0c35c2856e032"},"cell_type":"code","source":"df_train, df_valid = split_vals(df_raw[df_keep.columns], n_trn)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"b318772eee555ce93cee5777e00f1865a703a291"},"cell_type":"code","source":"row = X_valid.values[None,0]; row","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"1127bfe13855a458442d123bfa09cad11f8d4561"},"cell_type":"code","source":"prediction, bias, contributions = ti.predict(m, row)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"0e652c912da28909c3b2d470656704bf1c99cab0"},"cell_type":"code","source":"len(contributions[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"9f8de958b23f949b0899115778455024dfa21149"},"cell_type":"code","source":"prediction[0], bias[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"628fa181c7bba3ef5718e46d5c5d15b7d44116d1"},"cell_type":"code","source":"idxs = np.argsort(contributions[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"d37588affbfb1d05cbde75bc8798a59b16c3010f"},"cell_type":"code","source":"df_valid.iloc[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"99731e804b350482f24c91a62d0692a3f5d8dd78"},"cell_type":"code","source":"[o for o in zip(df_keep.columns[idxs], df_valid.iloc[0][idxs], contributions[0][idxs])]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"48f89b08bb8cc6a57d19423305f1e2ef398fa7f5"},"cell_type":"code","source":"contributions[0].sum()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6213c126a40e9e542c81b3f0b761d3ff03538004"},"cell_type":"markdown","source":"# Extrapolation\n#### It's not totally clear what this section says about extrapolation. We identify and eliminate unnecessary features, and thereby realize an improvement in the metric."},{"metadata":{"trusted":false,"_uuid":"d5bb066188ff9666f1289feaa0f5c88d8b48b041"},"cell_type":"code","source":"df_ext = df_keep.copy()\ndf_ext['is_valid'] = 1\ndf_ext.is_valid[:n_trn] = 0\nx, y, nas = proc_df(df_ext, 'is_valid')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"4b46affca9f1773bd7da1b8456b43e13f390c700"},"cell_type":"code","source":"m = RandomForestClassifier(n_estimators=40, min_samples_leaf=3, max_features=0.5, n_jobs=-1, oob_score=True)\nm.fit(x, y);\nm.oob_score_","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"bde0e25b2ea16dd5c62a6e4007f04c4c84e569fd"},"cell_type":"code","source":"fi = rf_feat_importance(m, x); fi[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"64b77108877422d12560f38461905fb3eca68f45"},"cell_type":"code","source":"# top 3 features\nfeats=['SalesID', 'saleElapsed', 'MachineID']","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"49bcaac62c951b9618a39603615b157cde9e139b"},"cell_type":"code","source":"(X_train[feats]/1000).describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"13bb14584047a49bed669a773eaf4f3a2de0e76f"},"cell_type":"code","source":"(X_valid[feats]/1000).describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"3ad8cc3b180eac067355edc3c7bbc07b759b9c8d"},"cell_type":"code","source":"# drop top three features\nx.drop(feats, axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"fe4e705309bf80d5aef65dc32adeb1d7e9199f35"},"cell_type":"code","source":"# score is a bit worse\nm = RandomForestClassifier(n_estimators=40, min_samples_leaf=3, max_features=0.5, n_jobs=-1, oob_score=True)\nm.fit(x, y)\nm.oob_score_","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"ecc280bf31b82787353f9265de6643c81009647a"},"cell_type":"code","source":"fi = rf_feat_importance(m, x); fi[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"fbe305a2ed2bff5b7bbccc879df50946c38d8642"},"cell_type":"code","source":"#speed up by subsampling\nset_rf_samples(50000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"95c8c7682f83a69f97359b68327c8a199d894369"},"cell_type":"code","source":"# return to original sample\nX_train, X_valid = split_vals(df_keep, n_trn)\nm = RandomForestRegressor(n_estimators=40, min_samples_leaf=3, max_features=0.5, n_jobs=-1, oob_score=True)\nm.fit(X_train, y_train)\nprint_score(m)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"31dcd957597e996e40aed9295078c1c7689d40c3"},"cell_type":"code","source":"# top six features\nfeats=['SalesID', 'saleElapsed', 'MachineID', 'age', 'YearMade', 'saleDayofyear']","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"0ae55df041f55005b8b2c8345a7c93ca55383c02"},"cell_type":"code","source":"# remove top six features, one at a time to see effect on metric\n# metrics vary between 0.245 and 0.255\nfor f in feats:\n    df_subs = df_keep.drop(f, axis=1)\n    X_train, X_valid = split_vals(df_subs, n_trn)\n    m = RandomForestRegressor(n_estimators=40, min_samples_leaf=3, max_features=0.5, n_jobs=-1, oob_score=True)\n    m.fit(X_train, y_train)\n    print(f)\n    print_score(m)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"5287d38786463a7c3768f00007af751b22590958"},"cell_type":"code","source":"# revert to full bootstrap sample\nreset_rf_samples()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"817f8b871aafff00c95dd96f15b56f9fe6cdf6e4"},"cell_type":"code","source":"# removing these features gave a significant score reduction\n#     recall that previously with full bootstrap sample we\n#     got a score of 0.2182, original score was 0.2268\n# drop SalesID, MachineID, saleDayOfyear because dropping\n#    them individually reduced the metric more than any of \n#    the other three.\ndf_subs = df_keep.drop(['SalesID', 'MachineID', 'saleDayofyear'], axis=1)\nX_train, X_valid = split_vals(df_subs, n_trn)\nm = RandomForestRegressor(n_estimators=40, min_samples_leaf=3, max_features=0.5, n_jobs=-1, oob_score=True)\nm.fit(X_train, y_train)\nprint_score(m)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"28a5046bc464384bddc13e8e2ae5a240cfe0eaef"},"cell_type":"code","source":"plot_fi(rf_feat_importance(m, X_train));","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"c797c207222f056470b14e542cebfb97d371329a"},"cell_type":"code","source":"np.save('tmp/subs_cols.npy', np.array(df_subs.columns))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"da8338301c2dc3d814a8e803eb92902e4fa64aa5"},"cell_type":"markdown","source":"# Our final model!"},{"metadata":{"trusted":false,"_uuid":"0893416e329676e491845a128efdfc9eed6fdc9b"},"cell_type":"code","source":"# use more trees, and grow them completely\n# our metric improved by 0.007 to 0.2114\nm = RandomForestRegressor(n_estimators=160, max_features=0.5, n_jobs=-1, oob_score=True)\n%time m.fit(X_train, y_train)\nprint_score(m)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"c859221d83418f255b4aeb2fa8b8acf061d128b4"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"nbformat":4,"nbformat_minor":1}