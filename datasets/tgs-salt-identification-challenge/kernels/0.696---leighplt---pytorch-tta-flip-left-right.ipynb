{"cells":[{"metadata":{"_uuid":"d60ed94a7be1554705049ec2a7f39a412540f23e"},"cell_type":"markdown","source":"## Intro\nThis kernel demonstrate 1 simple test-time-augmentation (TTA). It's flip image left-right"},{"metadata":{"trusted":true,"_uuid":"37c5c102aef7a1d9c387e2ef0fd23ac21c52b7cf"},"cell_type":"code","source":"# Loading libs\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom torch.utils import data\nimport torchvision\nfrom torchvision import models\n\nimport cv2\nfrom pathlib import Path\nimport glob","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3aa2744ad750399793239de287274941e01b8ad8"},"cell_type":"code","source":"# Up TensorFlow, so driver up too - > torch can upgrade too\ntorch.__version__","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"111bcf74224748ed1c06dd07d7cf4430a6531663"},"cell_type":"code","source":"# class with metod tta_flip\n# use it below for 'Class Inheritance'\nclass TTAFunction:\n    \"\"\"\n    Simple TTA function\n    \"\"\"\n    @staticmethod\n    def hflip(x):\n        return x.flip(3)\n    \n    @staticmethod\n    def vflip(x):\n        return x.flip(2)\n    \n    def tta(self, x):\n        self.eval()\n        with torch.no_grad():\n            result = self.forward(x)\n            x = self.hflip(x)\n            result += self.hflip(self.forward(x))\n        return 0.5*result","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Model from:\n# https://github.com/ternaus/TernausNet/blob/master/unet_models.py\ndef conv3x3(in_, out):\n    return nn.Conv2d(in_, out, 3, padding=1)\n\nclass ConvRelu(nn.Module):\n    def __init__(self, in_, out):\n        super().__init__()\n        self.conv = conv3x3(in_, out)\n        self.activation = nn.ReLU(inplace=True)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.activation(x)\n        return x\n\nclass DecoderBlock(nn.Module):\n    def __init__(self, in_channels, middle_channels, out_channels):\n        super().__init__()\n\n        self.block = nn.Sequential(\n            ConvRelu(in_channels, middle_channels),\n            nn.ConvTranspose2d(middle_channels, out_channels, kernel_size=3, stride=2, padding=1, output_padding=1),\n            nn.ReLU(inplace=True)\n        )\n\n    def forward(self, x):\n        return self.block(x)\n\nclass UNet11(TTAFunction, nn.Module): # use our class with TTA function\n    def __init__(self, num_filters=32):\n        \"\"\"\n        :param num_classes:\n        :param num_filters:\n        \"\"\"\n        super().__init__()\n        self.pool = nn.MaxPool2d(2, 2)\n\n        # Convolutions are from VGG11\n        self.encoder = models.vgg11().features\n        \n        # \"relu\" layer is taken from VGG probably for generality, but it's not clear \n        self.relu = self.encoder[1]\n        \n        self.conv1 = self.encoder[0]\n        self.conv2 = self.encoder[3]\n        self.conv3s = self.encoder[6]\n        self.conv3 = self.encoder[8]\n        self.conv4s = self.encoder[11]\n        self.conv4 = self.encoder[13]\n        self.conv5s = self.encoder[16]\n        self.conv5 = self.encoder[18]\n\n        self.center = DecoderBlock(num_filters * 8 * 2, num_filters * 8 * 2, num_filters * 8)\n        self.dec5 = DecoderBlock(num_filters * (16 + 8), num_filters * 8 * 2, num_filters * 8)\n        self.dec4 = DecoderBlock(num_filters * (16 + 8), num_filters * 8 * 2, num_filters * 4)\n        self.dec3 = DecoderBlock(num_filters * (8 + 4), num_filters * 4 * 2, num_filters * 2)\n        self.dec2 = DecoderBlock(num_filters * (4 + 2), num_filters * 2 * 2, num_filters)\n        self.dec1 = ConvRelu(num_filters * (2 + 1), num_filters)\n        \n        self.final = nn.Conv2d(num_filters, 1, kernel_size=1, )\n\n    def forward(self, x):\n        conv1 = self.relu(self.conv1(x))\n        conv2 = self.relu(self.conv2(self.pool(conv1)))\n        conv3s = self.relu(self.conv3s(self.pool(conv2)))\n        conv3 = self.relu(self.conv3(conv3s))\n        conv4s = self.relu(self.conv4s(self.pool(conv3)))\n        conv4 = self.relu(self.conv4(conv4s))\n        conv5s = self.relu(self.conv5s(self.pool(conv4)))\n        conv5 = self.relu(self.conv5(conv5s))\n\n        center = self.center(self.pool(conv5))\n\n        # Deconvolutions with copies of VGG11 layers of corresponding size \n        dec5 = self.dec5(torch.cat([center, conv5], 1))\n        dec4 = self.dec4(torch.cat([dec5, conv4], 1))\n        dec3 = self.dec3(torch.cat([dec4, conv3], 1))\n        dec2 = self.dec2(torch.cat([dec3, conv2], 1))\n        dec1 = self.dec1(torch.cat([dec2, conv1], 1))\n        return torch.sigmoid(self.final(dec1))\n\ndef unet11(**kwargs):\n    model = UNet11(**kwargs)\n    return model\n\ndef get_model():\n    np.random.seed(717)\n    torch.cuda.manual_seed(717);\n    torch.manual_seed(717);\n    model = unet11()\n    model.train()\n    return model.to(device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"96e6dd8d99d2d9b5053102711702105b12f6d666"},"cell_type":"code","source":"model_pth = '../input/goto-pytorch-fix-for-v0-3/tgs-13.pth'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ac908ed05b1c89c6b4c7ecc43389bffca5af7d1b"},"cell_type":"code","source":"directory = '../input/tgs-salt-identification-challenge'\ndevice = 'cuda'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7114b9f3da03d4688ecfdecd7c7008a0be0c8004"},"cell_type":"code","source":"def load_image(path, mask = False):\n    \"\"\"\n    Load image from a given path and pad it on the sides, so that eash side is divisible by 32 (newtwork requirement)\n    \n    if pad = True:\n        returns image as numpy.array, tuple with padding in pixels as(x_min_pad, y_min_pad, x_max_pad, y_max_pad)\n    else:\n        returns image as numpy.array\n    \"\"\"\n    img = cv2.imread(str(path))\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    \n    height, width, _ = img.shape\n\n    # Padding in needed for UNet models because they need image size to be divisible by 32 \n    if height % 32 == 0:\n        y_min_pad = 0\n        y_max_pad = 0\n    else:\n        y_pad = 32 - height % 32\n        y_min_pad = int(y_pad / 2)\n        y_max_pad = y_pad - y_min_pad\n        \n    if width % 32 == 0:\n        x_min_pad = 0\n        x_max_pad = 0\n    else:\n        x_pad = 32 - width % 32\n        x_min_pad = int(x_pad / 2)\n        x_max_pad = x_pad - x_min_pad\n    \n    img = cv2.copyMakeBorder(img, y_min_pad, y_max_pad, x_min_pad, x_max_pad, cv2.BORDER_REFLECT_101)\n    if mask:\n        # Convert mask to 0 and 1 format\n        img = img[:, :, 0:1] // 255\n        return torch.from_numpy(np.transpose(img, (2, 0, 1)).astype('float32'))\n    else:\n        img = img / 255.0\n        return torch.from_numpy(np.transpose(img, (2, 0, 1)).astype('float32'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"87e0c6c34c6916e43b8f4e8e1f6eb708f8049b3d"},"cell_type":"code","source":"class TGSSaltDataset(data.Dataset):\n    def __init__(self, root_path, file_list, is_test = False):\n        self.is_test = is_test\n        self.root_path = root_path\n        self.file_list = file_list\n    \n    def __len__(self):\n        return len(self.file_list)\n    \n    def __getitem__(self, index):\n        if index not in range(0, len(self.file_list)):\n            return self.__getitem__(np.random.randint(0, self.__len__()))\n        \n        file_id = self.file_list[index]\n        \n        image_folder = os.path.join(self.root_path, \"images\")\n        image_path = os.path.join(image_folder, file_id + \".png\")\n        \n        mask_folder = os.path.join(self.root_path, \"masks\")\n        mask_path = os.path.join(mask_folder, file_id + \".png\")\n        \n        image = load_image(image_path)\n        \n        if self.is_test:\n            return (image,)\n        else:\n            mask = load_image(mask_path, mask = True)\n            return image, mask\n\ndepths_df = pd.read_csv(os.path.join(directory, 'train.csv'))\n\ntrain_path = os.path.join(directory, 'train')\nfile_list = list(depths_df['id'].values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e82e4b9d3fb425042aadd47c926a47199d271023"},"cell_type":"code","source":"file_list_val = file_list[::10]\nfile_list_train = [f for f in file_list if f not in file_list_val]\ndataset = TGSSaltDataset(train_path, file_list_train)\ndataset_val = TGSSaltDataset(train_path, file_list_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7ae9e7e3832de8755d513b8872167bd1c3faf6cc"},"cell_type":"code","source":"model = get_model()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"79df478eba4f0306ad9ccc03782f218b7c288b0b"},"cell_type":"code","source":"# load weights from previous model\nmodel.load_state_dict(torch.load(model_pth)['state_dict'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0f62063821145c15222a26a227fc707e6c72e1b0"},"cell_type":"code","source":"test_path = os.path.join(directory, 'test')\ntest_file_list = glob.glob(os.path.join(test_path, 'images', '*.png'))\ntest_file_list = [f.split('/')[-1].split('.')[0] for f in test_file_list]\nprint('First 3 names of test files:', test_file_list[:3])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"74dfe8ef0d10f46a148d91160da9f65c603689c8"},"cell_type":"markdown","source":"### Use tta for prediction"},{"metadata":{"trusted":true,"_uuid":"dfee8c746dcc0d6b599758188a61023604d9076a"},"cell_type":"code","source":"print(f\"Test size: {len(test_file_list)}\")\ntest_dataset = TGSSaltDataset(test_path, test_file_list, is_test = True)\n\nall_predictions = []\nfor image in data.DataLoader(test_dataset, batch_size = 30):\n    image = image[0].type(torch.FloatTensor).to(device)\n    y_pred = model.tta(image).cpu().data.numpy() # use tta_flip\n    all_predictions.append(y_pred)\nall_predictions_stacked = np.vstack(all_predictions)[:, 0, :, :]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e4768316a848dba083ea5cbe4ff6ba85a07df852"},"cell_type":"code","source":"height, width = 101, 101\n\nif height % 32 == 0:\n    y_min_pad = 0\n    y_max_pad = 0\nelse:\n    y_pad = 32 - height % 32\n    y_min_pad = int(y_pad / 2)\n    y_max_pad = y_pad - y_min_pad\n\nif width % 32 == 0:\n    x_min_pad = 0\n    x_max_pad = 0\nelse:\n    x_pad = 32 - width % 32\n    x_min_pad = int(x_pad / 2)\n    x_max_pad = x_pad - x_min_pad","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c7908c8b6a88df2f83d0348f914908b3e863cfd6"},"cell_type":"code","source":"all_predictions_stacked = all_predictions_stacked[:, y_min_pad:128 - y_max_pad, x_min_pad:128 - x_max_pad]\nall_predictions_stacked.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7fd0cfc95174aabe60cd9712e8fff94c8d9d9bc8"},"cell_type":"code","source":"test_dataset = TGSSaltDataset(test_path, test_file_list, is_test = True)\n\nval_predictions = []\nval_masks = []\nfor image, mask in data.DataLoader(dataset_val, batch_size = 30):\n    image = image.type(torch.FloatTensor).to(device)\n    y_pred = model.tta(image).cpu().data.numpy()\n    val_predictions.append(y_pred)\n    val_masks.append(mask)\n    \nval_predictions_stacked = np.vstack(val_predictions)[:, 0, :, :]\n\nval_masks_stacked = np.vstack(val_masks)[:, 0, :, :]\nval_predictions_stacked = val_predictions_stacked[:, y_min_pad:128 - y_max_pad, x_min_pad:128 - x_max_pad]\n\nval_masks_stacked = val_masks_stacked[:, y_min_pad:128 - y_max_pad, x_min_pad:128 - x_max_pad]\nval_masks_stacked.shape, val_predictions_stacked.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f89021b94dbbf8c5c5c879375741ddbcfe05485b"},"cell_type":"code","source":"from sklearn.metrics import jaccard_similarity_score\n\nmetric_by_threshold = []\nfor threshold in np.linspace(0, 1, 11):\n    val_binary_prediction = (val_predictions_stacked > threshold).astype(int)\n    \n    iou_values = []\n    for y_mask, p_mask in zip(val_masks_stacked, val_binary_prediction):\n        iou = jaccard_similarity_score(y_mask.flatten(), p_mask.flatten())\n        iou_values.append(iou)\n    iou_values = np.array(iou_values)\n    \n    accuracies = [\n        np.mean(iou_values > iou_threshold)\n        for iou_threshold in np.linspace(0.5, 0.95, 10)\n    ]\n    print('Threshold: %.1f, Metric: %.3f' % (threshold, np.mean(accuracies)))\n    metric_by_threshold.append((np.mean(accuracies), threshold))\n    \nbest_metric, best_threshold = max(metric_by_threshold)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5c6b629b39d1d758cc979e3c0aebd676d4b30cd7"},"cell_type":"code","source":"threshold = best_threshold\nbinary_prediction = (all_predictions_stacked > threshold).astype(int)\n\ndef rle_encoding(x):\n    dots = np.where(x.T.flatten() == 1)[0]\n    run_lengths = []\n    prev = -2\n    for b in dots:\n        if (b > prev+1): run_lengths.extend((b + 1, 0))\n        run_lengths[-1] += 1\n        prev = b\n    return run_lengths\n\nall_masks = []\nfor p_mask in list(binary_prediction):\n    p_mask = rle_encoding(p_mask)\n    all_masks.append(' '.join(map(str, p_mask)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"479502106656a1d87c1ae9151aad7596ca855b3b"},"cell_type":"markdown","source":"### After fix wrong flip score chenged from 0.684 to 0.696. Its still be only example! "},{"metadata":{"trusted":true,"_uuid":"0c993942eebd68e08f347aa26daf9b3c6e4de6f5"},"cell_type":"code","source":"submit = pd.DataFrame([test_file_list, all_masks]).T\nsubmit.columns = ['id', 'rle_mask']\nsubmit.to_csv('submit_baseline_torch_with_tta.csv', index = False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}