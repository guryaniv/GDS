{"cells":[{"metadata":{"_uuid":"5c23f3101199c38222b13ba2ae0406c4d43dc97e"},"cell_type":"markdown","source":"Just for fun:)\n\nUsing knowledge gained by competing in quora incencere questions:\n\nhttps://www.kaggle.com/xsakix/pytorch-bilstm-meta-v2\n\n# What to do?\n\nWhat if I would to change the activation function?\n\nWhat if I would to change the optimizer?\n\nWhat if I would to change the learning rate?\n\nWhat if I would to change the weight decay?\n\nWhat if I would to change the momentum?\n\n**What if I could do this whole thing in some kind of grid search???\n**\n\nWhat if I would to use a cnn to filter \"important\" features?\n\nWhat if I would to use gru/lstm to learn if features have sequential dependencies?\n\nWhat if I would to use some kind of bayesian feature selector?\n\nWhat if I would to use some kind of weighted feature selection?\n\nHumans can generalize from limited number of samples. What would happen if I would to use RL for this problem? Could a RL alg learn to generalize this problem?\n\nRemarks:\n\n2019-02-19:\nLooks like a good starting point for improving score.\nHigher validation aucroc leads to higher LB.\n\n2019-02-20:\n\nEverything is nondeterministic. I don't know currently how to make this kernel deterministic. I tried to turn off GPU and cuda, but it doesn't work. \n\nCould It be that the amount of data for training is so low, that it can't be deterministic? (i don't believe this)\nCould It be that pytorch is nodeterministic? (also i don't believe this)\nWhat's the reason? How can I make this deterministic?\n\nSolution:\nseeding needs to be done before each model training. Simple test is:\n\nseed_everything(seed)\nprint(np.random.rand(10))\nseed_everything(seed)\nprint(np.random.rand(10))\n\nvs\n\nprint(np.random.rand(10))\nprint(np.random.rand(10))\n\n21-02-2019:\n\nDid a CV over activation functions, optimizers, learning rates and weight decay, but the resulting \"setup\" gained only 0.561 on LB.\nThis means that a good validation AUCROC doesn't lead in all cases to a good LB.\n\nWould a situation in which training AUCROC and validation AUCROC differ only little be better in this case? If they differ too much (what is too much?) does it mean it can't generalize properly?\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2cb7d4ec05661a3023a790cb659a024c9b6e7c5c"},"cell_type":"code","source":"import random\nimport torch\n\nseed = 12\n\ndef seed_everything(seed=1234):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    \nseed_everything(seed)\nprint('Seeding done...')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"df_train = pd.read_csv(\"../input/train.csv\")\ntrain_y = df_train.target.values\ntrain_x = df_train.drop(columns=['id','target']).values\nprint(train_x[:5])\nprint('-'*80)\nprint(train_y[:5])\nprint('-'*80)\nprint(train_y.shape)\nprint(train_x.shape)\nprint('-'*80)\n\ndf_test = pd.read_csv(\"../input/test.csv\")\nprint(df_test.shape)\ntest_x = df_test.drop(columns=['id']).values\nprint(test_x.shape)\nprint('-'*80)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e7d2183392ccd01fc9ff0842e42834975714bd65"},"cell_type":"code","source":"# oversampling doesnt work\n# from imblearn.over_sampling import RandomOverSampler,ADASYN, SMOTE, SMOTENC\n\n# resampled_train_x1, resampled_train_y1 = RandomOverSampler(random_state=seed).fit_resample(train_x, train_y)\n# resampled_train_x2, resampled_train_y2 = ADASYN(random_state=seed).fit_resample(train_x, train_y)\n# resampled_train_x3, resampled_train_y3 = SMOTE(random_state=seed).fit_resample(train_x, train_y)\n# resampled_train_x4, resampled_train_y4 = SMOTENC(random_state=seed,categorical_features=[0,1]).fit_resample(train_x, train_y)\n# resampled_train_x = np.concatenate([resampled_train_x1,resampled_train_x2,resampled_train_x3,resampled_train_x4])\n# resampled_train_y = np.concatenate([resampled_train_y1,resampled_train_y2,resampled_train_y3,resampled_train_y4])\n# print(resampled_train_x.shape)\n# print(resampled_train_y.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3b2b8ebdf86ce3de18585e379197efc4202197fe"},"cell_type":"code","source":"# takes too long, not feasible\n# import pymc3 as pm\n\n# betas = []\n\n# with pm.Model() as model:\n#     sigma = pm.Uniform(name='sigma', lower=np.min(train_x), upper=np.max(train_y))\n#     for i in range(train_x.shape[1]):\n#         betas.append(pm.Normal(name='b'+str(i), mu=mu_x[i], sd=std_x[i]))\n#     mu = pm.Deterministic('mu', sum([betas[i]*train_x[:,i] for i in range(train_x.shape[1])]))\n#     target = pm.Normal(name='target', mu=mu, sd=sigma, observed=train_y)\n#     trace_model = pm.sample(1000, tune=1000)\n\n\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b4893074ab477530d93da58f79f46b514f95a532"},"cell_type":"code","source":"#src: https://github.com/Bjarten/early-stopping-pytorch/blob/master/pytorchtools.py\nclass EarlyStopping:\n    \"\"\"Early stops the training if validation loss dosen't improve after a given patience.\"\"\"\n    def __init__(self, patience=7, verbose=False):\n        \"\"\"\n        Args:\n            patience (int): How long to wait after last time validation loss improved.\n                            Default: 7\n            verbose (bool): If True, prints a message for each validation loss improvement. \n                            Default: False\n        \"\"\"\n        self.patience = patience\n        self.verbose = verbose\n        self.counter = 0\n        self.best_score = None\n        self.early_stop = False\n        self.val_loss_min = np.Inf\n\n    def __call__(self, val_loss, model):\n\n        score = -val_loss\n\n        if self.best_score is None:\n            self.best_score = score\n            self.save_checkpoint(val_loss, model)\n        elif score < self.best_score:\n            self.counter += 1\n            if self.verbose:\n                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n            if self.counter >= self.patience:\n                self.early_stop = True\n        else:\n            self.best_score = score\n            self.save_checkpoint(val_loss, model)\n            self.counter = 0\n\n    def save_checkpoint(self, val_loss, model):\n        '''Saves model when validation loss decrease.'''\n        if self.verbose:\n            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n        torch.save(model.state_dict(), 'checkpoint.pt')\n        self.val_loss_min = val_loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"38de77025d7e3ed531c43c2da250de2eb614c049"},"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\n#https://www.kaggle.com/shujian/single-rnn-with-4-folds-v1-9\ndef threshold_search(y_true, y_proba):\n    best_threshold = 0\n    best_score = 0\n    for threshold in [i * 0.01 for i in range(100)]:\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\")\n            score = roc_auc_score(y_true=y_true, y_score=y_proba > threshold)\n#         print('\\rthreshold = %f | score = %f'%(threshold,score),end='')\n        if score > best_score:\n            best_threshold = threshold\n            best_score = score\n#     print('\\nbest threshold is % f with score %f'%(best_threshold,best_score))\n    search_result = {'threshold': best_threshold, 'AUCROC': best_score}\n    return search_result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bedc9bbec1cd714b340dea71ad56d3a75cdb1002"},"cell_type":"code","source":"# generates same data\n# X_train1,X_val,y_train1, y_val = train_test_split(train_x,train_y,random_state=seed,stratify=train_y)\n# X_train2,X_val2,y_train2, y_val2 = train_test_split(train_x,train_y,random_state=seed,stratify=train_y)\n\n# print(X_train1[X_train1 != X_train2])\n# print(X_val[X_val != X_val2])\n# print(y_train1[y_train1 != y_train2])\n# print(y_val[y_val != y_val2])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"28c53c0c24f0776362bac369306da09943d5582f"},"cell_type":"code","source":"import torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torch.utils.data\nimport torchtext.data\nimport warnings\nfrom sklearn.metrics import accuracy_score\nfrom torch.autograd import Variable\nimport warnings\nfrom sklearn.model_selection import StratifiedKFold,train_test_split\n\ndef weights_init(m):\n    if isinstance(m, nn.Linear):\n        nn.init.xavier_uniform_(m.weight)\n\nclass OverfitModel(nn.Module):\n    \n    def __init__(self,activation,size):\n        super(OverfitModel,self).__init__()\n        self.classifier = nn.Sequential(            \n            nn.Linear(300,size, bias=False),\n            activation,\n            nn.Dropout(0.5),\n            nn.Linear(size,1,bias=False)\n        );\n\n    def forward(self,x):\n        return self.classifier(x)\n\n\ndef eval_on_set(model,test_loader,loss_function):\n    pred = []\n    avg_loss = 0.\n    model.eval()\n    with torch.no_grad():\n        for batch,(x_test_batch,y_test_batch) in enumerate(list(test_loader),1):\n            y_pred = model(x_test_batch).squeeze(1)\n            pred += torch.sigmoid(y_pred).cpu().detach().numpy().tolist()\n            loss = loss_function(y_pred,y_test_batch)\n            avg_loss += loss.item()\n            \n    return np.array(pred),avg_loss/batch\n\n\ndef train(model, train_loader,optimizer,loss_function ):    \n    \n    model.train()\n    avg_loss = 0\n    pred = []\n    for batch,(x_batch,y_true) in enumerate(list(iter(train_loader)),1):\n        optimizer.zero_grad()\n\n        y_pred = model(x_batch).squeeze(1)\n        pred += torch.sigmoid(y_pred).cpu().detach().numpy().tolist()\n        loss = loss_function(y_pred,y_true)\n        avg_loss += loss.item()\n\n        loss.backward()\n        optimizer.step()\n    \n    return np.array(pred),avg_loss/batch\n\ndef eval_sub(model,submission_loader):\n\n    pred = []\n    model.eval()\n    with torch.no_grad():\n        for (x,) in list(submission_loader):       \n            y_pred = torch.sigmoid(model(x).squeeze(1)).detach()\n            pred += y_pred.cpu().numpy().tolist()\n\n    return np.array(pred)\n\nclass Stats:\n    def __init__(self):\n        self.split_losses = []\n        self.split_aucrocs = []\n        self.split_val_losses = []\n        self.split_val_aucrocs = []\n        \n        self.model_split_losses = []\n        self.model_split_aucrocs = []\n        self.model_split_val_losses = []\n        self.model_split_val_aucrocs = []\n        \n    def clear_model_stat(self):\n        self.model_split_losses = []\n        self.model_split_aucrocs = []\n        self.model_split_val_losses = []\n        self.model_split_val_aucrocs = []\n            \n    def append_train_epoch(self,loss,auroc):\n        self.model_split_losses.append(loss)\n        self.model_split_aucrocs.append(auroc)\n    \n    def append_val_epoch(self,loss,auroc):\n        self.model_split_val_losses.append(loss)\n        self.model_split_val_aucrocs.append(auroc)\n    \n    def append_model(self):\n        self.split_losses.append(self.model_split_losses)\n        self.split_aucrocs.append(self.model_split_aucrocs)\n        self.split_val_losses.append(self.model_split_val_losses)\n        self.split_val_aucrocs.append(self.model_split_val_aucrocs)\n\ndef init_fc(worker_id):\n    np.random.seed(seed)\n\ndef create_data_loader(X,Y,batch_size=50):\n#     x_tensor = torch.tensor(X, dtype=torch.float32).cuda()\n#     y_tensor = torch.tensor(Y, dtype=torch.float32).cuda()\n    x_tensor = torch.tensor(X, dtype=torch.float32)\n    y_tensor = torch.tensor(Y, dtype=torch.float32)\n    dataset = torch.utils.data.TensorDataset(x_tensor,y_tensor)\n    return torch.utils.data.DataLoader(dataset=dataset,batch_size=batch_size,shuffle=True, worker_init_fn=init_fc, num_workers=0)\n    \ndef create_submission_loader(X,batch_size=50):\n#     submission_dataset = torch.utils.data.TensorDataset(torch.tensor(X, dtype=torch.float32).cuda())\n    submission_dataset = torch.utils.data.TensorDataset(torch.tensor(X, dtype=torch.float32))\n    submission_loader = torch.utils.data.DataLoader(dataset=submission_dataset,batch_size=batch_size, shuffle=False)\n    return submission_loader\n\ndef one_cv(model,optimizer):\n    seed_everything(seed)\n#     model = model.cuda()\n    model.apply(weights_init)\n    batch_size=50\n    \n    stats = Stats()\n#     loss_function = nn.BCEWithLogitsLoss().cuda()        \n    loss_function = nn.BCEWithLogitsLoss()\n    early_stop = EarlyStopping(patience=2)\n\n    X_train1,X_val,y_train1, y_val = train_test_split(train_x,train_y,random_state=seed,stratify=train_y)\n    train_loader =create_data_loader(X_train1,y_train1,batch_size)\n    val_loader =create_data_loader(X_val,y_val,batch_size)\n\n    stats.clear_model_stat()\n    for epoch in range(20):\n        y_pred,loss = train(model, train_loader,optimizer,loss_function)\n        search = threshold_search(y_train1,y_pred)\n        aucroc = search['AUCROC']\n        stats.append_train_epoch(loss,aucroc)\n\n        y_pred, val_loss = eval_on_set(model,val_loader, loss_function)\n        search = threshold_search(y_val,y_pred)\n        val_aucroc = search['AUCROC']\n        stats.append_val_epoch(val_loss,val_aucroc)\n\n#         print('EPOCH: ',epoch,': loss :',loss,': aucroc : ',aucroc,' : val loss: ',val_loss, ': val aucroc: ',val_aucroc)\n#         print('-'*80)\n\n        early_stop(np.round(1.-search['AUCROC'],decimals=5),model)\n        if early_stop.early_stop:\n            break\n\n    stats.append_model()\n\n#     print('FINISHED TRAINING META...')\n    #load best performing\n    model.load_state_dict(torch.load('checkpoint.pt'))\n\n    y_pred,_ = eval_on_set(model,train_loader, loss_function)\n    search = threshold_search(y_train1,y_pred)\n    train_aucroc = search['AUCROC']\n#     print('AUCROC:',train_aucroc)\n\n    y_pred,_ = eval_on_set(model,val_loader, loss_function)\n    search = threshold_search(y_val,y_pred)\n    val_aucroc = search['AUCROC']\n#     print('VAL AUCROC:',val_aucroc)\n    score = np.abs(val_aucroc-train_aucroc)/(val_aucroc*100.)\n    if val_aucroc < 0.7:\n        # add penalty for any less then 0.6 \n        score+=1.\n#     print('SCORE:', score)\n    \n    \n    return score,stats,search","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8265183a7a99ca117ff8c556551a0bfb4596a4ed"},"cell_type":"code","source":"activations = [\n#     OverfitModel(nn.AdaptiveLogSoftmaxWithLoss(in_features=300,n_classes=2,cutoffs= [1])),\n    'nn.ReLU',\n    'nn.Tanh',\n    'nn.Sigmoid',    \n    'nn.Softmax',    \n]\n\nrequeres_dim = [\n    'nn.Softmin',    \n    'nn.Softmax',    \n    'nn.LogSoftmax']\n\n# for threshold in [i * 0.01 for i in range(100)]:\n#     models.append(OverfitModel(nn.Hardshrink(threshold)))\n\noptimizers =[\n    'optim.Adam',\n    'optim.RMSprop',\n    'optim.Adadelta',\n    'optim.Adagrad',\n    'optim.Adamax',\n    'optim.ASGD'\n]\n\nlrs = [1,1e-1,1e-2,1e-3,1e-4]\ndecays = [1,1e-1,1e-2,1e-3,1e-4,1e-5]\nnetworks = [16,32,64,128,256,512,1024]\n\nresults = {}\n\n#inefficient...\nfor a in activations:\n    for o in optimizers:\n        for lr in lrs:\n            for decay in decays:\n                for size in networks:\n                    if a not in requeres_dim:\n                        activation = eval(a)()\n                    else:\n                        activation = eval(a)(dim=1)\n                    model = OverfitModel(activation,size)\n#                     print(a,':',o,':',lr,':',decay)\n                    optimizer = eval(o)(model.parameters(),lr=lr,weight_decay=decay)\n                    score,_,_ = one_cv(model,optimizer)\n                    if score >= 1.:\n                        continue\n                    results[score] = {'size':size,'activation':a,'optim':o,'lr':lr,'decay':decay}\n                    print('-'*80)\n                    print(score,':',results[score])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"31b960e40d5131ced857455cddd7276078d1074f"},"cell_type":"code","source":"# if searching for model is the case\n# best_model = None\n# best_search = None\n\n# for model in results.keys():\n#     print('* ',model,':',results[model]['AUCROC'])\n#     if best_search is None or best_search['AUCROC'] < results[model]['AUCROC']:\n#         best_search = results[model]\n#         best_model = model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0eb470857bbffd539adaa8c149c003f553b4cb01"},"cell_type":"code","source":"results","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"442967b28ddd8996fbf596d4b8e22745b345beb2"},"cell_type":"code","source":"import collections\n\n\nordered_keys = list(sorted(results.keys()))\n\nprint(results[ordered_keys[0]])\nbest_optim = results[ordered_keys[0]]['optim']\nbest_lr = results[ordered_keys[0]]['lr']\nbest_activation = results[ordered_keys[0]]['activation']\nbest_decay = results[ordered_keys[0]]['decay']\nbest_size=results[ordered_keys[0]]['size']\n# {'activation': 'nn.Hardtanh', 'optim': 'optim.Adamax', 'lr': 1, 'decay': 1e-05}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6d765d575a8390cb08a126190e9dc2ffbf95fe8b"},"cell_type":"code","source":"if best_activation in requeres_dim:\n    activation = eval(best_activation)(dim=1)\nelse:\n    activation = eval(best_activation)()\nmodel = OverfitModel(activation,size=best_size)\nprint(model)\nscore,stats,search = one_cv(model,eval(best_optim)(model.parameters(),lr=best_lr,weight_decay=best_decay))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"44fb167d5d9ab1822708abc900c3619bee138530"},"cell_type":"code","source":"def plot(loss,val_loss,aucroc,val_aucroc):\n    f,ax = plt.subplots(1,2)\n    f.set_size_inches(14,6)\n    ax[0].plot(loss, label='loss')\n    ax[0].plot(val_loss, label='val_loss')\n    ax[0].legend()\n    ax[1].plot(aucroc, label ='aucroc')\n    ax[1].plot(val_aucroc,label='val_aucroc')\n    ax[1].legend()\n    plt.plot()\n    \n    \nfor i in range(len(stats.split_losses)):\n    plot(stats.split_losses[i],stats.split_val_losses[i],stats.split_aucrocs[i],stats.split_val_aucrocs[i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8630ab81c8f865624c185ce0b71b645a30e46c3a"},"cell_type":"code","source":"submission_loader = create_submission_loader(test_x)\ny_pred = eval_sub(model,submission_loader)\nprint(search)\n\ndf_subm = pd.DataFrame()\ndf_subm['id'] = df_test.id\ndf_subm['target'] = (y_pred > search['threshold']).astype(int)\nprint(df_subm.head())\nprint(df_subm.shape)\ndf_subm.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}