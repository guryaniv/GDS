Names	Desc	Eval	Tag	Over
20-newsgroups-ciphertext-challenge				
advance-u.s.-international-trade-in-goods-series			['economics', 'small']	Content More details about each file are in the individual file descriptions.Context This is a dataset from the U.S. Census Bureau hosted by the FederalReserve Economic Database (FRED). FRED has a data platform found here and theyupdate their information according the amount of data that is brought in.Explore the U.S. Census Bureau using Kaggle and all of the data sourcesavailable through the U.S. Census Bureau organization page! Update Frequency:This dataset is updated daily. Acknowledgements This dataset is maintainedusing FRED's API and Kaggle's API. Cover photo by Lisa Zoe on UnsplashUnsplash Images are distributed under a unique Unsplash License.
aerial-cactus-identification				
airbus-ship-detection				
all-crypto-currencies			['finance', 'internet', 'business', 'medium', 'featured']	Cryptocurrency Market Data Historical Cryptocurrency Prices For ALL Tokens!Summary > Observations: 758,534 > Variables: 13 > Crypto Tokens: 1,584 > StartDate: 28/04/2017 > End Date: 21/05/2018 Description All historic open, high,low, close, trading volume and market cap info for all cryptocurrencies. I'vehad to go over the code with a fine tooth comb to get it compatible with CRANso there have been significant enhancements to how some of the fieldconversions have been undertaken and the data being cleaned. This shouldeliminate a few issues around number formatting or unexpected handling ofscientific notations. Data Structure Observations: 649,051 Variables: 13 $slug
allstate-claims-severity	When you’ve been devastated by a serious car accident, your focus is on thethings that matter the most: family, friends, and other loved ones. Pushingpaper with your insurance agent is the last place you want your time or mentalenergy spent. This is why Allstate, a personal insurer in the United States,is continually seeking fresh ideas to improve their claims service for theover 16 million households they protect. Allstate is currently developingautomated methods of predicting the cost, and hence severity, of claims. Inthis recruitment challenge, Kagglers are invited to show off their creativityand flex their technical chops by creating an algorithm which accuratelypredicts claims severity. Aspiring competitors will demonstrate insight intobetter ways to predict claims severity for the chance to be part of Allstate’sefforts to ensure a worry-free customer experience. New to Kaggle? Thiscompetition is a recruiting competition, your chance to get a foot in the doorwith the hiring team at Allstate.	Submissions are evaluated on the mean absolute error (MAE) between thepredicted loss and the actual loss. Submission File For every id in the testset, you should predict the loss value. The file should contain a header andhave the following format: id,loss 4,0 6,1 9,99.3 etc.	['regression', 'tabular data', 'mae', 'small']	
avito-demand-prediction	When selling used goods online, a combination of tiny, nuanced details in aproduct description can make a big difference in drumming up interest. Detailslike: And, even with an optimized product listing, demand for a product maysimply not exist–frustrating sellers who may have over-invested in marketing.Avito, Russia’s largest classified advertisements website, is deeply familiarwith this problem. Sellers on their platform sometimes feel frustrated withboth too little demand (indicating something is wrong with the product or theproduct listing) or too much demand (indicating a hot item with a gooddescription was underpriced). In their fourth Kaggle competition, Avito ischallenging you to predict demand for an online advertisement based on itsfull description (title, description, images, etc.), its context(geographically where it was posted, similar ads already posted) andhistorical demand for similar ads in similar contexts. With this information,Avito can inform sellers on how to best optimize their listing and providesome indication of how much interest they should realistically expect toreceive.	Root Mean Squared Error (RMSE) Submissions are scored on the root mean squarederror. RMSE is defined as: RMSE= √ 1 n n ∑ i=1 (yi− ˆ y i)2 , where y hat isthe predicted value and y is the original value. Submission File For eachitem_id in the test set, you must predict a probability for thedeal_probability. Your predictions must be in the range [0, 1]. The submissionfile should contain a header and have the following format:item_id,deal_probability 2,0 5,0 6,0 etc.	['image data', 'text data', 'tabular data']	
bitcoin-historical-data			['finance', 'history', 'medium', 'featured']	"Context Bitcoin is the longest running and most well known cryptocurrency,first released as open source in 2009 by the anonymous Satoshi Nakamoto.Bitcoin serves as a decentralized medium of digital exchange, withtransactions verified and recorded in a public distributed ledger (theblockchain) without the need for a trusted record keeping authority or centralintermediary. Transaction blocks contain a SHA-256 cryptographic hash ofprevious transaction blocks, and are thus ""chained"" together, serving as animmutable record of all transactions that have ever occurred. As with anycurrency/commodity on the market, bitcoin trading and financial instrumentssoon followed public adoption of bitcoin and continue to grow. Included hereis historical bitcoin market data at 1-min intervals for select bitcoinexchanges where trading takes place. Happy (data) mining! ContentcoincheckJPY_1-min_data_2014-10-31_to_2018-06-27.csvbitflyerJPY_1-min_data_2017-07-04_to_2018-06-27.csvcoinbaseUSD_1-min_data_2014-12-01_to_2018-06-27.csvbitstampUSD_1-min_data_2012-01-01_to_2018-06-27.csv CSV files for selectbitcoin exchanges for the time period of Jan 2012 to July 2018, with minute tominute updates of OHLC (Open, High, Low, Close), Volume in BTC and indicatedcurrency, and weighted bitcoin price. Timestamps are in Unix time. Timestampswithout any trades or activity have their data fields forward filled from thelast valid time period. If a timestamp is missing, or if there are jumps, thismay be because the exchange (or its API) was down, the exchange (or its API)did not exist, or some other unforseen technical error in data reporting orgathering. All effort has been made to deduplicate entries and verify thecontents are correct and complete to the best of my ability, but obviouslytrust at your own risk. Acknowledgements and Inspiration Bitcoin charts forthe data. The various exchange APIs, for making it difficult or unintuitiveenough to get OHLC and volume data at 1-min intervals that I set out on thisdata scraping project. Satoshi Nakamoto and the novel core concept of theblockchain, as well as its first execution via the bitcoin protocol. I'd alsolike to thank viewers like you! Can't wait to see what code or insights youall have to share. I am a lowly Ph.D. student who did this for fun in mymeager spare time. If you find this data interesting and you can spare acoffee to fuel my science, send it my way and I'd be immensely grateful!1kmWmcQa8qN9ZrdGfdkw8EHKBgugKBRcF"
BLA - all-crypto-currencies			['finance', 'internet', 'business', 'medium', 'featured']	Cryptocurrency Market Data Historical Cryptocurrency Prices For ALL Tokens!Summary > Observations: 758,534 > Variables: 13 > Crypto Tokens: 1,584 > StartDate: 28/04/2017 > End Date: 21/05/2018 Description All historic open, high,low, close, trading volume and market cap info for all cryptocurrencies. I'vehad to go over the code with a fine tooth comb to get it compatible with CRANso there have been significant enhancements to how some of the fieldconversions have been undertaken and the data being cleaned. This shouldeliminate a few issues around number formatting or unexpected handling ofscientific notations. Data Structure Observations: 649,051 Variables: 13 $slug
bluebook-for-bulldozers	"The goal of the contest is to predict the sale price of a particular piece ofheavy equiment at auction based on it's usage, equipment type, andconfiguaration. The data is sourced from auction result postings and includesinformation on usage and equipment configurations. Fast Iron is creating a""blue book for bull dozers,"" for customers to value what their heavy equipmentfleet is worth at auction. About Fast Iron Fast Iron are a content-focusedbusiness that aids customers in creating enterprise data standards, cleansingdata, and maintaining clean data. Utilizing proprietary applications and anever growing data cleansing team, Fast Iron has normalized data for more than2.5 million machine and customer records for the heavy equipment industry.This competition was launched under the Kaggle Startup Program. If you're astartup with a predictive modelling challenge, please apply! Photo credits:Antonis Lamnatos"	"The evaluation metric for this competition is the RMSLE (root mean squared logerror) between the actual and predicted auction prices. Sample submissionfiles can be downloaded from the data page. Submission files should beformatted as follows: Have a header: ""SalesID,SalePrice"" Contain two columnsSalesID: SalesID for the validation set in sorted order SalePrice: Yourpredicted price of the sale Example lines of the submission format:SalesID,SalePrice 1222837,36205 3044012,74570 1222841,31910.50 ..."	['rmsle', 'small']	
breakdown-of-revenue-by-type-of-customer-series			['economics', 'small']	Content More details about each file are in the individual file descriptions.Context This is a dataset from the U.S. Census Bureau hosted by the FederalReserve Economic Database (FRED). FRED has a data platform found here and theyupdate their information according the amount of data that is brought in.Explore the U.S. Census Bureau using Kaggle and all of the data sourcesavailable through the U.S. Census Bureau organization page! Update Frequency:This dataset is updated daily. Acknowledgements This dataset is maintainedusing FRED's API and Kaggle's API. Cover photo by Rob Bye on Unsplash UnsplashImages are distributed under a unique Unsplash License.
career-con-2019	CareerCon 2019 is upon us! CareerCon is a digital event all about landing yourfirst data science job — and registration is now open! Ahead of the event, wehave a fun competition to get you started. See below for a unique challengeand opportunity to share your resume with select CareerCon sponsors. Read moreabout CareerCon 2019 here. ___________________________________ The CompetitionRobots are smart… by design. To fully understand and properly navigate a task,however, they need input about their environment. In this competition, you’llhelp robots recognize the floor surface they’re standing on using datacollected from Inertial Measurement Units (IMU sensors). We’ve collected IMUsensor data while driving a small mobile robot over different floor surfaceson the university premises. The task is to predict which one of the nine floortypes (carpet, tiles, concrete) the robot is on using sensor data such asacceleration and velocity. Succeed and you'll help improve the navigation ofrobots without assistance across many different surfaces, so they won’t falldown on the job. Special thanks for making this competition possible: The datafor this competition has been collected by Heikki Huttunen and Francesco Lomiofrom the Department of Signal Processing and Damoon Mohamadi, Kaan Celikbilek,Pedram Ghazi and Reza Ghabcheloo from the Department of Automation andMechanical Engineering both from Tampere University, Finland. We at Kagglewould like thank them all for kindly donating the data that has made thiscompetition possible!	Submissions are evaluated on Multiclass Accuracy, which is simply the averagenumber of observations with the correct label. Submission File For eachseries_id in the test set, you must predict a value for the surface variable.The file should have the following format: series_id,surface 0,fine_concrete1,concrete 2,concrete etc.	['tabular data', 'signal processing', 'robotics']	
carvana-image-masking-challenge	As with any big purchase, full information and transparency are key. Whilemost everyone describes buying a used car as frustrating, it’s just asannoying to sell one, especially online. Shoppers want to know everythingabout the car but they must rely on often blurry pictures and littleinformation, keeping used car sales a largely inefficient, local industry.Carvana, a successful online used car startup, has seen opportunity to buildlong term trust with consumers and streamline the online buying process. Aninteresting part of their innovation is a custom rotating photo studio thatautomatically captures and processes 16 standard images of each vehicle intheir inventory. While Carvana takes high quality photos, bright reflectionsand cars with similar colors as the background cause automation errors, whichrequires a skilled photo editor to change. In this competition, you’rechallenged to develop an algorithm that automatically removes the photo studiobackground. This will allow Carvana to superimpose cars on a variety ofbackgrounds. You’ll be analyzing a dataset of photos, covering differentvehicles with a wide variety of year, make, and model combinations.	This competition is evaluated on the mean Dice coefficient. The Dicecoefficient can be used to compare the pixel-wise agreement between apredicted segmentation and its corresponding ground truth. The formula isgiven by: 2∗|X∩Y| |X|+|Y| , where X is the predicted set of pixels and Y isthe ground truth. The Dice coefficient is defined to be 1 when both X and Yare empty. The leaderboard score is the mean of the Dice coefficients for eachimage in the test set. Submission File In order to reduce the submission filesize, our metric uses run-length encoding on the pixel values. Instead ofsubmitting an exhaustive list of indices for your segmentation, you willsubmit pairs of values that contain a start position and a run length. E.g. '13' implies starting at pixel 1 and running a total of 3 pixels (1,2,3). Thecompetition format requires a space delimited list of pairs. For example, '1 310 5' implies pixels 1,2,3,10,11,12,13,14 are to be included in the mask. Themetric checks that the pairs are sorted, positive, and the decoded pixelvalues are not duplicated. The pixels are numbered from top to bottom, thenleft to right: 1 is pixel (1,1), 2 is pixel (2,1), etc. The file shouldcontain a header and have the following format: img,rle_mask 0004d4463b50_01,11 5 1 0004d4463b50_02,1 1 0004d4463b50_03,1 1 etc. Submission files may takeseveral minutes to process due to the size.	['image data', 'object segmentation', 'automobiles']	
cdiscount-image-classification-challenge	Cdiscount.com generated nearly 3 billion euros last year, making it France’slargest non-food e-commerce company. While the company already sellseverything from TVs to trampolines, the list of products is still rapidlygrowing. By the end of this year, Cdiscount.com will have over 30 millionproducts up for sale. This is up from 10 million products only 2 years ago.Ensuring that so many products are well classified is a challenging task.Currently, Cdiscount.com applies machine learning algorithms to the textdescription of the products in order to automatically predict their category.As these methods now seem close to their maximum potential, Cdiscount.combelieves that the next quantitative improvement will be driven by theapplication of data science techniques to images. In this challenge you willbe building a model that automatically classifies the products based on theirimages. As a quick tour of Cdiscount.com's website can confirm, one productcan have one or several images. The data set Cdiscount.com is making availableis unique and characterized by superlative numbers in several ways: Almost 9million products: half of the current catalogue More than 15 million images at180x180 resolution More than 5000 categories: yes this is quite an extrememulti-class classification!	Goal The goal of this competition is to predict the category of a productbased on its image(s). Note that a product can have one or several imagesassociated. For every product _id in the test set, you should predict thecorrect category_id. Metric This competition is evaluated on thecategorization accuracy of your predictions (the percentage of products youget correct). Submission File For each _id in the test set, you must predict acategory_id. The file should contain a header and have the following format:_id,category_id 2,1000000055 5,1000016018 6,1000016055 etc.	['multiclass classification']	
competitive-data-science-predict-future-sales	"This challenge serves as final project for the ""How to win a data sciencecompetition"" Coursera course. In this competition you will work with achallenging time-series dataset consisting of daily sales data, kindlyprovided by one of the largest Russian software firms - 1C Company. We areasking you to predict total sales for every product and store in the nextmonth. By solving this competition you will be able to apply and enhance yourdata science skills."	Submissions are evaluated by root mean squared error (RMSE). True targetvalues are clipped into [0,20] range. Submission File For each id in the testset, you must predict a total number of sales. The file should contain aheader and have the following format: ID,item_cnt_month 0,0.5 1,0.5 2,0.53,0.5 etc.	[]	
costa-rican-household-poverty-prediction	The Inter-American Development Bank is asking the Kaggle community for helpwith income qualification for some of the world's poorest families. Are you upfor the challenge? Here's the backstory: Many social programs have a hard timemaking sure the right people are given enough aid. It’s especially tricky whena program focuses on the poorest segment of the population. The world’spoorest typically can’t provide the necessary income and expense records toprove that they qualify. In Latin America, one popular method uses analgorithm to verify income qualification. It’s called the Proxy Means Test (orPMT). With PMT, agencies use a model that considers a family’s observablehousehold attributes like the material of their walls and ceiling, or theassets found in the home to classify them and predict their level of need.While this is an improvement, accuracy remains a problem as the region’spopulation grows and poverty declines. To improve on PMT, the IDB (the largestsource of development financing for Latin America and the Caribbean) hasturned to the Kaggle community. They believe that new methods beyondtraditional econometrics, based on a dataset of Costa Rican householdcharacteristics, might help improve PMT’s performance. Beyond Costa Rica, manycountries face this same problem of inaccurately assessing social need. IfKagglers can generate an improvement, the new algorithm could be implementedin other countries around the world. This is a Kernels-Only Competition, soyou must submit your code through Kernels, rather than uploading .csvpredictions. You can create private Kernels and even share/edit your work withteammates by adding them as collaborators.	Submissions will be evaluated based on their macro F1 score. KernelSubmissions As this is a Kernels-Only Competition, you must make submissionsdirectly from Kaggle Kernels. By adding your teammates as collaborators on akernel, you can share and edit code privately with them. Submission File Foreach Id in the test set, you must predict a class for the Target variable asdescribed in the data page. The file should contain a header and have thefollowing format: Id,Target ID_2f6873615,1	['multiclass classification', 'tabular data']	
country-policy-and-institutional-assessment			['world', 'small']	Content Rating of countries against a set of 16 criteria grouped in fourclusters: economic management, structural policies, policies for socialinclusion and equity, and public sector management and institutions. ContextThis is a dataset hosted by the World Bank. The organization has an open dataplatform found here and they update their information according the amount ofdata that is brought in. Explore the World Bank using Kaggle and all of thedata sources available through the World Bank organization page! UpdateFrequency: This dataset is updated daily. Acknowledgements This dataset ismaintained using the World Bank's APIs and Kaggle's API. Cover photo by Danielvon Appen on Unsplash Unsplash Images are distributed under a unique UnsplashLicense.
coupon-purchase-prediction	Recruit Ponpare is Japan's leading joint coupon site, offering huge discountson everything from hot yoga, to gourmet sushi, to a summer concert bonanza.Ponpare's coupons open doors for customers they've only dreamed of steppingthrough. They can learn difficult to acquire skills, go on unheard ofadventures, and dine like (and with) the stars. Investing in a new experienceis not cheap. We fear wasting our time and money on a product or service thatwe may not enjoy or fully understand. Ponpare takes the high price out of thisequation, making it easier for you to take the leap towards your first sky-dive or diamond engagement ring. Using past purchase and browsing behavior,this competition asks you to predict which coupons a customer will buy in agiven period of time. The resulting models will be used to improve Ponpare'srecommendation system, so they can make sure their customers don't miss out ontheir next favorite thing.	Submissions are evaluated according to the Mean Average Precision @ 10(MAP@10): MAP@10= 1 |U| |U| ∑ u=1 1 min(m,10) min(n,10) ∑ k=1 P(k) where |U|is the number of users, P(k) is the precision at cutoff k, n is the number ofpredicted coupons, and m is the number of purchased coupons for the givenuser. If m = 0, the precision is defined to be 0. Submission File For everyuser, you must predict a space-delimited list of the coupons they purchased.The file should contain a header and have the following format (we havesubstituted the coupon hashes with dummy values to fit below, but in yourprediction file you should use the real hash values):USER_ID_hash,PURCHASED_COUPONS 0004901ba699a49fd93a3c6bb1768b8f,hash40006d6ac7c6ef3fc0ab0dc40deb3c960,hash1 hash2 00078d03b4dda619293c1793c251f783,etc...	['multiclass classification', 'tabular data', 'marketing', 'map@{k}', 'small']	
creditcardfraud			['finance', 'crime', 'medium', 'featured']	Context It is important that credit card companies are able to recognizefraudulent credit card transactions so that customers are not charged foritems that they did not purchase. Content The datasets contains transactionsmade by credit cards in September 2013 by european cardholders. This datasetpresents transactions that occurred in two days, where we have 492 frauds outof 284,807 transactions. The dataset is highly unbalanced, the positive class(frauds) account for 0.172% of all transactions. It contains only numericalinput variables which are the result of a PCA transformation. Unfortunately,due to confidentiality issues, we cannot provide the original features andmore background information about the data. Features V1, V2, ... V28 are theprincipal components obtained with PCA, the only features which have not beentransformed with PCA are 'Time' and 'Amount'. Feature 'Time' contains theseconds elapsed between each transaction and the first transaction in thedataset. The feature 'Amount' is the transaction Amount, this feature can beused for example-dependant cost-senstive learning. Feature 'Class' is theresponse variable and it takes value 1 in case of fraud and 0 otherwise.Inspiration Identify fraudulent credit card transactions. Given the classimbalance ratio, we recommend measuring the accuracy using the Area Under thePrecision-Recall Curve (AUPRC). Confusion matrix accuracy is not meaningfulfor unbalanced classification. Acknowledgements The dataset has been collectedand analysed during a research collaboration of Worldline and the MachineLearning Group (http://mlg.ulb.ac.be) of ULB (Université Libre de Bruxelles)on big data mining and fraud detection. More details on current and pastprojects on related topics are available on http://mlg.ulb.ac.be/BruFence andhttp://mlg.ulb.ac.be/ARTML Please cite: Andrea Dal Pozzolo, Olivier Caelen,Reid A. Johnson and Gianluca Bontempi. Calibrating Probability withUndersampling for Unbalanced Classification. In Symposium on ComputationalIntelligence and Data Mining (CIDM), IEEE, 2015
cryptocurrencypricehistory			['finance', 'history', 'small']	Context Things like Block chain, Bitcoin, Bitcoin cash, Ethereum, Ripple etcare constantly coming in the news articles I read. So I wanted to understandmore about it and this post helped me get started. Once the basics are done,the data scientist inside me started raising questions like: How manycryptocurrencies are there and what are their prices and valuations? Why isthere a sudden surge in the interest in recent days? For getting answers toall these questions (and if possible to predict the future prices ;)), Istarted collecting data from coinmarketcap about the cryptocurrencies. So whatnext? Now that we have the price data, I wanted to dig a little more about thefactors affecting the price of coins. I started of with Bitcoin and there arequite a few parameters which affect the price of Bitcoin. Thanks to BlockchainInfo, I was able to get quite a few parameters on once in two day basis. Thiswill help understand the other factors related to Bitcoin price and also helpone make future predictions in a better way than just using the historicalprice. Content The dataset has one csv file for each currency. Price historyis available on a daily basis from April 28, 2013. This dataset has thehistorical price information of some of the top crypto currencies by marketcapitalization. The currencies included are: Bitcoin Ethereum Ripple Bitcoincash Bitconnect Dash Ethereum Classic Iota Litecoin Monero Nem Neo NumeraireStratis Waves Date : date of observation Open : Opening price on the given dayHigh : Highest price on the given day Low : Lowest price on the given dayClose : Closing price on the given day Volume : Volume of transactions on thegiven day Market Cap : Market capitalization in USD Bitcoin Dataset(bitcoin_dataset.csv) : This dataset has the following features. Date : Dateof observation btc_market_price : Average USD market price across majorbitcoin exchanges. btc_total_bitcoins : The total number of bitcoins that havealready been mined. btc_market_cap : The total USD value of bitcoin supply incirculation. btc_trade_volume : The total USD value of trading volume on majorbitcoin exchanges. btc_blocks_size : The total size of all block headers andtransactions. btc_avg_block_size : The average block size in MB.btc_n_orphaned_blocks : The total number of blocks mined but ultimately notattached to the main Bitcoin blockchain. btc_n_transactions_per_block : Theaverage number of transactions per block. btc_median_confirmation_time : Themedian time for a transaction to be accepted into a mined block. btc_hash_rate: The estimated number of tera hashes per second the Bitcoin network isperforming. btc_difficulty : A relative measure of how difficult it is to finda new block. btc_miners_revenue : Total value of coinbase block rewards andtransaction fees paid to miners. btc_transaction_fees : The total value of alltransaction fees paid to miners. btc_cost_per_transaction_percent : minersrevenue as percentage of the transaction volume. btc_cost_per_transaction :miners revenue divided by the number of transactions. btc_n_unique_addresses :The total number of unique addresses used on the Bitcoin blockchain.btc_n_transactions : The number of daily confirmed Bitcoin transactions.btc_n_transactions_total : Total number of transactions.btc_n_transactions_excluding_popular : The total number of Bitcointransactions, excluding the 100 most popular addresses.btc_n_transactions_excluding_chains_longer_than_100 : The total number ofBitcoin transactions per day excluding long transaction chains.btc_output_volume : The total value of all transaction outputs per day.btc_estimated_transaction_volume : The total estimated value of transactionson the Bitcoin blockchain. btc_estimated_transaction_volume_usd : Theestimated transaction value in USD value. Ethereum Dataset(ethereum_dataset.csv): This dataset has the following features Date(UTC) :Date of transaction UnixTimeStamp : unix timestamp eth_etherprice : price ofethereum eth_tx : number of transactions per day eth_address : Cumulativeaddress growth eth_supply : Number of ethers in supply eth_marketcap : Marketcap in USD eth_hashrate : hash rate in GH/s eth_difficulty : Difficulty levelin TH eth_blocks : number of blocks per day eth_uncles : number of uncles perday eth_blocksize : average block size in bytes eth_blocktime : average blocktime in seconds eth_gasprice : Average gas price in Wei eth_gaslimit : Gaslimit per day eth_gasused : total gas used per day eth_ethersupply : new ethersupply per day eth_chaindatasize : chain data size in bytes eth_ens_register :Ethereal Name Service (ENS) registrations per day Acknowledgements This datais taken from coinmarketcap and it is free to use the data. Bitcoin dataset isobtained from Blockchain Info. Ethereum dataset is obtained from Etherscan.Cover Image : Photo by Thomas Malama on Unsplash Inspiration Some of thequestions which could be inferred from this dataset are: How did thehistorical prices / market capitalizations of various currencies change overtime? Predicting the future price of the currencies Which currencies are morevolatile and which ones are more stable? How does the price fluctuations ofcurrencies correlate with each other? Seasonal trend in the price fluctuationsBitcoin / Ethereum dataset could be used to look at the following: Factorsaffecting the bitcoin / ether price. Directional prediction of bitcoin / etherprice. (refer this paper for more inspiration) Actual bitcoin priceprediction.
data-on-statistical-capacity			['world', 'small']	Content The Data on Statistical Capacity website provides information onvarious aspects of national statistical systems of developing countries,including a country-level statistical capacity indicator. Context This is adataset hosted by the World Bank. The organization has an open data platformfound here and they update their information according the amount of data thatis brought in. Explore the World Bank using Kaggle and all of the data sourcesavailable through the World Bank organization page! Update Frequency: Thisdataset is updated daily. Acknowledgements This dataset is maintained usingthe World Bank's APIs and Kaggle's API. Cover photo by Etienne Gobeli onUnsplash Unsplash Images are distributed under a unique Unsplash License.
data-science-bowl-2017	In the United States, lung cancer strikes 225,000 people every year, andaccounts for $12 billion in health care costs. Early detection is critical togive patients the best chance at recovery and survival. One year ago, theoffice of the U.S. Vice President spearheaded a bold new initiative, theCancer Moonshot, to make a decade's worth of progress in cancer prevention,diagnosis, and treatment in just 5 years. In 2017, the Data Science Bowl willbe a critical milestone in support of the Cancer Moonshot by convening thedata science and medical communities to develop lung cancer detectionalgorithms. Using a data set of thousands of high-resolution lung scansprovided by the National Cancer Institute, participants will developalgorithms that accurately determine when lesions in the lungs are cancerous.This will dramatically reduce the false positive rate that plagues the currentdetection technology, get patients earlier access to life-savinginterventions, and give radiologists more time to spend with their patients.This year, the Data Science Bowl will award $1 million in prizes to those whoobserve the right patterns, ask the right questions, and in turn, createunprecedented impact around cancer screening care and prevention. The fundsfor the prize purse will be provided by the Laura and John Arnold Foundation.Visit DataScienceBowl.com to: • Sign up to receive news about the competition• Learn about the history of the Data Science Bowl and past competitions •Read our latest insights on emerging analytics techniques Acknowledgments TheData Science Bowl is presented by Competition Sponsors Laura and John ArnoldFoundation Cancer Imaging Program of the National Cancer Institute AmericanCollege of Radiology Amazon Web Services NVIDIA Data Support ProvidersNational Lung Screening Trial The Cancer Imaging Archive Diagnostic ImageAnalysis Group, Radboud University Lahey Hospital & Medical Center CopenhagenUniversity Hospital Supporting Organizations Bayes Impact Black Data ProcessngAssociates Code the Change Data Community DC DataKind Galvanize Great Minds inSTEM Hortonworks INFORMS Lesbians Who Tech NSBE Society of Asian Scientists &Engineers Society of Women Engineers University of Texas Austin, BusinessAnalytics Program, McCombs School of Business US Dept. of Health and HumanServices US Food and Drug Administration Women in Technology Women ofCyberjutsu	Submissions are scored on the log loss: LogLoss=− 1 n ∑ i=1 n [ y i log( y ^ i)+(1− y i )log(1− y ^ i )], where n is the number of patients in the test sety ^ i y is the predicted probability of the image belonging to a patient withcancer y i y is 1 if the diagnosis is cancer, 0 otherwise log() l is thenatural (base e) logarithm Note: the actual submitted predicted probabilitiesare replaced with max(min(p,1− 10 −15 ), 10 −15 ) m . A smaller log loss isbetter. Submission File For each patient id in the test set, you must submit aprobability. The file should have a header and be in the following format:id,cancer 01e349d34c02410e1da273add27be25c,0.505a20caf6ab6df4643644c923f06a5eb,0.5 0d12f1c627df49eb223771c28548350e,0.5 ...	['image data', 'healthcare', 'binary classification']	
data-science-bowl-2018	Spot Nuclei. Speed Cures. Imagine speeding up research for almost everydisease, from lung cancer and heart disease to rare disorders. The 2018 DataScience Bowl offers our most ambitious mission yet: create an algorithm toautomate nucleus detection. We’ve all seen people suffer from diseases likecancer, heart disease, chronic obstructive pulmonary disease, Alzheimer’s, anddiabetes. Many have seen their loved ones pass away. Think how many liveswould be transformed if cures came faster. By automating nucleus detection,you could help unlock cures faster—from rare disorders to the common cold.Want a snapshot about the 2018 Data Science Bowl? View this video. Why nuclei?Identifying the cells’ nuclei is the starting point for most analyses becausemost of the human body’s 30 trillion cells contain a nucleus full of DNA, thegenetic code that programs each cell. Identifying nuclei allows researchers toidentify each individual cell in a sample, and by measuring how cells react tovarious treatments, the researcher can understand the underlying biologicalprocesses at work. By participating, teams will work to automate the processof identifying nuclei, which will allow for more efficient drug testing,shortening the 10 years it takes for each new drug to come to market. Checkout this video overview to find out more. What will participants do? Teamswill create a computer model that can identify a range of nuclei across variedconditions. By observing patterns, asking questions, and building a model,participants will have a chance to push state-of-the-art technology farther.Visit DataScienceBowl.com to: • Sign up to receive news about the competition• Learn about the history of the Data Science Bowl and past competitions •Read our latest insights on emerging analytics techniques	"This competition is evaluated on the mean average precision at differentintersection over union (IoU) thresholds. The IoU of a proposed set of objectpixels and a set of true object pixels is calculated as: IoU(A,B)= A∩B A∪B .The metric sweeps over a range of IoU thresholds, at each point calculating anaverage precision value. The threshold values range from 0.5 to 0.95 with astep size of 0.05: (0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95). Inother words, at a threshold of 0.5, a predicted object is considered a ""hit""if its intersection over union with a ground truth object is greater than 0.5.At each threshold value t t , a precision value is calculated based on thenumber of true positives (TP), false negatives (FN), and false positives (FP)resulting from comparing the predicted object to all ground truth objects:TP(t) TP(t)+FP(t)+FN(t) . TP(t)TP(t)+FP(t)+FN(t). A true positive is countedwhen a single predicted object matches a ground truth object with an IoU abovethe threshold. A false positive indicates a predicted object had no associatedground truth object. A false negative indicates a ground truth object had noassociated predicted object. The average precision of a single image is thencalculated as the mean of the above precision values at each IoU threshold: 1|thresholds| ∑ t TP(t) TP(t)+FP(t)+FN(t) . Lastly, the score returned by thecompetition metric is the mean taken over the individual average precisions ofeach image in the test dataset. Submission File In order to reduce thesubmission file size, our metric uses run-length encoding on the pixel values.Instead of submitting an exhaustive list of indices for your segmentation, youwill submit pairs of values that contain a start position and a run length.E.g. '1 3' implies starting at pixel 1 and running a total of 3 pixels(1,2,3). The competition format requires a space delimited list of pairs. Forexample, '1 3 10 5' implies pixels 1,2,3,10,11,12,13,14 are to be included inthe mask. The pixels are one-indexed and numbered from top to bottom, thenleft to right: 1 is pixel (1,1), 2 is pixel (2,1), etc. The metric checks thatthe pairs are sorted, positive, and the decoded pixel values are notduplicated. It also checks that no two predicted masks for the same image areoverlapping. The file should contain a header and have the following format.Each row in your submission represents a single predicted nucleus segmentationfor the given ImageId. ImageId,EncodedPixels0114f484a16c152baa2d82fdd43740880a762c93f436c8988ac461c5c9dbe7d5,1 10999dab07b11bc85fb8464fc36c947fbd8b5d6ec49817361cb780659ca805eac,1 10999dab07b11bc85fb8464fc36c947fbd8b5d6ec49817361cb780659ca805eac,2 3 8 9etc... Submission files may take several minutes to process due to the size."	['biology']	
data-science-for-good-careervillage	Welcome In this competition you'll notice there isn't a leaderboard, and youare not required to develop a predictive model. This isn't a traditionalsupervised Kaggle machine learning competition. CareerVillage.org is anonprofit that crowdsources career advice for underserved youth. Founded in2011 in four classrooms in New York City, the platform has now served careeradvice from 25,000 volunteer professionals to over 3.5M online learners. Theplatform uses a Q&A; style similar to StackOverflow or Quora to providestudents with answers to any question about any career. In this Data Sciencefor Good challenge, CareerVillage.org, in partnership with Google.org, isinviting you to help recommend questions to appropriate volunteers. To supportthis challenge, CareerVillage.org has supplied five years of data. ProblemStatement The U.S. has almost 500 students for every guidance counselor.Underserved youth lack the network to find their career role models, makingCareerVillage.org the only option for millions of young people in America andaround the globe with nowhere else to turn. To date, 25,000 volunteers havecreated profiles and opted in to receive emails when a career question is agood fit for them. This is where your skills come in. To help students get theadvice they need, the team at CareerVillage.org needs to be able to send theright questions to the right volunteers. The notifications sent to volunteersseem to have the greatest impact on how many questions are answered. Yourobjective: develop a method to recommend relevant questions to theprofessionals who are most likely to answer them. Criteria for MeasuringSolutions Performance: How well does the solution match professionals to thequestions they would be motivated to answer? CareerVillage.org will not beable to live-test every submission, so a strong entry will clearly articulatewhy it will be effective at motivating answers. Easy to implement: TheCareerVillage.org team wants to put the winning submissions to work, quickly.A good entry will be well documented and easy to test in production.Extensibility: In the future, CareerVillage.org aims to add more data featuresand to accommodate new objectives. Winning submissions should allow for thisand other augmentations to be added in the future.	Evaluation Your objective is to develop a method for recommending careeradvice questions to relevant volunteers that are most likely to answer them. Avalid submission will include: Kernel Notebook: At least one kernel containingyour method that recommends questions to volunteers. All kernels submittedmust be made public on or before the submission deadline to be eligible. Ifsubmitting as a team, all team members must be listed as collaborators on allkernels submitted. Criteria for Measuring Solutions Performance: How well doesthe solution match questions to volunteers that would be most likely toanswer? CareerVillage.org will not be able to live-test every submission, so astrong entry will clearly articulate why it is effective. Easy to implement:The CareerVillage.org team wants to put the winning submissions to workquickly. A good entry will be well documented and easy to test in production.Extensibility: In the future, CareerVillage.org aims to add more datafeatures. The best submissions will make it easy to add new features and datanot presently available.	['education', 'children']	
default-of-credit-card-clients-dataset			['finance', 'small', 'featured']	Dataset Information This dataset contains information on default payments,demographic factors, credit data, history of payment, and bill statements ofcredit card clients in Taiwan from April 2005 to September 2005. Content Thereare 25 variables: ID: ID of each client LIMIT_BAL: Amount of given credit inNT dollars (includes individual and family/supplementary credit SEX: Gender(1=male, 2=female) EDUCATION: (1=graduate school, 2=university, 3=high school,4=others, 5=unknown, 6=unknown) MARRIAGE: Marital status (1=married, 2=single,3=others) AGE: Age in years PAY_0: Repayment status in September, 2005 (-1=payduly, 1=payment delay for one month, 2=payment delay for two months, ...8=payment delay for eight months, 9=payment delay for nine months and above)PAY_2: Repayment status in August, 2005 (scale same as above) PAY_3: Repaymentstatus in July, 2005 (scale same as above) PAY_4: Repayment status in June,2005 (scale same as above) PAY_5: Repayment status in May, 2005 (scale same asabove) PAY_6: Repayment status in April, 2005 (scale same as above) BILL_AMT1:Amount of bill statement in September, 2005 (NT dollar) BILL_AMT2: Amount ofbill statement in August, 2005 (NT dollar) BILL_AMT3: Amount of bill statementin July, 2005 (NT dollar) BILL_AMT4: Amount of bill statement in June, 2005(NT dollar) BILL_AMT5: Amount of bill statement in May, 2005 (NT dollar)BILL_AMT6: Amount of bill statement in April, 2005 (NT dollar) PAY_AMT1:Amount of previous payment in September, 2005 (NT dollar) PAY_AMT2: Amount ofprevious payment in August, 2005 (NT dollar) PAY_AMT3: Amount of previouspayment in July, 2005 (NT dollar) PAY_AMT4: Amount of previous payment inJune, 2005 (NT dollar) PAY_AMT5: Amount of previous payment in May, 2005 (NTdollar) PAY_AMT6: Amount of previous payment in April, 2005 (NT dollar)default.payment.next.month: Default payment (1=yes, 0=no) Inspiration Someideas for exploration: How does the probability of default payment vary bycategories of different demographic variables? Which variables are thestrongest predictors of default payment? Acknowledgements Any publicationsbased on this dataset should acknowledge the following: Lichman, M. (2013).UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA:University of California, School of Information and Computer Science. Theoriginal dataset can be found here at the UCI Machine Learning Repository.
demand-forecasting-kernels-only	This competition is provided as a way to explore different time seriestechniques on a relatively simple and clean dataset. You are given 5 years ofstore-item sales data, and asked to predict 3 months of sales for 50 differentitems at 10 different stores. What's the best way to deal with seasonality?Should stores be modeled separately, or can you pool them together? Does deeplearning work better than ARIMA? Can either beat xgboost? This is a greatcompetition to explore different models and improve your skills inforecasting.	Submissions are evaluated on SMAPE between forecasts and actual values. Wedefine SMAPE = 0 when the actual and predicted values are both 0. KernelSubmissions You can only make submissions directly from Kaggle Kernels. Byadding your teammates as collaborators on a kernel, you can share and editcode privately with them. Submission File For each id in the test set, youmust predict a probability for the sales variable. The file should contain aheader and have the following format: id,sales 0,35 1,22 2,5 etc.	['time series', 'tabular data']	
demonetization-in-india-twitter-data			['finance', 'internet', 'twitter', 'human-computer interaction', 'small', 'featured']	"Context The demonetization of ₹500 and ₹1000 banknotes was a step taken by theGovernment of India on 8 November 2016, ceasing the usage of all ₹500 and₹1000 banknotes of the Mahatma Gandhi Series as a form of legal tender inIndia from 9 November 2016. The announcement was made by the Prime Minister ofIndia Narendra Modi in an unscheduled live televised address to the nation at20:15 Indian Standard Time (IST) the same day. In the announcement, Modideclared circulation of all ₹500 and ₹1000 banknotes of the Mahatma GandhiSeries as invalid and announced the issuance of new ₹500 and ₹2000 banknotesof the Mahatma Gandhi New Series in exchange for the old banknotes. ContentThe data contains 6000 most recent tweets on #demonetization. There are 6000rows(one for each tweet) and 14 columns. Metadata: Text (Tweets) favoritedfavoriteCount replyToSN created truncated replyToSID id replyToUIDstatusSource screenName retweetCount isRetweet retweeted Acknowledgement Thedata was collected using the ""twitteR"" package in R using the twitter API.Past Research I have performed my own analysis on the data. I only did asentiment analysis and formed a word cloud. Click here to see the analysis onGitHub Inspiration What percentage of tweets are negative, positive or neutral? What are the most famous/re-tweeted tweets ?"
diamonds			['finance', 'clothing', 'small', 'featured']	Context This classic dataset contains the prices and other attributes ofalmost 54,000 diamonds. It's a great dataset for beginners learning to workwith data analysis and visualization. Content price price in US dollars(\$326--\$18,823) carat weight of the diamond (0.2--5.01) cut quality of thecut (Fair, Good, Very Good, Premium, Ideal) color diamond colour, from J(worst) to D (best) clarity a measurement of how clear the diamond is (I1(worst), SI2, SI1, VS2, VS1, VVS2, VVS1, IF (best)) x length in mm (0--10.74)y width in mm (0--58.9) z depth in mm (0--31.8) depth total depth percentage =z / mean(x, y) = 2 * z / (x + y) (43--79) table width of top of diamondrelative to widest point (43--95)
digit-recognizer	"Start here if... You have some experience with R or Python and machinelearning basics, but you’re new to computer vision. This competition is theperfect introduction to techniques like neural networks using a classicdataset including pre-extracted features. Competition Description MNIST(""Modified National Institute of Standards and Technology"") is the de facto“hello world” dataset of computer vision. Since its release in 1999, thisclassic dataset of handwritten images has served as the basis for benchmarkingclassification algorithms. As new machine learning techniques emerge, MNISTremains a reliable resource for researchers and learners alike. In thiscompetition, your goal is to correctly identify digits from a dataset of tensof thousands of handwritten images. We’ve curated a set of tutorial-stylekernels which cover everything from regression to neural networks. Weencourage you to experiment with different algorithms to learn first-hand whatworks well and how techniques compare. Practice Skills Computer visionfundamentals including simple neural networks Classification methods such asSVM and K-nearest neighbors Acknowledgements More details about the dataset,including algorithms that have been tried on it and their levels of success,can be found at http://yann.lecun.com/exdb/mnist/index.html. The dataset ismade available under a Creative Commons Attribution-Share Alike 3.0 license."	Goal The goal in this competition is to take an image of a handwritten singledigit, and determine what that digit is. For every in the test set, you shouldpredict the correct label. Metric This competition is evaluated on thecategorization accuracy of your predictions (the percentage of images you getcorrect). Submission File Format The file should contain a header and have thefollowing format: ImageId,Label 1,0 2,0 3,0 etc.	['image data', 'multiclass classification', 'tabular data', 'object identification']	
dog-breed-identification	Who's a good dog? Who likes ear scratches? Well, it seems those fancy deepneural networks don't have all the answers. However, maybe they can answerthat ubiquitous question we all ask when meeting a four-legged stranger: whatkind of good pup is that? In this playground competition, you are provided astrictly canine subset of ImageNet in order to practice fine-grained imagecategorization. How well you can tell your Norfolk Terriers from your NorwichTerriers? With 120 breeds of dogs and a limited number training images perclass, you might find the problem more, err, ruff than you anticipated.Acknowledgments We extend our gratitude to the creators of the Stanford DogsDataset for making this competition possible: Aditya Khosla, NityanandaJayadevaprakash, Bangpeng Yao, and Fei-Fei Li.	Submissions are evaluated on Multi Class Log Loss between the predictedprobability and the observed target. Submission File For each image in thetest set, you must predict a probability for each of the different breeds. Thefile should contain a header and have the following format:id,affenpinscher,afghan_hound,..,yorkshire_terrier000621fb3cbb32d8935728e48679680e,0.0083,0.0,...,0.0083 etc.	['image data', 'multiclass classification', 'animals', 'object identification']	
dogs-vs-cats-redux-kernels-edition	In 2013, we hosted one of our favorite for-fun competitions: Dogs vs. Cats.Much has since changed in the machine learning landscape, particularly in deeplearning and image analysis. Back then, a tensor flow was the diffusion of thecreamer in a bored mathematician's cup of coffee. Now, even the cucumberfarmers are neural netting their way to a bounty. Much has changed at Kaggleas well. Our online coding environment Kernels didn't exist in 2013, and so itwas that we approached sharing by scratching primitive glpyhs on cave wallswith sticks and sharp objects. No more. Now, Kernels have taken over as theway to share code on Kaggle. IPython is out and Jupyter Notebook is in. Weeven have TensorFlow. What more could a data scientist ask for? But seriously,what more? Pull requests welcome. We are excited to bring back the infamousDogs vs. Cats classification problem as a playground competition with kernelsenabled. Although modern techniques may make light of this once-difficultproblem, it is through practice of new techniques on old datasets that we willmake light of machine learning's future challenges.	Submissions are scored on the log loss: LogLoss=− 1 n n ∑ i=1 [yilog( ˆ yi)+(1−yi)log(1− ˆ y i)], where n is the number of images in the test set ˆ y iis the predicted probability of the image being a dog yi is 1 if the image isa dog, 0 if cat log() is the natural (base e) logarithm Submission File Foreach image in the test set, you must submit a probability that image is a dog.The file should have a header and be in the following format: id,label 1,0.52,0.5 3,0.5 ...	['image data', 'binary classification', 'animals', 'object identification']	
donorschoose-application-screening	Founded in 2000 by a high school teacher in the Bronx, DonorsChoose.orgempowers public school teachers from across the country to request much-neededmaterials and experiences for their students. At any given time, there arethousands of classroom requests that can be brought to life with a gift of anyamount. DonorsChoose.org receives hundreds of thousands of project proposalseach year for classroom projects in need of funding. Right now, a large numberof volunteers is needed to manually screen each submission before it'sapproved to be posted on the DonorsChoose.org website. Next year,DonorsChoose.org expects to receive close to 500,000 project proposals. As aresult, there are three main problems they need to solve: How to scale currentmanual processes and resources to screen 500,000 projects so that they can beposted as quickly and as efficiently as possible How to increase theconsistency of project vetting across different volunteers to improve theexperience for teachers How to focus volunteer time on the applications thatneed the most assistance The goal of the competition is to predict whether ornot a DonorsChoose.org project proposal submitted by a teacher will beapproved, using the text of project descriptions as well as additionalmetadata about the project, teacher, and school. DonorsChoose.org can then usethis information to identify projects most likely to need further reviewbefore approval. With an algorithm to pre-screen applications,DonorsChoose.org can auto-approve some applications quickly so that volunteerscan spend their time on more nuanced and detailed project vetting processes,including doing more to help teachers develop projects that qualify forspecific funding opportunities. Your machine learning algorithm can help moreteachers get funded more quickly, and with less cost to DonorsChoose.org,allowing them to channel even more funding directly to classrooms across thecountry. Getting Started with Kernels Get familiar with the competition dataand the machine learning objective quickly using Kernels. Google's engineeringeducation team has put together a starter tutorial implementing benchmarklinear classification model. Acknowledgments Machine Learning Crash Course wascreated by Google's engineering education team in partnership with numerousMachine Learning subject matter experts across Google.	The goal of this competition is to predict whether an application toDonorsChoose is accepted. Submissions are evaluated on area under the ROCcurve between the predicted probability and the observed target. SubmissionFile For each id in the test set, you must predict a probability for theproject_is_approved variable. The file should contain a header and have thefollowing format (order does not matter): id,project_is_approved p233245,0.84p096795,0.84 p236235,0.84 etc.	['binary classification', 'crowdfunding']	
dont-call-me-turkey	Hungry for a new competition? Give thanks for this opportunity to avoid thoseawkward family political dinner discussions and endless holiday moviemarathons over the Thanksgiving break. Spend time with your Kaggle familyinstead to find the real turkey! In this competition you are tasked withfinding the turkey sound signature from pre-extracted audio features. A simplebinary problem, or is it? What does a turkey really sound like? How manysounds are similar? Will you be able to find the turkey or will you go a-fowl?This is a short, fun, holiday, playground competition. Please, do not ruin thefun for yourself and for everyone by using a model trained on the answers.Don't be a turkey!	Submissions are evaluated on area under the ROC curve between the predictedprobability and the observed target. Submission File For each ID in the testset, you must predict a probability for the TARGET variable. The file shouldcontain a header and have the following format: vid_id,is_turkey pyKh38FXD3E,0THhP1idrWXA,0.3 etc.	['binary classification', 'tabular data', 'animals', 'sound technology']	
dont-overfit-ii	Long ago, in the distant, fragrant mists of time, there was a competition...It was not just any competition. It was a competition that challenged meremortals to model a 20,000x200 matrix of continuous variables using only 250training samples... without overfitting. Data scientists ― including Kaggle'svery own Will Cukierski ― competed by the hundreds. Legends were made. (Willtook 5th place, and eventually ended up working at Kaggle!) People overfitlike crazy. It was a Kaggle-y, data science-y madhouse. So... we're doing itagain. Don't Overfit II: The Overfittening This is the next logical step inthe evolution of weird competitions. Once again we have 20,000 rows ofcontinuous variables, and a mere handful of training samples. Once again, wechallenge you not to overfit. Do your best, model without overfitting, andadd, perhaps, to your own legend. In addition to bragging rights, the winneralso gets swag. Enjoy! Acknowledgments We hereby salute the hard work thatwent into the original competition, created by Phil Brierly. Thank you!	Submissions are evaluated using AUCROC between the predicted target and theactual target value. Submission File For each id in the test set, you mustpredict a probability for the target variable. The file should contain aheader and have the following format: id,target 300,0 301,0 302,0 303,0 304,0305,0 306,0 307,0 308,0	['binary classification', 'tabular data']	
dstl-satellite-imagery-feature-detection	The proliferation of satellite imagery has given us a radically improvedunderstanding of our planet. It has enabled us to better achieve everythingfrom mobilizing resources during disasters to monitoring effects of globalwarming. What is often taken for granted is that advancements such as thesehave relied on labeling features of significance like building footprints androadways fully by hand or through imperfect semi-automated methods. As theselarge, complex datasets continue to increase exponentially in number, theDefence Science and Technology Laboratory (Dstl) is seeking novel solutions toalleviate the burden on their image analysts. In this competition, Kagglersare challenged to accurately classify features in overhead imagery. Automatingfeature labeling will not only help Dstl make smart decisions more quicklyaround the defense and security of the UK, but also bring innovation tocomputer vision methodologies applied to satellite imagery.	"Submissions are evaluated on Average Jaccard Index between the predictedmultipolygons and the actual multipolygons. This is a vector-based metricwhere we use polygon geometries to evaluate how well your predictions arealigned with the answer. The Jaccard Index for two regions A and B, also knownas the ""intersection over union"", is defined as: Jaccard= TP TP+FP+FN = |A∩B||A∪B| = |A∩B| |A|+|B|−|A∩B| where TP is the true positives area, FP is thefalse positives area, and FN is the false negatives area. For each objectclass, of each image, we calculate the TP, FP, and FN areas. We then sum thetotal TP, total FP, and total FN across all the images, then the Jaccard iscalculated for that class using total TP, total FP, and total FN. Then, weaverage all the Jaccard Indexes for all the 10 classes. Submission File Forevery row in the dataset, submission files should contain the following:ImageId (string), ClassType (int), and MultipolygonWKT (multipolygoncoordinates in WKT format). You must submit a row for every ImageId and everyClassType. The complete format is in the sample_submission.csv file. If youwant to predict an empty polygon for the class, put down MULTIPOLYGON EMPTY.Please submit all the rows in exactly the same order as sample_submission.csv,otherwise you might get a ""key not found error"". The file should contain aheader and have the following format: ImageId,ClassType,MultipolygonWKT6020_0_1, 1, MULTIPOLYGON EMPTY 6120_2_4,1,""POLYGON ((0 0, 0.009188 0,0.009188 -0.009039999999999999, 0 -0.009039999999999999, 0 0))"" etc.Submission Time If you have complex polygons, it could take a longer time (4+mins) to evaluate your submission. Please be patient when the submission isgetting evaluated! If you find yourself getting timed out, please considersimplifying (smoothing) your polygons to reduce both the submission file sizeand the time it takes to evaluate the submission."	['image data', 'multiclass classification', 'object segmentation']	
education-statistics			['world', 'education', 'utility', 'countries', 'medium', 'featured']	Content The World Bank EdStats All Indicator Query holds over 4,000internationally comparable indicators that describe education access,progression, completion, literacy, teachers, population, and expenditures. Theindicators cover the education cycle from pre-primary to vocational andtertiary education. The query also holds learning outcome data frominternational and regional learning assessments (e.g. PISA, TIMSS, PIRLS),equity data from household surveys, and projection/attainment data to 2050.For further information, please visit the EdStats website. Context This is adataset hosted by the World Bank. The organization has an open data platformfound here and they update their information according the amount of data thatis brought in. Explore the World Bank using Kaggle and all of the data sourcesavailable through the World Bank organization page! Update Frequency: Thisdataset is updated daily. Acknowledgements This dataset is maintained usingthe World Bank's APIs and Kaggle's API. Cover photo by Josefa nDiaz onUnsplash Unsplash Images are distributed under a unique Unsplash License.
elo-merchant-category-recommendation	Imagine being hungry in an unfamiliar part of town and getting restaurantrecommendations served up, based on your personal preferences, at just theright moment. The recommendation comes with an attached discount from yourcredit card provider for a local place around the corner! Right now, Elo, oneof the largest payment brands in Brazil, has built partnerships with merchantsin order to offer promotions or discounts to cardholders. But do thesepromotions work for either the consumer or the merchant? Do customers enjoytheir experience? Do merchants see repeat business? Personalization is key.Elo has built machine learning models to understand the most important aspectsand preferences in their customers’ lifecycle, from food to shopping. But sofar none of them is specifically tailored for an individual or profile. Thisis where you come in. In this competition, Kagglers will develop algorithms toidentify and serve the most relevant opportunities to individuals, byuncovering signal in customer loyalty. Your input will improve customers’lives and help Elo reduce unwanted campaigns, to create the right experiencefor customers.	Root Mean Squared Error (RMSE) Submissions are scored on the root mean squarederror. RMSE is defined as: RMSE= √ 1 n n ∑ i=1 (yi− ˆ y i)2 , where ˆ y is thepredicted loyalty score for each card_id, and y is the actual loyalty scoreassigned to a card_id. Submission File card_id, target C_ID_9e86007114,0C_ID_1c9f77086c,0.5 C_ID_07b20e9908,0 C_ID_63d6bac69a,0 C_ID_bbc26a86eb,0C_ID_f749aad790,0 C_ID_7b5c15ff41,-0.25 C_ID_ec6b0f2d30,0 C_ID_0a11e759c5,0	['regression', 'tabular data', 'banking']	
facial-keypoints-detection	The objective of this task is to predict keypoint positions on face images.This can be used as a building block in several applications, such as:tracking faces in images and video analysing facial expressions detectingdysmorphic facial signs for medical diagnosis biometrics / face recognitionDetecing facial keypoints is a very challenging problem. Facial features varygreatly from one individual to another, and even for a single individual,there is a large amount of variation due to 3D pose, size, position, viewingangle, and illumination conditions. Computer vision research has come a longway in addressing these difficulties, but there remain many opportunities forimprovement. This getting-started competition provides a benchmark data setand an R tutorial to get you going on analysing face images. Get started withR >> Acknowledgements The data set for this competition was graciouslyprovided by Dr. Yoshua Bengio of the University of Montreal. James Petterson.	Root Mean Squared Error (RMSE) Submissions are scored on the root mean squarederror. RMSE is very common and is a suitable general-purpose error metric.Compared to the Mean Absolute Error, RMSE punishes large errors: RMSE= √ 1 n n∑ i=1 (yi− ˆ y i)2 , where y hat is the predicted value and y is the originalvalue.	['image data', 'object labeling']	
favorita-grocery-sales-forecasting	Brick-and-mortar grocery stores are always in a delicate dance with purchasingand sales forecasting. Predict a little over, and grocers are stuck withoverstocked, perishable goods. Guess a little under, and popular items quicklysell out, leaving money on the table and customers fuming. The problem becomesmore complex as retailers add new locations with unique needs, new products,ever transitioning seasonal tastes, and unpredictable product marketing.Corporación Favorita, a large Ecuadorian-based grocery retailer, knows thisall too well. They operate hundreds of supermarkets, with over 200,000different products on their shelves. Corporación Favorita has challenged theKaggle community to build a model that more accurately forecasts productsales. They currently rely on subjective forecasting methods with very littledata to back them up and very little automation to execute plans. They’reexcited to see how machine learning could better ensure they please customersby having just enough of the right products at the right time.	Submissions are evaluated on the Normalized Weighted Root Mean SquaredLogarithmic Error (NWRMSLE), calculated as follows: NWRMSLE= √ ∑ n i=1 wi(ln(ˆ y i+1)−ln(yi+1))2 ∑ n i=1 wi where for row i, ˆ y i is the predictedunit_sales of an item and yi is the actual unit_sales; n is the total numberof rows in the test set. The weights, wi , can be found in the items.csv file(see the Data page). Perishable items are given a weight of 1.25 where allother items are given a weight of 1.00. This metric is suitable whenpredicting values across a large range of orders of magnitudes. It avoidspenalizing large differences in prediction when both the predicted and thetrue number are large: predicting 5 when the true value is 50 is penalizedmore than predicting 500 when the true value is 545. Submission File Foreachid in the test set, you must predict theunit_sales. Because the metricuses ln(y+1), submissions are validated to ensure there are no negativepredictions. The file should contain a header and have the following format:id,unit_sales 125497040,2.5 125497041,0.0 125497042,27.9 etc.	['food and drink', 'regression', 'tabular data', 'future prediction']	
forest-cover-type-kernels-only	Random forests? Cover trees? Not so fast, computer nerds. We're talking aboutthe real thing. In this competition you are asked to predict the forest covertype (the predominant kind of tree cover) from cartographic variables. Theactual forest cover type for a given 30 x 30 meter cell was determined from USForest Service (USFS) Region 2 Resource Information System data. Independentvariables were then derived from data obtained from the US Geological Surveyand USFS. The data is in raw form and contains binary columns of data forqualitative independent variables such as wilderness areas and soil type. Thisstudy area includes four wilderness areas located in the Roosevelt NationalForest of northern Colorado. These areas represent forests with minimal human-caused disturbances, so that existing forest cover types are more a result ofecological processes rather than forest management practices. This competitionoriginally ran in 2015. We are relaunching it as a kernels-only version here.Acknowledgements Kaggle is hosting this competition for the machine learningcommunity to use for fun and practice. This dataset was provided by Jock A.Blackard and Colorado State University. We also thank the UCI machine learningrepository for hosting the dataset. If you use the problem in publication,please cite: Bache, K. & Lichman, M. (2013). UCI Machine Learning Repository.Irvine, CA: University of California, School of Information and ComputerScience	Submissions are evaluated on multi-class classification accuracy. SubmissionFile Your submission file should have the observation Id and a predicted covertype (an integer between 1 and 7, inclusive). The file should contain a headerand have the following format: Id,Cover_Type 15121,1 15122,1 15123,1 ...	['tabular data', 'ecology', 'forestry']	
freesound-audio-tagging	Some sounds are distinct and instantly recognizable, like a baby’s laugh orthe strum of a guitar. Other sounds aren’t clear and are difficult topinpoint. If you close your eyes, can you tell which of the sounds below is achainsaw versus a blender? Moreover, we often experience a mix of sounds thatcreate an ambience – like the clamoring of construction, a hum of traffic fromoutside the door, blended with loud laughter from the room, and the ticking ofthe clock on your wall. The sound clip below is of a busy food court in theUK. Partly because of the vastness of sounds we experience, no reliableautomatic general-purpose audio tagging systems exist. Currently, a lot ofmanual effort is required for tasks like annotating sound collections andproviding captions for non-speech events in audiovisual content. To tacklethis problem, Freesound (an initiative by MTG-UPF that maintains acollaborative database with over 370,000 Creative Commons Licensed sounds) andGoogle Research’s Machine Perception Team (creators of AudioSet, a large-scaledataset of manually annotated audio events with over 500 classes) have teamedup to develop the dataset for this competition. You’re challenged to build ageneral-purpose automatic audio tagging system using a dataset of audio filescovering a wide range of real-world environments. Sounds in the datasetinclude things like musical instruments, human sounds, domestic sounds, andanimals from Freesound’s library, annotated using a vocabulary of more than 40labels from Google’s AudioSet ontology. To succeed in this competition yoursystems will need to be able to recognize an increased number of sound eventsof very diverse nature, and to leverage subsets of training data featuringannotations of varying reliability (see Data section for more information).	Submissions are evaluated according to the Mean Average Precision @ 3 (MAP@3):MAP@3= 1 U ∑ u=1 U ∑ k=1 min(n,3) P(k) where U is the number of scored audiofiles in the test data, P(k) is the precision at cutoff k, and n is the numberpredictions per audio file. You can learn more about this metric works fromthis kernel, and from this python code. Submission File For each fname (audiofile) in the test set, you may predict up to 3 audio classification labels.The file should contain a header and have the following format: fname,label00063640.wav,Laughter Hi-Hat Flute 0013a1db.wav,Laughter Hi-Hat Flute ...	['sound technology']	
ga-customer-revenue-prediction	The 80/20 rule has proven true for many businesses–only a small percentage ofcustomers produce most of the revenue. As such, marketing teams are challengedto make appropriate investments in promotional strategies. RStudio, thedeveloper of free and open tools for R and enterprise-ready products for teamsto scale and share work, has partnered with Google Cloud and Kaggle todemonstrate the business impact that thorough data analysis can have. In thiscompetition, you’re challenged to analyze a Google Merchandise Store (alsoknown as GStore, where Google swag is sold) customer dataset to predictrevenue per customer. Hopefully, the outcome will be more actionableoperational changes and a better use of marketing budgets for those companieswho choose to use data analysis on top of GA data.	Root Mean Squared Error (RMSE) Submissions are scored on the root mean squarederror. RMSE is defined as: RMSE= 1 n ∑ i=1 n ( y i − y ^ i ) 2 − − − − − − − −− − − − √ , where y hat is the natural log of the predicted revenue for acustomer and y is the natural log of the actual summed revenue value plus one.Submission File For each fullVisitorId in the test set, you must predict thenatural log of their total revenue in PredictedLogRevenue. The submission fileshould contain a header and have the following format:fullVisitorId,PredictedLogRevenue 0000000259678714014,0 0000049363351866189,00000053049821714864,0 etc.	['regression', 'tabular data', 'rmse', 'extra large']	
gendered-pronoun-resolution	Can you help end gender bias in pronoun resolution? Pronoun resolution is partof coreference resolution, the task of pairing an expression to its referringentity. This is an important task for natural language understanding, and theresolution of ambiguous pronouns is a longstanding challenge. Unfortunately,recent studies have suggested gender bias among state-of-the-art coreferenceresolvers. Google AI Language aims to improve gender-fairness in modeling byreleasing the Gendered Ambiguous Pronouns (GAP) dataset, containing gender-balanced pronouns (50% of its examples containing feminine pronouns, and 50%containing masculine pronouns). In this two-stage competition, Kagglers arechallenged to build pronoun resolution systems that perform equally wellregardless of pronoun gender. Stage two's final evaluation will use a newdataset following the same format. To encourage gender-fair modeling, theratio of masculine to feminine examples in the official test data will not beknown ahead of time. \---------- Please cite the original paper if you use GAPin your work: @inproceedings{webster2018gap, title = {Mind the GAP: A BalancedCorpus of Gendered Ambiguous Pronouns}, author = {Webster, Kellie andRecasens, Marta and Axelrod, Vera and Baldridge, Jason}, booktitle ={Transactions of the ACL}, year = {2018}, pages = {to appear}, }	Submissions are evaluated using the multi-class logarithmic loss. Each pronounhas been labeled with whether it refers to A, B, or NEITHER. For each pronoun,you must submit a set of predicted probabilities (one for each class). Theformula is then, logloss=− 1 N ∑ i=1 N ∑ j=1 M y ij log( p ij ), where N isthe number of samples in the test set, M is 3, log l is the natural logarithm,y ij y is 1 if observation i i belongs to class j j and 0 otherwise, and p ijp is the predicted probability that observation i i belongs to class j j . Thesubmitted probabilities are not required to sum to one because they arerescaled prior to being scored (each row is divided by the row sum). In orderto avoid the extremes of the log function, predicted probabilities arereplaced with max(min(p,1− 10 −15 ), 10 −15 ) m . Submission File You mustsubmit a csv file with the probabilities that a pronoun refers to A, B, orNEITHER. The order of the rows does not matter. The file must have a headerand should look like the following: ID,A,B,NEITHERdevelopment-1,0.33333,0.33333,0.33333 development-2,0.33333,0.33333,0.33333development-3,0.33333,0.33333,0.33333 etc.	['nlp', 'text data']	
global-economic-monitor			['world', 'medium', 'featured']	Content Providing daily updates of global economic developments, with coverageof high income- as well as developing countries. Daily data updates areprovided for exchange rates, equity markets, and emerging market bond indices.Monthly data coverage (updated daily and populated upon availability) isprovided for consumer prices, high-tech market indicators, industrialproduction and merchandise trade. Context This is a dataset hosted by theWorld Bank. The organization has an open data platform found here and theyupdate their information according the amount of data that is brought in.Explore the World Bank using Kaggle and all of the data sources availablethrough the World Bank organization page! Update Frequency: This dataset isupdated daily. Acknowledgements This dataset is maintained using the WorldBank's APIs and Kaggle's API. Cover photo by Patrick Hendry on UnsplashUnsplash Images are distributed under a unique Unsplash License.
global-financial-development			['world', 'small']	"Content The Global Financial Development Database is an extensive dataset offinancial system characteristics for 206 economies. The database includesmeasures of (1) size of financial institutions and markets (financial depth),(2) degree to which individuals can and do use financial services (access),(3) efficiency of financial intermediaries and markets in intermediatingresources and facilitating financial transactions (efficiency), and (4)stability of financial institutions and markets (stability). For a completedescription of the dataset and a discussion of the underlying literature, see:Martin Čihák, Aslı Demirgüç-Kunt, Erik Feyen, and Ross Levine, 2012.""Benchmarking Financial Systems Around the World."" World Bank Policy ResearchWorking Paper 6175, World Bank, Washington, D.C. Context This is a datasethosted by the World Bank. The organization has an open data platform foundhere and they update their information according the amount of data that isbrought in. Explore the World Bank using Kaggle and all of the data sourcesavailable through the World Bank organization page! Update Frequency: Thisdataset is updated daily. Acknowledgements This dataset is maintained usingthe World Bank's APIs and Kaggle's API. Cover photo by Raphael Rychetsky onUnsplash Unsplash Images are distributed under a unique Unsplash License."
global-financial-inclusion-global-findex-data			['world', 'small']	Content The Global Financial Inclusion Database provides 800 country-levelindicators of financial inclusion summarized for all adults and disaggregatedby key demographic characteristics-gender, age, education, income, and ruralresidence. Covering more than 140 economies, the indicators of financialinclusion measure how people save, borrow, make payments and manage risk. Thereference citation for the data is: Demirguc-Kunt, Asli, Leora Klapper,Dorothe Singer, and Peter Van Oudheusden. 2015. “The Global Findex Database2014: Measuring Financial Inclusion around the World.” Policy Research WorkingPaper 7255, World Bank, Washington, DC. Context This is a dataset hosted bythe World Bank. The organization has an open data platform found here and theyupdate their information according the amount of data that is brought in.Explore the World Bank using Kaggle and all of the data sources availablethrough the World Bank organization page! Update Frequency: This dataset isupdated daily. Acknowledgements This dataset is maintained using the WorldBank's APIs and Kaggle's API. Cover photo by ZACHARY STAINES on UnsplashUnsplash Images are distributed under a unique Unsplash License.
google-ai-open-images-object-detection-track	"Introduction Google AI (Google’s AI research arm, tasked with advancing AI foreveryone) is challenging you to build an algorithm that detects objectsautomatically using an absolutely massive training dataset ― one with morevaried and complex bounding-box annotations and object classes than everbefore. Here's the background. Computers are getting better and better atvision. But in a few critical ways, they still can't match a human’s intuitiveperception. For example, what do you see when you look at this photo? Most ofus would answer, “a sandy beach, the ocean, a few people walking, some trees,grass, and buildings…a woman walking her dog right there! Oh yeah, and thereis a man holding a plastic cup.” Can a computer provide as precise an imagedescription? Google AI wants to further push the capabilities of computervision. We hope that providing very large training set will stimulate researchinto more sophisticated object and relationship detection models that willexceed current state-of-the-art performance. The results of this Challengewill be presented at a workshop at the European Conference on Computer Vision2018. Object Detection Track Object detection is a central task in computervision, with applications ranging across search, robotics, self-driving cars,and many others. As deep network solutions become deeper and more complex,they are often limited by the amount of training data available. With this inmind, to spur advances in analyzing and understanding images, Google AI haspublicly released the Open Images dataset. Open Images follows the traditionof PASCAL VOC, ImageNet and COCO, now at an unprecedented scale. The OpenImages Challenge is based on Open Images dataset. The training set of theChallenge contains: 12M bounding-box annotations for 500 object classes on1.7M training images Images of complex scenes with several objects–an averageof 7 boxes per image Highly varied images that contain brand new objects like“fedora” and “snowman” Class hierarchy that reflects the relationships betweenclasses of Open Images. In this track of the Challenge, you are asked to buildthe best performing algorithm for automatically detecting objects. Pleaserefer to the Open Images Challenge page for additional details on the dataset.In addition to this Object Detection track, the Challenge also includes aVisual Relationship Detection track to detect pairs of objects in particularrelations, e.g. ""woman playing guitar,"" ""beer on table,"" ""dog inside car"",""man holding coffee"", etc. The Visual Relationship Detection track isavailable here. Example annotations. Left: Mark Paul Gosselaar plays theguitar by Rhys A. Right: the house by anita kluska. Both images used under CCBY 2.0 license."	Submissions are evaluated by computing mean Average Precision (AP), modifiedto take into account the annotation process of Open Images dataset (mean istaken over per-class APs). The metric is described on the Open ImagesChallenge website. The final mAP is computed as the average AP over the 500classes. The participants will be ranked on this final metric. Kaggle'sproduction code in C# can be viewed here. The metric is also implemented as apart of Tensorflow Object Detection API. See this Tutorial on running theevaluation in Python. Kernel Submissions You can make submissions directlyfrom Kaggle Kernels. By adding your teammates as collaborators on a kernel,you can share and edit code privately with them. Submission File For eachimage in the test set, you must predict a list of boxes describing objects inthe image. Each box is described as ImageID,PredictionString ImageID,{LabelConfidence XMin YMin XMax YMax},{...}	[]	
govdata360			['world', 'medium']	Content GovData360 is a compendium of the most important governanceindicators, from 26 datasets with worldwide coverage and more than 10 years ofinfo, designed to provide guidance on the design of reforms and the monitoringof impacts. We have an Unbalanced Panel Data by Dataset - Country for around3260 governance focused indicators. Context This is a dataset hosted by theWorld Bank. The organization has an open data platform found here and theyupdate their information according the amount of data that is brought in.Explore the World Bank using Kaggle and all of the data sources availablethrough the World Bank organization page! Update Frequency: This dataset isupdated daily. Acknowledgements This dataset is maintained using the WorldBank's APIs and Kaggle's API. Cover photo by John Jason on Unsplash UnsplashImages are distributed under a unique Unsplash License.
health-insurance-marketplace			['economics', 'healthcare', 'large', 'featured']	"The Health Insurance Marketplace Public Use Files contain data on health anddental plans offered to individuals and small businesses through the US HealthInsurance Marketplace. Exploration Ideas To help get you started, here aresome data exploration ideas: How do plan rates and benefits vary acrossstates? How do plan benefits relate to plan rates? How do plan rates vary byage? How do plans vary across insurance network providers? See this forumthread for more ideas, and post there if you want to add your own ideas oranswer some of the open questions! Data Description This data was originallyprepared and released by the Centers for Medicare & Medicaid Services (CMS).Please read the CMS Disclaimer-User Agreement before using this data. Here,we've processed the data to facilitate analytics. This processed version hasthree components: 1\. Original versions of the data The original versions ofthe 2014, 2015, 2016 data are available in the ""raw"" directory of the downloadand ""../input/raw"" on Kaggle Scripts. Search for ""dictionaries"" on this pageto find the data dictionaries describing the individual raw files. 2\.Combined CSV files that contain In the top level directory of the download(""../input"" on Kaggle Scripts), there are six CSV files that contain thecombined at across all years: BenefitsCostSharing.csv BusinessRules.csvNetwork.csv PlanAttributes.csv Rate.csv ServiceArea.csv Additionally, thereare two CSV files that facilitate joining data across years: Crosswalk2015.csv- joining 2014 and 2015 data Crosswalk2016.csv - joining 2015 and 2016 data3\. SQLite database The ""database.sqlite"" file contains tables correspondingto each of the processed CSV files. The code to create the processed versionof this data is available on GitHub."
histopathologic-cancer-detection	"In this competition, you must create an algorithm to identify metastaticcancer in small image patches taken from larger digital pathology scans. Thedata for this competition is a slightly modified version of the PatchCamelyon(PCam) benchmark dataset (the original PCam dataset contains duplicate imagesdue to its probabilistic sampling, however, the version presented on Kaggledoes not contain duplicates). PCam is highly interesting for both its size,simplicity to get started on, and approachability. In the authors' words:[PCam] packs the clinically-relevant task of metastasis detection into astraight-forward binary image classification task, akin to CIFAR-10 and MNIST.Models can easily be trained on a single GPU in a couple hours, and achievecompetitive scores in the Camelyon16 tasks of tumor detection and whole-slideimage diagnosis. Furthermore, the balance between task-difficulty andtractability makes it a prime suspect for fundamental machine learningresearch on topics as active learning, model uncertainty, and explainability.Acknowledgements Kaggle is hosting this competition for the machine learningcommunity to use for fun and practice. This dataset was provided by BasVeeling, with additional input from Babak Ehteshami Bejnordi, Geert Litjens,and Jeroen van der Laak. You may view and download the official Pcam datasetfrom GitHub. The data is provided under the CC0 License, following the licenseof Camelyon16. If you use PCam in a scientific publication, please referencethe following papers: [1] B. S. Veeling, J. Linmans, J. Winkens, T. Cohen, M.Welling. ""Rotation Equivariant CNNs for Digital Pathology"". arXiv:1806.03962[2] Ehteshami Bejnordi et al. Diagnostic Assessment of Deep LearningAlgorithms for Detection of Lymph Node Metastases in Women With Breast Cancer.JAMA: The Journal of the American Medical Association, 318(22), 2199–2210.doi:jama.2017.14585 Photo by Ousa Chea"	Submissions are evaluated on area under the ROC curve between the predictedprobability and the observed target. Submission File For each id in the testset, you must predict a probability that center 32x32px region of a patchcontains at least one pixel of tumor tissue. The file should contain a headerand have the following format: id,label0b2ea2a822ad23fdb1b5dd26653da899fbd2c0d5,095596b92e5066c5c52466c90b69ff089b39f2737,0248e6738860e2ebcf6258cdc1f32f299e0c76914,0 etc.	['medicine', 'research', 'oncology and cancer']	
home-credit-default-risk	Many people struggle to get loans due to insufficient or non-existent credithistories. And, unfortunately, this population is often taken advantage of byuntrustworthy lenders. Home Credit strives to broaden financial inclusion forthe unbanked population by providing a positive and safe borrowing experience.In order to make sure this underserved population has a positive loanexperience, Home Credit makes use of a variety of alternative data--includingtelco and transactional information--to predict their clients' repaymentabilities. While Home Credit is currently using various statistical andmachine learning methods to make these predictions, they're challengingKagglers to help them unlock the full potential of their data. Doing so willensure that clients capable of repayment are not rejected and that loans aregiven with a principal, maturity, and repayment calendar that will empowertheir clients to be successful.	Submissions are evaluated on area under the ROC curve between the predictedprobability and the observed target. Submission File For each SK_ID_CURR inthe test set, you must predict a probability for the TARGET variable. The fileshould contain a header and have the following format: SK_ID_CURR,TARGET100001,0.1 100005,0.9 100013,0.2 etc.	['tabular data', 'banking', 'home', 'auc', 'medium']	
homesite-quote-conversion	Before asking someone on a date or skydiving, it's important to know yourlikelihood of success. The same goes for quoting home insurance prices to apotential customer. Homesite, a leading provider of homeowners insurance, doesnot currently have a dynamic conversion rate model that can give themconfidence a quoted price will lead to a purchase. Using an anonymizeddatabase of information on customer and sales activity, including property andcoverage information, Homesite is challenging you to predict which customerswill purchase a given quote. Accurately predicting conversion would helpHomesite better understand the impact of proposed pricing changes and maintainan ideal portfolio of customer segments.	Submissions are evaluated on area under the ROC curve between the predictedprobability and the observed target. Submission File For each QuoteNumber inthe test set, you must predict a probability for QuoteConversion_Flag. Thefile should contain a header and have the following format:QuoteNumber,QuoteConversion_Flag 3,0 5,0.3 7,0 etc.	['binary classification', 'tabular data', 'auc', 'small']	
house-prices-advanced-regression-techniques	Start here if... You have some experience with R or Python and machinelearning basics. This is a perfect competition for data science students whohave completed an online course in machine learning and are looking to expandtheir skill set before trying a featured competition. Competition DescriptionAsk a home buyer to describe their dream house, and they probably won't beginwith the height of the basement ceiling or the proximity to an east-westrailroad. But this playground competition's dataset proves that much moreinfluences price negotiations than the number of bedrooms or a white-picketfence. With 79 explanatory variables describing (almost) every aspect ofresidential homes in Ames, Iowa, this competition challenges you to predictthe final price of each home. Practice Skills Creative feature engineeringAdvanced regression techniques like random forest and gradient boostingAcknowledgments The Ames Housing dataset was compiled by Dean De Cock for usein data science education. It's an incredible alternative for data scientistslooking for a modernized and expanded version of the often cited BostonHousing dataset.	Goal It is your job to predict the sales price for each house. For each Id inthe test set, you must predict the value of the SalePrice variable. MetricSubmissions are evaluated on Root-Mean-Squared-Error (RMSE) between thelogarithm of the predicted value and the logarithm of the observed salesprice. (Taking logs means that errors in predicting expensive houses and cheaphouses will affect the result equally.) Submission File Format The file shouldcontain a header and have the following format: Id,SalePrice 1461,169000.11462,187724.1233 1463,175221 etc. You can download an example submission file(sample_submission.csv) on the Data page.	['regression', 'tabular data']	
housesalesprediction			['finance', 'home', 'small', 'featured']	This dataset contains house sale prices for King County, which includesSeattle. It includes homes sold between May 2014 and May 2015. It's a greatdataset for evaluating simple regression models.
human-protein-atlas-image-classification	In this competition, Kagglers will develop models capable of classifying mixedpatterns of proteins in microscope images. The Human Protein Atlas will usethese models to build a tool integrated with their smart-microscopy system toidentify a protein's location(s) from a high-throughput image. Proteins are“the doers” in the human cell, executing many functions that together enablelife. Historically, classification of proteins has been limited to singlepatterns in one or a few cell types, but in order to fully understand thecomplexity of the human cell, models must classify mixed patterns across arange of different human cells. Images visualizing proteins in cells arecommonly used for biomedical research, and these cells could hold the key forthe next breakthrough in medicine. However, thanks to advances in high-throughput microscopy, these images are generated at a far greater pace thanwhat can be manually evaluated. Therefore, the need is greater than ever forautomating biomedical image analysis to accelerate the understanding of humancells and disease. Nature Methods has indicated interest in considering apaper discussing the outcome and approaches of the challenge. The HumanProtein Atlas team would like to invite top performing teams to join as co-authors in the writing of this paper. Top performing teams will also beeligible to compete for the special prize. Additional information for both thespecial prize and co-authoring for Nature Methods will become availablethrough the Discussion posts once the main competition is complete.Acknowledgements The Human Protein Atlas is a Sweden-based initiative aimed atmapping all human proteins in cells, tissues and organs. All the data in theknowledge resource is open access to allow anyone to pursue exploration of thehuman proteome. In a recent publication, the Human Protein Atlas team hasdemonstrated the promise of both citizen science and artificial intelligenceapproaches in describing the location of human proteins in images, howevercurrent results are yet to approach expert-level annotations (Sullivan et al,Nature Biotechnology, Oct 2018).	Submissions will be evaluated based on their macro F1 score. Submission FileFor each Id in the test set, you must predict a class for the Target variableas described in the data page. Note that multiple labels can be predicted foreach sample. The file should contain a header and have the following format:Id,Predicted 00008af0-bad0-11e8-b2b8-ac1f6b6435d0,0 10000a892-bacf-11e8-b2b8-ac1f6b6435d0,2 30006faa6-bac7-11e8-b2b7-ac1f6b6435d0,0 0008baca-bad7-11e8-b2b9-ac1f6b6435d0,0000cce7e-bad4-11e8-b2b8-ac1f6b6435d0,0 00109f6a-bac8-11e8-b2b7-ac1f6b6435d0,128 ...	['classification', 'image data']	
humpback-whale-identification	After centuries of intense whaling, recovering whale populations still have ahard time adapting to warming oceans and struggle to compete every day withthe industrial fishing industry for food. To aid whale conservation efforts,scientists use photo surveillance systems to monitor ocean activity. They usethe shape of whales’ tails and unique markings found in footage to identifywhat species of whale they’re analyzing and meticulously log whale poddynamics and movements. For the past 40 years, most of this work has been donemanually by individual scientists, leaving a huge trove of data untapped andunderutilized. In this competition, you’re challenged to build an algorithm toidentify individual whales in images. You’ll analyze Happywhale’s database ofover 25,000 images, gathered from research institutions and publiccontributors. By contributing, you’ll help to open rich fields ofunderstanding for marine mammal population dynamics around the globe. Note,this competition is similar in nature to this competition with an expanded andupdated dataset. We'd like to thank Happywhale for providing this data andproblem. Happywhale is a platform that uses image process algorithms to letanyone to submit their whale photo and have it automatically identified.	Submissions are evaluated according to the Mean Average Precision @ 5 (MAP@5):MAP@5= 1 U U ∑ u=1 min(n,5) ∑ k=1 P(k)×rel(k) where U is the number of images,P(k) is the precision at cutoff k , n is the number predictions per image, andrel(k) is an indicator function equaling 1 if the item at rank k is a relevant(correct) label, zero otherwise. Once a correct label has been scored for anobservation, that label is no longer considered relevant for that observation,and additional predictions of that label are skipped in the calculation. Forexample, if the correct label is A for an observation, the followingpredictions all score an average precision of 1.0. [A, B, C, D, E] [A, A, A,A, A] [A, B, A, C, A] Submission File For each Image in the test set, you maypredict up to 5 labels for the whale Id. Whales that are not predicted to beone of the labels in the training data should be labeled as new_whale. Thefile should contain a header and have the following format: Image,Id00028a005.jpg,new_whale w_23a388d w_9b5109b w_9c506f6 w_0369a5c000dcf7d8.jpg,new_whale w_23a388d w_9b5109b w_9c506f6 w_0369a5c ...	['image data', 'animals']	
ida-results-measurement-system			['world', 'small']	Content The IDA Results Measurement System dataset measures progress onaggregate outcomes for IDA countries for selected indicators. It includes keycountry outcome indicators covering areas that are consistent with theMillennium Development Goals, are priorities in many national developmentplans and/or poverty reduction strategies, and reflect IDA's activities in IDAcountries. The indicators capture both the economic growth and the humandevelopment priorities of ongoing IDA programs. Context This is a datasethosted by the World Bank. The organization has an open data platform foundhere and they update their information according the amount of data that isbrought in. Explore the World Bank using Kaggle and all of the data sourcesavailable through the World Bank organization page! Update Frequency: Thisdataset is updated daily. Acknowledgements This dataset is maintained usingthe World Bank's APIs and Kaggle's API. Cover photo by The Creative Exchangeon Unsplash Unsplash Images are distributed under a unique Unsplash License.
imaterialist-challenge-fashion-2018	As shoppers move online, it would be a dream come true to have products inphotos classified automatically. But, automatic product recognition is toughbecause for the same product, a picture can be taken in different lighting,angles, backgrounds, and levels of occlusion. Meanwhile different fine-grainedcategories may look very similar, for example, royal blue vs turquoise incolor. Many of today’s general-purpose recognition machines simply cannotperceive such subtle differences between photos, yet these differences couldbe important for shopping decisions. Tackling issues like this is why theConference on Computer Vision and Pattern Recognition (CVPR) has put togethera workshop specifically for data scientists focused on fine-grained visualcategorization called the FGVC5 workshop. As part of this workshop, CVPR ispartnering with Google, Wish, and Malong Technologies to challenge the datascience community to help push the state of the art in automatic imageclassification. In this competition, FGVC workshop organizers with Wish andMalong Technologies challenge you to develop algorithms that will help with animportant step towards automatic product detection – to accurately assignattribute labels for fashion images. Individuals/Teams with top submissionswill be invited to present their work live at the FGVC5 workshop. Kaggle isexcited to partner with research groups to push forward the frontier ofmachine learning. Research competitions make use of Kaggle's platform andexperience, but are largely organized by the research group's data scienceteam. Any questions or concerns regarding the competition data, quality, ortopic will be addressed by them.	For this competition each image has multiple ground truth labels. We will useMean F1 score (micro-averaged, see details here) to measure the algorithmquality. The metric is also known as the example based F-score in the multi-label learning literature. The F1 metric weights recall and precision equally,and a good recognition algorithm will maximize both precision and recallsimultaneously. Thus, moderately good performance on both will be favored overextremely good performance on one and poor performance on the other.Submission File For every image in the dataset, submission files shouldcontain two columns: image id and predicted labels. Labels should be a space-delimited list. Note that if the algorithm don’t predict anything, the columncan be left blank. The file must have a header and should look like thefollowing: id,predicted 12345,0 1 3 67890,83 293, etc.	[]	
inclusive-images-challenge	"Making products that work for people all over the globe is an important valueat Google AI. In the field of classification, this means developing modelsthat work well for regions all over the world. Today, the dataset a model istrained on greatly dictates the performance of that model. A system trained ona dataset that doesn’t represent a broad range of localities could performworse on images drawn from geographic regions underrepresented in the trainingdata. Google and the industry at large are working to create more diverse &representative datasets. But it is also important for the field to makeprogress in understanding how to build models when the data available may notcover all audiences a model is meant to reach. Google AI is challengingKagglers to develop models that are robust to blind spots that might exist ina data set, and to create image recognition systems that can perform well ontest images drawn from different geographic distributions than the ones theywere trained on. By finding ways to teach image classifiers to generalize tonew geographic and cultural contexts, we hope the community will make evenmore progress in inclusive machine learning that benefits everyone,everywhere. Note: This competition is run in two stages. Refer to the FAQ foran explanation of how this works & the Timeline for specific dates. Thiscompetition is a part of the NIPS 2018 competition track. Winners will beinvited to attend and present their solutions at the workshop. Shankar et al.""No Classification without Representation: Assessing Geodiversity Issues inOpen Data Sets for the Developing World"" NIPS 2017 Workshop on MachineLearning for the Developing World"	For this competition each image has multiple ground truth labels. We will useMean F2 score to measure the algorithm quality. The metric is also known asthe example based F-score with a beta of 2. The F2 metric weights recall moreheavily than precision, but a good recognition algorithm will still balanceprecision and recall. Moderately good performance on both will be favored overextremely good performance on one and poor performance on the other.Submission File For every image in the dataset, submission files shouldcontain two columns: image id and predicted labels. Labels should be a space-delimited list. Note that if the algorithm doesn’t predict anything, thecolumn can be left blank. The file must have a header and should look like thefollowing: image_id,labels 2b2b327132556c767a736b3d,/m/0sgh53y /m/0g4cd02b2b394755692f303963553d,/m/0sgh70d /m/0g44ag etc	['image data', 'multiclass classification']	
instacart-market-basket-analysis	Whether you shop from meticulously planned grocery lists or let whimsy guideyour grazing, our unique food rituals define who we are. Instacart, a groceryordering and delivery app, aims to make it easy to fill your refrigerator andpantry with your personal favorites and staples when you need them. Afterselecting products through the Instacart app, personal shoppers review yourorder and do the in-store shopping and delivery for you. Instacart’s datascience team plays a big part in providing this delightful shoppingexperience. Currently they use transactional data to develop models thatpredict which products a user will buy again, try for the first time, or addto their cart next during a session. Recently, Instacart open sourced thisdata - see their blog post on 3 Million Instacart Orders, Open Sourced. Inthis competition, Instacart is challenging the Kaggle community to use thisanonymized data on customer orders over time to predict which previouslypurchased products will be in a user’s next order. They’re not only lookingfor the best model, Instacart’s also looking for machine learning engineers togrow their team. Winners of this competition will receive both a cash prizeand a fast track through the recruiting process. For more information aboutexciting opportunities at Instacart, check out their careers page here ore-mail their recruiting team directly at ml.jobs@instacart.com.	Submissions will be evaluated based on their mean F1 score. Submission FileFor each order_id in the test set, you should predict a space-delimited listof product_ids for that order. If you wish to predict an empty order, youshould submit an explicit 'None' value. You may combine 'None' withproduct_ids. The spelling of 'None' is case sensitive in the scoring metric.The file should have a header and look like the following: order_id,products17,1 2 34,None 137,1 2 3 etc.	['food and drink', 'market basket']	
intel-mobileodt-cervical-cancer-screening	Cervical cancer is so easy to prevent if caught in its pre-cancerous stagethat every woman should have access to effective, life-saving treatment nomatter where they live. Today, women worldwide in low-resource settings arebenefiting from programs where cancer is identified and treated in a singlevisit. However, due in part to lacking expertise in the field, one of thegreatest challenges of these cervical cancer screen and treat programs isdetermining the appropriate method of treatment which can vary depending onpatients’ physiological differences. Especially in rural parts of the world,many women at high risk for cervical cancer are receiving treatment that willnot work for them due to the position of their cervix. This is a tragedy:health providers are able to identify high risk patients, but may not have theskills to reliably discern which treatment which will prevent cancer in thesewomen. Even worse, applying the wrong treatment has a high cost. A treatmentwhich works effectively for one woman may obscure future cancerous growth inanother woman, greatly increasing health risks. Currently, MobileODT offers aQuality Assurance workflow to support remote supervision which helpshealthcare providers make better treatment decisions in rural settings.However, their workflow would be greatly improved given the ability to makereal-time determinations about patients’ treatment eligibility based on cervixtype. In this competition, Intel is partnering with MobileODT to challengeKagglers to develop an algorithm which accurately identifies a woman’s cervixtype based on images. Doing so will prevent ineffectual treatments and allowhealthcare providers to give proper referral for cases that require moreadvanced treatment. Competition Partner MobileODT has developed and sells theEnhanced Visual Assessment (EVA) System, a digital toolkit for health careworkers of every level to provide expert services to patients, anchored at thepoint-of-care by an FDA-approved, intelligent, mobile-phone based medicaldevice. Combining the algorithmic power of biomedical optics with thecomputational capabilities and connectivity of mobile phones, MobileODT'sconnected, intelligent medical systems can be used everywhere, under nearlyany conditions. MobileODT's first product, the FDA approved EVA System forcolposcopy, is in use by health providers in 31 hospital systems across theUS, and in 22 countries, to better screen and treat women for cervical cancerand to conduct forensic colposcopy.	Submissions are evaluated using the multi-class logarithmic loss. Each imagehas been labeled with one type. For each image, you must submit a set ofpredicted probabilities (one for every category). The formula is then,logloss=− 1 N ∑ i=1 N ∑ j=1 M y ij log( p ij ), where N is the number ofimages in the test set, M is the number of categories, log is the naturallogarithm, (y_{ij}) is 1 if observation (i) belongs to class (j) and 0otherwise, and p ij is the predicted probability that observation i belongs toclass j . The submitted probabilities for a given image are not required tosum to one because they are rescaled prior to being scored (each row isdivided by the row sum). In order to avoid the extremes of the log function,predicted probabilities are replaced with max(min(p,1− 10 −15 ), 10 −15 ) .Submission File You must submit a csv file with the image id, and aprobability for each class. The order of the rows does not matter. The filemust have a header and should look like the following:image_name,Type_1,Type_2,Type_3 0.jpg,0.1,0.3,0.6 1.jpg,0,0,1 ...	['image data', 'healthcare', 'multiclass classification', 'object identification']	
international-debt-statistics			['finance', 'world', 'small', 'featured']	Content More details about each file are in the individual file descriptions.Context This is a dataset hosted by the World Bank. The organization has anopen data platform found here and they update their information according theamount of data that is brought in. Explore the World Bank using Kaggle and allof the data sources available through the World Bank organization page! UpdateFrequency: This dataset is updated daily. Acknowledgements This dataset ismaintained using the World Bank's APIs and Kaggle's API. Cover photo by NA onUnsplash Unsplash Images are distributed under a unique Unsplash License.
invasive-species-monitoring	Tangles of kudzu overwhelm trees in Georgia while cane toads threaten habitatsin over a dozen countries worldwide. These are just two invasive species ofmany which can have damaging effects on the environment, the economy, and evenhuman health. Despite widespread impact, efforts to track the location andspread of invasive species are so costly that they’re difficult to undertakeat scale. Currently, ecosystem and plant distribution monitoring depends onexpert knowledge. Trained scientists visit designated areas and take note ofthe species inhabiting them. Using such a highly qualified workforce isexpensive, time inefficient, and insufficient since humans cannot cover largeareas when sampling. Because scientists cannot sample a large quantity ofareas, some machine learning algorithms are used in order to predict thepresence or absence of invasive species in areas that have not been sampled.The accuracy of this approach is far from optimal, but still contributes toapproaches to solving ecological problems. In this playground competition,Kagglers are challenged to develop algorithms to more accurately identifywhether images of forests and foliage contain invasive hydrangea or not.Techniques from computer vision alongside other current technologies likeaerial imaging can make invasive species monitoring cheaper, faster, and morereliable. Acknowledgments Data providers: Christian Requena Mesa, Thore Engel,Amrita Menon, Emma Bradley.	Submissions are evaluated on area under the ROC curve between the predictedprobability and the observed target. Submission File For each image in thetest set, you must predict a probability for the target variable on whetherthe image contains invasive species or not. The file should contain a headerand have the following format: name,invasive 2,0.5 5,0 6,0.2 etc.	['image data', 'object detection', 'plants']	
jigsaw-toxic-comment-classification-challenge	Discussing things you care about can be difficult. The threat of abuse andharassment online means that many people stop expressing themselves and giveup on seeking different opinions. Platforms struggle to effectively facilitateconversations, leading many communities to limit or completely shut down usercomments. The Conversation AI team, a research initiative founded by Jigsawand Google (both a part of Alphabet) are working on tools to help improveonline conversation. One area of focus is the study of negative onlinebehaviors, like toxic comments (i.e. comments that are rude, disrespectful orotherwise likely to make someone leave a discussion). So far they’ve built arange of publicly available models served through the Perspective API,including toxicity. But the current models still make errors, and they don’tallow users to select which types of toxicity they’re interested in finding(e.g. some platforms may be fine with profanity, but not with other types oftoxic content). In this competition, you’re challenged to build a multi-headedmodel that’s capable of detecting different types of of toxicity like threats,obscenity, insults, and identity-based hate better than Perspective’s currentmodels. You’ll be using a dataset of comments from Wikipedia’s talk pageedits. Improvements to the current model will hopefully help online discussionbecome more productive and respectful. Disclaimer: the dataset for thiscompetition contains text that may be considered profane, vulgar, oroffensive.	Update: Jan 30, 2018. Due to changes in the competition dataset, we havechanged the evaluation metric of this competition. Submissions are nowevaluated on the mean column-wise ROC AUC. In other words, the score is theaverage of the individual AUCs of each predicted column. Submission File Foreach id in the test set, you must predict a probability for each of the sixpossible types of comment toxicity (toxic, severe_toxic, obscene, threat,insult, identity_hate). The columns must be in the same order as shown below.The file should contain a header and have the following format:id,toxic,severe_toxic,obscene,threat,insult,identity_hate00001cee341fdb12,0.5,0.5,0.5,0.5,0.5,0.50000247867823ef7,0.5,0.5,0.5,0.5,0.5,0.5 etc.	['text data']	
kkbox-churn-prediction-challenge	"The 11th ACM International Conference on Web Search and Data Mining (WSDM2018) is challenging you to build an algorithm that predicts whether asubscription user will churn using a donated dataset from KKBOX. WSDM(pronounced ""wisdom"") is one of the the premier conferences on web inspiredresearch involving search and data mining. They're committed to publishingoriginal, high quality papers and presentations, with an emphasis on practicalbut principled novel models. For a subscription business, accuratelypredicting churn is critical to long-term success. Even slight variations inchurn can drastically affect profits. KKBOX is Asia’s leading music streamingservice, holding the world’s most comprehensive Asia-Pop music library withover 30 million tracks. They offer a generous, unlimited version of theirservice to millions of people, supported by advertising and paidsubscriptions. This delicate model is dependent on accurately predicting churnof their paid users. In this competition you’re tasked to build an algorithmthat predicts whether a user will churn after their subscription expires.Currently, the company uses survival analysis techniques to determine theresidual membership life time for each subscriber. By adopting differentmethods, KKBOX anticipates they’ll discover new insights to why users leave sothey can be proactive in keeping users dancing. Winners will present theirfindings at the WSDM conference February 6-8, 2018 in Los Angeles, CA. Formore information on the conference, click here."	The evaluation metric for this competition is Log Loss logloss=− 1 N ∑ i=1 N (y i log( p i )+(1− y i )log(1− p i )) where N is the number of observations,log l is the natural logarithm, y i y is the binary target, and p i p is thepredicted probability that y i y equals 1. Note: the actual submittedpredicted probabilities are replaced with max(min(p,1− 10 −15 ), 10 −15 ) m .Submission File For each user id (msno) in the test set, you must predict theprobability of churn (a number between 0 and 1). The file should contain aheader and have the following format: msno,is_churnugx0CjOMzazClkFzU2xasmDZaoIqOUAZPsH1q0teWCg=,0.5zLo9f73nGGT1p21ltZC3ChiRnAVvgibMyazbCxvWPcg=,0.4f/NmvEzHfhINFEYZTR05prUdr+E+3+oewvweYz9cCQE=,0.9 etc.	['binary classification']	
kkbox-music-recommendation-challenge	"The 11th ACM International Conference on Web Search and Data Mining (WSDM2018) is challenging you to build a better music recommendation system using adonated dataset from KKBOX. WSDM (pronounced ""wisdom"") is one of the thepremier conferences on web inspired research involving search and data mining.They're committed to publishing original, high quality papers andpresentations, with an emphasis on practical but principled novel models. Notmany years ago, it was inconceivable that the same person would listen to theBeatles, Vivaldi, and Lady Gaga on their morning commute. But, the glory daysof Radio DJs have passed, and musical gatekeepers have been replaced withpersonalizing algorithms and unlimited streaming services. While the public’snow listening to all kinds of music, algorithms still struggle in key areas.Without enough historical data, how would an algorithm know if listeners willlike a new song or a new artist? And, how would it know what songs torecommend brand new users? WSDM has challenged the Kaggle ML community to helpsolve these problems and build a better music recommendation system. Thedataset is from KKBOX, Asia’s leading music streaming service, holding theworld’s most comprehensive Asia-Pop music library with over 30 million tracks.They currently use a collaborative filtering based algorithm with matrixfactorization and word embedding in their recommendation system but believenew techniques could lead to better results. Winners will present theirfindings at the conference February 6-8, 2018 in Los Angeles, CA. For moreinformation on the conference, click here, and don't forget to check out theother KKBox/WSDM competition: KKBox Music Churn Prediction Challenge"	Submissions are evaluated on area under the ROC curve between the predictedprobability and the observed target. Submission File For each id in the testset, you must predict a probability for the target variable. The file shouldcontain a header and have the following format: id,target 2,0.3 5,0.1 6,1 etc.	[]	
landmark-recognition-challenge	Did you ever go through your vacation photos and ask yourself: What is thename of this temple I visited in China? Who created this monument I saw inFrance? Landmark recognition can help! This technology can predict landmarklabels directly from image pixels, to help people better understand andorganize their photo collections. Today, a great obstacle to landmarkrecognition research is the lack of large annotated datasets. In thiscompetition, we present the largest worldwide dataset to date, to fosterprogress in this problem. This competition challenges Kagglers to build modelsthat recognize the correct landmark (if any) in a dataset of challenging testimages. Many Kagglers are familiar with image classification challenges likethe ImageNet Large Scale Visual Recognition Challenge (ILSVRC), which aims torecognize 1K general object categories. Landmark recognition is a littledifferent from that: it contains a much larger number of classes (there are atotal of 15K classes in this challenge), and the number of training examplesper class may not be very large. Landmark recognition is challenging in itsown way. This challenge is organized in conjunction with the LandmarkRetrieval Challenge (https://www.kaggle.com/c/landmark-retrieval-challenge).In particular, note that the test set for both challenges is the same, toencourage participants to compete in both. We also encourage participants touse the training data from the recognition challenge to train models whichcould be useful for the retrieval challenge. Note, however, that there are nolandmarks in common between the training/index sets of the two challenges.	"Submissions are evaluated using Global Average Precision (GAP) at k k , wherek=1 k . This metric is also known as micro Average Precision (microAP), as per[1]. It works as follows: For each query image, you will predict one landmarklabel and a corresponding confidence score. The evaluation treats eachprediction as an individual data point in a long list of predictions (sortedin descending order by confidence scores), and computes the Average Precisionbased on this list. If a submission has N N predictions (label/confidencepairs) sorted in descending order by their confidence scores, then the GlobalAverage Precision is computed as: GAP= 1 M ∑ i=1 N P(i)rel(i) where: N N isthe total number of predictions returned by the system, across all queries M Mis the total number of queries with at least one landmark from the trainingset visible in it (note that some queries may not depict landmarks) P(i) P isthe precision at rank i i rel(i) r denotes the relevance of prediciton i i :it’s 1 if the i i -th prediction is correct, and 0 otherwise [1] F. Perronnin,Y. Liu, and J.-M. Renders, ""A Family of Contextual Measures of Similaritybetween Distributions with Application to Image Retrieval,"" Proc. CVPR'09Submission File For each id in the test set, you can predict at most onelandmark and its corresponding confidence score. Some query images may containno landmarks. You may decide not to predict any result for a given query, bysubmitting an empty prediction. The submission file should contain a headerand have the following format (larger scores denote more confident matches):id,landmarks 000088da12d664db,8815 0.03 0001623c6d808702,0001bbb682d45002,5328 0.5 etc."	['image data']	
LANL-Earthquake-Prediction	Forecasting earthquakes is one of the most important problems in Earth sciencebecause of their devastating consequences. Current scientific studies relatedto earthquake forecasting focus on three key points: when the event willoccur, where it will occur, and how large it will be. In this competition, youwill address when the earthquake will take place. Specifically, you’ll predictthe time remaining before laboratory earthquakes occur from real-time seismicdata. If this challenge is solved and the physics are ultimately shown toscale from the laboratory to the field, researchers will have the potential toimprove earthquake hazard assessments that could save lives and billions ofdollars in infrastructure. This challenge is hosted by Los Alamos NationalLaboratory which enhances national security by ensuring the safety of the U.S.nuclear stockpile, developing technologies to reduce threats from weapons ofmass destruction, and solving problems related to energy, environment,infrastructure, health, and global security concerns. Acknowledgments:Geophysics Group: The competition builds on initial work from Bertrand Rouet-Leduc, Claudia Hulbert, and Paul Johnson. B. Rouet-Leduc prepared the data forthe competition. Department of Geosciences: Data are from experimentsperformed by Chas Bolton, Jacques Riviere, Paul Johnson and Prof. ChrisMarone. Department of Physics & Astronomy: This competition stemmed from theDOE Council workshop “Information is in the Noise: Signatures of EvolvingFracture and Fracture Networks” held March 2018 that was organized by Prof.Laura J. Pyrak-Nolte. Department of Energy Office of Science, Basic EnergySciences, Chemical Sciences, Geosciences and Biosciences Division: TheGeosciences core research. Photo by Nik Shuliahin on Unsplash	Submissions are evaluated using the mean absolute error between the predictedtime remaining before the next lab earthquake and the act remaining time.Submission File For each seg_id in the test set folder, you must predicttime_to_failure, which is the remaining time before the next lab earthquake.The file should contain a header and have the following format:seg_id,time_to_failure seg_00030f,0 seg_0012b5,0 seg_00184e,0 ...	['physics', 'signal processing', 'earth sciences']	
leaf-classification	There are estimated to be nearly half a million species of plant in the world.Classification of species has been historically problematic and often resultsin duplicate identifications. Automating plant recognition might have manyapplications, including: The objective of this playground competition is touse binary leaf images and extracted features, including shape, margin &texture, to accurately identify 99 species of plants. Leaves, due to theirvolume, prevalence, and unique characteristics, are an effective means ofdifferentiating plant species. They also provide a fun introduction toapplying techniques that involve image-based features. As a first step, trybuilding a classifier that uses the provided pre-extracted features. Next, trycreating a set of your own features. Finally, examine the errors you're makingand see what you can do to improve. Acknowledgments Kaggle is hosting thiscompetition for the data science community to use for fun and education. Thisdataset originates from leaf images collected by James Cope, Thibaut Beghin,Paolo Remagnino, & Sarah Barman of the Royal Botanic Gardens, Kew, UK. CharlesMallah, James Cope, James Orwell. Plant Leaf Classification UsingProbabilistic Integration of Shape, Texture and Margin Features. SignalProcessing, Pattern Recognition and Applications, in press. 2013. We thank theUCI machine learning repository for hosting the dataset.	Submissions are evaluated using the multi-class logarithmic loss. Each imagehas been labeled with one true species. For each image, you must submit a setof predicted probabilities (one for every species). The formula is then,logloss=− 1 N ∑ i=1 N ∑ j=1 M y ij log( p ij ), where N is the number ofimages in the test set, M is the number of species labels, log is the naturallogarithm, y ij is 1 if observation i is in class j and 0 otherwise, and p ijis the predicted probability that observation i belongs to class j . Thesubmitted probabilities for a given device are not required to sum to onebecause they are rescaled prior to being scored (each row is divided by therow sum), but they need to be in the range of [0, 1]. In order to avoid theextremes of the log function, predicted probabilities are replaced withmax(min(p,1− 10 −15 ), 10 −15 ) . Submission File You must submit a csv filewith the image id, all candidate species names, and a probability for eachspecies. The order of the rows does not matter. The file must have a headerand should look like the following:id,Acer_Capillipes,Acer_Circinatum,Acer_Mono,... 2,0.1,0.5,0,0.2,...5,0,0.3,0,0.4,... 6,0,0,0,0.7,... etc.	['image data', 'multiclass classification', 'object identification']	
lending-club-loan-data			['finance', 'medium', 'featured']	"These files contain complete loan data for all loans issued through the2007-2015, including the current loan status (Current, Late, Fully Paid, etc.)and latest payment information. The file containing loan data through the""present"" contains complete loan data for all loans issued through theprevious completed calendar quarter. Additional features include creditscores, number of finance inquiries, address including zip codes, and state,and collections among others. The file is a matrix of about 890 thousandobservations and 75 variables. A data dictionary is provided in a separatefile. k"
loandata			['finance', 'small', 'featured']	Context This data set includes customers who have paid off their loans, whohave been past due and put into collection without paying back their loan andinterests, and who have paid off only after they were put in collection. Thefinancial product is a bullet loan that customers should pay off all of theirloan debt in just one time by the end of the term, instead of an installmentschedule. Of course, they could pay off earlier than their pay schedule.Content Loan_id A unique loan number assigned to each loan customersLoan_status Whether a loan is paid off, in collection, new customer yet topayoff, or paid off after the collection efforts Principal Basic principalloan amount at the origination terms Can be weekly (7 days), biweekly, andmonthly payoff schedule Effective_date When the loan got originated and tookeffects Due_date Since it’s one-time payoff schedule, each loan has one singledue date Paidoff_time The actual time a customer pays off the loanPastdue_days How many days a loan has been past due Age, education, gender Acustomer’s basic demographic information
march-machine-learning-mania-2017	Another year, another chance to predict the upsets, call the probabilities,and put your bracketology skills to the leaderboard test. In our fourth annualMarch Machine Learning Mania competition, Kagglers will once again join themillions of fans who attempt to predict the outcomes of this year's US men'scollege basketball tournament. But unlike most fans, you will pick the winnersand losers using a combination of rich historical data and computing power,while the ground truth unfolds on national television. In the first stage ofthe competition, Kagglers will rely on results of past tournaments to buildand test models. We encourage you to post any useful external data as adataset. In the second stage, competitors will forecast outcomes of allpossible match-ups in the 2017 tournament. You don't need to participate inthe first stage to enter the second. The first stage exists to incentivizemodel building and provide a means to score predictions. The real competitionis forecasting the 2017 results.	"Submissions are scored on the log loss: LogLoss=− 1 n ∑ i=1 n [ y i log( y ^ i)+(1− y i )log(1− y ^ i )], where n is the number of games played y ^ i is thepredicted probability of team 1 beating team 2 y i is 1 if team 1 wins, 0 ifteam 2 wins log() is the natural (base e) logarithm The use of the logarithmprovides extreme punishments for being both confident and wrong. In the worstpossible case, a prediction that something is true when it is actually falsewill add infinite to your error score. In order to prevent this, predictionsare bounded away from the extremes by a small value. Submission File The fileyou submit will depend on whether the competition is in stage 1 (historicalmodel building) or stage 2 (the 2017 tournament). Sample submission files willbe provided for both stages. The format is a list of every possible matchupbetween the tournament teams. Since team1 vs. team2 is the same as team2 vs.team1, we only include the game pairs where team1 has the lower team id. Forexample, in a tournament of 68 teams (64 + 4 play-in teams), you will predict(68*67)/2 = 2278 matchups. Each game has a unique id created by concatenatingthe season in which the game was played, the team1 id, and the team2 id. Forexample, ""2013_1104_1129"" indicates team 1104 played team 1129 in the year2013. You must predict the probability that the team with the lower id beatsthe team with the higher id. The resulting submission format looks like thefollowing, where ""pred"" represents the predicted probability that the firstteam will win: id,pred 2013_1103_1107,0.5 2013_1103_1112,0.52013_1103_1125,0.5 ..."	['sports', 'future prediction', 'basketball']	
median-listing-price-1-bedroom			['cities', 'home', 'small', 'featured']	Context This dataset includes the median list price divided by the squarefootage of a 1-bedroom home for a select number of neighborhoods around theUnited States. Content When available, data includes median price per squarefoot on a monthly basis between January 2010 and September 2016. Selectedneighborhoods include: Upper East Side, New York, NY Spring Valley, Las Vegas,NV Hollywood, Los Angeles, CA Williamsburg, New York, NY Harlem, New York, NYEnterprise, Las Vegas,NV Downtown, San Jose, CA Sheepshead Bay, New York, NYForest Hills, New York, NY Jackson Heights, New York, NY Gramercy, New York,NY Flagami, Miami, FL Downtown, Memphis, TN Chelsea, New York, NY Oak Lawn,Dallas, TX Greater Uptown, Houston, TX South Loop, Chicago, IL Makiki-LowerPunchbowl-Tantalus, Honolulu, HI Downtown, Los Angeles, CA Capitol Hill,Seattle, WA Clinton, New York, NY Alexandria West, Alexandria, VA FinancialDistrict, New York, NY Flatiron District, New York, NY Landmark-Van Dom,Alexandria, VA Flamingo Lummus, Miami Beach, FL Winchester, Las Vegas, NVBrickell, Miami, FL Waikiki, Honolulu, HI Back Bay, Boston, MA Sutton Place,New York, NY and several others Inspiration What neighborhoods have the mostexpensive real estate per square foot? Least expensive? Which neighborhoodsand/or cities have the fastest growth rates in price? Are there anyneighborhoods that remain relatively steady in price? Given that this metricis listing price per square foot, is there a similar dataset that could helpyou compare median square footage in a 1-bedroom home across neighborhoods?Acknowledgement This dataset is part of Zillow Data, and the original sourcecan be found here, under the Neighborhoods link.
melbourne-university-seizure-prediction	Epilepsy afflicts nearly 1% of the world's population, and is characterized bythe occurrence of spontaneous seizures. For many patients, anticonvulsantmedications can be given at sufficiently high doses to prevent seizures, butpatients frequently suffer side effects. For 20-40% of patients with epilepsy,medications are not effective. Even after surgical removal of epilepsy, manypatients continue to experience spontaneous seizures. Despite the fact thatseizures occur infrequently, patients with epilepsy experience persistentanxiety due to the possibility of a seizure occurring. Seizure forecastingsystems have the potential to help patients with epilepsy lead more normallives. In order for electrical brain activity (EEG) based seizure forecastingsystems to work effectively, computational algorithms must reliably identifyperiods of increased probability of seizure occurrence. If these seizure-permissive brain states can be identified, devices designed to warn patientsof impeding seizures would be possible. Patients could avoid potentiallydangerous activities like driving or swimming, and medications could beadministered only when needed to prevent impending seizures, reducing overallside effects. The Competition Transitioning from the Kaggle contests held onseizure detection and seizure prediction in 2014 that primarily involved long-term electrical brain activity recordings from dogs, the current contestfocuses on seizure prediction using long-term electrical brain activityrecordings from humans obtained from the world-first clinical trial of theimplantable NeuroVista Seizure Advisory System. Human brain activity wasrecorded in the form of intracranial EEG (iEEG), which involves electrodespositioned on the surface of the cerebral cortex and the recording ofelectrical signals with an ambulatory monitoring system. These are longduration recordings, spanning multiple months up to multiple years andrecording large numbers of seizures in some humans. The challenge is todistinguish between ten minute long data clips covering an hour prior to aseizure, and ten minute iEEG clips of interictal activity. AcknowledgmentsThis competition is sponsored by MathWorks, the National Institutes of Health(NINDS), the American Epilepsy Society and the University of Melbourne, andorganised in partnership with the Alliance for Epilepsy Research, theUniversity of Pennsylvania and the Mayo Clinic. References	Submissions are evaluated on area under the ROC curve between the predictedprobability and the observed target. Submission File For each File in the testset, you must predict a probability for the Class variable. A probability of 1indicates a prediction of the preictal class. The file should contain a headerand have the following format: File,Class 1_1.mat,0.1 1_2.mat,0.00 1_3.mat,1etc.	['healthcare', 'diseases', 'signal data']	
mens-machine-learning-competition-2018	Google Cloud and NCAA® have teamed up to bring you this year’s version of theKaggle machine learning competition. Another year, another chance toanticipate the upsets, call the probabilities, and put your bracketologyskills to the leaderboard test. Kagglers will join the millions of fans whoattempt to forecast the outcomes of March Madness® during this year's NCAADivision I Men’s and Women’s Basketball Championships. But unlike most fans,you will pick your bracket using a combination of NCAA’s historical data andyour computing power, while the ground truth unfolds on national television.In the first stage of the competition, Kagglers will rely on results of pasttournaments to build and test models. We encourage you to post any usefulexternal data as a dataset. In the second stage, competitors will forecastoutcomes of all possible match-ups in the 2018 NCAA Division I Men’s andWomen’s Basketball Championships. You don't need to participate in the firststage to enter the second. The first stage exists to incentivize modelbuilding and provide a means to score predictions. The real competition isforecasting the 2018 results. This page is for the NCAA Division I Men'stournament. Check out the NCAA Division I Women's tournament here.	"Submissions are scored on the log loss: LogLoss=− 1 n ∑ i=1 n [ y i log( y ^ i)+(1− y i )log(1− y ^ i )], where n is the number of games played y ^ i y isthe predicted probability of team 1 beating team 2 y i y is 1 if team 1 wins,0 if team 2 wins log() l is the natural (base e) logarithm The use of thelogarithm provides extreme punishments for being both confident and wrong. Inthe worst possible case, a prediction that something is true when it isactually false will add infinite to your error score. In order to preventthis, predictions are bounded away from the extremes by a small value.Submission File The file you submit will depend on whether the competition isin stage 1 (historical model building) or stage 2 (the 2018 tournament).Sample submission files will be provided for both stages. The format is a listof every possible matchup between the tournament teams. Since team1 vs. team2is the same as team2 vs. team1, we only include the game pairs where team1 hasthe lower team id. For example, in a tournament of 68 teams (64 + 4 play-inteams), you will predict (68*67)/2 = 2278 matchups. Each game has a unique idcreated by concatenating the season in which the game was played, the team1id, and the team2 id. For example, ""2013_1104_1129"" indicates team 1104 playedteam 1129 in the year 2013. You must predict the probability that the teamwith the lower id beats the team with the higher id. The resulting submissionformat looks like the following, where ""pred"" represents the predictedprobability that the first team will win: id,pred 2013_1103_1107,0.52013_1103_1112,0.5 2013_1103_1125,0.5 ..."	['basketball']	
mercari-price-suggestion-challenge	It can be hard to know how much something’s really worth. Small details canmean big differences in pricing. For example, one of these sweaters cost $335and the other cost $9.99. Can you guess which one’s which? Product pricinggets even harder at scale, considering just how many products are sold online.Clothing has strong seasonal pricing trends and is heavily influenced by brandnames, while electronics have fluctuating prices based on product specs.Mercari, Japan’s biggest community-powered shopping app, knows this problemdeeply. They’d like to offer pricing suggestions to sellers, but this is toughbecause their sellers are enabled to put just about anything, or any bundle ofthings, on Mercari's marketplace. In this competition, Mercari’s challengingyou to build an algorithm that automatically suggests the right productprices. You’ll be provided user-inputted text descriptions of their products,including details like product category name, brand name, and item condition.Note that, because of the public nature of this data, this competition is a“Kernels Only” competition. In the second stage of the challenge, files willonly be available through Kernels and you will not be able to modify yourapproach in response to new data. Read more details in the data tab andKernels FAQ page.	The evaluation metric for this competition is Root Mean Squared LogarithmicError. The RMSLE is calculated as ϵ= √ 1 n n ∑ i=1 (log(pi+1)−log(ai+1))2Where: ϵ is the RMSLE value (score) n is the total number of observations inthe (public/private) data set, pi is your prediction of price, and ai is theactual sale price for i. log(x) is the natural logarithm of x Submission FileFor every row in the dataset, submission files should contain two columns:test_id and price. The id corresponds to the column of that id in thetest.tsv. The file should contain a header and have the following format:test_id,price 0,1.50 1,50 2,500 3,100 etc.	['rmsle', 'small']	
mercedes-benz-greener-manufacturing	Since the first automobile, the Benz Patent Motor Car in 1886, Mercedes-Benzhas stood for important automotive innovations. These include, for example,the passenger safety cell with crumple zone, the airbag and intelligentassistance systems. Mercedes-Benz applies for nearly 2000 patents per year,making the brand the European leader among premium car makers. Daimler’sMercedes-Benz cars are leaders in the premium car industry. With a hugeselection of features and options, customers can choose the customizedMercedes-Benz of their dreams. . To ensure the safety and reliability of eachand every unique car configuration before they hit the road, Daimler’sengineers have developed a robust testing system. But, optimizing the speed oftheir testing system for so many possible feature combinations is complex andtime-consuming without a powerful algorithmic approach. As one of the world’sbiggest manufacturers of premium cars, safety and efficiency are paramount onDaimler’s production lines. In this competition, Daimler is challengingKagglers to tackle the curse of dimensionality and reduce the time that carsspend on the test bench. Competitors will work with a dataset representingdifferent permutations of Mercedes-Benz car features to predict the time ittakes to pass testing. Winning algorithms will contribute to speedier testing,resulting in lower carbon dioxide emissions without reducing Daimler’sstandards.	Submissions are evaluated on the R^2 value, also called the coefficient ofdetermination. Submission File For each 'ID' in the test set, you must predictthe 'y' variable. The file should contain a header and have the followingformat: ID,y 1,100 2,100.33 3,105.81 ...	['regression', 'tabular data', 'automobiles']	
microsoft-malware-prediction	The malware industry continues to be a well-organized, well-funded marketdedicated to evading traditional security measures. Once a computer isinfected by malware, criminals can hurt consumers and enterprises in manyways. With more than one billion enterprise and consumer customers, Microsofttakes this problem very seriously and is deeply invested in improvingsecurity. As one part of their overall strategy for doing so, Microsoft ischallenging the data science community to develop techniques to predict if amachine will soon be hit with malware. As with their previous, MalwareChallenge (2015), Microsoft is providing Kagglers with an unprecedentedmalware dataset to encourage open-source progress on effective techniques forpredicting malware occurrences. Can you help protect more than one billionmachines from damage BEFORE it happens? Acknowledgements This competition ishosted by Microsoft, Windows Defender ATP Research, Northeastern UniversityCollege of Computer and Information Science, and Georgia Tech Institute forInformation Security & Privacy. Microsoft contacts Rob McCann(Robert.McCann@microsoft.com) Christian Seifert (chriseif@microsoft.com) SusanHiggs (Susan.Higgs@microsoft.com) Matt Duncan (Matthew.Duncan@microsoft.com)Northeastern University contact Mansour Ahmadi (m.ahmadi@northeastern.edu)Georgia Tech contacts Brendan Saltaformaggio (brendan@ece.gatech.edu) TaesooKim (taesoo@gatech.edu)	Submissions are evaluated on area under the ROC curve between the predictedprobability and the observed label. Submission File For each MachineIdentifierin the test set, you must predict a probability for the HasDetections column.The file should contain a header and have the following format:MachineIdentifier,HasDetections 1,0.5 6,0.5 14,0.5 etc.	[]	
movie-review-sentiment-analysis-kernels-only	"""There's a thin line between likably old-fashioned and fuddy-duddy, and TheCount of Monte Cristo ... never quite settles on either side."" The RottenTomatoes movie review dataset is a corpus of movie reviews used for sentimentanalysis, originally collected by Pang and Lee [1]. In their work on sentimenttreebanks, Socher et al. [2] used Amazon's Mechanical Turk to create fine-grained labels for all parsed phrases in the corpus. This competition presentsa chance to benchmark your sentiment-analysis ideas on the Rotten Tomatoesdataset. You are asked to label phrases on a scale of five values: negative,somewhat negative, neutral, somewhat positive, positive. Obstacles likesentence negation, sarcasm, terseness, language ambiguity, and many othersmake this task very challenging. Kaggle is hosting this competition for themachine learning community to use for fun and practice. This competition wasinspired by the work of Socher et al [2]. We encourage participants to explorethe accompanying (and dare we say, fantastic) website that accompanies thepaper: http://nlp.stanford.edu/sentiment/ There you will find have sourcecode, a live demo, and even an online interface to help train the model. [1]Pang and L. Lee. 2005. Seeing stars: Exploiting class relationships forsentiment categorization with respect to rating scales. In ACL, pages 115–124.[2] Recursive Deep Models for Semantic Compositionality Over a SentimentTreebank, Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, ChrisManning, Andrew Ng and Chris Potts. Conference on Empirical Methods in NaturalLanguage Processing (EMNLP 2013)."	Submissions are evaluated on classification accuracy (the percent of labelsthat are predicted correctly) for every parsed phrase. The sentiment labelsare: 0 - negative 1 - somewhat negative 2 - neutral 3 - somewhat positive 4 -positive Submission Format For each phrase in the test set, predict a labelfor the sentiment. Your submission should have a header and look like thefollowing: PhraseId,Sentiment 156061,2 156062,2 156063,2 ...	['text data', 'multiclass classification']	
msk-redefining-cancer-treatment	A lot has been said during the past several years about how precision medicineand, more concretely, how genetic testing is going to disrupt the way diseaseslike cancer are treated. But this is only partially happening due to the hugeamount of manual work still required. Memorial Sloan Kettering Cancer Center(MSKCC) launched this competition, accepted by the NIPS 2017 CompetitionTrack, because we need your help to take personalized medicine to its fullpotential. Once sequenced, a cancer tumor can have thousands of geneticmutations. But the challenge is distinguishing the mutations that contributeto tumor growth (drivers) from the neutral mutations (passengers). Currentlythis interpretation of genetic mutations is being done manually. This is avery time-consuming task where a clinical pathologist has to manually reviewand classify every single genetic mutation based on evidence from text-basedclinical literature. For this competition MSKCC is making available an expert-annotated knowledge base where world-class researchers and oncologists havemanually annotated thousands of mutations. We need your help to develop aMachine Learning algorithm that, using this knowledge base as a baseline,automatically classifies genetic variations. Kaggle is excited to partner withresearch groups to push forward the frontier of machine learning. Researchcompetitions make use of Kaggle's platform and experience, but are largelyorganized by the research group's data science team. Any questions or concernsregarding the competition data, quality, or topic will be addressed by them.	Submissions are evaluated on Multi Class Log Loss between the predictedprobability and the observed target. Submission File For each ID in the testset, you must predict a probability for each of the different classes agenetic mutation can be classified on. The file should contain a header andhave the following format:ID,class1,class2,class3,class4,class5,class6,class7,class8,class90,0.1,0.7,0.05,0.05,0.1,0,0,0,0 1,0.7,0.1,0.05,0.05,0.1,0,0,0,02,0.05,0.05,0.1,0.7,0.1,0,0,0,0 3,0,0,0,0,0.05,0.05,0.1,0,7,0,1 etc.	['text data', 'multiclass classification', 'health sciences', 'human genetics']	
new-york-city-taxi-fare-prediction	In this playground competition, hosted in partnership with Google Cloud andCoursera, you are tasked with predicting the fare amount (inclusive of tolls)for a taxi ride in New York City given the pickup and dropoff locations. Whileyou can get a basic estimate based on just the distance between the twopoints, this will result in an RMSE of $5-$8, depending on the model used (seethe starter code for an example of this approach in Kernels). Your challengeis to do better than this using Machine Learning techniques! To learn how tohandle large datasets with ease and solve this problem using TensorFlow,consider taking the Machine Learning with TensorFlow on Google Cloud Platformspecialization on Coursera -- the taxi fare problem is one of several real-world problems that are used as case studies in the series of courses. To makethis easier, head to Coursera.org/NEXTextended to claim this specializationfor free for the first month!	The evaluation metric for this competition is the root mean-squared error orRMSE. RMSE measures the difference between the predictions of a model, and thecorresponding ground truth. A large RMSE is equivalent to a large averageerror, so smaller values of RMSE are better. One nice property of RMSE is thatthe error is given in the units being measured, so you can tell very directlyhow incorrect the model might be on unseen data. RMSE is given by: RMSE= √ 1 nn ∑ i=1 ( ˆ y i−yi)2 where yi is the i observation and ˆ y i is the predictionfor that observation. Example 1. Suppose we have one observation, with anactual value of 12.5 and a prediction of 12.5 (good job!). The RMSE will be:RMSEexample1= √ 1 1 (12.5−12.5)2 =0 Example 2. We'll add another data point.Your prediction for the second data point is 11.0 and the actual value is14.0. The RMSE will be: RMSEexample2= √ 1 2 ((12.5−12.5)2+(11.0−14.0)2) = √ 92 ≈2.12 Kernel Submissions You can make submissions directly from KaggleKernels. By adding your teammates as collaborators on a kernel, you can shareand edit code privately with them. Submission File For each key in the testset, you must predict a value for the fare_amount variable. The file shouldcontain a header and have the following format: key,fare_amount 2015-01-2713:08:24.0000002,11.00	['regression', 'tabular data']	
NFL-Punt-Analytics-Competition	Welcome In this challenge you'll notice there isn't a leaderboard, and you arenot required to develop a predictive model. This isn't a traditionalsupervised Kaggle machine learning competition. Instead, this challenge asksyou to use data to propose specific rule modifications for the NFL that aim toreduce the occurrence of concussions during punt plays. For more informationon this challenge format, see this forum thread. This challenge is part of NFL1st & Future, presented by Arrow Electronics – the NFL’s annual Super Bowlcompetition designed to spur innovation in player health, safety andperformance. The Challenge For the 2018 season, the NFL revised their kickoffrules in an effort to reduce the risk of injury during those plays. Byexamining injury reports, player position and velocity data, and game video,they were able to understand the game-play circumstances that may exacerbatethe risk of injury to players. This comprehensive review showed that over thecourse of all games during the 2015-2017 seasons, the kickoff represented onlysix percent of plays but 12 percent of concussions. Players had approximatelyfour times the risk of concussion on returned kickoffs compared to running orpassing plays. The changes to the kickoff rule aim to address the componentsthat posed the most risk, like the use of a two-man wedge. Now, the NFL ischallenging Kagglers to help them perform the same examination, this time onpunt play rules. They have provided data for all punt plays from the 2016 and2017 NFL seasons that includes player rosters, on-field position data andvideo data, including the plays in which a player suffered a concussion. Yourchallenge is to propose specific rule modifications (e.g. changes to theinitial formation, tackling techniques, blocking rules etc.), supported bydata, that may reduce the occurrence of concussions during punt plays. Moredetails on the entry criteria are available in Overview tab > Evaluation.About The NFL The National Football League is America's most popular sportsleague, comprised of 32 franchises that compete each year to win the SuperBowl, the world's biggest annual sporting event. Founded in 1920, the NFLdeveloped the model for the successful modern sports league, includingnational and international distribution, extensive revenue sharing,competitive excellence, and strong franchises across the country. The NFL iscommitted to advancing progress in the diagnosis, prevention and treatment ofsports-related injuries. The NFL's ongoing health and safety efforts includesupport for independent medical research and engineering advancements and acommitment to look at anything and everything to protect players and make thegame safer, including enhancements to medical protocols and improvements tohow our game is taught and played. As more is learned, the league evaluatesand changes rules to evolve the game and try to improve protections forplayers. Since 2002 alone, the NFL has made 50 rules changes intended toeliminate potentially dangerous tactics and reduce the risk of injuries. Formore information about the NFL's health and safety efforts, please visitwww.PlaySmartPlaySafe.com. Evaluation	Evaluation Your challenge is to propose specific rule modifications (e.g.changes to the initial formation, tackling techniques, blocking etc.),supported by data, that may reduce the occurrence of concussions during puntplays. A valid submission will include: Summary Slides: A summary of theproposed rule change presented in slide format and delivered as either a PDFor PPT file. Kernels Analysis: At least one kernel containing the analysis tosupport your proposal. All kernels submitted must be made public on or beforethe submission deadline to be eligible. If submitting as a team, all teammembers must be listed as collaborators on all kernels submitted. Submissionswill be judged by the NFL on the following criteria: Solution Efficacy - Haveyou clearly demonstrated, through your data analysis, that you have anunderstanding of what play features may be associated with concussions and howyour proposed rule change(s) will reduce these injuries? Your kernels shouldbe easy to understand and the analysis should be reproducible. Game Integrity- Is your proposal actionable by the NFL? Could they implement your rulechange and still maintain the integrity of the game? Have you considered theway your proposed changes to game dynamics could introduce new risks to playersafety? Strong submissions will demonstrate an understanding for the gameoverall.	['sports', 'health', 'american football', 'safety']	
noaa-fisheries-steller-sea-lion-population-count	Steller sea lions in the western Aleutian Islands have declined 94 percent inthe last 30 years. The endangered western population, found in the NorthPacific, are the focus of conservation efforts which require annual populationcounts. Specially trained scientists at NOAA Fisheries Alaska FisheriesScience Center conduct these surveys using airplanes and unoccupied aircraftsystems to collect aerial images. Having accurate population estimates enablesus to better understand factors that may be contributing to lack of recoveryof Stellers in this area. Currently, it takes biologists up to four months tocount sea lions from the thousands of images NOAA Fisheries collects eachyear. Once individual counts are conducted, the tallies must be reconciled toconfirm their reliability. The results of these counts are time-sensitive. Inthis competition, Kagglers are invited to develop algorithms which accuratelycount the number of sea lions in aerial photographs. Automating the annualpopulation count will free up critical resources allowing NOAA Fisheries tofocus on ensuring we hear the sea lion’s roar for many years to come. Plus,advancements in computer vision applied to aerial population counts may alsogreatly benefit other endangered species. Resources Learn more about researchbeing done to better understand what's going on with the endangered Stellersea lion populations by joining scientists on a research vessel to the westernAleutian Islands in the video below.	The aim of the competition is to count each type of steller sea lion in eachphoto. See the Data tab for more details. Your submission file should have thefollowing format:test_id,adult_males,subadult_males,adult_females,juveniles,pups 0,1,1,1,1,11,1,1,1,1,1 2,1,1,1,1,1 etc Your submissions will be evaluated by their RMSEfrom the human-labelled ground truth, averaged over the columns.	['image data', 'animals', 'oceanography', 'counting']	
nomad2018-predict-transparent-conductors	Innovative materials design is needed to tackle some of the most importanthealth, environmental, energy, social, and economic challenges of thiscentury. In particular, improving the properties of materials that areintrinsically connected to the generation and utilization of energy is crucialif we are to mitigate environmental damage due to a growing global demand.Transparent conductors are an important class of compounds that are bothelectrically conductive and have a low absorption in the visible range, whichare typically competing properties. A combination of both of thesecharacteristics is key for the operation of a variety of technological devicessuch as photovoltaic cells, light-emitting diodes for flat-panel displays,transistors, sensors, touch screens, and lasers. However, only a small numberof compounds are currently known to display both transparency and conductivitysuitable enough to be used as transparent conducting materials. Aluminum (Al),gallium (Ga), indium (In) sesquioxides are some of the most promisingtransparent conductors because of a combination of both large bandgapenergies, which leads to optical transparency over the visible range, and highconductivities. These materials are also chemically stable and relativelyinexpensive to produce. Alloying of these binary compounds in ternary orquaternary mixtures could enable the design of a new material at a specificcomposition with improved properties over what is current possible. Thesealloys are described by the formula (A l x G a y I n z ) 2N O 3N ( ; where x,y, and z can vary but are limited by the constraint x+y+z = 1. The totalnumber of atoms in the	Submissions are evaluated on the column-wise root mean squared logarithmicerror. The RMSLE for a single column calculated as 1 n ∑ i=1 n (log( p i+1)−log( a i +1) ) 2 − − − − − − − − − − − − − − − − − − − − − − − − − − √ ,where: n n is the total number of observations p i p is your prediction a i ais the actual value log(x) log is the natural logarithm of x x The final scoreis the mean of the RMSLE over all columns (in this case, 2). Submission FileFor each id in the test set, you must predict a value for bothformation_energy_ev_natom and bandgap_energy_ev. The file should contain aheader and have the following format:id,formation_energy_ev_natom,bandgap_energy_ev 1,0.1779,1.8892 2,0.1779,1.88923,0.1779,1.8892 ...	['chemistry', 'semiconductors']	
nyc-taxi-trip-duration	In this competition, Kaggle is challenging you to build a model that predictsthe total ride duration of taxi trips in New York City. Your primary datasetis one released by the NYC Taxi and Limousine Commission, which includespickup time, geo-coordinates, number of passengers, and several othervariables. Longtime Kagglers will recognize that this competition objective issimilar to the ECML/PKDD trip time challenge we hosted in 2015. But, thischallenge comes with a twist. Instead of awarding prizes to the top finisherson the leaderboard, this playground competition was created to rewardcollaboration and collective learning. We are encouraging you (with cashprizes!) to publish additional training data that other participants can usefor their predictions. We also have designated bi-weekly and final prizes toreward authors of kernels that are particularly insightful or valuable to thecommunity.	The evaluation metric for this competition is Root Mean Squared LogarithmicError. The RMSLE is calculated as ϵ= √ 1 n n ∑ i=1 (log(pi+1)−log(ai+1))2Where: ϵ is the RMSLE value (score) n is the total number of observations inthe (public/private) data set, pi is your prediction of trip duration, and aiis the actual trip duration for i . log(x) is the natural logarithm of xSubmission File For every row in the dataset, submission files should containtwo columns: id and trip_duration. The id corresponds to the column of that idin the test.csv. The file should contain a header and have the followingformat: id,trip_duration id00001,978 id00002,978 id00003,978 id00004,978 etc.	['regression', 'tabular data', 'taxi services']	
nyse			['finance', 'medium', 'featured']	Context This dataset is a playground for fundamental and technical analysis.It is said that 30% of traffic on stocks is already generated by machines, cantrading be fully automated? If not, there is still a lot to learn fromhistorical data. Content Dataset consists of following files: prices.csv: raw,as-is daily prices. Most of data spans from 2010 to the end 2016, forcompanies new on stock market date range is shorter. There have been approx.140 stock splits in that time, this set doesn't account for that. prices-split-adjusted.csv: same as prices, but there have been added adjustments forsplits. securities.csv: general description of each company with division onsectors fundamentals.csv: metrics extracted from annual SEC 10K fillings(2012-2016), should be enough to derive most of popular fundamentalindicators. Acknowledgements Prices were fetched from Yahoo Finance,fundamentals are from Nasdaq Financials, extended by some fields from EDGARSEC databases. Inspiration Here is couple of things one could try out withthis data: One day ahead prediction: Rolling Linear Regression, ARIMA, NeuralNetworks, LSTM Momentum/Mean-Reversion Strategies Security clustering,portfolio construction/hedging Which company has biggest chance of beingbankrupt? Which one is undervalued (how prices behaved afterwards), what isReturn on Investment?
outbrain-click-prediction	The internet is a stimulating treasure trove of possibility. Every day westumble on news stories relevant to our communities or experience theserendipity of finding an article covering our next travel destination.Outbrain, the web’s leading content discovery platform, delivers these momentswhile we surf our favorite sites. Currently, Outbrain pairs relevant contentwith curious readers in about 250 billion personalized recommendations everymonth across many thousands of sites. In this competition, Kagglers arechallenged to predict which pieces of content its global base of users arelikely to click on. Improving Outbrain’s recommendation algorithm will meanmore users uncover stories that satisfy their individual tastes.	Submissions are evaluated according to the Mean Average Precision @12(MAP@12): MAP@12= 1 |U| |U| ∑ u=1 min(12,n) ∑ k=1 P(k) where |U| is the numberof display_ids, P(k) is the precision at cutoff k, n is the number ofpredicted ad_ids. Submission File For each display_id in the test set, youmust predict a space-delimited list of ad_ids, ordered by decreasinglikelihood of being clicked. The candidate ad_ids for each display_id areprovided in clicks_test.csv. Note that each display_id can have a differentnumber of associated ads. The file should contain a header and have thefollowing format: display_id,ad_id 16874594,66758 150083 162754 170392 172888180797 16874595,8846 30609 143982 16874596,11430 57197 132820 153260 173005288385 289122 289915 etc.	['internet', 'tabular data', 'click prediction']	
passenger-screening-algorithm-challenge	While long lines and frantically shuffling luggage into plastic bins isn’t afun experience, airport security is a critical and necessary requirement forsafe travel. No one understands the need for both thorough security screeningsand short wait times more than U.S. Transportation Security Administration(TSA). They’re responsible for all U.S. airport security, screening more thantwo million passengers daily. As part of their Apex Screening at SpeedProgram, DHS has identified high false alarm rates as creating significantbottlenecks at the airport checkpoints. Whenever TSA’s sensors and algorithmspredict a potential threat, TSA staff needs to engage in a secondary, manualscreening process that slows everything down. And as the number of travelersincrease every year and new threats develop, their prediction algorithms needto continually improve to meet the increased demand. Currently, TSA purchasesupdated algorithms exclusively from the manufacturers of the scanningequipment used. These algorithms are proprietary, expensive, and oftenreleased in long cycles. In this competition, TSA is stepping outside theirestablished procurement process and is challenging the broader data sciencecommunity to help improve the accuracy of their threat prediction algorithms.Using a dataset of images collected on the latest generation of scanners,participants are challenged to identify the presence of simulated threatsunder a variety of object types, clothing types, and body types. Even a modestdecrease in false alarms will help TSA significantly improve the passengerexperience while maintaining high levels of security. This is a two-stagecompetition. Please read our two-stage FAQs to understand more about what thismeans. All persons contained in the dataset are volunteers who have agreed tohave their images used for this competition. The images may contain sensitivecontent. We kindly request that you conduct yourself with professionalism,respect, and maturity when working with this data.	For every scan in the dataset, you will be predicting the probability that athreat is present in each of 17 body zones. A diagram of the body zonelocations is available in the competition files section. The description ofeach zone is as follows: If there are N images, you will be making 17Npredictions. Submissions are scored on the log loss: − 1 N ∑ i=1 N [ y i log(y ^ i )+(1− y i )log(1− y ^ i )], where: N is the 17 * the number of scans inthe test set y ^ i is the predicted probability of the scan having a threat inthe given body zone y i is 1 if a threat is present, 0 otherwise log() is thenatural (base e) logarithm Note: the actual submitted predicted probabilitiesare replaced with max(min(p,1− 10 −15 ), 10 −15 ) . A smaller log loss isbetter. Submission File You must predict a probability for each Id and bodyzone. The Id used for the submission is created by concatenating the image Idwith the zone for which you are predicting ('_Zone1' through '_Zone17'). Thefile should have a header and be in the following format: Id,Probability0397026df63bbc8fd88f9860c6e35b4a_Zone1,0.0020397026df63bbc8fd88f9860c6e35b4a_Zone2,0.320397026df63bbc8fd88f9860c6e35b4a_Zone3,0.88 etc...	['image data', 'object detection', 'terrorism']	
paysim1			['finance', 'crime', 'medium', 'featured']	"Context There is a lack of public available datasets on financial services andspecially in the emerging mobile money transactions domain. Financial datasetsare important to many researchers and in particular to us performing researchin the domain of fraud detection. Part of the problem is the intrinsicallyprivate nature of financial transactions, that leads to no publicly availabledatasets. We present a synthetic dataset generated using the simulator calledPaySim as an approach to such a problem. PaySim uses aggregated data from theprivate dataset to generate a synthetic dataset that resembles the normaloperation of transactions and injects malicious behaviour to later evaluatethe performance of fraud detection methods. Content PaySim simulates mobilemoney transactions based on a sample of real transactions extracted from onemonth of financial logs from a mobile money service implemented in an Africancountry. The original logs were provided by a multinational company, who isthe provider of the mobile financial service which is currently running inmore than 14 countries all around the world. This synthetic dataset is scaleddown 1/4 of the original dataset and it is created just for Kaggle. HeadersThis is a sample of 1 row with headers explanation:1,PAYMENT,1060.31,C429214117,1089.0,28.69,M1591654462,0.0,0.0,0,0 step - mapsa unit of time in the real world. In this case 1 step is 1 hour of time. Totalsteps 744 (30 days simulation). type - CASH-IN, CASH-OUT, DEBIT, PAYMENT andTRANSFER. amount - amount of the transaction in local currency. nameOrig -customer who started the transaction oldbalanceOrg - initial balance beforethe transaction newbalanceOrig - new balance after the transaction nameDest -customer who is the recipient of the transaction oldbalanceDest - initialbalance recipient before the transaction. Note that there is not informationfor customers that start with M (Merchants). newbalanceDest - new balancerecipient after the transaction. Note that there is not information forcustomers that start with M (Merchants). isFraud - This is the transactionsmade by the fraudulent agents inside the simulation. In this specific datasetthe fraudulent behavior of the agents aims to profit by taking control orcustomers accounts and try to empty the funds by transferring to anotheraccount and then cashing out of the system. isFlaggedFraud - The businessmodel aims to control massive transfers from one account to another and flagsillegal attempts. An illegal attempt in this dataset is an attempt to transfermore than 200.000 in a single transaction. Past Research There are 5 similarfiles that contain the run of 5 different scenarios. These files are betterexplained at my PhD thesis chapter 7 (PhD Thesis Available herehttp://urn.kb.se/resolve?urn=urn:nbn:se:bth-12932). We ran PaySim severaltimes using random seeds for 744 steps, representing each hour of one month ofreal time, which matches the original logs. Each run took around 45 minutes onan i7 intel processor with 16GB of RAM. The final result of a run containsapproximately 24 million of financial records divided into the 5 types ofcategories: CASH-IN, CASH-OUT, DEBIT, PAYMENT and TRANSFER. AcknowledgementsThis work is part of the research project ”Scalable resource-efficient systemsfor big data analytics” funded by the Knowledge Foundation (grant: 20140032)in Sweden. Please refer to this dataset using the following citations: PaySimfirst paper of the simulator: E. A. Lopez-Rojas , A. Elmir, and S. Axelsson.""PaySim: A financial mobile money simulator for fraud detection"". In: The 28thEuropean Modeling and Simulation Symposium-EMSS, Larnaca, Cyprus. 2016"
petfinder-adoption-prediction	Millions of stray animals suffer on the streets or are euthanized in sheltersevery day around the world. If homes can be found for them, many preciouslives can be saved — and more happy families created. PetFinder.my has beenMalaysia’s leading animal welfare platform since 2008, with a database of morethan 150,000 animals. PetFinder collaborates closely with animal lovers,media, corporations, and global organizations to improve animal welfare.Animal adoption rates are strongly correlated to the metadata associated withtheir online profiles, such as descriptive text and photo characteristics. Asone example, PetFinder is currently experimenting with a simple AI tool calledthe Cuteness Meter, which ranks how cute a pet is based on qualities presentin their photos. In this competition you will be developing algorithms topredict the adoptability of pets - specifically, how quickly is a pet adopted?If successful, they will be adapted into AI tools that will guide shelters andrescuers around the world on improving their pet profiles' appeal, reducinganimal suffering and euthanization. Top participants may be invited tocollaborate on implementing their solutions into AI tools for assessing andimproving pet adoption performance, which will benefit global animal welfare.Important Note Be aware that this is being run as a Kernels Only Competition,requiring that all submissions be made via a Kernel output. Photo by KristaMangulsone on Unsplash	As we will be switching out test data to re-evaluate kernels on stage 2 datato populate the private leaderboard, submissions must be named submission.csvSubmissions are scored based on the quadratic weighted kappa, which measuresthe agreement between two ratings. This metric typically varies from 0 (randomagreement between raters) to 1 (complete agreement between raters). In theevent that there is less agreement between the raters than expected by chance,the metric may go below 0. The quadratic weighted kappa is calculated betweenthe scores which are expected/known and the predicted scores. Results have 5possible ratings, 0,1,2,3,4. The quadratic weighted kappa is calculated asfollows. First, an N x N histogram matrix O is constructed, such that Ocorresponds to the number of adoption records that have a rating of i (actual)and received a predicted rating j. An N-by-N matrix of weights, w, iscalculated based on the difference between actual and predicted rating scores:An N-by-N histogram matrix of expected ratings, E, is calculated, assumingthat there is no correlation between rating scores. This is calculated as theouter product between the actual rating's histogram vector of ratings and thepredicted rating's histogram vector of ratings, normalized such that E and Ohave the same sum. From these three matrices, the quadratic weighted kappa iscalculated as: PetID,AdoptionSpeed 378fcc4fc,3 73c10e136,2 72000c4c5,1e147a4b9f,4 etc..	['image data', 'text data']	
planet-understanding-the-amazon-from-space	Every minute, the world loses an area of forest the size of 48 footballfields. And deforestation in the Amazon Basin accounts for the largest share,contributing to reduced biodiversity, habitat loss, climate change, and otherdevastating effects. But better data about the location of deforestation andhuman encroachment on forests can help governments and local stakeholdersrespond more quickly and effectively. Planet, designer and builder of theworld’s largest constellation of Earth-imaging satellites, will soon becollecting daily imagery of the entire land surface of the earth at 3-5 meterresolution. While considerable research has been devoted to tracking changesin forests, it typically depends on coarse-resolution imagery from Landsat (30meter pixels) or MODIS (250 meter pixels). This limits its effectiveness inareas where small-scale deforestation or forest degradation dominate.Furthermore, these existing methods generally cannot differentiate betweenhuman causes of forest loss and natural causes. Higher resolution imagery hasalready been shown to be exceptionally good at this, but robust methods havenot yet been developed for Planet imagery. In this competition, Planet and itsBrazilian partner SCCON are challenging Kagglers to label satellite imagechips with atmospheric conditions and various classes of land cover/land use.Resulting algorithms will help the global community better understand where,how, and why deforestation happens all over the world - and ultimately how torespond. To dig into/explore more Planet data, sign up for a free account. Andif you're interested in building applications on Planet data, check out ourApplication Developer Program. Getting Started Review the data page, whichincludes detailed information about the labels and the labeling process.Download a subsample of the data to get familiar with how it looks. Explorethe subsample on Kernels. We’ve created a notebook for you to get started.	Submissions will be evaluated based on their mean (F_{2}) score. The F score,commonly used in information retrieval, measures accuracy using the precisionp and recall r. Precision is the ratio of true positives (tp) to all predictedpositives (tp + fp). Recall is the ratio of true positives to all actualpositives (tp + fn). The (F_{2}) score is given by (1+ β 2 ) pr β 2 p+r wherep= tp tp+fp , r= tp tp+fn , β=2. Note that the (F_{2}) score weights recallhigher than precision. The mean (F_{2}) score is formed by averaging theindividual (F_{2}) scores for each row in the test set. Submission File Foreach image listed in the test set, predict a space-delimited list of tagswhich you believe are associated with the image. There are 17 possible tags:agriculture, artisinal_mine, bare_ground, blooming, blow_down, clear, cloudy,conventional_mine, cultivation, habitation, haze, partly_cloudy, primary,road, selective_logging, slash_burn, water. The file should contain a headerand have the following format: image_name,tags test_0,agriculture road watertest_1,primary clear test_2,haze primary etc.	['image data', 'object identification', 'ecology', 'forestry']	
plant-seedlings-classification	Can you differentiate a weed from a crop seedling? The ability to do soeffectively can mean better crop yields and better stewardship of theenvironment. The Aarhus University Signal Processing group, in collaborationwith University of Southern Denmark, has recently released a datasetcontaining images of approximately 960 unique plants belonging to 12 speciesat several growth stages. We're hosting this dataset as a Kaggle competitionin order to give it wider exposure, to give the community an opportunity toexperiment with different image recognition techniques, as well to provide aplace to cross-pollenate ideas. Acknowledgments We extend our appreciation tothe Aarhus University Department of Engineering Signal Processing Group forhosting the original data. Citation A Public Image Database for Benchmark ofPlant Seedling Classification Algorithms	Submissions are evaluated on MeanFScore, which at Kaggle is actually a micro-averaged F1-score. Given positive/negative rates for each class k, theresulting score is computed this way: Precisio n micro = ∑ k∈C T P k ∑ k∈C T Pk +F P k Recal l micro = ∑ k∈C T P k ∑ k∈C T P k +F N k F1-score is theharmonic mean of precision and recall MeanFScore=F 1 micro = 2Precisio n microRecal l micro Precisio n micro +Recal l microMeanFScore=F1micro=2PrecisionmicroRecallmicroPrecisionm Submission File Foreachfile in the test set, you must predict a probability for the speciesvariable. The file should contain a header and have the following format:file,species 0021e90e4.png,Maize 003d61042.png,Sugar beet 007b3da8b.png,Commonwheat etc.	['image data', 'multiclass classification', 'plants']	
PLAsTiCC-2018	Help some of the world's leading astronomers grasp the deepest properties ofthe universe. The human eye has been the arbiter for the classification ofastronomical sources in the night sky for hundreds of years. But a newfacility -- the Large Synoptic Survey Telescope (LSST) -- is about torevolutionize the field, discovering 10 to 100 times more astronomical sourcesthat vary in the night sky than we've ever known. Some of these sources willbe completely unprecedented! The Photometric LSST Astronomical Time-SeriesClassification Challenge (PLAsTiCC) asks Kagglers to help prepare to classifythe data from this new survey. Competitors will classify astronomical sourcesthat vary with time into different classes, scaling from a small training setto a very large test set of the type the LSST will discover. More backgroundinformation is available here. Acknowledgements PLAsTiCC is funded throughLSST Corporation Grant Award # 2017-03 and administered by the University ofToronto. Financial support for LSST comes from the National Science Foundation(NSF) through Cooperative Agreement No. 1258333, the Department of Energy(DOE) Office of Science under Contract No. DE-AC02-76SF00515, and privatefunding raised by the LSST Corporation. The NSF-funded LSST Project Office forconstruction was established as an operating center under management of theAssociation of Universities for Research in Astronomy (AURA). The DOE-fundedeffort to build the LSST camera is managed by the SLAC National AcceleratorLaboratory (SLAC). The National Science Foundation (NSF) is an independentfederal agency created by Congress in 1950 to promote the progress of science.NSF supports basic research and people to create knowledge that transforms thefuture. Photo Credit: M. Park/Inigo Films/LSST/AURA/NSF [1]:https://arxiv.org/abs/1810.00001	Submissions are evaluated using a weighted multi-class logarithmic loss. Theoverall effect is such that each class is roughly equally important for thefinal score. Each object has been labeled with one type. For each object, youmust submit a set of predicted probabilities (one for every category). Theformula is then Log Loss=−( ∑ M i=1 wi⋅∑ Ni j=1 yij Ni ⋅lnpij ∑ M i=1 wi )where N is the number of objects in the class set, M is the number of classes,ln is the natural logarithm, yij is 1 if observation (i) belongs to class (j)and 0 otherwise, pij is the predicted probability that observation i belongsto class j . The submitted probabilities for a given object are not requiredto sum to one because they are rescaled prior to being scored (each row isdivided by the row sum). In order to avoid the extremes of the log function,predicted probabilities are replaced with max(min(p,1−10−15),10−15) .Submission File For each object ID in the test set, you must predict aprobability for each of the different possible classes. The file shouldcontain a header and have the following format:object_id,class_6,class_15,class_16,class_42,class_52,class_53,class_62,class_64,class_65,class_67,class_88,class_90,class_92,class_95,class_9913,0,0.1,0,0.1,0,0.3,0,0,0,0,0,0.5,0,0,0 14,0,0,0,0,0,0,0,0,0,0,0,1,0,0,017,0.75,0.23,0,0,0.01,0,0,0,0,0.01,0,0,0,0,0 etc.	['time series', 'tabular data', 'astronomy']	
porto-seguro-safe-driver-prediction	Nothing ruins the thrill of buying a brand new car more quickly than seeingyour new insurance bill. The sting’s even more painful when you know you’re agood driver. It doesn’t seem fair that you have to pay so much if you’ve beencautious on the road for years. Porto Seguro, one of Brazil’s largest auto andhomeowner insurance companies, completely agrees. Inaccuracies in carinsurance company’s claim predictions raise the cost of insurance for gooddrivers and reduce the price for bad ones. In this competition, you’rechallenged to build a model that predicts the probability that a driver willinitiate an auto insurance claim in the next year. While Porto Seguro has usedmachine learning for the past 20 years, they’re looking to Kaggle’s machinelearning community to explore new, more powerful methods. A more accurateprediction will allow them to further tailor their prices, and hopefully makeauto insurance coverage more accessible to more drivers.	Scoring Metric Submissions are evaluated using the Normalized GiniCoefficient. During scoring, observations are sorted from the largest to thesmallest predictions. Predictions are only used for ordering observations;therefore, the relative magnitude of the predictions are not used duringscoring. The scoring algorithm then compares the cumulative proportion ofpositive class observations to a theoretical uniform proportion. The GiniCoefficient ranges from approximately 0 for random guessing, to approximately0.5 for a perfect score. The theoretical maximum for the discrete calculationis (1 - frac_pos) / 2. The Normalized Gini Coefficient adjusts the score bythe theoretical maximum so that the maximum score is 1. The code to calculateNormalized Gini Coefficient in a number of different languages can be found inthis forum thread. Submission File For each id in the test set, you mustpredict a probability of an insurance claim in the target column. The fileshould contain a header and have the following format: id,target 0,0.1 1,0.92,1.0 etc.	['binary classification', 'tabular data', 'normalizedgini', 'extra small']	
price-volume-data-for-all-us-stocks-etfs			['finance', 'economics', 'business', 'artificial intelligence', 'medium', 'featured']	Context High-quality financial data is expensive to acquire and is thereforerarely shared for free. Here I provide the full historical daily price andvolume data for all US-based stocks and ETFs trading on the NYSE, NASDAQ, andNYSE MKT. It's one of the best datasets of its kind you can obtain. ContentThe data (last updated 11/10/2017) is presented in CSV format as follows:Date, Open, High, Low, Close, Volume, OpenInt. Note that prices have beenadjusted for dividends and splits. Acknowledgements This dataset belongs tome. I’m sharing it here for free. You may do with it as you wish. InspirationMany have tried, but most have failed, to predict the stock market's ups anddowns. Can you do any better?
prudential-life-insurance-assessment	Picture this. You are a data scientist in a start-up culture with thepotential to have a very large impact on the business. Oh, and you are backedup by a company with 140 years' business experience. Curious? Great! You arethe kind of person we are looking for. Prudential, one of the largest issuersof life insurance in the USA, is hiring passionate data scientists to join anewly-formed Data Science group solving complex challenges and identifyingopportunities. The results have been impressive so far but we want more. TheChallenge In a one-click shopping world with on-demand everything, the lifeinsurance application process is antiquated. Customers provide extensiveinformation to identify risk classification and eligibility, includingscheduling medical exams, a process that takes an average of 30 days. Theresult? People are turned off. That’s why only 40% of U.S. households ownindividual life insurance. Prudential wants to make it quicker and less laborintensive for new and existing customers to get a quote while maintainingprivacy boundaries. By developing a predictive model that accuratelyclassifies risk using a more automated approach, you can greatly impact publicperception of the industry. The results will help Prudential better understandthe predictive power of the data points in the existing assessment, enablingus to significantly streamline the process.	Submissions are scored based on the quadratic weighted kappa, which measuresthe agreement between two ratings. This metric typically varies from 0 (randomagreement) to 1 (complete agreement). In the event that there is lessagreement between the raters than expected by chance, this metric may go below0. The response variable has 8 possible ratings. Each application ischaracterized by a tuple (ea,eb), which corresponds to its scores by Rater A(actual risk) and Rater B (predicted risk). The quadratic weighted kappa iscalculated as follows. First, an N x N histogram matrix O is constructed, suchthat Oi,j corresponds to the number of applications that received a rating iby A and a rating j by B. An N-by-N matrix of weights, w, is calculated basedon the difference between raters' scores: w i,j = (i−j) 2 (N−1) 2wi,j=(i−j)2(N−1)2 An N-by-N histogram matrix of expected ratings, E, iscalculated, assuming that there is no correlation between rating scores. Thisis calculated as the outer product between each rater's histogram vector ofratings, normalized such that E and O have the same sum. From these threematrices, the quadratic weighted kappa is calculated as: κ=1− ∑ i,j w i,j Oi,j ∑ i,j w i,j E i,j . Submission File For each Id in the test set, you mustpredict the Response variable. The file should contain a header and have thefollowing format: Id,Response 1,4 3,8 4,3 etc.	['tabular data', 'ranking', 'quadraticweightedkappa', 'extra small']	
pubg-finish-placement-prediction	"So, where we droppin' boys and girls? Battle Royale-style video games havetaken the world by storm. 100 players are dropped onto an island empty-handedand must explore, scavenge, and eliminate other players until only one is leftstanding, all while the play zone continues to shrink. PlayerUnknown'sBattleGrounds (PUBG) has enjoyed massive popularity. With over 50 millioncopies sold, it's the fifth best selling game of all time, and has millions ofactive monthly players. The team at PUBG has made official game data availablefor the public to explore and scavenge outside of ""The Blue Circle."" Thiscompetition is not an official or affiliated PUBG site - Kaggle collected datamade possible through the PUBG Developer API. You are given over 65,000 games'worth of anonymized player data, split into training and testing sets, andasked to predict final placement from final in-game stats and initial playerratings. What's the best strategy to win in PUBG? Should you sit in one spotand hide your way into victory, or do you need to be the top shot? Let's letthe data do the talking!"	Submissions are evaluated on Mean Absolute Error between your predictedwinPlacePerc and the observed winPlacePerc. Submission File For each Id in thetest set, you must predict their placement as a percentage (0 for last, 1 forfirst place) for the winPlacePerc variable. The file should contain a headerand have the following format: Id,winPlacePerc 47734,0 47735,0.5 47736,047737,1 etc. See sample_submission.csv on the data page for a full samplesubmission.	['video games', 'tabular data']	
quickdraw-doodle-recognition	"""Quick, Draw!"" was released as an experimental game to educate the public in aplayful way about how AI works. The game prompts users to draw an imagedepicting a certain category, such as ”banana,” “table,” etc. The gamegenerated more than 1B drawings, of which a subset was publicly released asthe basis for this competition’s training set. That subset contains 50Mdrawings encompassing 340 label categories. Sounds fun, right? Here's thechallenge: since the training data comes from the game itself, drawings can beincomplete or may not match the label. You’ll need to build a recognizer thatcan effectively learn from this noisy data and perform well on a manually-labeled test set from a different distribution. Your task is to build a betterclassifier for the existing Quick, Draw! dataset. By advancing models on thisdataset, Kagglers can improve pattern recognition solutions more broadly. Thiswill have an immediate impact on handwriting recognition and its robustapplications in areas including OCR (Optical Character Recognition), ASR(Automatic Speech Recognition) & NLP (Natural Language Processing)."	"Submissions are evaluated according to the Mean Average Precision @ 3 (MAP@3):MAP@3= 1 U U ∑ u=1 min(n,3) ∑ k=1 P(k) where U is the number of scoreddrawings in the test data, P(k) is the precision at cutoff k, and n is thenumber predictions per drawing. You can learn more about this metric worksfrom this kernel, and from this python code. Kernel Submissions You can makesubmissions directly from Kaggle Kernels. By adding your teammates ascollaborators on a kernel, you can share and edit code privately with them.Submission File For each key_id in the test set, you should predict up to 3word values. The file should contain a header and have the following format.IMPORTANT: Some ""words"" are actually more than one word! The training dataaligns to the Quick Draw dataset that that was previously released, and usesspaces to delimit multi-word labels. The Kaggle metric for this competitionrequires labels with no spaces, so you will need to adjust your labelpredictions to replace spaces with underscores. For example, ""roller coaster""should be predicted as ""roller_coaster"". key_id,word9000003627287624,The_Eiffel_Tower airplane donut9000010688666847,The_Eiffel_Tower airplane donut etc."	['image data', 'writing']	
quora-insincere-questions-classification	An existential problem for any major website today is how to handle toxic anddivisive content. Quora wants to tackle this problem head-on to keep theirplatform a place where users can feel safe sharing their knowledge with theworld. Quora is a platform that empowers people to learn from each other. OnQuora, people can ask questions and connect with others who contribute uniqueinsights and quality answers. A key challenge is to weed out insincerequestions -- those founded upon false premises, or that intend to make astatement rather than look for helpful answers. In this competition, Kagglerswill develop models that identify and flag insincere questions. To date, Quorahas employed both machine learning and manual review to address this problem.With your help, they can develop more scalable methods to detect toxic andmisleading content. Here's your chance to combat online trolls at scale. HelpQuora uphold their policy of “Be Nice, Be Respectful” and continue to be aplace for sharing and growing the world’s knowledge. Important Note Be awarethat this is being run as a Kernels Only Competition, requiring that allsubmissions be made via a Kernel output. Please read the Kernels FAQ and thedata page very carefully to fully understand how this is designed.	Submissions are evaluated on F1 Score between the predicted and the observedtargets. Submission File For each qid in the test set, you must predictwhether the corresponding question_text is insincere (1) or not (0).Predictions should only be the integers 0 or 1. The file should contain aheader and have the following format: qid,prediction 0000163e3ea7c7a74cd7,000002bd4fb5d505b9161,0 00007756b4a147d2b0b3,0 ... Kernel Submissions For thiscompetition, you will make submissions directly from Kaggle Kernels. By addingyour teammates as collaborators on a kernel, you can share and edit codeprivately with them. For more details, please visit the Kernels-FAQ for thiscompetition.	['text data', 'binary classification']	
quora-question-pairs	Where else but Quora can a physicist help a chef with a math problem and getcooking tips in return? Quora is a place to gain and share knowledge—aboutanything. It’s a platform to ask questions and connect with people whocontribute unique insights and quality answers. This empowers people to learnfrom each other and to better understand the world. Over 100 million peoplevisit Quora every month, so it's no surprise that many people ask similarlyworded questions. Multiple questions with the same intent can cause seekers tospend more time finding the best answer to their question, and make writersfeel they need to answer multiple versions of the same question. Quora valuescanonical questions because they provide a better experience to active seekersand writers, and offer more value to both of these groups in the long term.Currently, Quora uses a Random Forest model to identify duplicate questions.In this competition, Kagglers are challenged to tackle this natural languageprocessing problem by applying advanced techniques to classify whetherquestion pairs are duplicates or not. Doing so will make it easier to findhigh quality answers to questions resulting in an improved experience forQuora writers, seekers, and readers.	Submissions are evaluated on the log loss between the predicted values and theground truth. Submission File For each ID in the test set, you must predictthe probability that the questions are duplicates (a number between 0 and 1).The file should contain a header and have the following format:test_id,is_duplicate 0,0.5 1,0.4 2,0.9 etc.	['internet', 'linguistics', 'text data', 'tabular data', 'duplicate detection']	
recruit-restaurant-visitor-forecasting	Running a thriving local restaurant isn't always as charming as firstimpressions appear. There are often all sorts of unexpected troubles poppingup that could hurt business. One common predicament is that restaurants needto know how many customers to expect each day to effectively purchaseingredients and schedule staff members. This forecast isn't easy to makebecause many unpredictable factors affect restaurant attendance, like weatherand local competition. It's even harder for newer restaurants with littlehistorical data. Recruit Holdings has unique access to key datasets that couldmake automated future customer prediction possible. Specifically, RecruitHoldings owns Hot Pepper Gourmet (a restaurant review service), AirREGI (arestaurant point of sales service), and Restaurant Board (reservation logmanagement software). In this competition, you're challenged to usereservation and visitation data to predict the total number of visitors to arestaurant for future dates. This information will help restaurants be muchmore efficient and allow them to focus on creating an enjoyable diningexperience for their customers.	Submissions are evaluated on the root mean squared logarithmic error. TheRMSLE is calculated as 1 n ∑ i=1 n (log( p i +1)−log( a i +1) ) 2 − − − − − −− − − − − − − − − − − − − − − − − − − − √ , where: n is the total number ofobservations p i is your prediction of visitors a i is the actual number ofvisitors log(x) is the natural logarithm of x Submission File For every storeand date combination in the test set, submission files should contain twocolumns: id and visitors. The id is formed by concatenating the air_store_idand visit_date with an underscore. The file should contain a header and havethe following format: id,visitors air_00a91d42b08b08d9_2017-04-23,0air_00a91d42b08b08d9_2017-04-24,0 air_00a91d42b08b08d9_2017-04-25,0 etc.	[]	
reducing-commercial-aviation-fatalities	Most flight-related fatalities stem from a loss of “airplane state awareness.”That is, ineffective attention management on the part of pilots who may bedistracted, sleepy or in other dangerous cognitive states. Your challenge isto build a model to detect troubling events from aircrew’s physiological data.You'll use data acquired from actual pilots in test situations, and yourmodels should be able to run calculations in real time to monitor thecognitive states of pilots. With your help, pilots could then be alerted whenthey enter a troubling state, preventing accidents and saving lives. Reducingaircraft fatalities is just one of the complex problems that Booz AllenHamilton has been solving for business, government, and military leaders forover 100 years. Through devotion, candor, courage, and character, they produceoriginal solutions where there are no roadmaps. Now you can help them findanswers, save lives, and change the world.	Submissions are evaluated on the Multi Class Log Loss between the predictedprobabilities and the observed target. The submitted probabilities are notrequired to sum to one because they are rescaled prior to being scored (eachrow is divided by the row sum). In order to avoid the extremes of the logfunction, predicted probabilities are replaced with max(min(p,1− 10 −15 ), 10−15 ) m . Submission File For each id in the test set, you must predict aprobability for each of the 4 possible cognitive states ( A = baseline / noevent, B = SS, C = CA, D = DA). The file must have a header and should looklike the following: id,A,B,C,D 0,1,0,0,0 1,1,0,0,0 2,1,0,0,0 etc.	[]	
restaurant-revenue-prediction	With over 1,200 quick service restaurants across the globe, TFI is the companybehind some of the world's most well-known brands: Burger King, Sbarro,Popeyes, Usta Donerci, and Arby’s. They employ over 20,000 people in Europeand Asia and make significant daily investments in developing new restaurantsites. Right now, deciding when and where to open new restaurants is largely asubjective process based on the personal judgement and experience ofdevelopment teams. This subjective data is difficult to accurately extrapolateacross geographies and cultures. New restaurant sites take large investmentsof time and capital to get up and running. When the wrong location for arestaurant brand is chosen, the site closes within 18 months and operatinglosses are incurred. Finding a mathematical model to increase theeffectiveness of investments in new restaurant sites would allow TFI to investmore in other important business areas, like sustainability, innovation, andtraining for new employees. Using demographic, real estate, and commercialdata, this competition challenges you to predict the annual restaurant salesof 100,000 regional locations. TFI would love to hire an expert Kaggler likeyou to head up their growing data science team in Istanbul or Shanghai. You'dbe tackling problems like the one featured in this competition on a globalscale. See the job description here >>	Root Mean Squared Error (RMSE) Submissions are scored on the root mean squarederror. RMSE is very common and is a suitable general-purpose error metric.Compared to the Mean Absolute Error, RMSE punishes large errors: RMSE= √ 1 n n∑ i=1 (yi− ˆ y i)2 , where y hat is the predicted value and y is the originalvalue. Submission File For every restaurant in the dataset, submission filesshould contain two columns: Id and Prediction. The file should contain aheader and have the following format: Id,Prediction 0,1.0 1,1.0 2,1.0 etc.	['regression', 'tabular data', 'rmse', 'extra small']	
rsna-pneumonia-detection-challenge	In this competition, you’re challenged to build an algorithm to detect avisual signal for pneumonia in medical images. Specifically, your algorithmneeds to automatically locate lung opacities on chest radiographs. Here’s thebackstory and why solving this problem matters. Pneumonia accounts for over15% of all deaths of children under 5 years old internationally. In 2015,920,000 children under the age of 5 died from the disease. In the UnitedStates, pneumonia accounts for over 500,000 visits to emergency departments[1] and over 50,000 deaths in 2015 [2], keeping the ailment on the list of top10 causes of death in the country. While common, accurately diagnosingpneumonia is a tall order. It requires review of a chest radiograph (CXR) byhighly trained specialists and confirmation through clinical history, vitalsigns and laboratory exams. Pneumonia usually manifests as an area or areas ofincreased opacity [3] on CXR. However, the diagnosis of pneumonia on CXR iscomplicated because of a number of other conditions in the lungs such as fluidoverload (pulmonary edema), bleeding, volume loss (atelectasis or collapse),lung cancer, or post-radiation or surgical changes. Outside of the lungs,fluid in the pleural space (pleural effusion) also appears as increasedopacity on CXR. When available, comparison of CXRs of the patient taken atdifferent time points and correlation with clinical symptoms and history arehelpful in making the diagnosis. CXRs are the most commonly performeddiagnostic imaging study. A number of factors such as positioning of thepatient and depth of inspiration can alter the appearance of the CXR [4],complicating interpretation further. In addition, clinicians are faced withreading high volumes of images every shift. To improve the efficiency andreach of diagnostic services, the Radiological Society of North America(RSNA®) has reached out to Kaggle’s machine learning community andcollaborated with the US National Institutes of Health, The Society ofThoracic Radiology, and MD.ai to develop a rich dataset for this challenge.The RSNA is an international society of radiologists, medical physicists andother medical professionals with more than 54,000 members from 146 countriesacross the globe. They see the potential for ML to automate initial detection(imaging screening) of potential pneumonia cases in order to prioritize andexpedite their review. Challenge participants may be invited to present theirAI models and methodologies during an award ceremony at the RSNA AnnualMeeting which will be held in Chicago, Illinois, USA, from November 25-30,2018. Acknowledgements Thank you to the National Institutes of Health ClinicalCenter for publicly providing the Chest X-Ray dataset [5]. NIH News release:NIH Clinical Center provides one of the largest publicly available chest x-raydatasets to scientific community Original source files and documents Also, abig thank you to the competition organizers! References Rui P, Kang K.National Ambulatory Medical Care Survey: 2015 Emergency Department SummaryTables. Table 27. Available from:www.cdc.gov/nchs/data/nhamcs/web_tables/2015_ed_web_tables.pdf Deaths: FinalData for 2015. Supplemental Tables. Tables I-21, I-22. Available from:www.cdc.gov/nchs/data/nvsr/nvsr66/nvsr66_06_tables.pdf Franquet T. Imaging ofcommunity-acquired pneumonia. J Thorac Imaging 2018 (epub ahead of print).PMID 30036297 Kelly B. The Chest Radiograph. Ulster Med J 2012;81(3):143-148Wang X, Peng Y, Lu L, Lu Z, Bagheri M, Summers RM. ChestX-ray8: Hospital-scaleChest X-ray Database and Benchmarks on Weakly-Supervised Classification andLocalization of Common Thorax Diseases. IEEE CVPR 2017,http://openaccess.thecvf.com/content_cvpr_2017/papers/Wang_ChestX-ray8_Hospital-Scale_Chest_CVPR_2017_paper.pdf	"This competition is evaluated on the mean average precision at differentintersection over union (IoU) thresholds. The IoU of a set of predictedbounding boxes and ground truth bounding boxes is calculated as: IoU(A,B)= A∩BA∪B . The metric sweeps over a range of IoU thresholds, at each pointcalculating an average precision value. The threshold values range from 0.4 to0.75 with a step size of 0.05: (0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75).In other words, at a threshold of 0.5, a predicted object is considered a""hit"" if its intersection over union with a ground truth object is greaterthan 0.5. At each threshold value t , a precision value is calculated based onthe number of true positives (TP), false negatives (FN), and false positives(FP) resulting from comparing the predicted object to all ground truthobjects: TP(t) TP(t)+FP(t)+FN(t) . A true positive is counted when a singlepredicted object matches a ground truth object with an IoU above thethreshold. A false positive indicates a predicted object had no associatedground truth object. A false negative indicates a ground truth object had noassociated predicted object. Important note: if there are no ground truthobjects at all for a given image, ANY number of predictions (false positives)will result in the image receiving a score of zero, and being included in themean average precision. The average precision of a single image is calculatedas the mean of the above precision values at each IoU threshold: 1|thresholds| ∑ t TP(t) TP(t)+FP(t)+FN(t) . In your submission, you are alsoasked to provide a confidence level for each bounding box. Bounding boxes willbe evaluated in order of their confidence levels in the above process. Thismeans that bounding boxes with higher confidence will be checked first formatches against solutions, which determines what boxes are considered true andfalse positives. NOTE: In nearly all cases confidence will have no impact onscoring. It exists primarily to allow for submission boxes to be evaluated ina particular order to resolve extreme edge cases. None of these edge cases areknown to exist in the data set. If you do not wish to use or calculateconfidence you can use a placeholder value - like 1.0 - to indicate that noparticular order applies to the evaluation of your submission boxes. Lastly,the score returned by the competition metric is the mean taken over theindividual average precisions of each image in the test dataset. Intersectionover Union (IoU) Intersection over Union is a measure of the magnitude ofoverlap between two bounding boxes (or, in the more general case, twoobjects). It calculates the size of the overlap between two objects, dividedby the total area of the two objects combined. It can be visualized as thefollowing: The two boxes in the visualization overlap, but the area of theoverlap is insubstantial compared with the area taken up by both objectstogether. IoU would be low - and would likely not count as a ""hit"" at higherIoU thresholds. Submission File The submission format requires a spacedelimited set of bounding boxes. For example:0004cfab-14fd-4e49-80ba-63a80b6bddd6,0.5 0 0 100 100 indicates that image0004cfab-14fd-4e49-80ba-63a80b6bddd6 has a bounding box with a confidence of0.5, at x == 0 and y == 0, with a width and height of 100. The file shouldcontain a header and have the following format. Each row in your submissionshould contain ALL bounding boxes for a given image.patientId,PredictionString 0004cfab-14fd-4e49-80ba-63a80b6bddd6,0.5 0 0 100100 00313ee0-9eaa-42f4-b0ab-c148ed3241cd,00322d4d-1c29-4943-afc9-b6754be640eb,0.8 10 10 50 50 0.75 100 100 5 5 etc..."	['image data', 'medicine']	
santa-gift-matching	‘Tis the night before Christmas year: two thousand seventeen. Santa’s growngrouchy, borderline mean. What used to be simple for Old St. Nick, is now toopuzzling, it’s making him sick! See, Santa always knew, deep down in his gut,what toy each kid wanted–no ifs, ands, or buts. But fierce population growth,more twins, and toy innovation, has left too complex a problem, in dire needof optimization. “Don’t worry, Mr. Santa”, said an Elf named McMaggle, “I havea solution! Have you heard of Kaggle?” As she explained Kaggle in-depth,Santa’s doubt began turning, he became a believer in the magic of...machinelearning. So, Santa’s team needs YOU more than ever this year, to solve thispainful problem and save Christmas cheer. The Challenge In this playgroundcompetition, you’re challenged to build a toy matching algorithm thatmaximizes happiness by pairing kids with toys they want. In the dataset, eachkid has 10 preferences for their gift (from 1000) and Santa has 1000 preferredkids for every gift available. What makes this extra difficult is that 0.4% ofthe kids are twins, and by their parents’ request, require the same gift.	Your goal is to maximize the Average Normalized Happiness (ANH) =(AverageNormalizedChildHappiness (ANCH) ) ^ 3 +(AverageNormalizedSantaHappiness (ANSH) ) ^ 3 where NormalizedChildHappinessis the happiness of each child, divided by the maximum possible happiness, andNormalizedSantaHappiness is the happiness of each gift, divided by the maximumpossible happiness. Note the cubic terms with ANCH and ANSH. in the equationform: ANCH= 1 n c ∑ i=0 n c −1 ChildHappiness MaxChildHappiness , ANSH= 1 n g∑ i=0 n g −1 GiftHappiness MaxGiftHappiness . n c is the number of children. ng is the number of gifts MaxChildHappiness = len(ChildWishList) * 2,MaxGiftHappiness = len(GiftGoodKidsList) * 2. ChildHappiness = 2 * GiftOrderif the gift is found in the wish list of the child. ChildHappiness = -1 if thegift is out of the child's wish list. Similarly, GiftHappiness = 2 *ChildOrder if the child is found in the good kids list of the gift.GiftHappiness = -1 if the child is out of the gift's good kids list. Forexample, if a child has a preference of gifts [5,2,3,1,4], and is given gift3, then ChildHappiness = [len(WishList)-indexOf(gift_3)] * 2 = [5 - 2] * 2 = 6If this child is given gift 4, then ChildHappiness = [5-4] * 2 = 2 Code sampleof Average Normalized Happiness can be seen from this Kernel. Submission FileFor each child in the dataset, you will match it with a gift. Remember, thefirst 0.5% of rows (ChildId 0 to 5000) are triplets, and the following 4%(ChildId 5001-45000) are twins. ChildId,GiftId 0,669 1,669 2,669 3,8 4,8 5,86,689 7,689 8,689	['mathematical optimization']	
santander-customer-transaction-prediction	At Santander our mission is to help people and businesses prosper. We arealways looking for ways to help our customers understand their financialhealth and identify which products and services might help them achieve theirmonetary goals. Our data science team is continually challenging our machinelearning algorithms, working with the global data science community to makesure we can more accurately identify new ways to solve our most commonchallenge, binary classification problems such as: is a customer satisfied?Will a customer buy this product? Can a customer pay this loan? In thischallenge, we invite Kagglers to help us identify which customers will make aspecific transaction in the future, irrespective of the amount of moneytransacted. The data provided for this competition has the same structure asthe real data we have available to solve this problem.	Submissions are evaluated on area under the ROC curve between the predictedprobability and the observed target. Submission File For each Id in the testset, you must make a binary prediction of the target variable. The file shouldcontain a header and have the following format: ID_code,target test_0,0test_1,1 test_2,0 etc.	['binary classification', 'tabular data', 'banking']	
santander-product-recommendation	Ready to make a downpayment on your first house? Or looking to leverage theequity in the home you have? To support needs for a range of financialdecisions, Santander Bank offers a lending hand to their customers throughpersonalized product recommendations. Under their current system, a smallnumber of Santander’s customers receive many recommendations while many othersrarely see any resulting in an uneven customer experience. In their secondcompetition, Santander is challenging Kagglers to predict which products theirexisting customers will use in the next month based on their past behavior andthat of similar customers. With a more effective recommendation system inplace, Santander can better meet the individual needs of all customers andensure their satisfaction no matter where they are in life. Disclaimer: Thisdata set does not include any real Santander Spain's customer, and thus it isnot representative of Spain's customer base.	Submissions are evaluated according to the Mean Average Precision @ 7 (MAP@7):MAP@7= 1 |U| |U| ∑ u=1 1 min(m,7) min(n,7) ∑ k=1 P(k) where |U| is the numberof rows (users in two time points), P(k) is the precision at cutoff k, n isthe number of predicted products, and m is the number of added products forthe given user at that time point. If m = 0, the precision is defined to be 0.Submission File For every user at each time point, you must predict a space-delimited list of the products they added. The file should contain a headerand have the following format: ncodpers,added_products 15889,ind_tjcr_fin_ult115890,ind_tjcr_fin_ult1 ind_recibo_ult1 15892,ind_nomina_ult1 15893, etc.	['multiclass classification', 'tabular data', 'banking']	
santander-value-prediction-challenge	According to Epsilon research, 80% of customers are more likely to do businesswith you if you provide personalized service. Banking is no exception. Thedigitalization of everyday lives means that customers expect services to bedelivered in a personalized and timely manner… and often before they´ve evenrealized they need the service. In their 3rd Kaggle competition, SantanderGroup aims to go a step beyond recognizing that there is a need to provide acustomer a financial service and intends to determine the amount or value ofthe customer's transaction. This means anticipating customer needs in a moreconcrete, but also simple and personal way. With so many choices for financialservices, this need is greater now than ever before. In this competition,Santander Group is asking Kagglers to help them identify the value oftransactions for each potential customer. This is a first step that Santanderneeds to nail in order to personalize their services at scale.	The evaluation metric for this competition is Root Mean Squared LogarithmicError. The RMSLE is calculated as ϵ= √ 1 n n ∑ i=1 (log(pi+1)−log(ai+1))2Where: ϵ is the RMSLE value (score) n is the total number of observations inthe (public/private) data set, pi is your prediction of target, and ai is theactual target for i . log(x) is the natural logarithm of x Submission File Forevery row in the test.csv, submission files should contain two columns: ID andtarget. The ID corresponds to the column of that ID in the test.tsv. The fileshould contain a header and have the following format: ID,target000137c73,5944923.322036332 00021489f,5944923.3220363320004d7953,5944923.322036332 etc.	['finance', 'banking']	
santas-uncertain-bags	All was well in Santa's workshop. The gifts were made, the route was planned,the naughty and nice list complete. Santa thought this would finally be theyear he didn't need Kaggle's help with his combinatorial conundrums. At last,the Claus family could take the elves and reindeer on that well deservedvacation to the South Pole. Then, with just days until the big night, Santareceived an email from a panicked database admin elf. Attached was a serverlog with the six least jolly words a jolly old St. Nick could read: ALTERTABLE Gifts DROP COLUMN Weight One of the North Pole elf interns hadmistakenly deleted the weights for all of the inventory in the workshop! Santadidn't have a backup (remember, this is a guy who makes a list and checks ittwice) and, without knowing each present's weight, he didn't know how he wouldsafely pack his many gift bags. Gifts were already on their way to the sleighpacking facility and there wasn't time to re-weigh all the presents. It wasonce again necessary to summon the holiday talents of Kaggle's elite. Can youhelp Santa fill his multiple bags with sets of uncertain gifts? Save theseason by turning Santa's uncertain probabilities into presents for goodlittle boys and girls.	Submissions are evaluated on the total amount of weight you fit into Santa's1000 bags. The rules that govern gift packing are as follows: Overfilling abag above the 50lb limit will cause the entire bag to count for nothing,without warning! No gift may be used more than once. Every bag must have 3 ormore gifts. Submission File For each bag, you must provide a space-delimitedlist of GiftIds. Each line represents one bag. The file should contain aheader and have the following format: Gifts horse_0 book_345 gloves_48doll_873 train_714 coal23 bike_85 etc.	['tabular data', 'mathematical optimization']	
sberbank-russian-housing-market	Housing costs demand a significant investment from both consumers anddevelopers. And when it comes to planning a budget—whether personal orcorporate—the last thing anyone needs is uncertainty about one of theirbiggets expenses. Sberbank, Russia’s oldest and largest bank, helps theircustomers by making predictions about realty prices so renters, developers,and lenders are more confident when they sign a lease or purchase a building.Although the housing market is relatively stable in Russia, the country’svolatile economy makes forecasting prices as a function of apartmentcharacteristics a unique challenge. Complex interactions between housingfeatures such as number of bedrooms and location are enough to make pricingpredictions complicated. Adding an unstable economy to the mix means Sberbankand their customers need more than simple regression models in their arsenal.In this competition, Sberbank is challenging Kagglers to develop algorithmswhich use a broad spectrum of features to predict realty prices. Competitorswill rely on a rich dataset that includes housing data and macroeconomicpatterns. An accurate forecasting model will allow Sberbank to provide morecertainty to their customers in an uncertain economy.	Submissions are evaluated on the RMSLE between their predicted prices and theactual data. The target variable, called price_doc in the training set, is thesale price of each property. Submission File For each id in the test set, youmust predict the price that the property sold for. The file should contain aheader and have the following format: id,price_doc 30474,7118500.4430475,7118500.44 30476,7118500.44 etc.	['regression', 'tabular data', 'housing', 'banking', 'rmsle', 'extra small']	
sp-society-camera-model-identification	Finding footage of a crime caught on tape is an investigator's dream. But evenwith crystal clear, damning evidence, one critical question always remains–isthe footage real? Today, one way to help authenticate footage is to identifythe camera that the image was taken with. Forgeries often require splicingtogether content from two different cameras. But, unfortunately, the mostcommon way to do this now is using image metadata, which can be easilyfalsified itself. This problem is actively studied by several researchersaround the world. Many machine learning solutions have been proposed in thepast: least-squares estimates of a camera's color demosaicing filters asclassification features, co-occurrences of pixel value prediction errors asfeatures that are passed to sophisticated ensemble classifiers, and using CNNsto learn camera model identification features. However, this is a problem yetto be sufficiently solved. For this competition, the IEEE Signal ProcessingSociety is challenging you to build an algorithm that identifies which cameramodel captured an image by using traces intrinsically left in the image.Helping to solve this problem would have a big impact on the verification ofevidence used in criminal and civil trials and even news reporting.	This competition is evaluated on the weighted categorization accuracy of yourpredictions (the percentage of camera models correctly predicted).weighted_accuracy(y, y ^ )= 1 n ∑ i=1 n w i ( y i = y ^ i ) ∑ w i where n isthe number of samples in the test set, y is the true camera label, y_hat isthe predicted camera label, and w_i is 0.7 for unaltered images, and 0.3 foraltered images. Submission File For each image fname in the test set, you mustpredict a the correct camera model. The submission file should contain aheader and have the following format: fname,cameraimg_0002a04_manip.tif,iPhone-6 img_001e31c_unalt.tif,iPhone-6img_00275cf_manip.tif,iPhone-6 img_0034113_unalt.tif,iPhone-6	['image data']	
spooky-author-identification	"As I scurried across the candlelit chamber, manuscripts in hand, I thought I'dmade it. Nothing would be able to hurt me anymore. Little did I know there wasone last fright lurking around the corner. DING! My phone pinged me with adisturbing notification. It was Will, the scariest of Kaggle moderators,sharing news of another data leak. ""ph’nglui mglw’nafh Cthulhu R’lyehwgah’nagl fhtagn!"" I cried as I clumsily dropped my crate of unbound, spookybooks. Pages scattered across the chamber floor. How will I ever figure outhow to put them back together according to the authors who wrote them? Or arethey lost, forevermore? Wait, I thought... I know, machine learning! In thisyear's Halloween playground competition, you're challenged to predict theauthor of excerpts from horror stories by Edgar Allan Poe, Mary Shelley, andHP Lovecraft. We're encouraging you (with cash prizes!) to share your insightsin the competition's discussion forum and code in Kernels. We've designatedprizes to reward authors of kernels and discussion threads that areparticularly valuable to the community. Click the ""Prizes"" tab on thisoverview page to learn more. Getting Started New to Kernels or working withnatural language data? We've put together some starter kernels in Python and Rto help you hit the ground running."	Submissions are evaluated using multi-class logarithmic loss. Each id has onetrue class. For each id, you must submit a predicted probability for eachauthor. The formula is then: logloss=− 1 N ∑ i=1 N ∑ j=1 M y ij log( p ij ),where N is the number of observations in the test set, M is the number ofclass labels (3 classes), log l is the natural logarithm, y ij y is 1 ifobservation i i belongs to class j j and 0 otherwise, and p ij p is thepredicted probability that observation i i belongs to class j j . Thesubmitted probabilities for a given sentences are not required to sum to onebecause they are rescaled prior to being scored (each row is divided by therow sum). In order to avoid the extremes of the log function, predictedprobabilities are replaced with max(min(p,1− 10 −15 ), 10 −15 ) m . SubmissionFile You must submit a csv file with the id, and a probability for each of thethree classes. The order of the rows does not matter. The file must have aheader and should look like the following: id,EAP,HPL,MWSid07943,0.33,0.33,0.33 ...	['linguistics', 'multiclass classification', 'literature']	
statoil-iceberg-classifier-challenge	Drifting icebergs present threats to navigation and activities in areas suchas offshore of the East Coast of Canada. Currently, many institutions andcompanies use aerial reconnaissance and shore-based support to monitorenvironmental conditions and assess risks from icebergs. However, in remoteareas with particularly harsh weather, these methods are not feasible, and theonly viable monitoring option is via satellite. Statoil, an internationalenergy company operating worldwide, has worked closely with companies likeC-CORE. C-CORE have been using satellite data for over 30 years and have builta computer vision based surveillance system. To keep operations safe andefficient, Statoil is interested in getting a fresh new perspective on how touse machine learning to more accurately detect and discriminate againstthreatening icebergs as early as possible. In this competition, you’rechallenged to build an algorithm that automatically identifies if a remotelysensed target is a ship or iceberg. Improvements made will help drive thecosts down for maintaining safe working conditions.	Submissions are evaluated on the log loss between the predicted values and theground truth. Submission File For each id in the test set, you must predictthe probability that the image contains an iceberg (a number between 0 and 1).The file should contain a header and have the following format: id,is_iceberg809385f7,0.5 7535f0cd,0.4 3aa99a38,0.9 etc.	['image data', 'binary classification', 'weather', 'shipping']	
stocknews			['finance', 'news agencies', 'medium', 'featured']	"Actually, I prepare this dataset for students on my Deep Learning and NLPcourse. But I am also very happy to see kagglers play around with it. Havefun! Description: There are two channels of data provided in this dataset:News data: I crawled historical news headlines from Reddit WorldNews Channel(/r/worldnews). They are ranked by reddit users' votes, and only the top 25headlines are considered for a single date. (Range: 2008-06-08 to 2016-07-01)Stock data: Dow Jones Industrial Average (DJIA) is used to ""prove theconcept"". (Range: 2008-08-08 to 2016-07-01) I provided three data files in.csv format: RedditNews.csv: two columns The first column is the ""date"", andsecond column is the ""news headlines"". All news are ranked from top to bottombased on how hot they are. Hence, there are 25 lines for each date.DJIA_table.csv: Downloaded directly from Yahoo Finance: check out the web pagefor more info. Combined_News_DJIA.csv: To make things easier for my students,I provide this combined dataset with 27 columns. The first column is ""Date"",the second is ""Label"", and the following ones are news headlines ranging from""Top1"" to ""Top25"". ========================================= To my students: Imade this a binary classification task. Hence, there are only two labels: ""1""when DJIA Adj Close value rose or stayed as the same; ""0"" when DJIA Adj Closevalue decreased. For task evaluation, please use data from 2008-08-08 to2014-12-31 as Training Set, and Test Set is then the following two years data(from 2015-01-02 to 2016-07-01). This is roughly a 80%/20% split. And, ofcourse, use AUC as the evaluation metric.=========================================+++++++++++++++++++++++++++++++++++++++++ To all kagglers: Please upvote thisdataset if you like this idea for market prediction. If you think you coded anamazing trading algorithm, friendly advice do play safe with your own money :)+++++++++++++++++++++++++++++++++++++++++ Feel free to contact me if there isany question~ And, remember me when you become a millionaire :P"
sustainable-development-goals			['world', 'medium']	Content Relevant indicators drawn from the World Development Indicators,reorganized according to the goals and targets of the Sustainable DevelopmentGoals (SDGs). These indicators may help to monitor SDGs, but they are notalways the official indicators for SDG monitoring. Context This is a datasethosted by the World Bank. The organization has an open data platform foundhere and they update their information according the amount of data that isbrought in. Explore the World Bank using Kaggle and all of the data sourcesavailable through the World Bank organization page! Update Frequency: Thisdataset is updated daily. Acknowledgements This dataset is maintained usingthe World Bank's APIs and Kaggle's API. Cover photo by NA on Unsplash UnsplashImages are distributed under a unique Unsplash License.
talkingdata-adtracking-fraud-detection	Fraud risk is everywhere, but for companies that advertise online, click fraudcan happen at an overwhelming volume, resulting in misleading click data andwasted money. Ad channels can drive up costs by simply clicking on the ad at alarge scale. With over 1 billion smart mobile devices in active use everymonth, China is the largest mobile market in the world and therefore suffersfrom huge volumes of fradulent traffic. TalkingData, China’s largestindependent big data service platform, covers over 70% of active mobiledevices nationwide. They handle 3 billion clicks per day, of which 90% arepotentially fraudulent. Their current approach to prevent click fraud for appdevelopers is to measure the journey of a user’s click across their portfolio,and flag IP addresses who produce lots of clicks, but never end up installingapps. With this information, they've built an IP blacklist and deviceblacklist. While successful, they want to always be one step ahead offraudsters and have turned to the Kaggle community for help in furtherdeveloping their solution. In their 2nd competition with Kaggle, you’rechallenged to build an algorithm that predicts whether a user will download anapp after clicking a mobile app ad. To support your modeling, they haveprovided a generous dataset covering approximately 200 million clicks over 4days!	Submissions are evaluated on area under the ROC curve between the predictedprobability and the observed target. Submission File For each click_id in thetest set, you must predict a probability for the target is_attributedvariable. The file should contain a header and have the following format:click_id,is_attributed 1,0.003 2,0.001 3,0.000 etc.	[]	
tensorflow-speech-recognition-challenge	We might be on the verge of too many screens. It seems like everyday, newversions of common objects are “re-invented” with built-in wifi and brighttouchscreens. A promising antidote to our screen addiction are voiceinterfaces. But, for independent makers and entrepreneurs, it’s hard to builda simple speech detector using free, open data and code. Many voicerecognition datasets require preprocessing before a neural network model canbe built on them. To help with this, TensorFlow recently released the SpeechCommands Datasets. It includes 65,000 one-second long utterances of 30 shortwords, by thousands of different people. In this competition, you'rechallenged to use the Speech Commands Dataset to build an algorithm thatunderstands simple spoken commands. By improving the recognition accuracy ofopen-sourced voice interface tools, we can improve product effectiveness andtheir accessibility.	Submissions are evaluated on Multiclass Accuracy, which is simply the averagenumber of observations with the correct label. Note: There are only 12possible labels for the Test set: yes, no, up, down, left, right, on, off,stop, go, silence, unknown. The unknown label should be used for a commandthat is not one one of the first 10 labels or that is not silence. SubmissionFile For audio clip in the test set, you must predict the correct label. Thesubmission file should contain a header and have the following format:fname,label clip_000044442.wav,silence clip_0000adecb.wav,leftclip_0000d4322.wav,unknown etc.	[]	
text-normalization-challenge-english-language	"As many of us can attest, learning another language is tough. Picking up onnuances like slang, dates and times, and local expressions, can often be adistinguishing factor between proficiency and fluency. This challenge is evenmore difficult for a machine. Many speech and language applications, includingtext-to-speech synthesis (TTS) and automatic speech recognition (ASR), requiretext to be converted from written expressions into appropriate ""spoken"" forms.This is a process known as text normalization, and helps convert 12:47 to""twelve forty-seven"" and $3.16 into ""three dollars, sixteen cents."" However,one of the biggest challenges when developing a TTS or ASR system for a newlanguage is to develop and test the grammar for all these rules, a task thatrequires quite a bit of linguistic sophistication and native speakerintuition. A  baby  giraffe  is  6ft six feet tall  and  weighs  150lb onehundred fifty pounds . sil In this competition, you are challenged to automatethe process of developing text normalization grammars via machine learning.This track will focus on English, while a separate will focus on Russian here:Russian Text Normalization Challenge About the sponsor Google's TextNormalization Research Group conducts research and creates tools for thedetection, normalization and denormalization of non-standard words such asabbreviations, numbers or currency expressions; and semiotic classes -- texttokens and token sequences that represent particular entities that aresemantically constrained, such as measure phrases, addresses or dates.Applications of this work include text-to-speech synthesis, automatic speechrecognition, and information extraction/retrieval."	"Submissions are evaluated on prediction accuracy (the total percent of correcttokens). The predicted and actual string must match exactly in order to countas correct. In other words, we are measuring sequence accuracy, in that anyerror in the output for a given token in the input sequence means that thaterror is wrong. For example, if the input is ""145"" and the predicted output is""one forty five"" but the correct output is ""one hundred forty five"", this iscounted as a single error. Submission File For each token (id) in the testset, you must predict the normalized text. The file should contain a headerand have the following format: id,after 0_0,""the"" 0_1,""quick"" 0_2,""fox"" ..."	['linguistics', 'text data', 'languages']	
tgs-salt-identification-challenge	Several areas of Earth with large accumulations of oil and gas also have hugedeposits of salt below the surface. But unfortunately, knowing where largesalt deposits are precisely is very difficult. Professional seismic imagingstill requires expert human interpretation of salt bodies. This leads to verysubjective, highly variable renderings. More alarmingly, it leads topotentially dangerous situations for oil and gas company drillers. To createthe most accurate seismic images and 3D renderings, TGS (the world’s leadinggeoscience data company) is hoping Kaggle’s machine learning community will beable to build an algorithm that automatically and accurately identifies if asubsurface target is salt or not.	"This competition is evaluated on the mean average precision at differentintersection over union (IoU) thresholds. The IoU of a proposed set of objectpixels and a set of true object pixels is calculated as: IoU(A,B)= A∩B A∪B .The metric sweeps over a range of IoU thresholds, at each point calculating anaverage precision value. The threshold values range from 0.5 to 0.95 with astep size of 0.05: (0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95). Inother words, at a threshold of 0.5, a predicted object is considered a ""hit""if its intersection over union with a ground truth object is greater than 0.5.At each threshold value t t , a precision value is calculated based on thenumber of true positives (TP), false negatives (FN), and false positives (FP)resulting from comparing the predicted object to all ground truth objects:TP(t) TP(t)+FP(t)+FN(t) . TP(t)TP(t)+FP(t)+FN(t). A true positive is countedwhen a single predicted object matches a ground truth object with an IoU abovethe threshold. A false positive indicates a predicted object had no associatedground truth object. A false negative indicates a ground truth object had noassociated predicted object. The average precision of a single image is thencalculated as the mean of the above precision values at each IoU threshold: 1|thresholds| ∑ t TP(t) TP(t)+FP(t)+FN(t) . Lastly, the score returned by thecompetition metric is the mean taken over the individual average precisions ofeach image in the test dataset. Submission File In order to reduce thesubmission file size, our metric uses run-length encoding on the pixel values.Instead of submitting an exhaustive list of indices for your segmentation, youwill submit pairs of values that contain a start position and a run length.E.g. '1 3' implies starting at pixel 1 and running a total of 3 pixels(1,2,3). The competition format requires a space delimited list of pairs. Forexample, '1 3 10 5' implies pixels 1,2,3,10,11,12,13,14 are to be included inthe mask. The pixels are one-indexed and numbered from top to bottom, thenleft to right: 1 is pixel (1,1), 2 is pixel (2,1), etc. The metric checks thatthe pairs are sorted, positive, and the decoded pixel values are notduplicated. It also checks that no two predicted masks for the same image areoverlapping. The file should contain a header and have the following format.Each row in your submission represents a single predicted salt segmentationfor the given image. id,rle_mask 3e06571ef3,1 1 a51b08d882,1 1 c32590b06f,1 1etc."	['image data', 'geology']	
the-nature-conservancy-fisheries-monitoring	Nearly half of the world depends on seafood for their main source of protein.In the Western and Central Pacific, where 60% of the world’s tuna is caught,illegal, unreported, and unregulated fishing practices are threatening marineecosystems, global seafood supplies and local livelihoods. The NatureConservancy is working with local, regional and global partners to preservethis fishery for the future. Currently, the Conservancy is looking to thefuture by using cameras to dramatically scale the monitoring of fishingactivities to fill critical science and compliance monitoring data gaps.Although these electronic monitoring systems work well and are ready for widerdeployment, the amount of raw data produced is cumbersome and expensive toprocess manually. The Conservancy is inviting the Kaggle community to developalgorithms to automatically detect and classify species of tunas, sharks andmore that fishing boats catch, which will accelerate the video review process.Faster review and more reliable data will enable countries to reallocate humancapital to management and enforcement activities which will have a positiveimpact on conservation and our planet. Machine learning has the ability totransform what we know about our oceans and how we manage them. You can bepart of the solution. Resources You can learn more about this competition andThe Nature Conservancy in the video below.	Submissions are evaluated using the multi-class logarithmic loss. Each imagehas been labeled with one true class. For each image, you must submit a set ofpredicted probabilities (one for every image). The formula is then, logloss=−1 N N ∑ i=1 M ∑ j=1 yijlog(pij), where N is the number of images in the testset, M is the number of image class labels, log is the natural logarithm, yijis 1 if observation i belongs to class j and 0 otherwise, and pij is thepredicted probability that observation i belongs to class j . The submittedprobabilities for a given image are not required to sum to one because theyare rescaled prior to being scored (each row is divided by the row sum). Inorder to avoid the extremes of the log function, predicted probabilities arereplaced with max(min(p,1−10−15),10−15) . Submission File You must submit acsv file with the image file name, and a probability for each class. The 8classes to predict are: 'ALB', 'BET', 'DOL', 'LAG', 'NoF', 'OTHER','SHARK','YFT' The order of the rows does not matter. The file must have aheader and should look like the following:image,ALB,BET,DOL,LAG,NoF,OTHER,SHARK,YFT img_00001.jpg,1,0,0,0,0,...,0img_00002.jpg,0.3,0.1,0.6,0,...,0 ...	['image data', 'multiclass classification', 'object detection', 'fishing']	
titanic	Start here if... You're new to data science and machine learning, or lookingfor a simple intro to the Kaggle prediction competitions. CompetitionDescription The sinking of the RMS Titanic is one of the most infamousshipwrecks in history. On April 15, 1912, during her maiden voyage, theTitanic sank after colliding with an iceberg, killing 1502 out of 2224passengers and crew. This sensational tragedy shocked the internationalcommunity and led to better safety regulations for ships. One of the reasonsthat the shipwreck led to such loss of life was that there were not enoughlifeboats for the passengers and crew. Although there was some element of luckinvolved in surviving the sinking, some groups of people were more likely tosurvive than others, such as women, children, and the upper-class. In thischallenge, we ask you to complete the analysis of what sorts of people werelikely to survive. In particular, we ask you to apply the tools of machinelearning to predict which passengers survived the tragedy. Practice SkillsBinary classification Python and R basics	"Goal It is your job to predict if a passenger survived the sinking of theTitanic or not. For each in the test set, you must predict a 0 or 1 value forthe variable. Metric Your score is the percentage of passengers you correctlypredict. This is known simply as ""accuracy”. Submission File Format You shouldsubmit a csv file with exactly 418 entries plus a header row. Your submissionwill show an error if you have extra columns (beyond PassengerId and Survived)or rows. The file should have exactly 2 columns: PassengerId (sorted in anyorder) Survived (contains your binary predictions: 1 for survived, 0 fordeceased) PassengerId,Survived 892,0 893,1 894,0 Etc. You can download anexample submission file (gender_submission.csv) on the Data page."	['tutorial', 'binary classification', 'tabular data']	
tmdb-box-office-prediction	"We're going to make you an offer you can't refuse: a Kaggle competition! In aworld... where movies made an estimated $41.7 billion in 2018, the filmindustry is more popular than ever. But what movies make the most money at thebox office? How much does a director matter? Or the budget? For some movies,it's ""You had me at 'Hello.'"" For others, the trailer falls short ofexpectations and you think ""What we have here is a failure to communicate."" Inthis competition, you're presented with metadata on over 7,000 past films fromThe Movie Database to try and predict their overall worldwide box officerevenue. Data points provided include cast, crew, plot keywords, budget,posters, release dates, languages, production companies, and countries. Youcan collect other publicly available data to use in your model predictions,but in the spirit of this competition, use only data that would have beenavailable before a movie's release. Join in, ""make our day"", and then ""you'vegot to ask yourself one question: 'Do I feel lucky?'"""	It is your job to predict the international box office revenue for each movie.For each id in the test set, you must predict the value of the revenuevariable. Submissions are evaluated on Root-Mean-Squared-Logarithmic-Error(RMSLE) between the predicted value and the actual revenue. Logs are taken tonot overweight blockbuster revenue movies. Submission File Format The fileshould contain a header and have the following format: id,revenue 1461,10000001462,50000 1463,800000000 etc. You can download an example submission file(sample_submission.csv) on the Data page.	['tabular data', 'film']	
total-revenue-time-series-collection			['economics', 'small']	Content More details about each file are in the individual file descriptions.Context This is a dataset from the U.S. Census Bureau hosted by the FederalReserve Economic Database (FRED). FRED has a data platform found here and theyupdate their information according the amount of data that is brought in.Explore the U.S. Census Bureau using Kaggle and all of the data sourcesavailable through the U.S. Census Bureau organization page! Update Frequency:This dataset is updated daily. Acknowledgements This dataset is maintainedusing FRED's API and Kaggle's API. Cover photo by Nathan Dumlao on UnsplashUnsplash Images are distributed under a unique Unsplash License.
trackml-particle-identification	To explore what our universe is made of, scientists at CERN are collidingprotons, essentially recreating mini big bangs, and meticulously observingthese collisions with intricate silicon detectors. While orchestrating thecollisions and observations is already a massive scientific accomplishment,analyzing the enormous amounts of data produced from the experiments isbecoming an overwhelming challenge. Event rates have already reached hundredsof millions of collisions per second, meaning physicists must sift throughtens of petabytes of data per year. And, as the resolution of detectorsimprove, ever better software is needed for real-time pre-processing andfiltering of the most promising events, producing even more data. To helpaddress this problem, a team of Machine Learning experts and physicsscientists working at CERN (the world largest high energy physics laboratory),has partnered with Kaggle and prestigious sponsors to answer the question: canmachine learning assist high energy physics in discovering and characterizingnew particles? Specifically, in this competition, you’re challenged to buildan algorithm that quickly reconstructs particle tracks from 3D points left inthe silicon detectors. This challenge consists of two phases: The Accuracyphase has run on Kaggle from May to 13th August 2018 (Winners to be announcedby end September). Here we’ll be focusing on the highest score, irrespectiveof the evaluation time. This phase is an official IEEE WCCI competition (Riode Janeiro, Jul 2018). The Throughput phase will run on Codalab starting inSeptember 2018. Participants will submit their software which is evaluated bythe platform. Incentive is on the throughput (or speed) of the evaluationwhile reaching a good score. This phase is an official NIPS competition(Montreal, Dec 2018). All the necessary information for the Accuracy phase isavailable here on Kaggle site. The overall TrackML challenge web site isthere.	Custom metric The evaluation metric for this competition is a custom metric.In one line : it is the intersection between the reconstructed tracks and theground truth particles, normalized to one for each event, and averaged on theevents of the test set. First, each hit is assigned a weight: the few first(starting from the center of the detector) and last hits have a larger weighthits from the more straight tracks (more rare, but more interesting) have alarger weight random hits or hits from very short tracks have weight zero thesum of the weights of all the hits of one event is 1 by construction the hitweights are available in the truth file. They are not revealed for the testdataset Then, the score is constructed as follows: tracks are uniquely matchedto particles by the double majority rule: for a given track, the matchingparticle is the one to which the absolute majority (strictly more that 50%) ofthe track points belong. the track should have the absolute majority of thepoints of the matching particle. If any of these constraints is not met, thescore for this track is zero the score of a surviving track is the sum of theweights of the points of the intersection between the track and the matchingparticle. the score of an event is the sum of the score of all its tracks. thefinal score is the average on the events of the public and private leaderboardtest respectively. A perfect algorithm will have a score of 1, while a randomone will have a score 0. An example implementation can be found in the trackmlpython library. Submission Format The submission file should contain threecolumns: event_id, hit_id, track_id, and should have exactly one line forevery hit of every event. event_id is the event number hit_id is the hitnumber, within that event track_id is the user defined numerical identifier(non negative integer) of the track (the track being the group or cluster ofhits). The file should contain a header and have the following format:event_id,hit_id,track_id 0,0,21 0,1,49 0,3,32 0,4,0 0,5,21 etc...	['tabular data', 'physics']	
transfer-learning-on-stack-exchange-tags	"What does physics have in common with biology, cooking, cryptography, diy,robotics, and travel? If you answered ""all pursuits are governed by theimmutable laws of physics"" we'll begrudgingly give you partial credit. If youanswered ""all were chosen randomly by a scheming Kaggle employee for a twistedtransfer learning competition"", congratulations, we accept your answer andmark the question as solved. In this competition, we provide the titles, text,and tags of Stack Exchange questions from six different sites. We then ask fortag predictions on unseen physics questions. Solving this problem via astandard machine approach might involve training an algorithm on a corpus ofrelated text. Here, you are challenged to train on material from outside thefield. Can an algorithm learn appropriate physics tags from ""extreme-tourismAntarctica""? Let's find out. Kaggle is hosting this competition for the datascience community to use for fun and education. This dataset originates fromthe Stack Exchange data dump."	The evaluation metric for this competition is Mean F1-Score. The F1 scoremeasures accuracy using the statistics precision p and recall r. Precision isthe ratio of true positives (tp) to all predicted positives (tp + fp). Recallis the ratio of true positives to all actual positives (tp + fn). The F1 scoreis given by: F1=2 p⋅r p+r where p= tp tp+fp , r= tp tp+fn F1=2p⋅rp+r wherep=tptp+fp, r=tptp+fn The F1 metric weights recall and precision equally, and agood retrieval algorithm will maximize both precision and recallsimultaneously. Thus, moderately good performance on both will be favored overextremely good performance on one and poor performance on the other.Submission File For every question in the dataset, submission files shouldcontain two columns: id and tags. The predicted tags should be a space-delimited list. The file must have a header and should look like thefollowing: id,tags 1,physics poetry 2,physics poetry chemistry 3,physicselectrons etc.	['text data', 'multiclass classification', 'tabular data']	
traveling-santa-2018-prime-paths	Rudolph the red-nosed reindeer Had some very tired hooves But he had a job tofinish Could he do it with the shortest moves? All of the other reindeer Usedto laugh and mock his code They always said poor Rudolph Couldn't handle theworkload Then one foggy Christmas Eve Santa came to say I see you've takennumber theory Please make this night a bit less dreary? Then how the reindeerloved him and each enrolled in an AI degree Rudolph the red-nosed reindeer Weget to go to bed early! Rudolph has always believed in working smarter, notharder. And what better way to earn the respect of Comet and Blitzen thanshowing the initiative to improve Santa's annual route for delivering toys onChristmas Eve? This year, Rudolph believes he can motivate the overworkedReindeer team by wisely choosing the order in which they visit the houses onSanta's list. The houses in prime cities always leave carrots for theReindeers alongside the usual cookies and milk. These carrots are just thesustenance the Reindeers need to keep pace. In fact, Rudolph has found that ifthe Reindeer team doesn't originate from a prime city exactly every 10th step,it takes the 10% longer than it normally would to make their next destination!Can you help Rudolph solve the Traveling Santa problem subject to his carrotconstraint? His team--and Santa--are counting on you! Attributions: ReindeerPhoto: Norman Tsui Stocking Photo: Wesley Tingey	Your submission is scored on the Euclidean distance of your submitted path,subject to the constraint that every 10th step is 10% more lengthy unlesscoming from a prime CityId. Submission file Your submission file contains theordered Path that Santa should use to visit all the cities. Paths must startand end at the North Pole (CityId = 0) and you must visit every city exactlyonce. Submission files must have a header and should look like: Path 0 1 2 ...0	['optimization', 'mathematical optimization']	
two-sigma-connect-rental-listing-inquiries	Finding the perfect place to call your new home should be more than browsingthrough endless listings. RentHop makes apartment search smarter by using datato sort rental listings by quality. But while looking for the perfectapartment is difficult enough, structuring and making sense of all availablereal estate data programmatically is even harder. Two Sigma and RentHop, aportfolio company of Two Sigma Ventures, invite Kagglers to unleash theircreative engines to uncover business value in this unique recruitingcompetition. Two Sigma invites you to apply your talents in this recruitingcompetition featuring rental listing data from RentHop. Kagglers will predictthe number of inquiries a new listing receives based on the listing’s creationdate and other features. Doing so will help RentHop better handle fraudcontrol, identify potential listing quality issues, and allow owners andagents to better understand renters’ needs and preferences. Two Sigma has beenat the forefront of applying technology and data science to financialforecasts. While their pioneering advances in big data, AI, and machinelearning in the financial world have been pushing the industry forward, aswith all other scientific progress, they are driven to make continualprogress. This challenge is an opportunity for competitors to gain a sneakpeek into Two Sigma's data science work outside of finance. AcknowledgmentsThis competition is co-hosted by Two Sigma and RentHop (a portfolio company ofTwo Sigma Ventures, which is a division of Two Sigma Investments) to encouragecreativity in using real world data to solve everyday problems.	Submissions are evaluated using the multi-class logarithmic loss. Each listinghas one true class. For each listing, you must submit a set of predictedprobabilities (one for every listing). The formula is then, logloss=− 1 N ∑i=1 N ∑ j=1 M y ij log( p ij ), where N is the number of listings in the testset, M is the number of class labels (3 classes), log is the naturallogarithm, y ij is 1 if observation i belongs to class j and 0 otherwise, andp ij is the predicted probability that observation i belongs to class j . Thesubmitted probabilities for a given listing are not required to sum to onebecause they are rescaled prior to being scored (each row is divided by therow sum). In order to avoid the extremes of the log function, predictedprobabilities are replaced with max(min(p,1− 10 −15 ), 10 −15 ) . SubmissionFile You must submit a csv file with the listing_id, and a probability foreach class. The order of the rows does not matter. The file must have a headerand should look like the following: listing_id,high,medium,low7065104,0.07743170693194379,0.2300252644876046,0.6925430285804516 7089035,0.0,1.0, 0.0 ...	['text data', 'multiclass classification', 'tabular data', 'housing']	
two-sigma-financial-modeling	"How can we use the world’s tools and intelligence to forecast economicoutcomes that can never be entirely predictable? This question is at the coreof countless economic activities around the world – including at Two SigmaInvestments, who has been applying technology and systematic strategies tofinancial trading since 2001. For over 15 years, Two Sigma has been at theforefront of applying technology and data science to financial forecasts.While their pioneering advances in big data, AI, and machine learning in thefinancial world have been pushing the industry forward, as with all otherscientific progress, they are driven to make continual progress. Through thisexclusive partnership, Two Sigma is excited to explore what untapped valueKaggle's diverse data science community can discover in the financial markets.Economic opportunity depends on the ability to deliver singularly accurateforecasts in a world of uncertainty. By accurately predicting financialmovements, Kagglers will learn about scientifically-driven approaches tounlocking significant predictive capability. Two Sigma is excited to findpredictive value and gain a better understanding of the skills offered by theglobal data science crowd. What is a Code Competition? Welcome to Kaggle'svery first Code Competition! In contrast to our traditional competitions,where competitors submit only prediction outputs, participants in CodeCompetitions will submit their code via Kaggle Kernels. All kernels areprivate by default in Code Competitions. You can build your models in Kernelsby running them on a training set and, once you're ready to submit your code,your model's performance will be evaluated against the test set and your scoreand public leaderboard position revealed. As with our traditionalcompetitions, we still maintain a private leaderboard test set, which yourcode is also evaluated against for final scoring, but is not revealed untilthe competition closes. Since Code Competitions are brand new, we ask for yourpatience if you encounter bugs or frustrating platform quirks. Please reportany issues you find in the forums and we'll do our best to respond. Who ownsmy code? You do. Even though you are submitting code, the intellectualproperty exchange here works similarly to a standard prediction competition,whereby prize winners have the option to grant a non-exclusive license inexchange for a prize. There is a new addition to the terms for CodeCompetitions: Kaggle and the competition host reserve a right to reviewsubmissions ""for purposes related to evaluation and scoring in thisCompetition, including but not limited to the assessment of potential cheatingbehavior."" Please refer to the official competition rules for full details.Getting Started Review the data page for details about the data and theevaluation metric. You may download the train set for local training. Take alook at the tutorial covering the new code submission process under thesubmission instructions tab. You'll find step-by-step instructions, somehelpful pointers, plus details on environment constraints. Get feedback onyour benchmark code and share exploratory analyses with the community bymaking any of your kernels public. Improve your score! Note: there is no costof entry for participation."	"Submissions will be evaluated on the R value between the predicted and actualvalues. The R value similar to the R squared value, also called thecoefficient of determination. R squared can be calculated as: R 2 =1− ∑ i ( yi − y ^ i ) 2 ∑ i ( y i −μ ) 2 . To calculate R, we then use: R=sign( R 2 ) ∣∣ R 2 ∣ ∣ − − − − √ , where y is the actual value, μ is the mean of the actualvalues, and y ^ is the predicted value. Do not be discouraged by low R values;in finance, given the high ratio of signal-to-noise, even a small R candeliver meaningful value! Negative R values are clipped at -1, i.e. the scoreyou see will be max(−1,R) . Additionally, if a submission errors for anyreason, you will receive a simple ""Error"" status."	['finance', 'future prediction']	
two-sigma-financial-news	Can we use the content of news analytics to predict stock price performance?The ubiquity of data today enables investors at any scale to make betterinvestment decisions. The challenge is ingesting and interpreting the data todetermine which data is useful, finding the signal in this sea of information.Two Sigma is passionate about this challenge and is excited to share it withthe Kaggle community. As a scientifically driven investment manager, Two Sigmahas been applying technology and data science to financial forecasts for over17 years. Their pioneering advances in big data, AI, and machine learning havepushed the investment industry forward. Now, they're eager to engage withKagglers in this continuing pursuit of innovation. By analyzing news data topredict stock prices, Kagglers have a unique opportunity to advance the stateof research in understanding the predictive power of the news. This power, ifharnessed, could help predict financial outcomes and generate significanteconomic impact all over the world. Data for this competition comes from thefollowing sources: Market data provided by Intrinio. News data provided byThomson Reuters. Copyright ©, Thomson Reuters, 2017. All Rights Reserved. Use,duplication, or sale of this service, or data contained herein, except asdescribed in the Competition Rules, is strictly prohibited. The THOMSONREUTERS Kinesis Logo and THOMSON REUTERS are trademarks of Thomson Reuters andits affiliated companies in the United States and other countries and usedherein under license.	In this competition, you must predict a signed confidence value, y ^ ti∈[−1,1] , which is multiplied by the market-adjusted return of a givenassetCode over a ten day window. If you expect a stock to have a largepositive return--compared to the broad market--over the next ten days, youmight assign it a large, positive confidenceValue (near 1.0). If you expect astock to have a negative return, you might assign it a large, negativeconfidenceValue (near -1.0). If unsure, you might assign it a value near zero.For each day in the evaluation time period, we calculate: x t = ∑ i y ^ ti rti u ti , where r ti is the 10-day market-adjusted leading return for day tfor instrument i, and u ti is a 0/1 universe variable (see the datadescription for details) that controls whether a particular asset is includedin scoring on a particular day. Your submission score is then calculated asthe mean divided by the standard deviation of your daily x t values: score= x¯ t σ( x t ) . If the standard deviation of predictions is 0, the score isdefined as 0. Submission File You must make submissions directly from KaggleKernels. By adding your teammates as collaborators on a kernel, you can shareand edit code privately with them. The kernels environment automaticallyformats and creates your submission files in this competition when callingenv.write_submission_file(). There is no need to manually create yoursubmissions. Submissions will have the following format:time,assetCode,confidenceValue 2017-01-03,RPXC.O,0.1 2017-01-04,RPXC.O,0.022017-01-05,RPXC.O,-0.3 etc.	['finance', 'time series', 'money', 'news agencies', 'custom metric', 'extra small']	
us-consumer-finance-complaints			['finance', 'medium', 'featured']	Each week the CFPB sends thousands of consumers’ complaints about financialproducts and services to companies for response. Those complaints arepublished here after the company responds or after 15 days, whichever comesfirst. By adding their voice, consumers help improve the financialmarketplace.
us-stocks-fundamentals			['finance', 'medium', 'featured']	This dataset contains US stocks fundamental data, such as income statement,balance sheet and cash flows. 12,129 companies 8,526 unique indicators ~20indicators comparable across most companies Five years of data, yearly Thedata is provided by http://usfundamentals.com.
vsb-power-line-fault-detection	Medium voltage overhead power lines run for hundreds of miles to supply powerto cities. These great distances make it expensive to manually inspect thelines for damage that doesn't immediately lead to a power outage, such as atree branch hitting the line or a flaw in the insulator. These modes of damagelead to a phenomenon known as partial discharge — an electrical dischargewhich does not bridge the electrodes between an insulation system completely.Partial discharges slowly damage the power line, so left unrepaired they willeventually lead to a power outage or start a fire. Your challenge is to detectpartial discharge patterns in signals acquired from these power lines with anew meter designed at the ENET Centre at VŠB. Effective classifiers using thisdata will make it possible to continuously monitor power lines for faults.ENET Centre researches and develops renewable energy resources with the goalof reducing or eliminating harmful environmental impacts. Their efforts focuson developing technology solutions around transportation and processing ofenergy raw materials. By developing a solution to detect partial dischargeyou’ll help reduce maintenance costs, and prevent power outages.	Submissions are evaluated on the Matthews correlation coefficient (MCC)between the predicted and the observed response. The MCC is given by: MCC=(TP∗TN)−(FP∗FN) √ (TP+FP)(TP+FN)(TN+FP)(TN+FN) , where TP is the number oftrue positives, TN the number of true negatives, FP the number of falsepositives, and FN the number of false negatives. Submission File For eachsignal in the test set, you must predict a binary prediction for the targetvariable. The file should contain a header and have the following format:signal_id,target 0,0 1,1 2,0 etc.	['binary classification', 'tabular data', 'signal processing']	
web-traffic-time-series-forecasting	This competition focuses on the problem of forecasting the future values ofmultiple time series, as it has always been one of the most challengingproblems in the field. More specifically, we aim the competition at testingstate-of-the-art methods designed by the participants, on the problem offorecasting future web traffic for approximately 145,000 Wikipedia articles.Sequential or temporal observations emerge in many key real-world problems,ranging from biological data, financial markets, weather forecasting, to audioand video processing. The field of time series encapsulates many differentproblems, ranging from analysis and inference to classification and forecast.What can you do to help predict future views? This competition will run as twostages and involves prediction of actual future events. There will be atraining stage during which the leaderboard is based on historical data,followed by a stage where participants are scored on real future events. Youhave complete freedom in how to produce your forecasts: e.g. use of univariatevs multi-variate models, use of metadata (article identifier), hierarchicaltime series modeling (for different types of traffic), data augmentation (e.g.using Google Trends data to extend the dataset), anomaly and outlier detectionand cleaning, different strategies for missing value imputation, and many moretypes of approaches. We thank Google Inc. and Voleon for sponsorship of thiscompetition, and Oren Anava and Vitaly Kuznetsov for organizing it. Kaggle isexcited to partner with research groups to push forward the frontier ofmachine learning. Research competitions make use of Kaggle's platform andexperience, but are largely organized by the research group's data scienceteam. Any questions or concerns regarding the competition data, quality, ortopic will be addressed by them.	Submissions are evaluated on SMAPE between forecasts and actual values. Wedefine SMAPE = 0 when the actual and predicted values are both 0. SubmissionFile For each article and day combination (see key.csv), you must predict theweb traffic. The file should contain a header and have the following format:Id,Visits bf4edcf969af,0 929ed2bf52b9,0 ff29d0f51d5c0 etc. Due to the largefile size and number of rows, submissions may take a few minutes to score.Thank you for your patience.	['internet', 'time series', 'tabular data', 'future prediction']	
whale-categorization-playground	After centuries of intense whaling, recovering whale populations still have ahard time adapting to warming oceans and struggle to compete every day withthe industrial fishing industry for food. To aid whale conservation efforts,scientists use photo surveillance systems to monitor ocean activity. They usethe shape of whales’ tails and unique markings found in footage to identifywhat species of whale they’re analyzing and meticulously log whale poddynamics and movements. For the past 40 years, most of this work has been donemanually by individual scientists, leaving a huge trove of data untapped andunderutilized. In this competition, you’re challenged to build an algorithm toidentifying whale species in images. You’ll analyze Happy Whale’s database ofover 25,000 images, gathered from research institutions and publiccontributors. By contributing, you’ll help to open rich fields ofunderstanding for marine mammal population dynamics around the globe. We'dlike to thank Happy Whale for providing this data and problem. Happy Whale isa platform that uses image process algorithms to let anyone to submit theirwhale photo and have it automatically identified.	Submissions are evaluated according to the Mean Average Precision @ 5 (MAP@5):MAP@5= 1 U ∑ u=1 U ∑ k=1 min(n,5) P(k) where U is the number of images, P(k)is the precision at cutoff k, and n is the number predictions per image.Submission File For each Image in the test set, you may predict up to 5 labelsfor the whale Id. Whales that are not predicted to be one of the labels in thetraining data should be labeled as new_whale. The file should contain a headerand have the following format: Image,Id 00029b3a.jpg,new_whale w_1287fbcw_98baff9 w_7554f44 w_1eafe46 0003c693.jpg,new_whale w_1287fbc w_98baff9w_7554f44 w_1eafe46 ...	['image data', 'animals']	
whats-cooking-kernels-only	Picture yourself strolling through your local, open-air market... What do yousee? What do you smell? What will you make for dinner tonight? If you're inNorthern California, you'll be walking past the inevitable bushels of leafygreens, spiked with dark purple kale and the bright pinks and yellows ofchard. Across the world in South Korea, mounds of bright red kimchi greet you,while the smell of the sea draws your attention to squids squirming nearby.India’s market is perhaps the most colorful, awash in the rich hues and aromasof dozens of spices: turmeric, star anise, poppy seeds, and garam masala asfar as the eye can see. Some of our strongest geographic and culturalassociations are tied to a region's local foods. This playground competitionsasks you to predict the category of a dish's cuisine given a list of itsingredients. Acknowledgements We want to thank Yummly for providing thisunique dataset. Kaggle is hosting this playground competition for fun andpractice.	Submissions are evaluated on the categorization accuracy (the percent ofdishes that you correctly classify). Submission File Your submission fileshould predict the cuisine for each recipe in the test set. The file shouldcontain a header and have the following format: id,cuisine 35203,italian17600,italian 35200,italian 17602,italian ... etc.	['food and drink', 'text data', 'multiclass classification']	
wine-reviews			['food and drink', 'critical theory', 'medium', 'featured']	Context After watching Somm (a documentary on master sommeliers) I wonderedhow I could create a predictive model to identify wines through blind tastinglike a master sommelier would. The first step in this journey was gatheringsome data to train a model. I plan to use deep learning to predict the winevariety using words in the description/review. The model still won't be ableto taste the wine, but theoretically it could identify the wine based on adescription that a sommelier could give. If anyone has any ideas on how toaccomplish this, please post them! Content This dataset contains three files:winemag-data-130k-v2.csv contains 10 columns and 130k rows of wine reviews.winemag-data_first150k.csv contains 10 columns and 150k rows of wine reviews.winemag-data-130k-v2.json contains 6919 nodes of wine reviews. Click on thedata tab to see individual file descriptions, column-level metadata andsummary statistics. Acknowledgements The data was scraped from WineEnthusiastduring the week of June 15th, 2017. The code for the scraper can be found hereif you have any more specific questions about data collection that I didn'taddress. UPDATE 11/24/2017 After feedback from users of the dataset I scrapedthe reviews again on November 22nd, 2017. This time around I collected thetitle of each review, which you can parse the year out of, the tasters name,and the taster's Twitter handle. This should also fix the duplicate entryissue. Inspiration I think that this dataset offers some great opportunitiesfor sentiment analysis and other text related predictive models. My overallgoal is to create a model that can identify the variety, winery, and locationof a wine based on a description. If anyone has any ideas, breakthroughs, orother interesting insights/models please post them.
womens-machine-learning-competition-2018	Google Cloud and NCAA® have teamed up to bring you this year’s version of theKaggle machine learning competition. Another year, another chance toanticipate the upsets, call the probabilities, and put your bracketologyskills to the leaderboard test. Kagglers will join the millions of fans whoattempt to forecast the outcomes of March Madness® during this year's NCAADivision I Men’s and Women’s Basketball Championships. But unlike most fans,you will pick your bracket using a combination of NCAA’s historical data andyour computing power, while the ground truth unfolds on national television.In the first stage of the competition, Kagglers will rely on results of pasttournaments to build and test models. We encourage you to post any usefulexternal data as a dataset. In the second stage, competitors will forecastoutcomes of all possible match-ups in the 2018 NCAA Division I Men’s andWomen’s Basketball Championships. You don't need to participate in the firststage to enter the second. The first stage exists to incentivize modelbuilding and provide a means to score predictions. The real competition isforecasting the 2018 results. This page is for the NCAA Division I Women'stournament. Check out the NCAA Division I Men's tournament here.	"Submissions are scored on the log loss: LogLoss=− 1 n ∑ i=1 n [ y i log( y ^ i)+(1− y i )log(1− y ^ i )], where n is the number of games played y ^ i y isthe predicted probability of team 1 beating team 2 y i y is 1 if team 1 wins,0 if team 2 wins log() l is the natural (base e) logarithm The use of thelogarithm provides extreme punishments for being both confident and wrong. Inthe worst possible case, a prediction that something is true when it isactually false will add infinite to your error score. In order to preventthis, predictions are bounded away from the extremes by a small value.Submission File The file you submit will depend on whether the competition isin stage 1 (historical model building) or stage 2 (the 2018 tournament).Sample submission files will be provided for both stages. The format is a listof every possible matchup between the tournament teams. Since team1 vs. team2is the same as team2 vs. team1, we only include the game pairs where team1 hasthe lower team id. For example, in a tournament of 64 teams, you will predict(64*63)/2 = 2,016 matchups. Each game has a unique id created by concatenatingthe season in which the game was played, the team1 id, and the team2 id. Forexample, ""2013_3104_3129"" indicates team 3104 played team 3129 in the year2013. You must predict the probability that the team with the lower id beatsthe team with the higher id. The resulting submission format looks like thefollowing, where ""pred"" represents the predicted probability that the firstteam will win: id,pred 2013_1103_1107,0.5 2013_1103_1112,0.52013_1103_1125,0.5 ..."	[]	
world-bank-gni-ranking,-atlas-method-and-ppp-based			['world', 'small', 'featured']	Content More details about each file are in the individual file descriptions.Context This is a dataset hosted by the World Bank. The organization has anopen data platform found here and they update their information according theamount of data that is brought in. Explore the World Bank using Kaggle and allof the data sources available through the World Bank organization page! UpdateFrequency: This dataset is updated daily. Acknowledgements This dataset ismaintained using the World Bank's APIs and Kaggle's API. Cover photo by DanielCorneschi on Unsplash Unsplash Images are distributed under a unique UnsplashLicense.
world-bank-health-nutrition-and-population-data			['world', 'medium']	Content More details about each file are in the individual file descriptions.Context This is a dataset hosted by the World Bank. The organization has anopen data platform found here and they update their information according theamount of data that is brought in. Explore the World Bank using Kaggle and allof the data sources available through the World Bank organization page! UpdateFrequency: This dataset is updated daily. Acknowledgements This dataset ismaintained using the World Bank's APIs and Kaggle's API. Cover photo by SimonRae on Unsplash Unsplash Images are distributed under a unique UnsplashLicense.
world-bank-international-debt-statistics			['world', 'small']	Content More details about each file are in the individual file descriptions.Context This is a dataset hosted by the World Bank. The organization has anopen data platform found here and they update their information according theamount of data that is brought in. Explore the World Bank using Kaggle and allof the data sources available through the World Bank organization page! UpdateFrequency: This dataset is updated daily. Acknowledgements This dataset ismaintained using the World Bank's APIs and Kaggle's API. Cover photo by AnnieSpratt on Unsplash Unsplash Images are distributed under a unique UnsplashLicense.
world-bank-population-estimates-and-ranking			['world', 'medium']	Content More details about each file are in the individual file descriptions.Context This is a dataset hosted by the World Bank. The organization has anopen data platform found here and they update their information according theamount of data that is brought in. Explore the World Bank using Kaggle and allof the data sources available through the World Bank organization page! UpdateFrequency: This dataset is updated daily. Acknowledgements This dataset ismaintained using the World Bank's APIs and Kaggle's API. Cover photo by TraceyHocking on Unsplash Unsplash Images are distributed under a unique UnsplashLicense.
world-bank-projects-operations			['world', 'medium']	Content World Bank Projects & Operations provides access to basic informationon all of the World Bank's lending projects from 1947 to the present. Thedataset includes basic information such as the project title, task manager,country, project id, sector, themes, commitment amount, product line,procurement notices, contract awards, and financing. It also provides links topublicly disclosed online documents. For older projects, there is a link tothe Archives catalog, which contains records of older documents. Whereavailable, there are also links to contract awards since July 2000. ContextThis is a dataset hosted by the World Bank. The organization has an open dataplatform found here and they update their information according the amount ofdata that is brought in. Explore the World Bank using Kaggle and all of thedata sources available through the World Bank organization page! UpdateFrequency: This dataset is updated daily. Acknowledgements This dataset ismaintained using the World Bank's APIs and Kaggle's API. Cover photo by JackCatterall on Unsplash Unsplash Images are distributed under a unique UnsplashLicense.
world-bank-quarterly-external-debt-statistics			['world', 'medium']	Content More details about each file are in the individual file descriptions.Context This is a dataset hosted by the World Bank. The organization has anopen data platform found here and they update their information according theamount of data that is brought in. Explore the World Bank using Kaggle and allof the data sources available through the World Bank organization page! UpdateFrequency: This dataset is updated daily. Acknowledgements This dataset ismaintained using the World Bank's APIs and Kaggle's API. Cover photo by StefanSteinbauer on Unsplash Unsplash Images are distributed under a unique UnsplashLicense.
world-bank-trade-costs-and-trade-in-services			['world', 'medium']	Content More details about each file are in the individual file descriptions.Context This is a dataset hosted by the World Bank. The organization has anopen data platform found here and they update their information according theamount of data that is brought in. Explore the World Bank using Kaggle and allof the data sources available through the World Bank organization page! UpdateFrequency: This dataset is updated daily. Acknowledgements This dataset ismaintained using the World Bank's APIs and Kaggle's API. Cover photo by NA onUnsplash Unsplash Images are distributed under a unique Unsplash License.
world-development-indicators			['economics', 'international relations', 'large', 'featured']	The World Development Indicators from the World Bank contain over a thousandannual indicators of economic development from hundreds of countries aroundthe world. Here's a list of the available indicators along with a list of theavailable countries. For example, this data includes the life expectancy atbirth from many countries around the world: The dataset hosted here is aslightly transformed verion of the raw files available here to facilitateanalytics.
zillow-prize-1	Zillow’s Zestimate home valuation has shaken up the U.S. real estate industrysince first released 11 years ago. A home is often the largest and mostexpensive purchase a person makes in his or her lifetime. Ensuring homeownershave a trusted way to monitor this asset is incredibly important. TheZestimate was created to give consumers as much information as possible abouthomes and the housing market, marking the first time consumers had access tothis type of home value information at no cost. “Zestimates” are estimatedhome values based on 7.5 million statistical and machine learning models thatanalyze hundreds of data points on each property. And, by continuallyimproving the median margin of error (from 14% at the onset to 5% today),Zillow has since become established as one of the largest, most trustedmarketplaces for real estate information in the U.S. and a leading example ofimpactful machine learning. Zillow Prize, a competition with a one milliondollar grand prize, is challenging the data science community to help push theaccuracy of the Zestimate even further. Winning algorithms stand to impact thehome values of 110M homes across the U.S. In this million-dollar competition,participants will develop an algorithm that makes predictions about the futuresale prices of homes. The contest is structured into two rounds, thequalifying round which opens May 24, 2017 and the private round for the 100top qualifying teams that opens on Feb 1st, 2018. In the qualifying round,you’ll be building a model to improve the Zestimate residual error. In thefinal round, you’ll build a home valuation algorithm from the ground up, usingexternal data sources to help engineer new features that give your model anedge over the competition. Because real estate transaction data is publicinformation, there will be a three-month sales tracking period after eachcompetition round closes where your predictions will be evaluated against theactual sale prices of the homes. The final leaderboard won’t be revealed untilthe close of the sales tracking period.	Submissions are evaluated on Mean Absolute Error between the predicted logerror and the actual log error. The log error is defined aslogerror=log(Zestimate)−log(SalePrice) logerror=log(Zestimate)−log(SalePrice)and it is recorded in the transactions training data. If a transaction didn'thappen for a property during that period of time, that row is ignored and notcounted in the calculation of MAE. Submission File For each property (uniqueparcelid), you must predict a log error for each time point. You should bepredicting 6 timepoints: October 2016 (201610), November 2016 (201611),December 2016 (201612), October 2017 (201710), November 2017 (201711), andDecember 2017 (201712). The file should contain a header and have thefollowing format: ParcelId,201610,201611,201612,201710,201711,20171210754147,0.1234,1.2234,-1.3012,1.4012,0.8642-3.1412 10759547,0,0,0,0,0,0 etc.Note that the actual log errors are accurate the 4th decimal places, so youcan adjust your decimal formats to limit the size of your submission file.	['housing', 'real estate']	
