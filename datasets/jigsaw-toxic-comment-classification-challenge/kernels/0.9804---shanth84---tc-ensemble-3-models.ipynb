{"cells":[{"metadata":{"_cell_guid":"ca8aab5a-820c-434c-beb7-ac2ac16ce55c","_uuid":"651195d9b2b152d311182c4302db9fca758495c1","collapsed":true,"trusted":true},"cell_type":"code","source":"# Increase batch size \n# Batch based prediction \n# Architecture - Batch Normalization, PReLU\n# Add another sparse matrix formulation \n#-----------------\n# Model 2 with Dropouts ","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"64de75ca-3184-4345-8a38-86450fda8eb5","_uuid":"885517681dd95775ab5c017589d989ec5fee49cd","trusted":true,"collapsed":true},"cell_type":"code","source":"import time \nimport gc\nstart_time = time.time()\n\nimport numpy as np\nnp.random.seed(42)\nimport pandas as pd\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\n\nfrom keras.models import Model\nfrom keras.layers import Input, Dense, Embedding, SpatialDropout1D, concatenate\nfrom keras.layers import GRU, Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D, BatchNormalization, Conv1D, MaxPooling1D, Flatten\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.preprocessing import text, sequence\nfrom keras.callbacks import Callback\nfrom keras import backend as K\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nos.environ['OMP_NUM_THREADS'] = '4'\n\nimport threading\nimport multiprocessing\nfrom multiprocessing import Pool, cpu_count\nfrom contextlib import closing\ncores = 4\n\n#Get Training and test data \n\ntrain1 = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/train.csv')\n\n#train = train1[\"comment_text\"].fillna(\"fillna\").values\n#test = test1[\"comment_text\"].fillna(\"fillna\").values\n\ntrain = train1\ny_train = train[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]].values\n\nmax_features = 50000\nmaxlen = 100\nembed_size = 300","execution_count":22,"outputs":[]},{"metadata":{"_cell_guid":"12ae2824-baeb-4540-97f4-208c1d445b4d","_uuid":"6439074cae264e396eed1b66850f5717870d0fc0","collapsed":true,"trusted":true},"cell_type":"code","source":"from nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import wordnet as wn\nfrom nltk.corpus import sentiwordnet as swn\nfrom nltk import sent_tokenize, word_tokenize, pos_tag\n \nlemmatizer = WordNetLemmatizer()\n \ndef penn_to_wn(tag):\n    \"\"\"\n    Convert between the PennTreebank tags to simple Wordnet tags\n    \"\"\"\n    if tag.startswith('J'):\n        return wn.ADJ\n    elif tag.startswith('N'):\n        return wn.NOUN\n    elif tag.startswith('R'):\n        return wn.ADV\n    elif tag.startswith('V'):\n        return wn.VERB\n    return None\n \ndef clean_text(text):\n    text = text.replace(\"<br />\", \" \")\n    #text = text.decode(\"utf-8\")\n    return text\n\ndef swn_polarity(text):\n    \"\"\"\n    Return a sentiment polarity: 0 = negative, 1 = positive\n    \"\"\"\n    tokens_count = 0\n    sentiment = 0.0\n    text = clean_text(text)\n    tagged_sentence = pos_tag(word_tokenize(text))\n    for word, tag in tagged_sentence:\n        wn_tag = penn_to_wn(tag)\n        if wn_tag not in (wn.NOUN, wn.ADJ, wn.ADV):\n            continue\n        lemma = lemmatizer.lemmatize(word, pos=wn_tag)\n        if not lemma:\n            continue\n        synsets = wn.synsets(lemma, pos=wn_tag)\n        if not synsets:\n            continue\n            \n        # Take the first sense, the most common\n        synset = synsets[0]\n        swn_synset = swn.senti_synset(synset.name())\n        #print(word, lemma)\n        #print(swn_synset.pos_score(), swn_synset.neg_score(), swn_synset.obj_score())\n        \n        if swn_synset.obj_score() == 1:\n            sentiment += swn_synset.pos_score() - swn_synset.neg_score() \n        \n        elif swn_synset.obj_score() != 1:\n            sentiment += swn_synset.pos_score() - swn_synset.neg_score() + swn_synset.obj_score()\n        \n        tokens_count += 1\n            \n    return sentiment\n\n# Function to be used in Parallelized data frame \ndef sentiment(df):\n    return df.apply(lambda x: swn_polarity(x))\n\n# Parallelize data frame operation \ncores = 4\ndef parallelize_dataframe(df, func):\n    df_split = np.array_split(df, cores)\n    with closing(Pool(cores)) as pool:\n        df = pd.concat(pool.map(func, df_split))\n    return df\n\n# Calculate Sentiment score \n\ndef sentiment_score(df):\n    df['senti_score'] = parallelize_dataframe(df['comment_text'], sentiment)\n    return df","execution_count":23,"outputs":[]},{"metadata":{"_cell_guid":"80798496-2b00-491c-a373-27fcd1d9d4ac","_uuid":"3a2fd63adbb572e81042855b72e05b290e0bea78","trusted":true},"cell_type":"code","source":"# Calculate sentiment score for Training and test data \nimport time \nt1 = time.time()\ntrain = sentiment_score(train)\nt2 = time.time()\nprint(\"Time taken is \"+str(t2-t1))\nprint(train.shape)","execution_count":24,"outputs":[]},{"metadata":{"_cell_guid":"1c77305e-9f35-42c5-a515-7bbece351d9e","_uuid":"8fa1915e72e1a5686f140ca33fdca9c943406adf","collapsed":true,"trusted":true},"cell_type":"code","source":"k1 = train['senti_score'].reshape(-1,1)","execution_count":25,"outputs":[]},{"metadata":{"_cell_guid":"849c8807-24b1-44ac-be8b-2b79ded423a2","_uuid":"80bb803ff004d4646fd74f7b1df1ddde7575612f","collapsed":true,"trusted":true},"cell_type":"code","source":"from sklearn import preprocessing\nscaler = preprocessing.MinMaxScaler()\ntrain['senti_score_scaled'] = scaler.fit_transform(k1)","execution_count":26,"outputs":[]},{"metadata":{"_cell_guid":"ba367f8d-6fb4-496e-aa62-b0a7c60a3bd7","_uuid":"ae64e37465c750565dbdeba26207972721a0a77a","collapsed":true,"trusted":true},"cell_type":"code","source":"train['comment_text'] = train['comment_text'].astype(str)","execution_count":27,"outputs":[]},{"metadata":{"_cell_guid":"876cf80b-6889-4b8a-b3f4-1afb87042b02","_uuid":"6c58281ae768d1be72e2a3686c17a261feb7b7be","trusted":true},"cell_type":"code","source":"tokenizer = text.Tokenizer(num_words=max_features)\n#all_text = np.hstack([test['comment_text'].str.lower(), train['comment_text'].str.lower()])\nall_text = np.hstack([train['comment_text'].str.lower()])\n\ntokenizer.fit_on_texts(all_text)\n\nprint(\"Fitting Done...Start text to sequence transform\")\n\ntrain['seq_comment']= tokenizer.texts_to_sequences(train.comment_text.str.lower())\nprint(\"Transform done for train \")\n","execution_count":28,"outputs":[]},{"metadata":{"_cell_guid":"f9396670-4eb4-4440-a55e-82d934efd5d7","_uuid":"ef45ce67afeeed0549b6e9b4d4ab4559a5991e33","collapsed":true,"trusted":true},"cell_type":"code","source":"#y_train = train[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]].values","execution_count":29,"outputs":[]},{"metadata":{"_cell_guid":"7b22c3b0-2ac4-48eb-9f32-d0c3a95cbebd","_uuid":"5efee36572aba31e38127d2dbbc31417a51bc363","trusted":true},"cell_type":"code","source":"x_train, x_valid, y_train, y_valid = train_test_split(train, y_train, train_size=0.95, random_state=233)\nprint(x_train.shape, x_valid.shape)\nprint(y_train.shape, y_valid.shape)","execution_count":30,"outputs":[]},{"metadata":{"_cell_guid":"47361681-1d1f-42e5-b200-f1750c15a68e","_uuid":"a0eb5e25c9ec800c969ca51af3b45ca25bca9714","collapsed":true,"trusted":true},"cell_type":"code","source":"def get_keras_data(dataset):\n    X = { \n          'comment_text' : pad_sequences(dataset.seq_comment, maxlen=maxlen), \n          'senti_score_scaled': np.array(dataset.senti_score)\n    } \n    return X","execution_count":31,"outputs":[]},{"metadata":{"_cell_guid":"3fcade4d-b1ce-4287-8055-3235d9d5edd8","_uuid":"7374044ff80a6a670db83de508bd7bb3d0577441","collapsed":true,"trusted":true},"cell_type":"code","source":"X_train = get_keras_data(x_train)\nX_valid = get_keras_data(x_valid)","execution_count":32,"outputs":[]},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","scrolled":false,"trusted":true},"cell_type":"code","source":"class RocAucEvaluation(Callback):\n    def __init__(self, validation_data=(), interval=1):\n        super(Callback, self).__init__()\n\n        self.interval = interval\n        print(self.interval)\n        self.X_val, self.y_val = validation_data\n        print(self.y_val)\n\n    def on_epoch_end(self, epoch, logs={}):\n        if epoch % self.interval == 0:\n            y_pred = self.model.predict(self.X_val, verbose=0)\n            score = roc_auc_score(self.y_val, y_pred)\n            print(\"\\n ROC-AUC - epoch: %d - score: %.6f \\n\" % (epoch+1, score))\n\ndef get_model1():\n    \n    inp = Input(shape = [X_train['comment_text'].shape[1]], name = 'comment_text')\n    #senti_score = Input(shape=[1], name=\"senti_score_scaled\")\n    x = Embedding(max_features, embed_size)(inp)\n    x = SpatialDropout1D(0.2)(x)\n    x = Bidirectional(GRU(40, return_sequences=True, dropout = 0.15, recurrent_dropout = 0.15))(x)\n    avg_pool = GlobalAveragePooling1D()(x)\n    max_pool = GlobalMaxPooling1D()(x)\n    conc = concatenate([avg_pool, max_pool])\n    outp = Dense(6, activation='sigmoid')(conc)\n    \n    model = Model([inp], outp)\n    model.compile(loss='binary_crossentropy',\n                  optimizer='adam',\n                  metrics=['accuracy'])\n\n    return model\n\nmodel1 = get_model1()\n\nbatch_size = 128\nepochs = 1\n\nRocAuc = RocAucEvaluation(validation_data=(X_valid, y_valid), interval=1)\nos.environ['OMP_NUM_THREADS'] = '4'\n\nexp_decay = lambda init, fin, steps: (init/fin)**(1/(steps-1)) - 1\nsteps = int(len(X_train)/batch_size) * epochs\nlr_init, lr_fin = 0.009, 0.0045\nlr_decay = exp_decay(lr_init, lr_fin, steps)\nK.set_value(model1.optimizer.lr, lr_init)\nK.set_value(model1.optimizer.decay, lr_decay)\n\nfor i in range(3):\n    hist = model1.fit(X_train, y_train, batch_size=batch_size+(batch_size*(2*i)), epochs=epochs, validation_data=(X_valid,y_valid), callbacks=[RocAuc], verbose=1)\n\nprint(\"Model 1 Done\")","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ff89e7b2-7645-4b3d-adc3-002a1c654bf0","_uuid":"1974a4b4b68546e62c3b6cf9e978bdb5d444acef","collapsed":true,"trusted":true},"cell_type":"code","source":"import gensim \nw2v = gensim.models.KeyedVectors.load_word2vec_format('../input/googlenews-vectors-negative300/GoogleNews-vectors-negative300.bin', binary=True)\nprint(\"Done loading model\")","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f3a15c14-2a21-403f-8f82-07b37082cf2b","_uuid":"c502557be7cdf417c1b60f458da359ab750c3c39","collapsed":true,"trusted":true},"cell_type":"code","source":"vocab_size = len(tokenizer.word_index)+1\nEMBEDDING_DIM = 300 # this is from the pretrained vectors\nembedding_matrix = np.zeros((vocab_size, EMBEDDING_DIM))\nprint(embedding_matrix.shape)\n# Creating Embedding matrix \nc = 0 \nc1 = 0 \nw_Y = []\nw_No = []\nfor word, i in tokenizer.word_index.items():\n    if word in w2v:\n        c +=1\n        embedding_vector = w2v[word]\n        w_Y.append(word)\n    else:\n        embedding_vector = None\n        #embedding_vector = np.sum(embedding_matrix, axis = 0)\n        w_No.append(word)\n        c1 +=1\n    if embedding_vector is not None:    \n        embedding_matrix[i] = embedding_vector\n\nprint(c,c1, len(w_No), len(w_Y))\nprint(embedding_matrix.shape)\n\nclass RocAucEvaluation(Callback):\n    def __init__(self, validation_data=(), interval=1):\n        super(Callback, self).__init__()\n\n        self.interval = interval\n        print(self.interval)\n        self.X_val, self.y_val = validation_data\n        print(self.y_val)\n\n    def on_epoch_end(self, epoch, logs={}):\n        if epoch % self.interval == 0:\n            y_pred = self.model.predict(self.X_val, verbose=0)\n            score = roc_auc_score(self.y_val, y_pred)\n            print(\"\\n ROC-AUC - epoch: %d - score: %.6f \\n\" % (epoch+1, score))\n            \ndef get_model2():\n    \n    inp = Input(shape = [X_train['comment_text'].shape[1]], name = 'comment_text')\n    #senti_score = Input(shape=[1], name=\"senti_score_scaled\")\n    #key_words_scaled = Input(shape=[1], name=\"key_words_scaled\")\n    print(\"Here\")\n    x = Embedding(vocab_size, EMBEDDING_DIM, weights = [embedding_matrix], trainable = True)(inp)\n\n    x = SpatialDropout1D(0.2)(x)\n    x = Bidirectional(GRU(40, return_sequences=True, dropout = 0.15, recurrent_dropout = 0.15))(x)\n    avg_pool = GlobalAveragePooling1D()(x)\n    max_pool = GlobalMaxPooling1D()(x)\n    conc = concatenate([avg_pool, max_pool])\n    outp = Dense(6, activation=\"sigmoid\")(conc)\n    \n    model = Model([inp], outp)\n    model.compile(loss='binary_crossentropy',\n                  optimizer='adam',\n                  metrics=['accuracy'])\n\n    return model\n\nmodel2 = get_model2()\n\nRocAuc = RocAucEvaluation(validation_data=(X_valid, y_valid), interval=1)\nos.environ['OMP_NUM_THREADS'] = '4'\n\nexp_decay = lambda init, fin, steps: (init/fin)**(1/(steps-1)) - 1\nsteps = int(len(X_train)/batch_size) * epochs\nlr_init, lr_fin = 0.009, 0.0045\nlr_decay = exp_decay(lr_init, lr_fin, steps)\nK.set_value(model2.optimizer.lr, lr_init)\nK.set_value(model2.optimizer.decay, lr_decay)\n\nfor i in range(3):\n    hist = model2.fit(X_train, y_train, batch_size=batch_size+(batch_size*(2*i)), epochs=epochs, validation_data=(X_valid,y_valid), callbacks=[RocAuc], verbose=1)\n\nprint(\"Model 2 Done\")\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"380de146-a503-48a7-8118-a40c5d7e9b73","_uuid":"0f6000e99d769b6c7784df67a55751bff83b63d6","collapsed":true,"trusted":true},"cell_type":"code","source":"del X_train, X_valid, y_valid\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"d2eaa2ef-0bd7-4426-a7be-cf0f9d2d6c60","_uuid":"85fc2b7bb38270611c5a61fe35df05224fb33ebd","collapsed":true,"trusted":true},"cell_type":"code","source":"y_train = train[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]].values","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c8f8659b-5386-429a-8e15-af40115ede89","_uuid":"b128093458d9737dcb6774ae38f266ffd9c3b2ef","collapsed":true},"cell_type":"markdown","source":"# sklearn \nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.pipeline import Pipeline, FeatureUnion \nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.preprocessing import MinMaxScaler\n\nclass RocAucEvaluation(Callback):\n    def __init__(self, validation_data=(), interval=1):\n        super(Callback, self).__init__()\n\n        self.interval = interval\n        print(self.interval)\n        self.X_val, self.y_val = validation_data\n        print(self.y_val)\n\n    def on_epoch_end(self, epoch, logs={}):\n        if epoch % self.interval == 0:\n            y_pred = self.model.predict(self.X_val, verbose=0)\n            score = roc_auc_score(self.y_val, y_pred)\n            print(\"\\n ROC-AUC - epoch: %d - score: %.6f \\n\" % (epoch+1, score))\n            \nclass TextSelector(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Transformer to select a single column from the data frame to perform additional transformations on\n    Use on text columns in the data\n    \"\"\"\n    def __init__(self, key):\n        self.key = key\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        return X[self.key]\n    \nclass NumberSelector(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Transformer to select a single column from the data frame to perform additional transformations on\n    Use on numeric columns in the data\n    \"\"\"\n    def __init__(self, key):\n        self.key = key\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        return X[[self.key]]\n    \nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.linear_model import LogisticRegressionCV\n\np1 = Pipeline([('selector', TextSelector(key='comment_text')),('comment_text', TfidfVectorizer( stop_words='english', max_features = 50000, ngram_range =(1,2)))])\np2 = Pipeline([('selector', NumberSelector(key='senti_score_scaled')),('senti_score_scaled', MinMaxScaler() )])\nvectorizer = FeatureUnion([('comment_text', p1), ('senti_score_scaled', p2)], n_jobs =4)\n\nimport time\nt1 = time.time()\nvectorizer.fit(train)\nprint(\"Vectorizer Fitting done\")\nX_train = vectorizer.transform(train).astype(np.float32)\nprint(type(X_train))\nprint(\"Vectorizer Transformation completed\")\n\nprint(X_train.shape, y_train.shape)\nX_train1, X_valid1, y_train1, y_valid1 = train_test_split(X_train, y_train, train_size=0.95, random_state=233)\nprint(X_train1.shape, X_valid1.shape)\nprint(y_train1.shape, y_valid1.shape)\n\ndef get_model5():\n    \n    inp = Input(shape = [X_train.shape[1]], sparse = True)\n    #senti_score = Input(shape=[1], name=\"senti_score_scaled\")\n    #key_words_scaled = Input(shape=[1], name=\"key_words_scaled\")\n    #x = Embedding(max_features, embed_size)(inp)\n    #x = Embedding(vocab_size, EMBEDDING_DIM, weights = [embedding_matrix], trainable = True)(inp)\n    #x = concatenate([x, senti_score])\n    #x = BatchNormalization()(x)\n    \n    x  = Dense(128, activation=\"sigmoid\")(inp)\n    x  = Dense(64, activation=\"sigmoid\")(x)\n    \n    #conc = concatenate([x])\n\n    outp = Dense(6, activation=\"sigmoid\")(x)\n    \n    model = Model([inp], outp)\n    model.compile(loss='binary_crossentropy',\n                  optimizer='adam',\n                  metrics=['accuracy'])\n\n    return model\n\nmodel5 = get_model5()\n\nRocAuc = RocAucEvaluation(validation_data=(X_valid1, y_valid1), interval=1)\nos.environ['OMP_NUM_THREADS'] = '4'\n\nbatch_size = 64\nepochs = 1\n\nexp_decay = lambda init, fin, steps: (init/fin)**(1/(steps-1)) - 1\nsteps = int(X_train1.shape[0]/batch_size) * epochs\nlr_init, lr_fin = 0.001, 0.0005\nlr_decay = exp_decay(lr_init, lr_fin, steps)\nK.set_value(model5.optimizer.lr, lr_init)\nK.set_value(model5.optimizer.decay, lr_decay)\n\nfor i in range(5):\n    hist = model5.fit(x = X_train1, y = y_train1, batch_size=batch_size+(batch_size*(2*i)), epochs=epochs, validation_data=(X_valid1,y_valid1), callbacks=[RocAuc], verbose=1)\n\nprint(\"Model 5 Done\")"},{"metadata":{"_cell_guid":"68ac27d6-40f3-40fd-b24f-63a40ba75c2b","_uuid":"52856d5a5a507eb6c970f4d12a47f878875740c6","collapsed":true,"scrolled":true,"trusted":true},"cell_type":"code","source":"t1 = time.time()\n\nimport gc\ndef load_test():\n    for df in pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/test.csv', chunksize= 150000):\n        yield df\n\nprint(\"Yield complete\")\n\ntest_ids = np.array([], dtype=np.int32)\npreds= np.zeros((0,6), dtype = np.int32)\n\nprint(\"Start Batch Prediction\")\nc = 0 \n\nfor df in load_test():\n    \n    c +=1\n    print(\"Chunk number is \"+str(c))\n    sentiment_score(df)\n    k2 = df['senti_score'].reshape(-1,1)\n    df['senti_score_scaled'] = scaler.transform(k2)\n    df['seq_comment'] = tokenizer.texts_to_sequences(df.comment_text.str.lower())\n    print(\"Transform done for test \")\n    X_test = get_keras_data(df)\n    y_pred1 = model1.predict(X_test, batch_size=1024, verbose =1)\n    y_pred2 = model2.predict(X_test, batch_size=1024, verbose =1)\n    \n    test_id = df['id']\n    del df['seq_comment'], df['senti_score'], X_test\n    gc.collect()\n    \n    #X_test = vectorizer.transform(df).astype(np.float32)\n    #y_pred5 = model5.predict(X_test, batch_size=1024, verbose =1)\n    #print(y_pred5.shape)\n    \n    k = (y_pred1+y_pred2)/2\n    print(k.shape)\n    preds= np.append(preds,k, axis = 0)\n    print(preds.shape)\n    test_ids = np.append(test_ids, test_id)\n\nprint(\"All chunks done\")\nt2 = time.time()\nprint(\"Total time for Parallel Batch Prediction is \"+str(t2-t1))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"4b64cc9d-a0a3-4956-adb6-4aa9267c1a18","_uuid":"4a1c1d67f9e0dae03f65a351a72142144069d397","collapsed":true,"trusted":true},"cell_type":"code","source":"submission = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/sample_submission.csv')\n#y_pred1 = model.predict(X_test, batch_size=1024, verbose =1)\n#y_pred2 = model1.predict(X_test, batch_size=1024, verbose =1)\n#y_pred3 = model3.predict(X_test, batch_size=1024, verbose =1)\nsubmission[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]] = preds\nsubmission.to_csv('submission.csv', index=False)\nend_time = time.time()\nprint(\"Total time taken is \"+str(end_time-start_time))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"68873cf1-0c77-4acf-94e4-8fe8d7951046","_uuid":"60a49464ac19136364b4ada5442d31a59e2dc448","collapsed":true,"trusted":true},"cell_type":"code","source":"y_pred1[0]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"4b55be71-6942-4565-b905-6ade957bf5cc","_uuid":"ee05cb90a0eca2bc38844d112cc38614cb3bcbea","collapsed":true,"trusted":true},"cell_type":"code","source":"y_pred2[0]","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}