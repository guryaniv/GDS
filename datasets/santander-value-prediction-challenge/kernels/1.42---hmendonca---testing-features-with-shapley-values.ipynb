{"cells":[{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import gc \n\nimport scipy\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.decomposition import FastICA\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nfrom sklearn.random_projection import GaussianRandomProjection\nfrom sklearn.random_projection import SparseRandomProjection\nfrom sklearn.manifold import TSNE\nfrom sklearn.preprocessing import scale\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"41d836946df80423691f23bd307e739ac1a6a031"},"cell_type":"markdown","source":"# 1. Feature Engineering\n## 1.1. Data Loading & Pre-processing\nSummary:\n* Remove columns with zero variance\n* Remove duplicate columns and rows\n* Log-transform all columns\n* Mean-variance scale all columns excepting sparse entries"},{"metadata":{"trusted":true,"_uuid":"86cbbd9ddd5ad94b4914c13dda1f2096e759f74c","collapsed":true},"cell_type":"code","source":"# Read train and test files\ntrain_df = pd.read_csv('../input/train.csv')\ntest_df = pd.read_csv('../input/test.csv')\n\n# Find and drop duplicate rows\nt = train_df.iloc[:,2:].duplicated(keep=False)\nduplicated_indices = t[t].index.values\nprint(\"Removed {} duplicated rows: {}\".format(len(duplicated_indices), duplicated_indices))\ntrain_df.iat[duplicated_indices[0], 1] = np.expm1(np.log1p(train_df.target.loc[duplicated_indices]).mean()) # keep and update first with log mean\ntrain_df.drop(duplicated_indices[1:], inplace=True) # drop remaining\n\n# Get the combined data\ntotal_df = pd.concat([train_df.drop('target', axis=1), test_df], axis=0).drop('ID', axis=1)\n\n# Get the target\ny = np.log1p(train_df.target)\n\n# Train and test\ntrain_idx = range(0, len(train_df))\ntest_idx = range(len(train_df), len(total_df))\n\n# Columns to drop because there is no variation in training set\nzero_std_cols = train_df.drop(\"ID\", axis=1).columns[train_df.std() == 0]\ntotal_df.drop(zero_std_cols, axis=1, inplace=True)\nprint(\"Removed {} constant columns\".format(len(zero_std_cols)))\n\n# Removing duplicate columns\n_, unique_indices = np.unique(total_df.iloc[train_idx], axis=1, return_index=True)\ncolsToRemove = [i for i in range(total_df.shape[1]) if i not in unique_indices]\ntotal_df = total_df.iloc[:, unique_indices]\nprint(\"Dropped {} duplicate columns: {}\".format(len(colsToRemove), colsToRemove))\n\n# Log-transform all column\ntotal_df = np.log1p(total_df)\n\n# Scale non-zero column values\nfor col in total_df.columns:    \n    nonzero_rows = total_df[col] != 0\n    total_df.loc[nonzero_rows, col] = scale(total_df.loc[nonzero_rows, col].values)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"baa7a8420cc1362737fae2ad5d9532edec0af2de"},"cell_type":"markdown","source":"## 1.2. Aggregates\nSome of the suggested aggregation features..."},{"metadata":{"trusted":true,"_uuid":"b4d062e00c7f0988cf9453c493b4bef5a4698808","_kg_hide-input":true,"collapsed":true},"cell_type":"code","source":"aggregate_df = pd.DataFrame()\n\nn0_df = total_df[total_df>0]\n\n# V1 Features\naggregate_df['n0_count'] = total_df.astype(bool).sum(axis=1)\naggregate_df['n0_mean'] = n0_df.mean(axis=1)\naggregate_df['n0_median'] = n0_df.median(axis=1)\naggregate_df['n0_kurt'] = n0_df.kurt(axis=1)\naggregate_df['n0_min'] = n0_df.min(axis=1)\naggregate_df['n0_std'] = n0_df.std(axis=1)\naggregate_df['n0_skew'] = n0_df.skew(axis=1)\naggregate_df['mean'] = total_df.mean(axis=1)\naggregate_df['std'] = total_df.std(axis=1)\naggregate_df['max'] = total_df.max(axis=1)\naggregate_df['nunique'] = total_df.nunique(axis=1)\naggregate_df['sum_zeros'] = (total_df == 0).astype(int).sum(axis=1)\naggregate_df['geometric_mean'] = n0_df.apply(\n    lambda x: np.exp(np.log(x).mean()), axis=1\n)\ndel n0_df\n\naggregate_df.reset_index(drop=True, inplace=True)\nprint(\"Created features for: {}\".format(aggregate_df.columns.values))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"acf6ca30388ac0b6432fb77cebe3c216d2eae13d"},"cell_type":"markdown","source":"## 1.3. Unsupervised Feature Learning\n### 1.3.1. Decomposition Methods\nLots of people have been using decomposition methods to reduce the number of features. From my trials in [this notebook](https://www.kaggle.com/nanomathias/linear-regression-with-elastic-net), it seems like often it's only the first 10-20 components that are actually important for the modeling. Since we are testing features now, here I'll include 10 of each decomposition method."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"663e156528c534a8c64a39fb21553627b678ae63","collapsed":true},"cell_type":"code","source":"COMPONENTS = 10\n\n# Convert to sparse matrix\nsparse_matrix = scipy.sparse.csr_matrix(total_df.values)\n\n# V1 List of decomposition methods\nmethods = [\n    {'method': TSNE(n_components=3, init='pca'), 'data': 'train'},\n    {'method': TruncatedSVD(n_components=COMPONENTS), 'data': 'sparse'},\n    {'method': PCA(n_components=COMPONENTS), 'data': 'total'},\n    {'method': FastICA(n_components=COMPONENTS), 'data': 'total'},\n    {'method': GaussianRandomProjection(n_components=COMPONENTS, eps=0.1), 'data': 'total'},\n    {'method': SparseRandomProjection(n_components=COMPONENTS, dense_output=True), 'data': 'total'}\n]\n\n# Run all the methods\nembeddings = []\nfor run in methods:\n    name = run['method'].__class__.__name__\n    \n    # Run method on appropriate data\n    if run['data'] == 'sparse':\n        embedding = run['method'].fit_transform(sparse_matrix)\n    elif run['data'] == 'train':\n        embedding = run['method'].fit_transform(total_df.iloc[train_idx])\n    else:\n        embedding = run['method'].fit_transform(total_df)\n        \n    # Save in list of all embeddings\n    embeddings.append(\n        pd.DataFrame(embedding, columns=[f\"{name}_{i}\" for i in range(embedding.shape[1])])\n    )\n    print(f\">> Ran {name}\")\n    \n# Put all components into one dataframe\ncomponents_df = pd.concat(embeddings, axis=1).reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a57cbc94ac6bdb5e7fa53802f371fcb5aa26caf0"},"cell_type":"markdown","source":"### 1.3.2. Dense Autoencoder\nI saw a few people use autoencoders, but here I just implement a very simple one. From empirical tests it seems that the components I extract from this it doesn't make sense to have an embedded dimension higher than about 10, in terms of local CV score on the target at the end at least. \n\nThese features do decently well, so I think it's worth investigating further in terms of hyperparameter tuning."},{"metadata":{"trusted":true,"_uuid":"d0194d5f53a8b942290250896182ceb4587c0367","_kg_hide-input":true,"collapsed":true},"cell_type":"code","source":"from keras.layers import *\nfrom keras.optimizers import *\nfrom keras.models import Model, Sequential\n\nencoding_dim = 5\n\nenc_input = Input((total_df.shape[1], ))\nenc_output = Dense(128, activation='relu')(enc_input)\nenc_output = Dropout(0.5)(enc_output)\nenc_output = Dense(encoding_dim, activation='relu')(enc_output)\n\ndec_input = Dense(128, activation='relu')(enc_output)\ndec_output = Dropout(0.5)(dec_input)\ndec_output = Dense(total_df.shape[1], activation='relu')(dec_output)\n\n# This model maps an input to its reconstruction\nvanilla_encoder = Model(enc_input, enc_output)\nvanilla_autoencoder = Model(enc_input, dec_output)\nvanilla_autoencoder.compile(optimizer=Adam(0.0001), loss='mean_squared_error')\nvanilla_autoencoder.summary()\n\n# Fit the autoencoder\nvanilla_autoencoder.fit(\n    total_df.values, total_df.values,\n    epochs=6, batch_size=64,\n    shuffle=True\n)\n\n# Put into dataframe\ndense_ae_df = pd.DataFrame(\n    vanilla_encoder.predict(total_df.values, batch_size=64), \n    columns=['dense_AE_{}'.format(i) for i in range(encoding_dim)]\n).reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"71678831153ff2fd3d8ffcd87cd069e3d5f49c7e"},"cell_type":"markdown","source":"## 1.4 Time series features\nfrom: https://www.kaggle.com/hmendonca/training-data-analyzes-time-series"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"448cd8670344675467bd1e2239db7b0b1c6a0904"},"cell_type":"code","source":"cols = ['f190486d6', '58e2e02e6', 'eeb9cd3aa', '9fd594eec', '6eef030c1',\n        '15ace8c9f', 'fb0f5dbfe', '58e056e12', '20aa07010', '024c577b9',\n        'd6bb78916', 'b43a7cfd5', '58232a6fb', '1702b5bf0', '324921c7b',\n        '62e59a501', '2ec5b290f', '241f0f867', 'fb49e4212', '66ace2992',\n        'f74e8f13d', '5c6487af1', '963a49cdc', '26fc93eb7', '1931ccfdd',\n        '703885424', '70feb1494', '491b9ee45', '23310aa6f', 'e176a204a',\n        '6619d81fc', '1db387535', 'fc99f9426', '91f701ba2', '0572565c2',\n        '190db8488', 'adb64ff71', 'c47340d97', 'c5a231d81', '0ff32eb98']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8d071ece4722b7806156a0b10c6eab94737f76d5","collapsed":true},"cell_type":"code","source":"def moving_avg(df, prefix, win_size):\n    print('Creating rolling average on {} columns'.format(win_size))\n    ts = df.T.rolling(win_size*2, min_periods=1, center=True, win_type='gaussian').mean(std=win_size/2).iloc[ win_size//2 : : win_size ].T # rolling average\n    ts.columns = [prefix+str(n) for n in np.arange(ts.columns.size)]\n    ## also calculate moving deltas\n    dts = pd.DataFrame(ts.iloc[:,:-1].values - ts.iloc[:,1:].values)\n    dts.columns = ['d_'+prefix+str(n) for n in np.arange(dts.columns.size)]\n    return ts.join(dts)\n\ndef moving_max(df, prefix, win_size):\n    print('Creating rolling max on {} columns'.format(win_size))\n    ts = df.T.rolling(win_size*2, min_periods=1, center=True).max().iloc[ win_size//2 : : win_size ].T # rolling max\n    ts.columns = [prefix+str(n) for n in np.arange(ts.columns.size)]\n    ## also calculate moving deltas\n    dts = pd.DataFrame(ts.iloc[:,:-1].values - ts.iloc[:,1:].values)\n    dts.columns = ['d_'+prefix+str(n) for n in np.arange(dts.columns.size)]\n    return ts.join(dts)\n\nrolling_df = pd.DataFrame()    \n## yearly, quartally, monthly, fortnightly, weekly mean\n# rolling_df = pd.concat([rolling_df, moving_avg(total_df, 'y_', 365)], axis=1)\n# rolling_df = pd.concat([rolling_df, moving_avg(total_df, 'q_', 91)], axis=1)\nrolling_df = pd.concat([rolling_df, moving_avg(total_df[cols], 'm_', 30)], axis=1)\nrolling_df = pd.concat([rolling_df, moving_avg(total_df[cols], 'f_', 15)], axis=1)\nrolling_df = pd.concat([rolling_df, moving_avg(total_df[cols], 'd_', 7)], axis=1)\n##  ... and max\n# rolling_df = pd.concat([rolling_df, moving_max(total_df, 'ym_', 365)], axis=1)\n# rolling_df = pd.concat([rolling_df, moving_max(total_df, 'qm_', 91)], axis=1)\nrolling_df = pd.concat([rolling_df, moving_max(total_df[cols], 'mm_', 30)], axis=1)\nrolling_df = pd.concat([rolling_df, moving_max(total_df[cols], 'fm_', 15)], axis=1)\nrolling_df = pd.concat([rolling_df, moving_max(total_df[cols], 'dm_', 7)], axis=1)\n\nrolling_df.reset_index(inplace=True, drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2a5085768ccfbf98ff0395e0132eff442839cfe5","collapsed":true},"cell_type":"code","source":"# Put all features into one dataframe (i.e. aggregate, timeseries, components)\nfeature_df = pd.concat([components_df, aggregate_df, dense_ae_df, rolling_df], axis=1).fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"513753bf0c80e17f343f70304278d541df9adc36"},"cell_type":"markdown","source":"## 2. Lightgbm test + feature importance"},{"metadata":{"trusted":true,"_uuid":"369529da61b678fab226e88f95fa99ad35592d97","collapsed":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nimport lightgbm as lgb\n\ndev_X, val_X, dev_y, val_y = train_test_split(feature_df.iloc[train_idx], y, test_size = 0.15, random_state = 42)\n\ndef run_lgb(train_X, train_y, val_X, val_y, test_X=None):\n    params = {\n        \"objective\" : \"regression\",\n        \"metric\" : \"rmse\",\n        \"num_leaves\" : 140,\n        #\"n_estimators\" : 700,\n        \"max_depth\" : 13,\n        \"max_bin\" : 55,\n        \"learning_rate\" : 0.005,\n        \"feature_fraction\" : 0.9,\n        \"bagging_fraction\" : 0.8,\n        \"bagging_frequency\" : 5,\n        \"bagging_seed\" : 42,\n        \"verbosity\" : -1,\n        \"seed\": 42\n    }\n    \n    lgtrain = lgb.Dataset(train_X, label=train_y)\n    lgval = lgb.Dataset(val_X, label=val_y)\n    evals_result = {}\n    model = lgb.train(params, lgtrain, 5000, \n                      valid_sets=[lgval], \n                      early_stopping_rounds=100, \n                      verbose_eval=200, \n                      evals_result=evals_result)\n    \n    if test_X is not None:\n        pred_test_y = np.expm1(model.predict(test_X, num_iteration=model.best_iteration))\n        return pred_test_y, model, evals_result\n    else:\n        return model, evals_result\n\n#pred_test, model, evals_result = run_lgb(dev_X, dev_y, val_X, val_y, feature_df.iloc[test_idx])\nmodel, evals_result = run_lgb(dev_X, dev_y, val_X, val_y)\nprint(\"LightGBM Training Completed...\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cb2dcc547c80587c4ce7b73815060f8354b2c9ac","collapsed":true},"cell_type":"code","source":"# feature importance\ngain = model.feature_importance('gain')\nfeatureimp = pd.DataFrame({\n        'feature':model.feature_name(),\n        'gain': gain / gain.sum()\n    }).sort_values('gain', ascending=False)\n\nplt.figure(figsize=(15,20))\nsns.barplot(x=\"gain\", y=\"feature\", data=featureimp[:50])\nplt.title('LightGBM Feature Importance (gain)')\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bfcf6e2682171d4e4ffc6990d4ca02f76e57bf16","collapsed":true},"cell_type":"code","source":"# feature importance (split)\nsplit = model.feature_importance('split')\nfeatureimp = pd.DataFrame({\n        'feature':model.feature_name(), \n        'split': split / split.sum()\n    }).sort_values('split', ascending=False)\n\nplt.figure(figsize=(15,20))\nsns.barplot(x=\"split\", y=\"feature\", data=featureimp[:50])\nplt.title('LightGBM Feature Importance (split)')\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a85b8a51e85c7ea214a3fabbb4aff719103dff89"},"cell_type":"markdown","source":"## Shapley values\nhttps://towardsdatascience.com/interpretable-machine-learning-with-xgboost-9ec80d148d27"},{"metadata":{"trusted":true,"_uuid":"86cfa407d9119085400867340c3968efc1d5e35c","collapsed":true},"cell_type":"code","source":"import shap, xgboost\n\n# train XGBoost model\n# xgb_model = xgboost.train({\"learning_rate\": 0.01}, xgboost.DMatrix(dev_X, label=dev_y), 100)\n\n# explain the model's predictions using SHAP values\n# (same syntax works for LightGBM, CatBoost, and scikit-learn models)\nexplainer = shap.TreeExplainer(model)\nshap_values = explainer.shap_values(dev_X)\n\n# feature importance\nshap.summary_plot(shap_values, dev_X, plot_type=\"bar\", max_display=50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e990032fd7dcb961dc675fa800e82c4e5b53bc9a","collapsed":true},"cell_type":"code","source":"# load JS visualization code to notebook\nshap.initjs()\n# visualize the first prediction's explanation\nshap.force_plot(shap_values[0,:], dev_X.iloc[0,:])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fe86f33312d1fc87f19c4318309f98f2f9d119dc","collapsed":true},"cell_type":"code","source":"# summarize the effects of top features\nshap.summary_plot(shap_values, dev_X, max_display=50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"39b7ef391c308476a288b065016f3b538b320184","collapsed":true},"cell_type":"code","source":"from sklearn.feature_selection import SelectPercentile\npercentile = 20\nskb = SelectPercentile(score_func=lambda X,y: np.mean(np.abs(shap_values[:, :dev_X.shape[1]]), axis=0), percentile=percentile).fit(dev_X.values, dev_y.values)\nprint(\"Using {} features\".format(skb.transform(val_X.values).shape[1]))\npred_test, s_model, s_evals_result = run_lgb(skb.transform(dev_X.values), dev_y,\n                                         skb.transform(val_X.values), val_y,\n                                         skb.transform(feature_df.iloc[test_idx].values))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"88eab58b06c79e8fba19e7e53af653468e7f8bd7","collapsed":true},"cell_type":"code","source":"sub = pd.read_csv('../input/sample_submission.csv')\nsub[\"target\"] = pred_test\nprint(sub.head())\nsub.to_csv('feat_lgb{:.3f}.csv'.format(evals_result['valid_0']['rmse'][-1]), index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}