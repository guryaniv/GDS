{"cells":[{"metadata":{"collapsed":true,"trusted":false,"_uuid":"db4c118e567b00216e00bedd7a1d27956a4b44cc"},"cell_type":"code","source":"#Imbalanced Classification Problem\n#Import the necessary libraries\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"9afe1dce8b60b069d13df85588c965463f0ce202"},"cell_type":"code","source":"#Read the input file\n\ndataset = pd.read_csv('../input/creditcard.csv')\ndataset.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"21f3351288223e9690ef54442baed45828217bb8"},"cell_type":"code","source":"#Check for missing values\ndataset.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"9bcaeafcf160b75f02f00ce0e84d57620ccf8f7a"},"cell_type":"code","source":"dataset.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"48e59cd447d647e25d2b7df0f2412b1c209a4425"},"cell_type":"code","source":"#Find out number of values in each class. \ndataset['Class'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"16a301df862f37510dad02b565d516004d7c68c1"},"cell_type":"code","source":"#The ratio of 0s to 1s is 1:577. This is a clear case of Imbalances class classification\nX = dataset.drop('Class',axis=1)\ny = dataset['Class']","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"9b8125f47efaadd5733560bd7a2b0ce391bd3f69"},"cell_type":"code","source":"X.head()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"0d02e0bc84ae2127eb01ad5394dde2b8443d2e14"},"cell_type":"code","source":"#Being imbalanced classification, we cannot ensure the predictions will be correct for the minority class\n#So we will perform a SMOTE to balance out the two classes. Lets split the dataset into train and test before that\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"e2d7a12e4343bad6a57fb384e208dd350bef02cd"},"cell_type":"code","source":"y_test.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"2db1ca83c3774029c47af88c554c74534460f78d"},"cell_type":"code","source":"#Smote will only be applied on the training dataset\n\nfrom imblearn.over_sampling import SMOTE\nprint('Before Oversampling: \\n')\nprint('Count of labels with 0: {}\\n'.format(sum(y_train==0)))\nprint('Count of labels with 1: {}\\n'.format(sum(y_train==1)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"2aa2d9ef8e33a1fa1af82d138c7db98cb92997ac"},"cell_type":"code","source":"sm = SMOTE(random_state=2,k_neighbors=5)\nX_train_res,y_train_res = sm.fit_sample(X_train,y_train.ravel())\n\nprint('After Oversampling \\n')\nprint('Count of labels with 0: {}\\n'.format(sum(y_train_res==0)))\nprint('Count of labels with 1: {}\\n'.format(sum(y_train_res==1)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"cb3b7eb16f017229749ab03b87989ff9a8b2b68d"},"cell_type":"code","source":"#Perform a Kfold cross validation and train/test the folds using LogisticRegression algorithm\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\n\nparameters = {'C':(0.01,0.1,2,10,50)}\nlr = LogisticRegression()\nclf = GridSearchCV(lr,parameters,cv=5,verbose=5)\nclf.fit(X_train_res,y_train_res.ravel()) \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"d9cc01369df4365dab42a538f1072c6a1081674b"},"cell_type":"code","source":"clf.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"5d801af15fb27630f7209489ab498d99d8d4f883"},"cell_type":"code","source":"lr1 = LogisticRegression(C=0.1,penalty='l2',verbose=5)\nlr1.fit(X_train_res,y_train_res.ravel())","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"6ea530b03053ed43c27f7bf9e1785b24c6cb6af1"},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix,recall_score,roc_auc_score,precision_score\ny_pre = lr1.predict(X_test)\ncnf_matrix = confusion_matrix(y_test, y_pre)\nprint(cnf_matrix)\n#The number of correct predictions total 84192+129 = 84321. Only 18 predictions have turned out to be false negative. \n#It is reasonable because lower the FN, (higher recall) better the accuracy of the predictions in this case\n#Number of false positives (precision is low) is high","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"a624db5509a8bdf5d820b0d5c6b1781d388a57a6"},"cell_type":"code","source":"print('Recall metric in the test dataset: ',recall_score(y_test,y_pre))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"efd388ff9c79a80f05bf45f33779371047dfa074"},"cell_type":"code","source":"print('Precision metric in the test dataset: ',precision_score(y_test,y_pre))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"1e1ea7d77c17233f903ac7b9458f3771fb47c97e"},"cell_type":"code","source":"from sklearn.metrics import roc_curve,auc\n\ntmp1 = lr1.fit(X_train_res,y_train_res.ravel())\ny_pred_sample_score = tmp1.decision_function(X_test)\n\nfpr,tpr,thresholds = roc_curve(y_test,y_pred_sample_score)\nroc_auc = auc(fpr,tpr)\n\nprint(roc_auc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"533237b77f81263dd74caf047a35a38719bd4874"},"cell_type":"code","source":"plt.title('Receiver Operating Characteristic')\nplt.plot(fpr,tpr,'b',label = 'AOC %0.2f'% roc_auc)\nplt.plot([0,1],[0,1],'r--')\nplt.legend(loc='lower right')\n\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"95b78a9e578b99d048bd52a8358180c02d128de7"},"cell_type":"code","source":"#Lets run a quick few steps to figure out what would have happened if we had not applied the SMOTE. \n#Running the train_test_Split again\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=0)\n\nlr1 = LogisticRegression()\nlr1.fit(X_train,y_train)\ny_pred_1 = lr1.predict(X_test)\ncnf_matrix = confusion_matrix(y_test, y_pred_1)\nprint(cnf_matrix)\n#The number of True Positives reduces from 129 to 77, The number of false negatives increased to 70 from 18\n#The accuracy 85285+77 = 85362 is higher than the accuracy derived after SMOTE. So clearly accuracy does not help judge the \n#goodness of the algorithm in certain imbalanced cases as there","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"fcdd5bea0822021b1c7df9f7bbf8f76f1a9bd9d8"},"cell_type":"code","source":"print('Recall metric in the test dataset: ',recall_score(y_test,y_pred_1))\n#Recall reduces drastically. Reason is obvious","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}