Submissions are evaluated using Global Average Precision (GAP) at k k , where
k=1 k . This metric is also known as micro Average Precision (microAP), as per
[1]. It works as follows: For each query image, you will predict one landmark
label and a corresponding confidence score. The evaluation treats each
prediction as an individual data point in a long list of predictions (sorted
in descending order by confidence scores), and computes the Average Precision
based on this list. If a submission has N N predictions (label/confidence
pairs) sorted in descending order by their confidence scores, then the Global
Average Precision is computed as: GAP= 1 M ∑ i=1 N P(i)rel(i) where: N N is
the total number of predictions returned by the system, across all queries M M
is the total number of queries with at least one landmark from the training
set visible in it (note that some queries may not depict landmarks) P(i) P is
the precision at rank i i rel(i) r denotes the relevance of prediciton i i :
it’s 1 if the i i -th prediction is correct, and 0 otherwise [1] F. Perronnin,
Y. Liu, and J.-M. Renders, "A Family of Contextual Measures of Similarity
between Distributions with Application to Image Retrieval," Proc. CVPR'09
Submission File For each id in the test set, you can predict at most one
landmark and its corresponding confidence score. Some query images may contain
no landmarks. You may decide not to predict any result for a given query, by
submitting an empty prediction. The submission file should contain a header
and have the following format (larger scores denote more confident matches):
id,landmarks 000088da12d664db,8815 0.03 0001623c6d808702,
0001bbb682d45002,5328 0.5 etc.

