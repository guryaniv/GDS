{"metadata": {"kernelspec": {"display_name": "Python 3", "name": "python3", "language": "python"}, "language_info": {"name": "python", "version": "3.6.0", "codemirror_mode": {"name": "ipython", "version": 3}, "mimetype": "text/x-python", "file_extension": ".py", "nbconvert_exporter": "python", "pygments_lexer": "ipython3"}}, "cells": [{"cell_type": "markdown", "execution_count": null, "execution_state": "idle", "source": "# Python for Padawans\n\nThis tutorial will go throughthe basic data wrangling workflow I'm sure you all love to hate, in Python! \nFYI: I come from a R background (aka I'm not a proper programmer) so if you see any formatting issues please cut me a bit of slack. \n\n**The aim for this post is to show people how to easily move their R workflows to Python (especially pandas/scikit)**\n\nOne thing I especially like is how consistent all the functions are. You don't need to switch up style like you have to when you move from base R to dplyr etc. \n|\nAnd also, it's apparently much easier to push code to production using Python than R. So there's that. \n\n### 1. Reading in libraries", "metadata": {"_cell_guid": "733275c0-cdb5-4f49-99d0-fbd08304801b", "_active": false, "_uuid": "ba8834fac267a43c78c32a93933cf1cd8e4c3fe9"}, "outputs": []}, {"cell_type": "code", "execution_count": null, "execution_state": "idle", "source": "%matplotlib inline\nimport os\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport math", "metadata": {"_cell_guid": "469c6bc3-7f72-40c2-831f-9d68e05e70d1", "_active": false, "collapsed": false, "_uuid": "f22214c47381751f8c8b1971451675554f897ed5"}, "outputs": []}, {"cell_type": "markdown", "execution_count": null, "execution_state": "idle", "source": "#### Don't forget that %matplotlib function. Otherwise your graphs will pop up in separate windows and stop the execution of further cells. And nobody got time for that.\n\n### 2. Reading in data", "metadata": {"_cell_guid": "c20b08e4-2469-4471-9662-15ccb8910d61", "_active": false, "_uuid": "14ced02cff4e2a91f9cd8dbe9a61b0c88c1fd871"}, "outputs": []}, {"cell_type": "code", "execution_count": null, "execution_state": "idle", "source": "data = pd.read_csv('../input/loan.csv', low_memory=False)\ndata.drop(['id', 'member_id', 'emp_title'], axis=1, inplace=True)\n\ndata.replace('n/a', np.nan,inplace=True)\ndata.emp_length.fillna(value=0,inplace=True)\n\ndata['emp_length'].replace(to_replace='[^0-9]+', value='', inplace=True, regex=True)\ndata['emp_length'] = data['emp_length'].astype(int)\n\ndata['term'] = data['term'].apply(lambda x: x.lstrip())", "metadata": {"_cell_guid": "028a5a22-15ff-4c43-9465-77b9a2681ad6", "_active": false, "collapsed": false, "_uuid": "96750b98f3cd2bf3daf4fdb5b66ab5b709368103"}, "outputs": []}, {"cell_type": "markdown", "execution_count": null, "execution_state": "idle", "source": "### 3. Basic plotting using Seaborn\n\nNow let's make some pretty graphs. Coming from R I definitely prefer ggplot2 but the more I use Seaborn, the more I like it. If you kinda forget about adding \"+\" to your graphs and instead use the dot operator, it does essentially the same stuff.\n\n**And I've just found out that you can create your own style sheets to make life easier. Wahoo!**\n\nBut anyway, below I'll show you how to format a decent looking Seaborn graph, as well as how to summarise a given dataframe.", "metadata": {"_cell_guid": "e41ada7d-98e9-4ca5-ba58-2af8dd93092c", "_active": false, "_uuid": "ea673c964d42ec9f8b2a789344fa9e8cf9f6a88c"}, "outputs": []}, {"cell_type": "code", "execution_count": null, "execution_state": "idle", "source": "import seaborn as sns\nimport matplotlib\n\ns = pd.value_counts(data['emp_length']).to_frame().reset_index()\ns.columns = ['type', 'count']\n\ndef emp_dur_graph(graph_title):\n\n    sns.set_style(\"whitegrid\")\n    ax = sns.barplot(y = \"count\", x = 'type', data=s)\n    ax.set(xlabel = '', ylabel = '', title = graph_title)\n    ax.get_yaxis().set_major_formatter(\n    matplotlib.ticker.FuncFormatter(lambda x, p: format(int(x), ',')))\n    _ = ax.set_xticklabels(ax.get_xticklabels(), rotation=0)\n    \nemp_dur_graph('Distribution of employment length for issued loans')", "metadata": {"_cell_guid": "02eaf09c-87e5-4bc1-a129-73eee78d4bc2", "_active": false, "collapsed": false, "_uuid": "9f0e40a75b3a19839c10affd3aac856567eb8ac5"}, "outputs": []}, {"cell_type": "markdown", "execution_count": null, "execution_state": "idle", "source": "### 4. Using Seaborn stylesheets\n\nNow before we move on, we'll look at using style sheets to customize our graphs nice and quickly.", "metadata": {"_cell_guid": "e41c20f2-5010-4c05-8325-77011f6c2857", "_active": false, "_uuid": "2916c22d199b7addcb951616f28dae5f23b0a8a8"}, "outputs": []}, {"cell_type": "code", "execution_count": null, "execution_state": "idle", "source": "import seaborn as sns\nimport matplotlib\n\nprint (plt.style.available)", "metadata": {"_cell_guid": "37da2d4e-e797-44ce-8885-118ab70f73fe", "_active": false, "collapsed": false, "_uuid": "9239b0ee3345732e7c2459e4bb5b94fbeb6b20a9"}, "outputs": []}, {"cell_type": "markdown", "execution_count": null, "execution_state": "idle", "source": "Now you can see that we've got quite a few to play with. I'm going to focus on the following styles:\n\n- fivethirtyeight (because it's my fav website)\n- seaborn-notebook\n- ggplot\n- classic", "metadata": {"_cell_guid": "dd14338d-f1a8-472d-880d-8645632769ab", "_active": false, "_uuid": "e21427c3ba274128c1a586e37ac7153951abf382"}, "outputs": []}, {"cell_type": "code", "execution_count": null, "execution_state": "idle", "source": "import seaborn as sns\nimport matplotlib\n\nplt.style.use('fivethirtyeight')\nax = emp_dur_graph('Fivethirty eight style')", "metadata": {"_cell_guid": "e2abe370-0f39-4d5e-a5ba-5e5c82c0cfb7", "_active": false, "collapsed": false, "_uuid": "fc4f25eabd0e6ac5ce925a3d6316dd4e34550abb"}, "outputs": []}, {"cell_type": "code", "execution_count": null, "execution_state": "idle", "source": "plt.style.use('seaborn-notebook')\nax = emp_dur_graph('Seaborn-notebook style')", "metadata": {"_cell_guid": "4178fd2a-7d72-4348-9dbf-f5437f33cd87", "_active": false, "collapsed": false, "_uuid": "ee75cc81f7951451effe86d7ed454649b8a0cffe"}, "outputs": []}, {"cell_type": "code", "execution_count": null, "execution_state": "idle", "source": "plt.style.use('ggplot')\nax = emp_dur_graph('ggplot style')", "metadata": {"_cell_guid": "b82a22b6-e1e0-4cb0-81bb-5f7a18df0580", "_active": false, "collapsed": false, "_uuid": "62d3c6ca1baa932fe544e14c22079daec16789e8"}, "outputs": []}, {"cell_type": "code", "execution_count": null, "execution_state": "idle", "source": "plt.style.use('classic')\nax = emp_dur_graph('classic style')", "metadata": {"_cell_guid": "859ed359-72c1-4f46-a620-f33e358bbe26", "_active": false, "collapsed": false, "_uuid": "37cd0035395ec8c8545e7d7e25e59885e4cbbfa6"}, "outputs": []}, {"cell_type": "markdown", "execution_count": null, "execution_state": "idle", "source": "### 5. Working with dates\n\nNow we want to looking at datetimes. Dates can be quite difficult to manipulate but it's worth the wait. Once they're formatted correctly life becomes much easier", "metadata": {"_cell_guid": "70180477-346b-44a6-85c6-275f0a2be3c6", "_active": false, "_uuid": "ccbf4e17e9c3799062a9241ada4095736958959b"}, "outputs": []}, {"cell_type": "code", "execution_count": null, "execution_state": "idle", "source": "import datetime\n\ndata.issue_d.fillna(value=np.nan,inplace=True)\nissue_d_todate = pd.to_datetime(data.issue_d)\ndata.issue_d = pd.Series(data.issue_d).str.replace('-2015', '')\ndata.emp_length.fillna(value=np.nan,inplace=True)\n\ndata.drop(['loan_status'],1, inplace=True)\n\ndata.drop(['pymnt_plan','url','desc','title' ],1, inplace=True)\n\ndata.earliest_cr_line = pd.to_datetime(data.earliest_cr_line)\nimport datetime as dt\ndata['earliest_cr_line_year'] = data['earliest_cr_line'].dt.year", "metadata": {"_cell_guid": "4716b44f-5f69-4789-89da-cb494b23cda1", "_active": false, "collapsed": false, "_uuid": "45ce4c323793767ac5de603f99abb6228aae429d"}, "outputs": []}, {"cell_type": "markdown", "execution_count": null, "execution_state": "idle", "source": "### 6. Making faceted graphs using Seaborn\n\nNow I'll show you how you can build on the above data frame summaries as well as make some facet graphs.", "metadata": {"_cell_guid": "2ca46f70-76fc-4d50-be9a-bd2a9f4194ca", "_active": false, "_uuid": "763b85b10306577f3b3b17735d39d3dac8727083"}, "outputs": []}, {"cell_type": "code", "execution_count": null, "execution_state": "idle", "source": "import seaborn as sns\nimport matplotlib.pyplot as plt\n\ns = pd.value_counts(data['earliest_cr_line']).to_frame().reset_index()\ns.columns = ['date', 'count']\n\ns['year'] = s['date'].dt.year\ns['month'] = s['date'].dt.month\n\nd = s[s['year'] > 2008]\n\nplt.rcParams.update(plt.rcParamsDefault)\nsns.set_style(\"whitegrid\")\n\ng = sns.FacetGrid(d, col=\"year\")\ng = g.map(sns.pointplot, \"month\", \"count\")\ng.set(xlabel = 'Month', ylabel = '')\naxes = plt.gca()\n_ = axes.set_ylim([0, d.year.max()])\nplt.tight_layout()", "metadata": {"_cell_guid": "b4e02d70-e587-463f-bbf2-f0dc6b93ecff", "_active": false, "collapsed": false, "_uuid": "4eff7ff79b481eb27fdb5ed92bffa11aeae92e97"}, "outputs": []}, {"cell_type": "markdown", "execution_count": null, "execution_state": "idle", "source": "Now I want to show you how to easily drop columns that match a given pattern. Let's drop any column that includes \"mths\" in it.", "metadata": {"_cell_guid": "e7bcefb5-4962-4b5d-af0e-3640c91f4b24", "_active": false, "_uuid": "48c717d4a17053d8c880fcab7adf97f33dd3a995"}, "outputs": []}, {"cell_type": "code", "execution_count": null, "execution_state": "idle", "source": "mths = [s for s in data.columns.values if \"mths\" in s]\nmths\n\ndata.drop(mths, axis=1, inplace=True)", "metadata": {"_cell_guid": "b223f733-0a21-4bd6-920e-0f8a737b4961", "_active": false, "collapsed": false, "_uuid": "2185bc82c9e0b9adcfacbb52dde3abc07453a294"}, "outputs": []}, {"cell_type": "markdown", "execution_count": null, "execution_state": "idle", "source": "### 7. Using groupby to create summary graphs", "metadata": {"_cell_guid": "04930fd5-f119-41ef-834a-df7712d9980c", "_active": false, "_uuid": "15dab20069799c2c4d5101c40c005fbe9c12ecee"}, "outputs": []}, {"cell_type": "code", "execution_count": null, "execution_state": "idle", "source": "group = data.groupby('grade').agg([np.mean])\nloan_amt_mean = group['loan_amnt'].reset_index()\n\nimport seaborn as sns\nimport matplotlib\n\nplt.style.use('fivethirtyeight')\n\nsns.set_style(\"whitegrid\")\nax = sns.barplot(y = \"mean\", x = 'grade', data=loan_amt_mean)\nax.set(xlabel = '', ylabel = '', title = 'Average amount loaned, by loan grade')\nax.get_yaxis().set_major_formatter(\nmatplotlib.ticker.FuncFormatter(lambda x, p: format(int(x), ',')))\n_ = ax.set_xticklabels(ax.get_xticklabels(), rotation=0)", "metadata": {"_cell_guid": "6b853955-3011-4a46-a2fd-cd8fc240c1b7", "_active": false, "collapsed": false, "_uuid": "a89eabe70e5774a823207d377520727169cdcfa9"}, "outputs": []}, {"cell_type": "markdown", "execution_count": null, "execution_state": "idle", "source": "### 8. More advanced groupby statements visualised with faceted graphs", "metadata": {"_cell_guid": "04170649-a3df-4b6b-803c-f1fe56e77c8e", "_active": false, "_uuid": "e5df26dafdf008f88558bc333295cf56a88877fd"}, "outputs": []}, {"cell_type": "code", "execution_count": null, "execution_state": "idle", "source": "filtered  = data[data['earliest_cr_line_year'] > 2008]\ngroup = filtered.groupby(['grade', 'earliest_cr_line_year']).agg([np.mean])\n\ngraph_df = group['int_rate'].reset_index()\n\nimport seaborn as sns\nimport matplotlib\n\nplt.style.use('fivethirtyeight')\nplt.suptitle('bold figure suptitle', fontsize=14, fontweight='bold')\n\nsns.set_style(\"whitegrid\")\ng = sns.FacetGrid(graph_df, col=\"grade\", col_wrap = 2)\ng = g.map(sns.pointplot, \"earliest_cr_line_year\", \"mean\")\ng.set(xlabel = 'Year', ylabel = '')\naxes = plt.gca()\naxes.set_ylim([0, graph_df['mean'].max()])\n_ = plt.tight_layout()", "metadata": {"_cell_guid": "e589cac5-c176-45c3-8f20-69f6a4e53cbe", "_active": false, "collapsed": false, "_uuid": "764fcfb32ff72d82a160efaa8a21964479a66015"}, "outputs": []}, {"cell_type": "markdown", "execution_count": null, "execution_state": "idle", "source": "### 9. Treatment of missing values\nThis section is a toughie because there really is no correct answer. A pure data science/mining approach would test each of the approaches here using a CV split and include the most accurate treatment in their modelling pipeline.\nHere I have included the code for the following treatments:\n\n- Mean imputation\n- Median imputation\n- Algorithmic imputation\n\nI spent a large amount of time looking at 3. because I couldn't find anyone else who has implemented it, so I built it myself. In R it's very easy to use supervised learning techniques to impute missing values for a given variable (as shown here: https://www.kaggle.com/mrisdal/shelter-animal-outcomes/quick-dirty-randomforest) but sadly I couldn't find it done in Python.", "metadata": {"_cell_guid": "b4e9e5ce-bed5-4d5a-b61e-9b794f399847", "_active": false, "_uuid": "80826ae8f3a1845f335772d2c2f1805fe3cb8c15"}, "outputs": []}, {"cell_type": "code", "execution_count": null, "execution_state": "idle", "source": "#data['emp_length'].fillna(data['emp_length'].mean())\n#data['emp_length'].fillna(data['emp_length'].median())\n#data['emp_length'].fillna(data['earliest_cr_line_year'].median())\n\nfrom sklearn.ensemble import RandomForestClassifier\nrf =  RandomForestClassifier(max_depth=5, n_estimators=100, max_features=1)\n\ndata['emp_length'].replace(to_replace=0, value=np.nan, inplace=True, regex=True)\n\ncat_variables = ['term', 'purpose', 'grade']\ncolumns = ['loan_amnt', 'funded_amnt', 'funded_amnt_inv', 'int_rate', 'grade', 'purpose', 'term']\n\ndef impute_missing_algo(df, target, cat_vars, cols, algo):\n\n    y = pd.DataFrame(df[target])\n    X = df[cols].copy()\n    X.drop(cat_vars, axis=1, inplace=True)\n\n    cat_vars = pd.get_dummies(df[cat_vars])\n\n    X = pd.concat([X, cat_vars], axis = 1)\n\n    y['null'] = y[target].isnull()\n    y['null'] = y.loc[:, target].isnull()\n    X['null'] = y[target].isnull()\n\n    y_missing = y[y['null'] == True]\n    y_notmissing = y[y['null'] == False]\n    X_missing = X[X['null'] == True]\n    X_notmissing = X[X['null'] == False]\n\n    y_missing.loc[:, target] = ''\n\n    dfs = [y_missing, y_notmissing, X_missing, X_notmissing]\n    \n    for df in dfs:\n        df.drop('null', inplace = True, axis = 1)\n\n    y_missing = y_missing.values.ravel(order='C')\n    y_notmissing = y_notmissing.values.ravel(order='C')\n    X_missing = X_missing.as_matrix()\n    X_notmissing = X_notmissing.as_matrix()\n    \n    algo.fit(X_notmissing, y_notmissing)\n    y_missing = algo.predict(X_missing)\n\n    y.loc[(y['null'] == True), target] = y_missing\n    y.loc[(y['null'] == False), target] = y_notmissing\n    \n    return(y[target])\n\ndata['emp_length'] = impute_missing_algo(data, 'emp_length', cat_variables, columns, rf)\ndata['earliest_cr_line_year'] = impute_missing_algo(data, 'earliest_cr_line_year', cat_variables, columns, rf)", "metadata": {"_cell_guid": "a8946a3f-1049-44a0-9f4e-97c23e56cd7a", "_active": false, "collapsed": false, "_uuid": "8b59ce2d39870c25dd00e08f0381be4a3bb8c9c3"}, "outputs": []}, {"cell_type": "markdown", "execution_count": null, "execution_state": "idle", "source": "### 10. Running a simple classification model\nHere I take my cleaned variables (missing values have been imputed using random forests) and run a simple sklearn algo to classify the term of the loan.\nThis step in the analytics pipeline does take longer in Python than in R (as R handles factor variables out of the box while sklearn only accepts numeric features) but it isn't that hard.\nThis is just indicative though! A number of the variables are likely to introduce leakage to the prediction problem as they'll influence the term of the loan either directly or indirectly.", "metadata": {"_cell_guid": "8ae7b731-e59f-4892-91e3-deffbb8c27ed", "_active": false, "_uuid": "037793f1fbb64f861905810636533794d98f9b43"}, "outputs": []}, {"cell_type": "code", "execution_count": null, "execution_state": "idle", "source": "y = data.term\n\ncols = ['loan_amnt', 'funded_amnt', 'funded_amnt_inv', 'int_rate', 'grade', 'emp_length', 'purpose', 'earliest_cr_line_year']\nX = pd.get_dummies(data[cols])\n\nfrom sklearn import preprocessing\n\ny = y.apply(lambda x: x.lstrip())\n\nle = preprocessing.LabelEncoder()\nle.fit(y)\n\ny = le.transform(y)\nX = X.as_matrix()\n\nfrom sklearn import linear_model\n\nlogistic = linear_model.LogisticRegression()\n\nlogistic.fit(X, y)", "metadata": {"_cell_guid": "9a1502ca-6180-49ae-82b9-25fb3fdd8731", "_active": false, "collapsed": false, "_uuid": "c0573bfdd42fd00e5cdd75d5389b60b73e6c8530"}, "outputs": []}, {"cell_type": "markdown", "execution_count": null, "execution_state": "idle", "source": "### 11. Pipelining in sklearn\n\nIn this section I'll go through how you can combine multiple techniques (supervised an unsupervised) in a pipeline.\nThese can be useful for a number of reasons:\n\n- You can score the output of the whole pipeline\n- You can gridsearch for the whole pipeline making finding optimal parameters easier\n\nSo next we'll combine some a PCA (unsupervised) and Random Forests (supervised) to create a pipeline for modelling the data. \n\nIn addition to this I'll show you an easy way to grid search for the optimal hyper parameters.", "metadata": {"_cell_guid": "b3e28cde-0ba8-4c52-9b7e-8786fca57de6", "_active": false, "_uuid": "075a84faa24b39711a31b52fa639b044f3b72fa9"}, "outputs": []}, {"cell_type": "code", "execution_count": null, "execution_state": "idle", "source": "from sklearn import linear_model, decomposition\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.grid_search import GridSearchCV\n\nrf = RandomForestClassifier(max_depth=5, max_features=1)\n\npca = decomposition.PCA()\npipe = Pipeline(steps=[('pca', pca), ('rf', rf)])\n\nn_comp = [3, 5]\nn_est = [10, 20]\n\nestimator = GridSearchCV(pipe,\n                         dict(pca__n_components=n_comp,\n                              rf__n_estimators=n_est))\n\nestimator.fit(X, y)", "metadata": {"_cell_guid": "30d64b18-0568-4b84-b922-6d38bffa6fc0", "_active": false, "collapsed": false, "_uuid": "a74a02c10e3530cefbbcaff907d95f2d8405e7bb"}, "outputs": []}], "nbformat": 4, "nbformat_minor": 0}