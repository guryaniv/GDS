{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "42730bc6-ab49-0b88-b57b-1d35c4e0e072"
      },
      "outputs": [],
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load in \n",
        "# from autocorrect import spell\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem.porter import *\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "# Input data files are available in the \"../input/\" directory.\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
        "\n",
        "from subprocess import check_output\n",
        "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n",
        "\n",
        "# Any results you write to the current directory are saved as output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d8df10bf-6520-68f4-2989-b2116d8cbb4c"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"../input/Combined_News_DJIA.csv\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d76af499-9593-acd5-823e-30383ba92797"
      },
      "outputs": [],
      "source": [
        "def tokenize_text(text):\n",
        "    stop = set(stopwords.words('english'))\n",
        "    text = text.lower()\n",
        "    # tokenizer = get_tokenizer(\"en_US\")\n",
        "    tokenizer = RegexpTokenizer(r'[a-z]+')  # strips non-alphabetical characters\n",
        "    # cleaned_tokens = text_col.apply(lambda row: spell(tokenizer.tokenize(row)).lower)\n",
        "    cleaned_tokens = [spell(word) for word in tokenizer.tokenize(text) if word not in stop]\n",
        "    # tagged_tokens = nltk.pos_tag(cleaned_tokens)\n",
        "    return cleaned_tokens\n",
        "\n",
        "def clean_and_tag(text, tokenizer):\n",
        "    stop = set(stopwords.words('english'))\n",
        "    text = str(text).lower()  # apply(lambda x: x.lower())\n",
        "    # tokenizer = get_tokenizer(\"en_US\")\n",
        "    cleaned_tokens = [word for word in tokenizer.tokenize(text) if word not in stop and len(word) > 1]\n",
        "    tagged_tokens = nltk.pos_tag(cleaned_tokens)\n",
        "    return tagged_tokens\n",
        "\n",
        "def lemmatize_tokens(tokens_col, lemma):\n",
        "    tokens_col = tokens_col.apply(lambda row: [get_wordnet_pos(word) for word in row if get_wordnet_pos(word)[1] != ''])\n",
        "    lemmatized_words = tokens_col.apply(lambda row: [lemma.lemmatize(word[0],word[1]) for word in row])  # [lemma.lemmatize(x) for x in noun_tokens]\n",
        "    return lemmatized_words\n",
        "\n",
        "# Convert to wordnet part of speech from UPenn\n",
        "def get_wordnet_pos(word_tag_tuple):\n",
        "    tag = word_tag_tuple[1]\n",
        "    if tag.startswith('J'):  # adjectives\n",
        "        tag = 'a'\n",
        "    elif tag.startswith('V'):  # verbs\n",
        "        tag = 'v'\n",
        "    elif tag.startswith('N'):  # nouns\n",
        "        tag = 'n'\n",
        "    elif tag.startswith('R'):  # adverbs\n",
        "        tag = 'r'\n",
        "    else:                      # default tag in nltk lemmatizer is noun\n",
        "        tag = ''\n",
        "    return (word_tag_tuple[0], tag)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "acfdef0f-98a0-d4e2-95ba-93d7903581ff"
      },
      "outputs": [],
      "source": [
        "tokenizer = RegexpTokenizer(r'[a-z]+')  # strips non-alphabetical characters\n",
        "lemma = WordNetLemmatizer()\n",
        "\n",
        "end = 3\n",
        "headline_cols = ['0']\n",
        "for x in range(1, end):\n",
        "    headline = df['Top%s' %x] \n",
        "    tokenized_headline = headline.apply(lambda row: clean_and_tag(row, tokenizer))\n",
        "    print(tokenized_headline)\n",
        "    lemmatized_headline = lemmatize_tokens(tokenized_headline, lemma)\n",
        "    print(lemmatized_headline)\n",
        "    # temp = temp.concat(headline_col, axis=1)\n",
        "    headline_cols.append(lemmatized_headline)\n",
        "temp = pd.concat(headline_cols[1:end-1])\n",
        "temp.head()\n",
        "temp.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "f161f535-8e89-d133-4a7f-75a9cb4b8437"
      },
      "outputs": [],
      "source": [
        "\n",
        "for x in range(1, end):"
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}