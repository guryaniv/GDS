{"cells":[{"metadata":{"_uuid":"b87931c8223ce06b4921b61b73da4a9a3f8bf2fd"},"cell_type":"markdown","source":"![eee](http://googletoday.net/wp-content/uploads/2015/10/GoogleMerchandiseStore-Haul-1024x488.jpg)\n\nData science has a lot of applications in e-commerce and marketing. This competition introduces yet another challenge in these domains that can be tackled using Machine Learning techniques.\n\n**The goal** is to predict the natural log of the sum of all transactions per user. In layman's terms, we want to design an algorithm that will identify clients who spend a lot of money on Google Merchandise Store and those who don't.\n\n**This Kernel is dedicated to Exploratory Data Analysis**. I will try to gain as many insights as possible. I will do another kernel to benchmark different models on this dataset, from the most interpretable to the most complex one.\n\nHere is what I will do :\n1. Check missing values and data processing\n2. Compute statistics on the target variable / Hypothesis Testing\n3. Exploratory Data Analysis\n4. Features Selection\n\nEnjoy and feel free to give any constructive critics !"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"\nimport numpy as np \nimport pandas as pd \nfrom pandas.io.json import json_normalize\nimport json\nimport os\n\n#Libraries for plots\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom plotly.offline import init_notebook_mode, iplot\nimport plotly.graph_objs as go\nfrom plotly import tools\ninit_notebook_mode(connected = True)\nfrom IPython.display import HTML\nplt.style.use('fivethirtyeight')\nsns.set_context(rc = {\"lines.linewidth\": 2})\n\nimport random\nimport datetime as dt\n\n# For feature Selection\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.metrics import accuracy_score\n\n\n\n# Stats from Scipy for Hypothesis testing\nfrom scipy.stats import norm\nfrom scipy.stats import kurtosis, skew\nfrom scipy.stats import shapiro\nfrom scipy.stats import normaltest\n\n\n\ndef load_df(csv_path = '../input/train.csv', nrows=None):\n    JSON_COLUMNS = ['device', 'geoNetwork', 'totals', 'trafficSource']\n    \n    df = pd.read_csv(csv_path, \n                     converters = {column: json.loads for column in JSON_COLUMNS}, \n                     dtype = {'fullVisitorId': 'str'}, \n                     nrows = nrows)\n    \n    for column in JSON_COLUMNS:\n        column_as_df = json_normalize(df[column])\n        column_as_df.columns = [f\"{column}.{subcolumn}\" for subcolumn in column_as_df.columns]\n        df = df.drop(column, axis=1).merge(column_as_df, right_index=True, left_index=True)\n    print(f\"Loaded {os.path.basename(csv_path)}. Shape: {df.shape}\")\n    return df\n\nprint(os.listdir(\"../input\"))\n\n# For random color in my Plotly plots.\ndef randomc():\n    r = random.randint(1,255)\n    g = random.randint(1,255)\n    b = random.randint(1,255)\n    return('rgb({},{},{})'.format(r,g,b))\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"12997a7062e02d544ac3ae0b916c49d7d8fb8031"},"cell_type":"markdown","source":"I only load the training set to avoid memory issues. If you fork this kernel, the data processing steps on both sets."},{"metadata":{"trusted":true,"_uuid":"e98a8d55e189aac4a4d2e7a58c8676cbbeea41d7","scrolled":false},"cell_type":"code","source":"%%time\ntrain_df = load_df()\n#test_df = load_df(\"../input/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"09b1b6988b7b38b2d5e9f68c2e4345b223b09fbb"},"cell_type":"markdown","source":"**A quick summary of the training set.**"},{"metadata":{"trusted":true,"_uuid":"4d202a7d801c8ff3785823ea13dd158b38600019"},"cell_type":"code","source":"train_df.info()\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"da9cbd1012621c234116da5ff522d36d5b176514"},"cell_type":"markdown","source":"**Data Pre-Processing**"},{"metadata":{"trusted":true,"_uuid":"18dec0a12d956a41fa6fc1763f67e96f72e1424a"},"cell_type":"code","source":"# to numeric values\ntrain_df['totals.transactionRevenue'] = pd.to_numeric(train_df['totals.transactionRevenue'])\ntrain_df['totals.transactionRevenue'] = train_df['totals.transactionRevenue'].fillna(0)\ntrain_df['totals.hits'] = pd.to_numeric(train_df['totals.hits']).fillna(0)\ntrain_df['totals.pageviews'] = pd.to_numeric(train_df['totals.pageviews']).fillna(0)\n\n#dates from int to Timestamp\ntrain_df['date'] = pd.to_datetime(train_df.date, format='%Y%m%d')\n\n# drop useless colums that have only 1 value\ntrain_const_cols = [ col for col in train_df.columns if len(train_df[col].unique()) == 1]\ntrain_df.drop(train_const_cols, axis = 1, inplace = True)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"746ca23f9b63a7aacc3f3191e145799b24acc6a9"},"cell_type":"markdown","source":"Compute the ratio of Missing values"},{"metadata":{"trusted":true,"_uuid":"c56fed9312291a9101f4eee37adf7659bfece9ae"},"cell_type":"code","source":"misvalue_dic = {}\nfor column in train_df :\n    v = 100 * train_df[column].isna().sum() / len(train_df)\n    column\n    if v > 0 :\n        misvalue_dic[column]=v\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"38563fd26c1b6660edf7f5e22a9b0f669a89cf1b"},"cell_type":"code","source":"misvalue_dic","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"73789a37867afa9a8a4df3456cf7eb32d1aa2ecd"},"cell_type":"markdown","source":"First plot : \n**Ratio of missing values**"},{"metadata":{"trusted":true,"_uuid":"000308064e04ad0d02f4cfb4aa2559443706eddb"},"cell_type":"code","source":"trace1 = go.Bar(\n                x = list(misvalue_dic.keys()),\n                y = list(misvalue_dic.values()),\n                name = \"Missing Values\",\n                marker = dict(color=randomc()))\ndata = [trace1]\nlayout = go.Layout(\n    xaxis = dict(tickangle = -25),\n    title='Percentage of missing value for uncomplete columns',\n)\nfig = go.Figure(data = data, layout = layout)\niplot(fig)\n\n    ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"793e303aae4481b7f55f5161c08dabb3128468a4"},"cell_type":"markdown","source":"Some columns have almost 100% of missing values ! \n<br/> Might be tempting to drop them thinking they are useless. We don't know anything yet about this dataset, so it would be risky."},{"metadata":{"trusted":true,"_uuid":"e5a0d1ae084a4de6dcc16604a87d28d6c8ad69f9"},"cell_type":"code","source":"#Some insight on the target variable : Total Transaction Revenue\ngdf = train_df.groupby(\"fullVisitorId\")[\"totals.transactionRevenue\"].sum().reset_index()\n\n#take non zero total transaction revenues\nnz_df = gdf.loc[gdf[\"totals.transactionRevenue\"] > 0]\nlog_nz = np.log1p(nz_df[\"totals.transactionRevenue\"])\nnz = nz_df[\"totals.transactionRevenue\"]\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b12c4a7a3efea3b94c3452bec6139d795a6dd91f"},"cell_type":"code","source":"print(\"Among all the visitors, only {0:.3f} percent have bought something from August 2016 to August 2017\".format( round(100*len(nz)/len(gdf),4)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"72c998e93909d72e72f004bd9442115af41cc302"},"cell_type":"markdown","source":"**Distribution of the Target Variable\n**\n\nLet's plot the distribution of the Target Variable, non null values only<br/>\nWithout taking the log of totals, here is what it looks like."},{"metadata":{"trusted":true,"_uuid":"287036b214ed82f68c67c865beac04e43ecca64e"},"cell_type":"code","source":"plt.subplots(figsize = (14, 7))\nax = sns.distplot(a = nz).set_title('Distribution of non null Totals Transaction Revenues')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"23ff6d377af0b9fce42d7b78b254297398cd577d"},"cell_type":"markdown","source":"Taking the log here is what we get. Much nicer !"},{"metadata":{"trusted":true,"_uuid":"703aebdbbbef90eadb0ea130d2e01f4d49b548d7"},"cell_type":"code","source":"plt.subplots(figsize = (16, 7))\nax = sns.distplot(a = log_nz,axlabel = \"ln(1+ totals.transactionRevenue)\" ).set_title('Distribution of Ln(1+non null Totals Transaction Revenues)')\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bf2245fc50f314f16e7b3c6c4d664ae5e0471eaa"},"cell_type":"markdown","source":" So the distribution has a nice bell shape but the tails are too light to be a normal distribution. We can verify this assumption with **normality tests**.  <br><br>\nHere is a quick description of the distribution of log values. Let's compute the [skewness](https://en.wikipedia.org/wiki/Skewness) and [kurtosis](https://fr.wikipedia.org/wiki/Kurtosis) before the normality test"},{"metadata":{"trusted":true,"_uuid":"121f9b0f0c9eafd61674f4ad870a2ca319c48607"},"cell_type":"code","source":"log_nz.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"86d8cfadc0907279cac37efbde083eb181fda8ec"},"cell_type":"code","source":"print( \"The skewness of the distribution is {}.\".format(log_nz.skew()))\nprint( \"The kurtosis of the distribution is {}.\".format(log_nz.kurt()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b567857cb996894e948e8e81e870eb8a047eda67","collapsed":true},"cell_type":"markdown","source":"|skewness|< 0.5 ==> The distribution is symmetric around the mean <br>\nNo conclusion for the kurtosis though as it is between -2 and 2.\n<br> <br>\n\n**Normality Test:**\nWe will perform [Shapiro-Wilk test](https://en.wikipedia.org/wiki/Shapiro%E2%80%93Wilk_test), based on expected values, and [d'Agostino test](https://en.wikipedia.org/wiki/D%27Agostino%27s_K-squared_test), based on kurtosis and skewness. Noteworthy : the Shapiro test is said to be less acurate when n is large (>1000) .\n"},{"metadata":{"trusted":true,"_uuid":"3686a67a92cf2f84ce172695c1e741d0370a7395"},"cell_type":"code","source":"# Shapiro Normality Test\nstat, p = shapiro(log_nz)\nprint('Statistics=%.3f, p=%.3f' % (stat, p))\n\nalpha = 0.05\nif p > alpha:\n    print('Sample looks Gaussian (fail to reject H0)')\nelse:\n    print('Sample does not look Gaussian (reject H0)')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2cae20a3d66313764841db2babe23ae14edcc801"},"cell_type":"code","source":"#D'Agostino Test\nstat, p = normaltest(log_nz)\nprint('Statistics=%.3f, p=%.3f' % (stat, p))\n\nalpha = 0.05\nif p > alpha:\n    print('Sample looks Gaussian (fail to reject H0)')\nelse:\n    print('Sample does not look Gaussian (reject H0)')\n    \n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8180ee17c6fd762187b92cf3cb5ead01aa826106"},"cell_type":"markdown","source":"Conclusion : WIth such a low p-value, we reject the hypothesis of normality <br>\nLink to remind [Misunderstanding of p-values](https://en.wikipedia.org/wiki/Misunderstandings_of_p-values)"},{"metadata":{"_uuid":"6f21bb0e49cd035a01c5e0c4a5f25b498e642414"},"cell_type":"markdown","source":"![](https://juicebubble.co.za/wp-content/uploads/2018/03/normal-paranormal-distribution-white-400x400.png)"},{"metadata":{"_uuid":"c444b82d42373d5bc2844f998a88618c01f4591b"},"cell_type":"markdown","source":"Are there specific periods where people are show more interes for Google products ? For example when a new version of Android is released ? Let's see for ourself <br> <br>\n\n**Number of visits per day : **"},{"metadata":{"trusted":true,"_uuid":"248d0e12f21e31cb829630d4585b7b749f79dcd8"},"cell_type":"code","source":"nz2 = train_df.loc[train_df['totals.transactionRevenue'] > 0]\nz2 = train_df.loc[train_df['totals.transactionRevenue'] == 0]\nfig, ax1 = plt.subplots(figsize = (18, 10))\nplt.title(\"Revenue and Non Revenue visits\");\nz2.groupby(['date'])['totals.transactionRevenue'].count().plot()\nax1.set_ylabel('Visits count')\nplt.legend(['Non-Revenue and Revenue users'], loc =(0.70,0.9) )\nax2 = ax1.twinx()\nnz2.groupby(['date'])['totals.transactionRevenue'].count().plot(color='brown')\nax2.set_ylabel('Visits count')\nplt.legend(['Revenue users'], loc = (0.7, 0.95))\nplt.grid(False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"db6ffac85599b1da6b4406991752d3568f8867d8"},"cell_type":"markdown","source":"**Worth noticing :**\n<br>\nRevenue transaction increase a lot in december. Good Christmas gift !"},{"metadata":{"trusted":true,"_uuid":"1f0ba516ba2020a6eb9be82a0d84727ebe639f0f"},"cell_type":"code","source":"def barplot_visit(feat):\n    feat_data = 100*train_df[feat].value_counts()/len(train_df)\n    feat_data = feat_data.to_frame().reset_index()\n    \n    nz = 100 * train_df.loc[train_df['totals.transactionRevenue'] > 0][feat].value_counts() / len(train_df.loc[train_df['totals.transactionRevenue'] > 0])\n    nz = nz.to_frame().reset_index()\n    \n    trace1 = go.Bar(\n        x=feat_data['index'],\n        y=feat_data[feat],\n        name='Zero Revenue',\n        marker=dict(color=randomc())\n    )\n    \n    trace2 = go.Bar(\n        x=nz['index'],\n        y=nz[feat],\n        name='Non Zero Revenue',\n        marker=dict(color=randomc())\n    )\n    \n    layout = go.Layout(\n        title=feat,\n        height=100, width=100,\n        xaxis=dict(\n            tickfont=dict(size=14)\n        ),\n        yaxis=dict(\n            title='Percentage of visits for each {}'.format(feat),\n            titlefont=dict(size=16),\n            tickfont=dict(size=14)\n        ),\n        legend=dict(\n            x=1.0,\n            y=1.0,\n            bgcolor='rgb(255, 255, 255)',\n            bordercolor='rgb(255, 255, 255)'\n        ),\n       \n    )\n    \n\n    fig = tools.make_subplots(rows=1, cols=2)\n\n    fig.append_trace(trace1, 1, 1)\n    fig.append_trace(trace2, 1, 2)\n    fig['layout'].update(autosize=False, height=300, width=1000, title='Percentage of visits for each {}'.format(feat))\n\n    iplot(fig)\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a0813ad3d8b2be54b66d0112d9d9c6346616e06b"},"cell_type":"markdown","source":"**Let's see how the continent, the device and other  features are related to the number of visits**"},{"metadata":{"trusted":true,"_uuid":"834f645a6e4efa029fa0158666ba24969558c40b"},"cell_type":"code","source":"barplot_visit('channelGrouping')\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cebc378ee639e86b002179e142160fa5dd642eef"},"cell_type":"markdown","source":"Visits from Referal Channel group tend to buy more whereas they represent only 11% of the visits"},{"metadata":{"trusted":true,"_uuid":"686059709e75b8e1d7ca0a39f3eb010678165f8c"},"cell_type":"code","source":"barplot_visit('device.operatingSystem')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2de62799a55461ecabe89c834953a3dc194b5524"},"cell_type":"markdown","source":"How surprising ! People using Macintosh seem buy more often than Windows and Linux users"},{"metadata":{"trusted":true,"_uuid":"2e301a376cd4fba3d8058acb3fb234a5a5deee1e"},"cell_type":"code","source":"barplot_visit('geoNetwork.continent')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e826df1fcd22e8bef1e62369cbdd575c6064300b"},"cell_type":"code","source":"barplot_visit('geoNetwork.subContinent')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ee69496534389d0a1c6f5121b43a0c0bbb30ef0b"},"cell_type":"code","source":"barplot_visit('device.deviceCategory')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b5886108016049a4206dfb7da84624a75f4c1fa9"},"cell_type":"markdown","source":"As expected, most people visit the website from their desktop/laptop"},{"metadata":{"trusted":true,"_uuid":"fdbc03966205d1f8110baee48fdf8d2029300817"},"cell_type":"code","source":"def barplot_revenue(feat):\n    feat_data = 100*train_df.groupby(feat)['totals.transactionRevenue'].sum()/train_df['totals.transactionRevenue'].sum()\n    feat_data = feat_data.to_frame().reset_index()\n\n    trace0 = go.Bar(\n        x=feat_data[feat],\n        y=feat_data['totals.transactionRevenue'],\n        name=feat,\n        marker=dict(color=randomc())\n    )\n    \n    layout = go.Layout(\n        title=feat,\n        autosize=False,\n        width=800,\n        height=300,\n        xaxis=dict(\n            tickfont=dict(size=14)\n        ),\n        yaxis=dict(\n            title='Percentage of revenue for each {}'.format(feat),\n            titlefont=dict(size=16),\n            tickfont=dict(size=14)\n        ),\n        legend=dict(\n            x=1.0,\n            y=1.0),\n       \n    )\n    \n    fig = go.Figure(data=[trace0], layout=layout)\n    iplot(fig)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a971eba0e5a45fc8c30828cdabfe60bcbb3725b9"},"cell_type":"code","source":"barplot_revenue('channelGrouping')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a9814a1b29c389b2bc24bb2fdf832371d7e6e5e4"},"cell_type":"code","source":"barplot_revenue('device.operatingSystem')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"af2533a61b805a39d437182eb1a4b18d4475e8df"},"cell_type":"code","source":"barplot_revenue('geoNetwork.continent')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bcc173aae426a8ba8a5a8cb82f4427aa3b30de98"},"cell_type":"code","source":"barplot_revenue('device.deviceCategory')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ff37c64c04b6c9f0bd7e0f596c88e97be075a323"},"cell_type":"markdown","source":"Revenue comes mostly from desktop users"},{"metadata":{"trusted":true,"_uuid":"c272349e6ec46605f4e95f6140204c99424bbaff"},"cell_type":"code","source":"date_revenue = 100* train_df.groupby('date')['totals.transactionRevenue'].sum()/train_df['totals.transactionRevenue'].sum()\n\nplt.subplots(figsize = (14, 6))\nplt.title(\"Percenntage of revenue per day\");\n\ndate_revenue.plot(linewidth=1.25)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3a53064a7167eca309a7f8db521da1b60c3602d4"},"cell_type":"markdown","source":"**What we get from that :**\n<br>\n<br>\nFor every month we see approximately 4 drop & rise patterns. And the peaks are sharper. We can easily infer than people are more likely to buy during the week end. So if we were doind manual feature engineering we could add a column of binary variable indicating if the corresponding day is in the week end or not."},{"metadata":{"_uuid":"43c5c1fa463f06831e944f6323a3e5b6ad31304a"},"cell_type":"markdown","source":"**Conclusion of EDA :** \n<br>\n<br>\nWe don't do EDA just for the sake of EDA, here is  what we can conclude :\n\n1. The distribution of the sums of non zero transaction revenues does not fit a popular distribution. Especially not the normal distribution. \n2. Appart from this last feature creation ( binary variable to indicate week end days), this EDA does not give many hint for potential manual feature engineering\n<br>\n<br>\nThe dataset has many categorical features and only two numerical features. So I think ensembles will be of a good help for feature engineering and predictions too. If anyone has a non-blackbox method or a more interpretable way to do feature engineering, please tell me ! I would like to know. \n"},{"metadata":{"_uuid":"2df196c1159f9f6f21e7ae9a4acd0c5ed2e1d47e"},"cell_type":"markdown","source":"**Feature Selection**\n<br>\n<br>\nAs manual feature engineering is not obvious here, let's use **Random Forest Algorithm** for Feature Selection. We will first one-hot encode the whole training set."},{"metadata":{"trusted":true,"_uuid":"be36e61a86afc06e9dff1093025f31828fe0d5b2"},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\n#categorical features\ncat_feat = list(train_df.columns.values)\ncat_feat.remove('totals.transactionRevenue')\ncat_feat.remove(\"totals.pageviews\")\ncat_feat.remove(\"totals.hits\")\n\n#numerical features\nnum_feat = [\"totals.hits\", \"totals.pageviews\"]\n\n\nfor feat in cat_feat:\n    lbl = LabelEncoder()\n    lbl.fit(list(train_df[feat].values.astype('str')))\n    train_df[feat] = lbl.transform(list(train_df[feat].values.astype('str')))\nfor feat in num_feat:\n    train_df[feat] = train_df[feat].astype(float)\n\ny= train_df['totals.transactionRevenue']\n\nfeats= cat_feat + num_feat\n\nX = train_df[feats]\n\n# Train test split without shuffle to keep the date order\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle=False, random_state=0)\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fd3805cbf81b4570fe394c2e203fbb52f8249a1b"},"cell_type":"code","source":"# Create a random forest classifier\nfrom sklearn.ensemble import RandomForestRegressor\n\nrgr = RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=2,\n           max_features='auto', max_leaf_nodes=None,\n           min_impurity_decrease=0.0, min_impurity_split=None,\n           min_samples_leaf=1, min_samples_split=2,\n           min_weight_fraction_leaf=0.0, n_estimators=1000, n_jobs=-1,\n           oob_score=False, random_state=0, verbose=1, warm_start=False)\n# Train the classifier. You can try with other parameters, \n# Especially try to use more estimators if your machine is more powerful.\nrgr.fit(X_train, y_train)\n\n# Print the name and gini importance of each feature\nfor feature in zip(feats, rgr.feature_importances_):\n    print(feature)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4e70d396bf8df701126604b5f949ab7ac9f8007e"},"cell_type":"code","source":"feat_score =sorted(zip(feats, rgr.feature_importances_), key=lambda tup: tup[1], reverse=True)\nscore_list=[x[1] for x in feat_score]\nfeat_list=[x[0] for x in feat_score]\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d9392976c802ba8ac50357a4076d1bcce4a32de4"},"cell_type":"code","source":"trace0 = go.Bar(\n                x = feat_list,\n                y = score_list,\n                name = \"Score of features\",\n                marker = dict(color=randomc()))\ndata = [trace0]\nlayout = go.Layout(\n    xaxis = dict(tickangle = -25),\n    title='Score of features',\n)\nfig = go.Figure(data = data, layout = layout)\niplot(fig)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6cacad01e14e1ce9e02431ee5fe0a2ef79415a72"},"cell_type":"markdown","source":"**This is the end of my kernel**\n<br>\n<br>\nEDA did not give many hints so manual feature engineering did not seem useful. We have performed feature engineering with a randomforest algorithm. The most important feature seems to be the total page viewed, by far.\n<br>\nQuite interesting analysis. Though, I wonder how this problem would be tackled by a professional data science team. What happens when you have a dataset where the only methods you can use for both feature engineering/selection, are ensemble or black-box model. How do you explain the result to the client ? \" I did some magic tricks and my algorithm has good performance \" ?? Any insight please tell me I want to know !\n\nThank you for reading !\n"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}