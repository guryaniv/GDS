{"cells":[{"metadata":{"_uuid":"0d75fde616b6bd38514203c664ef7f923011158b"},"cell_type":"markdown","source":"# MNIST - MLP and CNN with TF & Keras\n\n## Lucas Pierezan\n\n**Mar 2019**\n\nThe objectives:\n\n- Build and train models: MLP and CNN.\n- Experiment with Keras APIs (Sequential and Functional).\n- Experiment with ImageDataGenerator to do data augmentation.\n- Experiment with tf.data to build input pipeline (with data augmentation)\n\nReferences:\n\nFor building the CNN:  \n\n- https://medium.com/datadriveninvestor/five-powerful-cnn-architectures-b939c9ddd57b\n\nFor building a pipeline with tf.data:  \n\n- https://www.tensorflow.org/guide/datasets\n- https://gist.github.com/datlife/abfe263803691a8864b7a2d4f87c4ab8\n- https://dominikschmidt.xyz/tensorflow-data-pipeline/\n- https://cs230-stanford.github.io/tensorflow-input-data.html"},{"metadata":{"_uuid":"e2e84f6fa38fc27a530c4d074093175ae11e41c2","trusted":true},"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow.keras as keras\nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport pandas as pd\n\nimport os\nprint(os.listdir(\"../input\"))\n\n\nprint(tf.__version__)\nprint(tf.keras.__version__)\n\ntf.keras.backend.set_image_data_format('channels_first')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d6c390ab866d82201a12d6a9b271bfdcf53e21c2","trusted":true},"cell_type":"code","source":"def plot_train_hist(hist : keras.callbacks.History, before_min_percent = 0.7):\n    \"\"\"\n    Plot loss/acc curves over training data.\n    \"\"\"\n    min_val_loss_epoch = hist.epoch[np.argmin(hist.history['val_loss'])]    \n    st_epoch = int((1-before_min_percent)*min_val_loss_epoch)\n    x_epoch = hist.epoch[st_epoch:]\n    \n    fig, ax = plt.subplots(figsize = (10,10))\n    ax.plot(x_epoch, hist.history['val_loss'][st_epoch:], label = 'val_loss')\n    ax.plot(x_epoch, hist.history['loss'][st_epoch:], label = 'train_loss')\n    ax.set_ylabel('loss')\n\n    ax2 = ax.twinx()\n    ax2.plot(x_epoch, hist.history['val_acc'][st_epoch:], label = 'val_acc', c = 'r')\n    ax2.plot(x_epoch, hist.history['acc'][st_epoch:], label = 'train_acc', c = 'g')\n    ax2.set_ylabel('acc')\n    \n    \n    ax.vlines(min_val_loss_epoch, *ax.get_ylim(), linestyles='dashed', label = 'min_val_loss')\n    \n    ax.legend(loc = 'upper left')\n    ax2.legend(loc = 'upper right')\n    \n    #plt.legend()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"83dc3b834c31dbbf6d46860a01a3ec8db536ac65"},"cell_type":"markdown","source":"# Loading the Data"},{"metadata":{"_uuid":"401f363a7a2f5026fcabc11318869a1c4cf20e9e","trusted":true},"cell_type":"code","source":"def load_data(train = True):\n    fp = '../input/train.csv' if train else '../input/test.csv'\n    df = pd.read_csv(fp)\n    \n    if train:\n        y_train = df['label'].values\n        df.drop('label',axis=1, inplace = True)\n        \n    x_train = df.values\n    n,l2 = x_train.shape\n    l = int(l2**0.5)\n    x_train = x_train.reshape((n, l, l)).astype(float)\n    \n    if train:\n        return x_train, y_train\n    else:\n        return x_train\n\n\nx, y = load_data()\nx_test = load_data(train=False)\n\nx.shape, y.shape, x_test.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8d9227fae90e099f94d1d58422d4b6dd223cb2f5","trusted":true},"cell_type":"code","source":"np.random.seed(42)\n\ni = np.random.randint(0,len(x))\nplt.imshow(x[i], cmap='gray', interpolation='none')\nprint('label', y[i])\nprint('min:', x.min(), 'max:',  x.max())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"05a09148d650c24afd7fe2102770f78f7d046939"},"cell_type":"markdown","source":"### Creating Validation Set"},{"metadata":{"_uuid":"7fca9c00de8f046ea51cb8b1b9969c05e02d1f2b","trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nx_train, x_val, y_train, y_val = train_test_split(x, y, test_size = 0.2, random_state = 42, shuffle = True)\nx_train.shape, y_train.shape, x_val.shape, y_val.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"56f0b8f852d50ee98cd03d02a6d9434b3fb799d9","trusted":true},"cell_type":"code","source":"# are all classes represented?\npd.Series(y_val).value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a366ddd86896b6d5d092b716f9f32e937e484367"},"cell_type":"markdown","source":"### Add Channels dimension"},{"metadata":{"_uuid":"4f3b6cc11109e6a9cb6b37dc6fb57a874cb30649","trusted":true},"cell_type":"code","source":"# Expanding x_train, x_test to have a channels dimension (and to be channels first)\n\nx_train_k = np.expand_dims(x_train, axis = 1)\nx_val_k = np.expand_dims(x_val, axis = 1)\n\nx_train_k.shape, x_val_k.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b63667a0fe90d6a9a423e575c6deb51565c95154","trusted":true},"cell_type":"code","source":"num_classes = 10\ny_train_k = keras.utils.to_categorical(y_train, num_classes)\ny_val_k = keras.utils.to_categorical(y_val, num_classes)\ny_train_k.shape, y_val_k.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"26a15e4a5b511ceb409b2008d1ae845dbd29e399"},"cell_type":"markdown","source":"# Keras ImageDataGenerator\n\nLet압 use ImageDataGenerator to create a input pipelines with:\n\n- Spliting data to validation\n- Rescaling pixel by a constant value.\n- Control batch size\n\nThese generators loop over their data indefinitely."},{"metadata":{"_uuid":"0de603a3eeeaa141b64ebbfee75aeed3cb407f25","trusted":true},"cell_type":"code","source":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\n\nimg_gen_train = ImageDataGenerator(rescale=1/255., rotation_range = 12, width_shift_range=0.08, height_shift_range=0.08, zoom_range=0.08)\nimg_gen_val = ImageDataGenerator(rescale=1/255.)\n\n\nimg_gen_train.fit(x_train_k, seed=42)\nimg_gen_val.fit(x_train_k, seed=42)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"666829f970311b78d7e73747e6f604f61d1ec2b7"},"cell_type":"markdown","source":"**Testing the image data augmentation**"},{"metadata":{"_uuid":"623985df905d6d8aaa5369824dfc9ccb0fa14232","trusted":true},"cell_type":"code","source":"idx_example = np.random.randint(0,len(x_train_k))\nfig, ax = plt.subplots(figsize = (2.5,2.5))\nplt.imshow(x_train_k[idx_example][0], cmap='gray', interpolation='none')\nplt.show()\n\nn_row, n_col = 3, 8\nfig, axs = plt.subplots(n_row, n_col, figsize = (2.5*n_col,2.5*n_row))\ni = 0\nfor z in img_gen_train.flow(x_train_k[[idx_example]], batch_size=1):\n    i += 1\n    if i > n_row*n_col:\n        break\n    row, col = (i-1)//n_col, (i-1)%n_col    \n    axs[row,col].imshow(z[0,0], cmap='gray', interpolation='none')\n    \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"42d37700934cad0ec0f31a407f1ed24bac25ab40"},"cell_type":"markdown","source":"# MLP Model\n\nLet압 fix an architeture that is composed by dense hidden layers with dropout beetween then and before the output layer.\n\nInput -> Dense -> Dropout -> Dense -> Dropout -> Dense -> ... -> Dropout -> Output"},{"metadata":{"_uuid":"712b3ab4e78fce775516b0b5fba83b03793d8e15","trusted":true},"cell_type":"code","source":"def create_mlp_model(dense_sizes = [100,100], dropouts = [0.5, 0.5]):\n    mlp_model = keras.Sequential()\n    mlp_model.add(keras.layers.Flatten(input_shape = (1,28,28)))\n    \n    for n,p in zip(dense_sizes, dropouts):\n        mlp_model.add(keras.layers.Dense(n, activation='relu'))\n        mlp_model.add(keras.layers.Dropout(p))\n    \n    mlp_model.add(keras.layers.Dense(num_classes, activation='softmax'))\n    \n    mlp_model.summary()\n    \n    return mlp_model","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6698e194d1f40fedd961932dee2e26c923e41654"},"cell_type":"markdown","source":"### Training\n\n**Optimizer:**\n- Using the AdamOptimizer\n\n**Loss:**\n- Classical categorical cross entropy.\n\n**Model Checkpoint:**\n\n- Saving the models weight, after every epoch, if val_loss has improved.\n    \n**EarlyStopping:**\n\n- Stopping training when val_loss hasn앖 increased for 32 epochs.\n\n**Tensor Board:**\n\n- We experimented with Tensor Board, but it crashes training in a conflict with checkpoint files. It is possibly \nhttps://github.com/tensorflow/tensorflow/issues/21135 or https://github.com/tensorflow/tensorboard/issues/892\n"},{"metadata":{"_uuid":"1b856d6007a2beec1ed497426a217910ce9ae1a3","trusted":true},"cell_type":"code","source":"def create_callbacks(model_name, patience, lr_reduction_params = None):\n    callbacks = []\n    \n    # Model Checkpoint\n    ckpt_path = './{model_name}_model/{model_name}_weights.ckpt'.format(model_name = model_name)\n    os.makedirs(os.path.dirname(ckpt_path), exist_ok=True)\n    model_ckpt = keras.callbacks.ModelCheckpoint(ckpt_path, monitor='val_loss', verbose=1, save_best_only = True, save_weights_only = True, period=1)\n    callbacks.append(model_ckpt)\n    \n    # Early Stopping\n    early_stopping_cb = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=patience, verbose=1, mode='auto', baseline=None)\n    callbacks.append(early_stopping_cb)\n    \n    # TensorBoard\n    #tensor_board_folder = './{model_name}_model/tensor_board_log/'.format(model_name = model_name)\n    #os.makedirs(tensor_board_folder, exist_ok=True)\n    #tensor_board_cb = keras.callbacks.TensorBoard(tensor_board_folder, histogram_freq=5, write_grads=True, write_images=True)\n    \n    #\n    if lr_reduction_params is not None:\n        lr_reduction = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', verbose=1, **lr_reduction_params)\n        \n        callbacks.append(lr_reduction)\n    \n\n    return callbacks, ckpt_path","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"889191382acd02e0774c0cea57a923c7c776872d","trusted":true},"cell_type":"code","source":"%%time\n\n# == HYPERPARAMETERS\ndense_sizes = [256, 128]\ndropouts = [0.25, 0.25]\nlearning_rate = 0.001\nbatch_size = 128\nn_epoch = 128\npatience = 32\n\n# == CREATING MODEL\nmlp_model = create_mlp_model(dense_sizes, dropouts)\n\n# == OPTIMIZER, LOSS AND COMPILATION\nadam_opt = tf.train.AdamOptimizer(learning_rate = learning_rate, beta1=0.9, beta2=0.999)\nmy_loss = keras.losses.categorical_crossentropy\nmlp_model.compile(adam_opt, my_loss, metrics=['accuracy'])\n\n# == CALLBACKS\nmlp_callbacks, ckpt_path = create_callbacks('mlp', patience=patience)\n\n# == INPUT GENERATORS\n\n# generators that loops forever\ntrain_generator = img_gen_train.flow(x_train_k, y_train_k, batch_size = batch_size, seed=42, shuffle=True)\nval_generator = img_gen_val.flow(x_val_k, y_val_k, batch_size = len(x_val_k), shuffle=False)\n\n# == FIT\n\nhist = mlp_model.fit_generator(train_generator, steps_per_epoch=None, epochs=n_epoch, verbose=2, \n                        callbacks= mlp_callbacks, validation_data = val_generator)\n\n# == LOSS CURVE\nplot_train_hist(hist)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"403a47e71c2a2c323fe72923e34dd7725f1006a6"},"cell_type":"markdown","source":"### Predict\n\nLet압 restore best model weights and make a prediction on test data\n\n"},{"metadata":{"_uuid":"5b9ec391614eb48ab5f4723b5d1eaf1d667856dc","trusted":true},"cell_type":"code","source":"def predict_test(model, x_test, img_gen, n_augment = 1):\n    # channels dim\n    x_test_k = np.expand_dims(x_test, axis = 1)\n    \n    # use the trained image generator pipeline (it should not do data augmentation)\n    test_generator = img_gen.flow(x_test_k, batch_size= len(x_test_k), shuffle=False)\n    \n    y_test = None\n    i = 0\n    for x_test_prep in test_generator:\n        if y_test is None:\n            y_test = model.predict(x_test_prep)\n        else:\n            y_test += model.predict(x_test_prep)\n\n        i += 1\n        if i >= n_augment:\n            break\n    \n    y_test_label = y_test.argmax(axis = 1)\n    y_test = y_test/y_test.sum(axis = 1, keepdims=True)\n    \n    return y_test, y_test_label\n\ndef validation_metrics(model, x_val, y_val, img_gen_val, n_augment = 1):\n    \n    y_pred, y_pred_label = predict_test(model, x_val, img_gen_val, n_augment)\n    y_val_k = keras.utils.to_categorical(y_val, num_classes)\n    \n    loss = keras.metrics.categorical_crossentropy(y_val_k, y_pred)\n    loss = keras.backend.eval(loss).mean()\n    \n    acc = keras.metrics.categorical_accuracy(y_val_k, y_pred)\n    acc = keras.backend.eval(acc).mean()\n    \n    print('val loss:', loss, ' val acc:', acc)\n    ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3cc6be19d8ac6f703c424a246b56fde3421f92dd","trusted":true},"cell_type":"code","source":"# loading best weights\nmlp_model.load_weights(ckpt_path)\n\n# print validation metrics evaluation\nvalidation_metrics(mlp_model, x_val, y_val, img_gen_val)\n\n# test set prediction\ny_test, y_test_label = predict_test(mlp_model, x_test, img_gen_train, n_augment=10)\n\ny_test.shape, y_test_label.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f116a99e07d0f2a331a116b20b7e0e97383dbf0f","trusted":true},"cell_type":"code","source":"i = np.random.randint(0, len(x_test))\nplt.imshow(x_test[i]), y_test_label[i]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"727eacade7c2642605ed28e10afcd1a27c138c8d"},"cell_type":"markdown","source":"### Creating Submission"},{"metadata":{"_uuid":"37edb55ee0611aa7318dcea512c77eda2996df50","trusted":true},"cell_type":"code","source":"def create_submission(y_test_label, model_name = 'model'):\n    filepath = './' + model_name + '.csv'\n    df = pd.DataFrame(data = {'ImageId' : range(1, len(y_test) + 1), 'Label' : y_test_label})    \n    df.to_csv(filepath, index = False)\n    print('Submission saved in', filepath)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1cff82901bfc33f34a5ba67e690063860829ec8a","trusted":true},"cell_type":"code","source":"create_submission(y_test_label, 'mlp')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"89c1c9437ce39ae2a7fafe240471702cd293fa78"},"cell_type":"markdown","source":"The first submission, with \n\n- dense_sizes = [100,100]\n- dropouts = [0.5, 0.5]\n- learning_rate = 0.001\n- batch_size = 32\n- validation metrics: [0.11049949377775192, 0.9695237874984741]\n\nscored 0.9670 in LB.\n\n\nThe second submission, with\n\n- **ImageDataGenerator(rescale=1/255., rotation_range = 12, width_shift_range=0.08, height_shift_range=0.08, zoom_range=0.08)**\n- dense_sizes = [150,100]\n- dropouts = [0.2, 0.2]\n- learning_rate = 0.001\n- batch_size = 64\n- validation metrics: [0.049472346901893616, 0.9845238327980042]\n\nscored 0.98657 in LB.\n"},{"metadata":{"_uuid":"862e414cf80ee687cbde5f12f41ca194886ae824"},"cell_type":"markdown","source":"# CNN Model"},{"metadata":{"_uuid":"2aed5245878685ca75eb2edb2eb561bd53100099","trusted":true},"cell_type":"code","source":"def create_cnn_model(num_classes = 10, activation = 'relu'):\n    \n    inputs = keras.Input(shape=(1,28,28))\n    x = inputs    \n    \n    # == inception-like block ==\n    i_1 = keras.layers.Conv2D(32, 1, activation=activation)(x)\n    i_1 = keras.layers.MaxPool2D()(i_1)\n    \n    i_3 = keras.layers.Conv2D(32, 3, padding='same', activation=activation)(x)\n    i_3 = keras.layers.MaxPool2D()(i_3)\n    \n    i_5 = keras.layers.Conv2D(32, 5, padding='same', activation=activation)(x)\n    i_5 = keras.layers.MaxPool2D()(i_5)\n    \n    i_avg = keras.layers.AveragePooling2D()(x)\n    \n    i_conc = keras.layers.concatenate([i_1, i_3, i_5, i_avg], axis = 1)\n    x = i_conc\n    # ==\n    \n    x = keras.layers.Conv2D(64, 5, strides = 1, padding='valid', activation=activation)(x)\n    x = keras.layers.MaxPool2D()(x)\n    \n    x = keras.layers.Conv2D(256, 3, strides = 1, padding='valid', activation=activation)(x)\n    x = keras.layers.GlobalAveragePooling2D()(x)\n    \n    x = keras.layers.Flatten()(x)\n    #x = keras.layers.Dense(64, activation=activation, kernel_regularizer=keras.regularizers.l1(0.0001))(x)    \n    x = keras.layers.Dropout(0.5)(x)\n    \n    outputs = keras.layers.Dense(num_classes, activation='softmax')(x)\n    \n    cnn_model = keras.Model(inputs = inputs, outputs = outputs)    \n    cnn_model.summary()\n    \n    return cnn_model","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6a51336cd3d6e3b7cbf829f5e043e54a129f369d","trusted":true},"cell_type":"code","source":"def train_cnn(model_name = 'cnn', learning_rate = 1e-3, batch_size = 128, n_epoch = 32, patience= 16, activation = 'relu', \n              lr_reduction_params={'factor' : 0.5, 'patience' : 8, 'min_delta' : 0}, \n             train_generator = None, val_generator = None, train_ds = None, val_ds = None, steps_per_epoch = None):\n    \n    # == CREATING MODEL\n    cnn_model = create_cnn_model(activation=activation)\n\n    # == OPTIMIZER, LOSS AND COMPILATION\n    # The ReduceLROnPlateau callback didn't work with tf.train.AdamOptimizer\n    #adam_opt = tf.train.AdamOptimizer(learning_rate = learning_rate, beta1=0.9, beta2=0.999)\n    \n    adam_opt = tf.keras.optimizers.Adam(lr = learning_rate, beta_1=0.9, beta_2=0.999)\n\n    my_loss = keras.losses.categorical_crossentropy\n    cnn_model.compile(adam_opt, my_loss, metrics=['accuracy'])\n\n    # == CALLBACKS\n    cnn_callbacks, cnn_ckpt_path = create_callbacks(model_name, patience=patience, lr_reduction_params=lr_reduction_params)\n\n    # == FIT\n    if train_generator is not None:\n        cnn_hist = cnn_model.fit_generator(train_generator, steps_per_epoch=None, epochs=n_epoch, verbose=2, \n                                           callbacks = cnn_callbacks, validation_data = val_generator)\n    elif train_ds is not None:\n        cnn_hist = cnn_model.fit(train_ds.make_one_shot_iterator(), steps_per_epoch=steps_per_epoch, epochs=n_epoch, \n                                 verbose=2, callbacks = cnn_callbacks, validation_data = val_ds.make_one_shot_iterator(), \n                                 validation_steps = 1)\n\n    # == LOSS CURVE\n    plot_train_hist(cnn_hist)\n    \n    return cnn_model, cnn_hist, cnn_ckpt_path","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"587c994ac8a453ec1b618119b75826ec22a2a72c","scrolled":false,"trusted":true},"cell_type":"code","source":"%%time\n\n# == HYPERPARAMETERS\nlearning_rate = 0.001\nbatch_size = 128\nn_epoch = 128\npatience = 16\n\n# generators that loops forever\ntrain_generator = img_gen_train.flow(x_train_k, y_train_k, batch_size = batch_size, seed=42, shuffle=True)\nval_generator = img_gen_val.flow(x_val_k, y_val_k, batch_size = len(x_val_k), shuffle=False)\n\ncnn_model, cnn_hist, cnn_ckpt_path = train_cnn(model_name = 'cnn', learning_rate = learning_rate, batch_size = batch_size, n_epoch = n_epoch, \n                                               patience= patience, activation = 'relu', lr_reduction_params={'factor' : 0.5, 'patience' : 8, 'min_delta' : 0}, \n                                               train_generator = train_generator, val_generator = val_generator)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"036ec8cc494fd92d52c07343a9fab1e648fc0d7f"},"cell_type":"markdown","source":"### Predict CNN"},{"metadata":{"_uuid":"23e4a9fa7c19c92b76b0cf59b5b35240ec53e22c","trusted":true},"cell_type":"code","source":"%%time\n\ncnn_model.load_weights(cnn_ckpt_path)\n\nvalidation_metrics(cnn_model, x_val, y_val, img_gen_val)\n\ny_test, y_test_label = predict_test(cnn_model, x_test, img_gen_val)\n\ncreate_submission(y_test_label, 'cnn')\n\ny_test_aug, y_test_label_aug = predict_test(cnn_model, x_test, img_gen_train, n_augment=10)\n\ncreate_submission(y_test_label_aug, 'cnn_aug')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7804323b15d4d0505750a6148560076cb173c510"},"cell_type":"markdown","source":"Submission 1:\n\n- conv5 -> maxpool /2 -> conv5 -> maxpool /2 -> dense -> dense10\n- learning_rate = 0.001\n- batch_size = 128\n- n_epoch = 128\n- patience = 32\n- validation metrics: [0.019111450761556625, 0.9936904907226562]\n\nscored 0.9920 in LB.\n\nSubmission 2:\n\n- inception_block -> conv5 -> maxpool -> conv3 -> maxpool -> dense10\n- learning_rate = 0.0005\n- batch_size = 128\n- n_epoch = 128\n- patience = 32\n- validation metrics: [0.011771622113883495, 0.9957143068313599]\n\nscored 0.9950 in LB.\n\nand Submission 3:\n- inception_block (/2) -> conv5 -> maxpool -> conv3 -> avgpool -> dropout -> dense10\n- learning_rate = 0.001 + ReduceLRonPlateau(factor = 0.5, patience = 8) \n- patience = 16\n\nscored 0.9950 in LB.\n\nSubmission 3 + test_data_augmentation (n_augment = 10)\n\n**scored 0.99585 in LB.**\n"},{"metadata":{"_uuid":"969af28072d85f762384cf21c5950fdd67a1a441"},"cell_type":"markdown","source":"# Data Input Pipeline with tf.data\n"},{"metadata":{"trusted":true,"_uuid":"ea3ba492b396480314994c52d96eb9db1b3bb2a5"},"cell_type":"code","source":"def plot_batch(imgs):\n    n_row, n_col = (int(np.ceil(len(imgs)/8)), 8) if len(imgs) > 8 else (1, len(imgs))\n    \n    fig, axs = plt.subplots(n_row, n_col, figsize = (2.5*n_col,2.5*n_row))\n\n    for i in range(len(imgs)):\n        z = imgs[i]\n        if len(z.shape) == 3:\n            z = z[0]\n        if i+1 > n_row*n_col:\n            break\n        row, col = (i)//n_col, (i)%n_col\n        if len(axs.shape) == 2:\n            axs[row,col].imshow(z, cmap='gray', interpolation='none')\n        else:\n            axs[col].imshow(z, cmap='gray', interpolation='none')\n\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2679cdf3272d4faefbcb9b91526aa446ef5f8c43","trusted":true},"cell_type":"code","source":"def _preprocess_img(img, label, angle_range = 12, translation_range = 0.08, \n                    zoom_range = 0.08, method = 'bilinear', augmentation_fraction = 0.8):\n    img = img / 255.\n    img_no_aug = tf.reshape(img, (1, 28, 28))\n    \n    # tf.image transformations need: NHWC    \n    img = tf.reshape(img, (1, 28, 28, 1))\n    \n    # == random rotation\n    if angle_range is not None:\n        angle_range_rad = np.deg2rad(angle_range)\n        random_angle = tf.random.uniform((1,), -angle_range_rad, angle_range_rad)\n        img = tf.contrib.image.rotate(img, angles = random_angle, interpolation=method.upper())\n        \n    \n    # == crop and resize (= translate and zoom)\n    min_corner = tf.constant([0,0], dtype=tf.float32, shape=[2])\n    \n    if translation_range is not None:\n        min_corner = min_corner + tf.random_uniform(shape = (2,) , minval=-translation_range, maxval=translation_range)\n    max_corner = min_corner + [1,1]\n    \n    if zoom_range is not None:\n        random_zoom = tf.random.uniform((1,), minval=1-zoom_range, maxval=1+zoom_range)\n        center = (min_corner + max_corner)/2.\n        min_corner_new = center + random_zoom * (min_corner - max_corner)/2.\n        max_corner_new = center + random_zoom * (max_corner - min_corner)/2.\n        min_corner = min_corner_new\n        max_corner = max_corner_new\n    \n    box = tf.concat([min_corner, max_corner], axis = 0)\n    boxes = tf.reshape(box, (1, 4))    \n    \n    img = tf.image.crop_and_resize(img, boxes, [0], crop_size = (28,28), method=method, extrapolation_value=0, name=None)\n    img = tf.cast(img, tf.float64)\n    \n    # changing to CHW\n    img = tf.reshape(img, (1, 28, 28))\n    \n    # final image -> randomly choose to do (or not) augmentation\n    do_aug = tf.random.uniform([], minval=0, maxval=1)\n    img = tf.cond(do_aug <= augmentation_fraction, lambda: img, lambda: img_no_aug)\n    \n    # label\n    y = tf.one_hot(label, 10)\n    return img , y\n\ndef build_input_dataset(x, y, batch_size = 32, mode = 'train', preprocess_img_params = dict()):\n    ds = tf.data.Dataset.from_tensor_slices((x, y))    \n    map_func = lambda img,label : _preprocess_img(img, label, **preprocess_img_params)\n    \n    if mode == 'train':\n        # ideally shuffle압 buffer_size should be n_samples but in this case data has already been shuffled\n        ds = ds.apply(tf.data.experimental.shuffle_and_repeat(buffer_size=1024))\n        ds = ds.apply(tf.data.experimental.map_and_batch(map_func, batch_size, num_parallel_batches=None, drop_remainder=False, num_parallel_calls=None))\n    else:\n        ds = ds.apply(tf.data.experimental.map_and_batch(map_func, len(x), num_parallel_batches=None, drop_remainder=False, num_parallel_calls=None))\n        df = ds.cache()\n        ds = ds.repeat()\n        \n    ds = ds.prefetch(1)\n    return ds","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1db18e422e50a12d21c9a3a794c57d78ce551a8d","trusted":true},"cell_type":"code","source":"%%time\n\nidx_examples = np.random.randint(0, len(x_train), size=8)\nprint('original')\nx_examples = x_train[idx_examples]\ny_examples = y[idx_examples]\nplot_batch(x_examples)\n\nds = build_input_dataset(x_examples, y_examples, batch_size=3, mode='train', preprocess_img_params={'method' : 'nearest', 'angle_range' : 12})\nit = ds.make_one_shot_iterator()\nelem = it.get_next()\n\nwith tf.Session() as sess:\n    for j in range(4):\n        print('='*20, 'batch',j+1, '='*20)\n        imgs, labels = sess.run(elem)\n        print('imgs.shape:', imgs.shape, 'labels.shape:', labels.shape)\n        plot_batch(imgs)\n\n    \n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e87ae09ca9741990428c88632f5b86b4d4e709ea","scrolled":false,"trusted":true},"cell_type":"code","source":"%%time\n\n# == HYPERPARAMETERS\nlearning_rate = 0.001\nbatch_size = 128\nn_epoch = 128\npatience = 16\nmethod = 'nearest'\naugmentation_fraction = 0.8\n\n# generators that loops forever\ntrain_ds = build_input_dataset(x_train, y_train, batch_size=batch_size, mode='train', \n                               preprocess_img_params={'method' : method, 'augmentation_fraction' : 0.8})\n\nsteps_per_epoch = len(x_train) // batch_size\n\nval_ds = build_input_dataset(x_val, y_val, mode='val')\n\ncnn2_model, _, cnn2_ckpt_path = train_cnn(model_name = 'cnn2_{}'.format(method), learning_rate = learning_rate, n_epoch = n_epoch, steps_per_epoch=steps_per_epoch,\n                                               patience = patience, activation = 'relu', lr_reduction_params={'factor' : 0.5, 'patience' : 8, 'min_delta' : 0}, \n                                               train_ds = train_ds, val_ds = val_ds)\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}