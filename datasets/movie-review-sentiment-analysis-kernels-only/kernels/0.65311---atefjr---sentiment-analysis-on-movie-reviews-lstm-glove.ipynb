{"cells":[{"metadata":{"trusted":true,"collapsed":true,"_uuid":"dc7115c84828ef553948cc6726ad55e582e2a725"},"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport datetime as dt\nimport gensim\nfrom nltk.stem import WordNetLemmatizer\npd.options.display.width = None","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"28d6b14baea316ed213b407d74d9450bba3fb4ef","collapsed":true},"cell_type":"code","source":"# Load data\ndata = pd.read_csv('../input/movie-review-sentiment-analysis-kernels-only/train.tsv',sep='\\t')\n# View first 10 rows\nprint(data.head(10))\n# Info about training data\nprint(data.describe())\nprint(data.info())\nprint(data.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b6d93273751198348273d607fa500c3339f85a48"},"cell_type":"markdown","source":"**Count number of rows in each sentiment class, We can observe that the train data is biased to class \"2\" -> \"neutral\".**"},{"metadata":{"trusted":true,"_uuid":"b91f3402b34a0e1114707d70ff6b08be3e92b824","collapsed":true},"cell_type":"code","source":"# Class counts\nsns.countplot(data['Sentiment']).set_title(\"Count plot for sentiment classes\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"69b937a5feea91bb3487be91f8d4966c5006d578"},"cell_type":"markdown","source":"**Removing empty rows from training data.**"},{"metadata":{"trusted":true,"_uuid":"20b2216324d77ceef5f08be9dddb5f4319de39a3","collapsed":true},"cell_type":"code","source":"data['text_length'] = data['Phrase'].apply(lambda x: len(x.split()))\n# Remove empty phrase rows\nprint(data.loc[data['text_length'] == 0])\ndata = data[data['text_length'] != 0]\nprint(data.describe())\nprint(data[['text_length','Phrase']].head(10))\nprint(data['text_length'].describe())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1be7568ce65aa152dfa7433549db694eaf1bf941"},"cell_type":"markdown","source":"**Histogram of training data lengths. We can find that most of training data lengths are short**"},{"metadata":{"trusted":true,"_uuid":"5a38bbbcbc798698ecb5c8930c97551e429a82bd","collapsed":true},"cell_type":"code","source":"# Text length histogram\nsns.countplot(data['text_length']).set_title(\"Count plot for text length\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cc44f910beba524914f26debd8073701ba3b02bd"},"cell_type":"markdown","source":"**Training data pre-processing: **\n\n1- We can observe from the data that we don't have to remove punctuations and stopwords as the sentiment of a word/sentence change by different punctuation and stopword.\n\n2- Training data were lemmitiezed, lowered and tokenized."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"e0dc903cad98b9cc76d8c63f98ba44becf8693b3"},"cell_type":"code","source":"# Pre-Processing\nX = []\nfor row in data['Phrase']:\n    #row = re.sub('[^a-zA-Z]', ' ',row)\n    row = row.lower()\n    row = row.split()\n    lemm = WordNetLemmatizer()\n    row = [lemm.lemmatize(w) for w in row]\n    X.append(' '.join(row))\n\ndata['Process_Phrase'] = X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bd4c50630548b8e486dba4da7794c51b4f0ae512","collapsed":true},"cell_type":"code","source":"data = data.drop_duplicates(subset=['Process_Phrase', 'Sentiment'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cb35ba23dde838de28989065901957987fd7aae7","collapsed":true},"cell_type":"code","source":"print(data.head(10))\nprint(data.describe())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f949bfefa40dc17aa774ba58d54a79d82d4385b4","collapsed":true},"cell_type":"code","source":"X = data['Process_Phrase']\nsen = data['Sentiment']\nnb_classes = 5\n#One-hot vectors\nY = np.eye(nb_classes)[sen]\n\nprint(X[0:10])\nprint()\nprint(Y[0:10])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"deee9dc731b436fab0819656f77de782ce1b12ac"},"cell_type":"markdown","source":"**Tensorflow flags**"},{"metadata":{"trusted":true,"_uuid":"4755d131e01b6129232d4366fa2774f91bc05857","collapsed":true},"cell_type":"code","source":"import tensorflow as tf\n# Configurations\ntf.app.flags.DEFINE_string(\"rnn_unit\", 'lstm', \"Type of RNN unit: rnn|gru|lstm.\")\ntf.app.flags.DEFINE_float(\"learning_rate\", 1e-2, \"Learning rate.\")\ntf.app.flags.DEFINE_float(\"learning_rate_decay_factor\", 0.9, \"Learning rate decays by this much.\")\ntf.app.flags.DEFINE_float(\"max_gradient_norm\", 5.0, \"Clip gradients to this norm (clipping ratio).\")\ntf.app.flags.DEFINE_integer(\"num_epochs\", 6, \"Number of epochs during training.\")\ntf.app.flags.DEFINE_integer(\"batch_size\", 264, \"Batch size to use during training.\")\ntf.app.flags.DEFINE_integer(\"num_hidden_units\", 300, \"Number of hidden units in each RNN cell (i/p vector length).\")\ntf.app.flags.DEFINE_integer(\"num_layers\", 2, \"Number of layers in the model.\")\ntf.app.flags.DEFINE_float(\"dropout\", 0.4, \"Amount to drop during training.\")\ntf.app.flags.DEFINE_integer(\"num_classes\", 5, \"Number of classification classes.\")\ntf.app.flags.DEFINE_string('f', '', 'kernel')\nCONFIGS = tf.app.flags.FLAGS","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1823d3c003ed8e3d23264e2efb25cc6207d12e5d"},"cell_type":"markdown","source":"**Utility functions**"},{"metadata":{"trusted":true,"_uuid":"94ea498cb207190042ae10580767eb792ec373e8","collapsed":true},"cell_type":"code","source":"def load_data_and_labels(X,Y):\n    # Load the data\n    X = [s.strip() for s in X]\n    X = [s.replace(\"\\\"\", \"\") for s in X]\n    X = [[w for w in sent.strip().split()] for sent in X]\n    #Y = [w.replace(\"\\n\", '') for w in Y]\n    #Y = [[float(w) for w in sent.split()] for sent in Y]\n    return X, Y\n\n\ndef load_glove_model():\n    glove_model = gensim.models.KeyedVectors.load_word2vec_format(\"../input/stanfords-glove-pretrained-word-vectors/glove.6B.300d.txt\")\n    print(\"GLOVE MODEL LOADED\")\n    return glove_model\n\n\ndef sentence_to_vectors(sentence, glove_model, num_hidden_units):\n    return [glove_model.wv[word].tolist() if word in glove_model.wv.vocab else [0.0] * num_hidden_units for word in sentence]\n\n\ndef data_to_vectors(X, glove_model, num_hidden_units):\n    max_len = max(len(sentence) for sentence in X)\n\n    data_as_vectors = []\n    for line in X:\n        vectors = sentence_to_vectors(line, glove_model, num_hidden_units)\n        # Padding\n        data_as_vectors.append(vectors + [[0.0] * num_hidden_units] * (max_len - len(line)))\n\n    return data_as_vectors\n\n\ndef data_to_seqs(X):\n\n    seq_lens = []\n\n    for line in X:\n        seq_lens.append(len(line))\n\n    return seq_lens\n\ndef generate_epoch(X, y, seq_lens, num_epochs, batch_size):\n    for epoch_num in range(num_epochs):\n        yield generate_batch(X, y, seq_lens, batch_size)\n\n\ndef generate_batch(X, y, seq_lens, batch_size):\n    data_size = len(X)\n\n    num_batches = (data_size // batch_size)\n    for batch_num in range(num_batches):\n        start_index = batch_num * batch_size\n        end_index = min((batch_num + 1) * batch_size, data_size)\n        yield X[start_index:end_index], y[start_index:end_index], seq_lens[start_index:end_index]\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e4797897fb6f1a14ae74f95ed74f5aeae21c477d"},"cell_type":"markdown","source":"**LSTM RNN model**"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"2d329b03c66729895591be20e0b703d36e9c3167"},"cell_type":"code","source":"def rnn_cell(CONFIGS, dropout):\n    # Choosing cell type\n    # Default activation is Tanh\n    if CONFIGS.rnn_unit == 'rnn':\n        rnn_cell_type = tf.nn.rnn_cell.BasicRNNCell\n    elif CONFIGS.rnn_unit == 'gru':\n        rnn_cell_type = tf.nn.rnn_cell.GRUCell\n    elif CONFIGS.rnn_unit == 'lstm':\n        rnn_cell_type = tf.nn.rnn_cell.BasicLSTMCell\n    else:\n        raise Exception(\"Choose a valid RNN cell type.\")\n\n    # Create a single cell\n    single_cell = rnn_cell_type(CONFIGS.num_hidden_units)\n\n    # Apply dropoutwrapper to RNN cell (Only output dropout is applied)\n    single_cell = tf.nn.rnn_cell.DropoutWrapper(single_cell, output_keep_prob=1 - dropout)\n\n    # Stack cells on each other (Layers)\n    stacked_cell = tf.nn.rnn_cell.MultiRNNCell([single_cell for _ in range(CONFIGS.num_layers)])\n\n    return stacked_cell\n\n\n# Softmax layer\ndef rnn_softmax(CONFIGS, outputs):\n    # Variable scopes is a way to share variable among different parts of the code\n    # helps in initializing variables in one place and reuse them in different parts of code\n\n    with tf.variable_scope('rnn_softmax', reuse=True):\n        W_softmax = tf.get_variable(\"W_softmax\", [CONFIGS.num_hidden_units, CONFIGS.num_classes])\n        b_softmax = tf.get_variable(\"b_softmax\", [CONFIGS.num_classes])\n\n    logits = tf.matmul(outputs, W_softmax) + b_softmax\n\n    return logits\n\n\nclass model(object):\n\n    def __init__(self, CONFIGS):\n\n        # Placeholders\n        self.inputs_X = tf.placeholder(tf.float32, shape=[None, None, CONFIGS.num_hidden_units], name='inputs_X')\n        self.targets_y = tf.placeholder(tf.float32, shape=[None, None], name='targets_y')\n        self.seq_lens = tf.placeholder(tf.int32, shape=[None], name='seq_lens')\n        self.dropout = tf.placeholder(tf.float32)\n        self.batch_size = tf.placeholder(tf.int32, [], name='batch_size')\n        with tf.name_scope(\"rnn\"):\n            # Create folded RNN network (depth) [RNN cell * num_layers]\n            stacked_cell = rnn_cell(CONFIGS, self.dropout)\n\n\n            # Initial state is zero for each i/p batch as each input example is independent on the other\n            initial_state = stacked_cell.zero_state(self.batch_size, tf.float32)\n\n        # Unfold RNN cells in time axis\n\n        # sequence_length ->  An int32/int64 vector sized [batch_size]\n        # is used to copy-through state and zero-out outputs\n        # when past a batch element's sequence length.\n\n        # 'outputs' is a tensor of shape [batch_size, max_time, cell_state_size]\n        # 'state' is a tuple of shape [num_layers, batch_size, cell_state_size]\n        #  state[0] is the state from first RNN layer, state[-1] is the state from last RNN layer\n\n            all_outputs, state = tf.nn.dynamic_rnn(cell=stacked_cell, inputs=self.inputs_X, initial_state=initial_state,\n                                                sequence_length=self.seq_lens, dtype=tf.float32)\n\n        # Since we are using variable length inputs padded to maximum input length and we are feeding\n        # sequence_length to tf.nn.dynamic_rnn, Outputs after input seq. length will be 0, and last state will\n        # be propagated, So we can't use output[:,-1,:], instead we will use state[-1]\n            if CONFIGS.rnn_unit == 'lstm':\n                outputs = state[-1][1]\n            else:\n                outputs = state[-1]\n        # Process RNN outputs\n        with tf.variable_scope('rnn_softmax'):\n            W_softmax = tf.get_variable(\"W_softmax\", [CONFIGS.num_hidden_units, CONFIGS.num_classes])\n            b_softmax = tf.get_variable(\"b_softmax\", [CONFIGS.num_classes])\n\n        # Softmax layer\n        # logits [batch_size, num_classes]\n        with tf.name_scope(\"logits\"):\n            logits = rnn_softmax(CONFIGS, outputs)\n            # Convert logits into probabilities\n            self.probabilities = tf.nn.softmax(logits)\n\n        with tf.name_scope(\"accuracy\"):\n            # Array of boolean\n            correct_prediction = tf.equal(tf.argmax(self.targets_y, 1), tf.argmax(self.probabilities, 1))\n            # Number of correct examples / batch size\n            self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\n\n        # Loss\n        with tf.name_scope(\"loss\"):\n            # Multi-class - One label - Mutually exclusive classification, so we use\n            # softmax cross entropy cost function\n            self.loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=self.targets_y))\n\n        ####################################################################\n\n\n        # Optimization\n        with tf.name_scope(\"optimizer\"):\n            # Define learning rate (Updated each epoch)\n            self.lr = tf.Variable(0.0, trainable=False)\n            trainable_vars = tf.trainable_variables()\n\n            # clip the gradient to avoid vanishing or blowing up gradients\n            # max_gradient_norm/sqrt(add each element square))\n            grads, _ = tf.clip_by_global_norm(tf.gradients(self.loss, trainable_vars), CONFIGS.max_gradient_norm)\n            optimizer = tf.train.AdamOptimizer(self.lr)\n            self.train_optimizer = optimizer.apply_gradients(zip(grads, trainable_vars))\n\n        ####################################################################\n        # for model saving\n        with tf.name_scope(\"saver\"):\n            self.saver = tf.train.Saver(tf.global_variables())\n\n    def step(self, sess, batch_X, batch_seq_lens, batch_y=None, dropout=0.0, forward_only=True, predict=False, batch_size=1.0):\n\n        input_feed = {self.inputs_X: batch_X, self.targets_y: batch_y, self.seq_lens: batch_seq_lens,\n                      self.dropout: dropout, self.batch_size: batch_size}\n\n        if forward_only:\n            if not predict:\n                output_feed = [self.accuracy]\n            elif predict:\n                input_feed = {self.inputs_X: batch_X, self.seq_lens: batch_seq_lens,\n                              self.dropout: dropout, self.batch_size: batch_size}\n                output_feed = [self.probabilities]\n        else:  # training\n            output_feed = [self.train_optimizer, self.loss, self.accuracy]\n\n        outputs = sess.run(output_feed, input_feed)\n\n        if forward_only:\n            return outputs[0]\n        else:  # training\n            return outputs[0], outputs[1], outputs[2]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"36d7b11ae50659fce24081f94ff0495d4150f1e1"},"cell_type":"markdown","source":"**Utility functions**"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"33a0c4b07be2982039ccbc339115297edde8fdb6"},"cell_type":"code","source":"def predict(tf_model, sess, glove_model, input):\n    sentence = [w for w in input.strip().split()]\n    data = [glove_model.wv[word].tolist() if word in glove_model.wv.vocab else [0.0] * CONFIGS.num_hidden_units for word\n            in sentence]\n    seqs = len(sentence)\n    data = np.array(data)\n    data = data[np.newaxis, :, :]\n    seqs = np.array(seqs)\n    seqs = seqs[np.newaxis]\n    probabilities = tf_model.step(sess, batch_X=data,\n                               batch_seq_lens=seqs,\n                               forward_only=True, predict=True)\n    predict_class = np.asscalar(np.argmax(probabilities, 1))\n    return predict_class\n\n\ndef create_model(sess, CONFIGS):\n    text_model = model(CONFIGS)\n    print(\"Created new model.\")\n    sess.run(tf.global_variables_initializer())\n\n    return text_model\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dfb093ddba2c079bf7832c3efcd0ec87a452f2f3"},"cell_type":"markdown","source":"**Run the model and submit the results**"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"273ac42389aa8f84bc0ca4a05ddedbbf72c73562"},"cell_type":"code","source":"def run_model(X,Y):\n    tf.reset_default_graph()\n    train_X, train_y = load_data_and_labels(X,Y)\n    glove_model = load_glove_model()\n    train_seq_lens = data_to_seqs(train_X)\n    print(\"DATA IS LOADED\")\n    with tf.Session() as sess:\n        # Load old model or create new one\n        model = create_model(sess, CONFIGS)\n\n        print(\"STARTING TRAINING\")\n        print(\"----------------------\")\n\n        # Train results\n        for epoch_num, epoch in enumerate(generate_epoch(train_X, train_y, train_seq_lens,\n                                                         CONFIGS.num_epochs, CONFIGS.batch_size)):\n            print(\"EPOCH #%i started:\" % (epoch_num + 1))\n            print(\"----------------------\")\n\n            # Assign learning rate\n            sess.run(tf.assign(model.lr, CONFIGS.learning_rate *\n                               (CONFIGS.learning_rate_decay_factor ** epoch_num)))\n\n            train_loss = []\n            train_accuracy = []\n            curr_time = dt.datetime.now()\n            for batch_num, (batch_X, batch_y, batch_seq_lens) in enumerate(epoch):\n                data = data_to_vectors(batch_X, glove_model, CONFIGS.num_hidden_units)\n                _, loss, accuracy = model.step(sess, data, batch_seq_lens, batch_y, dropout=CONFIGS.dropout,\n                                                     forward_only=False, batch_size=CONFIGS.batch_size)\n\n                train_loss.append(loss)\n                train_accuracy.append(accuracy)\n                #print(\"Epoch {}, Step {}, loss: {:.3f}, accuracy: {:.3f}\".format(epoch_num+1,batch_num, loss, accuracy))\n\n\n\n            seconds = (float((dt.datetime.now() - curr_time).seconds))\n            print()\n            print(\"EPOCH #%i SUMMARY\" % (epoch_num + 1))\n            print(\"Total Average Training loss %.3f\" % np.mean(train_loss))\n            print(\"Total Average Training accuracy %.3f\" % np.mean(train_accuracy))\n            print(\"Time taken (seconds) %.3f\" % seconds)\n            print(\"----------------------\")\n        print(\"TRAINING ENDED\")\n        print(\"----------------------\")\n\n        #################################################\n        data = pd.read_csv('../input/movie-review-sentiment-analysis-kernels-only/test.tsv', sep='\\t')\n        # Test data Pre-Processing\n        predicted = []\n        for row in data['Phrase']:\n            # row = re.sub('[^a-zA-Z]', ' ',row)\n            row = row.lower()\n            row = row.split()\n            lemm = WordNetLemmatizer()\n            row = [lemm.lemmatize(w) for w in row]\n            if len(row) == 0:\n                predicted.append(2)\n            else:\n                predicted.append(predict(model, sess, glove_model, ' '.join(row)))\n\n        data['Sentiment'] = predicted\n\n        data.drop(['Phrase', 'SentenceId'], axis=1, inplace=True)\n        print(data.head(10))\n        data.to_csv('Submission.csv', header=True, index=None, sep=',')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"df5bc75de269d5892165a10332d40cd87fe8ad75","collapsed":true},"cell_type":"code","source":"run_model(X,Y)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}