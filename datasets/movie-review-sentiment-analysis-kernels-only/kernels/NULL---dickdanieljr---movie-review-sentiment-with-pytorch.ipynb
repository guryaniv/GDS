{"cells":[{"metadata":{"_uuid":"6e722dd6b2e2df5d3654893946c120c4de30ef5c"},"cell_type":"markdown","source":"### Movie review sentiment with PyTorch\nIn this kernel I want to create a rather simple LSTM bidirectional model while using ElMo, Glove and optionally some FastText."},{"metadata":{"trusted":true,"_uuid":"cd9f8e163b7a2d6936d2a805ca3837cc3327b810"},"cell_type":"code","source":"!pip install allennlp","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"!pip show torch torchtext allennlp\n!echo '\\n'\n!ls ../input","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e67ff6d54d4318458610e09826f63c0c8ded83c6"},"cell_type":"code","source":"!ls ../input/movie-review-sentiment-analysis-kernels-only","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"009372dce9365bbe7a08e6848dab134bef6ccdb1"},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nfrom tqdm import tqdm_notebook\nimport allennlp.common.tqdm as tqdm\ntqdm._tqdm = tqdm_notebook","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/movie-review-sentiment-analysis-kernels-only/train.tsv', sep=\"\\t\", dtype={\"phrase\": np.str})\ntest = pd.read_csv('../input/movie-review-sentiment-analysis-kernels-only/test.tsv', sep=\"\\t\", dtype={\"phrase\": np.str})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3d683b6bba9265804e6c4f217e66ac7cd8f1cd4c"},"cell_type":"code","source":"len(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"64cba10e8410bf7780a3dfca0494973c1170a6b7"},"cell_type":"code","source":"train.where(train['Phrase'] == \" \").dropna()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"65e6798a52c3f4b78d945e5dbe68c219cb9c8f01"},"cell_type":"code","source":"train.drop(2005, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9ea3833d8590858d6d410bca72610da31f00518e"},"cell_type":"code","source":"train_len = train.shape[0]\nprint(\"len of train =\", train_len)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"96aa8ee519aa12f072bc8e67c44de5cf0ddd1688"},"cell_type":"code","source":"print('Average count of phrases per sentence in train is {0:.0f}.'.format(train.groupby('SentenceId')['Phrase'].count().mean()))\nprint('Average count of phrases per sentence in test is {0:.0f}.'.format(test.groupby('SentenceId')['Phrase'].count().mean()))\nprint('Number of phrases in train: {}. Number of sentences in train: {}.'.format(train.shape[0], len(train.SentenceId.unique())))\nprint('Number of phrases in test: {}. Number of sentences in test: {}.'.format(test.shape[0], len(test.SentenceId.unique())))\nprint('Average word length of phrases in train is {0:.0f}.'.format(np.mean(train['Phrase'].apply(lambda x: len(x.split())))))\nprint('Average word length of phrases in test is {0:.0f}.'.format(np.mean(test['Phrase'].apply(lambda x: len(x.split())))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ce2d950d983e9068f7a30c9c6905d31c57c63622"},"cell_type":"code","source":"#idx_separator = 124805\nidx_separator = int(len(train) * 0.8)\nidxs = np.random.permutation(train.shape[0])\ntrain_idxs = idxs[:idx_separator]\nval_idxs = idxs[idx_separator:]\ntrain_df = train.iloc[train_idxs,:]\nval_df = train.iloc[val_idxs,:]\n\nprint(\"train_df len =\", len(train_df))\nprint(\"val_df len =\", len(val_df))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"61959776e01d38cb2b3d3c63a9e293652b2c703e"},"cell_type":"code","source":"train_df.to_csv(\"../input/train.csv\")\nval_df.to_csv(\"../input/val.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1d0fb3722c281ce06b7a080c373335dd1843194d"},"cell_type":"code","source":"!ls ../input","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9c3d1582a5ae79aece8c39f39a3a82dbf6081dac"},"cell_type":"markdown","source":"#### Dataset\nPreparing the dataset for AllenNLP."},{"metadata":{"trusted":true,"_uuid":"8148a25fefed7255d83ceb89d961812a8ab66cca"},"cell_type":"code","source":"from typing import Dict\n\nfrom allennlp.common.file_utils import cached_path\nfrom allennlp.data.dataset_readers.dataset_reader import DatasetReader\nfrom allennlp.data.instance import Instance\nfrom allennlp.data.token_indexers import SingleIdTokenIndexer, TokenIndexer, ELMoTokenCharactersIndexer, TokenCharactersIndexer\nfrom allennlp.data.tokenizers import Token, Tokenizer, WordTokenizer, CharacterTokenizer\nfrom allennlp.data.fields import *\nfrom allennlp.data import Vocabulary","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f55e08549277af52ebe86580714ffbf51dc1ae78"},"cell_type":"code","source":"class MRDatasetReader(DatasetReader):\n    def __init__(self,\n             tokenizer: Tokenizer = None,\n             token_indexers: Dict[str, TokenIndexer] = None,\n             lazy: bool = False) -> None:\n        super().__init__(lazy)\n\n        self._tokenizer = tokenizer or WordTokenizer()\n        self._token_indexers = token_indexers or {'tokens': SingleIdTokenIndexer()}\n    \n    def _read(self, file_path):\n        with open(cached_path(file_path), \"r\") as data_file:\n            df = pd.read_csv(file_path, dtype={\"PhraseId\": np.int, \"Phrase\": np.str, \"Sentiment\": np.int8, \"SentenceId\": np.int})\n            #print(\"count = \", len(train.where(train['Sentiment'] == 0).dropna()))\n            for i, item in df.iterrows():\n                phrase_id = item[\"PhraseId\"]\n                sentence_id = item[\"SentenceId\"]\n                phrase = item[\"Phrase\"]\n                sentiment = item[\"Sentiment\"]\n                yield self.text_to_instance(phrase_id, sentence_id, phrase, sentiment)\n            \n    def text_to_instance(self, phrase_id, sentence_id, phrase, sentiment) -> Instance:\n        tokenized_phrase = self._tokenizer.tokenize(phrase)\n        \n        phrase_field = TextField(tokenized_phrase, self._token_indexers)\n        #phrase_id_field = MetadataField(phrase_id)\n        #sentence_id_field = MetadataField(sentence_id)\n        fields = {\n            \"phrase\": phrase_field\n        }\n        \n        #print(f\"sentiment = {sentiment} | sentiment-1 = {sentiment-1}\")\n        fields[\"labels\"] = LabelField(str(int(sentiment)))\n        #fields[\"labels\"] = LabelField(sentiment, skip_indexing=True)\n        \n        return Instance(fields)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3b70843336907d2651fb2709b5e4800050e3c175"},"cell_type":"markdown","source":"We use glove and Elmo as our embeddings for now."},{"metadata":{"trusted":true,"_uuid":"f1ce18cf6daa6cab050ce480596129997a0430c7"},"cell_type":"code","source":"glove_pretrained_embedding = \"https://s3-us-west-2.amazonaws.com/allennlp/datasets/glove/glove.6B.100d.txt.gz\"\noptions_file = \"https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_options.json\"\nweight_file = \"https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_weights.hdf5\"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5f749bdd021f3c3f2a44af79b652efe35be89054"},"cell_type":"markdown","source":"#### Model\nCreating the model."},{"metadata":{"trusted":true,"_uuid":"fbe32f6058c8da627e156160c7b50d0234f3559a"},"cell_type":"code","source":"import torch\nfrom torch import nn\nfrom torch.nn import functional as F\nimport torch.optim as optim\n\nfrom allennlp.models.model import Model\nfrom allennlp.modules import Seq2SeqEncoder, SimilarityFunction, TimeDistributed, TextFieldEmbedder\nfrom allennlp.modules.token_embedders import Embedding, TokenCharactersEncoder, ElmoTokenEmbedder\nfrom allennlp.modules.text_field_embedders import BasicTextFieldEmbedder\nfrom allennlp.modules.seq2vec_encoders.cnn_encoder import CnnEncoder\nfrom allennlp.modules import Highway\nfrom allennlp.modules.elmo import Elmo\nfrom allennlp.nn.util import get_text_field_mask, sequence_cross_entropy_with_logits\nfrom allennlp.modules.seq2seq_encoders.pytorch_seq2seq_wrapper import PytorchSeq2SeqWrapper\nfrom allennlp.modules.seq2vec_encoders import *\nfrom allennlp.training.metrics import BooleanAccuracy, CategoricalAccuracy\nfrom allennlp.data.iterators import *\nfrom allennlp.training.trainer import Trainer\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom allennlp.training.learning_rate_schedulers import LearningRateWithMetricsWrapper\nfrom allennlp.modules.token_embedders.embedding import _read_pretrained_embeddings_file","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"041c753b01f12663af551ab06c0516342d485379"},"cell_type":"code","source":"class MRModel(Model):\n    def __init__(self, vocab, text_field_embedder, input_size, hidden_size, dropout = 0.0):\n        super().__init__(vocab)\n        \n        self.text_field_embedder = text_field_embedder\n        \n        lstm1 = nn.LSTM(input_size=input_size, hidden_size=HIDDEN_DIM, num_layers=1, bidirectional=True, batch_first=True)\n        self.lstm1 = PytorchSeq2VecWrapper(lstm1)\n        \n        #self.lin1 = nn.Linear(in_features=self.lstm1.get_output_dim(), out_features=50)        \n        self.lin1 = nn.Linear(in_features=self.lstm1.get_output_dim(), out_features=5)\n        \n        self.loss = nn.CrossEntropyLoss()\n        self.accuracy = CategoricalAccuracy()\n        \n        if dropout > 0:\n            self.dropout = nn.Dropout(p=dropout)\n        else:\n            self.dropout = lambda x: x\n        \n    def forward(self, \n                phrase: Dict[str, torch.LongTensor],\n                labels: torch.LongTensor = None):\n        mask = get_text_field_mask(phrase)\n        embedded_phrase = self.text_field_embedder(phrase)\n        encoded_phrase = self.lstm1(embedded_phrase, mask)\n    \n        x = self.dropout(F.relu(self.lin1(encoded_phrase)))\n        \n        tag_logits = F.softmax(x, dim=1)\n\n        output = {\"tag_logits\": tag_logits}\n\n        if labels is not None:\n            self.accuracy(tag_logits, labels)\n            output[\"loss\"] = self.loss(tag_logits, labels)\n\n        return output\n    \n    def get_metrics(self, reset: bool = False) -> Dict[str, float]:\n        return {\"accuracy\": self.accuracy.get_metric(reset)}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d45806dc107e1e88d985832c4468d333476d4a7a"},"cell_type":"code","source":"token_indexers = { \n    \"tokens\": SingleIdTokenIndexer(lowercase_tokens=True),\n    \"elmo\": ELMoTokenCharactersIndexer(namespace=\"elmo\"),\n    \"token_characters\": TokenCharactersIndexer(character_tokenizer=CharacterTokenizer(byte_encoding=\"utf-8\", start_tokens=[259], end_tokens=[260]))\n}\n\nreader = MRDatasetReader(token_indexers=token_indexers)\n\ntrain_dataset = reader.read(\"../input/train.csv\")\nval_dataset = reader.read(\"../input/val.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7eb555b62c5ff760c9ba3a2d882d51418e40e2db"},"cell_type":"code","source":"vocab = Vocabulary.from_instances(train_dataset + val_dataset)\ntokens_token_embedder_weight = _read_pretrained_embeddings_file(file_uri=glove_pretrained_embedding, embedding_dim=100, vocab=vocab)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eae432f2ed4523d37166236d4ca8c3157b43c6e1"},"cell_type":"code","source":"EMBEDDING_DIM = 100\nHIDDEN_DIM = 100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e0a90077f0709edf19c349f5aa0de434caeed6a0"},"cell_type":"code","source":"tokens_token_embedder = Embedding(embedding_dim=100, trainable=False, weight=tokens_token_embedder_weight, num_embeddings=vocab.get_vocab_size('tokens'))\nelmo_token_embedder = ElmoTokenEmbedder(options_file=options_file, weight_file=weight_file, do_layer_norm=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0076a14d66c85e1376e4586efebf93567cd9d4d7"},"cell_type":"code","source":"token_characters = TokenCharactersEncoder(embedding=Embedding(embedding_dim=EMBEDDING_DIM, num_embeddings=vocab.get_vocab_size('tokens')), \n                                          encoder=CnnEncoder(embedding_dim=EMBEDDING_DIM, num_filters=50, ngram_filter_sizes=[4,5]), \n                                          dropout=0.2)\n\ntoken_embedders_config = {\n    \"tokens\": tokens_token_embedder,\n    \"elmo\": elmo_token_embedder,\n    \"token_characters\": token_characters\n}\n\ntf_embedder = BasicTextFieldEmbedder(token_embedders=token_embedders_config)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"07c816e85f5051ab21ff20036723a6e35b9cf84d"},"cell_type":"code","source":"model = MRModel(vocab, tf_embedder, input_size=1224, hidden_size=HIDDEN_DIM, dropout=0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"625c4c7b3580750304f82f33efb7bfedcfca35da"},"cell_type":"code","source":"lr = 1e-3\noptimizer = optim.Adam(model.parameters(), lr=lr)\n\niterator = BucketIterator(batch_size=256, sorting_keys=[(\"phrase\", \"num_tokens\")])\niterator.index_with(vocab)\n\nlearning_rate_scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=2)\n\ntrainer = Trainer(model=model,\n                  optimizer=optimizer,\n                  iterator=iterator,\n                  train_dataset=train_dataset,\n                  validation_dataset=val_dataset,\n                  patience=3,\n                  shuffle=True,\n                  num_epochs=5,\n                  cuda_device=0,\n                  learning_rate_scheduler=LearningRateWithMetricsWrapper(learning_rate_scheduler))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"368a557143cd1e9bd7f97a4c94ff1e70168d7141","scrolled":false},"cell_type":"code","source":"trainer.train()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b3483127ccd45791a4e5b73b4d12bf81a31d3e59"},"cell_type":"markdown","source":"## Notes\n* LR: higher than 1e-3 is bad\n* Dropout: 0.2 seems alright, has strong effect on train accuracy, while val accuracy seems to stay around the same (compared to 0.0 dropout)\n* Batch size?\n* Embedding: increasing i.e. from 16 to 100 results in a much worse val accuracy in epoch 1, from 0.6 to 0.5, when not adjusting CnnEncoder\n* CnnEncoder: has positive effect as filter size/layers are increased, but only when the Embedding is big enough (I set it to 100 I think)\n* Hidden size?"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}