{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","collapsed":true,"trusted":false},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom keras.preprocessing import text, sequence\nfrom keras.layers import GRU, Embedding, Dense, Input\nfrom keras.models import Model\nfrom sklearn.model_selection import train_test_split\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"trusted":false},"cell_type":"code","source":"train = pd.read_csv(\"../input/jigsaw-toxic-comment-classification-challenge/train.csv\")\ntest = pd.read_csv(\"../input/jigsaw-toxic-comment-classification-challenge/test.csv\")\n\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"602faa61-83bd-463f-a221-692b760809c7","_uuid":"9bb33ed2eaf58bdb48ad1e85d5467c9bd8887bcb","collapsed":true,"trusted":false},"cell_type":"code","source":"# Baseline Keras model\n# 1) padded sequences \n# 2) FastText Web Crawl embedding \n# 3) single GRU layer. \n\nmax_features = 10000\nmaxlen = 50\n\nX_train = train.comment_text.fillna('na').values\nX_test = test.comment_text.fillna('na').values\ny_train = train[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]].values\n\ntokenizer = text.Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(X_train) + list(X_test))\nX_train_sequence = tokenizer.texts_to_sequences(X_train)\nX_test_sequence = tokenizer.texts_to_sequences(X_test)\nx_train = sequence.pad_sequences(X_train_sequence, maxlen=maxlen)\nx_test = sequence.pad_sequences(X_test_sequence, maxlen=maxlen)\nprint(len(tokenizer.word_index))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"8d3b0728-bc6c-46e9-96ea-bf216d4c1891","_uuid":"8fe8a034e5f0da11c1f3af5afe8b080e624e9c95","collapsed":true,"trusted":false},"cell_type":"code","source":"# Load the FastText Web Crawl vectors\nEMBEDDING_FILE_FASTTEXT=\"../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec\"\ndef get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')\nembeddings_index_ft = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(EMBEDDING_FILE_FASTTEXT,encoding='utf-8'))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c5cb1175-0d64-40aa-ba8e-844478cc0f54","_uuid":"38a44f6c963283dac6d1cefc46d186b4c301b568","collapsed":true,"trusted":false},"cell_type":"code","source":"embed_size=300\nword_index = tokenizer.word_index\nnb_words = min(max_features, len(word_index))\nembedding_matrix = np.zeros((nb_words, embed_size))\nfor word, i in word_index.items():\n    if i >= max_features: continue\n    embedding_vector = embeddings_index_ft.get(word)\n    if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n    \n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"cbc38b37-1c92-4e69-b444-4e1c6ed90a09","_uuid":"54e58c905ec4916f7f9a492d823f06f4d4b76229","collapsed":true,"trusted":false},"cell_type":"code","source":"# Baseline Model:  padded sequences (max_features: 10,000, maxlen: 50, FastText Web Crawl, Embed Size=300, 1 GRU layer, num_filters=40)\n# Result:(Private: 9769, Public: 9775)\n# Uncomment to run baseline.\ndef get_model(maxlen, max_features, embed_size, num_filters=40):\n    inp = Input(shape=(maxlen, ))\n\n    x = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=False)(inp)\n    \n    x = GRU(num_filters, )(x)\n    \n    outp = Dense(6, activation=\"sigmoid\")(x)\n\n    model = Model(inputs=inp, outputs=outp)\n    \n    model.compile(loss='binary_crossentropy',\n                  optimizer='adam',\n                  metrics=['accuracy'])\n    return model","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"92ae4702-5448-40c9-aff8-8eb75e3ed2a6","_uuid":"912f2dbeb7e6acd01b748a5933af1a8ebe85eba4","trusted":false,"collapsed":true},"cell_type":"code","source":"# Avg Pooling Model:  padded sequences \n# Result:(Private: 9769, Public: 9775)\n# Uncomment to run baseline.\nfrom keras.layers import GlobalAveragePooling1D\n\ndef get_model2(maxlen, max_features, embed_size, num_filters=40):\n    inp = Input(shape=(maxlen, ))\n\n    x = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=False)(inp)\n    \n    x = GRU(num_filters, return_sequences=True)(x)\n    \n    avg_pool = GlobalAveragePooling1D()(x)\n    max_pool = GlobalMaxPooling1D()(x)\n    conc = concatenate([avg_pool, max_pool])\n    \n    outp = Dense(6, activation=\"sigmoid\")(conc)\n\n    model = Model(inputs=inp, outputs=outp)\n    \n    model.compile(loss='binary_crossentropy',\n                  optimizer='adam',\n                  metrics=['accuracy'])\n    return model","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"663b6d15-7071-483c-a58c-d13c1bfe8fb8","_uuid":"518f6fd237f88d8c15368f2b70f17e1cd2f87445","collapsed":true,"trusted":false},"cell_type":"code","source":"\n\n\nmodel = get_model2(maxlen, max_features, embed_size)\n\nbatch_size = 32\nepochs = 2\n\nX_tra, X_val, y_tra, y_val = train_test_split(x_train, y_train, train_size=0.95, random_state=233)\n\nmodel.fit(X_tra, y_tra, batch_size=batch_size, epochs=epochs, validation_data=(X_val, y_val), verbose=1)\n\ny_pred = model.predict(x_test, batch_size=batch_size,verbose=1)\n\nsample_submission = pd.read_csv(\"../input/jigsaw-toxic-comment-classification-challenge/sample_submission.csv\")\nclass_names = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\nsample_submission[class_names] = y_pred\n\nsample_submission.to_csv('pooled_submission.csv',index=False)\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b808f6c7-6044-4bf4-91cc-f994ad8a90fc","_uuid":"9bc974966e3f8992df63bf23d7529dc76bb0aa24","collapsed":true,"trusted":false},"cell_type":"code","source":"# Improvement #1: Preprocessing.  Clean up data before padding and vectorizing.\n#\n# I am using the output from xbf: Processing helps boosting about 0.0005 on LB\n\n'''\ntrain = pd.read_csv(\"../input/processing-helps-boosting-about-0-0005-on-lb/train_processed.csv\")\ntest = pd.read_csv(\"../input/processing-helps-boosting-about-0-0005-on-lb/test_processed.csv\")\n\nX_train = train.comment_text.fillna('na').values\nX_test = test.comment_text.fillna('na').values\ny_train = train[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]].values\n\ntokenizer = text.Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(X_train) + list(X_test))\nX_train_sequence = tokenizer.texts_to_sequences(X_train)\nX_test_sequence = tokenizer.texts_to_sequences(X_test)\nx_train = sequence.pad_sequences(X_train_sequence, maxlen=maxlen)\nx_test = sequence.pad_sequences(X_test_sequence, maxlen=maxlen)\nprint(len(tokenizer.word_index))\n\nembed_size=300\nword_index = tokenizer.word_index\nnb_words = min(max_features, len(word_index))\nembedding_matrix = np.zeros((nb_words, embed_size))\nfor word, i in word_index.items():\n    if i >= max_features: continue\n    embedding_vector = embeddings_index_ft.get(word)\n    if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n        \nmodel = get_model(maxlen, max_features, embed_size)\n\nbatch_size = 32\nepochs = \n\nX_tra, X_val, y_tra, y_val = train_test_split(x_train, y_train, train_size=0.95, random_state=233)\n\nmodel.fit(X_tra, y_tra, batch_size=batch_size, epochs=epochs, validation_data=(X_val, y_val), verbose=1)\n\ny_pred = model.predict(x_test, batch_size=batch_size,verbose=1)\n\nsample_submission = pd.read_csv(\"../input/jigsaw-toxic-comment-classification-challenge/sample_submission.csv\")\nclass_names = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\nsample_submission[class_names] = y_pred\n\nsample_submission.to_csv('preprocess_submission.csv',index=False)\n'''","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"8e86a51e-bdcc-4e08-89ae-53389da2be07","_uuid":"4b227fd934e1b5fa64a3739531d55275077f4997","collapsed":true,"trusted":false},"cell_type":"code","source":"# Improvement #2:  Run until roc auc score stops increasing using a callback\n# result:  (Private: 9766, Public: 9783)\n\nfrom keras.callbacks import Callback\nfrom sklearn.metrics import roc_auc_score\n\n'''\nclass RocAucEvaluation(Callback):\n    def __init__(self, validation_data=(), interval=1):\n        super(Callback, self).__init__()\n\n        self.interval = interval\n        self.X_val, self.y_val = validation_data\n        self.max_score = 0\n        self.not_lower_count = 0\n\n    def on_epoch_end(self, epoch, logs={}):\n        if epoch % self.interval == 0:\n            y_pred = self.model.predict(self.X_val, verbose=1)\n            score = roc_auc_score(self.y_val, y_pred)\n            print(\"\\n ROC-AUC - epoch: %d - score: %.6f \\n\" % (epoch+1, score))\n            if (score > self.max_score):\n                print(\"*** New High Score (previous: %.6f) \\n\" % self.max_score)\n                model.save_weights(\"best_weights.h5\")\n                self.max_score=score\n                self.not_lower_count = 0\n            else:\n                self.not_lower_count += 1\n                # in my code, I use 3 instead of 2.\n                if self.not_lower_count > 2:\n                    print(\"Epoch %05d: early stopping, high score = %.6f\" % (epoch,self.max_score))\n                    self.model.stop_training = True\n\nmodel = get_model(maxlen, max_features, embed_size)\n\nbatch_size = 32\nepochs = 100\n\nX_tra, X_val, y_tra, y_val = train_test_split(x_train, y_train, train_size=0.95, random_state=233)\n\nRocAuc = RocAucEvaluation(validation_data=(X_val, y_val), interval=1)\n\nmodel.fit(X_tra, y_tra, batch_size=batch_size, epochs=epochs, validation_data=(X_val, y_val), callbacks=[RocAuc], verbose=1)\n\nmodel.load_weights(\"best_weights.h5\")\n\ny_pred = model.predict(x_test, batch_size=batch_size,verbose=1)\n\nsample_submission = pd.read_csv(\"../input/jigsaw-toxic-comment-classification-challenge/sample_submission.csv\")\nclass_names = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\nsample_submission[class_names] = y_pred\n\nsample_submission.to_csv('improvement1_submission.csv',index=False,)\n'''","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"52b9b444-4d2a-4507-9965-224ae88d52fb","_uuid":"f269d6206590f6f40b6a6524d411b5b579546f97","collapsed":true,"trusted":false},"cell_type":"code","source":"\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"file_extension":".py","codemirror_mode":{"version":3,"name":"ipython"},"nbconvert_exporter":"python","mimetype":"text/x-python","version":"3.6.4","name":"python","pygments_lexer":"ipython3"}},"nbformat":4,"nbformat_minor":1}