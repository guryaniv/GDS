{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"#import the required libraries\nimport pandas as pd\n\nimport numpy as np\n\nimport matplotlib.pyplot as plt\n\nimport seaborn as sns\n\nfrom tqdm import tqdm\nfrom tqdm import tqdm_notebook\n\nfrom scipy.signal import hilbert\nfrom scipy.signal import hann\nfrom scipy.signal import convolve","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c93caa3cde2baaa2a606e74eda4ad1ac27c2d647"},"cell_type":"code","source":"train_data = pd.read_csv('../input/train.csv', dtype = {'acoustic_data': np.int16, 'time_to_failure': np.float32})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ae55f35acd70c6393af4bd3a01128f6a52a7500a"},"cell_type":"code","source":"#let's look at the training data\nprint(train_data.head())\nprint(100*'-')\nprint(train_data.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"acab4844ca941c8b6daf1510cf0ac84e0bef9371"},"cell_type":"code","source":"#let's prepare our data\nseg_length = 150000\ntotal_samples = int(np.floor((train_data.shape[0]) / seg_length))\n\nx_train = pd.DataFrame(index = range(total_samples), dtype = np.float64) #an empty dataframe holding our feature values\ny_train = pd.DataFrame(index = range(total_samples), columns = ['time_to_failure'], dtype = np.float64) #an empty dataframe holding our target labels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8ab3ead05a66c2db72094438192387413549979e","_kg_hide-output":true,"_kg_hide-input":false},"cell_type":"code","source":"def calc_change_rate(x):\n    change = (np.diff(x) / x[:-1]).values\n    change = change[np.nonzero(change)[0]]\n    change = change[~np.isnan(change)]\n    change = change[change != -np.inf]\n    change = change[change != np.inf]\n    return np.mean(change)\n\nfor value in tqdm_notebook(range(total_samples)):\n    sample = train_data.iloc[value*seg_length : value*seg_length + seg_length]\n    x = pd.Series(sample['acoustic_data'].values)\n    y = sample['time_to_failure'].values[-1]\n    y_train.loc[value, 'time_to_failure'] = y\n    \n    x_train.loc[value, 'AVERAGE'] = x.mean()\n    x_train.loc[value, 'STD'] = x.std()\n    x_train.loc[value, 'MAX'] = x.max()\n    x_train.loc[value, 'MIN'] = x.min() \n    x_train.loc[value, 'SUM'] = x.sum()\n    \n    x_train.loc[value, 'MEAN_CHANGE_ABS'] = np.mean(np.diff(x))\n    x_train.loc[value, 'MEAN_CHANGE_RATE'] = calc_change_rate(x)\n    \n    x_train.loc[value, 'MAX_TO_MIN'] = x.max() / np.abs(x.min())\n    x_train.loc[value, 'MAX_TO_MIN_DIFF'] = x.max() - np.abs(x.min())\n    x_train.loc[value, 'COUNT_BIG'] = len(x[np.abs(x) > 500])\n    \n    x_train.loc[value, 'AVERAGE_FIRST_10000'] = x[:10000].mean()\n    x_train.loc[value, 'AVERAGE_LAST_10000']  =  x[-10000:].mean()\n    x_train.loc[value, 'AVERAGE_FIRST_50000'] = x[:50000].mean()\n    x_train.loc[value, 'AVERAGE_LAST_50000'] = x[-50000:].mean()\n    \n    x_train.loc[value, 'STD_FIRST_10000'] = x[:10000].std()\n    x_train.loc[value, 'STD_LAST_10000']  =  x[-10000:].std()\n    x_train.loc[value, 'STD_FIRST_50000'] = x[:50000].std()\n    x_train.loc[value, 'STD_LAST_50000'] = x[-50000:].std()\n    \n    x_train.loc[value, 'ABS_AVERAGE'] = np.abs(x).mean()\n    x_train.loc[value, 'ABS_STD'] = np.abs(x).std()\n    x_train.loc[value, 'ABS_MAX'] = np.abs(x).max()\n    x_train.loc[value, 'ABS_MIN'] = np.abs(x).min()\n    \n    x_train.loc[value, '10Q'] = np.percentile(x, 0.10)\n    x_train.loc[value, '25Q'] = np.percentile(x, 0.25)\n    x_train.loc[value, '50Q'] = np.percentile(x, 0.50)\n    x_train.loc[value, '75Q'] = np.percentile(x, 0.75)\n    x_train.loc[value, '90Q'] = np.percentile(x, 0.90)\n    \n    x_train.loc[value, 'ABS_1Q'] = np.percentile(x, np.abs(0.01))\n    x_train.loc[value, 'ABS_5Q'] = np.percentile(x, np.abs(0.05))\n    x_train.loc[value, 'ABS_30Q'] = np.percentile(x, np.abs(0.30))\n    x_train.loc[value, 'ABS_60Q'] = np.percentile(x, np.abs(0.60))\n    x_train.loc[value, 'ABS_95Q'] = np.percentile(x, np.abs(0.95))\n    x_train.loc[value, 'ABS_99Q'] = np.percentile(x, np.abs(0.99))\n    \n    x_train.loc[value, 'KURTOSIS'] = x.kurtosis()\n    x_train.loc[value, 'SKEW'] = x.skew()\n    x_train.loc[value, 'MEDIAN'] = x.median()\n    \n    x_train.loc[value, 'HILBERT_MEAN'] = np.abs(hilbert(x)).mean()\n    x_train.loc[value, 'HANN_WINDOW_MEAN'] = (convolve(x, hann(150), mode = 'same') / sum(hann(150))).mean()\n    \n    for windows in [10, 100, 1000]:\n        x_roll_std = x.rolling(windows).std().dropna().values\n        x_roll_mean = x.rolling(windows).mean().dropna().values\n        \n        x_train.loc[value, 'AVG_ROLL_STD' + str(windows)] = x_roll_std.mean()\n        x_train.loc[value, 'STD_ROLL_STD' + str(windows)] = x_roll_std.std()\n        x_train.loc[value, 'MAX_ROLL_STD' + str(windows)] = x_roll_std.max()\n        x_train.loc[value, 'MIN_ROLL_STD' + str(windows)] = x_roll_std.min()\n        x_train.loc[value, '1Q_ROLL_STD' + str(windows)] = np.quantile(x_roll_std, 0.01)\n        x_train.loc[value, '5Q_ROLL_STD' + str(windows)] = np.quantile(x_roll_std, 0.05)\n        x_train.loc[value, '95Q_ROLL_STD' + str(windows)] = np.quantile(x_roll_std, 0.95)\n        x_train.loc[value, '99Q_ROLL_STD' + str(windows)] = np.quantile(x_roll_std, 0.99)\n        x_train.loc[value, 'AV_CHANGE_ABS_ROLL_STD' + str(windows)] = np.mean(np.diff(x_roll_std))\n        x_train.loc[value, 'ABS_MAX_ROLL_STD' + str(windows)] = np.abs(x_roll_std).max()\n        \n        x_train.loc[value, 'AVG_ROLL_MEAN' + str(windows)] = x_roll_mean.mean()\n        x_train.loc[value, 'STD_ROLL_MEAN' + str(windows)] = x_roll_mean.std()\n        x_train.loc[value, 'MAX_ROLL_MEAN' + str(windows)] = x_roll_mean.max()\n        x_train.loc[value, 'MIN_ROLL_MEAN' + str(windows)] = x_roll_mean.min()\n        x_train.loc[value, '1Q_ROLL_MEAN' + str(windows)] = np.quantile(x_roll_mean, 0.01)\n        x_train.loc[value, '5Q_ROLL_MEAN' + str(windows)] = np.quantile(x_roll_mean, 0.05)\n        x_train.loc[value, '95Q_ROLL_MEAN' + str(windows)] = np.quantile(x_roll_mean, 0.95)\n        x_train.loc[value, '99Q_ROLL_MEAN' + str(windows)] = np.quantile(x_roll_mean, 0.99)\n        x_train.loc[value, 'AV_CHANGE_ABS_ROLL_MEAN' + str(windows)] = np.mean(np.diff(x_roll_mean))\n        x_train.loc[value, 'ABS_MAX_ROLL_MEAN' + str(windows)] = np.abs(x_roll_mean).max()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e12b421aabd4d0f767f33826d67e4f579eb44757"},"cell_type":"code","source":"x_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fe610246f4db9d8295736719f546e46b4b0f61eb"},"cell_type":"code","source":"y_train.head(10) #our training dataframe holding our output labels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7559c273223d24c7ac8431fbac275bac66cad294"},"cell_type":"code","source":"y_train.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"76b0ca7f1f10204429da948a85871a62447a6578"},"cell_type":"code","source":"print(x_train.shape)\nprint(y_train.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1412dd3f7bf3a12be1c7a8bf1248d401ec23adc5"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_absolute_error","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"caa85cbf4e743f12381fcd08c7e5be070e2c28e9"},"cell_type":"code","source":"#normalizing the data\nscaler = StandardScaler()\nscaler.fit(x_train)\nx_train_scaled = scaler.transform(x_train)\n\ny_train_flatten = y_train.values.ravel() #flattening the y_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"10d45169a0344468e06bd5d61f9a197c252450e6"},"cell_type":"code","source":"#let's look at the mormalized data\nx_train_dataframe = pd.DataFrame(x_train_scaled)\nx_train_dataframe.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"69eb71c8d17bbc8593e12ffdba5158a8c0973a93"},"cell_type":"code","source":"#splitting the data into train and test\nx_train1, x_test1, y_train1, y_test1 = train_test_split(x_train, y_train, test_size = 0.2, random_state = 101)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2a8b875f45998753417d1592d68cf9fdb192bef4"},"cell_type":"code","source":"#normalizing the data\nscaler = StandardScaler()\nscaler.fit(x_train1)\nx_train_scaled1 = scaler.transform(x_train1)\nx_test_scaled1 = scaler.transform(x_test1)\n\ny_train_flatten1 = y_train1.values.ravel() #flattening the y_train\ny_test_flatten1 = y_test1.values.ravel() #flattening the y_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8758de879f5623ede2538ab50fd5e24955d8c1e6"},"cell_type":"code","source":"print(x_train_scaled1.shape)\nprint(x_test_scaled1.shape)\nprint(y_train_flatten1.shape)\nprint(y_test_flatten1.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b822d5344a7aefeea55836e697ea7c332482eca4"},"cell_type":"markdown","source":"**Here we will be using two types of neural networks. The first network is a simple feed forward neural network with the following architecture:**\n\n\n**1. BLOCK1: ((DENSE(32 filters) ---> ACTIVATION ---> BN) x 2) -----> DROPOUT**\n\n**2. BLOCK2 : ((DENSE(32 filters) ---> ACTIVATION ---> BN) x 2) -----> DROPOUT**\n\n**3. BLOCK3: ((DENSE(32 filters) ---> ACTIVATION ---> BN) x 2) -----> DROPOUT**\n\n**4. BLOCK4(output layer): (DENSE(classes) ---> ACTIVATION 2)**\n\n**The second type of the network will also be a feed forward neural net but unlike in the first one where we trained our dataset on a single neural net, here we will be training our dataset on multiplt neural networks. Each neural net will have its own output scores. The mean of all the individual scores will be considered to be the final otuput score. This type of model is called as Ensemble Model. Ensemble learning ususally helps us to achieve high performance of the model on our input data, hence such models are generally used instead of a sinlge model of neural network.**\n"},{"metadata":{"_uuid":"159cff929e01190ddd2bdebae4652d3f086c272b"},"cell_type":"markdown","source":"## SIMPLE NEURAL NETWORK"},{"metadata":{"trusted":true,"_uuid":"9b5e00977b12f97ccc08981a1449ba4b58c99123"},"cell_type":"code","source":"#import the libraries for building the neural net\nimport keras\nfrom keras.layers import Dense\nfrom keras.layers import BatchNormalization\nfrom keras.layers import Activation\nfrom keras.layers import Dropout\nfrom keras.models import Sequential\nfrom keras import optimizers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e6056e773e7794f0908bb56e2795024b10e6c1c3"},"cell_type":"code","source":"kernel_init = 'normal'\ndef Model(input_dim, activation, classes):\n    model = Sequential()\n\n    model.add(Dense(1024, kernel_initializer = kernel_init, input_dim = input_dim))\n    model.add(Activation(activation))\n    model.add(BatchNormalization())\n#     model.add(Dense(1024, kernel_initializer = kernel_init, input_dim = input_dim))\n#     model.add(Activation(activation))\n#     model.add(BatchNormalization())\n    model.add(Dropout(0.3))\n\n    model.add(Dense(512, kernel_initializer = kernel_init)) \n    model.add(Activation(activation))\n    model.add(BatchNormalization())\n#     model.add(Dense(512, kernel_initializer = kernel_init)) \n#     model.add(Activation(activation))\n#     model.add(BatchNormalization())\n    model.add(Dropout(0.3))\n\n    model.add(Dense(64, kernel_initializer = kernel_init))    \n    model.add(Activation(activation))\n    model.add(BatchNormalization())\n#     model.add(Dense(64, kernel_initializer = kernel_init))    \n#     model.add(Activation(activation))\n#     model.add(BatchNormalization())\n    model.add(Dropout(0.3))\n    \n#     model.add(Dense(32, kernel_initializer = kernel_init))    \n#     model.add(Activation(activation))\n#     model.add(BatchNormalization())\n#     model.add(Dense(32, kernel_initializer = kernel_init))    \n#     model.add(Activation(activation))\n#     model.add(BatchNormalization())\n#     model.add(Dropout(0.2))\n\n    model.add(Dense(classes, kernel_initializer = kernel_init))    \n    model.add(Activation('linear'))\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ea9c478c62997f9698e794e8c04ac73ae547acaa"},"cell_type":"code","source":"input_dim = x_train.shape[1]\nactivation = 'tanh'\nclasses = 1 #the output labels\nmodel = Model(input_dim = input_dim, activation = activation, classes = classes)\n\n#model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"700b52f38755533167441df311cc85c339372d54"},"cell_type":"code","source":"#compile the model\noptim = optimizers.Adam(lr = 0.001, decay = 0.001 / 32)\nmodel.compile(loss = 'mean_absolute_error', optimizer = optim, metrics = ['mean_absolute_error'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"4c575782c53a9d61114a954814b80b7aba4d8978"},"cell_type":"code","source":"#fit the model onto the dataset\nh = model.fit(x_train_scaled1, y_train_flatten1, epochs = 100, batch_size = 32, verbose = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"01a5f8f21a3fcec2d7e70ce770a95dedd9c98430"},"cell_type":"code","source":"#making predictions\nprediction_score = []\nmae_scores = []\npredictions = model.predict(x_test_scaled1, verbose = 1, batch_size = 32) #make predictons on the test data \nprediction_score.append(predictions)\n\nmae = mean_absolute_error(predictions, y_test_flatten1) #calculating MAE between our actual output labels and the predicted otuput labels\nmae_scores.append(mae)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e6d6b006a682b327d86a2d976465331f0ec66643"},"cell_type":"code","source":"print(np.mean(mae_scores)) #average of mean absolute errors","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"5c7c0e8273497b126e1bba801de9138d3772f27b"},"cell_type":"code","source":"predictions = np.hstack([p.reshape(-1,1) for p in prediction_score])\npredictions = np.mean(predictions, axis = 1)\nprint(mean_absolute_error(predictions, y_test1))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2d943cfcd1dc05df007aec42c967a448c3eb0e84"},"cell_type":"markdown","source":"## ENSEMBLE NEURAL NETWORK"},{"metadata":{"trusted":true,"_uuid":"4c85cb6eb12f4180a2fe678e7168bd3cedd51d96"},"cell_type":"code","source":"def calc_change_rate(x):\n    change = (np.diff(x) / x[:-1]).values\n    change = change[np.nonzero(change)[0]]\n    change = change[~np.isnan(change)]\n    change = change[change != -np.inf]\n    change = change[change != np.inf]\n    return np.mean(change)\n\nsubmission = pd.read_csv('../input/sample_submission.csv', index_col = 'seg_id')\nX_test = pd.DataFrame(columns = x_train.columns, dtype = np.float64, index = submission.index)\n\nfor id in tqdm_notebook(X_test.index):\n    seg = pd.read_csv('../input/test/' + id + '.csv')\n    \n    x = pd.Series(seg['acoustic_data'].values)\n    \n    X_test.loc[id, 'AVERAGE'] = x.mean()\n    X_test.loc[id, 'STD'] = x.std()\n    X_test.loc[id, 'MAX'] = x.max()\n    X_test.loc[id, 'MIN'] = x.min() \n    X_test.loc[id, 'SUM'] = x.sum()\n    \n    X_test.loc[id, 'MEAN_CHANGE_ABS'] = np.mean(np.diff(x))\n    X_test.loc[id, 'MEAN_CHANGE_RATE'] = calc_change_rate(x)\n    \n    X_test.loc[id, 'MAX_TO_MIN'] = x.max() / np.abs(x.min())\n    X_test.loc[id, 'MAX_TO_MIN_DIFF'] = x.max() - np.abs(x.min())\n    X_test.loc[id, 'COUNT_BIG'] = len(x[np.abs(x) > 500])\n    \n    X_test.loc[id, 'AVERAGE_FIRST_10000'] = x[:10000].mean()\n    X_test.loc[id, 'AVERAGE_LAST_10000']  =  x[-10000:].mean()\n    X_test.loc[id, 'AVERAGE_FIRST_50000'] = x[:50000].mean()\n    X_test.loc[id, 'AVERAGE_LAST_50000'] = x[-50000:].mean()\n    \n    X_test.loc[id, 'STD_FIRST_10000'] = x[:10000].std()\n    X_test.loc[id, 'STD_LAST_10000']  =  x[-10000:].std()\n    X_test.loc[id, 'STD_FIRST_50000'] = x[:50000].std()\n    X_test.loc[id, 'STD_LAST_50000'] = x[-50000:].std()\n    \n    X_test.loc[id, 'ABS_AVERAGE'] = np.abs(x).mean()\n    X_test.loc[id, 'ABS_STD'] = np.abs(x).std()\n    X_test.loc[id, 'ABS_MAX'] = np.abs(x).max()\n    X_test.loc[id, 'ABS_MIN'] = np.abs(x).min()\n    \n    X_test.loc[id, '10Q'] = np.percentile(x, 0.10)\n    X_test.loc[id, '25Q'] = np.percentile(x, 0.25)\n    X_test.loc[id, '50Q'] = np.percentile(x, 0.50)\n    X_test.loc[id, '75Q'] = np.percentile(x, 0.75)\n    X_test.loc[id, '90Q'] = np.percentile(x, 0.90)\n    \n    X_test.loc[id, 'ABS_1Q'] = np.percentile(x, np.abs(0.01))\n    X_test.loc[id, 'ABS_5Q'] = np.percentile(x, np.abs(0.05))\n    X_test.loc[id, 'ABS_30Q'] = np.percentile(x, np.abs(0.30))\n    X_test.loc[id, 'ABS_60Q'] = np.percentile(x, np.abs(0.60))\n    X_test.loc[id, 'ABS_95Q'] = np.percentile(x, np.abs(0.95))\n    X_test.loc[id, 'ABS_99Q'] = np.percentile(x, np.abs(0.99))\n    \n    X_test.loc[id, 'KURTOSIS'] = x.kurtosis()\n    X_test.loc[id, 'SKEW'] = x.skew()\n    X_test.loc[id, 'MEDIAN'] = x.median()\n    \n    X_test.loc[id, 'HILBERT_MEAN'] = np.abs(hilbert(x)).mean()\n    X_test.loc[id, 'HANN_WINDOW_MEAN'] = (convolve(x, hann(150), mode = 'same') / sum(hann(150))).mean()\n    \n    for windows in [10, 100, 1000]:\n        x_roll_std = x.rolling(windows).std().dropna().values\n        x_roll_mean = x.rolling(windows).mean().dropna().values\n        \n        X_test.loc[id, 'AVG_ROLL_STD' + str(windows)] = x_roll_std.mean()\n        X_test.loc[id, 'STD_ROLL_STD' + str(windows)] = x_roll_std.std()\n        X_test.loc[id, 'MAX_ROLL_STD' + str(windows)] = x_roll_std.max()\n        X_test.loc[id, 'MIN_ROLL_STD' + str(windows)] = x_roll_std.min()\n        X_test.loc[id, '1Q_ROLL_STD' + str(windows)] = np.quantile(x_roll_std, 0.01)\n        X_test.loc[id, '5Q_ROLL_STD' + str(windows)] = np.quantile(x_roll_std, 0.05)\n        X_test.loc[id, '95Q_ROLL_STD' + str(windows)] = np.quantile(x_roll_std, 0.95)\n        X_test.loc[id, '99Q_ROLL_STD' + str(windows)] = np.quantile(x_roll_std, 0.99)\n        X_test.loc[id, 'AV_CHANGE_ABS_ROLL_STD' + str(windows)] = np.mean(np.diff(x_roll_std))\n        X_test.loc[id, 'ABS_MAX_ROLL_STD' + str(windows)] = np.abs(x_roll_std).max()\n        \n        X_test.loc[id, 'AVG_ROLL_MEAN' + str(windows)] = x_roll_mean.mean()\n        X_test.loc[id, 'STD_ROLL_MEAN' + str(windows)] = x_roll_mean.std()\n        X_test.loc[id, 'MAX_ROLL_MEAN' + str(windows)] = x_roll_mean.max()\n        X_test.loc[id, 'MIN_ROLL_MEAN' + str(windows)] = x_roll_mean.min()\n        X_test.loc[id, '1Q_ROLL_MEAN' + str(windows)] = np.quantile(x_roll_mean, 0.01)\n        X_test.loc[id, '5Q_ROLL_MEAN' + str(windows)] = np.quantile(x_roll_mean, 0.05)\n        X_test.loc[id, '95Q_ROLL_MEAN' + str(windows)] = np.quantile(x_roll_mean, 0.95)\n        X_test.loc[id, '99Q_ROLL_MEAN' + str(windows)] = np.quantile(x_roll_mean, 0.99)\n        X_test.loc[id, 'AV_CHANGE_ABS_ROLL_MEAN' + str(windows)] = np.mean(np.diff(x_roll_mean))\n        X_test.loc[id, 'ABS_MAX_ROLL_MEAN' + str(windows)] = np.abs(x_roll_mean).max()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4b8d7ad743273344dc782ff15150e76f97fa11e6"},"cell_type":"code","source":"X_test.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"250f89215eca55e122530275aa75717614a97fa6"},"cell_type":"code","source":"print(X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"223c293b5a967e6cc712e04baf0139706ae85c14","_kg_hide-output":true},"cell_type":"code","source":"X_test.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e2e56b8181750426d686ded10484234f778ea986"},"cell_type":"code","source":"submission.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"daf1c41f1a61ee85ac1cc7d32fdc3ba15b563432"},"cell_type":"code","source":"#normalizing our testing data\nX_test_scaled = scaler.transform(X_test)\nprint(X_test_scaled.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"253119f7f56984b534f4d8859b4dc26ef448439c"},"cell_type":"code","source":"input_dim = x_train.shape[1]\nactivation = 'tanh'\nclasses = 1\n\nhistory = dict() #dictionery to store the history of individual models for later visualization\nprediction_scores = dict() #dictionery to store the predicted scores of individual models on the test dataset\n\n#here we will be training the same model for a total of 10 times and will be considering the mean of the output values for predictions\nfor i in np.arange(0, 15):\n    optim = optimizers.Adam(lr = 0.001, decay = 0.001 / 32)\n    ensemble_model = Model(input_dim = input_dim, activation = activation, classes = classes)\n    ensemble_model.compile(loss = 'mean_absolute_error', optimizer = optim, metrics = ['mean_absolute_error'])\n    print('TRAINING MODEL NO : {}'.format(i))\n    H = ensemble_model.fit(x_train_scaled, y_train_flatten,\n                  batch_size = 32,\n                  epochs = 200,\n                  verbose = 1)\n    history[i] = H\n    \n#     ensemble_model.save('MODEL_{}.model'.format(i))\n    \n    predictions = ensemble_model.predict(X_test_scaled, verbose = 1, batch_size = 32)\n    prediction_scores[i] = predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6fa2910e902777ca3b690b270cf9079690a6003b"},"cell_type":"code","source":"#making predictions\nprediction1 = np.hstack([p.reshape(-1,1) for p in prediction_scores.values()]) #taking the scores of all the trained models\nprediction1 = np.mean(prediction1, axis = 1)\n\nprint(prediction1.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"31498e4315d644801f2c1bbc6df14a2201d6545d"},"cell_type":"code","source":"#submitting the file\nsubmission['time_to_failure'] = prediction1\nsubmission.to_csv('submission.csv')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}