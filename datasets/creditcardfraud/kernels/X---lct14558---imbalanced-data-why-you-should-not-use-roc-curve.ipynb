{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "51ef1f7f-670f-c06b-dad2-1e3cc09d70f1"
      },
      "source": [
        "# Credit Card Fraud: Handling highly imbalance classes and why Receiver Operating Characteristics Curve (ROC Curve) should not be used, and Precision/Recall curve should be preferred in highly imbalanced situations\n",
        "\n",
        "## Motivation\n",
        "In this notebook, we will explore the data through initial EDA with some visualizations. Then we will hit the main point of the notebook, which is to explore ways to handle imbalanced data. We will use two methods:\n",
        "1. Using the weights parameters in Sci-Kit Learn classifiers\n",
        "2. Over and Undersampling (to be developed in the next version)\n",
        "\n",
        "Finally, we will quantify and illustrate the effects of the trade off between True Positive Rate and False Positive Rate using ROC and Precision/Recall (PR) curves, and disucss **why the popular ROC curve should not be used on highly imbalanced dataset** (or in general, why I prefer PR curves over ROC)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "da93dae5-d8e0-9b89-0379-76074ca77b27"
      },
      "source": [
        "## Imports and reading in the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "8a4ced0e-e4d9-6dea-8f5d-25050112d2ad"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "pd.options.display.max_colwidth = 200\n",
        "pd.options.display.max_columns = 200\n",
        "import numpy as np\n",
        "\n",
        "import time\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import GradientBoostingClassifier,RandomForestClassifier,AdaBoostClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import cross_val_predict,cross_val_score,train_test_split\n",
        "from sklearn.metrics import classification_report,confusion_matrix,roc_curve,auc,precision_recall_curve,roc_curve\n",
        "\n",
        "import os\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import seaborn as sns\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d9bd3e21-f2e6-6a32-6514-e2e08c02b118"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('../input/creditcard.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "f6aa59c8-ae9e-2ad7-3c1a-60234b249371"
      },
      "source": [
        "The data has a column called 'Time', which are seconds from which the very first data observation took place. Let's convert that to hours of a day. I'm guessing the data starts at midnight and ends at midnight."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "bcb7502e-5a9c-cac6-2d67-26071d99e6d9"
      },
      "outputs": [],
      "source": [
        "df['hour'] = df['Time'].apply(lambda x: np.ceil(float(x)/3600) % 24)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "65467e9f-52ff-88c6-414c-ba4f3952df1a"
      },
      "source": [
        "Seems right that we have the least transactions from 1AM to 5AM. Let's see a breakdown of our legit vs fraud transactions via a pivot table.\n",
        "\n",
        "**Note:**\n",
        "* Class 0 = Legit transactions\n",
        "* Class 1 = Fruadulent transactions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "dc9bf12f-e7f7-3406-7def-49ce6828c8bf"
      },
      "outputs": [],
      "source": [
        "df.pivot_table(values='Amount',index='hour',columns='Class',aggfunc='count')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "94820fc1-5ed8-1bc2-2e86-5b1ba449fd29"
      },
      "source": [
        "# Visualizing the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "afe75b8a-542a-f875-67e5-4f6a570fb421"
      },
      "outputs": [],
      "source": [
        "def PlotHistogram(df,norm):\n",
        "    bins = np.arange(df['hour'].min(),df['hour'].max()+2)\n",
        "    plt.figure(figsize=(15,4))\n",
        "    sns.distplot(df[df['Class']==0.0]['hour'],\n",
        "                 norm_hist=norm,\n",
        "                 bins=bins,\n",
        "                 kde=False,\n",
        "                 color='b',\n",
        "                 hist_kws={'alpha':.5},\n",
        "                 label='Legit')\n",
        "    sns.distplot(df[df['Class']==1.0]['hour'],\n",
        "                 norm_hist=norm,\n",
        "                 bins=bins,\n",
        "                 kde=False,\n",
        "                 color='r',\n",
        "                 label='Fraud',\n",
        "                 hist_kws={'alpha':.5})\n",
        "    plt.xticks(range(0,24))\n",
        "    plt.legend()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "1a01f05a-840d-f726-9ae8-a70c4fd4e08f"
      },
      "outputs": [],
      "source": [
        "start = time.time()\n",
        "print('Normalized histogram of Legit/Fraud over hour of the day')\n",
        "PlotHistogram(df,True)\n",
        "print('Counts histogram of Legit/Fraud over hour of the day')\n",
        "print('*you can barely see the Fraud cases since there are so little of them.')\n",
        "PlotHistogram(df,False)\n",
        "print(time.time()-start)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ad8c1464-8a7d-d5ae-2154-d9234cd9ca0b"
      },
      "outputs": [],
      "source": [
        "print('Fraud is {}% of our data.'.format(df['Class'].value_counts()[1] / float(df['Class'].value_counts()[0])*100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "5913d133-d8ad-0e1e-4508-2b7bf27cd0ee"
      },
      "source": [
        "Hour of the day seems to  have some impact on the number of Fraud cases. I'll be sure to to add the 'hour' dimension to visualizations later to further investigate its impact. \n",
        "\n",
        "Before we train our classifers, we need to normalize the Amount since it's on a totally different scale. The distributions are also highly skewed with a lot of statistical outliers. All Fraud cases are in the low dollar values i.e. Amount.\n",
        "\n",
        "**We also have a HUGE class imbalance.** More on that later when we start to train classifiers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "650d59ca-f6cf-9e93-e5a0-5468e10225f6"
      },
      "outputs": [],
      "source": [
        "mask_true = (df['Class'] == 1.0) \n",
        "mask_false = (df['Class'] == 0.0)\n",
        "\n",
        "df['Amount'] = StandardScaler().fit_transform(df[['Amount']])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "58952be6-1bc8-1579-131d-0ee0bf91890a"
      },
      "outputs": [],
      "source": [
        "def PlotViolins(minHour,maxHour):\n",
        "    plt.figure(figsize=(15,6))\n",
        "    plt.title('Amount by class throughout the day')\n",
        "    plt.ylim([-1,3.0])\n",
        "    sns.violinplot(data=df[df['hour'].isin(range(minHour,maxHour+1))],x='hour',y='Amount',hue='Class',split=True,palette='Set2',cut=0)\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.show()\n",
        "PlotViolins(0,11)\n",
        "PlotViolins(12,23)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "2524f9de-3a1a-def0-1cf5-9747a2622fa2"
      },
      "source": [
        "Let's see how well the PCA components complement each other by looking their interactions with each other. I'm only including the first 6 components here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "6005ee15-0f60-516b-bef5-ddae91f97a7e"
      },
      "outputs": [],
      "source": [
        "# Model building\n",
        "Let's start with a vanilla Logistic Regression since it seems like for some of the features, a sigmoid curve can sort of separate the classes.sns.pairplot(data=pd.concat([df.loc[:,'hour'],df.loc[:,'V1':'V6'],df.loc[:,'Class']],axis=1),\n",
        "             hue='Class',\n",
        "             diag_kind='kde',\n",
        "             plot_kws={'alpha':0.2})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "2ab9042a-a8a4-c981-2000-24c952a26877"
      },
      "source": [
        "Seems like most features show both Fraud and Legit purchases overlapping each other, with V4 showing a little promise. We can't rely on our eyes to do all the investigation work since we at most can make 4-dimensional charts (3 features + a color) to investigate our data, and our data has more than 30 features.\n",
        "\n",
        "Let's get to model building and see how well our data can separate the classes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "e2d19356-0a26-cdc8-1558-042fd897b19f"
      },
      "source": [
        "# Model building\n",
        "Let's start with a vanilla Logistic Regression since it seems like for some of the features, a sigmoid curve can sort of separate the classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "e2ae6d7f-00b9-a4e7-a36b-71bf7b5f38e7"
      },
      "outputs": [],
      "source": [
        "features = pd.concat([df.loc[:,'V1':'Amount'],df.loc[:,'Time']],axis=1)\n",
        "target = df['Class']\n",
        "\n",
        "x_train,x_test,y_train,y_test = train_test_split(features,target, stratify=target,test_size=0.35, random_state=1)\n",
        "\n",
        "print('y_train class counts')\n",
        "print(y_train.value_counts())\n",
        "print('')\n",
        "print('y_test class counts')\n",
        "print(y_test.value_counts())\n",
        "\n",
        "\n",
        "# Let's store our y_test legit and fraud counts for normalization purposes later on\n",
        "y_test_legit = y_test.value_counts()[0]\n",
        "y_test_fraud = y_test.value_counts()[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "7a34f699-90fe-e622-0c04-5d483413d49a"
      },
      "outputs": [],
      "source": [
        "lr_model = LogisticRegression()\n",
        "lr_model.fit(x_train,y_train)\n",
        "\n",
        "pred = lr_model.predict(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "51243ecb-b637-517c-3ea6-233a2c8e4b83"
      },
      "outputs": [],
      "source": [
        "def PlotConfusionMatrix(y_test,pred,y_test_legit,y_test_fraud):\n",
        "\n",
        "    cfn_matrix = confusion_matrix(y_test,pred)\n",
        "    cfn_norm_matrix = np.array([[1.0 / y_test_legit,1.0/y_test_legit],[1.0/y_test_fraud,1.0/y_test_fraud]])\n",
        "    norm_cfn_matrix = cfn_matrix * cfn_norm_matrix\n",
        "\n",
        "    fig = plt.figure(figsize=(15,5))\n",
        "    ax = fig.add_subplot(1,2,1)\n",
        "    sns.heatmap(cfn_matrix,cmap='coolwarm_r',linewidths=0.5,annot=True,ax=ax)\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.ylabel('Real Classes')\n",
        "    plt.xlabel('Predicted Classes')\n",
        "\n",
        "    ax = fig.add_subplot(1,2,2)\n",
        "    sns.heatmap(norm_cfn_matrix,cmap='coolwarm_r',linewidths=0.5,annot=True,ax=ax)\n",
        "\n",
        "    plt.title('Normalized Confusion Matrix')\n",
        "    plt.ylabel('Real Classes')\n",
        "    plt.xlabel('Predicted Classes')\n",
        "    plt.show()\n",
        "    \n",
        "    print('---Classification Report---')\n",
        "    print(classification_report(y_test,pred))\n",
        "\n",
        "PlotConfusionMatrix(y_test,pred,y_test_legit,y_test_fraud)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "3c964d05-222e-1cee-9ec2-d1c88e6512de"
      },
      "source": [
        "I'm not a huge fan of confusion matrix because they can be misleading. I almost ALWAYS go directly to my classification report for precision and recall scores for each class. And Seaborn's heatmap coloring scheme is not helping. Our True Positive rate is 0.59 as also shown in the classification report which is not TOO bad, but it's shown as blood red. The normalized confusion matrix tells a better story. But, actual precision and recall scores are my go-to metrics.\n",
        "\n",
        "It seems like we aren't very good with catching our frauds, which is expected with a vanilla Logistic Regression without addressing the class imbalance issue. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "1b90965e-8d3c-3d2d-6ac3-3559c7e9bc95"
      },
      "source": [
        "# Addressing class imbalance\n",
        "### Using weights to counteract the class imbalance\n",
        "Sci-Kit Learn classifiers can give heavier weights to the minority class using a simple parameter during model initiation. Let's see how that will improve our results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "7870ee9a-f4d5-4c39-4114-059ecd4800dd"
      },
      "outputs": [],
      "source": [
        "lr_model = LogisticRegression(class_weight='balanced')\n",
        "lr_model.fit(x_train,y_train)\n",
        "\n",
        "pred = lr_model.predict(x_test)\n",
        "\n",
        "PlotConfusionMatrix(y_test,pred,y_test_legit,y_test_fraud)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "6adfa933-8ebd-f1ea-4bfd-f64669ee56d3"
      },
      "source": [
        "Looking at the normalized confusion matrix, it seems like our classifer is doing very well! We have a 98% True Negative rate and a 92% True Positive rate! Seems like a perfect classifer, doesn't it?\n",
        "\n",
        "However, if we look at the individual precision scores, this classifer is now a lot less precise than before. This is because we have increased our Fraud recall score at the expense of more mis-classified Legit cases. With the \"balanced\" weight parameter, we have increased our false positive counts from 39 to 2300. 2300 is still only a small fraction of truely negative cases (out of 99511), that's why the percentage shown on the Normalized Confusion Matrix is still relatively small at 0.023%. Let's try to specify our own weights. The weights are somewhat arbitrary but it illustrates the tradeoff between precision and recall."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "967b0ac1-c68b-ec6a-ec8f-251cc459d95b"
      },
      "outputs": [],
      "source": [
        "for w in [1,5,10,100,500,1000]:\n",
        "    print('---Weight of {} for Fraud class---'.format(w))\n",
        "    lr_model = LogisticRegression(class_weight={0:1,1:w})\n",
        "    lr_model.fit(x_train,y_train)\n",
        "\n",
        "    pred = lr_model.predict(x_test)\n",
        "    PlotConfusionMatrix(y_test,pred,y_test_legit,y_test_fraud)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "37707e75-84b7-4b9e-fcd6-cc96491c0b5f"
      },
      "source": [
        "## ROC versus Precision/Recall Curves\n",
        "Just by manaually selecting a range of weights to boost the minority class already helped our model have better recall, and in some cases, better precision also. Recall and Precision are usually trade offs of each other, so when you can improve both **at the same time**, your model's overall performance is undeniably improved. \n",
        "\n",
        "\n",
        "To illustrate the trade off between precision vs recall, and let's also include False Positive Rate vs True Positive Rate (ROC), let's plot the ROC and Precision/Recall curves for different weights for the minority class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b33653c6-7d38-ae32-3cda-c687e4a0d10a"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(15,8))\n",
        "ax1 = fig.add_subplot(1,2,1)\n",
        "ax1.set_xlim([-0.05,1.05])\n",
        "ax1.set_ylim([-0.05,1.05])\n",
        "ax1.set_xlabel('Recall')\n",
        "ax1.set_ylabel('Precision')\n",
        "ax1.set_title('PR Curve')\n",
        "\n",
        "ax2 = fig.add_subplot(1,2,2)\n",
        "ax2.set_xlim([-0.05,1.05])\n",
        "ax2.set_ylim([-0.05,1.05])\n",
        "ax2.set_xlabel('False Positive Rate')\n",
        "ax2.set_ylabel('True Positive Rate')\n",
        "ax2.set_title('ROC Curve')\n",
        "\n",
        "for w,k in zip([1,5,10,20,50,100,10000],'bgrcmykw'):\n",
        "    lr_model = LogisticRegression(class_weight={0:1,1:w})\n",
        "    lr_model.fit(x_train,y_train)\n",
        "    pred_prob = lr_model.predict_proba(x_test)[:,1]\n",
        "\n",
        "    p,r,_ = precision_recall_curve(y_test,pred_prob)\n",
        "    tpr,fpr,_ = roc_curve(y_test,pred_prob)\n",
        "    \n",
        "    ax1.plot(r,p,c=k,label=w)\n",
        "    ax2.plot(tpr,fpr,c=k,label=w)\n",
        "ax1.legend(loc='lower left')    \n",
        "ax2.legend(loc='lower left')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "107cebd0-a1b5-7715-e175-12181f045a0b"
      },
      "source": [
        "# Conclusion\n",
        "\n",
        "For a PR curve, a good classifer aims for the upper right corner of the chart but upper left for the ROC curve.\n",
        "\n",
        "While PR and ROC curves use the same data, i.e. the real class labels and predicted probability for the class lables, you can see that the two charts tell very different stories, with some weights seem to perform better in ROC than in the PR curve.\n",
        "\n",
        "While the blue, w=1, line performed poorly in both charts, the black, w=10000, line performed \"well\" in the ROC but poorly in the PR curve. This is due to the high class imbalance in our data. ROC curve is not a good visual illustration for highly imbalanced data, because the False Positive Rate ( False Positives / Total Real Negatives ) does not drop drastically when the Total Real Negatives is huge.\n",
        "\n",
        "Whereas Precision ( True Positives / (True Positives + False Positives) ) is highly sensitive to False Positives and is not impacted by a large total real negative denominator. \n",
        "\n",
        "The biggest difference among the models are at around 0.8 recall rate. Seems like a lower weight, i.e. 5 and 10, out performs other weights significantly at 0.8 recall. This means that with those specific weights, our model can detect frauds fairly well (catching 80% of fraud) while not annoying a bunch of customers with false positives with an equally high precision of 80%.\n",
        "\n",
        "Without further tuning our model, and of course we should do cross validation for any real model tuning/validation, it seems like a vanilla Logistic Regression is stuck at around 0.8 Precision and Recall. \n",
        "\n",
        "So how do we know if we should sacrifice our precision for more recall, i.e. catching fraud? That is where data science meets your core business parameters. If the cost of missing a fraud highly outweighs the cost of canceling a bunch of legit customer transactions, i.e. false positives, then perhaps we can choose a weight that gives us a higher recall rate. Or maybe catching 80% of fraud is good enough for your business if you can minimize also minimize the \"user friction\" or credit card disruptions by keeping our precision high."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "1edb4ee8-2601-1a52-475c-3cca531d7723"
      },
      "outputs": [],
      "source": ""
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}