{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.utils import shuffle\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, cross_val_predict\nfrom sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix, make_scorer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import VotingClassifier\n\n%matplotlib inline\n\ntrain = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1b99503e984074fa79e651ea691d68e3d272aa51"},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1a88df03cd103869618fda034d153b827d0c3294"},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"868b2beeda145aef61ff22b35150fbe866e497b4"},"cell_type":"code","source":"sns.violinplot(x = 'Cover_Type', y = 'Elevation', data = train)\n\n#every cover type seems to have different range of elevation, which is a good feature","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f6e76e2c7ea3fbc69a7981b34e64731226802450"},"cell_type":"code","source":"sns.violinplot(x = 'Cover_Type', y = 'Aspect', data = train)\n\n#this feature is good for type 3 to 7, but 1 and 2 show really similar mean and std for this feature","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cc75d5f745d8a481609dd7dc24cf36c1138a420a"},"cell_type":"code","source":"sns.violinplot(x = 'Cover_Type', y = 'Slope', data = train)\n\n#slope is like aspect across different types ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"aaf15e2e22b079dcdb4a049d54ae1f848a172b53"},"cell_type":"code","source":"_, (ax1, ax2, ax3) = plt.subplots(1,3, figsize = (15,5)) \nsns.distplot(train['Elevation'], ax=ax1, label = \"Skewness: {0:.2f}\".format(train['Elevation'].skew())).legend()\nsns.distplot(train['Aspect'], ax=ax2 ,label = \"Skewness: {0:.2f}\".format(train['Aspect'].skew())).legend()\nsns.distplot(train['Slope'], ax=ax3, label = \"Skewness: {0:.2f}\".format(train['Slope'].skew())).legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7ac1c2b7324b0ccd64f107ee0e71607daa39e16d"},"cell_type":"code","source":"_, axarr = plt.subplots(2 ,2, figsize = (15,10))\nsns.violinplot(x = 'Cover_Type', y = 'Horizontal_Distance_To_Hydrology', data = train, ax = axarr[0,0])\nsns.violinplot(x = 'Cover_Type', y = 'Vertical_Distance_To_Hydrology', data = train, ax = axarr[0,1])\nsns.violinplot(x = 'Cover_Type', y = 'Horizontal_Distance_To_Roadways', data = train, ax = axarr[1,0])\nsns.violinplot(x = 'Cover_Type', y = 'Horizontal_Distance_To_Fire_Points', data = train, ax = axarr[1,1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"27b7227aa5acb2d52b7c070637bef12455e3e05a"},"cell_type":"code","source":"_, axarr = plt.subplots(2 ,2, figsize = (15,10))\nsns.distplot(train['Horizontal_Distance_To_Hydrology'], ax = axarr[0,0], \\\n             label = \"Skewness: {0:.2f}\".format(train['Horizontal_Distance_To_Hydrology'].skew())).legend()\nsns.distplot(train['Vertical_Distance_To_Hydrology'], ax = axarr[0,1], \\\n             label = \"Skewness: {0:.2f}\".format(train['Vertical_Distance_To_Hydrology'].skew())).legend()\nsns.distplot(train['Horizontal_Distance_To_Roadways'], ax = axarr[1,0], \\\n             label = \"Skewness: {0:.2f}\".format(train['Horizontal_Distance_To_Roadways'].skew())).legend()\nsns.distplot(train['Horizontal_Distance_To_Fire_Points'], ax = axarr[1,1], \\\n             label = \"Skewness: {0:.2f}\".format(train['Horizontal_Distance_To_Fire_Points'].skew())).legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cd9ef8ec79ff215bc99ba7f20b49b3ef541d2de9"},"cell_type":"code","source":"_, (ax1, ax2, ax3) = plt.subplots(1,3, figsize = (15,5)) \nsns.barplot(x = 'Cover_Type', y = 'Hillshade_9am', data = train, ax=ax1)\nsns.barplot(x = 'Cover_Type', y = 'Hillshade_Noon', data = train, ax=ax2)\nsns.barplot(x = 'Cover_Type', y = 'Hillshade_3pm', data = train, ax=ax3)\n\n#hillshade is also good for types 3 to 7, but weak for type 1 and 2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6da3565289811ff2174fcdf0d88aa5ed681ec2df"},"cell_type":"code","source":"def transform_data(data):\n    data['Euclidean_Distance_To_Hydrology'] = np.sqrt(data['Horizontal_Distance_To_Hydrology']**2 + \\\n                                                     data['Vertical_Distance_To_Hydrology']**2)\n    distance_features = ['Horizontal_Distance_To_Hydrology', 'Vertical_Distance_To_Hydrology',\\\n                    'Horizontal_Distance_To_Roadways', 'Horizontal_Distance_To_Fire_Points',\\\n                    ]\n    #calcualte Euclidean distance with horizontal and vertical distances \n    \n    def ratio_big_to_small(a, b): \n        #get ratio of larger of two to smaller of two\n        if a < 1:\n            a = 1\n        if b < 1:\n            b = 1\n        a, b = np.abs(a), np.abs(b)\n        if a >= b:\n            return a/b\n        else:\n            return b/a\n        \n    for feature1 in distance_features:\n        for feature2 in distance_features:\n            if feature1 != feature2:\n                new_feature1 = feature1 + '*' + feature2\n                data[new_feature1] = data[feature1] * data[feature2]\n                new_feature2 = feature1 + '+' + feature2\n                data[new_feature2] = data[feature1] + data[feature2]\n                new_feature3 = feature1 + '-' + feature2\n                data[new_feature3] = np.abs(data[feature1] + data[feature2])\n                new_feature4 = feature1 + '/' + feature2\n                data[new_feature4] = data.apply(lambda row: ratio_big_to_small(row[feature1], row[feature2]), axis=1)\n        new_feature5 = feature1 + '^2'\n        data[new_feature5] = data[feature1]**2\n    #generate new features with existing distance features for the purpose of helping prediction of type 1 and 2\n    \n    data['Hillshade_Range'] = data[['Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm']].apply(np.ptp, axis = 1)\n    data['Hillshade_Std'] = data[['Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm']].apply(np.std, axis = 1)\n    #find relationship between hillshade at different times\n    \n    return data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bdb61b57d7ac8c1cddd4fbb08ad282f57b2eaf4e"},"cell_type":"code","source":"train = shuffle(train, random_state = 0)\nnew_train = transform_data(train)\nX_train, y_train = new_train.drop(['Cover_Type'], axis = 1).values, new_train['Cover_Type'].values\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nprocessed_test = scaler.transform(transform_data(test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e5861490ea6847e9233aabf7e4bdefa0643ccac9"},"cell_type":"code","source":"def base_grid_search(clf, param, X_train, y_train, metric='accuracy'):\n    grid_search = GridSearchCV(clf, param_grid = param, cv = 5, scoring = metric, n_jobs = -1, verbose = 3)\n    grid_search.fit(X_train, y_train)\n    print (grid_search.best_score_, grid_search.best_params_)\n\ndef color_confu(y_true, y_predict):\n    #define metric on which optimizations of feature engineerings and algorithms are based upon\n    confu_mx = confusion_matrix(y_true, y_predict)\n    print('Accuracy for each type: ', confu_mx.diagonal()/confu_mx.sum(axis=1))\n    row_sums = confu_mx.sum(axis = 1, keepdims=True)\n    norm_conf_mx = confu_mx / row_sums\n    np.fill_diagonal(norm_conf_mx, 0)\n    sns.heatmap(norm_conf_mx, cmap=plt.cm.gray, linewidths=0.1, annot=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d5268754a856e3ce3fc4927bc937004877a4d941"},"cell_type":"code","source":"# a baseline model that got 69% in submisssion as a benchmark to gauge how well the voting classifier performs\nextra = ExtraTreesClassifier(max_depth = 85, n_estimators = 65, max_features = 33)\nextra.fit(X_train, y_train)\neach_type, each_weight = np.unique(extra.predict(processed_test), return_counts=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f1104bf6d161c6909808087c4995c3283d7be64f"},"cell_type":"code","source":"extra_base = ExtraTreesClassifier(max_depth = 85, n_estimators = 65, max_features = 33)\nextra_base_predict = cross_val_predict(extra_base, X_train, y_train, cv=5, n_jobs=-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c4be063ce9d395f8cb0483321f7144c9d4035480"},"cell_type":"code","source":"color_confu(y_train, extra_base_predict)\n# this accuracy is ok, and misclassifications happen at 2<->5, 1<->4, and 1<->2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b0ff88826e561d5b1615a9eb300c3eb3f820256a"},"cell_type":"code","source":"rf_base = RandomForestClassifier(n_estimators=100, max_depth = 70, max_features = 9)\nrf_base_predict = cross_val_predict(rf_base, X_train, y_train, n_jobs = -1, cv=5)\ncolor_confu(rf_base_predict, y_train)\n#though its accuracy is a little lower than that of the previous model,\n# it is less subject to erros when the actual class is 1 or 2, the diversity the voting classifier needs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9427908453b4d0a63d24967194281eaf0a0b371c"},"cell_type":"code","source":"#check feature importance with both classifiers\nfeatures = new_train.drop(['Cover_Type'], axis=1)\nrf_base.fit(X_train, y_train)\nextra_base.fit(X_train, y_train)\nsorted(zip(rf_base.feature_importances_, list(features)), reverse=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1e53a0b7d39006c800d8b3f537aee559a2d67740"},"cell_type":"code","source":"sorted(zip(extra_base.feature_importances_, list(features)), reverse=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9609b8e6b1ae15e9679d3bca5cae56516b8041dc"},"cell_type":"code","source":"#cut features that have importance values less than feature_threshold in both sets\nrf_importance = dict(zip(list(features), rf_base.feature_importances_))\nextra_importance = dict(zip(list(features), extra_base.feature_importances_))\ntrash_features = []\nfeature_threshold = 0.001\nfor feature in rf_importance:\n    ind = list(new_train).index(feature)\n    if rf_importance[feature] < feature_threshold and extra_importance[feature] < feature_threshold:\n        trash_features.append(ind)\nfor feature in extra_importance:\n    ind = list(new_train).index(feature)\n    if rf_importance[feature] < feature_threshold and extra_importance[feature] < feature_threshold and ind not in trash_features:\n        trash_features.append(ind)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"31f6405443de7715538bd3092a32493f0ad42620"},"cell_type":"code","source":"X_train_new = np.delete(X_train, trash_features, axis = 1)\nnew_test = np.delete(processed_test, trash_features, axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"786c769dbeb068c4da92ec9d435fd980ac4a1744"},"cell_type":"code","source":"#take advantage of these two classifiers thorugh combining them with an optimized weights\nvote_clf = VotingClassifier(estimators=[('extra', ExtraTreesClassifier(max_depth = 85, n_estimators = 65, max_features = 33,\n                                                                     )),\n                                        ('rf',RandomForestClassifier(n_estimators=100, max_depth = 70, max_features = 9,\n                                                                    ))],\n                            voting='soft', weights=[2.3,1])\n\nvote_predict = cross_val_predict(vote_clf, X_train_new, y_train, cv=5, n_jobs=-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b16fec8a6696bd02fd8723a4b09e009fc2a5743c"},"cell_type":"code","source":"#this voting classifier achieved 80% on Kaggle, a big improvement over ExtraTree\n#this improvement can be attributed to increase in precision of 1,2 classification\n#the next step can be extracting more features to differentiate type 1 and type 2\ncolor_confu(y_train, vote_predict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"75264babd86526f1c3247bb6a73fac5592334b4d"},"cell_type":"code","source":"#code to submit prediction on test data\n\nvote_clf.fit(X_train_new, y_train)\nsub = pd.DataFrame({'Id': test['Id'], \\\n                   'Cover_Type': vote_clf.predict(new_test)})\nsub = sub[['Id', 'Cover_Type']]\nsub.to_csv('prediction_type.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}