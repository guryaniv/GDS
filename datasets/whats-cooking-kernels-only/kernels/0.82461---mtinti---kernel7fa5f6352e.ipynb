{"cells":[{"metadata":{"_uuid":"a90623029a4b27b7d9ebde2b0a4074ea7f811192"},"cell_type":"markdown","source":"# Cook me\n## Gourmet guide to serve the best LightGBM model"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":false,"_kg_hide-output":false},"cell_type":"code","source":"#heavily borrwed from avaiable code","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input/whats-cooking-kernels-only\"))\nprint(os.listdir(\"../input/svm-12jul18\"))\nprint(os.listdir(\"../input/let-s-cook-model\"))\n# Any results you write to the current directory are saved as output.\n# Import the required libraries \nrandom_state = None\nimport time\nstarttime = time.monotonic()\nfrom lightgbm import LGBMClassifier\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom scipy.sparse import csr_matrix\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV, StratifiedShuffleSplit\nfrom scipy.sparse import hstack, csr_matrix\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\nimport gc\nimport pandas as pd\nimport json\nimport pdb\nfrom sklearn.model_selection import KFold, StratifiedKFold\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ef104d858476018a6ee6f4817f663717d762b353"},"cell_type":"code","source":"#get train\ntrain_df = pd.read_json('../input/whats-cooking-kernels-only/train.json')\ntrain_df.columns = ['target', 'id', 'ingredients']\ntrain_df[\"num_ingredients\"] = train_df['ingredients'].apply(lambda x: len(x))\ntrain_df = train_df[ (train_df['num_ingredients'] > 1) ]\nprint(train_df.shape)\ntrain_df_start = train_df.shape[0]\ntrain_df.head()\n#make beck up of target\ny_bk = list(train_df['target'].copy().values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"42126e54358e1c0744adb12af45e3ae1930577cd"},"cell_type":"code","source":"#get test\ntest_df = pd.read_json('../input/whats-cooking-kernels-only/test.json')\ntest_df['cousine']=np.nan\ntest_df=test_df[['cousine','id','ingredients']]\ntest_df.columns = ['target', 'id', 'ingredients']\ntest_df[\"num_ingredients\"] = test_df['ingredients'].apply(lambda x: len(x))\nprint (test_df.shape)\ntest_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c390cf1d7d75ac68fbad65e3bddf5b48cc835d09"},"cell_type":"code","source":"#make beck up of test ids\ntest_ids_for_sub = test_df['id'].values\ntest_ids_for_sub","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"642c74be17680024ddd3112a15f98410c79d38a9"},"cell_type":"code","source":"print(\"Combine Train and Submission\")\ndf = pd.concat([train_df, test_df],axis=0,ignore_index=True)\ndel train_df, test_df\ngc.collect()\nprint (df.shape)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"aa31ee4f42b3c452631737997b80106f5d589db0"},"cell_type":"code","source":"df.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f57a056d684d396878b54c76afe617e31128bc7c"},"cell_type":"code","source":"print (df.shape)\nprint(df.head())\nprint(df.tail())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d7bdc4743d94065f89d14bd726201d178b01fc64"},"cell_type":"code","source":"#brutal copy paste from public kernel\nfrom nltk.stem import WordNetLemmatizer\nimport re\nlemmatizer = WordNetLemmatizer()\ndef preprocess(ingredients):\n    ingredients_text = ' '.join(ingredients)\n    ingredients_text = ingredients_text.lower()\n    ingredients_text = ingredients_text.replace('-', ' ')\n    words = []\n    for word in ingredients_text.split():\n        if re.findall('[0-9]', word): continue\n        if len(word) <= 2: continue\n        if '’' in word: continue\n        word = lemmatizer.lemmatize(word)\n        if len(word) > 0: words.append(word)\n    return ' '.join(words)\n\nfor ingredient, expected in [\n    ('Eggs', 'egg'),\n    ('all-purpose flour', 'all purpose flour'),\n    ('purée', 'purée'),\n    ('1% low-fat milk', 'low fat milk'),\n    ('half & half', 'half half'),\n    ('safetida (powder)', 'safetida (powder)')\n]:\n    actual = preprocess([ingredient])\n    assert actual == expected, f'\"{expected}\" is excpected but got \"{actual}\"'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ea6832b128a011b6bd4e1831dee3a113a40542b8"},"cell_type":"code","source":"df['ingredients'] = df['ingredients'].apply(lambda ingredients: preprocess(ingredients))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a43c021d1e8a2813924e76d0118aaf24429d6726"},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b5d56f893fcc31b497b49c6b7b1993b3b71740a3","scrolled":true},"cell_type":"code","source":"df.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"620f0a805d85b1167c65170b2732c9d6811a2de5","scrolled":true},"cell_type":"code","source":"# Feature Engineering \nprint (\"TF-IDF on text data ... \")\ntfidf = TfidfVectorizer(binary=True)\nprint(\"combine other features with text vectors\")\ntext_features = tfidf.fit_transform(df['ingredients'])\nprint (text_features.shape)\ndf_1 = pd.DataFrame(text_features.toarray())\ndel text_features\ngc.collect()\nprint (df_1.shape)\nprint (df.shape)\ndf_1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5850024829f0b10dd86f1791d4eed9b2ef57afeb"},"cell_type":"code","source":"#less brutal copy paste from public kernel, we try to add a different ngram range\nfrom sklearn.pipeline import make_pipeline, make_union\nfrom sklearn.preprocessing import FunctionTransformer, LabelEncoder\nvectorizer = make_pipeline(\n    TfidfVectorizer(sublinear_tf=True, ngram_range =(2,4), max_features=2000),\n    FunctionTransformer(lambda x: x.astype('float32'), validate=False)\n)\ntext_features_2 = vectorizer.fit_transform(df['ingredients'].values)\ntext_features_2.shape\n\ndf_2 = pd.DataFrame(text_features_2.todense())\ndel text_features_2\ngc.collect()\nprint (df_2.shape)\nprint (df.shape)\ndf_2.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d3f21d3f7fd61c806d3b91e7a9cf2cff5737a116"},"cell_type":"code","source":"#combine all toghete... possible because the dataset is relatievely small\ndf = pd.concat([df, df_1, df_2],axis=1)\ndel df_1\ndel df_2\ngc.collect()\nprint (df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3202f6c65a3a3ea07f912eae87f5b1f50f7c3535"},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"20c0ef50b78ea259d814be554b0b19c3285b94be"},"cell_type":"code","source":"df.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f13d1d1e05395cc26d31e9f5f1d037b7e9c32944"},"cell_type":"code","source":"#rename columns, just to know where they come from\ndf.columns =['target', 'id', 'ingredients', 'num_ingredients']+['TfidfV_'+str(n) for n in range(2867)]+['Ngram_'+str(n) for n in range(2000)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5dd3d1effad61ebb0c1f41942615cba4211419a1"},"cell_type":"code","source":"df.drop('ingredients',inplace=True,axis=True)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"75a7e950c608dae7071c3bddd41257db41bc4616"},"cell_type":"code","source":"df.tail()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ba2075c138fb0367758d48db793cf0c436dbbe0f"},"cell_type":"markdown","source":"### First round of optimization\n### In 50 rounds of bayes optimization we are trying to find the best parameters to run the lightgbm model\n"},{"metadata":{"trusted":true,"_uuid":"3d7487f2be84e71dc4733fd73edb5329294457e3"},"cell_type":"code","source":"#now this is wrong!\n#i didnt manage to make the multiclass to work \n#so i'm using a regression to find the best parameters\n\nfrom hyperopt import hp\nfrom hyperopt import tpe\nfrom hyperopt import STATUS_OK\nfrom hyperopt import Trials\nfrom hyperopt import fmin\nimport hyperopt.pyll.stochastic as st\nimport  lightgbm as lgb\nimport csv\nfrom sklearn.preprocessing import OneHotEncoder\ntrain_df = df[df['target'].notnull()]\n\ntarget = train_df['target']\nprint (\"Label Encode the Target Variable ... \")\nlb = LabelEncoder()\ny = lb.fit_transform(target)\nlen_y = len(set(y))\n\n'''\n#good when i will mange to make the multiclass working\n#https://stackoverflow.com/questions/51139150/how-to-write-custom-f1-score-metric-in-light-gbm-python-in-multiclass-classifica\ndef evalerror(preds, dtrain):\n    labels = dtrain.get_label()\n    print (len(labels))\n    print(len(preds))\n    #preds = preds.reshape(-1, len_y)\n    #preds = preds.argmax(axis = 1)\n    f_score = f1_score(labels , preds,  average = 'weighted')\n    return 'f1_score', f_score, True\n'''\n\n\nbayes_trials = Trials()\ntpe_algorithm = tpe.suggest\n\nfinal_selection = [n for n in train_df.columns if n not in ['target', 'id']]\ntrain_set = lgb.Dataset(train_df[final_selection], y)\nprint (train_set)\nmin_value = 100000\niteration = 0\ndef objective(params, n_folds = 5):\n    global iteration\n    iteration+=1\n    \"\"\"Objective function for Gradient Boosting Machine Hyperparameter Tuning\"\"\"\n    #print (iteration)\n    # Perform n_fold cross validation with hyperparameters\n    # Use early stopping and evalute based on ROC AUC\n    params['num_leaves']=int(params['num_leaves'])\n    params['min_data_in_leaf']=int(params['min_data_in_leaf'])\n    params['max_depth']=int(params['max_depth'])\n    params['min_child_weight']=int(params['min_child_weight'])\n    params['colsample_bytree']=round(params['colsample_bytree'],5)\n    params['reg_alpha']=round(params['reg_alpha'],5)\n    params['reg_lambda']=round(params['reg_lambda'],5)\n    params['subsample']=round(params['subsample'],5)  \n    params['learning_rate']=round(params['learning_rate'],5)\n    params['learning_rate']=round(params['learning_rate'],5)\n    #params['objective']= 'multiclass'\n    #params['num_class']=len(train_df['target'].unique())\n    #params['subsample_for_bin']=int(params['subsample_for_bin'])\n    #params['is_unbalance']=True\n    \n    cv_results = lgb.cv(params, \n                        train_set, \n                        nfold = n_folds, \n                        num_boost_round = 10000, \n                        early_stopping_rounds = 200, \n                        metrics='multi_logloss',\n                        #feval=evalerror,\n                        seed = 50,\n                        stratified=False)\n    \n    # Extract the best score\n    #print(cv_results)\n    best_score = min(cv_results['multi_logloss-mean'])\n    optimal_rounds =len(cv_results['multi_logloss-mean']) \n    \n    \n    global min_value\n    if  str(best_score) == '-inf':\n        best_score = min_value+1\n        \n    if  best_score < 0:\n        best_score = min_value+1\n        \n    if best_score < min_value:\n        min_value = best_score\n    print (f'Round: {iteration}\\n the score is: {best_score:.3f}\\n the best score so far {min_value:.3f}')\n    \n    # Loss must be minimized\n    loss= best_score\n    of_connection = open(out_file, 'a')\n    writer = csv.writer(of_connection)\n    \n    writer.writerow([loss, params, best_score, optimal_rounds])\n    of_connection.close()\n    # Dictionary with information for evaluation\n    \n    return {'loss': loss, 'params': params, 'status': STATUS_OK}\n\n\nspace = {\n    'subsample': hp.uniform('subsample', 0.2, 1),\n    'num_leaves': hp.quniform('num_leaves', 5, 150, 1),\n    'learning_rate': hp.loguniform('learning_rate', np.log(0.001), np.log(0.9)),\n    'min_child_samples': hp.quniform('min_child_samples', 1, 100, 1),\n    'min_data_in_leaf': hp.quniform('min_data_in_leaf', 1, 100, 1),\n    'reg_alpha': hp.uniform('reg_alpha', 0.0, 1.0),\n    'reg_lambda': hp.uniform('reg_lambda', 0.0, 1.0),\n    'colsample_bytree': hp.uniform('colsample_bytree', 0.2, 1.0),\n    'max_depth':hp.uniform('max_depth', 2, 15),\n    'min_child_weight':hp.quniform('min_child_weight', 1, 100, 1),\n    #'subsample_for_bin': hp.loguniform('subsample_for_bin', np.log(10), np.log(3000)),\n}\n\n# File to save first results\nout_file = 'gbm_trials_essential.csv'\nof_connection = open(out_file, 'w')\nwriter = csv.writer(of_connection)\n\n# Write the headers to the file\nwriter.writerow(['loss', 'params', 'best_score', 'optimal_rounds'])\nof_connection.close() \n      \nMAX_EVALS = 40\n# Optimize\nbest = fmin(fn = objective,\n            space = space, \n            algo = tpe.suggest, \n            max_evals = MAX_EVALS, \n            trials = bayes_trials)  \n#print (best)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8e5fb214c5e1f259992dc13d9635627a8b757641"},"cell_type":"code","source":"best","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2aa246bcab244e0345e31942d49cdd9f7ce062b3"},"cell_type":"code","source":"params = {}\nparams['num_leaves']=int(best['num_leaves'])\nparams['min_data_in_leaf']=int(best['min_data_in_leaf'])\nparams['max_depth']=int(best['max_depth'])\nparams['min_child_weight']=int(best['min_child_weight'])\nparams['colsample_bytree']=round(best['colsample_bytree'],5)\nparams['reg_alpha']=round(best['reg_alpha'],5)\nparams['reg_lambda']=round(best['reg_lambda'],5)\nparams['subsample']=round(best['subsample'],5)  \nparams['learning_rate']=round(best['learning_rate'],5)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c13285f33ef268783a6726a613a95983cbb0dcec"},"cell_type":"code","source":"len(target.unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"09814247262563014b358549ceaa8822ccb1ef97"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8cd64d95b90711f32506f9a66a5dd3c44bb6afbe"},"cell_type":"code","source":"#function to facilitate cv prediction of lightgbm model\ndef kfold_lightgbm(train_df, test_df, \n                   num_folds, lr = 0.02, \n                   stratified = True,  params={},\n                   n_estimators=5000000, early_stopping_rounds= 200):    \n    # Divide in training/validation and test data\n    #print (test_df.head())\n    print(\"Starting LightGBM. Train shape: {}, test shape: {}\".format(train_df.shape, test_df.shape))\n    #del df\n    gc.collect()\n    # Cross validation model\n    if stratified:\n        folds = StratifiedKFold(n_splits= num_folds, shuffle=True, random_state=1001)\n    else:\n        folds = KFold(n_splits= num_folds, shuffle=True, random_state=1001)\n    # Create arrays and dataframes to store results\n    nclasses = len(train_df['target'].unique())\n    oof_preds = np.zeros( (train_df.shape[0],nclasses))\n    sub_preds = np.zeros( (test_df.shape[0],nclasses))\n    feature_importance_df = pd.DataFrame()\n    feats = [f for f in train_df.columns if f not in ['target', 'id']]\n    \n    print ('len_feat', len(feats))\n    feature_importance_df['f']=feats\n    \n    \n    y = train_df['target']\n\n    train_df = train_df[feats]\n    \n    \n    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(train_df, y)):\n        train_x, train_y = train_df.iloc[train_idx], y.iloc[train_idx]\n        valid_x, valid_y = train_df.iloc[valid_idx], y.iloc[valid_idx]\n\n        # LightGBM parameters found by Bayesian optimization\n        clf = LGBMClassifier(\n            nthread=10,\n            n_estimators=n_estimators,\n            silent=-1,\n            objective='multiclass',\n            num_leaves=params['num_leaves'],\n            min_data_in_leaf=params['min_data_in_leaf'],\n            max_depth=params['max_depth'],\n            min_child_weight=params['min_child_weight'],\n            colsample_bytree=params['colsample_bytree'],\n            reg_alpha=params['reg_alpha'],\n            reg_lambda=params['reg_lambda'],\n            subsample=params['subsample'],  \n            learning_rate=params['learning_rate']     \n        \n        )\n\n        clf.fit(train_x, train_y, eval_set=[(train_x, train_y), (valid_x, valid_y)], \n             verbose= 100, early_stopping_rounds= early_stopping_rounds)\n\n        oof_preds[valid_idx] = clf.predict_proba(valid_x, num_iteration=clf.best_iteration_)\n        preds = clf.predict_proba(test_df[feats], num_iteration=clf.best_iteration_)\n        #print (preds)\n        sub_preds += preds\n        #[:, 1] / float(folds.n_splits)\n        #print('inside', len(feats))\n        #print ('len clf.feature_importances_', len(clf.feature_importances_))\n        feature_importance_df[\"fold\"+str(n_fold)] = clf.feature_importances_\n\n        del train_x, train_y, valid_x, valid_y\n        gc.collect()\n    \n    sub_preds = sub_preds/float(folds.n_splits)\n    return feature_importance_df, clf, sub_preds, oof_preds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"92003d4bc85f8a7fac74b219cf72dc3f99545ad5"},"cell_type":"code","source":"test_df = df[df['target'].isnull()]\ndel df\ngc.collect()\ntest_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ab6513f14e069ab70820a847cc110310e1c39536"},"cell_type":"code","source":"test_df.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f4e5ba65d6eddb4650e829e03ec7b947860f2a5b"},"cell_type":"code","source":"#### Pass the best parameters to a lightgbm model\n#### Get cv prediction and feature importance","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a68038578732e368afe36320a7fe9d554b2923c6"},"cell_type":"code","source":"\nfeat_importance, clf, sub_preds, oof_preds = kfold_lightgbm(train_df, test_df, num_folds= 5, lr=0.01, \n                                 stratified= True, params=params,\n                                 #n_estimators=200, early_stopping_rounds= 2\n                                                                  )\nfeat_importance.to_csv('fimp_all.csv') ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e51d9b9f5d5fda8bd3696d2a4c8cde2f1ed70a83"},"cell_type":"code","source":"#### Again, Pass the best parameters to a lightgbm model after scramble of target\n#### Get cv prediction and feature importance","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ff24c92c8ff8df1c88fb5fcc092add2a4f98340c"},"cell_type":"code","source":"train_df['target'] = train_df['target'].sample(frac=1,random_state=1976).values\nfeat_importance_random, clf, sub_preds, oof_preds = kfold_lightgbm(train_df, test_df, num_folds= 5, lr=0.01, \n                                 stratified= True, params=params,\n                                 #n_estimators=200, early_stopping_rounds= 2\n                                                                  )\nfeat_importance_random.to_csv('fimp_all_random.csv') ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"334b2589dcb86d634f5031d1bf1d81f8e1ab231b"},"cell_type":"code","source":"#### Select the best feature comparing the normal scores vs the scrambled score ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"10ae0a7d6781ba105784c5b5c05350eb88a37232"},"cell_type":"code","source":"feat_importance.set_index('f',inplace=True)\nfeat_importance_random.set_index('f',inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e00e7489ce7232ac762c0f46217aa17d0f312be5"},"cell_type":"code","source":"from scipy import stats\nfrom statsmodels.sandbox.stats.multicomp import multipletests\nfrom tqdm import tqdm\n\nimportance = pd.DataFrame()\n#selection_df['f']=feat_importance.index.values\nstatistics = []\npvalues = []\nfor f in tqdm(feat_importance.index.values):\n    statistic, pvalue = stats.ks_2samp(feat_importance.loc[f], feat_importance_random.loc[f])\n    statistics.append(statistic)\n    pvalues.append(pvalue)\nimportance['sum']=feat_importance.sum(axis=1).values\nimportance['random_sum']=feat_importance_random.sum(axis=1).values\nimportance['f']=feat_importance.index.values\nimportance.set_index('f',inplace=True)\n#just for some plotting, we add to the sum the minimum value of the random dataset\nimportance['sum']=importance['sum']+importance['random_sum'].min()\nimportance['random_sum']=importance['random_sum']+importance['random_sum'].min()\nimportance['fc']=(importance['sum']+1)/(importance['random_sum']+1)\nimportance['pval']=pvalues\nimportance['ks_stat']=statistics\npadj = multipletests(importance['pval'], method='bonferroni')\nimportance['padj']=padj[1]\nimportance.sort_values('pval').head()    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ccf8774f53c6368b7a5597d085a0303680c0756c"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0a4b786a47daeec1b116c095b29998972b5579ab"},"cell_type":"code","source":"importance.sort_values('fc').tail(20)  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2988f2b8c4fc2143a23890810ea4b268ecdaa817"},"cell_type":"code","source":"importance.loc['num_ingredients']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e37e64db472e7817d2aca34aebf17cd152ed6b35"},"cell_type":"code","source":"np.log2(importance['fc']).plot(kind='hist')\nprint(importance[importance['padj']<0.05].shape)\nprint(importance[np.log2(importance['fc'])>1].shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4ec5be032c3bdd9f92e386c430e53de50f6c324e"},"cell_type":"markdown","source":"### Generally a thresold on fold change is enough\n### It is possible to explore a threshold on the p-value and p-value bonferroni adjusted of ks_2samp test"},{"metadata":{"trusted":true,"_uuid":"15fb64c30b760564eed2b86d31816284c62c819f"},"cell_type":"code","source":"feature_identified = list(importance[importance['fc']>1].index.values)\nprint (len(feature_identified))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"46c8f1f89bc9aa3f74aaea07be64122d3e7fee12"},"cell_type":"markdown","source":"### Run again bayes optimization with the selected features"},{"metadata":{"trusted":true,"_uuid":"c77ea240f04fbb2c17cd3567f4bb9040d9634cc6"},"cell_type":"code","source":"train_set = lgb.Dataset(train_df[feature_identified], y)\nprint (train_set)\nmin_value = 100000\niteration = 0\ndef objective(params, n_folds = 5):\n    global iteration\n    iteration+=1\n    \"\"\"Objective function for Gradient Boosting Machine Hyperparameter Tuning\"\"\"\n    #print (iteration)\n    # Perform n_fold cross validation with hyperparameters\n    # Use early stopping and evalute based on ROC AUC\n    params['num_leaves']=int(params['num_leaves'])\n    params['min_data_in_leaf']=int(params['min_data_in_leaf'])\n    params['max_depth']=int(params['max_depth'])\n    params['min_child_weight']=int(params['min_child_weight'])\n    params['colsample_bytree']=round(params['colsample_bytree'],5)\n    params['reg_alpha']=round(params['reg_alpha'],5)\n    params['reg_lambda']=round(params['reg_lambda'],5)\n    params['subsample']=round(params['subsample'],5)  \n    params['learning_rate']=round(params['learning_rate'],5)\n    params['learning_rate']=round(params['learning_rate'],5)\n    #params['objective']= 'multiclass'\n    #params['num_class']=len(train_df['target'].unique())\n    #params['subsample_for_bin']=int(params['subsample_for_bin'])\n    #params['is_unbalance']=True\n    \n    cv_results = lgb.cv(params, \n                        train_set, \n                        nfold = n_folds, \n                        num_boost_round = 1000000, \n                        early_stopping_rounds = 200, \n                        metrics='multi_logloss',\n                        #feval=evalerror,\n                        seed = 50,\n                        stratified=False)\n    \n    # Extract the best score\n    #print(cv_results)\n    best_score = min(cv_results['multi_logloss-mean'])\n    optimal_rounds =len(cv_results['multi_logloss-mean']) \n    \n    \n    global min_value\n    if  str(best_score) == '-inf':\n        best_score = min_value+1 \n        \n    if  best_score < 0:\n        best_score = min_value+1        \n        \n    if best_score < min_value:\n        min_value = best_score\n    print (f'Round: {iteration}\\n the score is: {best_score:.3f}\\n the best score so far {min_value:.3f}')\n    \n    # Loss must be minimized\n    loss= best_score\n    of_connection = open(out_file, 'a')\n    writer = csv.writer(of_connection)\n    \n    writer.writerow([loss, params, best_score, optimal_rounds])\n    of_connection.close()\n    # Dictionary with information for evaluation\n    \n    return {'loss': loss, 'params': params, 'status': STATUS_OK}\n\n\nspace = {\n    'subsample': hp.uniform('subsample', 0.2, 1),\n    'num_leaves': hp.quniform('num_leaves', 5, 150, 1),\n    'learning_rate': hp.loguniform('learning_rate', np.log(0.001), np.log(0.9)),\n    'min_child_samples': hp.quniform('min_child_samples', 1, 100, 1),\n    'min_data_in_leaf': hp.quniform('min_data_in_leaf', 1, 100, 1),\n    'reg_alpha': hp.uniform('reg_alpha', 0.0, 1.0),\n    'reg_lambda': hp.uniform('reg_lambda', 0.0, 1.0),\n    'colsample_bytree': hp.uniform('colsample_bytree', 0.2, 1.0),\n    'max_depth':hp.uniform('max_depth', 2, 15),\n    'min_child_weight':hp.quniform('min_child_weight', 1, 100, 1),\n    #'subsample_for_bin': hp.loguniform('subsample_for_bin', np.log(10), np.log(3000)),\n}\n\n# File to save first results\nout_file = 'gbm_trials_essential.csv'\nof_connection = open(out_file, 'w')\nwriter = csv.writer(of_connection)\n\n# Write the headers to the file\nwriter.writerow(['loss', 'params', 'best_score', 'optimal_rounds'])\nof_connection.close() \n      \nMAX_EVALS = 150\n# Optimize\nbest = fmin(fn = objective,\n            space = space, \n            algo = tpe.suggest, \n            max_evals = MAX_EVALS, \n            trials = bayes_trials)  \nprint (best)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b599b1615a85262c2c932b1e7eca7a4759f0bf64"},"cell_type":"code","source":"params = {}\nparams['num_leaves']=int(best['num_leaves'])\nparams['min_data_in_leaf']=int(best['min_data_in_leaf'])\nparams['max_depth']=int(best['max_depth'])\nparams['min_child_weight']=int(best['min_child_weight'])\nparams['colsample_bytree']=round(best['colsample_bytree'],5)\nparams['reg_alpha']=round(best['reg_alpha'],5)\nparams['reg_lambda']=round(best['reg_lambda'],5)\nparams['subsample']=round(best['subsample'],5)  \nparams['learning_rate']=round(best['learning_rate'],5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bcd8adfa66d4ff0f7041bbd7a9897d7d4d684cc6"},"cell_type":"code","source":"### run the final model, add back the non scarmbled target","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"91fc0be8dba9a124d6f7bbb7fb77052d1840792d"},"cell_type":"code","source":"train_df['target']=y_bk\nfeat_importance, clf, sub_preds, oof_preds = kfold_lightgbm(train_df[feature_identified+['target']], \n                                                            test_df[feature_identified+['target']],\n                                                            num_folds= 5, lr=0.01, stratified= True,params=params,\n                                                           #n_estimators=200, early_stopping_rounds= 2\n                                                           )\n                                 \n\nfeat_importance.to_csv('fimp_final.csv') ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7c4719548d48fc51906a629bc80bdb086c0b3c09"},"cell_type":"code","source":"print(clf.classes_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"af3d6b35ab575141465943b80f08fe39f25b0933"},"cell_type":"code","source":"sub = pd.DataFrame()\nfor index,c in enumerate(clf.classes_):\n    sub[c]=sub_preds[:,index]\ncuisine = []\nfor item in sub.index.values:\n    cuisine.append(np.argmax(sub.loc[item]))\nsub['cuisine1']=cuisine\nsub['id']=test_ids_for_sub","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bad9af8694b74ea01bd730bb394618a93bc6ee56"},"cell_type":"code","source":"### Blend with the top score models","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b021f9d7e8c5daf3a3bfbea14df426d677c96684"},"cell_type":"code","source":"temp_1 = pd.read_csv('../input/svm-12jul18/svm_output_None.csv')\ntemp_2 = pd.read_csv('../input/let-s-cook-model/submission.csv')\nsub['cuisine2']=temp_1['cuisine']\nsub['cuisine3']=temp_2['cuisine']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"83a1737e9f40d04e18cf5a805383976f3ab7fdc0"},"cell_type":"code","source":"sub['diff'] = [1 if a != b else 0 for a,b in zip(sub['cuisine2'], sub['cuisine3'])]\nsub['diff'].value_counts()\nsub[sub['diff']==1][['cuisine1','cuisine2','cuisine3']].head(50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eb99d2c46dc065a7f196d1f34c120948651fca6f"},"cell_type":"code","source":"cuisine = []\nfor index in sub.index.values:\n    temp = sub[['cuisine1', 'cuisine2', 'cuisine3']].iloc[index].value_counts()\n    temp_score = temp[0]\n    temp_res = temp.index.values[0]\n    if temp_score ==1:\n        temp_res = sub[['cuisine3']].iloc[index].values[0]\n    cuisine.append(temp_res)\nsub['cuisine']=cuisine\nsub[['id','cuisine']].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fd0e782798c1c53cd7ef1e9119fc4135f5b7a16e"},"cell_type":"code","source":"### et voila","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ab3a73094f88efe28fcd1d7ba19e3601fa730225"},"cell_type":"code","source":"sub[['id','cuisine']].to_csv('finalsub.csv',index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}