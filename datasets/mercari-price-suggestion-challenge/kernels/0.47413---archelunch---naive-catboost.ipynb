{"metadata": {"language_info": {"name": "python", "mimetype": "text/x-python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "version": "3.6.3"}, "kernelspec": {"name": "python3", "language": "python", "display_name": "Python 3"}}, "nbformat_minor": 1, "nbformat": 4, "cells": [{"metadata": {"_uuid": "a310d0c74ea89b076e00d89264dd66906dc589aa", "_cell_guid": "722432fc-69c1-434f-bd5d-1af6fa1945de"}, "cell_type": "markdown", "source": ["In this simple notebook, we will use CatBoost to predict the price using only categorical features."]}, {"metadata": {"_uuid": "f45bc2bed9e9205110da41317e609416cd8cd271", "collapsed": true, "_cell_guid": "6b7a283b-2e58-4359-a268-060abdcba95e"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["import numpy as np\n", "import pandas as pd\n", "import matplotlib.pyplot as plt\n", "import catboost as cboost\n", "import gc\n", "from scipy.sparse import csr_matrix, hstack\n", "from sklearn.linear_model import Ridge\n", "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n", "from sklearn.preprocessing import LabelBinarizer\n", "from sklearn.model_selection import train_test_split, cross_val_score\n", "import xgboost as xgb\n", "from sklearn.gaussian_process import GaussianProcessRegressor\n", "from sklearn.kernel_ridge import KernelRidge\n", "from sklearn.ensemble import RandomForestRegressor\n", "%matplotlib inline"]}, {"metadata": {"_uuid": "2dcaa7a5a34aa4608dfdfea3325e3026e6c2cec8", "collapsed": true, "_cell_guid": "7b4aeeec-fee3-45f4-9e7a-9f7a4c876ec1"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["def handle_missing_inplace(dataset):\n", "    dataset['category_name'].fillna(value='missing', inplace=True)\n", "    dataset['brand_name'].fillna(value='missing', inplace=True)\n", "    dataset['item_description'].fillna(value='missing', inplace=True)\n", "\n", "\n", "def cutting(dataset):\n", "    pop_brand = dataset['brand_name'].value_counts().loc[lambda x: x.index != 'missing'].index[:NUM_BRANDS]\n", "    dataset.loc[~dataset['brand_name'].isin(pop_brand), 'brand_name'] = 'missing'\n", "    pop_category = dataset['category_name'].value_counts().loc[lambda x: x.index != 'missing'].index[:NUM_BRANDS]\n", "    dataset.loc[~dataset['category_name'].isin(pop_category), 'category_name'] = 'missing'\n", "\n", "\n", "def to_categorical(dataset):\n", "    dataset['category_name'] = dataset['category_name'].astype('category')\n", "    dataset['brand_name'] = dataset['brand_name'].astype('category')\n", "    dataset['item_condition_id'] = dataset['item_condition_id'].astype('category')"]}, {"metadata": {"_uuid": "271d00e091e2f488d916f7782a3581b79e7287f5", "collapsed": true, "_cell_guid": "d99d8662-defd-4765-bcc4-896b8345329b"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["train = pd.read_table('../input/train.tsv', engine='c')\n", "test = pd.read_table('../input/test.tsv', engine='c')\n", "nrow_train = train.shape[0]"]}, {"metadata": {"_uuid": "6dc9cd3410e7e5e0367fc33660017ca79b1ae390", "collapsed": true, "_cell_guid": "b96100c8-f926-4ba4-98b9-c4bd2cbadffa"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["y = np.log1p(train[\"price\"])\n", "merge: pd.DataFrame = pd.concat([train, test])\n", "submission: pd.DataFrame = test[['test_id']]"]}, {"metadata": {"_uuid": "4e0cb24a4301a61726ab77be2904f4b991a75dda", "collapsed": true, "_cell_guid": "bc4bd951-74cd-49ca-858c-8c65c312def0"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["NUM_BRANDS = 4000\n", "NUM_CATEGORIES = 1000\n", "NAME_MIN_DF = 10\n", "MAX_FEATURES_ITEM_DESCRIPTION = 50000\n", "handle_missing_inplace(merge)\n", "cutting(merge)\n", "to_categorical(merge)"]}, {"metadata": {"_uuid": "ea84261ba05dcaf9c297431b80e45819d0ee3a40", "collapsed": true, "_cell_guid": "ada95393-c8ee-4ceb-954f-60f48b115c67"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["cv = CountVectorizer(min_df=NAME_MIN_DF)\n", "X_name = cv.fit_transform(merge['name'])\n", "cv = CountVectorizer()\n", "X_category = cv.fit_transform(merge['category_name'])\n", "tv = TfidfVectorizer(max_features=MAX_FEATURES_ITEM_DESCRIPTION,\n", "                         ngram_range=(1, 3),\n", "                         stop_words='english')\n", "X_description = tv.fit_transform(merge['item_description'])\n", "lb = LabelBinarizer(sparse_output=True)\n", "X_brand = lb.fit_transform(merge['brand_name'])\n", "X_dummies = csr_matrix(pd.get_dummies(merge[['item_condition_id', 'shipping']],\n", "                                          sparse=True).values)\n", "sparse_merge = hstack((X_dummies, X_description, X_brand, X_category, X_name)).tocsr()\n", "X = sparse_merge[:nrow_train]\n", "X_test = sparse_merge[nrow_train:]"]}, {"metadata": {"_uuid": "70041f428ae9bb8a7092457f27b0665ede5bb475", "collapsed": true, "_cell_guid": "bf57d489-3901-4952-bd5a-53bfc2929eca"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["model = Ridge(solver=\"sag\", fit_intercept=True, random_state=205, alpha=3)\n", "model.fit(X, y)\n", "predsR = model.predict(X=X_test)\n", "model = Ridge(solver=\"lsqr\", fit_intercept=False, random_state=145, alpha = 3)\n", "model.fit(X, y)\n", "predsR2 = model.predict(X=X_test)\n", "model = Ridge(solver=\"sag\", fit_intercept=False, random_state=205, alpha = 3)\n", "model.fit(X, y)\n", "predsR3 = model.predict(X=X_test)"]}, {"metadata": {"_uuid": "5fab225b655dc8647acf655f5b6803f3878a1a90", "collapsed": true, "_cell_guid": "34d10ae4-33f2-4888-95bf-6bdfe50471ec"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["model = RandomForestRegressor(max_features='log2', min_weight_fraction_leaf=0.1)\n", "model.fit(X, y)\n", "predsL = model.predict(X=X_test)\n", "# model = GaussianProcessRegressor(random_state=145, alpha = 3)\n", "# model.fit(X, y)\n", "# predsL2 = model.predict(X=X_test)\n"]}, {"metadata": {"_uuid": "95fda291e4ea580a20b1fdb5effb73d5486b2db3", "collapsed": true, "_cell_guid": "b563168c-aac5-42dd-aaac-5e183a8ec4a6"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": []}, {"metadata": {"_uuid": "b636ea1afa4648e4db637cd54c24fb4932416d36", "collapsed": true, "_cell_guid": "a8b7ad0f-a869-443c-baaa-a240db8f6174"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# # Create train and test Pool of train\n", "# ptrain = cboost.Pool(pd.DataFrame(X.toarray()), y)\n", "# ptest = cboost.Pool(pd.DataFrame(X_test.toarray()))"]}, {"metadata": {"_uuid": "049ee092618c4a499e0f86c24b9e4b7e94b905e9", "collapsed": true, "_cell_guid": "abf4f92f-5c9f-43e0-a248-ae780c8a5523"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# # Tune your parameters here!\n", "# cboost_params = {\n", "#     'nan_mode': 'Min',\n", "#     'loss_function': 'RMSE',  # Try 'LogLinQuantile' as well\n", "#     'iterations': 500,\n", "#     'learning_rate': 0.76,\n", "#     'depth': 3,\n", "#     'verbose': True\n", "# }\n", "\n", "# cboost_params2 = {\n", "#     'nan_mode': 'Min',\n", "#     'loss_function': 'RMSE',  # Try 'LogLinQuantile' as well\n", "#     'iterations': 500,\n", "#     'learning_rate': 0.85,\n", "#     'depth': 3,\n", "#     'verbose': True\n", "# }\n", "# best_iter = cboost_params['iterations']  # Initial 'guess' it not using CV\n", "# best_iter2 = cboost_params2['iterations']\n", "# # cv_result = cboost.cv(cboost_params, ptrain_sub, fold_count=3)\n", "\n", "# # df_cv_result = pd.DataFrame({'train': cv_result['RMSE_train_avg'],\n", "# #                              'valid': cv_result['RMSE_test_avg']})\n", "\n", "# # # Best results\n", "# # print('Best results:')\n", "# # best_iter = df_cv_result.valid.argmin()+1\n", "# # df_cv_bestresult = df_cv_result.iloc[best_iter-1]\n", "# # print(df_cv_bestresult)\n", "\n", "# # fig, ax = plt.subplots(1, 2, figsize=(15, 6))\n", "# # df_cv_result.plot(ax=ax[0])\n", "\n", "# # ax[1].plot(df_cv_result.train, df_cv_result.valid, 'o-')\n", "# # ax[1].scatter([df_cv_bestresult['train']], [df_cv_bestresult['valid']], c='red')\n", "# # ax[1].set_xlabel('train')\n", "# # ax[1].set_ylabel('valid')"]}, {"metadata": {"_uuid": "d752d3776f02dcd4bac7a7a8083b7e94dd4b39ea", "collapsed": true, "_cell_guid": "3ded0fe6-e4a0-4f7d-bb18-aa4a675dfff7"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# # Train model on full data\n", "# model = cboost.CatBoostRegressor(**dict(cboost_params, verbose=False, iterations=best_iter))\n", "# fit_model = model.fit(ptrain)\n", "# predsL = fit_model.predict(ptest).clip(0)\n", "# # Train model on full data\n", "# model = cboost.CatBoostRegressor(**dict(cboost_params2, verbose=False, iterations=best_iter2))\n", "# fit_model = model.fit(ptrain)\n", "# predsL2 = fit_model.predict(ptest).clip(0)"]}, {"metadata": {"_uuid": "c62c85b578b555883434834ddd45843d5bcaab1c", "collapsed": true, "_cell_guid": "b26427ac-3910-46b3-a69f-92439eb5915d"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Predict test and save to .csv\n", "preds1 = np.expm1(predsR3*0.24 + predsR2*0.24 + predsR*0.52)\n", "preds2 = np.expm1(predsL) #+ predsL2*0.5)\n", "submission['price'] = preds1*0.75 + preds2*0.25\n", "submission.to_csv('submission.csv', index=False)"]}, {"metadata": {"_uuid": "381489c655f110e6c0d911d4614fe765f56619f4", "collapsed": true, "_cell_guid": "fa9f74ae-ce84-4929-939d-a5e4d0f9c628"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["!head submission.csv"]}]}