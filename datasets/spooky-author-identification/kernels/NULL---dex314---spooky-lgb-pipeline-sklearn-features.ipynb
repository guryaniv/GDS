{"cells": [{"source": ["This is just a brief dive, some visual fun, and then a quick model. "], "cell_type": "markdown", "metadata": {}}, {"outputs": [], "metadata": {}, "cell_type": "code", "source": ["# This Python 3 environment comes with many helpful analytics libraries installed\n", "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n", "# For example, here's several helpful packages to load in \n", "import pandas as pd\n", "import numpy as np\n", "import lightgbm as lgb\n", "import matplotlib.pyplot as plt\n", "%matplotlib inline\n", "import seaborn as sns\n", "\n", "from nltk.corpus import stopwords\n", "from collections import Counter\n", "\n", "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n", "import cv2\n", "\n", "from sklearn.model_selection import train_test_split\n", "from sklearn.model_selection import train_test_split\n", "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n", "from sklearn.decomposition import TruncatedSVD, LatentDirichletAllocation, NMF\n", "from sklearn import preprocessing\n", "from sklearn import pipeline\n", "from sklearn.base import BaseEstimator, TransformerMixin\n", "\n", "# Input data files are available in the \"../input/\" directory.\n", "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n", "from subprocess import check_output\n", "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n", "# Any results you write to the current directory are saved as output."], "execution_count": 15}, {"outputs": [], "metadata": {}, "cell_type": "code", "source": ["trainDf = pd.read_csv('../input/spooky-author-identification/train.csv', index_col = 0)\n", "trainDf.head()"], "execution_count": 16}, {"outputs": [], "metadata": {"collapsed": true}, "cell_type": "code", "source": ["def textClean(text):\n", "    text=text.lower().split()\n", "    stops = {'so', 'his', 't', 'y', 'ours', 'herself', \n", "         'your', 'all', 'some', 'they', 'i', 'of', 'didn', \n", "         'them', 'when', 'will', 'that', 'its', 'because', \n", "         'while', 'those', 'my', 'don', 'again', 'her', 'if',\n", "         'further', 'now', 'does', 'against', 'won', 'same', \n", "         'a', 'during', 'who', 'here', 'have', 'in', 'being', \n", "         'it', 'other', 'once', 'itself', 'hers', 'after', 're',\n", "         'just', 'their', 'himself', 'theirs', 'whom', 'then', 'd', \n", "         'out', 'm', 'mustn', 'where', 'below', 'about', 'isn',\n", "         'shouldn', 'wouldn', 'these', 'me', 'to', 'doesn', 'into',\n", "         'the', 'until', 'she', 'am', 'under', 'how', 'yourself',\n", "         'couldn', 'ma', 'up', 'than', 'from', 'themselves', 'yourselves',\n", "         'off', 'above', 'yours', 'having', 'mightn', 'needn', 'on', \n", "         'too', 'there', 'an', 'and', 'down', 'ourselves', 'each',\n", "         'hadn', 'ain', 'such', 've', 'did', 'be', 'or', 'aren', 'he', \n", "         'should', 'for', 'both', 'doing', 'this', 'through', 'do', 'had',\n", "         'own', 'but', 'were', 'over', 'not', 'are', 'few', 'by', \n", "         'been', 'most', 'no', 'as', 'was', 'what', 's', 'is', 'you', \n", "         'shan', 'between', 'wasn', 'has', 'more', 'him', 'nor',\n", "         'can', 'why', 'any', 'at', 'myself', 'very', 'with', 'we', \n", "         'which', 'hasn', 'weren', 'haven', 'our', 'll', 'only',\n", "         'o', 'before'}\n", "#     stops = set(stopwords.words(\"English\"))\n", "    text = [w for w in text if not w in stops]\n", "    text = \" \".join(text)\n", "    return text"], "execution_count": 19}, {"outputs": [], "metadata": {}, "cell_type": "code", "source": ["texts = []\n", "for t in trainDf.text:\n", "    texts.append(textClean(t))"], "execution_count": 20}, {"outputs": [], "metadata": {}, "cell_type": "code", "source": ["texts[0][:100]"], "execution_count": 21}, {"outputs": [], "metadata": {}, "cell_type": "code", "source": ["tops = Counter(str(texts).split()).most_common()[:30]\n", "labs, vals = zip(*tops)\n", "idx = np.arange(len(labs))\n", "wid=0.6\n", "fig, ax=plt.subplots(1,1,figsize=(14,8))\n", "ax=plt.bar(idx, vals, wid, color='orange')\n", "ax=plt.xticks(idx - wid/8, labs, rotation=30, size=14)\n", "plt.title('Top Thirty Counts of Most-Common Words Among Text')"], "execution_count": 22}, {"source": ["Once __UPON__ a midnight dreary... This is great. The words dont seem all that specific though."], "cell_type": "markdown", "metadata": {}}, {"outputs": [], "metadata": {}, "cell_type": "code", "source": ["plt.figure(figsize=(12,8))\n", "ax = sns.countplot(x=\"author\", data=trainDf, \n", "#                    color='purple',\n", "                   palette=\"Greys\")\n", "plt.ylabel('Frequency'); plt.xlabel('Author')\n", "plt.title('Freq. of Authors')\n", "plt.show()"], "execution_count": 23}, {"source": ["A little more __POE__ than __Shelley__ and __Lovecraft__."], "cell_type": "markdown", "metadata": {}}, {"outputs": [], "metadata": {}, "cell_type": "code", "source": ["trainDf['text'].to_csv('wc_text.txt')\n", "img = cv2.imread(\"..\")\n", "spookyColors = 255.0-cv2.imread(\"../input/pumpkin-pic/hp.jpg\")"], "execution_count": 26}, {"outputs": [], "metadata": {}, "cell_type": "code", "source": ["wc_text = open('wc_text.txt').read()\n", "wordcloud = WordCloud(background_color=\"black\",\n", "                      width=1200,height=800, mask=spookyColors).generate(wc_text)\n", "image_colors = ImageColorGenerator(spookyColors)"], "execution_count": 27}, {"outputs": [], "metadata": {}, "cell_type": "code", "source": ["fig, ax=plt.subplots(1,1,figsize=(16,16))\n", "ax.grid(False);\n", "ax.imshow(wordcloud.recolor(color_func=image_colors), interpolation=\"bilinear\");"], "execution_count": 28}, {"source": ["[](http://)LOL....sweeeet. Its a pumpkin."], "cell_type": "markdown", "metadata": {}}, {"source": ["### And now for some modeling..."], "cell_type": "markdown", "metadata": {}}, {"outputs": [], "metadata": {}, "cell_type": "code", "source": ["trainDf = pd.read_csv('../input/spooky-author-identification/train.csv', index_col = 0)\n", "trainDf.head()"], "execution_count": 30}, {"outputs": [], "metadata": {"collapsed": true}, "cell_type": "code", "source": ["lbl = preprocessing.LabelEncoder()\n", "y = lbl.fit_transform(trainDf.author)"], "execution_count": 31}, {"outputs": [], "metadata": {}, "cell_type": "code", "source": ["testDf = pd.read_csv('../input/spooky-author-identification/test.csv',index_col=0)\n", "pid = testDf.index"], "execution_count": 33}, {"outputs": [], "metadata": {"collapsed": true}, "cell_type": "code", "source": ["texts = []\n", "for t in trainDf.text:\n", "    texts.append(textClean(t))\n", "testText = []\n", "for t in testDf.text:\n", "    testText.append(textClean(t))"], "execution_count": 34}, {"outputs": [], "metadata": {}, "cell_type": "code", "source": ["testText[0][:100]"], "execution_count": 35}, {"outputs": [], "metadata": {}, "cell_type": "code", "source": ["trainDf['newText'] = texts\n", "trainDf.drop(['author','text'],axis=1,inplace=True)\n", "trainDf.head()"], "execution_count": 36}, {"outputs": [], "metadata": {}, "cell_type": "code", "source": ["testDf['newText'] = testText\n", "testDf.drop(['text'],axis=1,inplace=True)\n", "testDf.head()"], "execution_count": 37}, {"outputs": [], "metadata": {}, "cell_type": "code", "source": ["ax = sns.countplot(x=y, color='black')\n", "## Quick Confirmation"], "execution_count": 38}, {"source": ["### Build some features..."], "cell_type": "markdown", "metadata": {}}, {"outputs": [], "metadata": {"collapsed": true}, "cell_type": "code", "source": ["cvec = CountVectorizer(analyzer=u'char', ngram_range=(1, 8), max_features=1000,\n", "                       strip_accents='unicode', stop_words='english',\n", "                       token_pattern=r'\\w+')\n", "\n", "tfidf = TfidfVectorizer(ngram_range=(1, 3), max_features=1000, \n", "                        strip_accents='unicode',\n", "                        lowercase =True, analyzer='word', token_pattern=r'\\w+',\n", "                        use_idf=True, smooth_idf=True, sublinear_tf=True, \n", "                        stop_words = 'english')\n", "\n", "nmfNC = 50\n", "nmf = NMF(n_components=nmfNC, random_state=42,\n", "          alpha=.1, l1_ratio=.5)\n", "ldaNT = 50\n", "lda = LatentDirichletAllocation(n_topics=ldaNT, max_iter=10,\n", "                                learning_method='online',\n", "                                learning_offset=50.,\n", "                                random_state=42)\n", "\n", "textNC = 150\n", "tsvdText = TruncatedSVD(n_components=textNC, n_iter=25, random_state=42)\n", "tsvdCount = TruncatedSVD(n_components=textNC, n_iter=25, random_state=42)"], "execution_count": 39}, {"outputs": [], "metadata": {}, "cell_type": "code", "source": ["class cust_txt_col(BaseEstimator, TransformerMixin):\n", "    def __init__(self, key):\n", "        self.key = key\n", "    def fit(self, x, y=None):\n", "        print('fit...')\n", "        return self\n", "    def transform(self, x):\n", "        print('transform...')\n", "        return x[self.key].apply(str)\n", "    \n", "class cust_regression_vals(BaseEstimator, TransformerMixin):\n", "    def fit(self, x, y=None):\n", "        print('fit...')\n", "        return self\n", "    def transform(self, x):\n", "        print('transform and drop...')\n", "        x = x.drop(['newText'],axis=1).values\n", "        return x\n", "    \n", "print('Pipeline...')\n", "fp = pipeline.Pipeline([\n", "    ('union', pipeline.FeatureUnion(\n", "#            n_jobs = -1,\n", "        transformer_list = [\n", "            ('standard', cust_regression_vals()),\n", "            ('pip1', pipeline.Pipeline([('newText', cust_txt_col('newText')),('counts', cvec),('tsvdCountText', tsvdCount)])),       \n", "            ('pip2', pipeline.Pipeline([('nmf_Text', cust_txt_col('newText')),('tfidf_Text', tfidf),('nmfText', nmf)])),\n", "            ('pip3', pipeline.Pipeline([('lda_Text', cust_txt_col('newText')),('tfidf_Text', tfidf),('ldaText', lda)])),\n", "            ('pip4', pipeline.Pipeline([('newText', cust_txt_col('newText')),('tfidf_Text', tfidf),('tsvdText', tsvdText)]))\n", "        ])\n", "    )])"], "execution_count": 40}, {"outputs": [], "metadata": {"collapsed": true}, "cell_type": "code", "source": ["for c in trainDf.columns:\n", "    if c == 'newText':\n", "        trainDf[c+'_len'] = trainDf[c].map(lambda x: len(str(x)))\n", "        trainDf[c+'_words'] = trainDf[c].map(lambda x: len(str(x).split(' ')))        \n", "        \n", "for c in testDf.columns:\n", "    if c == 'newText':\n", "        testDf[c+'_len'] = testDf[c].map(lambda x: len(str(x)))\n", "        testDf[c+'_words'] = testDf[c].map(lambda x: len(str(x).split(' ')))     "], "execution_count": 41}, {"outputs": [], "metadata": {}, "cell_type": "code", "source": ["trainDf.head()"], "execution_count": 42}, {"outputs": [], "metadata": {"scrolled": false}, "cell_type": "code", "source": ["train = fp.fit_transform(trainDf); print(train.shape)\n", "test = fp.transform(testDf); print(test.shape)"], "execution_count": 43}, {"source": ["This LGB model is taking quite a long time to run in this notebook and Im not sure why."], "cell_type": "markdown", "metadata": {}}, {"outputs": [], "metadata": {"collapsed": true}, "cell_type": "code", "source": ["params = {'learning_rate':0.05\n", "         ,'max_depth':4\n", "         ,'objective':'multiclass'\n", "         ,'num_class':3\n", "         ,'metric':{'multi_logloss'}\n", "#          ,'num_iterations':256\n", "         ,'num_leaves':128\n", "         ,'min_data_in_leaf':128\n", "         ,'bagging_fraction':0.85 \n", "         ,'feature_fraction':0.85 \n", "         ,'lambda_l1':1.0}"], "execution_count": 49}, {"source": ["Edited this for just 1 fold because it was taking so long. "], "cell_type": "markdown", "metadata": {}}, {"outputs": [], "metadata": {}, "cell_type": "code", "source": ["fold=1\n", "preds=0\n", "for i in range(fold):\n", "    xt,xv,yt,yv = train_test_split(train, y, test_size=0.15, random_state=i*314)\n", "    dtx = lgb.Dataset(xt, label=yt)\n", "    dtv = lgb.Dataset(xv, label=yv)\n", "    model = lgb.train(params, train_set=dtx, valid_sets=dtv, valid_names=['val'],\n", "                      num_boost_round=1000,\n", "                      early_stopping_rounds=100,\n", "                      verbose_eval=False)\n", "    pred = model.predict(test)\n", "    preds += pred\n", "preds /= fold    "], "execution_count": 51}, {"outputs": [], "metadata": {}, "cell_type": "code", "source": ["ax = lgb.plot_importance(model, max_num_features=50, figsize=(12,8), **{'color':'orange'})\n", "ax.grid(False)\n", "ax.set_facecolor('black')"], "execution_count": 52}, {"outputs": [], "metadata": {"collapsed": true}, "cell_type": "code", "source": ["import datetime\n", "now = datetime.datetime.now()\n", "submission = pd.DataFrame(preds, columns=['EAP','HPL','MWS'])\n", "submission['ID'] = pid\n", "submission.to_csv('lgbPipeline_'+str(now.strftime(\"%Y-%m-%d-%H-%M\"))+'.csv', index=False)"], "execution_count": 53}, {"outputs": [], "metadata": {"collapsed": true}, "cell_type": "code", "source": [], "execution_count": null}], "nbformat_minor": 1, "nbformat": 4, "metadata": {"kernelspec": {"language": "python", "display_name": "Python 3", "name": "python3"}, "language_info": {"codemirror_mode": {"version": 3, "name": "ipython"}, "nbconvert_exporter": "python", "version": "3.5.3", "file_extension": ".py", "pygments_lexer": "ipython3", "mimetype": "text/x-python", "name": "python"}}}