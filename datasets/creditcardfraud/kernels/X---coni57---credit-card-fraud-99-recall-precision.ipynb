{"nbformat": 4, "cells": [{"cell_type": "code", "metadata": {"collapsed": true}, "execution_count": null, "source": ["import numpy as np\n", "import pandas as pd\n", "import matplotlib.pyplot as plt\n", "\n", "from sklearn.decomposition import PCA, IncrementalPCA\n", "from sklearn.preprocessing import StandardScaler\n", "from sklearn.model_selection import StratifiedKFold\n", "from sklearn.metrics import recall_score, confusion_matrix, precision_score, accuracy_score\n", "from sklearn.linear_model import SGDClassifier\n", "from sklearn.base import clone\n", "from sklearn.tree import DecisionTreeClassifier\n", "from sklearn.svm import SVC\n", "from sklearn.neural_network import MLPClassifier\n", "\n", "import warnings\n", "warnings.filterwarnings('ignore')\n", "\n", "%matplotlib inline"], "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["# Dataset Exploration\n", "First let's explore quickly the dataset. I won't explain all what we have as it is described on above"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "source": ["dataset = pd.read_csv(\"../input/creditcard.csv\")"], "outputs": []}, {"cell_type": "code", "metadata": {}, "execution_count": null, "source": ["print(dataset.head())\n", "print(dataset.describe())\n", "print(dataset.info())"], "outputs": []}, {"cell_type": "code", "metadata": {}, "execution_count": null, "source": ["dataset['Class'].value_counts()"], "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["So as explain in this exercice we clearly have an unbalanced dataset with only 492 fraud and 284315 normal transaction (called non_fraud later)\n", "Let's now explore the repartition of the amount of each transactions"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "source": ["plt.hist(dataset['Amount'], bins=50)\n", "plt.show()"], "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["As we can imagine mainly all transaction are below 1500 \\$. In order to reduce the range of amount we can check how many frauds we have above 3000 \\$"]}, {"cell_type": "code", "metadata": {"scrolled": true}, "execution_count": null, "source": ["dataset[(dataset['Amount'] > 3000) & (dataset['Class']==1)]"], "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["Good point, there is none so we can remove transaction with an amoutn above 3000 \\$. After we can check the repartition of frauds based on the amount and the cost for the bank as Warranty"]}, {"cell_type": "code", "metadata": {"collapsed": true}, "execution_count": null, "source": ["dataset = dataset[dataset['Amount'] < 3000]"], "outputs": []}, {"cell_type": "code", "metadata": {}, "execution_count": null, "source": ["fraud = dataset[dataset['Class']==1]\n", "plt.hist(fraud['Amount'], bins=50)\n", "plt.show()"], "outputs": []}, {"cell_type": "code", "metadata": {}, "execution_count": null, "source": ["bins = 50\n", "Amount_max = 3000\n", "\n", "Y = []\n", "C = []\n", "X = list(range(0, Amount_max, bins))\n", "for i in X:\n", "    s = fraud[(fraud['Amount'] > i) & (fraud['Amount'] <= i + bins)]['Amount'].sum()\n", "    Y.append(s)\n", "    if len(C) > 0:\n", "        c = C[-1] + s\n", "    else:\n", "        c = s\n", "    C.append(c)\n", "    print(\"{} => {} $ - {}\".format(i, s, c))\n", "\n", "plt.bar(X, Y, width=50)\n", "plt.ylabel('Cost')\n", "plt.title('Cost of Frauds per amount')\n", "plt.show()\n", "\n", "plt.plot(X, C)\n", "plt.show()"], "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["So we can see that most of frauds are below 500 \\$. Nevertheless in term of cost, all fraud below 500 \\$ cost 31500 \\$ to the bank (50\\% of the total cost of frauds). If we want to avoid around 90\\% of fraud costs, we should consider frauds up to 1500 \\$. In the first time we will try to catch a maximum a fraud and depending on the result we may focus only on frauds < 1500 \\$"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Model simplification\n", "\n", "We clearly have a unbalanced dataset as we have only 0.17% of frauds. One good thing to do in such case is to try some <a href=\"https://en.wikipedia.org/wiki/Oversampling_and_undersampling_in_data_analysis\" target=\"_blank\">Random Undersampling</a>\n", "\n", "<b>For now, we only explore the dataset, the dataset will be splitted for evaluation later</b>"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "source": ["random_seed = 42\n", "n_non_fraud = [100, 1000, 10000, 100000, dataset[dataset[\"Class\"] == 0][\"Class\"].count()]         # min : 1 - max : 284807-492\n", "n_components = 3\n", "print(n_non_fraud)"], "outputs": []}, {"cell_type": "code", "metadata": {}, "execution_count": null, "source": ["for sample_size in n_non_fraud:\n", "    a = dataset[dataset[\"Class\"] == 1]                                                # we keep all frauds\n", "    b = dataset[dataset[\"Class\"] == 0].sample(sample_size, random_state=random_seed)  # we take \"sample_size\" non fraud to balance the ratio fraud/non_fraud\n", "\n", "    dataset_us = pd.concat([a, b]).sample(frac=1, random_state=random_seed)           # merge and shuffle both dataset\n", "    \n", "    y = dataset_us[\"Class\"]\n", "    X = dataset_us.drop([\"Time\", \"Class\"], axis=1)\n", "    \n", "    X_scale = StandardScaler().fit_transform(X)\n", "    X_proj = PCA(n_components=n_components).fit_transform(X_scale)\n", "    \n", "    plt.scatter(X_proj[:, 0], X_proj[:, 1], s=X_proj[:, 2], c=y)\n", "\n", "    plt.xlabel(\"PCA1\")\n", "    plt.ylabel(\"PCA2\")\n", "    plt.title(\"{}-points\".format(sample_size))\n", "    #plt.savefig(\"{}-points\".format(sample_size), dpi=600)\n", "    plt.show()"], "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["With 100 and 1000 non-frauds, we can see that non fraud are packed but some fraud are also grouped. With 10k and there is still some yellow points included in violet ones. With the full dataset, the reduction is useless as we packed all points.\n", "Nevertheless, with 100000 points, we have a nice split in 2 dimensions. We can fix this value to fit the PCA and use it in the full datraset afterward."]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "source": ["# fit the PCA with 100k non-frauds\n", "a = dataset[dataset[\"Class\"] == 1]\n", "b = dataset[dataset[\"Class\"] == 0].sample(100000, random_state=random_seed)\n", "\n", "dataset = pd.concat([a, b]).sample(frac=1, random_state=random_seed)\n", "\n", "y = dataset[\"Class\"]\n", "X = dataset.drop([\"Time\", \"Class\"], axis=1)\n", "\n", "X_scale = StandardScaler().fit_transform(dataset)\n", "pca = PCA(n_components=0.95, svd_solver=\"full\")\n", "X_proj = pca.fit(X_scale)\n", "\n", "# transform the full dataset with the pca create previously\n", "dataset = pd.read_csv(\"../input/creditcard.csv\")\n", "y = dataset[\"Class\"]\n", "X = dataset.drop([\"Time\", \"Class\"], axis=1)\n", "\n", "X_scale = StandardScaler().fit_transform(dataset)\n", "X_proj = pca.transform(X_scale)"], "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["# Setting up a model\n", "\n", "Above instead of keeping only the 3 main dimensions, we reduce dimensions until having 5% loss. We can check how many features we have :"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "source": ["print(X_proj.shape)"], "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["Unfortunately, we drop only 2 additionnal dimensions but it's better than nothing. We can check also that our reduction still allow a nice split."]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "source": ["plt.scatter(X_proj[:, 0], X_proj[:, 1], s=X_proj[:, 2], c=y)\n", "\n", "plt.xlabel(\"PCA1\")\n", "plt.ylabel(\"PCA2\")\n", "plt.title(\"{}-points\".format(X_proj.shape[0]))\n", "#plt.savefig(\"{}-points\".format(sample_size), dpi=600)\n", "plt.show()"], "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["For this model, it would be bad to use a standard split as we have an unbalanced dataset (492 frauds for 280k non-frauds). In such case we should definitely go for a StratifiedKFold with let say 5 folds to have around 100 frauds in each fold.\n", "\n", "For now we gonna try some classification model and our target won't be the count of good guess. In this exercice it makes no sense as we can easily reach 99.8% as we only have 0.17% fraud in total. A classifier saying non fraud everytime would get 99.8%.\n", "\n", "Instead our score will be the number of non detected frauds (False Negative). So we must maximise the <b>Precision</b>\n", "\n", "Just as reminder, confusion matrix is :\n", "\n", "\\begin{vmatrix}\n", "Non\\_Fraud\\_detected\\_as\\_non\\_fraud &  Fraud\\_detected\\_as\\_non\\_fraud \\\\\n", "Non\\_Fraud\\_detected\\_as\\_Fraud &  Fraud\\_detected\\_as\\_Fraud\n", "\\end{vmatrix}"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "source": ["skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=random_seed)  #shuffle is required to avoid having unbalance folds\n", "sgd_clf = SGDClassifier(random_state=random_seed)\n", "for train_index, test_index in skf.split(X_proj, y):\n", "    clone_clf = clone(sgd_clf)\n", "    X_train, X_test = X_proj[train_index], X_proj[test_index]\n", "    y_train, y_test = y[train_index], y[test_index]\n", "    clone_clf.fit(X_train, y_train)\n", "    y_pred = clone_clf.predict(X_test)\n", "    recall = recall_score(y_test, y_pred)\n", "    precision = precision_score(y_test, y_pred)\n", "    print(\"\\nRecall:\\t\\t {:.4f} \\nPrecision:\\t {:.4f}\".format(recall, precision))\n", "    print(confusion_matrix(y_test, y_pred))"], "outputs": []}, {"cell_type": "code", "metadata": {}, "execution_count": null, "source": ["tree_clf = DecisionTreeClassifier(max_depth=7, random_state=random_seed)\n", "for train_index, test_index in skf.split(X_proj, y):\n", "    clone_clf = clone(tree_clf)\n", "    X_train, X_test = X_proj[train_index], X_proj[test_index]\n", "    y_train, y_test = y[train_index], y[test_index]\n", "    clone_clf.fit(X_train, y_train)\n", "    y_pred = clone_clf.predict(X_test)\n", "    recall = recall_score(y_test, y_pred)\n", "    precision = precision_score(y_test, y_pred)\n", "    print(recall, precision)\n", "    print(confusion_matrix(y_test, y_pred))"], "outputs": []}, {"cell_type": "code", "metadata": {"collapsed": true}, "execution_count": null, "source": ["# svc_clf = SVC(gamma=2, C=1)\n", "# for train_index, test_index in skf.split(X_proj, y):\n", "#     clone_clf = clone(svc_clf)\n", "#     X_train, X_test = X_proj[train_index], X_proj[test_index]\n", "#     y_train, y_test = y[train_index], y[test_index]\n", "#     clone_clf.fit(X_train, y_train)\n", "#     y_pred = clone_clf.predict(X_test)\n", "#     recall = recall_score(y_test, y_pred)\n", "#     precision = precision_score(y_test, y_pred)\n", "#     print(recall, precision)\n", "#     print(confusion_matrix(y_test, y_pred))\n", "\n", "#     Usign this model make the computer crach :("], "outputs": []}, {"cell_type": "code", "metadata": {}, "execution_count": null, "source": ["mlp_clf = MLPClassifier(hidden_layer_sizes=(50, 20), random_state=random_seed)\n", "for train_index, test_index in skf.split(X_proj, y):\n", "    clone_clf = clone(mlp_clf)\n", "    X_train, X_test = X_proj[train_index], X_proj[test_index]\n", "    y_train, y_test = y[train_index], y[test_index]\n", "    clone_clf.fit(X_train, y_train)\n", "    y_pred = clone_clf.predict(X_test)\n", "    recall = recall_score(y_test, y_pred)\n", "    precision = precision_score(y_test, y_pred)\n", "    print(recall, precision)\n", "    print(confusion_matrix(y_test, y_pred))"], "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["The MLPClassifier give again a better result than the Tree Classifier. Topology hasn't been reviewed as we have a perfect catch but maybe layer size can be reduced to ease calculation. This is really great because that means we won't have refunds to do to victims as we catch all frauds and we won't also need employee to check some possible frauds (this model will ask for only few check every day (which are Non_Fraud detected as Fraud) ).\n", "\n", "\n", "# Conclusion\n", "\n", "By using the MLPClassifier, we can nearly all frauds and having nearly no False Positive. There is just a need to prepare all data first in the StandardScaler and in the PCA. Just to finish let's compute the score on the whole dataset (attention result may be below as we will also \"evaluate\" the training set)"]}, {"cell_type": "code", "metadata": {"collapsed": true}, "execution_count": null, "source": ["best_model = clone_clf\n", "y_pred = best_model.predict(X_proj)"], "outputs": []}, {"cell_type": "code", "metadata": {}, "execution_count": null, "source": ["print(\"Accuracy score : {}\".format(accuracy_score(y, y_pred)))\n", "print(\"Precision score : {}\".format(precision_score(y, y_pred)))\n", "print(\"Recall score : {}\".format(recall_score(y, y_pred)))\n", "print(\"Confusion Matrix : {}\".format(confusion_matrix(y, y_pred)))"], "outputs": []}, {"cell_type": "code", "metadata": {"collapsed": true}, "execution_count": null, "source": [], "outputs": []}], "metadata": {"kernelspec": {"language": "python", "display_name": "Python 3", "name": "python3"}, "language_info": {"file_extension": ".py", "mimetype": "text/x-python", "name": "python", "pygments_lexer": "ipython3", "version": "3.6.1", "codemirror_mode": {"version": 3, "name": "ipython"}, "nbconvert_exporter": "python"}}, "nbformat_minor": 1}