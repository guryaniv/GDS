{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport keras\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.utils import to_categorical\nfrom keras.layers import *\nfrom keras.losses import *\nfrom keras.optimizers import *\nimport sklearn\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import KFold\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.metrics import log_loss\nfrom sklearn.metrics import accuracy_score\nimport seaborn as sns\nimport re\n\ndf = pd.read_csv(\"../input/train.csv\")\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"75d2e9c2750bbcdd6cde16979cbd1b9e3677952b","collapsed":true},"cell_type":"code","source":"labelEncoder = LabelEncoder().fit(df['author'])\ny = labelEncoder.transform(df['author'])\ntokenizer = Tokenizer(lower=False)\ntokenizer.fit_on_texts(df['text'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"c75aeb40f180b2e905db6e1fd4a20112fbe4762c"},"cell_type":"code","source":"def get_model(name):\n    if name=='LogisticRegression':\n        return LogisticRegression()\n    elif name=='DecisionTreeClassifier':\n        return DecisionTreeClassifier()\n    elif name=='RandomForestClassifier':\n        return RandomForestClassifier()\n    elif name=='AdaBoostClassifier':\n        return AdaBoostClassifier(n_estimators=20)\n    elif name=='MultinomialNB':\n        return MultinomialNB()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6addb4e7a78578056efd9aaf2376f7b8ccc90cd1"},"cell_type":"markdown","source":"## Logistic Regression"},{"metadata":{"trusted":true,"_uuid":"5ad70ddc0f134b672987dfaa6dc9d8393db7dba0"},"cell_type":"code","source":"metrics_df = pd.DataFrame(columns=['model', 'tokenizer_mode', 'metrics', 'value'])\nfor model_name in ['LogisticRegression']:\n    for mode in ['binary', 'count', 'tfidf']:\n        print('=============', model_name, ' - ', mode, '=============')\n        model = get_model(model_name)\n        X = tokenizer.texts_to_matrix(df['text'], mode=mode)\n        kfold = KFold(4)\n        losses=[]\n        accuracys=[]\n        for train_index, test_index in kfold.split(X):\n            model.fit(X[train_index], y[train_index])\n            yp_prob = model.predict_proba(X[test_index])\n            yp_class = np.argmax(yp_prob, axis=1)\n            losses.append(log_loss(y[test_index], yp_prob))\n            accuracys.append(accuracy_score(y[test_index], yp_class))\n        print ('avg log_loss=', np.mean(np.array(losses)), )\n        print ('avg accuracy_score=', np.mean(np.array(accuracys)))\n        print ('std log_loss=', np.std(np.array(losses)), )\n        print ('std accuracy_score=', np.std(np.array(accuracys)))\n        metrics_df = metrics_df.append({'model':model_name, 'tokenizer_mode':mode, 'metrics':'loss', 'value':np.mean(np.array(losses))}, ignore_index=True)\n        metrics_df = metrics_df.append({'model':model_name, 'tokenizer_mode':mode, 'metrics':'accuracy', 'value':np.mean(np.array(accuracys))}, ignore_index=True)\n\nsns.factorplot(data=metrics_df, x='model', y='value', col='metrics', hue='tokenizer_mode', kind=\"bar\", ci=None)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"471abe86c1e7f6d83ab007db9a503aa6ce155eca"},"cell_type":"markdown","source":"## Decision Tree & Some Ensemble Models"},{"metadata":{"trusted":true,"_uuid":"93b172189dea9553bff01fcd5680de97c5852542","collapsed":true},"cell_type":"code","source":"metrics_df = pd.DataFrame(columns=['model', 'tokenizer_mode', 'metrics', 'value'])\n    \nfor model_name in ['LogisticRegression', 'MultinomialNB', 'DecisionTreeClassifier', 'RandomForestClassifier', 'AdaBoostClassifier']:\n    for mode in ['count']:\n        print('=============', model_name, ' - ', mode, '=============')\n        model = get_model(model_name)\n        X = tokenizer.texts_to_matrix(df['text'], mode=mode)\n        kfold = KFold(4)\n        losses=[]\n        accuracys=[]\n        for train_index, test_index in kfold.split(X):\n            model.fit(X[train_index], y[train_index])\n            yp_prob = model.predict_proba(X[test_index])\n            yp_class = np.argmax(yp_prob, axis=1)\n            losses.append(log_loss(y[test_index], yp_prob))\n            accuracys.append(accuracy_score(y[test_index], yp_class))\n        print ('avg log_loss=', np.mean(np.array(losses)), )\n        print ('avg accuracy_score=', np.mean(np.array(accuracys)))\n        print ('std log_loss=', np.std(np.array(losses)), )\n        print ('std accuracy_score=', np.std(np.array(accuracys)))\n        metrics_df = metrics_df.append({'model':model_name, 'tokenizer_mode':mode, 'metrics':'loss', 'value':np.mean(np.array(losses))}, ignore_index=True)\n        metrics_df = metrics_df.append({'model':model_name, 'tokenizer_mode':mode, 'metrics':'accuracy', 'value':np.mean(np.array(accuracys))}, ignore_index=True)\n        \ng=sns.factorplot(data=metrics_df, x='model', y='value', col='metrics', hue='tokenizer_mode', kind=\"bar\", ci=None, sharey=False)\ng.set_xticklabels(rotation=30)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f10fc4471589aa4ffeba1972fc1fb937922b0a86"},"cell_type":"markdown","source":"## Logistic Regression on 2-grams"},{"metadata":{"trusted":true,"_uuid":"37337883bc881c4461a66c6ee7ce74b7de603d0a","collapsed":true},"cell_type":"code","source":"def generate_ngram(doc, n=2):\n    words = re.sub(r'[!\"#$%&()*+,-./:;<=>?@\\[\\]\\\\^_\\'`{|}~]','',doc).split()\n    length = len(words)\n    for i in range(2, n+1):\n        for j in range(0, length - i + 1):\n            ngram = words[j]\n            for k in range(1, i):\n                ngram += '_'+words[j+k]\n            words.append(ngram)\n    return ' '.join(words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7486e838e2556409658324f2f34ec7ae264e9f12","collapsed":true},"cell_type":"code","source":"df['text'] = df['text'].apply(generate_ngram)\ntokenizer = Tokenizer(lower=False)\ntokenizer.fit_on_texts(df['text'])\n\nmetrics_df = pd.DataFrame(columns=['model', 'tokenizer_mode', 'metrics', 'value'])\nfor model_name in ['LogisticRegression']:\n    for mode in ['count']:\n        print('=============', model_name, ' - ', mode, '=============')\n        model = get_model(model_name)\n        X = tokenizer.texts_to_matrix(df['text'], mode=mode)\n        kfold = KFold(4)\n        losses=[]\n        accuracys=[]\n        for train_index, test_index in kfold.split(X):\n            model.fit(X[train_index], y[train_index])\n            yp_prob = model.predict_proba(X[test_index])\n            yp_class = np.argmax(yp_prob, axis=1)\n            losses.append(log_loss(y[test_index], yp_prob))\n            accuracys.append(accuracy_score(y[test_index], yp_class))\n        print ('avg log_loss=', np.mean(np.array(losses)), )\n        print ('avg accuracy_score=', np.mean(np.array(accuracys)))\n        print ('std log_loss=', np.std(np.array(losses)), )\n        print ('std accuracy_score=', np.std(np.array(accuracys)))\n        metrics_df = metrics_df.append({'model':model_name, 'tokenizer_mode':mode, 'metrics':'loss', 'value':np.mean(np.array(losses))}, ignore_index=True)\n        metrics_df = metrics_df.append({'model':model_name, 'tokenizer_mode':mode, 'metrics':'accuracy', 'value':np.mean(np.array(accuracys))}, ignore_index=True)\n\ng=sns.factorplot(data=metrics_df, x='model', y='value', col='metrics', hue='tokenizer_mode', kind=\"bar\", ci=None, sharey=False)\ng.set_xticklabels(rotation=30)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4857aa25aaf79b7beb0a7fa7dbe6bc67294a0a22"},"cell_type":"markdown","source":"## Preparing Submission\n\nLinear regresion on 1-grams is chosen."},{"metadata":{"trusted":true,"_uuid":"f5323ca21ddd0c9940faedac08e2f3ddc9ef06b8","collapsed":true},"cell_type":"code","source":"test_df = pd.read_csv(\"../input/test.csv\")\ntokenizer = Tokenizer(lower=False)\ntokenizer.fit_on_texts(df['text'])\nmodel = get_model('LogisticRegression')\nX = tokenizer.texts_to_matrix(df['text'], mode='count')\nmodel.fit(X, y)\nX_test = tokenizer.texts_to_matrix(test_df['text'], mode='count')\nyp_prob = model.predict_proba(X_test)\nsubmission_df = pd.concat([test_df[['id']], pd.DataFrame(yp_prob, columns=labelEncoder.classes_)], axis=1)\nsubmission_df.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}