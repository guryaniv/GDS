{"cells":[{"metadata":{"_uuid":"579dd497b9c0f34627a6a0354361b720e8fb521f"},"cell_type":"markdown","source":"#Sentiment Challenge"},{"metadata":{"_uuid":"ee74808c1472014dc9f0c3d61ebba914dbb4b463"},"cell_type":"markdown","source":"Welcome to my Kernel on Movie Review Sentiment. The goal of this Kernel is to predict the sentiment of movie reviews. The sentiment can be rated 0 to 4 with 0 being negative, 1 slightly negative, 2 neutral, 3 slightly positive and 4 positive. This kernel will walk through data preparation and constrution of the model. Links to the resources that help me learn the methods used in this Kernel are also listed below."},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"d242985865d80f1a5a7f0cc460cc852ff1f89a2d"},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\ntrain = pd.read_csv('../input/movie-review-sentiment-analysis-kernels-only/train.tsv', sep='\\t')\ntest = pd.read_csv('../input/movie-review-sentiment-analysis-kernels-only/test.tsv', sep='\\t')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"26830d9d73051c70d477dcb4a9492df74534c537"},"cell_type":"code","source":"xtrain = train[\"Phrase\"]\nytrain = pd.get_dummies(train[\"Sentiment\"])\nxtest = test[\"Phrase\"]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2e9dce282c6d760ec5ffcb7ae544cf59a215f0c6"},"cell_type":"markdown","source":"Below we will tokenize the text by using the Tokenizer class from Keras. This converts the text into words or what are known as tokens. The parameter \"lower\" allows us to convert each word to lower case when it is tokenized and \"num_words\" only keeps the most frequent 20,000 words. The tokens will then be converted to a sequence using texts_to_sequences from Keras.\n\nMore info on preparing text for deep learning: \n\nInspiration for data preprocessing: https://www.kaggle.com/antmarakis/cnn-baseline-model\nhttps://machinelearningmastery.com/prepare-text-data-deep-learning-keras/\nhttps://keras.io/preprocessing/text/"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"4fd9610665438fb8b7ce1cf21cffd807dfbb0a40"},"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing import sequence\n\nprint(xtrain.head(5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"f2f0a0233b76b14e310b316a78d10e294cfe46f9"},"cell_type":"code","source":"Token = Tokenizer(lower = True, num_words = 20000)\nToken.fit_on_texts(xtrain)\nToken.fit_on_texts(xtest)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"cee03837d1eb88ea204b45afa0967b6666ae6629"},"cell_type":"code","source":"xtrain = Token.texts_to_sequences(xtrain)\nxtest = Token.texts_to_sequences(xtest)\n\nword_index = Token.word_index\n\nxtrain = sequence.pad_sequences(xtrain, maxlen = 300)\nxtest = sequence.pad_sequences(xtest, maxlen = 300)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"183cc79d9f613c897cd054cc8fa5b6524daaa03f"},"cell_type":"markdown","source":"Next we will prepare the embedding layer on the model. Embeddings provide representations of a word and their relative meaning. The code below prepares a pre-trained embedding known as GloVe.\n\nUsing Pre-Trained Embeddings: https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\n\nGloVe: https://nlp.stanford.edu/projects/glove/\n\nBackground on Embedding: https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"6aaf3ca3c0cbc3c9eaae1d8880b1957eaf7bfb16"},"cell_type":"code","source":"embedding_index = {}\nf = open('../input/glove6b/glove.6B.100d.txt')\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coef = np.asarray(values[1:], dtype='float32')\n    embedding_index[word] = coef\nf.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"133f974550925b44013766dc98017085ce044c97"},"cell_type":"code","source":"vocab = len(word_index) + 1\n\nembedding_matrix = np.zeros((len(word_index) + 1, 100))\nfor word, i in word_index.items():\n    embedding_vector = embedding_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e1e527aaa8080ca65ed5bec2a8327983487c311c"},"cell_type":"markdown","source":"The model below combines a recurrent LSTM layer and convolutional layers. Inpiration for the model can be found below:\n\nhttps://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"3db7a950ff85863c6f0abf0e1c27bf0dc18b73a0"},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom keras.layers.convolutional import Conv1D\nfrom keras.layers.convolutional import MaxPooling1D\nfrom keras.layers.embeddings import Embedding\n\nmodel = Sequential()\nemb = Embedding(vocab, 100, weights = [embedding_matrix], input_length = 300, trainable = False)\nmodel.add(emb)\nmodel.add(Conv1D(32, 3, padding = 'same', activation = 'relu'))\nmodel.add(MaxPooling1D(3))\nmodel.add(Conv1D(32, 3, padding = 'same', activation = 'relu'))\nmodel.add(MaxPooling1D(3))\nmodel.add(Conv1D(32, 3, padding = 'same', activation = 'relu'))\nmodel.add(MaxPooling1D(3))\nmodel.add(LSTM(100))\nmodel.add(Dense(5, activation = 'softmax'))\nmodel.compile(optimizer='adam',loss='categorical_crossentropy', metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false,"collapsed":true,"_uuid":"c38a49385debaf8ce34d3473c133dc7b87ed5ac6"},"cell_type":"code","source":"model.fit(xtrain, ytrain, epochs = 10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"8ac8c27f75634039e59f1b5c7aadf2e6adf97cd2"},"cell_type":"code","source":"Submission = pd.read_csv('../input/movie-review-sentiment-analysis-kernels-only/sampleSubmission.csv')\nSubmission['Sentiment'] = model.predict_classes(xtest)\nSubmission.to_csv(\"SentimentSubmission.csv\", index = False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1372c646a0c5eb8d1278a8a222e54fc2ccc7ccba"},"cell_type":"markdown","source":"I hope you enjoyed this Kernel on using deep learning to predict Movie Review Sentiment. If you liked what you saw feel free to upvote and comment with any questions you may have."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}