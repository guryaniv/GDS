{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "3f4226ee-f09e-f088-1d36-dd7b4ef89b98"
      },
      "source": [
        "## Weighted Least Squares Fit of Sberbank Data (early, incomplete)\n",
        "\n",
        "Somehow this got moved to \"no data sources,\" so I forked it back."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "66678757-7952-bc9c-8114-5b49002d644e"
      },
      "source": [
        "This is the second checkpoint in my modeling progress.  I've cleaned it up a bit, compared to [my original sloppy OLS][1], and I've fixed some bugs (which had surprisingly little effect on the predictions but made the statsmodels results look ugly).  The biggest change is that I'm now weighting the data by time, with later observations getting more weight, because they more closely resemble the test data on which our predictions will be judged.  For now, the weights are linear in time, with zero assigned to a semi-arbitrary point before the data begin, and the maximum value assigned to the last point in the training set.  The zero point was chosen by a very dirty and ad-hoc process:  I tried several values and chose the one that made my test set results closest to the results from [Reynaldo's Naive XGB](https://www.kaggle.com/aharless/adding-my-dva-kopeky-to-reynaldo-s-naive-xgb/output), which is an extremely bad, but best available, proxy for ground truth.  This makes the zero date a dirty parameter, but maybe not nearly so dirty as it would be if I optimized to genuine ground truth rather than something that is typically off by more than 30%.\n",
        "\n",
        "I will note that I'm a still working with a quite limited subset of the available features.  This is mostly because I got tired of doing EDA and basic feature engineering and decided to defer the rest.  It's also partly because I'm trying to design a baseline model, which should in principle be something simple enough that it can get a reasonable fit without regularization.  I may delete some of what is here from the baseline model, or I may add things.  Ultimately, I intend to use the baseline model to re-engineer my features -- for example, perhaps converting a set of dummies for a categorical variable to a continuous variable by taking the OLS (or WLS, it turns out) coefficients.  How the cross-validation on this process will proceed, and how it will interact with the cross-validation of the actual prediction model, is an open question, but I have some ideas.\n",
        "\n",
        "Again I credit [Mark Waddoups](https://www.kaggle.com/mwaddoups/i-regression-workflow-various-models) (along with probably others I have forgotten) with the skeleton for much of the code here.\n",
        "\n",
        "\n",
        "  [1]: https://www.kaggle.com/aharless/sloppy-mess-ols"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "f51842c2-4f97-d969-6a75-ef63bfa4ac2a"
      },
      "outputs": [],
      "source": [
        "# Parameters\n",
        "use_pipe = True  # Old version had collinearity that was somehow masked w/o the pipe transform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "2985f0c3-8e53-5ef4-0310-0e2adc1980cc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "train = pd.read_csv('../input/train.csv')\n",
        "macro = pd.read_csv('../input/macro.csv')\n",
        "test = pd.read_csv('../input/test.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "beba4616-8d63-423f-c05e-552397ecbafd"
      },
      "outputs": [],
      "source": [
        "dfa = pd.concat([train, test])  # \"dfa\" stands for \"data frame all\"\n",
        "# Eliminate spaces and special characters in area names\n",
        "dfa.loc[:,\"sub_area\"] = dfa.sub_area.str.replace(\" \",\"\").str.replace(\"\\'\",\"\").str.replace(\"-\",\"\")\n",
        "dfa = dfa.merge(macro, on='timestamp', suffixes=['','_macro'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "851a8915-6cf4-65b9-2b4b-aedddb5654d5"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "9436b3e2-98e5-8796-c23c-a08eed2bd56b"
      },
      "outputs": [],
      "source": [
        "dfa[\"fullzero\"] = (dfa.full_sq==0)\n",
        "dfa[\"fulltiny\"] = (dfa.full_sq<4)\n",
        "dfa[\"fullhuge\"] = (dfa.full_sq>2000)\n",
        "dfa[\"lnfull\"] = np.log(dfa.full_sq+1)\n",
        "\n",
        "dfa[\"nolife\"] = dfa.life_sq.isnull()\n",
        "dfa.life_sq = dfa.life_sq.fillna(dfa.life_sq.median())\n",
        "dfa[\"lifezero\"] = (dfa.life_sq==0)\n",
        "dfa[\"lifetiny\"] = (dfa.life_sq<4)\n",
        "dfa[\"lifehuge\"] = (dfa.life_sq>2000)\n",
        "dfa[\"lnlife\"] = np.log( dfa.life_sq + 1 )\n",
        "\n",
        "dfa[\"nofloor\"] = dfa.floor.isnull()\n",
        "dfa.floor = dfa.floor.fillna(dfa.floor.median())\n",
        "dfa[\"floor1\"] = (dfa.floor==1)\n",
        "dfa[\"floor0\"] = (dfa.floor==0)\n",
        "dfa[\"floorhuge\"] = (dfa.floor>50)\n",
        "dfa[\"lnfloor\"] = np.log(dfa.floor+1)\n",
        "\n",
        "dfa[\"nomax\"] = dfa.max_floor.isnull()\n",
        "dfa.max_floor = dfa.max_floor.fillna(dfa.max_floor.median())\n",
        "dfa[\"max1\"] = (dfa.max_floor==1)\n",
        "dfa[\"max0\"] = (dfa.max_floor==0)\n",
        "dfa[\"maxhuge\"] = (dfa.max_floor>80)\n",
        "dfa[\"lnmax\"] = np.log(dfa.max_floor+1)\n",
        "\n",
        "dfa[\"norooms\"] = dfa.num_room.isnull()\n",
        "dfa.num_room = dfa.num_room.fillna(dfa.num_room.median())\n",
        "dfa[\"zerorooms\"] = (dfa.num_room==0)\n",
        "dfa[\"lnrooms\"] = np.log( dfa.num_room + 1 )\n",
        "\n",
        "dfa[\"nokitch\"] = dfa.kitch_sq.isnull()\n",
        "dfa.kitch_sq = dfa.kitch_sq.fillna(dfa.kitch_sq.median())\n",
        "dfa[\"kitch1\"] = (dfa.kitch_sq==1)\n",
        "dfa[\"kitch0\"] = (dfa.kitch_sq==0)\n",
        "dfa[\"kitchhuge\"] = (dfa.kitch_sq>400)\n",
        "dfa[\"lnkitch\"] = np.log(dfa.kitch_sq+1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "88336509-d2a4-fccb-95c4-2e170e2fc44c"
      },
      "outputs": [],
      "source": [
        "dfa[\"material0\"] = dfa.material.isnull()\n",
        "dfa[\"material1\"] = (dfa.material==1)\n",
        "dfa[\"material2\"] = (dfa.material==2)\n",
        "dfa[\"material3\"] = (dfa.material==3)\n",
        "dfa[\"material4\"] = (dfa.material==4)\n",
        "dfa[\"material5\"] = (dfa.material==5)\n",
        "dfa[\"material6\"] = (dfa.material==6)\n",
        "\n",
        "# \"state\" isn't explained but it looks like an ordinal number, so for now keep numeric\n",
        "dfa.loc[dfa.state>5,\"state\"] = np.NaN  # Value 33 seems to be invalid; others all 1-4\n",
        "dfa.state = dfa.state.fillna(dfa.state.median())\n",
        "\n",
        "# product_type gonna be ugly because there are missing values in the test set but not training\n",
        "# Check for the same problem with other variables\n",
        "dfa[\"owner_occ\"] = (dfa.product_type=='OwnerOccupier')\n",
        "dfa.owner_occ.fillna(dfa.owner_occ.mean())\n",
        "\n",
        "dfa = pd.get_dummies(dfa, columns=['sub_area'], drop_first=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "298e1453-8ca4-9430-03d4-a73bf071f0d3"
      },
      "outputs": [],
      "source": [
        "# Build year is ugly\n",
        "# Can be missing\n",
        "# Can be zero\n",
        "# Can be one\n",
        "# Can be some ridiculous pre-Medieval number\n",
        "# Can be some invalid huge number like 20052009\n",
        "# Can be some other invalid huge number like 4965\n",
        "# Can be a reasonable number but later than purchase year\n",
        "# Can be equal to purchase year\n",
        "# Can be a reasonable nubmer before purchase year\n",
        "\n",
        "dfa.loc[dfa.build_year>2030,\"build_year\"] = np.NaN\n",
        "dfa[\"nobuild\"] = dfa.build_year.isnull()\n",
        "dfa[\"sincebuild\"] = pd.to_datetime(dfa.timestamp).dt.year - dfa.build_year\n",
        "dfa.sincebuild.fillna(dfa.sincebuild.median(),inplace=True)\n",
        "dfa[\"futurebuild\"] = (dfa.sincebuild < 0)\n",
        "dfa[\"newhouse\"] = (dfa.sincebuild==0)\n",
        "dfa[\"tooold\"] = (dfa.sincebuild>1000)\n",
        "dfa[\"build0\"] = (dfa.build_year==0)\n",
        "dfa[\"build1\"] = (dfa.build_year==1)\n",
        "dfa[\"untilbuild\"] = -dfa.sincebuild.apply(np.min, args=[0]) # How many years until planned build\n",
        "dfa[\"lnsince\"] = dfa.sincebuild.mul(dfa.sincebuild>0).add(1).apply(np.log)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "83487b06-6239-e535-0858-587f4d2486bd"
      },
      "outputs": [],
      "source": [
        "# Note for later:\n",
        "# Want to check for valididty of relationships, e.g. kitch_sq < life_sq < full_sq\n",
        "# But this interacts with how variables are already processed, so that may have to be changed\n",
        "# For example, if kitch_sq is sometimes huge and there is a dummy to identify those huge cases,\n",
        "#  do we want a separate dummy to identify which of those cases are internally consistent?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c761ffcd-771a-bd2a-7ef0-ce7d3a8ba48d"
      },
      "outputs": [],
      "source": [
        "# The normalized target variable:  log real sale price\n",
        "training = dfa[dfa.price_doc.notnull()]\n",
        "training.lnrp = training.price_doc.div(training.cpi).apply(np.log)\n",
        "y = training.lnrp\n",
        "\n",
        "# The features used for prediction\n",
        "# (Turns out norooms, nomax, and nokitch are all identical to the omitted material=NA category, \n",
        "#  so include only norooms, and then only if material dummies are excluded.)\n",
        "X = training[[\n",
        "       # Features derived from full_sq\n",
        "       \"fullzero\", \"fulltiny\", \"fullhuge\", \"lnfull\",\n",
        "       # Features derived from life_sq\n",
        "       \"nolife\", \"lifezero\", \"lifetiny\", \"lifehuge\", \"lnlife\",\n",
        "       # Features derived from floor\n",
        "       \"nofloor\", \"floor1\", \"floor0\", \"floorhuge\", \"lnfloor\",\n",
        "       # Features derived from max_floor\n",
        "       \"max1\", \"max0\", \"maxhuge\", \"lnmax\",\n",
        "       # Features derived from num_room\n",
        "       \"zerorooms\", \"lnrooms\",\n",
        "       # Features derived from kitch_sq\n",
        "       \"kitch1\", \"kitch0\", \"kitchhuge\", \"lnkitch\",\n",
        "       # Features derived from bulid_year\n",
        "       \"nobuild\", \"futurebuild\", \"newhouse\", \"tooold\", \n",
        "       \"build0\", \"build1\", \"untilbuild\", \"lnsince\",\n",
        "       # Feature derived from product_type\n",
        "       \"owner_occ\",\n",
        "       # Included \"state\" as is for now, but later will recode to more meaningful ratio scale\n",
        "       \"state\",\n",
        "       # Features (dummy set) derived from material\n",
        "       \"material1\", \"material2\", \"material3\", \"material4\", \"material5\", \"material6\",\n",
        "       # Features (dummy set) derived from sub_area\n",
        "       'sub_area_Akademicheskoe',\n",
        "       'sub_area_Alekseevskoe', 'sub_area_Altufevskoe', 'sub_area_Arbat',\n",
        "       'sub_area_Babushkinskoe', 'sub_area_Basmannoe', 'sub_area_Begovoe',\n",
        "       'sub_area_Beskudnikovskoe', 'sub_area_Bibirevo',\n",
        "       'sub_area_BirjulevoVostochnoe', 'sub_area_BirjulevoZapadnoe',\n",
        "       'sub_area_Bogorodskoe', 'sub_area_Brateevo', 'sub_area_Butyrskoe',\n",
        "       'sub_area_Caricyno', 'sub_area_Cheremushki',\n",
        "       'sub_area_ChertanovoCentralnoe', 'sub_area_ChertanovoJuzhnoe',\n",
        "       'sub_area_ChertanovoSevernoe', 'sub_area_Danilovskoe',\n",
        "       'sub_area_Dmitrovskoe', 'sub_area_Donskoe', 'sub_area_Dorogomilovo',\n",
        "       'sub_area_FilevskijPark', 'sub_area_FiliDavydkovo',\n",
        "       'sub_area_Gagarinskoe', 'sub_area_Goljanovo',\n",
        "       'sub_area_Golovinskoe', 'sub_area_Hamovniki',\n",
        "       'sub_area_HoroshevoMnevniki', 'sub_area_Horoshevskoe',\n",
        "       'sub_area_Hovrino', 'sub_area_Ivanovskoe', 'sub_area_Izmajlovo',\n",
        "       'sub_area_Jakimanka', 'sub_area_Jaroslavskoe', 'sub_area_Jasenevo',\n",
        "       'sub_area_JuzhnoeButovo', 'sub_area_JuzhnoeMedvedkovo',\n",
        "       'sub_area_JuzhnoeTushino', 'sub_area_Juzhnoportovoe',\n",
        "       'sub_area_Kapotnja', 'sub_area_Konkovo', 'sub_area_Koptevo',\n",
        "       'sub_area_KosinoUhtomskoe', 'sub_area_Kotlovka',\n",
        "       'sub_area_Krasnoselskoe', 'sub_area_Krjukovo',\n",
        "       'sub_area_Krylatskoe', 'sub_area_Kuncevo', 'sub_area_Kurkino',\n",
        "       'sub_area_Kuzminki', 'sub_area_Lefortovo', 'sub_area_Levoberezhnoe',\n",
        "       'sub_area_Lianozovo', 'sub_area_Ljublino', 'sub_area_Lomonosovskoe',\n",
        "       'sub_area_Losinoostrovskoe', 'sub_area_Marfino',\n",
        "       'sub_area_MarinaRoshha', 'sub_area_Marino', 'sub_area_Matushkino',\n",
        "       'sub_area_Meshhanskoe', 'sub_area_Metrogorodok', 'sub_area_Mitino',\n",
        "       'sub_area_Molzhaninovskoe', 'sub_area_MoskvorecheSaburovo',\n",
        "       'sub_area_Mozhajskoe', 'sub_area_NagatinoSadovniki',\n",
        "       'sub_area_NagatinskijZaton', 'sub_area_Nagornoe',\n",
        "       'sub_area_Nekrasovka', 'sub_area_Nizhegorodskoe',\n",
        "       'sub_area_NovoPeredelkino', 'sub_area_Novogireevo',\n",
        "       'sub_area_Novokosino', 'sub_area_Obruchevskoe',\n",
        "       'sub_area_OchakovoMatveevskoe', 'sub_area_OrehovoBorisovoJuzhnoe',\n",
        "       'sub_area_OrehovoBorisovoSevernoe', 'sub_area_Ostankinskoe',\n",
        "       'sub_area_Otradnoe', 'sub_area_Pechatniki', 'sub_area_Perovo',\n",
        "       'sub_area_PokrovskoeStreshnevo', 'sub_area_PoselenieDesjonovskoe',\n",
        "       'sub_area_PoselenieFilimonkovskoe', 'sub_area_PoselenieKievskij',\n",
        "       'sub_area_PoselenieKlenovskoe', 'sub_area_PoselenieKokoshkino',\n",
        "       'sub_area_PoselenieKrasnopahorskoe',\n",
        "       'sub_area_PoselenieMarushkinskoe',\n",
        "       'sub_area_PoselenieMihajlovoJarcevskoe',\n",
        "       'sub_area_PoselenieMoskovskij', 'sub_area_PoselenieMosrentgen',\n",
        "       'sub_area_PoselenieNovofedorovskoe',\n",
        "       'sub_area_PoseleniePervomajskoe', 'sub_area_PoselenieRjazanovskoe',\n",
        "       'sub_area_PoselenieRogovskoe', 'sub_area_PoselenieShhapovskoe',\n",
        "       'sub_area_PoselenieShherbinka', 'sub_area_PoselenieSosenskoe',\n",
        "       'sub_area_PoselenieVnukovskoe', 'sub_area_PoselenieVoronovskoe',\n",
        "       'sub_area_PoselenieVoskresenskoe', 'sub_area_Preobrazhenskoe',\n",
        "       'sub_area_Presnenskoe', 'sub_area_ProspektVernadskogo',\n",
        "       'sub_area_Ramenki', 'sub_area_Rjazanskij', 'sub_area_Rostokino',\n",
        "       'sub_area_Savelki', 'sub_area_Savelovskoe', 'sub_area_Severnoe',\n",
        "       'sub_area_SevernoeButovo', 'sub_area_SevernoeIzmajlovo',\n",
        "       'sub_area_SevernoeMedvedkovo', 'sub_area_SevernoeTushino',\n",
        "       'sub_area_Shhukino', 'sub_area_Silino', 'sub_area_Sokol',\n",
        "       'sub_area_SokolinajaGora', 'sub_area_Sokolniki',\n",
        "       'sub_area_Solncevo', 'sub_area_StaroeKrjukovo', 'sub_area_Strogino',\n",
        "       'sub_area_Sviblovo', 'sub_area_Taganskoe', 'sub_area_Tekstilshhiki',\n",
        "       'sub_area_TeplyjStan', 'sub_area_Timirjazevskoe',\n",
        "       'sub_area_Troickijokrug', 'sub_area_TroparevoNikulino',\n",
        "       'sub_area_Tverskoe', 'sub_area_Veshnjaki', 'sub_area_Vnukovo',\n",
        "       'sub_area_Vojkovskoe', 'sub_area_Vostochnoe',\n",
        "       'sub_area_VostochnoeDegunino', 'sub_area_VostochnoeIzmajlovo',\n",
        "       'sub_area_VyhinoZhulebino', 'sub_area_Zamoskvoreche',\n",
        "       'sub_area_ZapadnoeDegunino', 'sub_area_Zjablikovo',\n",
        "       'sub_area_Zjuzino',\n",
        "       # Macro shot in the dark\n",
        "       'mortgage_growth',\n",
        "       # Need to keep timestamp to calculate weights\n",
        "       'timestamp'\n",
        "             ]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "812fdee4-1e69-14c1-e357-62cedd7666e6"
      },
      "outputs": [],
      "source": [
        "def get_weights(df):\n",
        "    # Weight cases linearly on time\n",
        "    # with later cases (more like test data) weighted more heavily\n",
        "    basedate = pd.to_datetime(\"2011-07-01\").toordinal() # Basedate gets a weight of zero\n",
        "    wtd = pd.to_datetime(df.timestamp).apply(lambda x: x.toordinal()) - basedate\n",
        "    wts = np.array(wtd)/1e3 # The denominator here shouldn't matter, just gives nice numbers.\n",
        "    return wts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "bfd91399-0cce-7bb1-b0f2-fcd91b0a9185"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "wts = get_weights(X_train)\n",
        "X_train = X_train.drop(\"timestamp\", axis=1)\n",
        "X_test = X_test.drop(\"timestamp\", axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b5520696-1bfa-27f1-d44c-57777f5a33eb"
      },
      "outputs": [],
      "source": [
        "if use_pipe:\n",
        "    from sklearn.preprocessing import Imputer, StandardScaler\n",
        "    from sklearn.pipeline import make_pipeline\n",
        "\n",
        "    # Make a pipeline that transforms X\n",
        "    pipe = make_pipeline(Imputer(), StandardScaler())\n",
        "    pipe.fit(X_train)\n",
        "    pipe.transform(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b76c276a-5ac6-2aa0-1cad-77666c282a10"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import make_scorer\n",
        "\n",
        "# Need to fix this so it will weight the points using the same weight scheme as the fit.\n",
        "\n",
        "def rmsle_exp(y_true_log, y_pred_log):\n",
        "    y_true = np.exp(y_true_log)\n",
        "    y_pred = np.exp(y_pred_log)\n",
        "    return np.sqrt(np.mean(np.power(np.log(y_true + 1) - np.log(y_pred + 1), 2)))\n",
        "\n",
        "def score_model(model, pipe=None):\n",
        "    if (pipe==None):\n",
        "        train_error = rmsle_exp(y_train, model.predict(X_train))\n",
        "        test_error = rmsle_exp(y_test, model.predict(X_test))\n",
        "    else:\n",
        "        train_error = rmsle_exp(y_train, model.predict(pipe.transform(X_train)))\n",
        "        test_error = rmsle_exp(y_test, model.predict(pipe.transform(X_test)))\n",
        "    return train_error, test_error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "50fcef5a-49a7-0a5c-70bb-78a538131d44"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "lr = LinearRegression(fit_intercept=True)\n",
        "if use_pipe:\n",
        "    lr.fit(pipe.transform(X_train), y_train, sample_weight=wts)\n",
        "    print(\"Train error: {:.4f}, Test error: {:.4f}\".format(*score_model(lr, pipe)))\n",
        "else:\n",
        "    lr.fit(X_train, y_train, sample_weight=wts)\n",
        "    print(\"Train error: {:.4f}, Test error: {:.4f}\".format(*score_model(lr)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "662e1ba8-9d31-a60b-dfc4-a003a6380de8"
      },
      "outputs": [],
      "source": [
        "wts = get_weights(X)\n",
        "X = X.drop(\"timestamp\", axis=1)\n",
        "\n",
        "if use_pipe:\n",
        "    pipe.fit(X)\n",
        "    pipe.transform(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "e5bca7dc-101d-c875-3efe-7c6b8efa9863"
      },
      "outputs": [],
      "source": [
        "if use_pipe:\n",
        "    lr.fit(pipe.transform(X), y, sample_weight=wts)\n",
        "else:\n",
        "    lr.fit(X, y, sample_weight=wts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c2135ce7-f945-2b0e-1f48-29c6d6e1638d"
      },
      "outputs": [],
      "source": [
        "testing = dfa[dfa.price_doc.isnull()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c020cda1-ec75-b00f-8eb6-3426438d509a"
      },
      "outputs": [],
      "source": [
        "df_test = pd.DataFrame(columns=X.columns)\n",
        "for column in df_test.columns:\n",
        "        df_test[column] = testing[column]        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b33db063-b804-b173-1887-cc8ad3c29b87"
      },
      "outputs": [],
      "source": [
        "# Make the predictions\n",
        "if use_pipe:\n",
        "    pred = lr.predict( pipe.transform(df_test) )\n",
        "else:\n",
        "    pred = lr.predict(df_test)\n",
        "predictions = np.exp(pred)*testing.cpi\n",
        "\n",
        "# And put this in a dataframe\n",
        "predictions_df = pd.DataFrame()\n",
        "predictions_df['id'] = testing['id']\n",
        "predictions_df['price_doc'] = predictions\n",
        "predictions_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "e37f8909-905f-ded9-5bd4-0a1cbde5ce9c"
      },
      "outputs": [],
      "source": [
        "predictions_df.to_csv('wls_predictions.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ab58d302-acf9-b749-f186-0af73cdcfcca"
      },
      "outputs": [],
      "source": [
        "# Check for ridiculous coefficients, likely indicating collinearity\n",
        "co = lr.coef_\n",
        "ra = range(len(co))\n",
        "mask = np.abs(co)>1e4\n",
        "X.columns[mask].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "dcef7eda-34a4-156e-a35a-9ba18a6ea4de"
      },
      "outputs": [],
      "source": [
        "# Turn the model into a formula, so I can fit with statsmodels and look at the result\n",
        "# (Never mind, I'll pass data frames, but I'm keeping this code in case I need it later.)\n",
        "\n",
        "#fo = \"y ~ \"\n",
        "#i = 0\n",
        "#for col in X.columns.values:\n",
        "#    fo += col\n",
        "#    i += 1\n",
        "#    if (i<len(X.columns.values)):\n",
        "#        fo += \"+\"\n",
        "\n",
        "#import statsmodels.formula.api as sm\n",
        "#dfwls = X.copy()\n",
        "#dfwls[\"y\"] = y\n",
        "#result = sm.ols(formula=fo, data=dfwls).fit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "9268dcc4-2296-b4d3-308a-59dcb8203a0c"
      },
      "outputs": [],
      "source": [
        "from statsmodels.regression.linear_model import WLS\n",
        "xdat = X.copy().astype(np.float64)\n",
        "xdat[\"constant\"] = 1\n",
        "ydat = y.copy().astype(np.float64)\n",
        "result = WLS(ydat, xdat, weights=wts).fit()\n",
        "result.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d7cbeaaf-309e-b4e5-6950-72464680036e"
      },
      "outputs": [],
      "source": [
        "pd.DataFrame(X.columns, co)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "89c1fc80-3e88-9af4-0bab-6d0c65dc06da"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b679c280-e7e0-69b3-1388-29af0392b3a7"
      },
      "outputs": [],
      "source": [
        "# Diagnostic stuff maybe I will want to use again\n",
        "# pt = pipe.transform(X)\n",
        "# pip = pt[:,X.columns.get_loc(\"norooms\")]\n",
        "# from scipy import stats\n",
        "# print( stats.describe(pip) )\n",
        "# print( stats.describe(X.norooms))\n",
        "# pt.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "2cbec523-d5e9-053f-47a5-65d9a51aa7b5"
      },
      "outputs": [],
      "source": [
        "dfa.mortgage_growth.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "8e9d08c0-99b3-8087-3c04-dec435f0b5fb"
      },
      "outputs": [],
      "source": ""
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}