{"cells":[{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"7646c73810d475601436c096d36498cfaa489ec4"},"cell_type":"code","source":"# Basic packages\nimport pandas as pd\nimport numpy as np\nimport warnings\nimport time\nimport glob\nimport sys\nimport os\nimport gc\n\n# ML packages\nfrom scipy.stats import spearmanr\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.metrics import roc_auc_score, roc_curve\nfrom sklearn import preprocessing\nfrom sklearn.decomposition import PCA\nfrom sklearn.ensemble import RandomForestClassifier\nfrom IPython.display import display\nimport matplotlib.patches as patch\nimport matplotlib.pyplot as plt\nfrom sklearn.svm import NuSVR\nfrom scipy.stats import norm\nimport lightgbm as lgb\nimport eli5\nfrom eli5.sklearn import PermutationImportance\nfrom scipy.stats import kurtosis, skew\n\nfrom sklearn.preprocessing import StandardScaler\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.autograd import Variable\nfrom torch.utils.data import Dataset, DataLoader\n\n# visualization packages\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# execution progress bar\nfrom tqdm import tqdm_notebook, tnrange\nfrom tqdm.auto import tqdm\ntqdm.pandas()\n","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"16768a965c3ced6a76d33642e11ecae18f5977e8"},"cell_type":"code","source":"# System Setup\n%matplotlib inline\n%precision 4\nwarnings.filterwarnings('ignore')\nplt.style.use('ggplot')\nnp.set_printoptions(suppress=True)\npd.set_option(\"display.precision\", 15)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c90af9d21a49adcbd478c56871149f1282c58b7f"},"cell_type":"markdown","source":"## Load Data"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"1c883e09dbe7e4b4d7caeb2cf380ec0f07209531"},"cell_type":"code","source":"print(os.listdir(\"../input/\"))","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"7427d2d9ce7dff9d63a355631a1718a6fb6c5847"},"cell_type":"code","source":"# import Dataset to play with it\ntrain= pd.read_csv(\"../input/train.csv\")\ntest = pd.read_csv('../input/test.csv')\nsample_submission = pd.read_csv('../input/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6acf9a55418407ab1da0e7a8c4e1f540a2618ac3"},"cell_type":"code","source":"train.shape, test.shape, sample_submission.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_uuid":"1d054476a562f14b8d7e88fd2a8ea3b7b70c371c"},"cell_type":"code","source":"train.head(5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3820345bbd08346359e699ead3f125f438dbaefc"},"cell_type":"markdown","source":"##   Data Exploration"},{"metadata":{"_kg_hide-output":true,"trusted":true,"_uuid":"6e5f5d0a8e6057a19c41d0698c76cfa5f0e61202"},"cell_type":"code","source":"train.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1d31392a34a8bba87565f4a00a44084641a28f5b"},"cell_type":"code","source":"print(len(train.columns))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2b8083e0022c342963869e609a65bae49cbe2d2a"},"cell_type":"code","source":"print(train.info())","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"ca9ae169e960fddc581b8da1b15c7e79d8bbe972"},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"51042af4ef472a7863f9698fdca2d99eefa0af7d"},"cell_type":"code","source":"# distribution of targets\ncolors = ['darkseagreen','lightcoral']\nplt.figure(figsize=(6,6))\nplt.pie(train[\"target\"].value_counts(), explode=(0, 0.25), labels= [\"0\", \"1\"], startangle=45, autopct='%1.1f%%', colors=colors)\nplt.axis('equal')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"91be361922ae92592339293b37ef08391a4c79d7"},"cell_type":"code","source":"# correlation with target\nlabels = []\nvalues = []\n\nfor col in train.columns:\n    if col not in ['ID_code', 'target']:\n        labels.append(col)\n        values.append(spearmanr(train[col].values, train['target'].values)[0])\n\ncorr_df = pd.DataFrame({'col_labels': labels, 'corr_values' : values})\ncorr_df = corr_df.sort_values(by='corr_values')\n\ncorr_df = corr_df[(corr_df['corr_values']>0.03) | (corr_df['corr_values']<-0.03)]\n\nind = np.arange(corr_df.shape[0])\nwidth = 0.9\nfig, ax = plt.subplots(figsize=(12,12))\nrects = ax.barh(ind, np.array(corr_df.corr_values.values), color='darkseagreen')\nax.set_yticks(ind)\nax.set_yticklabels(corr_df.col_labels.values, rotation='horizontal')\nax.set_xlabel(\"Correlation coefficient\")\nax.set_title(\"Variable correlation to Target\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"34b3bca0f0f6bbffa07380d413998dea7d59c9c6"},"cell_type":"code","source":"# check covariance among importance variables\ncols_to_use = corr_df[(corr_df['corr_values']>0.05) | (corr_df['corr_values']<-0.05)].col_labels.tolist()\n\ntemp_df = train[cols_to_use]\ncorrmat = temp_df.corr(method='spearman')\nf, ax = plt.subplots(figsize=(18, 18))\n\n#Draw the heatmap using seaborn\nsns.heatmap(corrmat, vmax=1., square=True, cmap=\"Blues\", annot=True)\nplt.title(\"Important variables correlation map\", fontsize=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8afaaae9f83971dd442a16af434d88318d82c352"},"cell_type":"markdown","source":"## Data Preprocessing"},{"metadata":{"trusted":true,"_uuid":"bbcb30c8b016481a47e46d02ef24241f17412027"},"cell_type":"code","source":"# Check missing data for test & train\ndef check_missing_data(df):\n    flag=df.isna().sum().any()\n    if flag==True:\n        total = df.isnull().sum()\n        percent = (df.isnull().sum())/(df.isnull().count()*100)\n        output = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n        data_type = []\n        # written by MJ Bahmani\n        for col in df.columns:\n            dtype = str(df[col].dtype)\n            data_type.append(dtype)\n        output['Types'] = data_type\n        return(np.transpose(output))\n    else:\n        return(False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false,"_uuid":"6adc1fb6f7ef818946843b3698583ade2458a218","_kg_hide-output":true,"scrolled":true},"cell_type":"code","source":"print('missing in train: ',check_missing_data(train))\nprint('missing in test: ',check_missing_data(test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fd025628beacfa85beaf2d59e16dd51e1643c383"},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"294e1dc1f0c881c58ad2a34e74520384ffb2c2a9"},"cell_type":"markdown","source":"## Variable Engineering"},{"metadata":{"_uuid":"0b2d297266390c411e524b7ed3f50e1b1050dcb4"},"cell_type":"markdown","source":"#### PCA"},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"9556172e521978bdc842435d51b9a28d7f6f20e3"},"cell_type":"code","source":"'''pca_df = preprocessing.normalize(train.drop(['ID_code','target'],axis=1))\npca_test_df = preprocessing.normalize(train.drop(['ID_code'],axis=1))\n\ndef _get_number_components(model, threshold):\n    component_variance = model.explained_variance_ratio_\n    explained_variance = 0.0\n    components = 0\n\n    for var in component_variance:\n        explained_variance += var\n        components += 1\n        if(explained_variance >= threshold):\n            break\n    return components\n\n### Get the optimal number of components\npca = PCA()\ntrain_pca = pca.fit_transform(pca_df)\ntest_pca = pca.fit_transform(pca_test_df)\ncomponents = _get_number_components(pca, threshold=0.9)\ncomponents'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"22e0b9776611e831d476c5ca7cf659afea5804aa"},"cell_type":"code","source":"# Implement PCA \n#obj_pca = model = PCA(n_components = components)\n#X_pca = obj_pca.fit_transform(pca_df)\n#X_t_pca = obj_pca.fit_transform(pca_test_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1b724f15664b24b9ed035a3e60d7a4b0864650ec"},"cell_type":"code","source":"'''# add the decomposed features in the train dataset\ndef _add_decomposition(df, decomp, ncomp, flag):\n    for i in range(1, ncomp+1):\n        df[flag+\"_\"+str(i)] = decomp[:, i - 1]'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d46210fc10430134b5125332d8f393409ce8aa7e"},"cell_type":"code","source":"'''pca_train = train[['ID_code','target']]\npca_test = test[['ID_code']]\n\n_add_decomposition(pca_train, X_pca, 90, 'pca')\n_add_decomposition(pca_test, X_t_pca, 90, 'pca')'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"6682b750841ae32032177300dd1ff1fdd7cb468c"},"cell_type":"code","source":"#pca_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"367431bd787d30f747cf0dd9e1aa9ba25a27c290"},"cell_type":"code","source":"#pca_test.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"48ff8489d3fb779f19641862c95ab7c538832d3d"},"cell_type":"markdown","source":"#### Summary Stats"},{"metadata":{"trusted":true,"_uuid":"0098409228cabbf75299a67c2681acc8ec45dd61"},"cell_type":"code","source":"'''#train_df.reset_index(drop=True, inplace=True)\ntrain_stats_var = pd.concat([train[['ID_code', 'target']],\n                             train.sum(axis=1),\n                             train.mean(axis=1),\n                             train.min(axis=1),\n                             train.max(axis=1),\n                             train.median(axis=1),\n                             train.var(axis=1),\n                             train.skew(axis=1),\n                             train.apply(kurtosis, axis=1)\n                            ], axis=1)\n\ntrain_stats_var.columns = ['ID_code','target', 'txn_ttl','avg_txn', 'min_txn', 'max_txn', 'med_txn','var_txn','skew', 'kurt']\n\ntrain_stats_var.loc[train_stats_var['var_txn'].isnull(), 'var_txn'] = 0\ntrain_stats_var.loc[train_stats_var['skew'].isnull(), 'skew'] = 0\ntrain_stats_var.loc[train_stats_var['kurt'].isnull(), 'kurt'] = 0\n\ntrain_stats_var['std_txn'] = train_stats_var['var_txn']**(.5)\ntrain_stats_var.loc[train_stats_var['std_txn'].isnull(), 'std_txn'] = 0\n\ntrain_stats_var.head()'''","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2569c256bf9b387c2c0e7c06c473f0e841b6db7d"},"cell_type":"markdown","source":"## Feature importance"},{"metadata":{"trusted":true,"_uuid":"9fafaf9cadb57f7c4d51489df141aa3eb85fd2a1"},"cell_type":"code","source":"cols=[\"target\",\"ID_code\"]\nX = train.drop(cols,axis=1)\ny = train[\"target\"]\n\n#cols=[\"target\",\"ID_code\"]\n#X = pca_train.drop(cols,axis=1)\n#y = pca_train[\"target\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b79b286d0b1d7bd46525c73efb4690a7145c5cc1"},"cell_type":"code","source":"X_test  = test.drop(\"ID_code\",axis=1)\n#X_test  = pca_test.drop(\"ID_code\",axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"56c4f79c0c33bc8710ee1fd30df191c4b7030d9f"},"cell_type":"code","source":"train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8683f992f4607265d084d9399326bf1af468f7b3"},"cell_type":"code","source":"# rfc_model = RandomForestClassifier(random_state=0).fit(train_X, train_y)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"40f4fa49e4294d640b6f96aa27c4730f4be126c2"},"cell_type":"markdown","source":"### Permutation Importance"},{"metadata":{"trusted":true,"_uuid":"9adf93667cd114f0c48b9d4bc362253a4a7ef6ad"},"cell_type":"code","source":"'''perm = PermutationImportance(rfc_model, random_state=1).fit(val_X, val_y)\neli5.show_weights(perm, feature_names = val_X.columns.tolist())'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9883467c298aff26fa9067a3c52e81914b7613a8"},"cell_type":"code","source":"# features = [c for c in train.columns if c not in ['ID_code', 'target']]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bd284df5d62d611a529251e5a9e9ded42956f450"},"cell_type":"markdown","source":" ## Model Development"},{"metadata":{"trusted":true,"_uuid":"f010d5d5373b09ef3e1cb0c1dd975f9b2a7cee44"},"cell_type":"code","source":"# for get better result chage fold_n to 5\nfold_n=5\nfolds = StratifiedKFold(n_splits=fold_n, shuffle=True, random_state=10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d29905d41994b8e97236023a2339ca17dbc54018"},"cell_type":"markdown","source":"### lightgbm"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"e69efb809c800429126a0652ea2ad6d142a7748c"},"cell_type":"code","source":"# https://www.kaggle.com/dromosys/sctp-working-lgb\nparams = {'num_leaves': 9,\n         'min_data_in_leaf': 42,\n         'objective': 'binary',\n         'max_depth': 12,\n         'learning_rate': 0.05,\n         'boosting': 'gbdt',\n         'bagging_freq': 5,\n         'bagging_fraction': 0.8,\n         'feature_fraction': 0.8201,\n         'bagging_seed': 11,\n         'reg_alpha': 5,\n         'reg_lambda': 5,\n         'random_state': 42,\n         'metric': 'auc',\n         'verbosity': -1,\n         'subsample': 0.81,\n         'min_gain_to_split': 0.01077313523861969,\n         'min_child_weight': 19.428902804238373,\n         'num_threads': 4}","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true,"_uuid":"ce482d4a37fbcaa01ddcfb181c5d81fac729a1e5","scrolled":true},"cell_type":"code","source":"%%time\ny_pred_lgb = np.zeros(len(X_test))\nfor fold_n, (train_index, valid_index) in enumerate(folds.split(X,y)):\n    print('Fold', fold_n, 'started at', time.ctime())\n    X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n    y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n    \n    train_data = lgb.Dataset(X_train, label=y_train)\n    valid_data = lgb.Dataset(X_valid, label=y_valid)\n        \n    lgb_model = lgb.train(params,train_data,num_boost_round=2000,\n                    valid_sets = [train_data, valid_data],verbose_eval=300,early_stopping_rounds = 200)\n            \n    y_pred_lgb += lgb_model.predict(X_test, num_iteration=lgb_model.best_iteration)/5","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a2ad6d700b17b5efff5e374c62985b66d0019ae1"},"cell_type":"markdown","source":"### Neural Net"},{"metadata":{"trusted":true,"_uuid":"2748550bafaecec788d32a621888b64df3b8444e","scrolled":true},"cell_type":"code","source":"train_features = train.drop(['target','ID_code'], axis = 1)\ntest_features = test.drop(['ID_code'],axis = 1)\ntrain_target = train['target']\n\nsc = StandardScaler()\ntrain_features = sc.fit_transform(train_features)\ntest_features = sc.transform(test_features)\n\nn_splits = 5 # Number of K-fold Splits\nsplits = list(StratifiedKFold(n_splits=n_splits, shuffle=True).split(train_features, train_target))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eeb34d8bddbd483b90a9a3a0b864d5a60c165255"},"cell_type":"code","source":"class CyclicLR(object):\n    def __init__(self, optimizer, base_lr=1e-3, max_lr=6e-3,\n                 step_size=2000, mode='triangular', gamma=1.,\n                 scale_fn=None, scale_mode='cycle', last_batch_iteration=-1):\n\n        if not isinstance(optimizer, Optimizer):\n            raise TypeError('{} is not an Optimizer'.format(\n                type(optimizer).__name__))\n        self.optimizer = optimizer\n\n        if isinstance(base_lr, list) or isinstance(base_lr, tuple):\n            if len(base_lr) != len(optimizer.param_groups):\n                raise ValueError(\"expected {} base_lr, got {}\".format(\n                    len(optimizer.param_groups), len(base_lr)))\n            self.base_lrs = list(base_lr)\n        else:\n            self.base_lrs = [base_lr] * len(optimizer.param_groups)\n\n        if isinstance(max_lr, list) or isinstance(max_lr, tuple):\n            if len(max_lr) != len(optimizer.param_groups):\n                raise ValueError(\"expected {} max_lr, got {}\".format(\n                    len(optimizer.param_groups), len(max_lr)))\n            self.max_lrs = list(max_lr)\n        else:\n            self.max_lrs = [max_lr] * len(optimizer.param_groups)\n\n        self.step_size = step_size\n\n        if mode not in ['triangular', 'triangular2', 'exp_range'] \\\n                and scale_fn is None:\n            raise ValueError('mode is invalid and scale_fn is None')\n\n        self.mode = mode\n        self.gamma = gamma\n\n        if scale_fn is None:\n            if self.mode == 'triangular':\n                self.scale_fn = self._triangular_scale_fn\n                self.scale_mode = 'cycle'\n            elif self.mode == 'triangular2':\n                self.scale_fn = self._triangular2_scale_fn\n                self.scale_mode = 'cycle'\n            elif self.mode == 'exp_range':\n                self.scale_fn = self._exp_range_scale_fn\n                self.scale_mode = 'iterations'\n        else:\n            self.scale_fn = scale_fn\n            self.scale_mode = scale_mode\n\n        self.batch_step(last_batch_iteration + 1)\n        self.last_batch_iteration = last_batch_iteration\n\n    def batch_step(self, batch_iteration=None):\n        if batch_iteration is None:\n            batch_iteration = self.last_batch_iteration + 1\n        self.last_batch_iteration = batch_iteration\n        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):\n            param_group['lr'] = lr\n\n    def _triangular_scale_fn(self, x):\n        return 1.\n\n    def _triangular2_scale_fn(self, x):\n        return 1 / (2. ** (x - 1))\n\n    def _exp_range_scale_fn(self, x):\n        return self.gamma**(x)\n\n    def get_lr(self):\n        step_size = float(self.step_size)\n        cycle = np.floor(1 + self.last_batch_iteration / (2 * step_size))\n        x = np.abs(self.last_batch_iteration / step_size - 2 * cycle + 1)\n\n        lrs = []\n        param_lrs = zip(self.optimizer.param_groups, self.base_lrs, self.max_lrs)\n        for param_group, base_lr, max_lr in param_lrs:\n            base_height = (max_lr - base_lr) * np.maximum(0, (1 - x))\n            if self.scale_mode == 'cycle':\n                lr = base_lr + base_height * self.scale_fn(cycle)\n            else:\n                lr = base_lr + base_height * self.scale_fn(self.last_batch_iteration)\n            lrs.append(lr)\n        return lrs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8f0e1a0ab529f4561d4511f650ac9b80e5202fd1"},"cell_type":"code","source":"class Simple_NN(nn.Module):\n    def __init__(self ,input_dim ,hidden_dim, dropout = 0.2):\n        super(Simple_NN, self).__init__()\n        \n        self.inpt_dim = input_dim\n        self.hidden_dim = hidden_dim\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(dropout)\n        self.fc1 = nn.Linear(input_dim, hidden_dim)\n        self.fc2 = nn.Linear(hidden_dim, int(hidden_dim/2))\n        self.fc3 = nn.Linear(int(hidden_dim/2), int(hidden_dim/4))\n        self.fc4 = nn.Linear(int(hidden_dim/4), int(hidden_dim/8))\n        self.fc5 = nn.Linear(int(hidden_dim/8), 1)\n        self.bn1 = nn.BatchNorm1d(hidden_dim)\n        self.bn2 = nn.BatchNorm1d(int(hidden_dim/2))\n        self.bn3 = nn.BatchNorm1d(int(hidden_dim/4))\n        self.bn4 = nn.BatchNorm1d(int(hidden_dim/8))\n    \n    def forward(self, x):\n        y = self.fc1(x)\n        y = self.relu(y)\n        #y = self.bn1(y)\n        y = self.dropout(y)\n        \n        y = self.fc2(y)\n        y = self.relu(y)\n        #y = self.bn2(y)\n        y = self.dropout(y)\n        \n        y = self.fc3(y)\n        y = self.relu(y)\n        #y = self.bn3(y)\n        y = self.dropout(y)\n        \n        y = self.fc4(y)\n        y = self.relu(y)\n        #y = self.bn4(y)\n        y = self.dropout(y)\n        \n        out= self.fc5(y)\n        \n        return out","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"28dc622c3019926fe5cfe59c13b9695f9d17dbb3"},"cell_type":"code","source":"def sigmoid(x):\n    return 1 / (1 + np.exp(-x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3a1a9aa9207077d797f76b04d2a3d0ffd8cceefe"},"cell_type":"code","source":"model = Simple_NN(200,512)\nmodel.cuda()\noptimizer = torch.optim.Adam(model.parameters(), lr = 0.0002) # Using Adam optimizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"99c05ca3e531ed22c2d1fb27d4bdeeb96fdbf126"},"cell_type":"code","source":"from torch.optim.optimizer import Optimizer\nn_epochs = 40\nbatch_size = 25000\n\ntrain_preds = np.zeros((len(train_features)))\ntest_preds = np.zeros((len(test_features)))\n\nx_test = np.array(test_features)\nx_test_cuda = torch.tensor(x_test, dtype=torch.float).cuda()\ntest = torch.utils.data.TensorDataset(x_test_cuda)\ntest_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False)\n\navg_losses_f = []\navg_val_losses_f = []\n\nfor i, (train_idx, valid_idx) in enumerate(splits):  \n    x_train = np.array(train_features)\n    y_train = np.array(train_target)\n    \n    x_train_fold = torch.tensor(x_train[train_idx.astype(int)], dtype=torch.float).cuda()\n    y_train_fold = torch.tensor(y_train[train_idx.astype(int), np.newaxis], dtype=torch.float32).cuda()\n    \n    x_val_fold = torch.tensor(x_train[valid_idx.astype(int)], dtype=torch.float).cuda()\n    y_val_fold = torch.tensor(y_train[valid_idx.astype(int), np.newaxis], dtype=torch.float32).cuda()\n    \n    loss_fn = torch.nn.BCEWithLogitsLoss()\n    \n    step_size = 300\n    base_lr, max_lr = 0.0001, 0.001  \n    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), \n                             lr=max_lr)\n    \n    ################################################################################################\n    scheduler = CyclicLR(optimizer, base_lr=base_lr, max_lr=max_lr,\n               step_size=step_size, mode='exp_range',\n               gamma=0.99994)\n    ###############################################################################################\n\n    train = torch.utils.data.TensorDataset(x_train_fold, y_train_fold)\n    valid = torch.utils.data.TensorDataset(x_val_fold, y_val_fold)\n    \n    train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n    valid_loader = torch.utils.data.DataLoader(valid, batch_size=batch_size, shuffle=False)\n    \n    print(f'Fold {i + 1}')\n    for epoch in range(n_epochs):\n        start_time = time.time()\n        model.train()\n        avg_loss = 0.\n        #avg_auc = 0.\n        for i, (x_batch, y_batch) in enumerate(train_loader):\n            y_pred = model(x_batch)\n            #########################\n            if scheduler:\n                #print('cycle_LR')\n                scheduler.batch_step()\n            ########################\n            loss = loss_fn(y_pred, y_batch)\n\n            optimizer.zero_grad()\n            loss.backward()\n\n            optimizer.step()\n            avg_loss += loss.item()/len(train_loader)\n            #avg_auc += round(roc_auc_score(y_batch.cpu(),y_pred.detach().cpu()),4) / len(train_loader)\n        model.eval()\n        \n        valid_preds_fold = np.zeros((x_val_fold.size(0)))\n        test_preds_fold = np.zeros((len(test_features)))\n        \n        avg_val_loss = 0.\n        #avg_val_auc = 0.\n        for i, (x_batch, y_batch) in enumerate(valid_loader):\n            y_pred = model(x_batch).detach()\n            \n            #avg_val_auc += round(roc_auc_score(y_batch.cpu(),sigmoid(y_pred.cpu().numpy())[:, 0]),4) / len(valid_loader)\n            avg_val_loss += loss_fn(y_pred, y_batch).item() / len(valid_loader)\n            valid_preds_fold[i * batch_size:(i+1) * batch_size] = sigmoid(y_pred.cpu().numpy())[:, 0]\n            \n        elapsed_time = time.time() - start_time \n        print('Epoch {}/{} \\t loss={:.4f} \\t val_loss={:.4f} \\t time={:.2f}s'.format(\n            epoch + 1, n_epochs, avg_loss, avg_val_loss, elapsed_time))\n        \n    avg_losses_f.append(avg_loss)\n    avg_val_losses_f.append(avg_val_loss) \n    \n    for i, (x_batch,) in enumerate(test_loader):\n        y_pred = model(x_batch).detach()\n\n        test_preds_fold[i * batch_size:(i+1) * batch_size] = sigmoid(y_pred.cpu().numpy())[:, 0]\n        \n    train_preds[valid_idx] = valid_preds_fold\n    test_preds += test_preds_fold / len(splits)\n\nauc  =  round(roc_auc_score(train_target,train_preds),4)      \nprint('All \\t loss={:.4f} \\t val_loss={:.4f} \\t auc={:.4f}'.format(np.average(avg_losses_f),np.average(avg_val_losses_f),auc))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eaba9811051def6dde9138636b495773107c81cf"},"cell_type":"code","source":"esemble = 0.6*y_pred_lgb + 0.4* train_preds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"abdbaa060a913ca9f1180f3ff1bd416410d35900"},"cell_type":"code","source":"print('NN auc = {:<8.5f}'.format(auc))\nprint('LightBGM auc = {:<8.5f}'.format(roc_auc_score(train_target, y_pred_lgb)))\nprint('NN+LightBGM auc = {:<8.5f}'.format(roc_auc_score(train_target, esemble)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b576919e7514d144a1a34933d98c341bdc93a854"},"cell_type":"markdown","source":"## Submission Files"},{"metadata":{"trusted":true,"_uuid":"40990b380a27e97f70caacbf81c6852a3842f0da"},"cell_type":"code","source":"submission_lgb = pd.DataFrame({\n        \"ID_code\": test[\"ID_code\"],\n        \"target\": y_pred_lgb\n    })\nsubmission_lgb.to_csv('submission_lgb.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"747a24aff77f385dea6aafd20f7c1ff02e4a8e15"},"cell_type":"code","source":"submission_nn = pd.DataFrame({\n        \"ID_code\": test[\"ID_code\"],\n        \"target\": test_preds\n    })\nsubmission_nn.to_csv('submission_nn.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"f3ebdc8b597def689a0ecf65c1bc3fbc7c80f167"},"cell_type":"code","source":"submission_ens = pd.DataFrame({\n        \"ID_code\": test[\"ID_code\"],\n        \"target\": esemble\n    })\nsubmission_ens.to_csv('submission_ens.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f07cdaa9c8022f6ded23b632f67d805e0c4f5b4b"},"cell_type":"markdown","source":" <a id=\"55\"></a> <br>\n## Stacking"},{"metadata":{"trusted":true,"_uuid":"9f1eeb3307d3c660c68cec153273b7e14e43ed73"},"cell_type":"code","source":"'''submission_rfc_cat = pd.DataFrame({\n        \"ID_code\": test[\"ID_code\"],\n        \"target\": (y_pred_rfc +y_pred_cat)/2\n    })\nsubmission_rfc_cat.to_csv('submission_rfc_cat.csv', index=False)'''","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a4fcaac6e4e792e6313b496c578aeaf5329ec5a3"},"cell_type":"markdown","source":"# References & credits\nThanks fo following kernels that help me to create this kernel."},{"metadata":{"_uuid":"f67456896c63e2834e461c68ec6a960b60789663"},"cell_type":"markdown","source":"1. [https://www.kaggle.com/dansbecker/permutation-importance](https://www.kaggle.com/dansbecker/permutation-importance)\n1. [https://www.kaggle.com/dansbecker/partial-plots](https://www.kaggle.com/dansbecker/partial-plots)\n1. [https://www.kaggle.com/miklgr500/catboost-with-gridsearch-cv](https://www.kaggle.com/miklgr500/catboost-with-gridsearch-cv)\n1. [https://www.kaggle.com/dromosys/sctp-working-lgb](https://www.kaggle.com/dromosys/sctp-working-lgb)\n1. [https://www.kaggle.com/gpreda/santander-eda-and-prediction](https://www.kaggle.com/gpreda/santander-eda-and-prediction)\n1. [permutation-importance](https://www.kaggle.com/dansbecker/permutation-importance)\n1. [partial-plots](https://www.kaggle.com/dansbecker/partial-plots)\n1. [https://www.kaggle.com/dansbecker/shap-values](https://www.kaggle.com/dansbecker/shap-values)\n1. [algorithm-choice](https://docs.microsoft.com/en-us/azure/machine-learning/studio/algorithm-choice)"},{"metadata":{"_uuid":"6ef2c570b8457a851fc753134b587d61a4d9082e"},"cell_type":"markdown","source":"Go to first step: [Course Home Page](https://www.kaggle.com/mjbahmani/10-steps-to-become-a-data-scientist)\n\nGo to next step : [Titanic](https://www.kaggle.com/mjbahmani/a-comprehensive-ml-workflow-with-python)\n"},{"metadata":{"_uuid":"c1f7f7d015529da63ac495e5c2dcd08dc563e249"},"cell_type":"markdown","source":"# Not Completed yet!!!"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}