{"metadata": {"language_info": {"nbconvert_exporter": "python", "version": "3.6.3", "mimetype": "text/x-python", "file_extension": ".py", "codemirror_mode": {"version": 3, "name": "ipython"}, "pygments_lexer": "ipython3", "name": "python"}, "kernelspec": {"language": "python", "display_name": "Python 3", "name": "python3"}}, "nbformat": 4, "cells": [{"metadata": {"_cell_guid": "f8206687-5637-4a63-bd90-199cfd4db707", "_uuid": "0ce971937ef9a2dd65023f09eee9fc6635160f34"}, "cell_type": "markdown", "source": ["# Statoil/C-CORE Iceberg Classifier Challenge\n", "\n", "## Summary\n", "\n", "This kernel explain the best approach that the result in the highest accuracy I had made in this competition. It will also discuss myriad of more complicated approach which didn't work as well as this simple approach.\n", "\n", "## Reading in the data"]}, {"metadata": {"_cell_guid": "12d9219e-1183-4362-b32e-0b95467ad117", "_uuid": "76bd850ca1ab07ca2d5906a6f2d3a2b1aeca517f"}, "outputs": [], "execution_count": null, "cell_type": "code", "source": ["import numpy as np # linear algebra\n", "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n", "\n", "from subprocess import check_output\n", "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n", "train_df = pd.read_json(\"../input/train.json\")\n", "test_df = pd.read_json(\"../input/test.json\")"]}, {"metadata": {"_cell_guid": "ffb91227-c385-4c42-aa60-acd277b4fd36", "_uuid": "b348235c2cd66a45b4d2c281bab8c46b30369016"}, "cell_type": "markdown", "source": ["## Preprocessing the data\n", "\n", "Here I take in the data and separate them into 2 bands. This is a very simple and typical ways to precess the data. \n", "\n", "I have also try diffrent ways to preprocess it like fliping the image, separate them into 3 bands, ... But it only seem to make the accuracy decrease."]}, {"metadata": {"_cell_guid": "cef968a2-2830-4bbc-b48d-0730a832599a", "_uuid": "7681b4cda702228fcdac2edc576e05f7df8a9887"}, "outputs": [], "execution_count": null, "cell_type": "code", "source": ["# Train data\n", "x_band1 = np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in train_df[\"band_1\"]])\n", "x_band2 = np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in train_df[\"band_2\"]])\n", "X_train = np.concatenate([x_band1[:, :, :, np.newaxis], x_band2[:, :, :, np.newaxis]], axis=-1)\n", "y_train = np.array(train_df[\"is_iceberg\"])\n", "print(\"Xtrain:\", X_train.shape)\n", "\n", "# Test data\n", "x_band1 = np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in test_df[\"band_1\"]])\n", "x_band2 = np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in test_df[\"band_2\"]])\n", "X_test = np.concatenate([x_band1[:, :, :, np.newaxis], x_band2[:, :, :, np.newaxis]], axis=-1)\n", "print(\"Xtest:\", X_test.shape)"]}, {"metadata": {"_cell_guid": "6acbc73e-ffb9-4c7d-bf9d-55c7153f8c71", "_uuid": "7a7e3ddfce76dab4b5f0a1d8e1baec0bc9d881f4"}, "outputs": [], "execution_count": null, "cell_type": "code", "source": ["from matplotlib import pyplot\n", "from keras.preprocessing.image import ImageDataGenerator\n", "from keras.models import Sequential\n", "from keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Input, Flatten, Activation\n", "from keras.layers import GlobalMaxPooling2D\n", "from keras.layers.normalization import BatchNormalization\n", "from keras.layers.merge import Concatenate\n", "from keras.models import Model\n", "from keras import initializers\n", "from keras.optimizers import Adam\n", "from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping\n", "from sklearn.model_selection import train_test_split\n", "import os"]}, {"metadata": {"_cell_guid": "ecba9e01-42d8-4fae-94a1-5d0cc00a8d53", "_uuid": "046f0b08b2698b8dd56caa8701c4b0efa337bed3"}, "cell_type": "markdown", "source": ["## Building the model\n", "\n", "Here is the CNN models which I used to train my data. \n", "\n", "I have of course try different parameter and layers for the model, but this seem to be working the best. Also this is the particular model which most other kernels used in terms of number of layers."]}, {"metadata": {"_cell_guid": "70c9c207-2d2a-4fa0-8b73-d55191957454", "_uuid": "5151cbbf3d81a1cbcbbce6137d5c55d3a282989c", "collapsed": true}, "outputs": [], "execution_count": null, "cell_type": "code", "source": ["def getModel():\n", "    #Building the model\n", "    gmodel=Sequential()\n", "    #Conv Layer 1\n", "    gmodel.add(Conv2D(64, kernel_size=(3, 3),activation='relu', input_shape=(75, 75, 2)))\n", "    gmodel.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))\n", "    gmodel.add(Dropout(0.2))\n", "\n", "    #Conv Layer 2\n", "    gmodel.add(Conv2D(64, kernel_size=(3, 3), activation='relu' ))\n", "    gmodel.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n", "    gmodel.add(Dropout(0.2))\n", "\n", "    #Conv Layer 3\n", "    gmodel.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n", "    gmodel.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n", "    gmodel.add(Dropout(0.2))\n", "\n", "    #Conv Layer 4\n", "    gmodel.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n", "    gmodel.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n", "    gmodel.add(Dropout(0.2))\n", "\n", "    #Flatten the data for upcoming dense layers\n", "    gmodel.add(Flatten())\n", "\n", "    #Dense Layers\n", "    gmodel.add(Dense(512))\n", "    gmodel.add(Activation('relu'))\n", "    gmodel.add(Dropout(0.2))\n", "\n", "    #Dense Layer 2\n", "    gmodel.add(Dense(256))\n", "    gmodel.add(Activation('relu'))\n", "    gmodel.add(Dropout(0.2))\n", "\n", "    #Sigmoid Layer\n", "    gmodel.add(Dense(1))\n", "    gmodel.add(Activation('sigmoid'))\n", "\n", "    mypotim=Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n", "    gmodel.compile(loss='binary_crossentropy',\n", "                  optimizer=mypotim,\n", "                  metrics=['accuracy'])\n", "    gmodel.summary()\n", "    return gmodel\n", "def get_callbacks(filepath, patience=2):\n", "    es = EarlyStopping('val_loss', patience=patience, mode=\"min\")\n", "    msave = ModelCheckpoint(filepath, save_best_only=True)\n", "    return [es, msave]\n", "file_path = \".model_weights.hdf5\"\n", "callbacks = get_callbacks(filepath=file_path, patience=5)"]}, {"metadata": {"_cell_guid": "593adccb-7a1b-4977-9175-ef25dd551a84", "_uuid": "2d3cd653c5bb31ccb234293b5ec071f89287f59f"}, "outputs": [], "execution_count": null, "cell_type": "code", "source": ["X_train_cv, X_valid, y_train_cv, y_valid = train_test_split(X_train, y_train, random_state=1, train_size=0.75)"]}, {"metadata": {"_cell_guid": "a8e46a34-ec4d-43ff-a76c-8998ba347394", "_uuid": "3157510fc50cc31b8c048620f81987aaeaeeb891"}, "outputs": [], "execution_count": null, "cell_type": "code", "source": ["model=getModel()\n", "model.fit(X_train_cv, y_train_cv,\n", "          batch_size=24,\n", "          epochs=50,\n", "          verbose=1,\n", "          validation_data=(X_valid, y_valid))"]}, {"metadata": {"_cell_guid": "63b6113a-ce7d-43b7-830c-ab1252f062e2", "_kg_hide-output": false, "_uuid": "453ebf10533bb17fd39334f8afedb1cc2bbdae5c", "collapsed": true, "scrolled": true}, "outputs": [], "execution_count": null, "cell_type": "code", "source": ["model.load_weights(filepath=file_path)\n", "predicted_test=model.predict_proba(X_test)"]}, {"metadata": {"_cell_guid": "5cd81f77-930f-444e-967c-5d9681719dc8", "_uuid": "2944092a73d076526b8b8e95a44a1e1b3021e213"}, "cell_type": "markdown", "source": ["## Conclusion\n", "\n", "Through this competition, I was able to learn many to preprocess an image input. Unfortunenately, the one turn outs to work out the best was the most simple one. I think what I could do more is try more different technique like using more than 1 model to predict and stacking. "]}, {"metadata": {"_cell_guid": "109e09a2-dd21-4e5d-be37-3651b546ef30", "_uuid": "d66d86c1741afcbff9dac4fc2f0fbdbe3987d2f9", "collapsed": true}, "outputs": [], "execution_count": null, "cell_type": "code", "source": ["submit_df = pd.DataFrame({'id': test_df[\"id\"], 'is_iceberg': predicted_test.flatten()})\n", "submit_df.to_csv(\"./naive_submission.csv\", index=False)"]}], "nbformat_minor": 1}