{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"Approaching NLP Problem on Kaggle with pytorch\n\nIn this post I'll talk about approaching natural language processing problems on Kaggle. As an example, we will use the data from this competition. We will create a very basic first model first and then improve it using different other features. We will also see how deep neural networks can be used and end this post with some ideas about ensembling in general.\n\n### This covers:\n- tfidf \n- count features\n- logistic regression\n- naive bayes\n- svm\n- xgboost\n- grid search\n- word vectors\n- LSTM\n- GRU\n- Ensembling\n\n\n"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport xgboost as xgb\nfrom tqdm import tqdm\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom sklearn.svm import SVC\nfrom sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom nltk import word_tokenize\nfrom nltk.corpus import stopwords\nstop_words = stopwords.words('english')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ec37d09cb82ff1c83b3f6b16307fe8d1b980dba7"},"cell_type":"markdown","source":"Let's load the datasets"},{"metadata":{"trusted":true,"_uuid":"4a673bfa9e5f901247040c9d12deb524bf26206d"},"cell_type":"code","source":"train = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\nsample = pd.read_csv('../input/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5f9839ea3138eb73d7490d872e2eab52cbc9e420"},"cell_type":"markdown","source":"The problem requires us to predict the author, i.e. EAP, HPL and MWS given the text. In simpler words, text classification with 3 different classes."},{"metadata":{"trusted":true,"_uuid":"ac1c5478fa60a4f4e2a2288587d283419bf365e7"},"cell_type":"code","source":"def multiclass_logloss(actual, predicted, eps=1e-15):\n    \"\"\"Multi class version of Logarithmic Loss metric.\n    :param actual: Array containing the actual target classes\n    :param predicted: Matrix with class predictions, one probability per class\n    \"\"\"\n    # Convert 'actual' to a binary array if it's not already:\n    if len(actual.shape) == 1:\n        actual2 = np.zeros((actual.shape[0], predicted.shape[1]))\n        for i, val in enumerate(actual):\n            actual2[i, val] = 1\n        actual = actual2\n\n    clip = np.clip(predicted, eps, 1 - eps)\n    rows = actual.shape[0]\n    vsota = np.sum(actual * np.log(clip))\n    return -1.0 / rows * vsota","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4a39ba014b9f0af8b4cf38437e556997a534a159"},"cell_type":"markdown","source":"We use the LabelEncoder from scikit-learn to convert text labels to integers, 0, 1 2"},{"metadata":{"trusted":true,"_uuid":"a7bbef70bb42e6c0c6c8de05586bb228e5725243"},"cell_type":"code","source":"lbl_enc = preprocessing.LabelEncoder()\ny = lbl_enc.fit_transform(train.author.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1d85f74c444e912231893c816e1f0e8281934d3d"},"cell_type":"markdown","source":"Before going further it is important that we split the data into training and validation sets. We can do it using `train_test_split` from the `model_selection` module of scikit-learn."},{"metadata":{"trusted":true,"_uuid":"af5029796b9f0a8b760300361638e2706bd2421a"},"cell_type":"code","source":"xtrain, xvalid, ytrain, yvalid = train_test_split(train.text.values, y, \n                                                  stratify=y, \n                                                  random_state=42, \n                                                  test_size=0.1, shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4c48b8014201eaac70b29a849d0f97d9840c3220"},"cell_type":"code","source":"print (xtrain.shape)\nprint (xvalid.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"62a1442aa51c29e3028723a1c445708229dda94c"},"cell_type":"markdown","source":"## Building Basic Models\n\nLet's start building our very first model. \n\nOur very first model is a simple TF-IDF (Term Frequency - Inverse Document Frequency) followed by a simple Logistic Regression."},{"metadata":{"trusted":true,"_uuid":"c713862622362f7679aa27cb66c5979b0cfe01d7"},"cell_type":"code","source":"# Always start with these features. They work (almost) everytime!\ntfv = TfidfVectorizer(min_df=3,  max_features=None, \n            strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n            ngram_range=(1, 3), use_idf=1,smooth_idf=1,sublinear_tf=1,\n            stop_words = 'english')\n\n# Fitting TF-IDF to both training and test sets (semi-supervised learning)\ntfv.fit(list(xtrain) + list(xvalid))\nxtrain_tfv =  tfv.transform(xtrain) \nxvalid_tfv = tfv.transform(xvalid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0a052d465115f51d571ac2cff95edc20cc758bbf"},"cell_type":"code","source":"# Fitting a simple Logistic Regression on TFIDF\nclf = LogisticRegression(C=1.0)\nclf.fit(xtrain_tfv, ytrain)\npredictions = clf.predict_proba(xvalid_tfv)\n\nprint (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9daa3edbfcc491eb2ac6c739a1f32da8b9106df1"},"cell_type":"markdown","source":"And there we go. We have our first model with a multiclass logloss of 0.626.\n\nBut we are greedy and want a better score. Lets look at the same model with a different data.\n\nInstead of using TF-IDF, we can also use word counts as features. This can be done easily using CountVectorizer from scikit-learn."},{"metadata":{"trusted":true,"_uuid":"3ec56cec8e3950c930ce6aa0fc3edc919dc9f381"},"cell_type":"code","source":"ctv = CountVectorizer(analyzer='word',token_pattern=r'\\w{1,}',\n            ngram_range=(1, 3), stop_words = 'english')\n\n# Fitting Count Vectorizer to both training and test sets (semi-supervised learning)\nctv.fit(list(xtrain) + list(xvalid))\nxtrain_ctv =  ctv.transform(xtrain) \nxvalid_ctv = ctv.transform(xvalid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3e3318eae12890f410ac7a978d54de0490a4f872"},"cell_type":"code","source":"# Fitting a simple Logistic Regression on Counts\nclf = LogisticRegression(C=1.0)\nclf.fit(xtrain_ctv, ytrain)\npredictions = clf.predict_proba(xvalid_ctv)\n\nprint (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"532e001354a42114c7b9a1b09bf43f3ff5746ceb"},"cell_type":"markdown","source":"Aaaaanddddddd Wallah! We just no improved our first model by 0.1!!!\n\nNext, let's try a very simple model which was quite famous in ancient times - Naive Bayes.\n\nLet's see what happens when we use naive bayes on these two datasets:"},{"metadata":{"trusted":true,"_uuid":"bfc93564c91e12b86e1876a915d834082e9b1054"},"cell_type":"code","source":"# Fitting a simple Naive Bayes on TFIDF\nclf = MultinomialNB()\nclf.fit(xtrain_tfv, ytrain)\npredictions = clf.predict_proba(xvalid_tfv)\n\nprint (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d49e7391230ee539b433247080f63e356c0d5689"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3d9617b6e07e54b988d072ec9317b56fc4796658"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fd65feae2e91cba0fc7046010b7a080478bdbe33"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"efa1388f6257956397782eb97176380b2c284b05"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e2a5db587661c7399ea277a264d7ffb84c4f46df"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c53fb473930e79e7e432857cf479c77f854abb8e"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2b47c6c144e92df75ecd0ca08ba0d592ffe9e592"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b65fa8e17aec7fdcf00bf90eb6688ad1327a6582"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e214a76732c75c9ea783907cd8bb9c494c85c8cf"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b0d312d2d81c8f739b9a4315abcba06b5cc6cce9"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"beeec9715514d918080bf8577e9b77cb622cb1bc"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"489fa094971792a434a87221e76a615f68d1feb2"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ee281aea389bd1ac8efd925d37f8ff05cf655c3e"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4165135b62285f0b4ba00c7f5073b984c636923f"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"222b75d36086a4b546cd1004cfa87df84910f432"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"399f0cec8ba6203561b083bce7204a8161d12c78"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"841889769211942f72e9f2dfa4a914e47dcba671"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ef89aedb9beceec11d8483baa5d31b3bc4a5f14b"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7ef3713ec8c6e3d280956efc50ef4aac01fccccc"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"899415bf1ccff7282621828a4a3b70c316602fbe"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9a36991e083ea2525370a460fb0b85311cfdb4f8"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d65d9dbb020c2ff9047082d86491351d6d8193eb"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b262b2b0283b65b66b6d288cafda46ceaef47326"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1b3c02c8e3373da91aca2233c11ad678be7398e5"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"957fc108e163a50b81f63885579971a72798e74c"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1ada9a22ce702c4398781386a655580d1fdcf242"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9301a5d3a9dbdd6acb2a3a80d1bfd33e57f33413"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4e702b9e4041ee0d33ba6a2cf675f7928212ccf2"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4b3b05ed41a4ae48db0951f0c37f4260abf5cb8d"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ce86dedb58f605b9a2b8cb56e54d7de624f1639d"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"329082d7a60055cd9e050be47e0648aedf24d4f2"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a137c595a738707683e0156a51d0ef8b09e501a9"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"57871de09b026a3a448128d3aae9304abc576fec"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a83fb7423e12a8d281d5cbec75f6553a2b8d353f"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"967d48e06699aef58b8ad2b8d02ff837ea7fb0c5"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0c6fbf439fcc09cf5cd2488930f2a3c2bc38b8de"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1b8fb4d6b14db3a5e762302c04dd96a28ac55489"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e86492048eeb72ee2f88168fea390cfb3a585cbe"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8d3a8f7f46646bf6aa02db93775bf934af70555c"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cddf34fa73eee9f71504c3c125c168a68a432d4d"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"583aedb3782722580096334ca7f7c3ed21b0a0f2"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0e7b3910f9cb951648c13f8cf3322759a51dbbe2"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"451ea1a36211407b5356312ffc4e838944ec88d2"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}