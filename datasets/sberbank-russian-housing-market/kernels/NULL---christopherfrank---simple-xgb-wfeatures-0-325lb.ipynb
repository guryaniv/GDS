{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "9c9dd449-7187-2428-5cd7-4237942c4620"
      },
      "source": [
        "Using XGB and some basic features this single model scored me a 0.325 on LB.\n",
        "\n",
        "This is one of my first kernels and only second competition so please feel free to point out anything I could do better or any flaws in my work so far.   \n",
        "\n",
        "I drew inspiration for a bunch of features from previous Kaggle competitions but also from a few very useful notebooks.  \n",
        "\n",
        "Most notably:\n",
        "- https://www.kaggle.com/sudalairajkumar/feature-engineering-validation-strategy\n",
        "-  https://www.kaggle.com/bguberfain/naive-xgb-lb-0-317\n",
        "\n",
        "Thank you!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "52617b5c-dd86-e2ba-4fa5-e4dc663ada2e"
      },
      "outputs": [],
      "source": [
        "import numpy as np \n",
        "import pandas as pd "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "4443d20b-9d9c-595b-1ed8-d0a101d1a946"
      },
      "outputs": [],
      "source": [
        "#Load Data\n",
        "train_raw = pd.read_csv('../input/train.csv', parse_dates=['timestamp'])\n",
        "test_raw = pd.read_csv('../input/test.csv', parse_dates=['timestamp'])\n",
        "macro_raw = pd.read_csv('../input//macro.csv', parse_dates=['timestamp']) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "2dc5293f-d0b2-8c9d-cd9f-0e01e2a8c156"
      },
      "outputs": [],
      "source": [
        "#Join macro-economic data\n",
        "train_full = pd.merge(train_raw, macro_raw, how='left', on='timestamp')\n",
        "test_full = pd.merge(test_raw, macro_raw, how='left', on='timestamp')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "c5557637-037e-c4a1-064d-0899925e0ead"
      },
      "source": [
        "## Feature Engineering\n",
        "#### First encode categorical features\n",
        "Question - I have seen pd.get_dummies(), pd.factorize, one hot encoding, and label encoder.  I understand how one hot encoding works, but is there a difference or a reason why get_dummies, factorize, and label encoder should be used over another?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "5003ecc6-2e7d-dc6f-6f50-ab27c893286e"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "def encode_object_features(train, test):\n",
        "    '''(DataFrame, DataFrame) -> DataFrame, DataFrame\n",
        "    \n",
        "    Will encode each non-numerical column.\n",
        "    '''\n",
        "    train = pd.DataFrame(train)\n",
        "    test = pd.DataFrame(test)\n",
        "    cols_to_encode = train.select_dtypes(include=['object'], exclude=['int64', 'float64']).columns\n",
        "    for col in cols_to_encode:\n",
        "        le = LabelEncoder()\n",
        "        #Fit encoder\n",
        "        le.fit(list(train[col].values.astype('str')) + list(test[col].values.astype('str')))\n",
        "        #Transform\n",
        "        train[col] = le.transform(list(train[col].values.astype('str')))\n",
        "        test[col] = le.transform(list(test[col].values.astype('str')))\n",
        "    \n",
        "    return train, test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "79dd6eb2-b9e4-3e82-b38d-343c58284b06"
      },
      "outputs": [],
      "source": [
        "train_df, test_df = encode_object_features(train_full, test_full)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "c9b9d4a8-abb6-9969-4e6c-be642e23dd5c"
      },
      "source": [
        "Add new features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "1be2f93a-5633-d0fd-ae3e-15635d5fd767"
      },
      "outputs": [],
      "source": [
        "def add_date_features(df):\n",
        "    '''(DataFrame) -> DataFrame\n",
        "    \n",
        "    Will add some specific columns based on the date\n",
        "    of the sale.\n",
        "    '''\n",
        "    #Convert to datetime to make extraction easier\n",
        "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
        "    #Extract features\n",
        "    df['month'] = df['timestamp'].dt.month\n",
        "    df['day'] = df['timestamp'].dt.day\n",
        "    df['year'] = df['timestamp'].dt.year\n",
        "    \n",
        "    #These features inspired by Bruno's Notebook at https://www.kaggle.com/bguberfain/naive-xgb-lb-0-317\n",
        "    #Month-Year\n",
        "    month_year = df['timestamp'].dt.month + df['timestamp'].dt.year * 100\n",
        "    month_year_map = month_year.value_counts().to_dict()\n",
        "    df['month_year'] = month_year.map(month_year_map)\n",
        "    #Week-Year\n",
        "    week_year = df['timestamp'].dt.weekofyear + df['timestamp'].dt.year * 100\n",
        "    week_year_map = week_year.value_counts().to_dict()\n",
        "    df['week_year'] = week_year.map(week_year_map)\n",
        "    df.drop('timestamp', axis=1, inplace=True)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "11c18330-9817-5845-1abe-44f547c2f693"
      },
      "outputs": [],
      "source": [
        "def add_state_features(df):\n",
        "    '''(DataFrame) -> DataFrame\n",
        "    \n",
        "    Add's features, meant to be used for both train and test df's.\n",
        "    Does some operations to the state grouping\n",
        "    '''\n",
        "    #Get median of full sq by state\n",
        "    df['state_median_full_sq'] = df['full_sq'].groupby(df['state']).transform('median')\n",
        "    #Build features from full sq median by state\n",
        "    df['full_sq_state_median_diff'] = df['full_sq'] - df['state_median_full_sq']\n",
        "    df['life_sq_state_median_full_diff'] = df['life_sq'] - df['state_median_full_sq']\n",
        "    #Drop helper columns\n",
        "    df.drop('state_median_full_sq', axis=1, inplace=True)\n",
        "    \n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "8f987756-0d39-a2b4-c7ef-9c7ae498df26"
      },
      "outputs": [],
      "source": [
        "def add_features(df):\n",
        "    '''(DataFrame) -> DataFrame\n",
        "    \n",
        "    Add's features, meant to be used for both train and test df's.\n",
        "    '''\n",
        "    #Floor\n",
        "    df['floor_ratio'] = df['floor'] / df['max_floor'].astype(float)\n",
        "    df['floor_from_top'] = df['max_floor'] - df['floor']\n",
        "    #Sq areas\n",
        "    df['kitch_sq_ratio'] = df['kitch_sq'] / df['full_sq'].astype(float)\n",
        "    df['life_sq_ratio'] = df['life_sq'] / df['full_sq'].astype(float)\n",
        "    df['full_sq_per_room'] = df['full_sq'] / df['num_room'].astype(float)\n",
        "    df['life_sq_per_room'] = df['life_sq'] / df['num_room'].astype(float)\n",
        "    df['full_living_sq_diff'] = df['full_sq'] - df['life_sq']\n",
        "    #df['full_sq_per_floor'] = df['full_sq'] / df['max_floor'].astype(float) #No value added\n",
        "    df = add_date_features(df)\n",
        "    df = add_state_features(df)\n",
        "    df['build_year_vs_year_diff'] = df['build_year'] - df['year']  #no change\n",
        "    \n",
        "    #Drop Id -> Made it worse\n",
        "    #df.drop('id', axis=1, inplace=True)\n",
        "    \n",
        "    #School Variables -> Made it worse\n",
        "    #df['preschool_quota_ratio'] = df[\"children_preschool\"] / df[\"preschool_quota\"].astype(\"float\")\n",
        "    #df['school_quota_ratio'] = df[\"children_school\"] / df[\"school_quota\"].astype(\"float\")\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "5a93dfb6-9cdf-b494-cfb2-dbcefab4462e"
      },
      "outputs": [],
      "source": [
        "train_df = add_features(train_df)\n",
        "test_df = add_features(test_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "7bef5c91-5ee6-50a7-739e-a138f996ccf0"
      },
      "outputs": [],
      "source": [
        "train_df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "8981f25a-dd66-889d-1a70-123883c0ae31"
      },
      "source": [
        "## Cross-Validate\n",
        "\n",
        "Here I use cross-validation to test my new features. After, I also train a model to take a look at the feature_importances as determined by the XGB algorithm. These importances can give you ideas of which features to focus on for further feature engineering."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "9db67b0d-0bb7-a943-2ccf-fc8d41c55a11"
      },
      "outputs": [],
      "source": [
        "from xgboost import XGBRegressor\n",
        "from sklearn.model_selection import TimeSeriesSplit, cross_val_score\n",
        "\n",
        "#Get Data\n",
        "#Y_train = train_df['price_doc'].values\n",
        "Y_train = np.log1p(train_df['price_doc'].values)\n",
        "X_train = train_df.ix[:, train_df.columns != 'price_doc'].values\n",
        "X_test = test_df.values\n",
        "\n",
        "#Initialize Model\n",
        "xgb = XGBRegressor()\n",
        "#Create cross-validation\n",
        "cv = TimeSeriesSplit(n_splits=5)\n",
        "#Train & Test Model\n",
        "cross_val_results = cross_val_score(xgb, X_train, Y_train, cv=cv, scoring='neg_mean_squared_error')\n",
        "print(cross_val_results.mean())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "8cde2d22-dca9-4af4-9ae6-54de738ab264"
      },
      "outputs": [],
      "source": [
        "model = xgb.fit(X_train, Y_train)\n",
        "model.feature_importances_;\n",
        "#Python 3 error with code below, need to fix\n",
        "#importances = zip(model.feature_importances_, train_df.ix[:, train_df.columns != 'price_doc'].columns)\n",
        "#importances = pd.DataFrame(importances, columns=['importance', 'feature'])\n",
        "#importances.sort_values('importance', ascending=False).head(30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "daf78057-e0fc-3664-9690-6bb94df8c148"
      },
      "source": [
        "## Train Model & Submit\n",
        "\n",
        "Here I train my final model with some tuned parameters to the XGB."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "df53acc9-969d-cc97-5fe1-9dcfc05d5e3a"
      },
      "outputs": [],
      "source": [
        "from xgboost import XGBRegressor\n",
        "\n",
        "#Get Data\n",
        "Y_train = train_df['price_doc'].values\n",
        "X_train = train_df.ix[:, train_df.columns != 'price_doc'].values\n",
        "X_test = test_df.values\n",
        "#Init Model\n",
        "xgb = XGBRegressor(learning_rate=0.05, max_depth=6, subsample=0.8, colsample_bytree=0.7)\n",
        "#Train Model\n",
        "model = xgb.fit(X_train, Y_train)\n",
        "#Make Predictions\n",
        "predictions = xgb.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b32130e7-b17d-af55-e6f9-4ef721ab5a25"
      },
      "outputs": [],
      "source": [
        "#Make Submission File\n",
        "submission_df = pd.DataFrame({'id':test_full['id'], 'price_doc':predictions})\n",
        "submission_df.to_csv('xgb-added_features.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "6699ebf3-a372-362c-59da-b77aa30b437f"
      },
      "source": [
        "## Next Steps\n",
        "\n",
        "Currently my best submission as me at 53% on the leaderboard.  \n",
        "\n",
        "Some next steps I want to take are:  \n",
        "-Engineer more features - play with groupby (sub_area, more state features, etc)  \n",
        "-Remove some features - try PCA? Learn more about when to remove features that aren't adding new info, how to tell  \n",
        "-NaN's - there is a lot of missing data and wrong data - should I remove some/all? Correct some (ie. incorrect years)?  \n",
        "-ID field seems to improve results - why is this? should I remove it anyway?  \n",
        "-Optimize XGB using GridSearch and Build Ensemble learner  \n",
        "\n",
        "Please feel free to let me know what I could do better! "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "a45346e0-ceee-6c03-ac3a-c1730c08ba18"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}