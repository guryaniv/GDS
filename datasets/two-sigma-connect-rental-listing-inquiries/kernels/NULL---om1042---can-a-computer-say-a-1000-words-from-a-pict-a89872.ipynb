{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "8268708a-ee92-9786-a503-eeccd324bec1"
      },
      "source": [
        "Colorspace\n",
        "=========="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "389ff303-c029-fa17-6264-4ee7d707ffc3"
      },
      "outputs": [],
      "source": [
        "from PIL import Image, ImageDraw, ImageFilter\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import nltk\n",
        "import glob\n",
        "\n",
        "plt.rcParams['figure.figsize'] = (10.0, 10.0)\n",
        "images = sorted(glob.glob('../input/images_sample/**/**.jpg'))\n",
        "for im in images:\n",
        "    im = Image.open(im)\n",
        "    h, w = im.size\n",
        "    qu = im.quantize(colors=8, kmeans=4)\n",
        "    crgb = qu.convert('RGB')\n",
        "    col_rank = sorted(crgb.getcolors(h*w), reverse=True)\n",
        "    print(col_rank) #legend\n",
        "    draw = ImageDraw.Draw(im)\n",
        "    i = 0\n",
        "    for cnt, rgb in col_rank:\n",
        "        draw.rectangle([(10, i*40+10),(40, i*40+30)], fill=(rgb[0],rgb[1],rgb[2]), outline=(0,0,0))\n",
        "        draw.text((10, i*40+30), str(cnt), fill=(0,0,0))\n",
        "        i += 1\n",
        "    del draw\n",
        "    plt.imshow(im); plt.axis('off')\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "5f310156-59d9-9883-0c47-670f9ea4592f"
      },
      "source": [
        "Image Statistics\n",
        "================"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "5bf0abed-3bfe-09a0-3e51-b574326e1368"
      },
      "outputs": [],
      "source": [
        "from PIL import ImageStat\n",
        "for im in images:\n",
        "    img = Image.open(im)\n",
        "    stats = ImageStat.Stat(img, mask=None)\n",
        "    print(stats.extrema)\n",
        "    print(stats.count)\n",
        "    print(stats.sum)\n",
        "    print(stats.sum2)\n",
        "    print(stats.mean)\n",
        "    print(stats.median)\n",
        "    print(stats.rms)\n",
        "    print(stats.var)\n",
        "    print(stats.stddev)\n",
        "    plt.imshow(img); plt.axis('off')\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "3a528c44-f184-de07-97f1-f78e834b9863"
      },
      "source": [
        "OCR Watermarks or Floor Plans for features\n",
        "=========================================="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "a39125d0-6bcf-3f68-9d3b-6bf9727098b8"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "#import pytesseract #sudo apt-get install tesseract-ocr or submit pull request to Kaggle Docker\n",
        "import glob\n",
        "\n",
        "#images = glob.glob('../input/images_sample/**/**.jpg')\n",
        "#for im in images:\n",
        "#    img = Image.open(im) #rotate images 90 degrees\n",
        "#    t = pytesseract.image_to_string(img)\n",
        "#    if len(t)>0:\n",
        "#        print(im, '\\n', t)\n",
        "\n",
        "\"\"\"\n",
        "../input/images_sample/6812223/6812223_906d2825311544e3ef052c315f4dddb7.jpg \n",
        " HABITATS\n",
        "../input/images_sample/6811964/6811964_552eab2b6974e995b419654faecc1cd8.jpg \n",
        " BALCONY\n",
        "\n",
        "Greenhouse ubwa\n",
        "a! m Mlnnv\n",
        "\n",
        "LIVING ROOM\n",
        "I2\u2018-5'n I9\u2018-2\"\n",
        "\n",
        "EEDROOM\n",
        "I! an IE Lo\"\n",
        "../input/images_sample/6811974/6811974_39be7f428f80beda5163e909ea05a95a.jpg \n",
        " MLLEJRE\n",
        "../input/images_sample/6811974/6811974_197bb9515b3d7929c2848e61a050ad1a.jpg \n",
        " U\n",
        "BALCONY\n",
        "UV\u2018NG/DININE\n",
        "H M' X Wl'\n",
        "m\n",
        "m x m- E\n",
        "r ..\n",
        "KIT NT ll:\n",
        "L D\n",
        "\u2014H Vi-\n",
        "AIH STURAE\n",
        "\"\"\"\n",
        "print('OCR..')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "9cfdfffc-e0c9-ca3e-310e-711aeb60bd46"
      },
      "source": [
        "Image Exif Tags\n",
        "==============="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "17333901-649e-1dfb-761c-18a14f0b8319"
      },
      "outputs": [],
      "source": [
        "from PIL import Image, ExifTags\n",
        "\n",
        "img = Image.open('../input/images_sample/6811960/6811960_3685d3542328b820980642535d8ccb72.jpg')\n",
        "ex = img._getexif()\n",
        "if ex != None:\n",
        "    for (k,v) in img._getexif().items():\n",
        "            print (ExifTags.TAGS.get(k), v)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "3b8ee8a9-85ec-6467-1acf-f4fc9f0348d3"
      },
      "source": [
        "Image Hash (Duplicate Images)\n",
        "==============="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b39a2483-8641-a6db-09ff-0b6bef035dae"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import imagehash, hashlib\n",
        "import random\n",
        "\n",
        "images = glob.glob('../input/images_sample/6812098/**.jpg') #just comparing two folders for demo\n",
        "images += glob.glob('../input/images_sample/6812035/**.jpg')\n",
        "\n",
        "for im in range(100):\n",
        "    im1 = random.choice(images)\n",
        "    im2 = random.choice(images)\n",
        "    h1 = imagehash.dhash(Image.open(im1))\n",
        "    h2 = imagehash.dhash(Image.open(im2))\n",
        "    feature = h1 - h2\n",
        "    if feature < 7 and im1 != im2:\n",
        "        print(feature, im1, im2)\n",
        "        imgx = np.concatenate((Image.open(im1).resize((400, 400), Image.ANTIALIAS), Image.open(im2).resize((400, 400), Image.ANTIALIAS)), axis=1)\n",
        "        plt.imshow(imgx); plt.axis('off')\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "fa481f69-06fd-16ca-770c-d4cd08129f12"
      },
      "outputs": [],
      "source": [
        "import time; start_time = time.time()\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn import model_selection, preprocessing\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.pipeline import FeatureUnion\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn import pipeline\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from nltk.stem.porter import *\n",
        "stemmer = PorterStemmer()\n",
        "from bs4 import BeautifulSoup\n",
        "import random; random.seed(7)\n",
        "import xgboost as xgb\n",
        "import datetime as dt\n",
        "\n",
        "train = pd.read_json(open(\"../input/train.json\", \"r\"))[:100] #limit\n",
        "y = train.interest_level.values\n",
        "n = len(train)\n",
        "\n",
        "test = pd.read_json(open(\"../input/test.json\", \"r\"))[:100] #limit\n",
        "listing_id = test.listing_id.values\n",
        "\n",
        "col = [x for x in train.columns if x not in ['listing_id','interest_level','street_address']]\n",
        "print(col)\n",
        "print(len(train),len(test))\n",
        "\n",
        "def str_stem(s): \n",
        "    if isinstance(s, str):\n",
        "        s = s.lower()\n",
        "        s = s.replace(\"  \",\" \")\n",
        "        b = BeautifulSoup(s, \"lxml\")\n",
        "        s = b.get_text(\" \").strip()\n",
        "        s = (\" \").join([z for z in s.split(\" \")])\n",
        "        s = (\" \").join([stemmer.stem(z) for z in s.split(\" \")])\n",
        "        s = s.lower().strip()\n",
        "        return s\n",
        "    else:\n",
        "        return \"\"\n",
        "\n",
        "class cust_regression_vals(BaseEstimator, TransformerMixin):\n",
        "    def fit(self, x, y=None):\n",
        "        return self\n",
        "    def transform(self, df):\n",
        "        d_col_drops=['xdescription', 'ydescription']\n",
        "        df = df.drop(d_col_drops, axis=1).values\n",
        "        return df\n",
        "\n",
        "class cust_txt_col(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, key):\n",
        "        self.key = key\n",
        "    def fit(self, x, y=None):\n",
        "        return self\n",
        "    def transform(self, data_dict):\n",
        "        return data_dict[self.key].apply(str)\n",
        "    \n",
        "df_all = pd.concat((train[col], test[col]), axis=0, ignore_index=True)\n",
        "train = []\n",
        "test = []\n",
        "\n",
        "df_all['photos'] = df_all.photos.apply(len)\n",
        "\n",
        "df_all[\"price_be\"] = df_all[\"price\"]/df_all[\"bedrooms\"]\n",
        "df_all[\"price_ba\"] = df_all[\"price\"]/df_all[\"bathrooms\"]\n",
        "\n",
        "df_all[\"created\"] = pd.to_datetime(df_all[\"created\"])\n",
        "df_all[\"created_year\"] = df_all[\"created\"].dt.year\n",
        "df_all[\"created_month\"] = df_all[\"created\"].dt.month\n",
        "df_all[\"created_day\"] = df_all[\"created\"].dt.day\n",
        "df_all['created_hour'] = df_all[\"created\"].dt.hour\n",
        "df_all['created_weekday'] = df_all['created'].dt.weekday\n",
        "df_all['created_week'] = df_all['created'].dt.week\n",
        "df_all['created_quarter'] = df_all['created'].dt.quarter\n",
        "df_all['created_weekend'] = ((df_all['created_weekday'] == 5) & (df_all['created_weekday'] == 6))\n",
        "df_all['created_wd'] = ((df_all['created_weekday'] != 5) & (df_all['created_weekday'] != 6))\n",
        "df_all['created'] = df_all['created'].map(lambda x: float((x - dt.datetime(1899, 12, 30)).days) + (float((x - dt.datetime(1899, 12, 30)).seconds) / 86400))\n",
        "\n",
        "df_all['x5'] = df_all['latitude'].map(lambda x : round(x,5))\n",
        "df_all['y5'] = df_all['longitude'].map(lambda x : round(x,5))\n",
        "df_all['x4'] = df_all['latitude'].map(lambda x : round(x,4))\n",
        "df_all['y4'] = df_all['longitude'].map(lambda x : round(x,4))\n",
        "df_all['x3'] = df_all['latitude'].map(lambda x : round(x,3))\n",
        "df_all['y3'] = df_all['longitude'].map(lambda x : round(x,3))\n",
        "df_all['x2'] = df_all['latitude'].map(lambda x : round(x,2))\n",
        "df_all['y2'] = df_all['longitude'].map(lambda x : round(x,2))\n",
        "\n",
        "dummies = df_all['features'].str.join(sep=',').str.lower().str.get_dummies(sep=',')\n",
        "df_all = pd.concat([df_all, dummies], axis=1)\n",
        "dummies = []\n",
        "df_all['features'] = df_all.features.apply(len)\n",
        "\n",
        "cat = ['building_id',  'description', 'display_address', 'manager_id']\n",
        "lbl = preprocessing.LabelEncoder()\n",
        "for c in cat:\n",
        "    if c in ['description']:\n",
        "        df_all['x'+c] = df_all[c].map(lambda x:str_stem(x))\n",
        "        df_all['y'+c] = df_all[c].values\n",
        "    df_all['words_of_'+c] = df_all[c].map(lambda x:len(x.strip().split(' ')))\n",
        "    df_all['len_of_'+c] = df_all[c].map(lambda x:len(x.strip()))\n",
        "    df_all[c] = lbl.fit_transform(list(df_all[c].values))\n",
        "    print(c, len(lbl.classes_))\n",
        "\n",
        "train = df_all.iloc[:n]\n",
        "test = df_all.iloc[n:]\n",
        "#df_all = []\n",
        "\n",
        "tfidf = TfidfVectorizer(stop_words ='english', max_df=0.9)\n",
        "tsvd = TruncatedSVD(n_components=25, random_state = 7)\n",
        "clf = pipeline.Pipeline([\n",
        "        ('union', FeatureUnion(\n",
        "                    transformer_list = [\n",
        "                        ('cst',  cust_regression_vals()),\n",
        "                        ('txt1', pipeline.Pipeline([('s1', cust_txt_col(key='xdescription')), ('tfidf1', tfidf), ('tsvd1', tsvd)])),\n",
        "                        ('txt2', pipeline.Pipeline([('s2', cust_txt_col(key='ydescription')), ('tfidf2', tfidf), ('tsvd2', tsvd)]))\n",
        "                        ],\n",
        "                    transformer_weights = {\n",
        "                        'cst': 1.0,\n",
        "                        'txt1': 1.0,\n",
        "                        'txt2': 1.0\n",
        "                        },\n",
        "                n_jobs = -1\n",
        "                ))])\n",
        "\n",
        "y_val = lbl.fit_transform(y)\n",
        "xtrain = pd.DataFrame(clf.fit_transform(train)).apply(pd.to_numeric)\n",
        "xtrain = xgb.DMatrix(xtrain.values, y_val)\n",
        "xtest = pd.DataFrame(clf.transform(test)).apply(pd.to_numeric)\n",
        "xtest = xgb.DMatrix(xtest.values)\n",
        "\n",
        "param = {}\n",
        "param['objective'] = 'multi:softprob'\n",
        "param['eta'] = 0.1\n",
        "#param['max_depth'] = 4\n",
        "param['silent'] = True\n",
        "param['num_class'] = 3\n",
        "param['eval_metric'] = \"mlogloss\"\n",
        "param['min_child_weight'] = 1\n",
        "param['subsample'] = 0.8\n",
        "param['colsample_bytree'] = 0.8\n",
        "param['seed'] = 7\n",
        "plst = list(param.items())\n",
        "nfolds = 5\n",
        "nrounds = 1000\n",
        "\n",
        "model = xgb.cv(plst, xtrain, nrounds, nfolds, early_stopping_rounds=20, verbose_eval=25)\n",
        "best_rounds = np.argmin(model['test-mlogloss-mean'])\n",
        "model = xgb.train(plst, xtrain, best_rounds)\n",
        "print(log_loss(y_val, model.predict(xtrain)))\n",
        "preds = model.predict(xtest)\n",
        "out_df = pd.DataFrame(preds)\n",
        "out_df.columns = lbl.inverse_transform(out_df.columns)\n",
        "out_df[\"listing_id\"] = listing_id\n",
        "out_df.to_csv(\"z09submission01.csv\", index=False)\n",
        "print('Done...',(time.time()-start_time)/60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "6b743033-cac4-285c-731c-87bdf471df5a"
      },
      "source": [
        "Future Review\n",
        "=============\n",
        "- Can appliances be identified\n",
        "- Can room be measured\n",
        "- What kind of flooring\n",
        "- Can windows and their view be ranked\n",
        "- Can defects be identified\n",
        "- Is it furnished, someone living there\n",
        "- Has picture been photoshopped (altered)\n",
        "- Add your own to the list on comments and fork to suggest/showcase additional features"
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}