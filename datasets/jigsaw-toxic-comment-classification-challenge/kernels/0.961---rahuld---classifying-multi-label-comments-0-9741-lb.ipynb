{"cells":[{"metadata":{"_uuid":"9dacb7a2a5a9e582d7e597be4ea479cedd79489a","_cell_guid":"d49b7eaa-0bf7-48e0-9fe6-51ba03036ba1"},"cell_type":"markdown","source":"# Classifying multi-label comments with Logistic Regression\n#### Rhodium Beng\nStarted on 20 December 2017\n\nThis kernel is inspired by:\n- kernel by Jeremy Howard : _NB-SVM strong linear baseline + EDA (0.052 lb)_\n- kernel by Issac : _logistic regression (0.055 lb)_\n- _Solving Multi-Label Classification problems_, https://www.analyticsvidhya.com/blog/2017/08/introduction-to-multi-label-classification/","outputs":[],"execution_count":null},{"metadata":{"collapsed":true,"_uuid":"5c5f4cc8865644748e11336736bbe584adebe7b1","_cell_guid":"8f6a95ee-cc95-4c9f-a8f7-72ae58ec13d6","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport re","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4f65d03ddbfd127307d3e415003346eb898b4d6b","_cell_guid":"80d61838-9025-4cba-bb0e-58175586b21b"},"cell_type":"markdown","source":"## Load training and test data","outputs":[],"execution_count":null},{"metadata":{"collapsed":true,"_uuid":"4e35cd5fcae1581dbd6bc51f14728e27fe63fe70","_cell_guid":"094fff47-db10-447c-965e-08056f718bde","trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('../input/train.csv')\ntest_df = pd.read_csv('../input/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"89d1c9a4f9598427e8a20d66fa9e56796ad720f6","_cell_guid":"09986b08-eda6-4438-9cbe-52a61d8d57fa"},"cell_type":"markdown","source":"## Examine the data (EDA)","outputs":[],"execution_count":null},{"metadata":{"_uuid":"9e53b7599d707a9420a75c37c7ac6d05bed9df7b","_cell_guid":"c4c7137d-6bc7-4b41-b50a-e511883155e9","_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"train_df.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6c824d91ae1e801e1489e93e1a9932c8c0cb0e0a","_cell_guid":"40597119-1274-4d5b-a054-b4b17dbcbb36"},"cell_type":"markdown","source":"In the training data, the comments are labelled as one or more of the six categories; toxic, severe toxic, obscene, threat, insult and identity hate. This is essentially a multi-label classification problem.","outputs":[],"execution_count":null},{"metadata":{"collapsed":true,"_uuid":"7f7f2581edb2f42a64812dec31622a011dceff80","_cell_guid":"7e29bebb-d9b7-44ab-a6ba-9f98e6507d5e","trusted":true},"cell_type":"code","source":"cols_target = ['obscene','insult','toxic','severe_toxic','identity_hate','threat']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e4a226272e319458391e117e8fb7f16b17c4884f","_cell_guid":"ce00e980-da07-4412-ae4c-5152cc2036e0","trusted":true},"cell_type":"code","source":"# check missing values in numeric columns\ntrain_df.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"474f4cf26190a2f2011ca0908052973fec1b1520","_cell_guid":"b2be6443-eb4e-4e12-81b3-faf2ea692ca4"},"cell_type":"markdown","source":"There are no missing numeric values. Based on the mean values, it also looks like there are many comments which are not labelled in any of the six categories.","outputs":[],"execution_count":null},{"metadata":{"_uuid":"99f1db4864c5c538f2a925d7b6733bb4b1c68707","_cell_guid":"997cc605-71a0-4ceb-a64b-e3e44c172aea","trusted":true},"cell_type":"code","source":"# check for any 'null' comment\nno_comment = train_df[train_df['comment_text'].isnull()]\nlen(no_comment)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"964e57595bb6e87a47aa82557c9d63aae3c24bd0","_cell_guid":"af5ba625-bd5d-4ad3-948b-5641b10d62fb","_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"test_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c616bcb0dcf611679dcb5009def9d71d6727cd0e","_cell_guid":"6bd65c22-9c8a-4756-a7bf-16187a6044d1","trusted":true},"cell_type":"code","source":"no_comment = test_df[test_df['comment_text'].isnull()]\nno_comment","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ad7f31ff8035dc60d37c8070f8bbaa9e7afac32f","_cell_guid":"ecce8833-b5b4-40b4-a164-015b7380b8b1"},"cell_type":"markdown","source":"All rows in the training and test data contain comments, so there's no need to clean up null fields.","outputs":[],"execution_count":null},{"metadata":{"_uuid":"e497fc6688e16602e3f49a11939f32324d295a8d","_cell_guid":"b3dcbd9f-dbb8-4a5a-96b2-b93882516e27","trusted":true},"cell_type":"code","source":"# let's see the total rows in train, test data and the numbers for the various categories\nprint('Total rows in test is {}'.format(len(test_df)))\nprint('Total rows in train is {}'.format(len(train_df)))\nprint(train_df[cols_target].sum())","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"2ff9ed83ba328872d446add97695285dc49f4165","_cell_guid":"981dff09-3acf-4014-b38a-22afc02a6654"},"cell_type":"markdown","source":"As mentioned earlier, majority of the comments in the training data are not labelled in one or more of these categories.","outputs":[],"execution_count":null},{"metadata":{"collapsed":true,"_uuid":"1e97432af65b6b75b436daabc83bdf57775a59c1","_cell_guid":"b26588a7-9a7f-4183-98b2-fb94a70bedaa","trusted":true},"cell_type":"code","source":"# Let's look at the character length for the rows in the training data and record these\ntrain_df['char_length'] = train_df['comment_text'].apply(lambda x: len(str(x)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"448c3492fc2fe24f30bd7b97047d69f16b58ca2f","scrolled":true,"_cell_guid":"d5ac5111-5bba-44c9-a039-7bb01a5bfd59","trusted":true},"cell_type":"code","source":"# look at the histogram plot for text length\nsns.set()\ntrain_df['char_length'].hist()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d28a801b4057518f87e20c46a77f9dc8757ab944","_cell_guid":"5b482de7-5fd1-4ef7-b7b4-2abf84ebdc25"},"cell_type":"markdown","source":"Most of the text length are within 500 characters, with some up to 5,000 characters long.","outputs":[],"execution_count":null},{"metadata":{"_uuid":"e1e0ecc1df6989b0ee73874f273f0dbbfc4c9d5e","_cell_guid":"f16a287f-27d4-4afa-82a1-dd441c2fd36c"},"cell_type":"markdown","source":"Next, let's examine the correlations among the target variables.","outputs":[],"execution_count":null},{"metadata":{"collapsed":true,"_uuid":"fab22b3f850c80a10d665ae43ee09b5107a79887","_cell_guid":"64164a2c-770f-469d-9020-e91714a9b2a8","trusted":true},"cell_type":"code","source":"data = train_df[cols_target]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"58968a44d8fdb3b93ac57f1ca20f81ffb71d164f","scrolled":true,"_cell_guid":"7fc7803b-a7f1-414d-84fa-d1d2201c8bb7","_kg_hide-output":false,"trusted":true},"cell_type":"code","source":"colormap = plt.cm.plasma\nplt.figure(figsize=(7,7))\nplt.title('Correlation of features & targets',y=1.05,size=14)\nsns.heatmap(data.astype(float).corr(),linewidths=0.1,vmax=1.0,square=True,cmap=colormap,\n           linecolor='white',annot=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4f9da9651b4e007088b19a06165d0727fb4f4e07","_cell_guid":"095fd031-32a0-400b-9357-af0c8f50dd6b"},"cell_type":"markdown","source":"Indeed, it looks like some of the labels are higher correlated, e.g. insult-obscene has the highest at 0.74, followed by toxic-obscene and toxic-insult.","outputs":[],"execution_count":null},{"metadata":{"_uuid":"b677d6a32b7b72e08ba3b46bce572a223db964de","_cell_guid":"caa00f5d-7e26-48cb-92b3-acc1f1de0aca"},"cell_type":"markdown","source":"What about the character length & distribution of the comment text in the test data?","outputs":[],"execution_count":null},{"metadata":{"collapsed":true,"_uuid":"36e4d00a5afc15e6b04ffa1e79421396d051f614","_cell_guid":"0993d06b-495e-428a-933a-6ffec6bdcef3","trusted":true},"cell_type":"code","source":"test_df['char_length'] = test_df['comment_text'].apply(lambda x: len(str(x)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1cc05875b88e54b5a1079eafc088207536dea2f3","_cell_guid":"828b9990-d78e-46c7-b011-1c66e6e6be79","trusted":true},"cell_type":"code","source":"plt.figure()\nplt.hist(test_df['char_length'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"db69eb685cd1be44de2224c399cbdeabb5ceaa06","_cell_guid":"b2bbf246-f098-425a-99e3-2eb2126b975c"},"cell_type":"markdown","source":"Now, the shape of character length distribution looks similar between the training data and the train data. For the training data, I guess the train data were clipped to 5,000 characters to facilitate the folks who did the labelling of the comment categories.","outputs":[],"execution_count":null},{"metadata":{"_uuid":"d88d9cea99dbd77e81a5b3c4b9309df88b04550b","_cell_guid":"fdf9d2f6-d248-452f-8f94-562755e3a3f3"},"cell_type":"markdown","source":"## Clean up the comment text","outputs":[],"execution_count":null},{"metadata":{"collapsed":true,"_uuid":"b42586552d4cdc79793b0de8e630f863f2b2c456","_cell_guid":"24392feb-0adc-4e27-bd20-41ed8cadce37","trusted":true},"cell_type":"code","source":"def clean_text(text):\n    text = text.lower()\n    text = re.sub(r\"what's\", \"what is \", text)\n    text = re.sub(r\"\\'s\", \" \", text)\n    text = re.sub(r\"\\'ve\", \" have \", text)\n    text = re.sub(r\"can't\", \"cannot \", text)\n    text = re.sub(r\"n't\", \" not \", text)\n    text = re.sub(r\"i'm\", \"i am \", text)\n    text = re.sub(r\"\\'re\", \" are \", text)\n    text = re.sub(r\"\\'d\", \" would \", text)\n    text = re.sub(r\"\\'ll\", \" will \", text)\n    text = re.sub(r\"\\'scuse\", \" excuse \", text)\n    text = re.sub('\\W', ' ', text)\n    text = re.sub('\\s+', ' ', text)\n    text = text.strip(' ')\n    return text","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"e67944653b23b6267fdb0634c03a5b8702ae6d26","_cell_guid":"ec11fdb5-22a8-4889-9a8d-c83d4179a0cf","_kg_hide-output":false,"trusted":true},"cell_type":"code","source":"# clean the comment_text in train_df\ncleaned_train_comment = []\nfor i in range(0,len(train_df)):\n    cleaned_comment = clean_text(train_df['comment_text'][i])\n    cleaned_train_comment.append(cleaned_comment)\ntrain_df['comment_text'] = pd.Series(cleaned_train_comment).astype(str)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"3ed1fff4601eb381b0c9f2da0dddef0453b248d3","_cell_guid":"5292f0f4-cdaf-4e9e-88f4-28efc3aa224d","trusted":true},"cell_type":"code","source":"# clean the comment_text in test_df\ncleaned_test_comment = []\nfor i in range(0,len(test_df)):\n    cleaned_comment = clean_text(test_df['comment_text'][i])\n    cleaned_test_comment.append(cleaned_comment)\ntest_df['comment_text'] = pd.Series(cleaned_test_comment).astype(str)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cae81f6b1d9bb475fc486d5fbb81981025cc3672","_cell_guid":"f5abe72a-13a7-41f6-ae8e-1c34dca97110"},"cell_type":"markdown","source":"\n## Define X from entire train & test data for use in tokenization by Vectorizer","outputs":[],"execution_count":null},{"metadata":{"collapsed":true,"_uuid":"b15dba3906f67e764af0a627e46d7b7e486888c5","_cell_guid":"30bf94de-36aa-4e8d-8d12-64172d8dc446","trusted":true},"cell_type":"code","source":"train_df = train_df.drop('char_length',axis=1)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"061d59552c6ef83bea8ecf9ffbf203286aeab6f8","_cell_guid":"49547fd7-9633-4f6a-bd46-84d8966f1e8b","trusted":true},"cell_type":"code","source":"X = train_df.comment_text\ntest_X = test_df.comment_text","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6b1e91df6163a437c5f55e9c4de88dc11a89e5ba","_cell_guid":"083686b8-483a-4fbd-8584-cb9da8357c57","trusted":true},"cell_type":"code","source":"print(X.shape, test_X.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cc4c2aeef221f5a97e1bd5ea1154052172175351","_cell_guid":"b2d898ae-79bc-48b8-8544-fb077f876c67"},"cell_type":"markdown","source":"## Vectorize the data","outputs":[],"execution_count":null},{"metadata":{"_uuid":"7b7adf15d16408eb99689884906d0d687c2f8407","_cell_guid":"9be5a4f2-0e85-4f9a-ac00-b8e9916116cb","trusted":true},"cell_type":"code","source":"# import and instantiate TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=5000,stop_words='english')\nvect","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b755b1d4db58eeb5b0ab668d1aaf4a651d3de441","_cell_guid":"283c1d48-c267-431b-834e-37c8d9222b3c","trusted":true},"cell_type":"code","source":"# learn the vocabulary in the training data, then use it to create a document-term matrix\nX_dtm = vect.fit_transform(X)\n# examine the document-term matrix created from X_train\nX_dtm","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"408301ccb78e3f4056a6d2ebbd239594d1a59da0","_cell_guid":"54050711-560a-47cf-b4f9-2fbaf59bc2e4","trusted":true},"cell_type":"code","source":"# transform the test data using the earlier fitted vocabulary, into a document-term matrix\ntest_X_dtm = vect.transform(test_X)\n# examine the document-term matrix from X_test\ntest_X_dtm","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"98a540b42db71f1639d47f31d2a4ea851aa1b9e5","_cell_guid":"c84eb9f4-4fa5-418c-883c-9d9316092db0"},"cell_type":"markdown","source":"## Solving a multi-label classification problem\nOne way to approach a multi-label classification problem is to transform the problem into separate single-class classifier problems. This is known as 'problem transformation'. There are three methods:\n* _**Binary Relevance.**_ This is probably the simplest which treats each label as a separate single classification problems. The key assumption here though, is that there are no correlation among the various labels.\n* _**Classifier Chains.**_ In this method, the first classifier is trained on the input X. Then the subsequent classifiers are trained on the input X and all previous classifiers' predictions in the chain. This method attempts to draw the signals from the correlation among preceding target variables.\n* _**Label Powerset.**_ This method transforms the problem into a multi-class problem  where the multi-class labels are essentially all the unique label combinations. In our case here, where there are six labels, Label Powerset would in effect turn this into a 2^6 or 64-class problem. {Thanks Joshua for pointing out.}","outputs":[],"execution_count":null},{"metadata":{"_uuid":"389245398dce9573dfa0f7c2facd2849d2174f1f","_cell_guid":"7abd771a-b0f3-47bc-b4ff-13a78aff48e7"},"cell_type":"markdown","source":"## Binary Relevance - build a multi-label classifier using Logistic Regression","outputs":[],"execution_count":null},{"metadata":{"_uuid":"e7e5707b19c35c8371a0cff7351bc8aadc33acd1","_cell_guid":"e30be87f-e0a6-4fd7-bff2-b3c37737cbfe","trusted":true},"cell_type":"code","source":"# import and instantiate the Logistic Regression model\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nlogreg = LogisticRegression(C=12.0)\n\n# create submission file\nsubmission_binary = pd.read_csv('../input/sample_submission.csv')\n\nfor label in cols_target:\n    print('... Processing {}'.format(label))\n    y = train_df[label]\n    # train the model using X_dtm & y\n    logreg.fit(X_dtm, y)\n    # compute the training accuracy\n    y_pred_X = logreg.predict(X_dtm)\n    print('Training accuracy is {}'.format(accuracy_score(y, y_pred_X)))\n    # compute the predicted probabilities for X_test_dtm\n    test_y_prob = logreg.predict_proba(test_X_dtm)[:,1]\n    submission_binary[label] = test_y_prob","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ee6d942b861235107e0d94174a8d21c2bad0e9ea","_cell_guid":"5d0970eb-bc44-4fad-91b7-e2a23c2f986d"},"cell_type":"markdown","source":"### Create submission file","outputs":[],"execution_count":null},{"metadata":{"_uuid":"b89e22016a7b4f282940dc23253cbf343c1fbf83","_cell_guid":"a2816552-c314-4584-ad49-8464e80b1b29","trusted":true},"cell_type":"code","source":"submission_binary.head()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"75d9571cc77eb1805d897af6ca0d86fd28405c6d","_cell_guid":"fb1bef7d-586e-437d-a1d3-b0868e7ac321","trusted":true},"cell_type":"code","source":"# generate submission file\nsubmission_binary.to_csv('submission_binary.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"18644d484121f3bab7a06207bd7fa739b15feff5","_cell_guid":"be8c8e90-f798-46d8-b77c-cd0668292646"},"cell_type":"markdown","source":"#### Binary Relevance with Logistic Regression classifier scored 0.074 on the public leaderboard.","outputs":[],"execution_count":null},{"metadata":{"_uuid":"0393cd387f508609c0d068c68fb7dbb0be659383","_cell_guid":"5018c7ea-dd27-4d4c-b9ad-ef2140c773e5"},"cell_type":"markdown","source":"## Classifier Chains - build a multi-label classifier using Logistic Regression","outputs":[],"execution_count":null},{"metadata":{"collapsed":true,"_uuid":"f1b4fea94c83661d7bbad5c6bc7a5994643128e2","_cell_guid":"b2172222-42b8-4f0a-8d20-160d52af6f62","trusted":false},"cell_type":"code","source":"# create submission file\nsubmission_chains = pd.read_csv('../input/sample_submission.csv')\n\n# create a function to add features\ndef add_feature(X, feature_to_add):\n    '''\n    Returns sparse feature matrix with added feature.\n    feature_to_add can also be a list of features.\n    '''\n    from scipy.sparse import csr_matrix, hstack\n    return hstack([X, csr_matrix(feature_to_add).T], 'csr')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3cdc6de563643c7a86f0c54ecff2f720695fe81d","_cell_guid":"20c3ff4a-8925-4c78-a8f1-74f080f0b890","trusted":false,"collapsed":true},"cell_type":"code","source":"for label in cols_target:\n    print('... Processing {}'.format(label))\n    y = train_df[label]\n    # train the model using X_dtm & y\n    logreg.fit(X_dtm,y)\n    # compute the training accuracy\n    y_pred_X = logreg.predict(X_dtm)\n    print('Training Accuracy is {}'.format(accuracy_score(y,y_pred_X)))\n    # make predictions from test_X\n    test_y = logreg.predict(test_X_dtm)\n    test_y_prob = logreg.predict_proba(test_X_dtm)[:,1]\n    submission_chains[label] = test_y_prob\n    # chain current label to X_dtm\n    X_dtm = add_feature(X_dtm, y)\n    print('Shape of X_dtm is now {}'.format(X_dtm.shape))\n    # chain current label predictions to test_X_dtm\n    test_X_dtm = add_feature(test_X_dtm, test_y)\n    print('Shape of test_X_dtm is now {}'.format(test_X_dtm.shape))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a0be9c78f3facd8215c0d83c446dcef61af3fc43","_cell_guid":"d917c9c5-5140-4b08-86b3-64e0b566bc1b"},"cell_type":"markdown","source":"### Create submission file","outputs":[],"execution_count":null},{"metadata":{"_uuid":"f1d85021f64b145d28a55f65e0d3e806a6c91625","_cell_guid":"4b7db16c-ca27-4fa8-933c-369711f60ea3","trusted":false,"collapsed":true},"cell_type":"code","source":"submission_chains.head()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"da35f9fd53f310366d0b760eb7573ae1b4e122c4","_cell_guid":"3e1dfebe-ecb4-4a0e-958c-56a2d27e6a7d","trusted":false},"cell_type":"code","source":"# generate submission file\nsubmission_chains.to_csv('submission_chains.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ecdadaa139a91703da20aab1f5ee77b25023bab4","_cell_guid":"6f76bfb5-0ad1-42f3-8481-1dc6226a13b9"},"cell_type":"markdown","source":"## Create a combined submission","outputs":[],"execution_count":null},{"metadata":{"collapsed":true,"_uuid":"b056f497fdcc13a46d5c83d988081dc2e5e99f1d","_cell_guid":"1a5fd709-81a7-4373-ad8a-45b61d4c1167","trusted":false},"cell_type":"code","source":"# create submission file\nsubmission_combined = pd.read_csv('../input/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0bb5e1b109a8b043801e5b6360fb858e233a9f2c","_cell_guid":"62d78056-24e4-409c-a5dd-221b1c9f3b2b"},"cell_type":"markdown","source":"Combine using simple average from Binary Relevance and Classifier Chains.","outputs":[],"execution_count":null},{"metadata":{"collapsed":true,"_uuid":"81258bf73f51973d923f2695d81cb8523d21b4a0","_cell_guid":"650d5945-f32f-49a9-b5ac-e932cf64e0dd","trusted":false},"cell_type":"code","source":"# corr_targets = ['obscene','insult','toxic']\nfor label in cols_target:\n    submission_combined[label] = 0.5*(submission_chains[label]+submission_binary[label])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1aeb3dbed4d2af2e1125edc00d5d699e71745cf4","_cell_guid":"3b7c66fd-d82e-46f1-bb41-004ed63c2569","trusted":false,"collapsed":true},"cell_type":"code","source":"submission_combined.head()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"eded1209e8bb777308d209c161d18c62c8a0c1ef","_cell_guid":"3af7d1e1-1d08-43b8-bdb1-0740dd4bdbbd","trusted":false},"cell_type":"code","source":"# generate submission file\nsubmission_combined.to_csv('submission_combined.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cbbce014f73f54e9bf3e88e096ad66b89b88fcf4","_cell_guid":"7a012382-ad72-4c76-b53c-1f756544e23e"},"cell_type":"markdown","source":"### That's all for now. \n### Tips and comments are most welcomed & appreciated.\n### Please upvote if you find it useful.","outputs":[],"execution_count":null},{"metadata":{"_uuid":"5febd3d5064c1f9643d2bccbf98fe415e55780d1","_cell_guid":"0b15d2bc-e563-4682-bfc4-a948abe2dae5"},"cell_type":"markdown","source":"","outputs":[],"execution_count":null},{"metadata":{"collapsed":true,"_uuid":"21a21a9ff3c174753c8d48495e058a1782bf7a76","_cell_guid":"2b370cff-d22e-4393-94d5-98b4883a32d8","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}