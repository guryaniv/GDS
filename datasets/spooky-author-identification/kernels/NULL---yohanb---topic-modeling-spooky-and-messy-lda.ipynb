{"nbformat_minor": 1, "cells": [{"metadata": {"_uuid": "172930148f95aa6a34f519f9433496e93942e3d8", "_cell_guid": "ead9bafa-386f-4edd-89d9-b8e910234e7a"}, "cell_type": "markdown", "source": ["## Introduction\n", "Here will try to cover some standard technics like tokenization, stemming, lemmatization and topic modeling with LDA.\n", "\n", "**Acknowledgments.**  This notebook was inspired by kernels [Spooky NLP and Topic Modelling tutorial](https://www.kaggle.com/arthurtok/spooky-nlp-and-topic-modelling-tutorial) (Anisotropic) and [NIPS papers visualized with NMF and t-SNE](https://www.kaggle.com/dschniertshauer/nips-papers-visualized-with-nmf-and-t-sne) (Lurchi)."]}, {"execution_count": null, "metadata": {"_uuid": "9cf220e3db0c1be4e176b95898eb12b076c614d6", "_cell_guid": "da26699d-f4b2-4397-b2b3-469bdd8c1b08"}, "outputs": [], "cell_type": "code", "source": ["import pandas as pd\n", "import numpy as np\n", "# LDA, tSNE\n", "from sklearn.manifold import TSNE\n", "from gensim.models.ldamodel import LdaModel\n", "from sklearn.metrics.pairwise import pairwise_distances\n", "# NLTK\n", "from nltk.tokenize import RegexpTokenizer\n", "from nltk.stem.wordnet import WordNetLemmatizer\n", "from nltk import pos_tag\n", "from nltk.corpus import stopwords\n", "import re\n", "# Bokeh\n", "from bokeh.io import output_notebook\n", "from bokeh.plotting import figure, show\n", "from bokeh.models import HoverTool, CustomJS, ColumnDataSource, Slider\n", "from bokeh.layouts import column\n", "from bokeh.palettes import all_palettes\n", "output_notebook()"]}, {"metadata": {"_uuid": "ef6bd53106de3cd33ee6f44bb8a26fa8fa5613f0", "_cell_guid": "c6acd75d-9dee-4250-a688-d2ed3fe739ca"}, "cell_type": "markdown", "source": ["## 1. Loading data\n", "Let's **load the dataset** with papers and glimpse some first rows of a paper."]}, {"execution_count": null, "metadata": {"_uuid": "9e94cb38f1a78f20311f48349d53a24271720a7c", "_cell_guid": "4340b982-e796-4972-a269-72d517cadc07"}, "outputs": [], "cell_type": "code", "source": ["an_author = 1211\n", "df = pd.read_csv(\"../input/train.csv\")\n", "df_test = pd.read_csv(\"../input/test.csv\")\n", "print(\"Author: {}, ID: {}\".format(df.author[an_author], df.id[an_author]))\n", "print(df.text[an_author][:500])"]}, {"metadata": {"_uuid": "bfae5a2fc4421f3dedf9c2b35ea30bfc3b1c8724", "_cell_guid": "993858df-b484-4e60-a7f3-fd68eaec0b65"}, "cell_type": "markdown", "source": ["## 2. Processing\n", "Here we'll process our corpus using some standard technics ...\n", "### 2.1. Initial cleaning\n", "Just **removing numbers** (if exist) and **reducing** all words **to the lowercase**. Let also see what we'll get:\n"]}, {"execution_count": null, "metadata": {"_uuid": "73c29802e092232a0c4d803f6b017bb739ab6855", "_cell_guid": "d176d426-163c-4e72-8c90-1ce148d929b6"}, "outputs": [], "cell_type": "code", "source": ["# Removing numerals:\n", "df['text_tokens'] = df.text.map(lambda x: re.sub(r'\\d+', '', x))\n", "# Lower case:\n", "df['text_tokens'] = df.text_tokens.map(lambda x: x.lower())\n", "print(\"Author: {}, ID: {}\".format(df.author[an_author], df.id[an_author]))\n", "print(df['text_tokens'][an_author][:500])"]}, {"metadata": {"_uuid": "4d2b9a4d06d641eac347b2a87b0e33d722f54485", "_cell_guid": "ab1dab0f-e0a5-4fd4-bec1-be53ef077280"}, "cell_type": "markdown", "source": ["### 2.2. Tokenize\n", "**Spliting** texts **into separete words**, also **removing punctuanions** and other stuff. After that procedure we should obtain texts as lists of words in lowercase:"]}, {"execution_count": null, "metadata": {"_uuid": "44470ac428f63f4008865c764944017893421a72", "_cell_guid": "4f8228aa-6eb9-4d5b-9522-70a9ecf56f91"}, "outputs": [], "cell_type": "code", "source": ["df['text_tokens'] = df.text_tokens.map(lambda x: RegexpTokenizer(r'\\w+').tokenize(x))\n", "print(\"Author: {}, ID: {}\".format(df.author[an_author], df.id[an_author]))\n", "print(df['text_tokens'][an_author][:25])"]}, {"metadata": {"_uuid": "8f2a8c4f276f568d4252ee23f39032436a296302", "_cell_guid": "ba298d1c-53ba-47d1-9d41-2e83ca655e13"}, "cell_type": "markdown", "source": ["### 2.3 Lemmatization\n", "*Stemming* is the process of reducing inflected (or sometimes derived) words to their word stem, base or root form. The stem **need not be identical to the morphological root of the word** (see [Wikipedia](https://en.wikipedia.org/wiki/Stemming) for more details).\n", "\n", "*Lemmatization* is the process of determining the [lemma](https://en.wikipedia.org/wiki/Lemma_(morphology) of a word based on its intended meaning. Unlike stemming, lemmatisation depends on **correctly identifying the intended part of speech and meaning of a word** in a sentence (see [Wikipedia](https://en.wikipedia.org/wiki/Lemmatisation) for more details.) We'll use `WordNetLemmatizer` from `nltk`. "]}, {"execution_count": null, "metadata": {"_uuid": "dd4441be11d14924efd54f8de9ce1f07208cc4cd", "_cell_guid": "c37e4a7a-e6d2-4595-a83e-1a4d426ad972"}, "outputs": [], "cell_type": "code", "source": ["lemma = WordNetLemmatizer()\n", "df['tags'] = df.text_tokens.map(lambda x: list(zip(*pos_tag(x)))[1])\n", "\n", "def recode_tag(tag):\n", "    if tag[0].lower() in ['n', 'r', 'v', 'j']:\n", "        if tag[0].lower() == 'j': return 'a'\n", "        else: return tag[0].lower()\n", "    else: return None\n", "\n", "df['tags'] = df.tags.map(lambda x: list(map(recode_tag, x)))\n", "df['tags'] = df.apply(lambda x: list(zip(x.text_tokens, x.tags)), axis=1)\n", "\n", "def lemmatize_tokens(pairs):\n", "    return [lemma.lemmatize(tok, pos=tag) if tag != None else tok \n", "            for (tok, tag) in pairs]\n", "\n", "df['text_tokens'] = df.tags.map(lemmatize_tokens)\n", "print(\"Author: {}, ID: {}\".format(df.author[an_author], df.id[an_author]))\n", "print(df['text_tokens'][an_author][:25])\n", "print(df['tags'][an_author][:25])"]}, {"metadata": {"_uuid": "5273a5bc7abc91a631bb1920f0bef87ace1e8cc9", "_cell_guid": "857b17df-f771-417e-be9e-6b0c2607ef58"}, "cell_type": "markdown", "source": ["\n", "### 2.4. Stop words\n", "**Removing common** English **words** like `and`, `the`, `of` and so on."]}, {"execution_count": null, "metadata": {"_uuid": "cbd9d7c7beafec021409060d94fcbb0d3d9afecb", "_cell_guid": "ed172446-1023-4c16-9c48-87403ca85849"}, "outputs": [], "cell_type": "code", "source": ["stop_en = stopwords.words('english')\n", "df['text_tokens'] = df.text_tokens.map(lambda x: [t for t in x if t not in stop_en])\n", "print(\"Author: {}, ID: {}\".format(df.author[an_author], df.id[an_author]))\n", "print(df['text_tokens'][an_author][:25])"]}, {"metadata": {"_uuid": "31d8f6a726c515d1c5ac4ea15cc4359365f9b4ab", "_cell_guid": "03ed1cf7-0922-426f-80c3-4bbee3470630"}, "cell_type": "markdown", "source": ["### 2.5. Bigrams\n", "Let's construct bigrams (**words' pairs**) for every text:"]}, {"execution_count": null, "metadata": {"_uuid": "675fd23d7f9c5e87ec2e0f035242bfc6ab0fb201", "_cell_guid": "49cd3a93-4802-4ec0-bf6b-f0d9fcd6079f"}, "outputs": [], "cell_type": "code", "source": ["df['text_tokens_bigrams'] = df.text_tokens.map(lambda x: [' '.join(x[i:i+2]) \n", "                                                          for i in range(len(x)-1)])\n", "print(\"Author: {}, ID: {}\".format(df.author[an_author], df.id[an_author]))\n", "print(df['text_tokens_bigrams'][an_author][:25])"]}, {"metadata": {"_uuid": "d3fd161b8bed8598ed9d624a36a06608169864a8", "_cell_guid": "bd57d342-571b-4a72-beb8-7a79d98708ba"}, "cell_type": "markdown", "source": ["### 2.5. Final cleaning\n", "#### 2.5.1. Short words\n", "Here we'll remove all \"extremely short\" words (that have less than 2 characters, if those are still exist in the texts for some reason)."]}, {"execution_count": null, "metadata": {"_uuid": "dc312b82d16aadc21f0ebc8074c6c382afd97efc", "_cell_guid": "aa169d7e-de52-4c49-8e4e-912582488414"}, "outputs": [], "cell_type": "code", "source": ["df['text_tokens'] = df.text_tokens.map(lambda x: [t for t in x if len(t) > 1])\n", "print(\"Author: {}, ID: {}\".format(df.author[an_author], df.id[an_author]))\n", "print(df['text_tokens'][an_author][:25])"]}, {"metadata": {"_uuid": "15934fbd9cb075905e5eda8c08af56ebc29541cb", "_cell_guid": "edeec33f-168c-4ae3-9737-4057affdcd1a"}, "cell_type": "markdown", "source": ["#### 2.5.1. Join tokens and bigrams\n", "Adding our bigrams to to pool of tokens for every text:"]}, {"execution_count": null, "metadata": {"_uuid": "4a5eeda60768070c8420d3b47c2eabd860ed3f41", "_cell_guid": "31e09b75-1594-4d8f-97d9-7ddd7b05e5c5"}, "outputs": [], "cell_type": "code", "source": ["df['text_tokens'] = df.text_tokens + df.text_tokens_bigrams\n", "print(\"Author: {}, ID: {}\".format(df.author[an_author], df.id[an_author]))\n", "print(df['text_tokens'][an_author][:100])"]}, {"metadata": {"_uuid": "cdfb77459a42212d271bb25bfbcacffcaff1f00e", "_cell_guid": "a8e7127b-79a2-4cc4-9857-67e063f36ddb"}, "cell_type": "markdown", "source": ["\n", "## 3. LDA\n", "Finally, let's use **LDA** ([Latent Dirichlet allocation](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation)) to extract topic structure from the corpus of texts.\n"]}, {"execution_count": null, "metadata": {"collapsed": true, "_uuid": "0b0844aaf726cd9f8fd1d52e7c1afb30c6147618", "_cell_guid": "b280791c-8860-4428-9342-5a50234933fa"}, "outputs": [], "cell_type": "code", "source": ["from gensim import corpora, models\n", "T = 4 # number of topics\n", "np.random.seed(2017)\n", "texts = df['text_tokens'].values\n", "dictionary = corpora.Dictionary(texts)\n", "corpus = [dictionary.doc2bow(text) for text in texts]\n", "ldamodel = models.ldamodel.LdaModel(corpus, id2word=dictionary, \n", "                                    num_topics=T, passes=7, minimum_probability=0)"]}, {"metadata": {"_uuid": "0e3f5afefa2884f4a54cb2c46fc6d3dc30132c04", "_cell_guid": "e66f4e0f-63d5-4c0f-9c78-f5681da05014"}, "cell_type": "markdown", "source": ["Top words for every topic are:"]}, {"execution_count": null, "metadata": {"_uuid": "66c3e929e9a7dfb1892fc9e3a8a5714432edc6c2", "_cell_guid": "03d6a324-1ec3-4f21-9669-96c8cbd54934"}, "outputs": [], "cell_type": "code", "source": ["ldamodel.print_topics(num_topics=3, num_words=5)"]}, {"metadata": {"_uuid": "1b74df5a7d0ec8a30b2b8206e9fe19c141265239", "_cell_guid": "315d8db6-6a27-4b88-8100-65c82e40207b"}, "cell_type": "markdown", "source": ["Refactoring results of LDA into numpy matrix (`number_of_texts` $\\times$ `number_of_topics`). \n", "Also let us precompute **pairwise distance** between text with **cosine distance**."]}, {"execution_count": null, "metadata": {"collapsed": true, "_uuid": "1c976c28a48c6812769ffbe51efaa8587c8582a3", "_cell_guid": "6d2c3e98-a64b-4671-a254-c2a34b1280ce"}, "outputs": [], "cell_type": "code", "source": ["# Matrix with topics probabilities for every text:\n", "hm = np.array([[y for (x,y) in ldamodel[corpus[i]]] for i in range(len(corpus))])\n", "# Computing pairwise cosine distance between texts:\n", "precomp_cosine = pairwise_distances(hm, metric='cosine')"]}, {"metadata": {"_uuid": "20ab69d415be7130431184fdff9777b916e5fa35", "_cell_guid": "b50688ac-5784-4266-8500-b3016fe0e250"}, "cell_type": "markdown", "source": ["And **reduce dimensionality** using **t-SNE**:"]}, {"execution_count": null, "metadata": {"collapsed": true, "_uuid": "dc99c3c9588d36c8506b2b46c4478ffdb88728dd", "_cell_guid": "d7056768-3bb5-49cb-a657-7f698adfb806"}, "outputs": [], "cell_type": "code", "source": ["tsne = TSNE(random_state=2017, perplexity=25, metric='precomputed', early_exaggeration=4)\n", "tsne_rep = tsne.fit_transform(precomp_cosine)\n", "tsne_rep = pd.DataFrame(tsne_rep, columns=['x','y'])\n", "tsne_rep['hue'] = [['EAP', 'HPL', 'MWS'].index(x) for x in df.author.values]"]}, {"metadata": {"_uuid": "6edd604320f04592fd1db8d59e1031c2edc99bed", "_cell_guid": "f2c9e484-c6a8-4383-9581-bf41e60b6592"}, "cell_type": "markdown", "source": ["## 4. Ploting\n", "Using `Bokeh` for scatter plot with interactions. Hover mouse over a dot to see the title of the respective text:"]}, {"execution_count": null, "metadata": {"_uuid": "7df72f4971bb759dde983e9459804e6fc559316a", "_kg_hide-input": true, "_cell_guid": "4d5c6842-cbe7-4a64-9192-9511f2092860"}, "outputs": [], "cell_type": "code", "source": ["source = ColumnDataSource(\n", "        data=dict(\n", "            x = tsne_rep.x,\n", "            y = tsne_rep.y,\n", "            colors = [all_palettes['Inferno'][4][i] for i in tsne_rep.hue],\n", "            author = df.author,\n", "            text = df.text,\n", "            alpha = [0.7] * tsne_rep.shape[0],\n", "            size = [7] * tsne_rep.shape[0]\n", "        )\n", "    )\n", "\n", "hover_tsne = HoverTool(names=[\"df\"], tooltips=\"\"\"\n", "    <div style=\"margin: 10\">\n", "        <div style=\"margin: 0 auto; width:300px;\">\n", "            <span style=\"font-size: 12px; font-weight: bold;\">Author:</span>\n", "            <span style=\"font-size: 12px\">@author</span>\n", "        </div>\n", "        <div style=\"margin: 0 auto; width:300px;\">\n", "            <span style=\"font-size: 12px; font-weight: bold;\">Text:</span>\n", "            <span style=\"font-size: 12px\">@text</span>\n", "        </div>\n", "    </div>\n", "    \"\"\")\n", "\n", "tools_tsne = [hover_tsne, 'pan', 'wheel_zoom', 'reset']\n", "plot_tsne = figure(plot_width=700, plot_height=700, tools=tools_tsne, title='Spooky')\n", "\n", "plot_tsne.circle('x', 'y', size='size', fill_color='colors', \n", "                 alpha='alpha', line_alpha=0, line_width=0.01, source=source, name=\"df\")\n", "\n", "layout = column(plot_tsne)"]}, {"execution_count": null, "metadata": {"_uuid": "c756e4de7384ec2a5c8bb9499fa056b97b704129", "_kg_hide-input": true, "_cell_guid": "bfefe1a0-2a38-4ef7-88c4-17ad245fcaff"}, "outputs": [], "cell_type": "code", "source": ["show(layout)"]}, {"metadata": {"_uuid": "28358d8a77865e37e9dc33d1e6e863c1b5497f17", "_cell_guid": "f82effe7-7ed0-42c7-8876-ffd8c581b865"}, "cell_type": "markdown", "source": ["Thanks for reading!"]}], "metadata": {"kernelspec": {"language": "python", "display_name": "Python 3", "name": "python3"}, "language_info": {"file_extension": ".py", "mimetype": "text/x-python", "name": "python", "version": "3.6.3", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python"}}, "nbformat": 4}