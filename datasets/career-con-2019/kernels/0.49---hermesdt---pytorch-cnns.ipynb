{"cells":[{"metadata":{"trusted":true,"_uuid":"2e3ae7d0ff5e331ab1752cb80decd3705bbdf9b5"},"cell_type":"code","source":"!pip install torchsummary > /dev/null","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"X_test = pd.read_csv(\"../input/X_test.csv\")\nX_train = pd.read_csv(\"../input/X_train.csv\")\ny_train = pd.read_csv(\"../input/y_train.csv\")\nsample = pd.read_csv(\"../input/sample_submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"48cd5acfbcd6a6d56c73ee54f0527efa5b84d599"},"cell_type":"code","source":"def quaternion_to_euler(x, y, z, w):\n    import math\n    t0 = +2.0 * (w * x + y * z)\n    t1 = +1.0 - 2.0 * (x * x + y * y)\n    X = math.atan2(t0, t1)\n\n    t2 = +2.0 * (w * y - z * x)\n    t2 = +1.0 if t2 > +1.0 else t2\n    t2 = -1.0 if t2 < -1.0 else t2\n    Y = math.asin(t2)\n\n    t3 = +2.0 * (w * z + x * y)\n    t4 = +1.0 - 2.0 * (y * y + z * z)\n    Z = math.atan2(t3, t4)\n\n    return X, Y, Z\n\ndef feature_extraction(df):\n    df['orientation'] = df['orientation_X'] + df['orientation_Y'] + df['orientation_Z']+ df['orientation_W']\n    df['angular_velocity'] = df['angular_velocity_X'] + df['angular_velocity_Y'] + df['angular_velocity_Z']\n    df['linear_acceleration'] = df['linear_acceleration_X'] + df['linear_acceleration_Y'] + df['linear_acceleration_Z']\n    df['velocity_to_acceleration'] = df['angular_velocity'] / df['linear_acceleration']\n    df['velocity_linear_acceleration'] = df['linear_acceleration'] * df['angular_velocity']\n    x, y, z, w = df['orientation_X'].tolist(), df['orientation_Y'].tolist(), df['orientation_Z'].tolist(), df['orientation_W'].tolist()\n    nx, ny, nz = [], [], []\n    for i in range(len(x)):\n        xx, yy, zz = quaternion_to_euler(x[i], y[i], z[i], w[i])\n        nx.append(xx)\n        ny.append(yy)\n        nz.append(zz)\n    \n    df['euler_x'] = nx\n    df['euler_y'] = ny\n    df['euler_z'] = nz\n    \n    df['total_angular_velocity'] = (df['angular_velocity_X'] ** 2 + df['angular_velocity_Y'] ** 2 + df['angular_velocity_Z'] ** 2) ** 0.5\n    df['total_linear_acceleration'] = (df['linear_acceleration_X'] ** 2 + df['linear_acceleration_Y'] ** 2 + df['linear_acceleration_Z'] ** 2) ** 0.5\n    df['acc_vs_vel'] = df['total_linear_acceleration'] / df['total_angular_velocity']\n    \n    df['total_angle'] = (df['euler_x'] ** 2 + df['euler_y'] ** 2 + df['euler_z'] ** 2) ** 5\n    df['angle_vs_acc'] = df['total_angle'] / df['total_linear_acceleration']\n    df['angle_vs_vel'] = df['total_angle'] / df['total_angular_velocity']\n    return df\nX_test = feature_extraction(X_test)\nX_train = feature_extraction(X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a2e4f03b8242f242c087b019eb99947980716d39"},"cell_type":"code","source":"X_train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"be33f02e7184e455f33811c41bd9ff99d6da8ab1"},"cell_type":"code","source":"X_train.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"86d65a15c5b567cdd99e68a2934447171b57f082"},"cell_type":"code","source":"cols = [\"orientation_X\",\n\"orientation_Y\",\n\"orientation_Z\",\n\"orientation_W\",\n\"angular_velocity_X\",\n\"angular_velocity_Y\",\n\"angular_velocity_Z\",\n\"linear_acceleration_X\",\n\"linear_acceleration_Y\",\n\"linear_acceleration_Z\"]\n\nnum_cols = len(cols)\nfig, axes = plt.subplots(nrows=np.ceil(len(cols)/3).astype(np.int), ncols=3)\nfor idx, ax in enumerate(np.array(axes).flatten()):\n    if idx < len(cols):\n        ax.hist(X_train[cols[idx]], bins=100)\n        ax.set_title(cols[idx])\n\nfig.set_figheight(7)\nfig.set_figwidth(10)\nfig.set_tight_layout(True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5f0e337a6c5ff91af2eae324877b9f8c7ed20221","scrolled":true},"cell_type":"code","source":"#sns.pairplot(data=X_train[cols].sample(n=200))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"ffccd05f1c75e21330617f71ab3bfa4fbb0bb58a"},"cell_type":"code","source":"X_train[[\"row_id\", \"series_id\", \"measurement_number\"]].iloc[120:140]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"799187f240021284c4f806e7ce319a725d552313"},"cell_type":"code","source":"y_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f2b260e217e5a99ac662de07c936be092beb018b"},"cell_type":"code","source":"X_train.groupby([\"series_id\"]).series_id.count().nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dd9c25dd1009724a8ba5c7202e59e8e7a146a3d7"},"cell_type":"code","source":"nunique_series_ids = X_train.series_id.nunique()\nnunique_series_ids","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"61e927f9bad1938bc5e1063db9fd06e874b34527"},"cell_type":"code","source":"nunique_surfaces = y_train.surface.nunique()\nnunique_surfaces","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bade4fb6172ef82e8537d4c9a30304da57570662"},"cell_type":"code","source":"cols = orig_cols = X_train.drop([\"row_id\", \"series_id\", \"measurement_number\"], axis=1).columns\n# cols = orig_cols = [\n# \"orientation_X\",\n# \"orientation_Y\",\n# \"orientation_Z\",\n# \"orientation_W\",\n# \"angular_velocity_X\",\n# \"angular_velocity_Y\",\n# \"angular_velocity_Z\",\n# \"linear_acceleration_X\",\n# \"linear_acceleration_Y\",\n# \"linear_acceleration_Z\",\n# 'orientation',\n# 'angular_velocity',\n# 'linear_acceleration',\n# 'velocity_to_acceleration',\n# 'velocity_linear_acceleration',\n# ]\n\nnum_cols = len(orig_cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dd8c8b45cfcac4153d4bb36e764c2ad10fd5a4d6"},"cell_type":"code","source":"import torch\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom torch.utils import data\nfrom fastai.tabular.data import Learner, DataBunch\nfrom torchsummary import summary\nfrom fastai import metrics\nfrom sklearn.preprocessing import StandardScaler","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2f88b917ced88f71b1053effb677e40c662875ed"},"cell_type":"code","source":"X_scaler = StandardScaler()\nX_scaler.fit(X_train[cols])\nX_train[cols] = X_scaler.transform(X_train[cols])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b95367d7d7f0e563c1514c7cc7c82e96cb385661"},"cell_type":"code","source":"X_test[cols] = X_scaler.transform(X_test[cols])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9568dba67ae5d8136d2867b3ca2c64cc77348abc"},"cell_type":"code","source":"y_train.surface.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4fba301ef1c99ec1e9bf5871bb5fc9ca97c32a35"},"cell_type":"code","source":"#others = ['fine_concrete', 'carpet', 'hard_tiles']\n#y_train.loc[y_train.surface.isin(others),'surface'] = 'Other'\nvcounts = y_train.surface.value_counts()\nsurfaces = i2s = vcounts.index.values\nsurfaces","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c14ec5fa7b062b9f21e74dfcde61d60fb45e5dd6"},"cell_type":"code","source":"# surfaces_weights = (1-(vcounts.values) / len(y_train))\nsurfaces_weights = np.ones(len(surfaces))\n# surfaces_weights = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 5]\ns2i = {s:i for i,s in enumerate(surfaces)}\nsurfaces, surfaces_weights","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b0fb0f6fd8f2f94814668d8dec452e2ef258d6b2"},"cell_type":"code","source":"#    raw_frame['angular_velocity'] = raw_frame['angular_velocity_X'] + raw_frame['angular_velocity_Y'] + raw_frame['angular_velocity_Z']\n#    raw_frame['linear_acceleration'] = raw_frame['linear_acceleration_X'] + raw_frame['linear_acceleration_Y'] + raw_frame['linear_acceleration_Y']\n#    raw_frame['velocity_to_acceleration'] = raw_frame['angular_velocity'] / raw_frame['linear_acceleration']\n#    \n#    for col in raw_frame.columns[3:]:\n#        frame[col + '_mean'] = raw_frame.groupby(['series_id'])[col].mean()\n#        frame[col + '_std'] = raw_frame.groupby(['series_id'])[col].std()\n#        frame[col + '_max'] = raw_frame.groupby(['series_id'])[col].max()\n#        frame[col + '_min'] = raw_frame.groupby(['series_id'])[col].min()\n#        frame[col + '_max_to_min'] = frame[col + '_max'] / frame[col + '_min']\n#        \n#        frame[col + '_mean_abs_change'] = raw_frame.groupby('series_id')[col].apply(lambda x: np.mean(np.abs(np.diff(x))))\n#        frame[col + '_abs_max'] = raw_frame.groupby('series_id')[col].apply(lambda x: np.max(np.abs(x)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9a951c36bdd2d6a784c09715d0ad3b9d7d7662d6"},"cell_type":"code","source":"cols = [c + t for c in orig_cols for t in [\"\"]]\nnum_cols = len(cols)\n\nclass Dataset(data.Dataset):\n    def __init__(self, X, y):\n        self.y = Dataset.convert_target(y.surface)\n        self.X = Dataset.convert_df(X)\n    \n    @classmethod\n    def convert_df(cls, df):\n        batches = []\n        for serie_id, group in df.groupby([\"series_id\"]):\n            batch_data = []\n            for col in orig_cols:\n                v = group[col].values\n                if col not in []:\n                    batch_data.append(v)\n                \n            batches.append(batch_data)\n        return np.array(batches)\n    \n    @classmethod\n    def convert_target(cls, target):\n        idxs = [s2i[s] for s in target.values]\n        num_classes = len(surfaces)\n\n        # return np.eye(num_classes)[idxs]\n        return np.array(idxs)\n    \n    def __len__(self):\n        return len(self.X)\n    \n    def __getitem__(self, idx):\n        x = self.X[idx].reshape(1, num_cols, 128)\n        return torch.Tensor(x), torch.Tensor([self.y[idx]]).long()\n\n# cols = [c + t for c in orig_cols for t in [\"\", \"_step1\", \"_step2\", \"_avg2\"]]\n# cols = set(cols) - {'orientation_X'}\n# num_cols = len(cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"960696c987534cfbe9c3cb8ef04ac20b0a2c5a0a"},"cell_type":"code","source":"np.random.seed(42)\nvalid_size = int(nunique_series_ids*0.2)\n\nvalid_series_ids = np.random.choice(X_train.series_id.unique(), replace=False, size=valid_size)\ntrain_series_ids = np.setdiff1d(X_train.series_id.unique(), valid_series_ids)\n# train_series_ids = np.concatenate([train_series_ids, extra_carpet_ids])\n\nvalid_idx = np.argwhere(X_train.series_id.isin(valid_series_ids)).reshape(-1)\ntrain_idx = np.argwhere(X_train.series_id.isin(train_series_ids)).reshape(-1)\n# train_idx = np.concatenate(\n#     train_idx,\n#     np.argwhere(X_train.series_id.isin(extra_carpet_ids)).reshape(-1)\n# ).reshape(-1)\n# np.setdiff1d(np.arange(len(X_train)), valid_idx)\n\nvalid_y_idx = np.argwhere(y_train.series_id.isin(valid_series_ids)).reshape(-1)\ntrain_y_idx = np.argwhere(y_train.series_id.isin(train_series_ids)).reshape(-1)\n# np.setdiff1d(np.arange(len(y_train)), valid_y_idx)\n\n\ntrain_ds = Dataset(X_train.iloc[train_idx], y_train.iloc[train_y_idx])\ntrain_dl = data.DataLoader(train_ds, batch_size=32, shuffle=True)\n\nvalid_ds = Dataset(X_train.iloc[valid_idx], y_train.iloc[valid_y_idx])\nvalid_dl = data.DataLoader(valid_ds, batch_size=32, shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"06ba321e30fa1c87de20d47f597c05806139399b"},"cell_type":"code","source":"y_train.iloc[train_y_idx].surface.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1631deac4049c82077fd65068192afd7ba482fde"},"cell_type":"code","source":"train_count = y_train.iloc[train_y_idx].surface.value_counts()\nvalid_count = y_train.iloc[valid_y_idx].surface.value_counts()[train_count.index]\npd.DataFrame({\"train\": train_count/len(train_y_idx), \"valid\": valid_count/len(valid_y_idx)}, index=train_count.index).plot.bar()\nplt.title(\"% of surfce samples per set\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6ae4a1e8ed89e785aa13df1d83b094a4dba6ec8c"},"cell_type":"code","source":"db = DataBunch(train_dl, valid_dl)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9317d4ddc16648f588e082e308e8c5dcbd5ab39b"},"cell_type":"code","source":"torch.manual_seed(32)\nclass Flatten(nn.Module):\n    def forward(self, input):\n        return input.view(input.size(0), -1)\n\nclass Model(nn.Module):\n    def __init__(self, num_cols, num_cats):\n        super().__init__()\n        self.num_cats = num_cats\n        self.num_cols = num_cols\n        self.convs = nn.Sequential(\n            # nn.Conv1d(num_cols, 32, 2, stride=2, dilation=1),\n            nn.Conv2d(1, 4, (1,8), stride=(1,2), dilation=1),\n            nn.ELU(),\n            nn.Dropout2d(0.1),\n            \n            nn.Conv2d(4, 4, (1,2), stride=(1,2), dilation=1),\n            nn.ELU(),\n            nn.Dropout2d(0.3),\n\n            nn.Conv2d(4, 8, (1,2), stride=(1,2), dilation=1),\n            nn.ELU(),\n            nn.Dropout2d(0.2),\n            # nn.AvgPool2d(kernel_size=(1,4), )\n            #nn.BatchNorm2d(256),\n            # nn.LeakyReLU(),\n        )\n        \n        self.convs2 = nn.Sequential(\n            nn.Conv2d(1, 32, (num_cols,1), stride=(1,2), dilation=1),\n            nn.Tanh(),\n        )\n        \n        self.head = nn.Sequential(\n            nn.Linear(4928, 1000),\n            nn.Dropout2d(0.1),\n            nn.BatchNorm1d(1000),\n            nn.ReLU(inplace=True),\n            nn.Linear(1000, 50),\n            nn.Dropout2d(0.2),\n            nn.BatchNorm1d(50),\n            nn.ReLU(inplace=True),\n            nn.Linear(50, num_cats),\n            nn.Softmax(dim=1)\n            # nn.LogSigmoid()\n        )\n    \n    def forward(self, x):\n        # output = self.layers(x)\n        bs = x.size(0)\n        c1 = self.convs(x).view(bs, -1)\n        c2 = self.convs2(x).view(bs, -1)\n        c = torch.cat([c1, c2], dim=1)\n        # c = c1\n        return self.head(c.view(bs, -1))\n        # return x.view(bs, -1)\n\nclass CrossEntropyLoss(nn.Module):\n    def forward(self, input, target):\n        return F.cross_entropy(input, target.view(-1),\n                               weight=torch.Tensor(surfaces_weights).cuda())\n\nclass FocalLoss(nn.Module):\n    def __init__(self, alpha=1, gamma=2, logits=False, reduction=False):\n        super(FocalLoss, self).__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n        self.logits = logits\n        self.reduction  = reduction \n    def forward(self, inputs, targets):\n        t = torch.zeros_like(inputs)\n        #print(t.scatter_(1, targets, 1))\n        #print(inputs.size(), targets.size())\n        targets = t\n        if self.logits:\n            BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduce=None)\n        else:\n            BCE_loss = F.binary_cross_entropy(inputs, targets, reduce=None)\n        pt = torch.exp(-BCE_loss)\n        F_loss = self.alpha * (1-pt)**self.gamma * BCE_loss\n\n        if self.reduction:\n            return torch.mean(F_loss)\n        else:\n            return F_loss\n\nmodel = Model(num_cols, len(surfaces)).cuda()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"999e36408ff57a92d54d09658a0a2aa2823ee431"},"cell_type":"code","source":"summary(model, input_size=(1, num_cols, 128))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"58dd3c7470bc96430bf604f4b4a84c67a2284466"},"cell_type":"code","source":"learner = Learner(db, model,\n                  loss_func=CrossEntropyLoss(),\n                  # loss_func=FocalLoss(reduction=False, logits=True, alpha=0.25, gamma=1),\n                  wd=0.1,\n                  metrics=[metrics.accuracy, metrics.fbeta])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ccbb1c9fa69aebff75c058e5614260eb061a0b64"},"cell_type":"code","source":"learner.lr_find(start_lr=1e-10, end_lr=10)\nlearner.recorder.plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b26e149642097150c5dc2ce9cb4113b4f490c3a4"},"cell_type":"code","source":"learner.fit(15, 1e-3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cfd8894109d0ef1c26c00d58b4d92d6a0e0a9b70","scrolled":true},"cell_type":"code","source":"learner.recorder.plot_losses()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"df8a17bb591c20d29aff7691467b863372e63644"},"cell_type":"code","source":"# learner.lr_find(start_lr=1e-10, end_lr=10)\n# learner.recorder.plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f4ae3b904c9408c16907d3f0c75b06b9ed27a166"},"cell_type":"code","source":"# learner.fit(4, 1e-6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c9000a8ee310b03b481057d651baaa59ff95fe62"},"cell_type":"code","source":"# learner.recorder.plot_losses()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7c822ac8ff0bec07ab6133d0d0f724418054cad2","scrolled":false},"cell_type":"code","source":"accuracy_df = pd.DataFrame({\"accuracy\": []})\nfor s in surfaces:\n    y_idxs = np.argwhere((y_train.surface == s) & y_train.series_id.isin(valid_series_ids)).reshape(-1)\n    y = y_train.iloc[y_idxs]\n    X = X_train[X_train.series_id.isin(y.series_id)]\n    d = Dataset.convert_df(X).reshape(-1, 1, num_cols, 128)\n    preds = model(torch.Tensor(d).cuda())\n    targets = torch.Tensor(Dataset.convert_target(y.surface)).long().cuda()\n    accuracy = \"%.4f\" % metrics.accuracy(preds, targets).data.item()\n    # print(\"accuracy for\", s, accuracy)\n    accuracy_df.loc[s] = {\"accuracy\": accuracy}\ntrain_count = y_train.iloc[train_y_idx].surface.value_counts()\nvalid_count = y_train.iloc[valid_y_idx].surface.value_counts()[train_count.index]\npd.DataFrame({\n    \"train\": train_count/len(train_y_idx),\n    \"valid\": valid_count/len(valid_y_idx),\n    \"accuracy\": accuracy_df.loc[train_count.index].accuracy.astype(np.float)\n}, index=train_count.index).plot.bar()\nplt.title(\"% of surfce samples per set\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dd07f76d6a68128f45753a6d88074e5b1866a8bc"},"cell_type":"code","source":"def score(X, y, convert=True):\n    if convert:\n        X = Dataset.convert_df(X_train)\n        y = Dataset.convert_target(y)\n        \n    preds = model(torch.Tensor(X.reshape(-1, 1, num_cols, 128)).cuda())\n    targets = torch.Tensor(y).long().cuda()\n    return metrics.accuracy(preds, targets)\n\nmax_score = score(X_train, y_train.surface)\nmax_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"85411d5e6306f0004f4244ab3fd12f0fd5fcf5af"},"cell_type":"code","source":"diffs = []\nfor idx,col in enumerate(cols):\n    d = train_ds.X.copy()\n    new_order = np.random.choice(np.arange(128), replace=False, size=128)\n    d[:,0] = d[:,idx,new_order]\n    diffs.append(max_score - score(d, train_ds.y, convert=False))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f552094ae08837a8a87f8dc81b69b7b55ed5837d"},"cell_type":"code","source":"order = np.argsort(diffs)\nsorted_diffs = np.array(diffs)[order]\nsorted_cols = np.array(cols)[order]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"458dd5d57cfe539e53ce2cfa885c0ac32c271527","scrolled":false},"cell_type":"code","source":"plt.figure(figsize=(10, 8), dpi=80)\nplt.barh(np.arange(len(sorted_diffs)), sorted_diffs)\nplt.yticks(np.arange(len(sorted_cols)), sorted_cols)\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8ee39b9b3b4f81a610d41d72699806f43a8319ec"},"cell_type":"code","source":"t = Dataset.convert_df(X_test).reshape(-1, 1, num_cols, 128)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"79dc9bd5469a7c242d93f543c4e9bc75904411bb"},"cell_type":"code","source":"probs, idxs = torch.exp(model(torch.Tensor(t).cuda())).max(dim=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8a90b387071d417bd6087e2d83ebb9fe5e58f500"},"cell_type":"code","source":"preds = [i2s[i] for i in idxs.cpu().detach().numpy()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8c4fae954b906f9dbde3831a56448101b9cc0deb"},"cell_type":"code","source":"train_count = y_train.iloc[train_y_idx].surface.value_counts()\nvalid_count = y_train.iloc[valid_y_idx].surface.value_counts()[train_count.index]\npd.DataFrame({\n    \"train\": train_count/len(train_y_idx),\n    \"valid\": valid_count/len(valid_y_idx),\n}, index=train_count.index).plot.bar()\nplt.title(\"% of surfce samples per set\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f767910a3a5395f6b8b24b87655bcf9bfc561fd4"},"cell_type":"code","source":"train_count = pd.value_counts(preds)[train_count.index]\npd.value_counts\npd.DataFrame({\n    \"train\": train_count/len(train_y_idx),\n}, index=train_count.index).plot.bar()\nplt.title(\"% of surfce samples per set\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c52bfdb13139283390a3330d41ef7ba2b39c299e"},"cell_type":"code","source":"submission =  pd.read_csv(\"../input/sample_submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"549dd00038c00bb8987ccd54ea4f9228430c07e2"},"cell_type":"code","source":"submission['surface'] = preds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"640174f45c4f2fe8eb72ce20071ef8b069555de8"},"cell_type":"code","source":"submission.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5226bf9120ebacdfb033ba0da32d6c0b4264bbb4"},"cell_type":"code","source":"X_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c5380e0b7958c1e3cc8b7023370dee37d2ef7501"},"cell_type":"code","source":"surfaces","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a25788bf0f68954032e0ddc9f3d6cfa6a66b6b74"},"cell_type":"code","source":"preds = torch.exp(\n    model(torch.Tensor(Dataset.convert_df(X_train).reshape(-1, 1, num_cols, 128)).cuda())\n)\ntargets = torch.Tensor(Dataset.convert_target(y_train.surface)).long().cuda()\npred_ids = preds.max(dim=1)[1].cpu().detach().numpy()\ncorrect_ids = np.argwhere((y_train.surface == 'tiled') & (pred_ids == 4)).reshape(-1)\n\nseries_ids = correct_ids[40:44]\nplt.figure(figsize=(14, 10), dpi=80)\nfor idx, s_id in enumerate(series_ids):\n    plt.subplot(len(series_ids), 2, idx*2+1)\n    plt.plot(np.arange(128), X_train[X_train.series_id == s_id].angular_velocity_X)\n    plt.plot(np.arange(128), X_train[X_train.series_id == s_id].angular_velocity_Y)\n    plt.plot(np.arange(128), X_train[X_train.series_id == s_id].angular_velocity_Z)\n    plt.legend()\n    plt.title(y_train.loc[s_id].surface)\n\n    plt.subplot(len(series_ids), 2, idx*2+2)\n    plt.plot(np.arange(128), X_train[X_train.series_id == s_id].linear_acceleration_X)\n    plt.plot(np.arange(128), X_train[X_train.series_id == s_id].linear_acceleration_Y)\n    plt.plot(np.arange(128), X_train[X_train.series_id == s_id].linear_acceleration_Z)\n    plt.legend()\n    plt.title(y_train.loc[s_id].surface)\nplt.tight_layout(True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"9a7e256353fc9d1e63fc9a19229f1428613be5c0"},"cell_type":"code","source":"preds = torch.exp(\n    model(torch.Tensor(Dataset.convert_df(X_train).reshape(-1, 1, num_cols, 128)).cuda())\n)\ntargets = torch.Tensor(Dataset.convert_target(y_train.surface)).long().cuda()\npred_ids = preds.max(dim=1)[1].cpu().detach().numpy()\nincorrect_ids = np.argwhere((y_train.surface == 'tiled') & (pred_ids != 4)).reshape(-1)\n\nseries_ids = incorrect_ids[40:44]\nplt.figure(figsize=(14, 10), dpi=80)\nfor idx, s_id in enumerate(series_ids):\n    plt.subplot(len(series_ids), 2, idx*2+1)\n    plt.plot(np.arange(128), X_train[X_train.series_id == s_id].angular_velocity_X)\n    plt.plot(np.arange(128), X_train[X_train.series_id == s_id].angular_velocity_Y)\n    plt.plot(np.arange(128), X_train[X_train.series_id == s_id].angular_velocity_Z)\n    plt.legend()\n    plt.title(y_train.loc[s_id].surface)\n\n    plt.subplot(len(series_ids), 2, idx*2+2)\n    plt.plot(np.arange(128), X_train[X_train.series_id == s_id].linear_acceleration_X)\n    plt.plot(np.arange(128), X_train[X_train.series_id == s_id].linear_acceleration_Y)\n    plt.plot(np.arange(128), X_train[X_train.series_id == s_id].linear_acceleration_Z)\n    plt.legend()\n    plt.title(y_train.loc[s_id].surface)\nplt.tight_layout(True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8bcdf442e8f99c6d79def9d42dcbc0a3c9bd1be6"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8f8c41e00df3ed7cd2ee7c05b7d539ab404212cc"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}