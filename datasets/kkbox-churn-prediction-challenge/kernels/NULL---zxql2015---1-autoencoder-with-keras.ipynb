{"nbformat_minor": 1, "nbformat": 4, "cells": [{"source": ["Note that the autoencoder code are borrowed from the following notebook: https://github.com/curiousily/Credit-Card-Fraud-Detection-using-Autoencoders-in-Keras/blob/master/fraud_detection.ipynb\n", "\n", "The code used for summary statistics / dtype fixing belongs to ZihaoXu."], "cell_type": "markdown", "metadata": {"_uuid": "7d76e8d6f55dd51420857a11cba092c840b911a4", "_cell_guid": "5216d274-8031-4b2f-9dc8-f2351992eaf5"}}, {"source": ["# important packages to import\n", "import pandas as pd\n", "import numpy as np\n", "import tensorflow as tf\n", "import matplotlib.pyplot as plt\n", "import seaborn as sns\n", "import sys, os\n", "import pickle\n", "\n", "from scipy import stats\n", "from pylab import rcParams\n", "from keras.models import Model, load_model\n", "from keras.layers import Input, Dense\n", "from keras.callbacks import ModelCheckpoint, TensorBoard\n", "from keras import regularizers\n", "\n", "from matplotlib import offsetbox\n", "from matplotlib.ticker import NullFormatter\n", "from sklearn import preprocessing, cross_validation, svm, manifold\n", "from sklearn.cross_validation import cross_val_score, KFold\n", "from sklearn.ensemble import RandomForestClassifier # Load scikit's random forest classifier library\n", "from sklearn.grid_search import GridSearchCV\n", "\n", "from time import time\n", "from datetime import datetime, timedelta\n", "from collections import defaultdict\n", "\n", "from subprocess import check_output\n", "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "429a16b108ea544970eed55093467fa23b0254ec", "_cell_guid": "596c0675-6567-459c-9fde-f08060ae37bd"}, "outputs": []}, {"source": ["# holistic summary of the given data set. \n", "# \"remove_bad_rowCol\" can be turned on to remove non-informative col / row\n", "def holistic_summary(df, remove_bad_rowCol = False, verbose = True):\n", "    # remove non-informative columns\n", "    if(remove_bad_rowCol):\n", "        df = df.drop(df.columns[df.isnull().sum() >= .9 * len(df)], axis = 1)\n", "        df = df.drop(df.index[df.isnull().sum(axis = 1) >= .5* len(df.columns)], axis = 0)\n", "        \n", "    # fix column names:\n", "    df.columns = [c.replace(\" \", \"_\").lower() for c in df.columns]\n", "    \n", "    print('***************************************************************')\n", "    print('Begin holistic summary: ')\n", "    print('***************************************************************\\n')\n", "    \n", "    print('Dimension of df: ' + str(df.shape))\n", "    print('Percentage of good observations: ' + str(1 - df.isnull().any(axis = 1).sum()/len(df)))\n", "    print('---------------------------------------------------------------\\n')\n", "    \n", "    print(\"Rows with nan values: \" + str(df.isnull().any(axis = 1).sum()))\n", "    print(\"Cols with nan values: \" + str(df.isnull().any(axis = 0).sum()))\n", "    print('Breakdown:')\n", "    print(df.isnull().sum()[df.isnull().sum()!=0])\n", "    print('---------------------------------------------------------------\\n')\n", "    \n", "    print('Columns details: ')\n", "    print('Columns with known dtypes: ')\n", "    good_cols = pd.DataFrame(df.dtypes[df.dtypes!='object'], columns = ['type'])\n", "    good_cols['nan_num'] = [df[col].isnull().sum() for col in good_cols.index]\n", "    good_cols['unique_val'] = [df[col].nunique() for col in good_cols.index]\n", "    good_cols['example'] = [df[col][1] for col in good_cols.index]\n", "    good_cols = good_cols.reindex(good_cols['type'].astype(str).str.len().sort_values().index)\n", "    print(good_cols)\n", "    print('\\n')\n", "    \n", "    try:\n", "        print('Columns with unknown dtypes:')\n", "        bad_cols = pd.DataFrame(df.dtypes[df.dtypes=='object'], columns = ['type'])\n", "        bad_cols['nan_num'] = [df[col].isnull().sum() for col in bad_cols.index]\n", "        bad_cols['unique_val'] = [df[col].nunique() for col in bad_cols.index]\n", "        bad_cols['example(sliced)'] = [str(df[col][1])[:10] for col in bad_cols.index]\n", "        bad_cols = bad_cols.reindex(bad_cols['example(sliced)'].str.len().sort_values().index)\n", "        print(bad_cols)\n", "    except Exception as e:\n", "        print('No columns with unknown dtypes!')\n", "    print('_______________________________________________________________\\n\\n\\n')\n", "    #if not verbose: enablePrint()\n", "    return df\n", "def memo(df):\n", "    mem = df.memory_usage(index=True).sum()\n", "    print(mem/ 1024**2,\" MB\")"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "61b81a75737450a0e07045a10e5fe74a8f67fd55", "collapsed": true, "_cell_guid": "71148697-53e3-4095-b1c9-96ac40566689"}, "outputs": []}, {"source": ["trans = pd.read_csv('../input/transactions.csv')\n", "# Memory Reduction\n", "def change_datatype(df):\n", "    int_cols = list(df.select_dtypes(include=['int']).columns)\n", "    for col in int_cols:\n", "        if ((np.max(df[col]) <= 127) and(np.min(df[col] >= -128))):\n", "            df[col] = df[col].astype(np.int8)\n", "        elif ((np.max(df[col]) <= 32767) and(np.min(df[col] >= -32768))):\n", "            df[col] = df[col].astype(np.int16)\n", "        elif ((np.max(df[col]) <= 2147483647) and(np.min(df[col] >= -2147483648))):\n", "            df[col] = df[col].astype(np.int32)\n", "        else:\n", "            df[col] = df[col].astype(np.int64)\n", "            \n", "def change_datatype_float(df):\n", "    float_cols = list(df.select_dtypes(include=['float']).columns)\n", "    for col in float_cols:\n", "        df[col] = df[col].astype(np.float32)\n", "\n", "change_datatype(trans)\n", "change_datatype_float(trans)\n", "memo(trans)\n"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "8b562d92cc9532e4b83713bc71a7a1ffca6e5e1a", "_cell_guid": "eaf53791-1202-4566-9ddc-3056ea19d419"}, "outputs": []}, {"source": ["# fixing dtypes: time and numeric variables\n", "def fix_dtypes(df, time_cols, num_cols):\n", "    \n", "    print('***************************************************************')\n", "    print('Begin fixing data types: ')\n", "    print('***************************************************************\\n')\n", "    \n", "    def fix_time_col(df, time_cols):\n", "        for time_col in time_cols:\n", "            df[time_col] = pd.to_datetime(df[time_col], errors = 'coerce', format = '%Y%m%d')\n", "        print('---------------------------------------------------------------')\n", "        print('The following time columns has been fixed: ')\n", "        print(time_cols)\n", "        print('---------------------------------------------------------------\\n')\n", "\n", "    def fix_num_col(df, num_cols):\n", "        for col in num_cols:\n", "            df[col] = pd.to_numeric(df[col], errors = 'coerce')\n", "        print('---------------------------------------------------------------')\n", "        print('The following number columns has been fixed: ')\n", "        print(num_cols)\n", "        print('---------------------------------------------------------------\\n')\n", "        \n", "    if(len(num_cols) > 0):\n", "        fix_num_col(df, num_cols)\n", "    fix_time_col(df, time_cols)\n", "\n", "    print('---------------------------------------------------------------')\n", "    print('Final data types:')\n", "    result = pd.DataFrame(df.dtypes, columns = ['type'])\n", "    result = result.reindex(result['type'].astype(str).str.len().sort_values().index)\n", "    print(result)\n", "    print('_______________________________________________________________\\n\\n\\n')\n", "    return df"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "98ef0cca74072861013f843f5d4aa9f8099265b6", "collapsed": true, "_cell_guid": "82c42e27-5b03-4f14-87b5-e8db4e06271e"}, "outputs": []}, {"source": ["np.random.seed(47)\n", "samp = trans.sample(frac = .01, replace = False)\n", "train = pd.read_csv('../input/train.csv')\n", "train = train.append(pd.read_csv('../input/train_v2.csv'))\n", "train.index = range(len(train))\n", "\n", "test = pd.read_csv('../input/sample_submission_zero.csv')\n", "test = test.append(pd.read_csv('../input/sample_submission_v2.csv'))\n", "test.index = range(len(test))\n", "\n", "members = pd.read_csv('../input/members_v3.csv')\n", "\n", "samp = samp.merge(train, on = 'msno', how = 'inner')\n", "samp = samp.merge(members, on = 'msno', how = 'inner')\n", "\n", "samp.head()"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "6d6197ebb2146593fa350860829cb72d15b07403", "_cell_guid": "403e1451-ff1d-4648-af48-2ed88c3b37b9"}, "outputs": []}, {"source": ["samp = fix_dtypes(samp, time_cols = ['transaction_date', 'membership_expire_date', 'registration_init_time'], num_cols = [])\n", "samp['year'] = [d.year for d in samp['transaction_date']]\n", "samp['month'] = [d.month for d in samp['transaction_date']]\n", "samp['day'] = [d.day for d in samp['transaction_date']]\n", "samp['wday'] = [d.weekday() for d in samp['transaction_date']]\n", "samp['transaction_date'] = [d.year + (d.month-1) / 12 + d.day / 365 for d in samp['transaction_date']]\n", "samp['membership_expire_date'] = [d.year + (d.month-1) / 12 + d.day / 365 for d in samp['membership_expire_date']]\n", "samp['registration_init_time'] = [d.year + (d.month-1) / 12 + d.day / 365 for d in samp['registration_init_time']]\n", "memo(samp)"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "fb5ee3c90c3c6a86ebe80d128bb288edcca68b18", "_cell_guid": "ef2f834c-a874-4b4e-94e7-4c83bbf1e09f"}, "outputs": []}, {"source": ["from multiprocessing import Pool, cpu_count\n", "import gc; gc.enable()\n", "\n", "def transform_df(df):\n", "    df = pd.DataFrame(df)\n", "    df = df.sort_values(by=['date'], ascending=[False])\n", "    df = df.reset_index(drop=True)\n", "    df = df.drop_duplicates(subset=['msno'], keep='first')\n", "    return df\n", "\n", "def transform_df2(df):\n", "    df = df.sort_values(by=['date'], ascending=[False])\n", "    df = df.reset_index(drop=True)\n", "    df = df.drop_duplicates(subset=['msno'], keep='first')\n", "    return df\n", "\n", "df_iter = pd.read_csv('../input/user_logs.csv', low_memory=False, iterator=True, chunksize=10000000)\n", "last_user_logs = []\n", "i = 0 #~400 Million Records - starting at the end but remove locally if needed\n", "for df in df_iter:\n", "    if i>35: # used to be 35, just testing\n", "        if len(df)>0:\n", "            print(df.shape)\n", "            p = Pool(cpu_count())\n", "            df = p.map(transform_df, np.array_split(df, cpu_count()))   \n", "            df = pd.concat(df, axis=0, ignore_index=True).reset_index(drop=True)\n", "            df = transform_df2(df)\n", "            p.close(); p.join()\n", "            last_user_logs.append(df)\n", "            print('...', df.shape)\n", "            df = []\n", "    i+=1\n", "\n", "last_user_logs = pd.concat(last_user_logs, axis=0, ignore_index=True).reset_index(drop=True)\n", "last_user_logs = transform_df2(last_user_logs)"], "execution_count": null, "cell_type": "code", "metadata": {}, "outputs": []}, {"source": ["print(last_user_logs.shape)\n", "print(list(last_user_logs))"], "execution_count": null, "cell_type": "code", "metadata": {}, "outputs": []}, {"source": ["print(\"Len before the merge: \", len(samp))\n", "samp = samp.merge(last_user_logs, on = 'msno', how = 'inner')\n", "print(\"Len after the merge: \", len(samp))"], "execution_count": null, "cell_type": "code", "metadata": {}, "outputs": []}, {"source": ["print(list(samp))\n", "print(\"Number of observations: \" + str(len(samp)))\n", "samp.head()"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "7d6f62b3cad3336953c90475c9170a69659921e3", "_cell_guid": "9ef5661f-767d-415f-9316-3792e0e73f25"}, "outputs": []}, {"source": ["# Visualization of the \"churn\" class"], "cell_type": "markdown", "metadata": {"_uuid": "1cd3ac048d76c1df78156286dad46c6e9b0fae33", "_cell_guid": "fdab7743-63de-444c-8b15-dd635a6051ff"}}, {"source": ["# We see that only the gender column has quite a lot of NANs\n", "df = samp\n", "df = holistic_summary(df)"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "bba4e268c51de78a3492ee32dd80d8370ecf3fca", "_cell_guid": "ee3a1996-150f-4eb3-a8de-6ccbdd7ee191"}, "outputs": []}, {"source": ["# impute the missing genders randomly\n", "import random\n", "np.random.seed(47)\n", "\n", "gender = []\n", "for x in df['gender']:\n", "    if type(x) == np.float:\n", "        gender.append(random.choice(['female', 'male']))\n", "    else:\n", "        gender.append(x)\n", "df['gender'] = gender\n", "df['gender'].isnull().any()"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "ec8a302d25c08a3546875098d84a2e94f1aa1fac", "_cell_guid": "ed7f412d-1cb1-423a-b5f2-ed5e5e62e6a8"}, "outputs": []}, {"source": ["sns.set(style = 'white')\n", "sns.countplot(data = df, x = 'is_churn')\n", "sns.despine()"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "5386bbc6e670e94881a4ff20bb94a4e372ce2d68", "_cell_guid": "403157c0-6857-4431-bad8-8a8d3c2ae0e5"}, "outputs": []}, {"source": ["The dataset is quite imbalanced..."], "cell_type": "markdown", "metadata": {"_uuid": "b2908e16012335fe1ff64ba5b3a67c65cebeeae0", "_cell_guid": "c3430b8c-5f2d-4ed7-a1c2-7ceaf155c354"}}, {"source": ["print(\"Churn ratio\", len(df[df['is_churn'] == 1])/len(df[df['is_churn'] == 0]))"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "00f79f4c2c2f887d4700dacb52b981c5bb8ef20e", "_cell_guid": "7534c489-63a3-40ba-b714-6bb05ba37fd2"}, "outputs": []}, {"source": ["churn = df[df['is_churn'] == 1]\n", "n_churn = df[df['is_churn'] == 0]\n", "\n", "f, (ax1, ax2) = plt.subplots(2, 1, sharex=True)\n", "f.suptitle('Amount per transaction by class')\n", "\n", "bins = 50\n", "ax1.hist(churn.actual_amount_paid, bins = bins)\n", "ax1.set_title('Churn')\n", "\n", "ax2.hist(n_churn.actual_amount_paid, bins = bins)\n", "ax2.set_title('Not Churn')\n", "\n", "plt.xlabel('Amount ($)')\n", "plt.ylabel('Number of Transactions')\n", "plt.show();"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "af9d82a0f0c30ffb4d9586d0aaaf40c6932fbdd4", "_cell_guid": "045d9a5b-3dfb-4354-b562-68d81fecd025"}, "outputs": []}, {"source": ["time_cols = ['wday', 'membership_expire_date']\n", "\n", "for t in time_cols:\n", "    f, (ax1, ax2) = plt.subplots(2, 1, sharex=True)\n", "    f.suptitle('Time of transaction vs Amount by class')\n", "    ax1.scatter(churn[t], churn['actual_amount_paid'], s = 2,alpha = .25)\n", "    ax1.set_title('Churn')\n", "\n", "    ax2.scatter(n_churn[t], n_churn['actual_amount_paid'], s = 2, alpha = .25)\n", "    ax2.set_title('Not Churn')\n", "\n", "    plt.xlabel('Time')\n", "    plt.ylabel('Amount')\n", "    plt.show()"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "ef7373559833cb8b4aa6e8248cbd8c56eb198b5c", "_cell_guid": "9416b1cb-bcae-4592-86ab-89791a857876"}, "outputs": []}, {"source": ["# Keras autoencoder"], "cell_type": "markdown", "metadata": {"_uuid": "70b99ac8b1432253d617f994a2842e4fade2ab20", "collapsed": true, "_cell_guid": "472783b4-ff8a-413b-901f-ef952e8f1d23"}}, {"source": ["# Drop the 'msno', 'bd' cols since have no value\n", "df = df.drop(['msno', 'bd'], axis = 1)"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "e27c92da6a4b36f59ba068cf17779a3a1d663e1d", "collapsed": true, "_cell_guid": "96b9da72-60b5-4b55-8ad3-a2c4569dbeea"}, "outputs": []}, {"source": ["# Encode the gender col to 1 for male and 0 for female\n", "df['gender'] = np.where(df['gender'] == 'male', 1, 0)"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "d401c977fe1aa22bf6c514a5994fc94c3f212b14", "collapsed": true, "_cell_guid": "511c07be-8a75-4d6d-ba5f-e6124c41d340"}, "outputs": []}, {"source": ["# Plot the correlation matrix\n", "def corr_plot(df, title = 'Correlation Matrix', annot=False, show = True):\n", "    sns.set(style = 'white')\n", "\n", "    # Compute the correlation matrix\n", "    corr = df.corr()\n", "\n", "    # Generate a mask for the upper triangle\n", "    mask = np.zeros_like(corr, dtype=np.bool)\n", "    mask[np.triu_indices_from(mask)] = True\n", "\n", "    # Set up the matplotlib figure\n", "    f, ax = plt.subplots(figsize=(11, 9))\n", "\n", "    # Generate a custom diverging colormap\n", "    cmap = sns.diverging_palette(220, 10, as_cmap=True)\n", "\n", "    # Draw the heatmap with the mask and correct aspect ratio\n", "    sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0, annot=annot,\n", "                square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n", "    plt.title(title)\n", "    if show:\n", "        plt.show()\n", "corr_plot(samp[sorted(list(df))])"], "execution_count": null, "cell_type": "code", "metadata": {}, "outputs": []}, {"source": ["features = np.array(df[[col for col in df.columns if col != 'is_churn']])\n", "response = np.array(df[['is_churn']])\n", "\n", "print(len(features))\n", "print(len(response))"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "a771ac9bbd629b80118452269b454ef11b4dedb5", "_cell_guid": "84bd8626-6ce0-4f05-9870-64db72fc89ca"}, "outputs": []}, {"source": ["features"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "41298a3a306ad7c2b320e776830f3f2d456be8bb", "_cell_guid": "2f12d22c-3cfc-48f4-88d4-c7c11c79e23c"}, "outputs": []}, {"source": ["Let's first standardize the data and apply PCA."], "cell_type": "markdown", "metadata": {"_uuid": "e8d50f444223422227cb62d9e79390d65213a072", "_cell_guid": "464c81b3-855e-4b8a-a33b-3d3e3baa015e"}}, {"source": ["from sklearn import decomposition\n", "from sklearn.preprocessing import StandardScaler\n", "\n", "# Fit the scaler to the features and transform\n", "features_std = StandardScaler().fit_transform(features)"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "455800e97adba1a028fbc274626d75101d1bde3f", "collapsed": true, "_cell_guid": "34a6d6e7-3ecd-423c-9966-51868ded7ce1"}, "outputs": []}, {"source": ["# Create a pca object with the 20 components as a parameter\n", "pca = decomposition.PCA(n_components=20)\n", "\n", "# Fit the PCA and transform the data\n", "features_pca = pca.fit_transform(features_std)"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "6f85ad72d0f6c5a577369b9478f84483a710aacb", "collapsed": true, "_cell_guid": "62a8ce50-2fc4-4e48-93ff-26b6c18e5fea"}, "outputs": []}, {"source": ["features_pca.shape"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "27222d0c747d72cc67b5421b62d8068c596f1f3a", "_cell_guid": "d9e17e12-6bb1-4c84-907f-4d8b75cccc2b"}, "outputs": []}, {"source": ["viz = pd.DataFrame(features_pca)[[0,1,2]]\n", "viz.columns = [str(c) for c in viz.columns]\n", "viz['is_churn'] = df['is_churn']\n", "sns.lmplot(data = viz, x = '0', y = '1', fit_reg=False, hue = 'is_churn')\n", "plt.show()"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "adb70cf0fa1ccf78f865a8e8b8937625d55eeea5", "_cell_guid": "06782548-ff84-4f9d-bd27-fa8c7ecf979c"}, "outputs": []}, {"source": ["# Convert features_pca back to a df and add the is_churn column\n", "features_pca = pd.DataFrame(features_pca)\n", "features_pca['is_churn'] = df['is_churn']\n", "\n", "features_std = pd.DataFrame(features_std)\n", "features_std['is_churn'] = df['is_churn']"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "96ce67d1e0741805293f984bcdc4796bfbbbeb48", "_cell_guid": "86b84c9c-cf00-4657-abff-a96f24a3c6da"}, "outputs": []}, {"source": ["from sklearn.cross_validation import train_test_split\n", "X_train, X_test = train_test_split(features_std, test_size=0.2, random_state=47)\n", "X_train = X_train[X_train['is_churn'] == 0]\n", "X_train = X_train.drop(['is_churn'], axis=1)\n", "\n", "y_test = X_test['is_churn']\n", "X_test = X_test.drop(['is_churn'], axis=1)\n", "\n", "X_train = X_train.values\n", "X_test = X_test.values"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "568c56e375415342fa5a2d69387eb4b17f7c3ed5", "collapsed": true, "_cell_guid": "bb4d21f4-4021-4c74-8105-8d707afbe29e"}, "outputs": []}, {"source": ["X_train.shape"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "e58689c62a36e39d2da353c9afaea80d37694222", "_cell_guid": "b9300c51-536c-4c10-8e8d-a467afc27df2"}, "outputs": []}, {"source": ["X_test.shape"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "0a5198af6c4727d33c2d5d64dc8972f3a68a66f6", "_cell_guid": "11a612f9-c38b-4e14-98eb-19a790203c65"}, "outputs": []}, {"source": ["## Building the model"], "cell_type": "markdown", "metadata": {"_uuid": "2035b7dbe6f6529630d411b1cf3511a29b97181a", "_cell_guid": "1095a30e-2bdb-4661-82db-79a17c6c5efc"}}, {"source": ["input_dim = X_train.shape[1]\n", "encoding_dim = 14"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "b8a9c37b99b1775a8ebc36c67a57510c3a9252fa", "collapsed": true, "_cell_guid": "7f85167c-50ba-4e16-b901-eee9821063d3"}, "outputs": []}, {"source": ["input_layer = Input(shape=(input_dim, ))\n", "\n", "encoder = Dense(encoding_dim, activation=\"tanh\", \n", "                activity_regularizer=regularizers.l1(10e-5))(input_layer)\n", "encoder = Dense(int(encoding_dim / 2), activation=\"relu\")(encoder)\n", "\n", "decoder = Dense(int(encoding_dim / 2), activation='tanh')(encoder)\n", "decoder = Dense(input_dim, activation='relu')(decoder)\n", "\n", "autoencoder = Model(inputs=input_layer, outputs=decoder)"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "760a5ac69247ae5ebe9676c91a7e2d8ccfcdc1d1", "collapsed": true, "_cell_guid": "d5fd75bc-75ab-4cab-9285-862a65578c10"}, "outputs": []}, {"source": ["nb_epoch = 100\n", "batch_size = 32\n", "\n", "autoencoder.compile(optimizer='adam', \n", "                    loss='mean_squared_error', \n", "                    metrics=['accuracy'])\n", "\n", "checkpointer = ModelCheckpoint(filepath=\"model.h5\",\n", "                               verbose=0,\n", "                               save_best_only=True)\n", "tensorboard = TensorBoard(log_dir='./logs',\n", "                          histogram_freq=0,\n", "                          write_graph=True,\n", "                          write_images=True)\n", "\n", "history = autoencoder.fit(X_train, X_train,\n", "                    epochs=nb_epoch,\n", "                    batch_size=batch_size,\n", "                    shuffle=True,\n", "                    validation_data=(X_test, X_test),\n", "                    verbose=1,\n", "                    callbacks=[checkpointer, tensorboard]).history"], "execution_count": null, "cell_type": "code", "metadata": {"scrolled": false, "_uuid": "025b0218858f700e1d5f6aa986bc7184838be59c", "_cell_guid": "f94b0b96-12f1-4fc6-ac50-e4e04b7567b2"}, "outputs": []}, {"source": ["autoencoder = load_model('model.h5')"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "74c8bbcbacdb134ad61c4fe54bdbbf06e1459d86", "collapsed": true, "_cell_guid": "30384add-4700-4084-983f-fe02e1effbc9"}, "outputs": []}, {"source": ["# Evaluate the Model"], "cell_type": "markdown", "metadata": {"_uuid": "135d67f70737840367462cb09110f9455a11749e", "_cell_guid": "9add179c-c60c-425e-98fa-accc87096fce"}}, {"source": ["plt.plot(history['loss'])\n", "plt.plot(history['val_loss'])\n", "plt.title('model loss')\n", "plt.ylabel('loss')\n", "plt.xlabel('epoch')\n", "plt.legend(['train', 'test'], loc='upper right');"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "38a6c8ea3e8989c08bb6d697f2e09905dffe86f9", "_cell_guid": "8580fa90-01b7-4894-a126-37ccc149e458"}, "outputs": []}, {"source": ["predictions = autoencoder.predict(X_test)"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "dd2aeb568944d234651e930102892a17249eb72e", "collapsed": true, "_cell_guid": "acfadaa5-91a3-4e88-a12d-7805a3523813"}, "outputs": []}, {"source": ["mse = np.mean(np.power(X_test - predictions, 2), axis=1)\n", "error_df = pd.DataFrame({'reconstruction_error': mse,\n", "                        'true_class': y_test})"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "9f0a8fbd2d1c1e2643960d7dee2dfa55383c84f9", "collapsed": true, "_cell_guid": "e244b189-cccf-443e-95b1-1b504cd767c0"}, "outputs": []}, {"source": ["error_df.describe()"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "3578fb3568fcba97406a4ad1fb0839befbf6048d", "_cell_guid": "e0e4cf8e-808d-44b1-a902-201edfe58cb0"}, "outputs": []}, {"source": ["fig = plt.figure()\n", "ax = fig.add_subplot(111)\n", "normal_error_df = error_df[(error_df['true_class']== 0) & (error_df['reconstruction_error'] < 10)]\n", "_ = ax.hist(normal_error_df.reconstruction_error.values, bins=10)\n", "plt.title('Reconstruction error without fraud')\n", "sns.despine()"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "e74ed5a635e7318b7fef01293c593680ea1c9672", "_cell_guid": "14a9ed40-0687-4458-a569-ea0611e79066"}, "outputs": []}, {"source": ["fig = plt.figure()\n", "ax = fig.add_subplot(111)\n", "fraud_error_df = error_df[error_df['true_class'] == 1]\n", "_ = ax.hist(fraud_error_df.reconstruction_error.values, bins=10)\n", "plt.title('Reconstruction error with fraud')\n", "sns.despine()"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "059bf0d1b3311470c3cae44c237052e83c6033ee", "_cell_guid": "3cc7e9c5-f97a-4055-8112-c4bcf34a6a20"}, "outputs": []}, {"source": ["from sklearn.metrics import (confusion_matrix, precision_recall_curve, auc,\n", "                             roc_curve, recall_score, classification_report, f1_score,\n", "                             precision_recall_fscore_support)"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "603728abbe2dd9fd4347ec5485233a5af4c1ee0f", "collapsed": true, "_cell_guid": "1fe82725-135b-4107-b58e-980dd44e3d35"}, "outputs": []}, {"source": ["fpr, tpr, thresholds = roc_curve(error_df.true_class, error_df.reconstruction_error)\n", "roc_auc = auc(fpr, tpr)\n", "\n", "plt.title('Receiver Operating Characteristic')\n", "plt.plot(fpr, tpr, label='AUC = %0.4f'% roc_auc)\n", "plt.legend(loc='lower right')\n", "plt.plot([0,1],[0,1],'r--')\n", "plt.xlim([-0.001, 1])\n", "plt.ylim([0, 1.001])\n", "plt.ylabel('True Positive Rate')\n", "plt.xlabel('False Positive Rate')\n", "plt.show();"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "65b32585322b32c837cf69a9f46308c5266dc068", "_cell_guid": "1359fb43-c33c-492e-af55-e8a151867cd9"}, "outputs": []}, {"source": ["precision, recall, th = precision_recall_curve(error_df.true_class, error_df.reconstruction_error)\n", "plt.plot(recall, precision, 'b', label='Precision-Recall curve')\n", "plt.title('Recall vs Precision')\n", "plt.xlabel('Recall')\n", "plt.ylabel('Precision')\n", "plt.show()"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "0c9ceededb3ce81e9708e349c5f95c15540db4b2", "_cell_guid": "82e80b6b-5629-4aa2-9b3b-5f19c159bd14"}, "outputs": []}, {"source": ["plt.plot(th, precision[1:], 'b', label='Threshold-Precision curve')\n", "plt.title('Precision for different threshold values')\n", "plt.xlabel('Threshold')\n", "plt.ylabel('Precision')\n", "plt.show()"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "68ec29e59ffd13d6a616ed8fd223b708ac012f93", "_cell_guid": "e22b457d-1d3a-405d-b599-5b0a18a234ab"}, "outputs": []}, {"source": ["plt.plot(th, recall[1:], 'b', label='Threshold-Recall curve')\n", "plt.title('Recall for different threshold values')\n", "plt.xlabel('Reconstruction error')\n", "plt.ylabel('Recall')\n", "plt.show()"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "63c35818d8658ba4773a72e64d7e2941fbf4bb60", "_cell_guid": "7b4b6d81-30b3-4977-b130-c6b434d58759"}, "outputs": []}, {"source": ["# Prediction"], "cell_type": "markdown", "metadata": {"_uuid": "fde86829e9efe58cce3c0d5d1181dcdb8a703136", "_cell_guid": "b8246f8e-10fb-4093-b90a-f85edaba723e"}}, {"source": ["threshold = 2.9"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "0b88beb36b33ccebcdfefc80ee71a4e1f16da9eb", "collapsed": true, "_cell_guid": "ad6c4ba9-157d-42bc-94bb-6e021faf7438"}, "outputs": []}, {"source": ["groups = error_df.groupby('true_class')\n", "fig, ax = plt.subplots()\n", "\n", "for name, group in groups:\n", "    ax.plot(group.index, group.reconstruction_error, marker='o', ms=3.5, linestyle='',\n", "            label= \"churn\" if name == 1 else \"not churn\")\n", "ax.hlines(threshold, ax.get_xlim()[0], ax.get_xlim()[1], colors=\"r\", zorder=100, label='Threshold')\n", "ax.legend()\n", "plt.title(\"Reconstruction error for different classes\")\n", "plt.ylabel(\"Reconstruction error\")\n", "plt.xlabel(\"Data point index\")\n", "sns.despine()\n", "plt.show();"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "8c9fc84b5767744009a9a4de653a01a343b289df", "_cell_guid": "63ef41f3-4bf2-42fa-bc0d-7abc8f79e87a"}, "outputs": []}, {"source": ["LABELS = ['churn', 'not churn']\n", "\n", "y_pred = [1 if e > threshold else 0 for e in error_df.reconstruction_error.values]\n", "conf_matrix = confusion_matrix(error_df.true_class, y_pred)\n", "\n", "plt.figure(figsize=(12, 12))\n", "sns.heatmap(conf_matrix, xticklabels=LABELS, yticklabels=LABELS, annot=True, fmt=\"d\");\n", "plt.title(\"Confusion matrix\")\n", "plt.ylabel('True class')\n", "plt.xlabel('Predicted class')\n", "plt.show()"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "7836721427595436909ecfef9d26b15886056d00", "_cell_guid": "9b963411-257e-4bbd-b472-b26abbf0ac73"}, "outputs": []}, {"source": [], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "f4d9eebe1fd131a7e5805468d07139589c85d2c2", "collapsed": true, "_cell_guid": "e2eb55ff-8b42-4a11-b734-2a728bd89127"}, "outputs": []}], "metadata": {"kernelspec": {"display_name": "Python 3", "name": "python3", "language": "python"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "pygments_lexer": "ipython3", "mimetype": "text/x-python", "name": "python", "version": "3.6.3", "nbconvert_exporter": "python"}}}