{"cells":[{"metadata":{"trusted":true,"_uuid":"fe37655557093c30117274ecddcf8e24fe0a60bc"},"cell_type":"code","source":"import os\nimport gc\nimport pickle\nfrom pathlib import Path\n#math\nimport numpy as np\nimport pandas as pd\nimport random\nfrom sklearn.preprocessing import minmax_scale\nfrom sklearn.model_selection import GridSearchCV, StratifiedKFold # Used to use Kfold to train our model\n#keras\nimport keras\nfrom keras.layers import *\nfrom keras.optimizers import *\nfrom keras.models import *\nfrom keras.callbacks import *\nfrom keras import regularizers\nfrom keras.utils.data_utils import *\nfrom keras.layers.normalization import BatchNormalization\nfrom keras import backend as K \nimport tensorflow as tf\n\nprint(os.listdir(\"../input\"))\n\n#force gpu in my local environment\n#os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n\n# really clears all things\ndef clear_session(model):\n    sess = K.get_session()\n    K.clear_session()\n    try:\n        del model\n    except:\n        pass\n    sess.close()\n    config = tf.ConfigProto()\n    config.gpu_options.per_process_gpu_memory_fraction = 1\n    K.set_session(tf.Session(config=config))\n    gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e4623935845fe6c106acea42a3d52f249b483eb6"},"cell_type":"code","source":"# config\nMODEL_NAME = \"santander_1\"\nRESCALE = True\nMIN_MAX = (-1,1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"57e60af32c5ed33857180f91931897c099b14f92"},"cell_type":"code","source":"#file paths\ninput_path = Path(\"../input\")\ntrain_csv = str(input_path / \"train.csv\")\ntest_csv = str(input_path / \"test.csv\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e0d3815eab1f6c65dfb2f0803708fac8e34e5df2"},"cell_type":"code","source":"#load original features\ntrain_df = pd.read_csv(train_csv)\ntest_df = pd.read_csv(test_csv)\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"664cfed66f6c5cf3ea6f6044c00457af7de76b82"},"cell_type":"code","source":"#rescale all feature cols to, MIN_MAX setting\nfeature_columns = []\nfor i in range(0,200,1):\n    key = 'var_' + str(i)\n    feature_columns.append(key)\n    if RESCALE:\n        test_df[key] = minmax_scale(test_df[key].values.astype(np.float32), feature_range=MIN_MAX, axis=0)\n        train_df[key] = minmax_scale(train_df[key].values.astype(np.float32), feature_range=MIN_MAX, axis=0)\n\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"e2c67998c101af220d080e405664b21b6d3f1b33"},"cell_type":"code","source":"#make a model\ndef crop(dimension, start, end):\n    # Crops (or slices) a Tensor on a given dimension from start to end\n    # example : to crop tensor x[:, :, 5:10]\n    # call slice(2, 5, 10) as you want to crop on the second dimension\n    def func(x):\n        if dimension == 0:\n            return x[start: end]\n        if dimension == 1:\n            return x[:, start: end]\n        if dimension == 2:\n            return x[:, :, start: end]\n        if dimension == 3:\n            return x[:, :, :, start: end]\n        if dimension == 4:\n            return x[:, :, :, :, start: end]\n    return Lambda(func)\n\ndef build_model(dim=(len(feature_columns),)):\n    input_tensor = Input(shape=dim)\n    x = input_tensor\n    x = Dense(75)(x)\n    x = Activation('tanh')(x)\n    x = Dense(15)(x)\n    x = Activation('tanh')(x)\n    x = Dense(1)(x)\n    x = Activation('hard_sigmoid')(x)\n    return Model(input_tensor, x, name='b1')\nmodel = build_model()\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"code_folding":[0],"trusted":true,"_uuid":"4d72d6547c262490d4bee8c2f53fb6346cf8c58b"},"cell_type":"code","source":"#checkpoint reverter, reloads saved if not better val score\nimport keras\nclass CheckpointReverter(keras.callbacks.Callback):\n    def __init__(self, filepath, monitor='val_loss', verbose=0,\n                 mode='auto',skip=3,max_lr=1e-4):\n        super(CheckpointReverter, self).__init__()\n        self.monitor = monitor\n        self.verbose = verbose\n        self.filepath = filepath\n        self.max_skip = skip\n        self.max_lr = max_lr\n        self.current_skip = 0\n        if mode not in ['auto', 'min', 'max']:\n            warnings.warn('ModelCheckpoint mode %s is unknown, '\n                          'fallback to auto mode.' % (mode),\n                          RuntimeWarning)\n            mode = 'auto'\n\n        if mode == 'min':\n            self.monitor_op = np.less\n            self.best = np.Inf\n        elif mode == 'max':\n            self.monitor_op = np.greater\n            self.best = -np.Inf\n        else:\n            if 'acc' in self.monitor or self.monitor.startswith('fmeasure'):\n                self.monitor_op = np.greater\n                self.best = -np.Inf\n            else:\n                self.monitor_op = np.less\n                self.best = np.Inf\n    def on_epoch_end(self, epoch, logs=None):\n        logs = logs or {}\n        current = logs.get(self.monitor)\n        if current is None:\n            warnings.warn('Can save best model only with %s available, '\n                          'skipping.' % (self.monitor), RuntimeWarning)\n        else:\n            filepath = self.filepath.format(epoch=epoch + 1, **logs)\n            if self.monitor_op(current, self.best):\n                if self.verbose > 0:\n                    print('\\nEpoch %05d: %s improved from %0.5f to %0.5f,'\n                          ' dont load model %s'\n                          % (epoch + 1, self.monitor, self.best,\n                             current, filepath))\n                self.best = current\n                self.current_skip = 0\n            elif current != 0.:\n                if self.current_skip >= self.max_skip:\n                    self.current_skip = 0\n                    try:\n                        self.model.load_weights(filepath)\n                        old_lr = K.get_value(self.model.optimizer.lr)\n                        new_lr = old_lr * 100\n                        if new_lr > self.max_lr:\n                            new_lr = self.max_lr\n                        K.set_value(self.model.optimizer.lr, new_lr)\n                    except:\n                        print(\"error loading weights\")\n\n                    if self.verbose > 0:\n                        print('\\nEpoch %05d: %s did not improve from %0.5f,'\n                              ' load best from %s, new lr: %f '\n                              % (epoch + 1, self.monitor, self.best,filepath,new_lr))\n                else:\n                    self.current_skip += 1\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"afaebeb3e1a284b49e6be5caac33320feb42e615"},"cell_type":"code","source":"# custom loss\nfrom keras.losses import *\n\n\nPOS_WEIGHT = 10  # multiplier for positive targets, needs to be tuned\n\ndef wbce(target, output):\n    \"\"\"\n    Weighted binary crossentropy between an output tensor \n    and a target tensor. POS_WEIGHT is used as a multiplier \n    for the positive targets.\n\n    Combination of the following functions:\n    * keras.losses.binary_crossentropy\n    * keras.backend.tensorflow_backend.binary_crossentropy\n    * tf.nn.weighted_cross_entropy_with_logits\n    \"\"\"\n    # transform back to logits\n    output = K.clip(output, K.epsilon(), 1-K.epsilon())\n    output = tf.log(output / (1 - output))\n\n    # compute weighted loss\n    loss = tf.nn.weighted_cross_entropy_with_logits(targets=target,\n                                                    logits=output,\n                                                    pos_weight=POS_WEIGHT)\n    loss = tf.where(tf.is_nan(loss), tf.zeros_like(loss), loss)\n    return K.mean(loss)\n    \ndef f1(y_true, y_pred):\n#    y_pred = binaryRound(y_pred)\n    tp = K.sum(K.cast(y_true*y_pred, K.floatx()), axis=0)\n    tn = K.sum(K.cast((1-y_true)*(1-y_pred), K.floatx()), axis=0)\n    fp = K.sum(K.cast((1-y_true)*y_pred, K.floatx()), axis=0)\n    fn = K.sum(K.cast(y_true*(1-y_pred), K.floatx()), axis=0)\n\n    p = tp / (tp + fp + K.epsilon())\n    r = tp / (tp + fn + K.epsilon())\n\n    f1 = 2*p*r / (p+r+K.epsilon())\n    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n    return K.mean(f1)\n\ndef f1_loss(y_true,y_pred):\n    return tf.cast(1-f1(y_true,y_pred),K.floatx())\n\ndef roc_auc_score( y_true,y_pred):\n    \"\"\" ROC AUC Score.\n    Approximates the Area Under Curve score, using approximation based on\n    the Wilcoxon-Mann-Whitney U statistic.\n    Yan, L., Dodier, R., Mozer, M. C., & Wolniewicz, R. (2003).\n    Optimizing Classifier Performance via an Approximation to the Wilcoxon-Mann-Whitney Statistic.\n    Measures overall performance for a full range of threshold levels.\n    Arguments:\n        y_pred: `Tensor`. Predicted values.\n        y_true: `Tensor` . Targets (labels), a probability distribution.\n    \"\"\"\n    with tf.name_scope(\"RocAucScore\"):\n\n        pos = tf.boolean_mask(y_pred, tf.cast(y_true, tf.bool))\n        neg = tf.boolean_mask(y_pred, ~tf.cast(y_true, tf.bool))\n\n        pos = tf.expand_dims(pos, 0)\n        neg = tf.expand_dims(neg, 1)\n\n        # original paper suggests performance is robust to exact parameter choice\n        gamma = 0.3\n        p     = 2\n\n        difference = tf.zeros_like(pos * neg) + pos - neg - gamma\n\n        masked = tf.boolean_mask(difference, difference < 0.0)\n\n    return tf.reduce_sum(tf.pow(-masked, p))\n\n#---------------------------\n# AUC for a binary classifier\ndef auc(y_true, y_pred):\n#     ptas = tf.stack([binary_PTA(y_true,y_pred)],axis=0)\n#     pfas = tf.stack([binary_PFA(y_true,y_pred)],axis=0)\n    ptas = tf.stack([binary_PTA(y_true,y_pred,k) for k in np.linspace(0, 1, 10)],axis=0)\n    pfas = tf.stack([binary_PFA(y_true,y_pred,k) for k in np.linspace(0, 1, 10)],axis=0)\n    pfas = tf.concat([tf.ones((1,)) ,pfas],axis=0)\n    binSizes = -(pfas[1:]-pfas[:-1])\n    s = ptas*binSizes\n    return K.sum(s, axis=0)\n\n#---------------------\n# PFA, prob false alert for binary classifier\ndef binary_PFA(y_true, y_pred, threshold=K.variable(value=0.5)):\n    y_pred = K.cast(y_pred >= threshold, 'float32')\n    # N = total number of negative labels\n    N = K.sum(1 - y_true)\n    # FP = total number of false alerts, alerts from the negative class labels\n    FP = K.sum(y_pred - y_pred * y_true)\n    return FP/N\n\n#----------------\n# P_TA prob true alerts for binary classifier\ndef binary_PTA(y_true, y_pred, threshold=K.variable(value=0.5)):\n    y_pred = K.cast(y_pred >= threshold, 'float32')\n    # P = total number of positive labels\n    P = K.sum(y_true)\n    # TP = total number of correct alerts, alerts from the positive class labels\n    TP = K.sum(y_pred * y_true)\n    return TP/P\n\n#---------------------\n# PFA, prob false alert for binary classifier\ndef binary_PFA_l(y_true, y_pred):\n    # N = total number of negative labels\n    N = K.sum(1 - y_true)\n    # FP = total number of false alerts, alerts from the negative class labels\n    FP = K.sum(y_pred - y_pred * y_true)\n    return FP/N\n\n#----------------\n# P_TA prob true alerts for binary classifier\ndef binary_PTA_l(y_true, y_pred):\n    # P = total number of positive labels\n    P = K.sum(y_true)\n    # TP = total number of correct alerts, alerts from the positive class labels\n    TP = K.sum(y_pred * y_true)\n    return TP/P\n\ndef auc_loss(t,p):\n    ptas = tf.stack([binary_PTA_l(t,p)],axis=0)\n    pfas = tf.stack([binary_PFA_l(t,p)],axis=0)\n    pfas = tf.concat([tf.ones((1,)) ,pfas],axis=0)\n    binSizes = -(pfas[1:]-pfas[:-1])\n    s = ptas*binSizes\n    return 1-K.sum(s, axis=0)\n\ndef brian_loss(t,p):\n    return (wbce(t,p) + f1_loss(t,p)) /2\n#     return auc_loss(t,p) + binary_crossentropy(t,p)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"3da003f55662cc4947f946bb5382b9c514146bb1"},"cell_type":"code","source":"BATCH_SIZE=50000\nEPOCHS=500\nFOLDS = 5\n\n# get the train data\nX = train_df[feature_columns].values\ny = np.array(train_df['target'].values)\n\n#k folds, saving best validation\nsplits = list(StratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=2).split(X, y))\nfor idx, (train_idx, val_idx) in enumerate(splits):\n    print(\"Beginning fold {}\".format(idx+1))\n    \n    train_X, train_y, val_X, val_y = X[train_idx], y[train_idx], X[val_idx], y[val_idx]\n\n    clear_session(model)\n    model = build_model()\n        \n    for i, layer in enumerate(model.layers):\n        model.layers[i].trainable = True\n\n\n    opt = Adam(5e-3)\n    metrics = [\"acc\",auc,wbce,f1]\n    model.compile(optimizer=opt, loss=wbce, metrics=metrics)\n    results = model.fit(train_X,train_y, \n                            epochs = EPOCHS, \n                            callbacks = [\n                                ReduceLROnPlateau(factor=0.4, patience=40, min_lr=1e-6, min_delta=1e-6, verbose=1, mode='min'),\n                                ModelCheckpoint(MODEL_NAME+\"_fold_{}.checkpoint\".format(idx), \n                                    verbose=0, \n                                    monitor='val_auc',\n                                    mode='max',\n                                    save_best_only=True, \n                                    save_weights_only=True\n                                ),\n                                CheckpointReverter(MODEL_NAME+\"_fold_{}.checkpoint\".format(idx),\n                                        verbose=0,\n                                        monitor='val_auc',\n                                        mode='max',\n                                        skip=50\n                                       )\n                            ],\n                            validation_data=[val_X, val_y],\n                            batch_size=BATCH_SIZE,\n                            shuffle=True,\n                            verbose=1\n                            )\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fa985fb6a6b80c90633820c7a34a52d93ae711e2"},"cell_type":"code","source":"preds_test = []\nX = test_df[feature_columns].values\n\nclear_session(model)\nmodel = build_model()\nfor i in range(FOLDS):\n    model.load_weights(MODEL_NAME+\"_fold_{}.checkpoint\".format(i))\n    pred = model.predict(X, batch_size=BATCH_SIZE, verbose=1)\n    preds_test.append(pred)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1d622aa7b67db5f3f8ee4146143d1b4da3dab7d4"},"cell_type":"code","source":"preds = np.mean(np.array(preds_test),axis=0)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"652db10cd8b7946e7d506806d2fc599d56d4aea6"},"cell_type":"code","source":"#TODO, build this better target ends up being a list of one for each row\nsubmission_df = pd.DataFrame(data={'ID_code': list(test_df['ID_code'].values), 'target': list(preds)})\nsubmission_df['target'] = submission_df['target'].apply(lambda x: float(x[0])).astype('float')\nsubmission_df[submission_df['target'] > 0.5].head(10)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"536139bd11c473ccdf7a75bb886bdfd38b1fc884"},"cell_type":"code","source":"#TODO, try different thresholds? No: submit the fractional instead\n# submission_df['target'] = submission_df['target'].apply(lambda x: round(x))\n# submission_df[submission_df['target'] > 0.5].head(10)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9afa1a697930f380ea5646de587a4b3aff92f234"},"cell_type":"code","source":"submission_df.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b50dd116cec8626a7e949d18afa8aedd96ad65b0"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}