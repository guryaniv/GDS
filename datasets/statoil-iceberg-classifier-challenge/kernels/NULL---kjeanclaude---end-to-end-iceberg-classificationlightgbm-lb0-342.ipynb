{"nbformat_minor": 1, "cells": [{"cell_type": "code", "metadata": {}, "execution_count": null, "source": ["# Required libraries\n", "# We will try several Machine Learning platforms\n", "from __future__ import print_function\n", "from builtins import str\n", "from builtins import range\n", "\n", "import os\n", "import sys\n", "import tarfile\n", "\n", "import numpy as np # linear algebra\n", "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n", "from io import BytesIO\n", "\n", "import bson\n", "import json \n", "import skimage\n", "\n", "import matplotlib.pyplot as plt\n", "import keras\n", "import tensorflow as tf\n", "\n", "from sklearn import *\n", "from sklearn.preprocessing import StandardScaler\n", "\n", "from sklearn.model_selection import train_test_split\n", "from sklearn.model_selection import KFold\n", "from sklearn import preprocessing\n", "from sklearn.preprocessing import LabelEncoder\n", "import time\n", "import datetime as dt\n", "\n", "\n", "import lightgbm as lgb\n", "from sklearn.model_selection import GridSearchCV\n", "from sklearn.metrics import log_loss\n", "\n", "# Config the matplotlib backend as plotting inline in IPython\n", "%matplotlib inline\n", "\n", "\n", "from subprocess import check_output\n", "#print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n", "\n", "print(\"tf.__version__ : \", tf.__version__)\n", "print(\"python --version : \", sys.version)\n", "PyVersion = sys.version"], "outputs": []}, {"cell_type": "code", "metadata": {"collapsed": true}, "execution_count": null, "source": [], "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["## 1- Data loading"]}, {"cell_type": "code", "metadata": {"_uuid": "07a5a5782894611e9006ae1b399b0b8fb8a0f06b", "_cell_guid": "52b50086-b405-4598-b11c-97887cdcce8e"}, "execution_count": null, "source": ["# Read data\n", "train = pd.read_json(\"../input/train.json\")\n", "#test = pd.read_json(\"test.json\")\n", "train.inc_angle = train.inc_angle.replace('na', 0)\n", "train.inc_angle = train.inc_angle.astype(float).fillna(0.0)\n", "print(\"Total number of images :\", len(train))\n", "train.head(0)\n", "print(\"done!\")\n", "train[:7]"], "outputs": []}, {"cell_type": "code", "metadata": {}, "execution_count": null, "source": ["# Test data\n", "test = pd.read_json('../input/test.json')\n", "test['inc_angle'] = pd.to_numeric(test['inc_angle'],errors='coerce')\n", "print(\"Total number of images :\", len(test))\n", "test.head(0)\n", "test[:7]"], "outputs": []}, {"cell_type": "code", "metadata": {"collapsed": true}, "execution_count": null, "source": [], "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["## 2- Data Engineering"]}, {"cell_type": "code", "metadata": {"collapsed": true}, "execution_count": null, "source": ["# Train data\n", "def get_stats(train,label=1):\n", "    train['max'+str(label)] = [np.max(np.array(x)) for x in train['band_'+str(label)] ]\n", "    train['maxpos'+str(label)] = [np.argmax(np.array(x)) for x in train['band_'+str(label)] ]\n", "    train['min'+str(label)] = [np.min(np.array(x)) for x in train['band_'+str(label)] ]\n", "    train['minpos'+str(label)] = [np.argmin(np.array(x)) for x in train['band_'+str(label)] ]\n", "    train['med'+str(label)] = [np.median(np.array(x)) for x in train['band_'+str(label)] ]\n", "    train['std'+str(label)] = [np.std(np.array(x)) for x in train['band_'+str(label)] ]\n", "    train['mean'+str(label)] = [np.mean(np.array(x)) for x in train['band_'+str(label)] ]\n", "    train['p25_'+str(label)] = [np.sort(np.array(x))[int(0.25*75*75)] for x in train['band_'+str(label)] ]\n", "    train['p75_'+str(label)] = [np.sort(np.array(x))[int(0.75*75*75)] for x in train['band_'+str(label)] ]\n", "    train['mid50_'+str(label)] = train['p75_'+str(label)]-train['p25_'+str(label)]\n", "\n", "    return train\n", "train = get_stats(train,1)\n", "train = get_stats(train,2)"], "outputs": []}, {"cell_type": "code", "metadata": {}, "execution_count": null, "source": ["train.head(2)"], "outputs": []}, {"cell_type": "code", "metadata": {"collapsed": true}, "execution_count": null, "source": [], "outputs": []}, {"cell_type": "code", "metadata": {"_uuid": "f8f87e54cb74b57d5b8ae5b20c4087dec753d26c", "_cell_guid": "96a221cb-9e07-4942-a7d0-846527fcec50", "collapsed": true}, "execution_count": null, "source": ["col1 = ['min1','max1','std1','med1','mean1','mid50_1']\n", "col2 = ['min2','max2','std2','med2','mean2','mid50_2']\n", "col = [c for c in train.columns if c not in ['id','is_iceberg', 'band_1', 'band_2']]\n", "#col = [c for c in train.columns if c not in ['id','is_iceberg', 'inc_angle', 'band_1', 'band_2']]"], "outputs": []}, {"cell_type": "code", "metadata": {}, "execution_count": null, "source": ["len(col)"], "outputs": []}, {"cell_type": "code", "metadata": {"collapsed": true}, "execution_count": null, "source": [], "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["## 3- Data splitting"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "source": ["# We could try several augmentation methods on the data to see the effect\n", "# Standardize values to 0 mean and unit standard deviation\n", "min_max_scaler = preprocessing.MinMaxScaler()\n", "train_minmax = min_max_scaler.fit_transform(train[col])\n", "\n", "# DATA SPLITING\n", "X_train, X_test, y_train, y_test = train_test_split(train[col], train['is_iceberg'], test_size=0.25, random_state=42)\n", "#X_train, X_test, y_train, y_test = train_test_split(train_minmax, train['is_iceberg'], test_size=0.25, random_state=42)\n", "\n", "X_train = X_train.values.astype(np.float32)\n", "X_test = X_test.values.astype(np.float32)\n", "y_train = y_train.values.astype(np.int)\n", "y_test = y_test.values.astype(np.int)\n", "#xtest = test[col].values.astype(np.float32)\n", "\n", "\n", "n_features = X_train.shape[1]\n", "\n", "n_classes = len(np.unique(y_train))\n", "\n", "print(\"n_features : {}\\nn_classes : {}\\nX_train.shape : {}\".format(n_features, n_classes, X_train.shape))"], "outputs": []}, {"cell_type": "code", "metadata": {}, "execution_count": null, "source": ["X_train.shape"], "outputs": []}, {"cell_type": "code", "metadata": {}, "execution_count": null, "source": ["X_train[1]"], "outputs": []}, {"cell_type": "code", "metadata": {}, "execution_count": null, "source": ["y_train.shape"], "outputs": []}, {"cell_type": "code", "metadata": {"collapsed": true}, "execution_count": null, "source": [], "outputs": []}, {"cell_type": "code", "metadata": {"collapsed": true}, "execution_count": null, "source": [], "outputs": []}, {"cell_type": "markdown", "metadata": {"collapsed": true}, "source": ["## 4- First training  &  Hyperparameter Optimization"]}, {"cell_type": "code", "metadata": {"scrolled": false}, "execution_count": null, "source": ["\n", "print('Start training...')\n", "# train\n", "gbm = lgb.LGBMClassifier(objective='binary',\n", "                        num_leaves=31,\n", "                        learning_rate=0.05,\n", "                        n_estimators=20)\n", "gbm.fit(X_train, y_train,\n", "        eval_set=[(X_test, y_test)],\n", "        eval_metric='binary_logloss',\n", "        early_stopping_rounds=100)\n", "\n", "print('Start predicting...')\n", "# predict\n", "y_pred = gbm.predict(X_test, num_iteration=gbm.best_iteration_)\n", "# eval\n", "print('The log_loss of prediction is:', log_loss(y_test, y_pred))\n", "\n", "# feature importances\n", "print('\\nNumber of features :', len(list(gbm.feature_importances_)))\n", "print('Features :', col)\n", "print('Importances :', list(gbm.feature_importances_))\n", "print('\\nFeature importances :', dict(zip(col,list(gbm.feature_importances_))))\n", "\n", "\n", "### ### Hyperparameter Optimization ##############\n", "# other scikit-learn modules\n", "estimator = lgb.LGBMClassifier(num_leaves=31)\n", "\n", "# The parameters used are in comment below, it will take too long time to run them here\n", "param_grid = {\n", "    'learning_rate': [0.1],\n", "    'n_estimators': [100, 500],\n", "    'num_leaves': [20, 31],\n", "    'min_data_in_leaf': [5, 10],\n", "    'reg_alpha': [0],\n", "    'reg_lambda': [1e-6], \n", "    'bagging_fraction': [0.8, 0.9],\n", "    'min_child_samples': [10, 20],\n", "    'min_child_weight': [1e-6], \n", "    'max_bin': [256]\n", "}\n", "\n", "gbm = GridSearchCV(estimator, param_grid)\n", "\n", "gbm.fit(X_train, y_train)\n", "\n", "print('\\n\\nBest parameters found by grid search are:', gbm.best_params_)\n", "\n", "'''\n", "param_grid = {\n", "    'learning_rate': [0.01, 0.1, 0.05, 0.07, 1],\n", "    'n_estimators': [20, 40, 100, 500],\n", "    'num_leaves': [20, 31, 50, 127],\n", "    'min_data_in_leaf': [5, 10, 20, 50, 100],\n", "    'reg_alpha': [0, 1e-3, 1e-6],\n", "    'reg_lambda': [0, 1e-3, 1e-6], \n", "    'bagging_fraction': [0.5, 0.6, 0.7, 0.8, 0.9],\n", "    'min_child_samples': [10, 20, 30],\n", "    'min_child_weight': [5, 1e-3, 1e-6], \n", "    'max_bin': [255, 256]\n", "}\n", "'''\n", "#"], "outputs": []}, {"cell_type": "code", "metadata": {"collapsed": true}, "execution_count": null, "source": [], "outputs": []}, {"cell_type": "code", "metadata": {"collapsed": true}, "execution_count": null, "source": [], "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["## 5- Fine Tuning  &  Evaluation"]}, {"cell_type": "code", "metadata": {"collapsed": true}, "execution_count": null, "source": ["# FEATURES TUNING IF NECESSARY\n", "## For example, we could remove least important features if required\n", "## And also use the best parameters provided by the grid search Cross Validation"], "outputs": []}, {"cell_type": "code", "metadata": {"collapsed": true}, "execution_count": null, "source": ["# Here I reuse the same previous splits instead of recreate a new one.\n", "X_train = pd.DataFrame(X_train, columns=col)\n", "X_test = pd.DataFrame(X_test, columns=col)"], "outputs": []}, {"cell_type": "code", "metadata": {}, "execution_count": null, "source": ["X_train.head(2)"], "outputs": []}, {"cell_type": "code", "metadata": {}, "execution_count": null, "source": ["X_test.head(2)"], "outputs": []}, {"cell_type": "code", "metadata": {"collapsed": true}, "execution_count": null, "source": [], "outputs": []}, {"cell_type": "code", "metadata": {}, "execution_count": null, "source": ["# I decided here to delete the features with lower importance (7 and 8 values) to see how that could improve the result\n", "# Using the Feature importances Dictionary\n", "new_cols = [c for c in train.columns if c not in ['id','is_iceberg', 'band_1', 'band_2', 'p75_2', 'mean2', 'minpos2']]\n", "len(new_cols)"], "outputs": []}, {"cell_type": "code", "metadata": {}, "execution_count": null, "source": ["X_train_new = X_train[new_cols]\n", "X_test_new = X_test[new_cols]\n", "X_train_new.shape"], "outputs": []}, {"cell_type": "code", "metadata": {"collapsed": true}, "execution_count": null, "source": ["# Come back into arrays for training\n", "X_train_new = X_train_new.values.astype(np.float32)\n", "X_test_new = X_test_new.values.astype(np.float32)"], "outputs": []}, {"cell_type": "code", "metadata": {"collapsed": true}, "execution_count": null, "source": [], "outputs": []}, {"cell_type": "code", "metadata": {"collapsed": true}, "execution_count": null, "source": ["# TRAINING"], "outputs": []}, {"cell_type": "code", "metadata": {"collapsed": true}, "execution_count": null, "source": ["# specify your configurations as a dict\n", "# Use the best parameters provided by the grid search Cross Validation\n", "params = {\"objective\": \"binary\",\n", "          #\"sigmoid\":1.0,\n", "          \"task\": \"train\",\n", "          \"boosting_type\": \"gbdt\",\n", "          \"learning_rate\": 0.1,\n", "          \"num_leaves\": 20, # 31\n", "          \"max_bin\": 256,\n", "          \"min_data_in_leaf\": 5, # Problem  2000\n", "          \"feature_fraction\": 0.6, # 0.6\n", "          \"verbosity\": 0,\n", "          \"seed\": 0,\n", "          \"drop_rate\": 0.1, # 0.1\n", "          \"is_unbalance\": False,\n", "          \"max_drop\": 50,\n", "          \"min_child_samples\": 10,\n", "          \"min_child_weight\": 1e-06, # 5\n", "          \"min_split_gain\": 0,\n", "          \"colsample_bytree\": 0.6343275033,\n", "          \"max_depth\": 8, # 8\n", "          \"n_estimators\": 500, # 500\n", "          \"nthread\": -1,\n", "          \"reg_alpha\": 0,\n", "          \"reg_lambda\": 1e-06,# 1\n", "          \"silent\": True,\n", "          \"subsample_for_bin\": 50000, # 50000\n", "          \"subsample_freq\": 1, # 1\n", "          #\"min_data\":1,\n", "          #\"min_data_in_bin\":1,\n", "          'metric': {'binary_logloss'},\n", "          'bagging_fraction': 0.8,\n", "          'bagging_freq': 5,\n", "          #'num_iterations':1000,\n", "          \"subsample\": 0.733\n", "          }\n"], "outputs": []}, {"cell_type": "code", "metadata": {}, "execution_count": null, "source": ["y_pred[:12]"], "outputs": []}, {"cell_type": "code", "metadata": {"scrolled": false}, "execution_count": null, "source": ["# create dataset for lightgbm\n", "lgb_train = lgb.Dataset(X_train_new, y_train)\n", "lgb_eval = lgb.Dataset(X_test_new, y_test, reference=lgb_train)\n", "\n", "\n", "print('Start training...')\n", "# train\n", "gbm = lgb.train(params,\n", "                lgb_train,\n", "                num_boost_round=2000,\n", "                valid_sets=lgb_eval,\n", "                early_stopping_rounds=100)\n", "\n", "print('Save model...')\n", "# save model to file\n", "gbm.save_model('model.txt')\n", "\n", "print('Start predicting...')\n", "# predict\n", "y_pred = gbm.predict(X_test_new, num_iteration=gbm.best_iteration)\n", "# eval\n", "#print('The rmse of prediction is:', mean_squared_error(y_test, y_pred) ** 0.5)\n", "print('The log_loss of prediction is:', log_loss(y_test, y_pred))"], "outputs": []}, {"cell_type": "code", "metadata": {"collapsed": true}, "execution_count": null, "source": [], "outputs": []}, {"cell_type": "code", "metadata": {"collapsed": true}, "execution_count": null, "source": [], "outputs": []}, {"cell_type": "markdown", "metadata": {"collapsed": true}, "source": ["## 6- SUBMISSION FILE CREATION"]}, {"cell_type": "code", "metadata": {"collapsed": true}, "execution_count": null, "source": ["# Lets ensure first that the test set is under the same preprocessing as the train set (to reproduce the trainer performance).\n", "test = get_stats(test,1)\n", "test = get_stats(test,2)"], "outputs": []}, {"cell_type": "code", "metadata": {"collapsed": true}, "execution_count": null, "source": ["# xtest = min_max_scaler.fit_transform(test[new_cols])\n", "xtest = test[new_cols]\n", "preds = gbm.predict(xtest, num_iteration=gbm.best_iteration)"], "outputs": []}, {"cell_type": "code", "metadata": {}, "execution_count": null, "source": ["preds"], "outputs": []}, {"cell_type": "code", "metadata": {}, "execution_count": null, "source": ["submission = pd.DataFrame({'id': test[\"id\"], 'is_iceberg': preds})\n", "submission.head(10)"], "outputs": []}, {"cell_type": "code", "metadata": {"collapsed": true}, "execution_count": null, "source": ["submission.to_csv(\"./LightGBM_CV_submission.csv\", index=False)"], "outputs": []}, {"cell_type": "code", "metadata": {"collapsed": true}, "execution_count": null, "source": [], "outputs": []}, {"cell_type": "code", "metadata": {}, "execution_count": null, "source": ["from IPython.display import FileLink\n", "#%cd $LESSON_HOME_DIR\n", "FileLink('LightGBM_CV_submission.csv')"], "outputs": []}, {"cell_type": "code", "metadata": {"collapsed": true}, "execution_count": null, "source": [], "outputs": []}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"file_extension": ".py", "codemirror_mode": {"version": 3, "name": "ipython"}, "mimetype": "text/x-python", "nbconvert_exporter": "python", "version": "3.6.3", "name": "python", "pygments_lexer": "ipython3"}}, "nbformat": 4}