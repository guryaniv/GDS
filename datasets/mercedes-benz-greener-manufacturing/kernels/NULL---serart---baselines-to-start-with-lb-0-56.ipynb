{"metadata": {"kernelspec": {"language": "python", "display_name": "Python 3", "name": "python3"}, "language_info": {"pygments_lexer": "ipython3", "name": "python", "file_extension": ".py", "mimetype": "text/x-python", "version": "3.6.1", "nbconvert_exporter": "python", "codemirror_mode": {"name": "ipython", "version": 3}}}, "nbformat": 4, "nbformat_minor": 0, "cells": [{"execution_count": null, "execution_state": "idle", "outputs": [], "metadata": {"_cell_guid": "f56612b7-5e56-a982-e444-2e803d037696", "_uuid": "a78b735faf783956212fc8fe0470f7dd5d7bfe51", "collapsed": false, "trusted": false, "_active": false}, "cell_type": "code", "source": "import warnings\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn import preprocessing\nfrom sklearn.random_projection import GaussianRandomProjection\nfrom sklearn.random_projection import SparseRandomProjection\nfrom sklearn.decomposition import PCA, FastICA\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.feature_selection import SelectFromModel\n\nwarnings.filterwarnings('ignore')\n\nseed = 7\nnp.random.seed(seed)\n\nscale_const = 1"}, {"metadata": {"_uuid": "86796791614c94c9eeb3d62f5e8361ba3655c985", "_active": false, "_cell_guid": "9b0c5167-5916-1295-6c9a-dd1f6398c2ce"}, "outputs": [], "cell_type": "markdown", "source": "### Import", "execution_count": null}, {"execution_count": null, "execution_state": "idle", "outputs": [], "metadata": {"trusted": false, "_uuid": "6fbfcfe44ddaf0f75eb427dcf490096bf86df96d", "_active": true, "_cell_guid": "c1860d05-f575-5b2b-af7e-53478e209504"}, "cell_type": "code", "source": "def load_data(path='../input/'):\n    df_train = pd.read_csv(path.__add__('train.csv'))\n    df_test = pd.read_csv(path.__add__('test.csv'))\n    \n    num_train = len(df_train)\n    \n    id_test = df_test['ID'].values\n    \n    y_train = df_train['y'].values.astype(np.float32)\n    \n    df_train_dummies = pd.get_dummies(df_train, drop_first=True)\n    df_test_dummies = pd.get_dummies(df_test, drop_first=True)\n\n    df_train_dummies = df_train_dummies.drop(['ID','y'], axis=1)\n    df_test_dummies = df_test_dummies.drop('ID', axis=1)\n    \n    df_temp = pd.concat([df_train_dummies, df_test_dummies], join='inner')\n    \n    df_train = df_temp[:num_train]\n    df_test = df_temp[num_train:]\n    \n    add_pca_ica_features(df_train, df_test)\n\n    clf = ExtraTreesRegressor(n_estimators=300, max_depth=4, random_state=seed)\n\n    clf.fit(df_train, y_train)\n\n    features = pd.DataFrame()\n    features['feature'] = df_train.columns\n    features['importance'] = clf.feature_importances_\n    features.sort_values(by=['importance'], ascending=True, inplace=True)\n    features.set_index('feature', inplace=True)\n\n    model = SelectFromModel(clf, prefit=True)\n    train_reduced = model.transform(df_train)   \n\n    test_reduced = model.transform(df_test.copy())\n    \n    df_train = pd.concat([df_train, pd.DataFrame(train_reduced)], axis=1)\n    df_test = pd.concat([df_test, pd.DataFrame(test_reduced)], axis=1)\n        \n    df_all = pd.concat([df_train, df_test])\n    \n    x_train, x_test = df_all.values[:num_train], df_all.values[num_train:]  \n                                   \n    y_train /= scale_const\n    \n    return id_test, x_train, y_train, x_test\n    \n           \ndef add_pca_ica_features(train, test):    \n    n_compute = 10\n\n    # tSVD\n    tsvd = TruncatedSVD(n_components=n_compute, random_state=seed)\n    tsvd_results_train = tsvd.fit_transform(train)\n    tsvd_results_test = tsvd.transform(test)\n\n    # PCA\n    pca = PCA(n_components=n_compute, random_state=seed)\n    pca2_results_train = pca.fit_transform(train)\n    pca2_results_test = pca.transform(test)\n\n    # ICA\n    ica = FastICA(n_components=n_compute, random_state=seed)\n    ica2_results_train = ica.fit_transform(train)\n    ica2_results_test = ica.transform(test)\n\n    # GRP\n    grp = GaussianRandomProjection(n_components=n_compute, eps=0.1, random_state=seed)\n    grp_results_train = grp.fit_transform(train)\n    grp_results_test = grp.transform(test)\n\n# SRP\n    srp = SparseRandomProjection(n_components=n_compute, dense_output=True, random_state=seed)\n    srp_results_train = srp.fit_transform(train)\n    srp_results_test = srp.transform(test)\n\n#save columns list before adding the decomposition components\n\n# Append decomposition components to datasets\n    for i in range(1, n_compute + 1):\n        train['pca_' + str(i)] = pca2_results_train[:, i - 1]\n        test['pca_' + str(i)] = pca2_results_test[:, i - 1]\n        \n        train['ica_' + str(i)] = ica2_results_train[:, i - 1]\n        test['ica_' + str(i)] = ica2_results_test[:, i - 1]\n\n        train['tsvd_' + str(i)] = tsvd_results_train[:, i - 1]\n        test['tsvd_' + str(i)] = tsvd_results_test[:, i - 1]\n\n        train['grp_' + str(i)] = grp_results_train[:, i - 1]\n        test['grp_' + str(i)] = grp_results_test[:, i - 1]\n\n        train['srp_' + str(i)] = srp_results_train[:, i - 1]\n        test['srp_' + str(i)] = srp_results_test[:, i - 1]\n\n\ndef inverse_scale(predict_value):  \n    return scale_const * predict_value\n\nid_test, x_train, y_train, x_test = load_data()"}, {"execution_count": null, "execution_state": "busy", "outputs": [], "metadata": {"_cell_guid": "29b158b3-ce67-f979-31d4-11103dd0b968", "_uuid": "fc1c612936849604ae7e21f4a156d879dcbe1d1c", "collapsed": false, "trusted": false, "_active": false}, "cell_type": "code", "source": " # mmm, xgboost, loved by everyone ^-^\nimport xgboost as xgb\n\n# prepare dict of params for xgboost to run with\nxgb_params = {\n    'n_trees': 1000, \n    'eta': 0.001,\n    'max_depth': 8,\n    'subsample': 0.95,\n    'objective': 'reg:linear',\n    'eval_metric': 'rmse',\n    'base_score': np.mean(y_train), # base prediction = mean(target)\n    'silent': 1\n}\n\n# form DMatrices for Xgboost training\ndtrain = xgb.DMatrix(x_train, y_train)\ndtest = xgb.DMatrix(x_test)\n\n# xgboost, cross-validation\ncv_result = xgb.cv(xgb_params, \n                   dtrain, \n                   num_boost_round=500, # increase to have better results (~700)\n                   early_stopping_rounds=50,\n                   verbose_eval=50, \n                   show_stdv=False\n                  )\n\nnum_boost_rounds = len(cv_result)\nprint(num_boost_rounds)\n\n# train model\nmodel = xgb.train(dict(xgb_params, silent=0), dtrain, num_boost_round=num_boost_rounds)"}, {"execution_count": null, "execution_state": "busy", "outputs": [], "metadata": {"trusted": false, "_uuid": "b4f9646204c68358921f533cf006ac36bd36f277", "_active": false, "_cell_guid": "5f634dac-4ad8-7c09-9603-5fa817140136"}, "cell_type": "code", "source": "from sklearn.metrics import r2_score\n\n# now fixed, correct calculation\nprint(r2_score(dtrain.get_label(), model.predict(dtrain)))"}, {"execution_count": null, "execution_state": "busy", "outputs": [], "metadata": {"_cell_guid": "169562ea-efbe-92b1-6f13-08f34b6597a5", "_uuid": "c417ffbcebbdae053ca7c75d3dcc11a977ff705d", "collapsed": false, "trusted": false, "_active": false}, "cell_type": "code", "source": "# make predictions and save results\ny_pred = inverse_scale(model.predict(dtest))\noutput = pd.DataFrame({'id': id_test, 'y': y_pred.ravel()})\noutput.to_csv('xgboost-depth{}-pca-ica.csv'.format(xgb_params['max_depth']), index=False)"}]}