{"cells": [{"source": ["Uses SRKs wonderful features so don't forget to vote for him!"], "cell_type": "markdown", "metadata": {}}, {"outputs": [], "execution_count": null, "source": ["import numpy as np # linear algebra\n", "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n", "import matplotlib.pyplot as plt\n", "import seaborn as sns\n", "from sklearn.preprocessing import StandardScaler\n", "import nltk\n", "from nltk.corpus import stopwords\n", "import string\n", "import xgboost as xgb\n", "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n", "from sklearn.decomposition import TruncatedSVD\n", "from sklearn import ensemble, metrics, model_selection, naive_bayes\n", "color = sns.color_palette()\n", "\n", "%matplotlib inline\n", "\n", "eng_stopwords = set(stopwords.words(\"english\"))\n", "pd.options.mode.chained_assignment = None"], "cell_type": "code", "metadata": {"_uuid": "d362e082042808a0a79a4ba5d73ecb4d43442489", "collapsed": true, "_cell_guid": "8c77bdad-b474-4265-b008-9e795a0ff33b"}}, {"outputs": [], "execution_count": null, "source": ["def runXGB(train_X, train_y, test_X, test_y=None, test_X2=None, seed_val=0, child=1, colsample=0.3):\n", "    param = {}\n", "    param['objective'] = 'multi:softprob'\n", "    param['eta'] = 0.1\n", "    param['max_depth'] = 3\n", "    param['silent'] = 1\n", "    param['num_class'] = 3\n", "    param['eval_metric'] = \"mlogloss\"\n", "    param['min_child_weight'] = child\n", "    param['subsample'] = 0.8\n", "    param['colsample_bytree'] = colsample\n", "    param['seed'] = seed_val\n", "    num_rounds = 2000\n", "\n", "    plst = list(param.items())\n", "    xgtrain = xgb.DMatrix(train_X, label=train_y)\n", "\n", "    if test_y is not None:\n", "        xgtest = xgb.DMatrix(test_X, label=test_y)\n", "        watchlist = [ (xgtrain,'train'), (xgtest, 'test') ]\n", "        model = xgb.train(plst, xgtrain, num_rounds, watchlist, early_stopping_rounds=50, verbose_eval=20)\n", "    else:\n", "        xgtest = xgb.DMatrix(test_X)\n", "        model = xgb.train(plst, xgtrain, num_rounds)\n", "\n", "    pred_test_y = model.predict(xgtest, ntree_limit = model.best_ntree_limit)\n", "    if test_X2 is not None:\n", "        xgtest2 = xgb.DMatrix(test_X2)\n", "        pred_test_y2 = model.predict(xgtest2, ntree_limit = model.best_ntree_limit)\n", "    return pred_test_y, pred_test_y2, model\n", "\n", "def runMNB(train_X, train_y, test_X, test_y, test_X2):\n", "    model = naive_bayes.MultinomialNB()\n", "    model.fit(train_X, train_y)\n", "    pred_test_y = model.predict_proba(test_X)\n", "    pred_test_y2 = model.predict_proba(test_X2)\n", "    return pred_test_y, pred_test_y2, model"], "cell_type": "code", "metadata": {"collapsed": true}}, {"outputs": [], "execution_count": null, "source": ["train_df = pd.read_csv(\"../input/train.csv\")\n", "test_df = pd.read_csv(\"../input/test.csv\")\n", "## Number of words in the text ##\n", "train_df[\"num_words\"] = train_df[\"text\"].apply(lambda x: len(str(x).split()))\n", "test_df[\"num_words\"] = test_df[\"text\"].apply(lambda x: len(str(x).split()))\n", "\n", "## Number of unique words in the text ##\n", "train_df[\"num_unique_words\"] = train_df[\"text\"].apply(lambda x: len(set(str(x).split())))\n", "test_df[\"num_unique_words\"] = test_df[\"text\"].apply(lambda x: len(set(str(x).split())))\n", "\n", "## Number of characters in the text ##\n", "train_df[\"num_chars\"] = train_df[\"text\"].apply(lambda x: len(str(x)))\n", "test_df[\"num_chars\"] = test_df[\"text\"].apply(lambda x: len(str(x)))\n", "\n", "## Number of stopwords in the text ##\n", "train_df[\"num_stopwords\"] = train_df[\"text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n", "test_df[\"num_stopwords\"] = test_df[\"text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n", "\n", "## Number of punctuations in the text ##\n", "train_df[\"num_punctuations\"] =train_df['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n", "test_df[\"num_punctuations\"] =test_df['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n", "\n", "## Number of title case words in the text ##\n", "train_df[\"num_words_upper\"] = train_df[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n", "test_df[\"num_words_upper\"] = test_df[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n", "\n", "## Number of title case words in the text ##\n", "train_df[\"num_words_title\"] = train_df[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n", "test_df[\"num_words_title\"] = test_df[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n", "\n", "## Average length of the words in the text ##\n", "train_df[\"mean_word_len\"] = train_df[\"text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n", "test_df[\"mean_word_len\"] = test_df[\"text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n", "\n", "author_mapping_dict = {'EAP':0, 'HPL':1, 'MWS':2}\n", "train_y = train_df['author'].map(author_mapping_dict)\n", "train_id = train_df['id'].values\n", "test_id = test_df['id'].values\n", "\n", "### recompute the trauncated variables again ###\n", "train_df[\"num_words\"] = train_df[\"text\"].apply(lambda x: len(str(x).split()))\n", "test_df[\"num_words\"] = test_df[\"text\"].apply(lambda x: len(str(x).split()))\n", "train_df[\"mean_word_len\"] = train_df[\"text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n", "test_df[\"mean_word_len\"] = test_df[\"text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n", "\n", "cols_to_drop = ['id', 'text']\n", "train_X = train_df.drop(cols_to_drop+['author'], axis=1)\n", "test_X = test_df.drop(cols_to_drop, axis=1)\n", "\n", "tfidf_vec = TfidfVectorizer(stop_words='english', ngram_range=(1,3))\n", "full_tfidf = tfidf_vec.fit_transform(train_df['text'].values.tolist() + test_df['text'].values.tolist())\n", "train_tfidf = tfidf_vec.transform(train_df['text'].values.tolist())\n", "test_tfidf = tfidf_vec.transform(test_df['text'].values.tolist())\n", "\n", "n_comp = 20\n", "svd_obj = TruncatedSVD(n_components=n_comp, algorithm='arpack')\n", "svd_obj.fit(full_tfidf)\n", "train_svd = pd.DataFrame(svd_obj.transform(train_tfidf))\n", "test_svd = pd.DataFrame(svd_obj.transform(test_tfidf))\n", "    \n", "train_svd.columns = ['svd_word_'+str(i) for i in range(n_comp)]\n", "test_svd.columns = ['svd_word_'+str(i) for i in range(n_comp)]\n", "train_df = pd.concat([train_df, train_svd], axis=1)\n", "test_df = pd.concat([test_df, test_svd], axis=1)\n", "del full_tfidf, train_tfidf, test_tfidf, train_svd, test_svd\n", "\n", "tfidf_vec = CountVectorizer(stop_words='english', ngram_range=(1,3))\n", "tfidf_vec.fit(train_df['text'].values.tolist() + test_df['text'].values.tolist())\n", "train_tfidf = tfidf_vec.transform(train_df['text'].values.tolist())\n", "test_tfidf = tfidf_vec.transform(test_df['text'].values.tolist())\n", "\n", "cv_scores = []\n", "pred_full_test = 0\n", "pred_train = np.zeros([train_df.shape[0], 3])\n", "kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\n", "for dev_index, val_index in kf.split(train_X):\n", "    dev_X, val_X = train_tfidf[dev_index], train_tfidf[val_index]\n", "    dev_y, val_y = train_y[dev_index], train_y[val_index]\n", "    pred_val_y, pred_test_y, model = runMNB(dev_X, dev_y, val_X, val_y, test_tfidf)\n", "    pred_full_test = pred_full_test + pred_test_y\n", "    pred_train[val_index,:] = pred_val_y\n", "    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n", "print(\"Mean cv score : \", np.mean(cv_scores))\n", "pred_full_test = pred_full_test / 5.\n", "\n", "# add the predictions as new features #\n", "train_df[\"nb_cvec_eap\"] = pred_train[:,0]\n", "train_df[\"nb_cvec_hpl\"] = pred_train[:,1]\n", "train_df[\"nb_cvec_mws\"] = pred_train[:,2]\n", "test_df[\"nb_cvec_eap\"] = pred_full_test[:,0]\n", "test_df[\"nb_cvec_hpl\"] = pred_full_test[:,1]\n", "test_df[\"nb_cvec_mws\"] = pred_full_test[:,2]\n", "\n", "### Fit transform the tfidf vectorizer ###\n", "tfidf_vec = CountVectorizer(ngram_range=(1,7), analyzer='char')\n", "tfidf_vec.fit(train_df['text'].values.tolist() + test_df['text'].values.tolist())\n", "train_tfidf = tfidf_vec.transform(train_df['text'].values.tolist())\n", "test_tfidf = tfidf_vec.transform(test_df['text'].values.tolist())\n", "\n", "cv_scores = []\n", "pred_full_test = 0\n", "pred_train = np.zeros([train_df.shape[0], 3])\n", "kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\n", "for dev_index, val_index in kf.split(train_X):\n", "    dev_X, val_X = train_tfidf[dev_index], train_tfidf[val_index]\n", "    dev_y, val_y = train_y[dev_index], train_y[val_index]\n", "    pred_val_y, pred_test_y, model = runMNB(dev_X, dev_y, val_X, val_y, test_tfidf)\n", "    pred_full_test = pred_full_test + pred_test_y\n", "    pred_train[val_index,:] = pred_val_y\n", "    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n", "print(\"Mean cv score : \", np.mean(cv_scores))\n", "pred_full_test = pred_full_test / 5.\n", "\n", "# add the predictions as new features #\n", "train_df[\"nb_cvec_char_eap\"] = pred_train[:,0]\n", "train_df[\"nb_cvec_char_hpl\"] = pred_train[:,1]\n", "train_df[\"nb_cvec_char_mws\"] = pred_train[:,2]\n", "test_df[\"nb_cvec_char_eap\"] = pred_full_test[:,0]\n", "test_df[\"nb_cvec_char_hpl\"] = pred_full_test[:,1]\n", "test_df[\"nb_cvec_char_mws\"] = pred_full_test[:,2]\n", "\n", "### Fit transform the tfidf vectorizer ###\n", "tfidf_vec = TfidfVectorizer(ngram_range=(1,5), analyzer='char')\n", "tfidf_vec.fit(train_df['text'].values.tolist() + test_df['text'].values.tolist())\n", "train_tfidf = tfidf_vec.transform(train_df['text'].values.tolist())\n", "test_tfidf = tfidf_vec.transform(test_df['text'].values.tolist())\n", "\n", "cv_scores = []\n", "pred_full_test = 0\n", "pred_train = np.zeros([train_df.shape[0], 3])\n", "kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\n", "for dev_index, val_index in kf.split(train_X):\n", "    dev_X, val_X = train_tfidf[dev_index], train_tfidf[val_index]\n", "    dev_y, val_y = train_y[dev_index], train_y[val_index]\n", "    pred_val_y, pred_test_y, model = runMNB(dev_X, dev_y, val_X, val_y, test_tfidf)\n", "    pred_full_test = pred_full_test + pred_test_y\n", "    pred_train[val_index,:] = pred_val_y\n", "    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n", "print(\"Mean cv score : \", np.mean(cv_scores))\n", "pred_full_test = pred_full_test / 5.\n", "\n", "# add the predictions as new features #\n", "train_df[\"nb_tfidf_char_eap\"] = pred_train[:,0]\n", "train_df[\"nb_tfidf_char_hpl\"] = pred_train[:,1]\n", "train_df[\"nb_tfidf_char_mws\"] = pred_train[:,2]\n", "test_df[\"nb_tfidf_char_eap\"] = pred_full_test[:,0]\n", "test_df[\"nb_tfidf_char_hpl\"] = pred_full_test[:,1]\n", "test_df[\"nb_tfidf_char_mws\"] = pred_full_test[:,2]"], "cell_type": "code", "metadata": {}}, {"outputs": [], "execution_count": null, "source": ["cols_to_drop = ['id', 'text']\n", "train_X = train_df.drop(cols_to_drop+['author'], axis=1)\n", "test_X = test_df.drop(cols_to_drop, axis=1)"], "cell_type": "code", "metadata": {"collapsed": true}}, {"outputs": [], "execution_count": null, "source": ["ss = StandardScaler()\n", "ss.fit(pd.concat([train_X,test_X]))\n", "features = train_X.columns\n", "train_X[features] = ss.transform(train_X[features])\n", "test_X[features] = ss.transform(test_X[features])"], "cell_type": "code", "metadata": {"collapsed": true}}, {"outputs": [], "execution_count": null, "source": ["def GPClusterX(data):\n", "    v = pd.DataFrame()\n", "    v[\"0\"] = np.tanh((((data[\"nb_cvec_eap\"] - ((data[\"svd_word_0\"] + data[\"svd_word_12\"]) + data[\"nb_cvec_char_hpl\"])) * 2.0) * 2.0))\n", "    v[\"1\"] = np.tanh(((data[\"nb_tfidf_char_eap\"] + ((((data[\"svd_word_1\"] * 2.0) * 2.0) - data[\"nb_cvec_char_hpl\"]) * 2.0)) * 2.0))\n", "    v[\"2\"] = np.tanh((((data[\"svd_word_1\"] - (data[\"svd_word_0\"] - (data[\"svd_word_17\"] - data[\"nb_cvec_char_hpl\"]))) * 2.0) * 2.0))\n", "    v[\"3\"] = np.tanh(((data[\"nb_tfidf_char_eap\"] - ((data[\"svd_word_0\"] + (data[\"nb_cvec_char_hpl\"] + data[\"svd_word_12\"])) * 2.0)) * 2.0))\n", "    v[\"4\"] = np.tanh((((((data[\"nb_cvec_eap\"] - data[\"svd_word_0\"]) * 2.0) - data[\"nb_cvec_char_hpl\"]) * 2.0) * 2.0))\n", "    v[\"5\"] = np.tanh(((((data[\"nb_cvec_char_eap\"] - data[\"num_words\"]) - (data[\"svd_word_12\"] + data[\"nb_cvec_char_hpl\"])) * 2.0) * 2.0))\n", "    v[\"6\"] = np.tanh((((((data[\"svd_word_1\"] * 2.0) - data[\"nb_cvec_char_hpl\"]) * 2.0) + data[\"svd_word_13\"]) * 2.0))\n", "    v[\"7\"] = np.tanh((((data[\"nb_cvec_char_eap\"] + ((data[\"svd_word_17\"] - data[\"nb_cvec_char_hpl\"]) - data[\"num_stopwords\"])) * 2.0) * 2.0))\n", "    v[\"8\"] = np.tanh(((((data[\"nb_cvec_eap\"] - data[\"svd_word_0\"]) - data[\"svd_word_0\"]) - data[\"nb_cvec_char_hpl\"]) * 2.0))\n", "    v[\"9\"] = np.tanh((((data[\"nb_tfidf_char_eap\"] - (data[\"svd_word_12\"] + data[\"num_unique_words\"])) - data[\"nb_cvec_hpl\"]) * 2.0))\n", "    v[\"10\"] = np.tanh((((((data[\"nb_cvec_eap\"] - data[\"num_words\"]) - data[\"svd_word_0\"]) * 2.0) - data[\"nb_tfidf_char_hpl\"]) * 2.0))\n", "    v[\"11\"] = np.tanh((((((-(data[\"nb_tfidf_char_hpl\"])) - data[\"svd_word_0\"]) - data[\"num_words\"]) * 2.0) - data[\"num_words_title\"]))\n", "    v[\"12\"] = np.tanh(((((data[\"nb_cvec_eap\"] - data[\"nb_cvec_char_hpl\"]) - data[\"num_punctuations\"]) - data[\"svd_word_12\"]) * 2.0))\n", "    v[\"13\"] = np.tanh(((((((data[\"svd_word_1\"] * 2.0) * 2.0) - data[\"num_chars\"]) + data[\"nb_tfidf_char_eap\"]) * 2.0) * 2.0))\n", "    v[\"14\"] = np.tanh((((data[\"svd_word_17\"] - data[\"nb_cvec_char_hpl\"]) + data[\"nb_cvec_char_eap\"]) * 2.0))\n", "    v[\"15\"] = np.tanh((((-(((data[\"svd_word_0\"] + data[\"svd_word_12\"]) + data[\"svd_word_8\"]))) - data[\"nb_cvec_char_hpl\"]) * 2.0))\n", "    v[\"16\"] = np.tanh((((((data[\"nb_tfidf_char_eap\"] - data[\"num_unique_words\"]) - data[\"svd_word_12\"]) - data[\"svd_word_2\"]) * 2.0) * 2.0))\n", "    v[\"17\"] = np.tanh((((data[\"svd_word_17\"] - data[\"svd_word_12\"]) - (data[\"nb_tfidf_char_hpl\"] + data[\"svd_word_0\"])) - data[\"num_unique_words\"]))\n", "    v[\"18\"] = np.tanh(((((data[\"svd_word_10\"] + data[\"nb_cvec_char_eap\"]) - data[\"svd_word_8\"]) - data[\"num_punctuations\"]) - data[\"svd_word_2\"]))\n", "    v[\"19\"] = np.tanh((data[\"svd_word_1\"] - (data[\"svd_word_0\"] + (data[\"num_words_upper\"] + (data[\"svd_word_12\"] + data[\"nb_tfidf_char_hpl\"])))))\n", "    v[\"20\"] = np.tanh(((((data[\"nb_cvec_char_eap\"] - data[\"svd_word_15\"]) - data[\"svd_word_8\"]) - data[\"svd_word_18\"]) * 2.0))\n", "    v[\"21\"] = np.tanh(((((((data[\"svd_word_1\"] * 2.0) * 2.0) - data[\"num_words\"]) - data[\"num_punctuations\"]) * 2.0) * 2.0))\n", "    v[\"22\"] = np.tanh((data[\"nb_cvec_char_eap\"] + (data[\"nb_cvec_eap\"] * ((data[\"num_unique_words\"] * -3.0) - data[\"nb_cvec_char_eap\"]))))\n", "    v[\"23\"] = np.tanh(((data[\"svd_word_17\"] - (data[\"svd_word_0\"] + data[\"nb_tfidf_char_hpl\"])) - (data[\"svd_word_3\"] + data[\"svd_word_8\"])))\n", "    v[\"24\"] = np.tanh((((-((data[\"svd_word_18\"] + (data[\"nb_cvec_char_eap\"] * data[\"num_unique_words\"])))) * 2.0) - data[\"svd_word_8\"]))\n", "    v[\"25\"] = np.tanh(((((-1.0 - (data[\"num_words_title\"] + data[\"num_words\"])) - data[\"svd_word_0\"]) * 2.0) * 2.0))\n", "    v[\"26\"] = np.tanh((((data[\"svd_word_17\"] - ((data[\"svd_word_3\"] + data[\"svd_word_12\"]) + data[\"num_words_upper\"])) * 2.0) * 2.0))\n", "    v[\"27\"] = np.tanh((-((((data[\"svd_word_7\"] * 2.0) * data[\"svd_word_7\"]) + (data[\"svd_word_4\"] + data[\"svd_word_8\"])))))\n", "    v[\"28\"] = np.tanh((((data[\"svd_word_0\"] * data[\"num_unique_words\"]) - (data[\"nb_cvec_mws\"] * data[\"nb_tfidf_char_mws\"])) * 2.0))\n", "    v[\"29\"] = np.tanh(((0.78320163488388062) - (data[\"svd_word_5\"] + (data[\"svd_word_8\"] + (data[\"svd_word_7\"] * data[\"svd_word_7\"])))))\n", "    v[\"30\"] = np.tanh((-1.0 + (data[\"svd_word_0\"] * (((data[\"nb_cvec_char_eap\"] * 2.0) * 2.0) * data[\"num_unique_words\"]))))\n", "    v[\"31\"] = np.tanh(((data[\"nb_cvec_char_eap\"] * ((data[\"svd_word_17\"] - data[\"num_words_upper\"]) - data[\"num_stopwords\"])) - data[\"svd_word_8\"]))\n", "    v[\"32\"] = np.tanh((((data[\"num_stopwords\"] * data[\"svd_word_0\"]) - (data[\"svd_word_7\"] * data[\"svd_word_7\"])) - data[\"svd_word_5\"]))\n", "    v[\"33\"] = np.tanh((((data[\"svd_word_11\"] * (data[\"svd_word_6\"] - data[\"svd_word_11\"])) - data[\"svd_word_9\"]) - data[\"svd_word_6\"]))\n", "    v[\"34\"] = np.tanh(((data[\"svd_word_8\"] * (data[\"nb_tfidf_char_eap\"] - data[\"svd_word_8\"])) + (data[\"num_words_upper\"] * data[\"num_unique_words\"])))\n", "    v[\"35\"] = np.tanh(((((data[\"svd_word_6\"] * data[\"svd_word_19\"]) + data[\"svd_word_7\"])/2.0) - (data[\"svd_word_6\"] * data[\"svd_word_6\"])))\n", "    v[\"36\"] = np.tanh(((((data[\"svd_word_1\"] * data[\"nb_cvec_char_eap\"]) * 2.0) - ((data[\"svd_word_6\"] + data[\"svd_word_9\"])/2.0)) * 2.0))\n", "    v[\"37\"] = np.tanh((((data[\"svd_word_12\"] * (data[\"svd_word_13\"] - data[\"svd_word_12\"])) - data[\"nb_cvec_char_mws\"]) - data[\"svd_word_12\"]))\n", "    v[\"38\"] = np.tanh(((data[\"nb_tfidf_char_eap\"] * data[\"nb_cvec_hpl\"]) - (data[\"svd_word_9\"] + (data[\"svd_word_8\"] - data[\"nb_cvec_hpl\"]))))\n", "    v[\"39\"] = np.tanh((data[\"svd_word_14\"] - (data[\"svd_word_4\"] + ((data[\"svd_word_11\"] * data[\"svd_word_11\"]) + data[\"svd_word_6\"]))))\n", "    v[\"40\"] = np.tanh(((data[\"svd_word_12\"] * data[\"svd_word_4\"]) - (data[\"svd_word_9\"] + (data[\"num_words_upper\"] * data[\"nb_cvec_char_eap\"]))))\n", "    v[\"41\"] = np.tanh(((data[\"svd_word_1\"] * data[\"svd_word_8\"]) - ((data[\"svd_word_2\"] + (data[\"svd_word_6\"] + data[\"svd_word_19\"]))/2.0)))\n", "    v[\"42\"] = np.tanh((data[\"svd_word_7\"] - ((data[\"svd_word_6\"] + (data[\"svd_word_7\"] * 2.0)) * (data[\"svd_word_7\"] * 2.0))))\n", "    v[\"43\"] = np.tanh(((((data[\"num_chars\"] + data[\"svd_word_11\"])/2.0) * (data[\"num_chars\"] - data[\"svd_word_11\"])) - data[\"svd_word_6\"]))\n", "    v[\"44\"] = np.tanh((((data[\"svd_word_11\"] - (data[\"num_unique_words\"] + data[\"num_stopwords\"])) - data[\"svd_word_9\"]) - data[\"nb_cvec_eap\"]))\n", "    v[\"45\"] = np.tanh(((data[\"nb_cvec_char_hpl\"] * (data[\"svd_word_11\"] - (data[\"svd_word_13\"] - data[\"nb_cvec_mws\"]))) - data[\"svd_word_9\"]))\n", "    v[\"46\"] = np.tanh((((data[\"num_unique_words\"] * data[\"nb_cvec_char_eap\"]) * (data[\"num_unique_words\"] - data[\"mean_word_len\"])) - data[\"nb_cvec_eap\"]))\n", "    v[\"47\"] = np.tanh(((((data[\"num_words\"] - (data[\"svd_word_0\"] * data[\"mean_word_len\"])) + data[\"svd_word_11\"])/2.0) - data[\"svd_word_9\"]))\n", "    v[\"48\"] = np.tanh((((data[\"svd_word_0\"] - (data[\"svd_word_0\"] * data[\"nb_cvec_char_hpl\"])) - data[\"svd_word_17\"]) * data[\"num_words\"]))\n", "    v[\"49\"] = np.tanh(((data[\"svd_word_18\"] * data[\"svd_word_11\"]) - (data[\"svd_word_15\"] + (data[\"nb_cvec_char_mws\"] * data[\"nb_cvec_mws\"]))))\n", "\n", "    return v.sum(axis=1)\n", "\n", "\n", "def GPClusterY(data):\n", "    v = pd.DataFrame()\n", "    v[\"0\"] = np.tanh((((data[\"svd_word_12\"] + (data[\"nb_cvec_char_mws\"] * 2.0)) * 2.0) + (data[\"svd_word_7\"] + data[\"svd_word_11\"])))\n", "    v[\"1\"] = np.tanh((((data[\"svd_word_8\"] + ((data[\"nb_cvec_char_mws\"] + data[\"svd_word_2\"]) + data[\"nb_cvec_mws\"])) * 2.0) * 2.0))\n", "    v[\"2\"] = np.tanh((((data[\"nb_cvec_char_mws\"] - data[\"nb_cvec_char_hpl\"]) + (data[\"svd_word_8\"] + data[\"nb_cvec_mws\"])) * 2.0))\n", "    v[\"3\"] = np.tanh((((data[\"svd_word_2\"] + ((data[\"nb_cvec_char_mws\"] + data[\"nb_cvec_mws\"]) - data[\"nb_cvec_char_hpl\"])) * 2.0) * 2.0))\n", "    v[\"4\"] = np.tanh(((((data[\"svd_word_2\"] - (data[\"nb_cvec_char_hpl\"] + data[\"nb_cvec_eap\"])) * 2.0) + data[\"svd_word_12\"]) * 2.0))\n", "    v[\"5\"] = np.tanh(((((data[\"svd_word_3\"] + (data[\"svd_word_8\"] + data[\"nb_cvec_char_mws\"])) + data[\"nb_cvec_char_mws\"]) * 2.0) * 2.0))\n", "    v[\"6\"] = np.tanh(((((data[\"svd_word_2\"] + (data[\"nb_cvec_mws\"] - data[\"nb_cvec_char_hpl\"])) * 2.0) + data[\"svd_word_12\"]) * 2.0))\n", "    v[\"7\"] = np.tanh(((((data[\"num_words_upper\"] + data[\"nb_cvec_char_mws\"]) + (data[\"svd_word_12\"] - data[\"nb_cvec_char_hpl\"])) * 2.0) * 2.0))\n", "    v[\"8\"] = np.tanh((((data[\"num_words_upper\"] + ((-(data[\"nb_cvec_eap\"])) - (data[\"nb_cvec_char_hpl\"] * 2.0))) * 2.0) * 2.0))\n", "    v[\"9\"] = np.tanh((((data[\"nb_cvec_char_mws\"] + (data[\"svd_word_7\"] + (data[\"svd_word_12\"] + data[\"svd_word_2\"]))) * 2.0) * 2.0))\n", "    v[\"10\"] = np.tanh(((((data[\"num_words_upper\"] - (data[\"nb_cvec_eap\"] + (data[\"nb_cvec_char_hpl\"] * 2.0))) * 2.0) * 2.0) * 2.0))\n", "    v[\"11\"] = np.tanh((((data[\"nb_cvec_char_mws\"] + (data[\"svd_word_7\"] + data[\"svd_word_8\"])) * 2.0) * 2.0))\n", "    v[\"12\"] = np.tanh((((data[\"nb_tfidf_char_mws\"] - data[\"nb_cvec_char_hpl\"]) * 2.0) - (data[\"mean_word_len\"] + data[\"nb_cvec_char_hpl\"])))\n", "    v[\"13\"] = np.tanh(((((((data[\"nb_tfidf_char_mws\"] - data[\"nb_cvec_char_hpl\"]) - data[\"svd_word_4\"]) * 2.0) * 2.0) * 2.0) * 2.0))\n", "    v[\"14\"] = np.tanh(((((data[\"svd_word_12\"] - (data[\"svd_word_4\"] + data[\"nb_cvec_char_hpl\"])) * 2.0) - data[\"nb_cvec_eap\"]) * 2.0))\n", "    v[\"15\"] = np.tanh(((1.0 + ((data[\"svd_word_0\"] + (data[\"nb_cvec_char_mws\"] + data[\"svd_word_12\"])) * 2.0)) * 2.0))\n", "    v[\"16\"] = np.tanh(((data[\"svd_word_2\"] * (data[\"nb_cvec_char_hpl\"] * (9.78357696533203125))) - ((data[\"nb_cvec_char_hpl\"] * 2.0) * 2.0)))\n", "    v[\"17\"] = np.tanh((((((data[\"svd_word_3\"] * 2.0) - data[\"nb_cvec_hpl\"]) - data[\"nb_cvec_eap\"]) * 2.0) - data[\"nb_tfidf_char_hpl\"]))\n", "    v[\"18\"] = np.tanh((((data[\"nb_cvec_char_mws\"] + ((data[\"svd_word_8\"] + data[\"svd_word_15\"]) + data[\"svd_word_7\"])) * 2.0) * 2.0))\n", "    v[\"19\"] = np.tanh(((((data[\"svd_word_4\"] * (data[\"nb_cvec_char_hpl\"] * 2.0)) - data[\"nb_cvec_char_hpl\"]) * 2.0) * 2.0))\n", "    v[\"20\"] = np.tanh((((data[\"svd_word_0\"] - ((data[\"svd_word_0\"] * 2.0) * data[\"nb_cvec_char_mws\"])) - data[\"svd_word_17\"]) * 2.0))\n", "    v[\"21\"] = np.tanh(((((data[\"nb_cvec_char_hpl\"] * 2.0) * (data[\"svd_word_0\"] * 2.0)) + 1.0) * 2.0))\n", "    v[\"22\"] = np.tanh((((data[\"nb_tfidf_char_mws\"] + (data[\"svd_word_15\"] - data[\"nb_cvec_char_hpl\"])) * 2.0) + data[\"svd_word_18\"]))\n", "    v[\"23\"] = np.tanh((((((data[\"nb_cvec_char_mws\"] + data[\"svd_word_6\"]) - data[\"svd_word_17\"]) + data[\"svd_word_11\"]) * 2.0) * 2.0))\n", "    v[\"24\"] = np.tanh((((data[\"nb_cvec_char_hpl\"] * 2.0) * ((data[\"nb_cvec_char_hpl\"] * data[\"svd_word_0\"]) * 2.0)) - -3.0))\n", "    v[\"25\"] = np.tanh(((((data[\"num_chars\"] * data[\"nb_cvec_char_hpl\"]) - (data[\"svd_word_0\"] * data[\"nb_cvec_char_mws\"])) * 2.0) * 2.0))\n", "    v[\"26\"] = np.tanh(((data[\"svd_word_14\"] + data[\"num_words_upper\"]) + (data[\"nb_tfidf_char_eap\"] * (data[\"svd_word_17\"] - data[\"nb_cvec_char_mws\"]))))\n", "    v[\"27\"] = np.tanh(((data[\"svd_word_0\"] * data[\"nb_cvec_char_hpl\"]) + (data[\"nb_tfidf_char_mws\"] + (data[\"svd_word_12\"] * data[\"nb_cvec_char_hpl\"]))))\n", "    v[\"28\"] = np.tanh(((((data[\"svd_word_12\"] - data[\"svd_word_7\"]) + data[\"svd_word_4\"]) + data[\"num_unique_words\"]) * data[\"nb_cvec_char_hpl\"]))\n", "    v[\"29\"] = np.tanh((data[\"num_words\"] + (data[\"nb_tfidf_char_mws\"] * (data[\"svd_word_7\"] - (data[\"svd_word_0\"] * 2.0)))))\n", "    v[\"30\"] = np.tanh((((data[\"nb_tfidf_char_mws\"] * data[\"nb_tfidf_char_mws\"]) + (data[\"num_stopwords\"] + data[\"nb_cvec_char_eap\"])) + data[\"svd_word_6\"]))\n", "    v[\"31\"] = np.tanh(((data[\"nb_tfidf_char_mws\"] + data[\"svd_word_18\"]) + ((data[\"svd_word_8\"] + data[\"svd_word_0\"]) - data[\"svd_word_17\"])))\n", "    v[\"32\"] = np.tanh((((data[\"nb_cvec_hpl\"] * 2.0) * 2.0) * ((data[\"svd_word_5\"] + data[\"num_chars\"]) - data[\"svd_word_7\"])))\n", "    v[\"33\"] = np.tanh((((data[\"mean_word_len\"] + data[\"svd_word_15\"]) + (data[\"nb_cvec_char_hpl\"] + data[\"nb_tfidf_char_mws\"])) * data[\"nb_cvec_char_mws\"]))\n", "    v[\"34\"] = np.tanh((((((data[\"num_unique_words\"] * data[\"nb_cvec_char_mws\"]) * data[\"num_words_upper\"]) * 2.0) * 2.0) + data[\"nb_cvec_hpl\"]))\n", "    v[\"35\"] = np.tanh(((data[\"nb_cvec_char_hpl\"] * (-((data[\"svd_word_7\"] - (data[\"num_punctuations\"] - data[\"svd_word_16\"]))))) * 2.0))\n", "    v[\"36\"] = np.tanh(((data[\"nb_cvec_char_eap\"] * data[\"svd_word_17\"]) + ((data[\"svd_word_8\"] - data[\"svd_word_4\"]) - data[\"nb_cvec_char_hpl\"])))\n", "    v[\"37\"] = np.tanh(((np.tanh(data[\"nb_cvec_hpl\"]) - (data[\"nb_cvec_char_mws\"] * (data[\"svd_word_0\"] + data[\"num_unique_words\"]))) * 2.0))\n", "    v[\"38\"] = np.tanh((((-(data[\"svd_word_16\"])) * data[\"nb_cvec_eap\"]) + ((-(data[\"mean_word_len\"])) * data[\"nb_cvec_eap\"])))\n", "    v[\"39\"] = np.tanh((((data[\"nb_cvec_char_hpl\"] * data[\"svd_word_5\"]) + ((data[\"nb_cvec_char_hpl\"] * data[\"svd_word_8\"]) - data[\"nb_cvec_char_hpl\"]))/2.0))\n", "    v[\"40\"] = np.tanh(((((data[\"num_words\"] * data[\"svd_word_0\"]) * data[\"nb_cvec_mws\"]) + ((data[\"svd_word_7\"] + data[\"nb_cvec_hpl\"])/2.0))/2.0))\n", "    v[\"41\"] = np.tanh((((((data[\"svd_word_17\"] / 2.0) * data[\"nb_cvec_char_eap\"]) + (data[\"svd_word_1\"] * data[\"nb_cvec_char_eap\"]))/2.0) * 2.0))\n", "    v[\"42\"] = np.tanh((((data[\"mean_word_len\"] * data[\"num_words_upper\"]) - data[\"nb_cvec_mws\"]) - (data[\"svd_word_18\"] * data[\"nb_cvec_char_mws\"])))\n", "    v[\"43\"] = np.tanh((data[\"nb_cvec_char_hpl\"] * ((data[\"svd_word_5\"] + (data[\"svd_word_8\"] - data[\"nb_cvec_char_hpl\"])) - data[\"svd_word_7\"])))\n", "    v[\"44\"] = np.tanh((((data[\"svd_word_16\"] + np.tanh((data[\"nb_tfidf_char_mws\"] / 2.0)))/2.0) * data[\"nb_tfidf_char_mws\"]))\n", "    v[\"45\"] = np.tanh((data[\"nb_tfidf_char_mws\"] * (data[\"nb_cvec_hpl\"] * data[\"svd_word_0\"])))\n", "    v[\"46\"] = np.tanh((data[\"mean_word_len\"] * (data[\"nb_cvec_char_eap\"] * data[\"svd_word_0\"])))\n", "    v[\"47\"] = np.tanh((((data[\"mean_word_len\"] * data[\"svd_word_0\"]) * (-(data[\"nb_tfidf_char_mws\"]))) - data[\"nb_tfidf_char_mws\"]))\n", "    v[\"48\"] = np.tanh(((data[\"svd_word_14\"] + (data[\"nb_cvec_char_hpl\"] - data[\"num_words_upper\"])) * (data[\"svd_word_12\"] + data[\"svd_word_12\"])))\n", "    v[\"49\"] = np.tanh(((data[\"nb_cvec_char_mws\"] * data[\"num_unique_words\"]) * (((data[\"nb_cvec_mws\"] * data[\"num_words\"]) + data[\"num_words\"])/2.0)))\n", "    return v.sum(axis=1)"], "cell_type": "code", "metadata": {"collapsed": true}}, {"outputs": [], "execution_count": null, "source": ["colors = ['red', 'green','blue']\n", "plt.figure(figsize=(15,15))\n", "plt.scatter(GPClusterX(train_X),GPClusterY(train_X),s=10, color=[colors[o]for o in train_y])"], "cell_type": "code", "metadata": {}}, {"outputs": [], "execution_count": null, "source": [], "cell_type": "code", "metadata": {"collapsed": true}}], "nbformat": 4, "metadata": {"kernelspec": {"name": "python3", "language": "python", "display_name": "Python 3"}, "language_info": {"name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "file_extension": ".py", "mimetype": "text/x-python", "version": "3.6.3", "codemirror_mode": {"name": "ipython", "version": 3}}}, "nbformat_minor": 1}