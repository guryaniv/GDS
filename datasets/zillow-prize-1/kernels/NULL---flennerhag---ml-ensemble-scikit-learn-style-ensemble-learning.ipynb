{"nbformat": 4, "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "_is_fork": false, "language_info": {"codemirror_mode": {"version": 3, "name": "ipython"}, "version": "3.6.4", "mimetype": "text/x-python", "file_extension": ".py", "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "name": "python"}, "_change_revision": 0}, "nbformat_minor": 1, "cells": [{"cell_type": "markdown", "source": ["The following notebook introduces ML-Ensemble, a Python library for memory-efficient parallel ensemble learning with a Scikit-learn API. \n", "\n", "ML-Ensemble also deploys a neural network-like API for building ensembles of several layers, and can accomodate a great variety of ensemble architectures. \n", "\n", "For more information, see [ml-ensemble.com](http://ml-ensemble.com) or visit the [github](https://github.com/flennerhag/mlens) repository."], "metadata": {"_uuid": "0711e8229ade0ae8332d9305f5958f75d82d32b5", "_cell_guid": "c0c2520a-71c8-c705-b362-7844e4b25b79"}}, {"cell_type": "code", "source": ["import gc\n", "import numpy as np\n", "import pandas as pd\n", "\n", "from sklearn.metrics import mean_absolute_error\n", "from sklearn.model_selection import train_test_split\n", "\n", "# Inputs\n", "from xgboost import XGBRegressor\n", "from sklearn.linear_model import Lasso, ElasticNet\n", "from sklearn.ensemble import RandomForestRegressor\n", "from sklearn.preprocessing import StandardScaler\n", "\n", "# Data viz\n", "from mlens.visualization import corr_X_y, corrmat\n", "\n", "# Model evaluation\n", "from mlens.metrics import make_scorer\n", "from mlens.model_selection import Evaluator\n", "\n", "# Ensemble\n", "from mlens.ensemble import SuperLearner\n", "\n", "from scipy.stats import uniform, randint\n", "\n", "from matplotlib.pyplot import show\n", "%matplotlib inline"], "execution_count": null, "metadata": {"_uuid": "6648337953f0f09329acf2036bcf8f1875fc6fa6", "_cell_guid": "1fed8d6f-4c44-b4c7-363b-2ff9e6a594c3"}, "outputs": []}, {"cell_type": "code", "source": ["SEED = 148\n", "np.random.seed(SEED)"], "execution_count": null, "metadata": {"_uuid": "9a8ca77596c1ac87edc5f3db24a4300009170330", "_cell_guid": "0a509d0f-ab3f-5e19-97ad-c88ed75bd393", "collapsed": true}, "outputs": []}, {"cell_type": "markdown", "source": ["# 1. Getting a good baseline for ensemble learning\n", "\n", "It's always good to check how inputs play along with the output.\n", "Here, we highlight one example functionality of the Ml-Ensemble's\n", "visualization library."], "metadata": {"_uuid": "c002f7e728821c091285df6cd316db9145e390e6", "_cell_guid": "082bc8ab-5880-8b96-f57c-1429432be317"}}, {"cell_type": "code", "source": ["def build_train():\n", "    \"\"\"Read in training data and return input, output, columns tuple.\"\"\"\n", "\n", "    # This is a version of Anovas minimally prepared dataset\n", "    # for the xgbstarter script\n", "    # https://www.kaggle.com/anokas/simple-xgboost-starter-0-0655\n", "\n", "    df = pd.read_csv('../input/train_2016_v2.csv')\n", "\n", "    prop = pd.read_csv('../input/properties_2016.csv')\n", "    convert = prop.dtypes == 'float64'\n", "    prop.loc[:, convert] = \\\n", "        prop.loc[:, convert].apply(lambda x: x.astype(np.float32))\n", "\n", "    df = df.merge(prop, how='left', on='parcelid')\n", "\n", "    y = df.logerror\n", "    df = df.drop(['parcelid',\n", "                  'logerror',\n", "                  'transactiondate',\n", "                  'propertyzoningdesc',\n", "                  'taxdelinquencyflag',\n", "                  'propertycountylandusecode'], axis=1)\n", "\n", "    convert = df.dtypes == 'object'\n", "    df.loc[:, convert] = \\\n", "        df.loc[:, convert].apply(lambda x: 1 * (x == True))\n", "\n", "    df.fillna(0, inplace=True)\n", "\n", "    return df, y, df.columns"], "execution_count": null, "metadata": {"_uuid": "47e86754ea1fe98665d213367c5a63e445b8da00", "_cell_guid": "96d981c5-a6a1-1e62-9bad-9aac5197f769", "collapsed": true}, "outputs": []}, {"cell_type": "code", "source": ["xtrain, ytrain, columns = build_train()\n", "xtrain, xtest, ytrain, ytest = train_test_split(\n", "    xtrain, ytrain, test_size=0.5, random_state=SEED)"], "execution_count": null, "metadata": {"_uuid": "f9abed76330ce6cfbaca1a608ba0910374ad18c5", "_cell_guid": "9d067d92-4703-d61b-5432-e9e58def803e"}, "outputs": []}, {"cell_type": "code", "source": ["corr_X_y(xtrain, ytrain, figsize=(16, 10), label_rotation=80, hspace=1, fontsize=14)"], "execution_count": null, "metadata": {"_uuid": "4e20207488c1c2985970342e0afc63ea59aef021", "_cell_guid": "b0e7fe89-12b7-eab5-d270-b2128313afa9"}, "outputs": []}, {"cell_type": "markdown", "source": ["A few features seems to be (first-order) uncorrelated with the output, suggesting estimators with inherent\n", "feature selection should be preferred."], "metadata": {"_uuid": "4866876e67c519a709bc01ae964090c15f6a217e", "_cell_guid": "91c81883-5c34-2ca4-afc1-bf95534bae26"}}, {"cell_type": "markdown", "source": ["Now, consider how set of base learners (estimators) perform as they are."], "metadata": {"_uuid": "c945fa51ccc5a542005e4d86faf88c795593b4e4", "_cell_guid": "278631f6-4a65-dda2-cbf1-82965ea6b893"}}, {"cell_type": "code", "source": ["# We consider the following models (or base learners)\n", "gb = XGBRegressor(n_jobs=1, random_state=SEED)\n", "ls = Lasso(alpha=1e-6, normalize=True)\n", "el = ElasticNet(alpha=1e-6, normalize=True)\n", "rf = RandomForestRegressor(random_state=SEED)\n", "\n", "base_learners = [\n", "    ('ls', ls), ('el', el), ('rf', rf), ('gb', gb)\n", "]"], "execution_count": null, "metadata": {"_uuid": "4766f173ae56fb0a76ee3e5f973a6e99a5d4aebd", "_cell_guid": "299d7916-8391-5a21-db3b-ac5014eb925e", "collapsed": true}, "outputs": []}, {"cell_type": "code", "source": ["P = np.zeros((xtest.shape[0], len(base_learners)))\n", "P = pd.DataFrame(P, columns=[e for e, _ in base_learners])\n", "\n", "for est_name, est in base_learners:\n", "    est.fit(xtrain, ytrain)\n", "    p = est.predict(xtest)\n", "    P.loc[:, est_name] = p\n", "    print(\"%3s : %.4f\" % (est_name, mean_absolute_error(ytest, p)))"], "execution_count": null, "metadata": {"_uuid": "33d6e7e936b3e8f65c3fd4175056ea0c28664fd0", "_cell_guid": "5d46059c-2c3c-b383-dd3f-0b129c4cf1db"}, "outputs": []}, {"cell_type": "markdown", "source": ["So they all score relatively close. However, they seem to capture different aspects of the feature space, as shown by the low correlation of their predictions:"], "metadata": {"_uuid": "87661df1d856fe08a4d1fb9234307f30b99535bb", "_cell_guid": "e180babd-e852-60ec-319d-c704606b4f71"}}, {"cell_type": "code", "source": ["ax = corrmat(P.corr())\n", "show()"], "execution_count": null, "metadata": {"_uuid": "bec7d505a04f83e5b3da5fb37373b057d3a4e339", "_cell_guid": "9ecb9ad7-5d71-0d26-3f6c-5b49ebb15cc6"}, "outputs": []}, {"cell_type": "markdown", "source": ["They are in fact not particularly correlated in their scoring (except the linear models), and hence\n", "an ensemble may be able to outperform any single model by learning to combine their respective strength."], "metadata": {"_uuid": "2f260b72c39f3192886f16071b235dd9dbd55d76", "_cell_guid": "75fc44d9-540a-d669-4f0f-2307fb1387d6"}}, {"cell_type": "markdown", "source": ["## 2. Comparing base learners\n", "\n", "*emphasized text*To facilitate base learner comparison, ML-Ensemble implements a randomized grid search\n", "class that allows specification of several estimators (and preprocessing pipelines) in\n", "one grid search."], "metadata": {"_uuid": "adb08a29e29267201942ece4b188dfd5b71c26c3", "_cell_guid": "a1b054d0-9385-26ad-be38-4f22c89e113a"}}, {"cell_type": "code", "source": ["# Put their parameter dictionaries in a dictionary with the\n", "# estimator names as keys\n", "param_dicts = {\n", "    'ls':\n", "    {'alpha': uniform(1e-6, 1e-5)},\n", "    'el':\n", "    {'alpha': uniform(1e-6, 1e-5),\n", "     'l1_ratio': uniform(0, 1)\n", "    },\n", "    'gb':\n", "    {'learning_rate': uniform(0.02, 0.04),\n", "     'colsample_bytree': uniform(0.55, 0.66),\n", "     'min_child_weight': randint(30, 60),\n", "     'max_depth': randint(3, 7),\n", "     'subsample': uniform(0.4, 0.2),\n", "     'n_estimators': randint(150, 200),\n", "     'colsample_bytree': uniform(0.6, 0.4),\n", "     'reg_lambda': uniform(1, 2),\n", "     'reg_alpha': uniform(1, 2),\n", "    },\n", "    'rf':\n", "    {'max_depth': randint(2, 5),\n", "     'min_samples_split': randint(5, 20),\n", "     'min_samples_leaf': randint(10, 20),\n", "     'n_estimators': randint(50, 100),\n", "     'max_features': uniform(0.6, 0.3)\n", "    }\n", "}"], "execution_count": null, "metadata": {"_uuid": "e6effb52441cc66701c26045a3cc48b7c3a26e60", "_cell_guid": "3f4fbe54-a250-e0fd-fcde-821228da7c18", "collapsed": true}, "outputs": []}, {"cell_type": "code", "source": ["scorer = make_scorer(mean_absolute_error, greater_is_better=False)\n", "\n", "evl = Evaluator(\n", "    scorer,\n", "    cv=2,\n", "    random_state=SEED,\n", "    verbose=5,\n", ")"], "execution_count": null, "metadata": {"_uuid": "f40f0ba8ba707e8064b6dc0a0f766cf921fbd92c", "_cell_guid": "3fd0c5c9-c9fe-2465-6341-9f54d899346f", "collapsed": true}, "outputs": []}, {"cell_type": "code", "source": ["evl.fit(\n", "    xtrain, ytrain,\n", "    estimators=base_learners,\n", "    param_dicts=param_dicts,\n", "    preprocessing={'sc': [StandardScaler()], 'none': []},\n", "    n_iter=2  # bump this up to do a larger grid search\n", ")"], "execution_count": null, "metadata": {"_uuid": "526deea1d7f9f2bf83a82b750b50abcb53801d93", "_cell_guid": "7dfd3238-b31d-6577-0613-0a71abaf95da"}, "outputs": []}, {"cell_type": "code", "source": ["pd.DataFrame(evl.results)"], "execution_count": null, "metadata": {"_uuid": "ec89cd5aedc99437e4388ea06c79384e9aa1c65c", "_cell_guid": "79510c7f-0465-e19f-1c70-6cc3d846f3c2"}, "outputs": []}, {"cell_type": "markdown", "source": ["There you have it, a comparison of tuned models in one grid search!\n", "\n", "Optimal parameters are then easily accessed."], "metadata": {"_uuid": "f26e58260b2fcd5648f5724413754f5b9c273f1d", "_cell_guid": "b1074dc1-70ea-4fee-5746-9d22fab75ffd"}}, {"cell_type": "code", "source": ["evl.results[\"params\"]['sc.gb']"], "execution_count": null, "metadata": {"_uuid": "930e9e92362d8598ae607da502d5872e805e742a", "_cell_guid": "a56f93f3-7f61-e402-dd4e-084e7e542f4d"}, "outputs": []}, {"cell_type": "markdown", "source": ["# 3. Comparing meta learners\n", "\n", "Running an entire ensemble several times just to compare different meta learners can be prohibitvely expensive. ML-Ensemble implements a class that acts as a transformer, allowing you to use ingoing layers as a \"preprocessing\" step, so that you need only evaluate the meta learners iteratively."], "metadata": {"_uuid": "8a10ba8da3b7348213b4cb0eebcc1278c6c6313e", "_cell_guid": "2b8820aa-85fe-7d5f-9697-9a7ac528007c"}}, {"cell_type": "code", "source": ["for case_name, params in evl.results[\"params\"].items():\n", "    case, case_est = case_name.split('.')\n", "    for est_name, est in base_learners:\n", "        if est_name == case_est:\n", "            est.set_params(**params)"], "execution_count": null, "metadata": {"_uuid": "97f39f5f3f0205e7c0c77176e86167fe20348af3", "_cell_guid": "38156447-69dd-9bc5-2f68-e9f1f59aebc0"}, "outputs": []}, {"cell_type": "code", "source": ["# We will compare a GBM and an elastic net as the meta learner\n", "# These are cloned internally so we can go ahead and grab the fitted ones\n", "meta_learners = [\n", "    ('gb', gb), ('el', el)\n", "]\n", "\n", "# Note that when we have a preprocessing pipeline,\n", "# keys are in the (prep_name, est_name) format\n", "param_dicts = {\n", "    'el':\n", "    {'alpha': uniform(1e-5, 1),\n", "     'l1_ratio': uniform(0, 1)\n", "    },\n", "    'gb':\n", "    {'learning_rate': uniform(0.01, 0.2),\n", "     'subsample': uniform(0.5, 0.5),\n", "     'reg_lambda': uniform(0.1, 1),\n", "     'n_estimators': randint(10, 100)\n", "    },\n", "}"], "execution_count": null, "metadata": {"_uuid": "13535ea3d7428624b20bef1e0b377de77fcbc062", "_cell_guid": "7b29ad65-eee3-b75c-7865-10bfbda05b54", "collapsed": true}, "outputs": []}, {"cell_type": "code", "source": ["# Put the layers you don't want to tune into an ensemble with model selection turned on\n", "# Just remember to turn it off when you're done!\n", "in_layer = SuperLearner(model_selection=True)\n", "in_layer.add(base_learners)\n", "\n", "preprocess = [in_layer]"], "execution_count": null, "metadata": {"_uuid": "96fe7196da7423dd060283e448452d7a3f196512", "_cell_guid": "d1230a30-cc93-ffc7-a0e5-2afeacee6577"}, "outputs": []}, {"cell_type": "code", "source": ["evl.fit(\n", "    xtrain, ytrain,\n", "    meta_learners,\n", "    param_dicts,\n", "    preprocessing={'meta': preprocess},\n", "    n_iter=4                            # bump this up to do a larger grid search\n", ")"], "execution_count": null, "metadata": {"_uuid": "c42ab7099d8325d4c90815545660dcc981397962", "_cell_guid": "fdaef90c-8b4e-ec0a-49b1-221ddc026300"}, "outputs": []}, {"cell_type": "code", "source": ["pd.DataFrame(evl.results)"], "execution_count": null, "metadata": {"_uuid": "4cbb073a96269e001dd215e7b7b991b96b72746f", "_cell_guid": "42e4b794-8ecc-4419-abb7-a682c0cf551b"}, "outputs": []}, {"cell_type": "markdown", "source": ["# 4. Ensemble learning\n", "\n", "With these results in mind, we now turn to building an ensemble estimator.\n", "\n", "ML-Ensemble uses a neural network-like API to specify layers of base learners to be\n", "fitted sequentially on the previous layer's predictions (or the raw input for the\n", "first layer). An ensemble is built as a Scikit-learn estimator, and can be used as\n", "any other Scikit-learn class."], "metadata": {"_uuid": "794b8592abb7b5fc35104c164f696617db517ad8", "_cell_guid": "64e69cf0-aee4-c65a-f9ce-d58b370066c9"}}, {"cell_type": "code", "source": ["# Let's pick the linear meta learner with the above tuned\n", "# hyper-parameters. Note that ideally, you'd want to tune\n", "# the ensemble as a whole, not each estimator at a time\n", "meta_learner = meta_learners[1][1]\n", "meta_learner.set_params(**evl.results[\"params\"][\"meta.el\"])\n", "\n", "# We can grab the preprocessing layer and turn model selection off\n", "ens = in_layer\n", "ens.model_selection = False\n", "ens.add_meta(meta_learner)"], "execution_count": null, "metadata": {"_uuid": "d4c76c85f97ba6d5758df8068bf2eddac8c75db5", "_cell_guid": "d7440c1d-f3a5-a001-b4f1-82c20f4229cd"}, "outputs": []}, {"cell_type": "markdown", "source": ["The ensemble we will implement is the Super Learner, also known as a stacking ensemble. There are several alternatives, see the documentation for further info."], "metadata": {"_uuid": "6c8b9d6d2a12f5c4700893698fef88b5325bf24d", "_cell_guid": "1ff3192d-5cd4-b8fa-8b4f-45b6d2bf4cbe"}}, {"cell_type": "markdown", "source": ["Once instantiated, the ensemble will behave like any other Scikit-learn estimator."], "metadata": {"_uuid": "bb61ee8f6f799100bc152afa434975a9155b2f14", "_cell_guid": "43b2ba6d-cf90-3de4-be58-a078fb01a1ee"}}, {"cell_type": "code", "source": ["ens.fit(xtrain, ytrain)"], "execution_count": null, "metadata": {"_uuid": "7c8210cc642b8842b588c349ce0a69aca8a65ab1", "_cell_guid": "f0f797a6-d467-5d5f-23a4-badc954e85ba"}, "outputs": []}, {"cell_type": "markdown", "source": ["Predictions are generated as usual:"], "metadata": {"_uuid": "aec31fb264f325cd0c51b117358365c6c50a9b78", "_cell_guid": "032e87cd-d503-03dd-e5e8-bb466d2a3fa4"}}, {"cell_type": "code", "source": ["pred = ens.predict(xtest)"], "execution_count": null, "metadata": {"_uuid": "5b94791d73be17f279ac50231d257dc1e539fd59", "_cell_guid": "3c64a85a-816c-3954-d5e2-c3b3b818bbc9"}, "outputs": []}, {"cell_type": "code", "source": ["print(\"ensemble score: %.4f\" % mean_absolute_error(ytest, pred))"], "execution_count": null, "metadata": {"_uuid": "066f1a995df166cc6b44dec3606a5a8f0f96f0e5", "_cell_guid": "2513f179-ea59-653e-7b4b-84084e64eaf8"}, "outputs": []}, {"cell_type": "markdown", "source": ["And that's it for this tutorial!\n", "\n", "You might have noticed that the ensemble did not achieve an increase in performance. This is partly due to the lack of proper hyper parameter tuning, but more importantly because the base learners are not sufficiently accurate for there to be anything meaningful for the meta learner to learn from (note that predicting the average gets you about 0.07) \n", "\n", "In these cases, unless the meta learner is underfitting, the ensemble will at least be on par with the best base learner.  Good features are always the primary source of predictive power. Once you have them, combining different estimators in an ensemble is a powerful way of learning as much of the signal in the data as possible.\n", "\n", "If you decide to give ML-Ensemble a try, note that the library is in beta testing so you may run into some unexpected behavior or see opportunities for improvements. Feel free to contribute to the project via the [github](https://github.com/flennerhag/mlens) repository! "], "metadata": {"_uuid": "3c7724e9fe3562b4963b895f0b9c5a5960959d05", "_cell_guid": "3ee96c61-675a-5428-b4b3-19991a8780be"}}]}