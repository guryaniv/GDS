{"nbformat_minor": 1, "cells": [{"source": ["import numpy as np\n", "import pandas as pd\n", "from collections import Counter\n", "from random import shuffle, sample\n", "import re\n", "import itertools\n", "import funcy\n", "import keras\n", "import keras.backend as k\n", "from keras.layers import Dense, GlobalAveragePooling1D, Embedding\n", "from keras.callbacks import EarlyStopping\n", "from keras.models import Sequential\n", "from keras.preprocessing.sequence import pad_sequences\n", "from keras.preprocessing.text import Tokenizer\n", "from keras.utils import to_categorical\n", "from keras.wrappers.scikit_learn import KerasClassifier\n", "import nltk\n", "import nltk.data\n", "from nltk.corpus import stopwords\n", "from nltk.stem.porter import PorterStemmer\n", "from nltk.stem import WordNetLemmatizer\n", "from scipy.stats import mstats\n", "from sklearn.preprocessing import LabelBinarizer, MinMaxScaler, StandardScaler, PolynomialFeatures\n", "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n", "from sklearn.decomposition import PCA\n", "from sklearn.pipeline import Pipeline\n", "from sklearn import metrics\n", "from sklearn.ensemble import RandomForestClassifier\n", "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n", "import matplotlib\n", "import matplotlib.pyplot as plt\n", "from mpl_toolkits.mplot3d import Axes3D\n", "import seaborn as sns\n", "% matplotlib inline"], "metadata": {"_kg_hide-output": true, "_cell_guid": "693dc49d-434f-4b78-9bb5-8f1e22549db6", "_uuid": "eaff806685d44401a789a796af0d53bb97e9b2c5", "_kg_hide-input": true}, "cell_type": "code", "outputs": [], "execution_count": null}, {"source": ["#### In this notebook I am going to try out some feature extraction, model stacking, and word embedding solutions to the Spooky Author Identification competition."], "cell_type": "markdown", "metadata": {"_uuid": "4b5435d171602a88f77ad05daffb476d25351d20", "_cell_guid": "a494e268-22b0-4d12-b0b5-b36e1d8fd04c"}}, {"source": ["### Load the data"], "cell_type": "markdown", "metadata": {"_uuid": "3e7d301d422fcdae5acef9e45db87fc9df80edfe", "_cell_guid": "11c39cf7-5527-46c3-9b97-7a28a3565d75"}}, {"source": ["train = pd.read_csv(\"../input/train.csv\")\n", "test = pd.read_csv(\"../input/test.csv\")"], "metadata": {"collapsed": true, "_uuid": "d3354010db9ea67524538ef02ad4d073ad8c0140", "_cell_guid": "6616e575-751d-4afd-a151-ac3bc3eacfbc"}, "cell_type": "code", "outputs": [], "execution_count": null}, {"source": ["print(train.shape, test.shape)"], "metadata": {"_uuid": "c3ffaea485ff3601153dfca903f18d034547ba77", "_cell_guid": "3a50f71e-b11b-4917-b512-583ccbeb71bd"}, "cell_type": "code", "outputs": [], "execution_count": null}, {"source": ["### Now let's extract some features using some tools from NLTK"], "cell_type": "markdown", "metadata": {"_uuid": "18f4a5681bcbf84b63a628357f0cb3b3f9c24a78", "_cell_guid": "006d94e6-8d78-43b6-85e4-6dc24bcae27d"}}, {"source": ["def content_words(text, target_tags):\n", "    return [x[0] for x in nltk.pos_tag(text)\n", "            if x[-1] in target_tags]\n", "\n", "def extract_features(df):\n", "    punct = re.compile('[\\\\.,\\\\?]')\n", "    df[\"char_count\"] = df[\"text\"].str.len()\n", "    df[\"word_count\"] = df[\"text\"].apply(lambda x: len(x.split()))\n", "    df[\"unique_chars\"] = df[\"text\"].apply(lambda x: len(set([ch for ch in x])))\n", "    df[\"av_word_len\"] = df[\"char_count\"] / df[\"word_count\"]\n", "    df[\"punct\"] = df[\"text\"].apply(lambda x: len(re.findall(punct, x)))\n", "    df[\"punct_per_w\"] = df[\"punct\"] / df[\"word_count\"]\n", "    df[\"content_words\"] = df[\"text\"].apply(lambda x: content_words(x.split(), ['NN', 'NNS', 'JJ', 'RB', \"VBG\", \"VBD\", \"VB\"]))\n", "    df[\"n_content_words\"] = df[\"content_words\"].apply(lambda x: len(x))\n", "    df[\"cw_per_w\"] = df[\"n_content_words\"] / df[\"word_count\"]\n", "    df[\"adjectives\"] = df[\"content_words\"].apply(lambda x: content_words(x, ['JJ', \"RB\"]))\n", "    df[\"n_adj\"] = df[\"adjectives\"].apply(lambda x: len(x))\n", "    df[\"adj_per_w\"] = df[\"n_adj\"] / df[\"word_count\"]\n", "    return df"], "metadata": {"collapsed": true, "_uuid": "9ad206b4472b59a9ebf445ff28c659c1e6203ed7", "_cell_guid": "be084299-f8c4-4667-b2d0-0d5efea139e2"}, "cell_type": "code", "outputs": [], "execution_count": null}, {"source": ["df = train.copy()\n", "df = extract_features(df)"], "metadata": {"collapsed": true, "_uuid": "c93c8c7a0d7ad091bc3adecf57b25eb066e01e93", "_cell_guid": "4270da96-05ef-4f42-9f11-be2b9db77a86"}, "cell_type": "code", "outputs": [], "execution_count": null}, {"source": ["grouped = df.groupby(\"author\")\n", "features = [\"word_count\", \"punct_per_w\", \"av_word_len\", \"char_count\",\n", "            \"n_content_words\", \"cw_per_w\", \"n_adj\", \"adj_per_w\", \"unique_chars\"]\n", "\n", "for feature in features:\n", "    print(feature)\n", "    print(grouped[feature].agg(np.mean), \"\\n\")"], "metadata": {"_uuid": "3bbc8de838335c71eca63b6705189cd9ac40a04d", "_cell_guid": "52fd5b98-8d9b-4e1e-8fa6-15e50484ddcc"}, "cell_type": "code", "outputs": [], "execution_count": null}, {"source": ["### Now we can plot some of the data"], "cell_type": "markdown", "metadata": {"_uuid": "0d38a68d30f84c89d5dcbd50ea773fba2c3d969b", "_cell_guid": "6af4f681-f207-43ec-b3d5-0518d32e8408"}}, {"source": ["dfn = df[features].apply(lambda x: mstats.winsorize(x, limits=[0.05, 0.05]))\n", "dfn.head()"], "metadata": {"_uuid": "f2a61c52eba5f5acdf08d9ae41600c7dfc856cb3", "_cell_guid": "fcbc21c4-7269-469b-ab80-6235f9fc4c93"}, "cell_type": "code", "outputs": [], "execution_count": null}, {"source": ["scaler = StandardScaler()\n", "pca = PCA(n_components=3)\n", "X_reduced = pca.fit_transform(scaler.fit_transform(dfn.values))\n", "d = {'EAP': 0, 'HPL': 1, 'MWS': 2}\n", "df[\"author\"] = df[\"author\"].apply(lambda x: d[x])\n", "y = to_categorical(df[\"author\"].values)\n", "xp = fig = plt.figure(1, figsize=(8, 6))\n", "ax = Axes3D(fig, elev=-150, azim=110)\n", "ax.scatter(X_reduced[:, 0], X_reduced[:, 1], X_reduced[:, 2], c=y,\n", "           cmap=plt.cm.Blues, edgecolor='k', s=40)\n", "ax.set_title(\"First three PCA directions\")\n", "ax.set_xlabel(\"1st eigenvector\")\n", "ax.w_xaxis.set_ticklabels([])\n", "ax.set_ylabel(\"2nd eigenvector\")\n", "ax.w_yaxis.set_ticklabels([])\n", "ax.set_zlabel(\"3rd eigenvector\")\n", "ax.w_zaxis.set_ticklabels([])\n", "plt.show()"], "metadata": {"_uuid": "0da9f98ff8e02c134a066564332107ed2ef5cd96", "_cell_guid": "1bb77e5b-2dc7-4743-8473-d6b6231ac814"}, "cell_type": "code", "outputs": [], "execution_count": null}, {"source": ["#### Not very promising - try adding some polynomial features"], "cell_type": "markdown", "metadata": {"_uuid": "c6705885ae4f1b7f31ffd990370f45e8077617c9", "_cell_guid": "73aabe9c-59fe-41e8-b4b1-cf3ca426718a"}}, {"source": ["pipe = Pipeline(\n", "    steps=[\n", "        (\"scaler\", StandardScaler()),\n", "        (\"poly_fs\", PolynomialFeatures(degree=3)),\n", "        (\"pca\", PCA(n_components=3))\n", "    ])\n", "\n", "X_poly_reduced = pipe.fit_transform(dfn[features].values)\n", "xp = fig = plt.figure(1, figsize=(8, 6))\n", "ax = Axes3D(fig, elev=-150, azim=110)\n", "ax.scatter(X_poly_reduced[:, 0], X_reduced[:, 1], X_reduced[:, 2], c=y,\n", "           cmap=plt.cm.Blues, edgecolor='k', s=40)\n", "ax.set_title(\"First three PCA directions\")\n", "ax.set_xlabel(\"1st eigenvector\")\n", "ax.w_xaxis.set_ticklabels([])\n", "ax.set_ylabel(\"2nd eigenvector\")\n", "ax.w_yaxis.set_ticklabels([])\n", "ax.set_zlabel(\"3rd eigenvector\")\n", "ax.w_zaxis.set_ticklabels([])\n", "plt.show()"], "metadata": {"_uuid": "59d68cf4b8ac1a10bcb5621105b3af0d9932b07d", "_cell_guid": "37bb30d1-b54b-401b-b533-2eb573b89145"}, "cell_type": "code", "outputs": [], "execution_count": null}, {"source": ["### Let's try a simple model on this data. No need to scale data for Random Forest classifier"], "cell_type": "markdown", "metadata": {"_uuid": "9e201b098f900541a13dd30ee0c945990fc400ad", "_cell_guid": "7489cb94-d422-43c1-815c-69a657866f14"}}, {"source": ["df = df.drop([\"id\", \"author\", \"text\", \"content_words\", \"adjectives\"], axis=1)\n", "skf = StratifiedKFold(n_splits=3)"], "metadata": {"collapsed": true, "_uuid": "7266557840c80cfc41c17cab613afd601a881885", "_cell_guid": "1937b096-3667-47b6-a004-da5ea7b6c644"}, "cell_type": "code", "outputs": [], "execution_count": null}, {"source": ["clf = RandomForestClassifier(n_estimators=1000, n_jobs=-1)\n", "X = dfn.values\n", "for tr, te in skf.split(X, np.array([np.argmax(row) for row in y])):\n", "    preds1 = clf.fit(X[tr], y[tr]).predict(X[te])\n", "    print(metrics.log_loss(y[te], preds1))"], "metadata": {"_uuid": "18306cf8f45d59b69e5be9c09fa811cda521273a", "_cell_guid": "189650a9-b138-41cf-9062-ea27c6886798"}, "cell_type": "code", "outputs": [], "execution_count": null}, {"source": ["### Not good, let's try an embedding model instead"], "cell_type": "markdown", "metadata": {"_uuid": "6677e01766b54ca09a182a4e9721f8fc0466c3da", "_cell_guid": "1cff122f-b8d9-42cc-84b4-8fd42a26d56c"}}, {"source": ["df = train.copy()\n", "dfte = test.copy()"], "metadata": {"collapsed": true, "_uuid": "fe78a822bb892931eceab19d0f78605afa6dedcf", "_cell_guid": "bc419c9d-1d69-4ce7-8fc0-59b254d810c6"}, "cell_type": "code", "outputs": [], "execution_count": null}, {"source": ["### Start by defining some pre-processing functions to apply to the text"], "cell_type": "markdown", "metadata": {"_uuid": "3abe6381c96cb162759e304b56568d17725169c3", "_cell_guid": "7696a98c-98c8-49dc-a366-9b2b53fa03ee"}}, {"source": ["def extract_symbols(df):\n", "    \"\"\" separate symbols from words so they are vectorised independently\"\"\"\n", "    print(\"extracting symbols\")\n", "    df2 = df.copy()\n", "    df2[\"text\"] = df[\"text\"].apply(lambda x: \"\".join([\" {} \".format(s)\n", "                                                  if s in [\",\", \".\", \"#\", \"!\", \"'\", \"?\",\n", "                                                           \"\u00a3\", \"$\", \"^\", \"&\", \"*\", \"(\",\n", "                                                          \")\", \"-\", \"+\", \"`\", \":\", \";\"]\n", "                                                  else \"{}\".format(s)\n", "                                                  for s in list(x)])).apply(lambda x: re.sub(\" +\", \" \", x))\n", "    return df2\n", "\n", "def remove_stopwords(df):\n", "    print(\"removing stopwords\")\n", "    sws = set(stopwords.words('english'))\n", "    df2 = df.copy()\n", "    df2[\"text\"] = df[\"text\"].apply(lambda x: \" \".join([w for w in x.split()\n", "                                                       if w.lower() not in sws]))\n", "    return df2\n", "\n", "def _ngrams(text, n):\n", "    words = text.split()\n", "    return (words[i:i + n] for i in range(len(words) - n + 1))\n", "\n", "def make_ngrams(df, n=3):\n", "    print(\"making ngrams\")\n", "    df2 = df.copy()\n", "    df2[\"ngrams\"] = df[\"text\"].apply(lambda x: \" \".join(_ngrams(x, n)))\n", "    return df2\n", "\n", "def stemmer(df):\n", "    print(\"stemming\")\n", "    pst = PorterStemmer()\n", "    df2 = df.copy()\n", "    df2[\"text\"] = df[\"text\"].apply(lambda x: \" \".join([pst.stem(w) for w in x.split()]))\n", "    return df2\n", "\n", "def remove_rare(df, n=3):\n", "    print(\"removing rare words...\")\n", "    # use the Counter class from python collections to calculate word frequencies\n", "    word_counter = Counter(list(itertools.chain(*[w for w in df[\"text\"].apply(lambda x: \n", "                                                                              [t.lower()\n", "                                                                               for t in x.split()])])))\n", "    rare_words = set([w for w in word_counter.keys() if word_counter[w] < n])\n", "    print(\"removing {} words: {}...\".format(len(rare_words), \" \".join(list(rare_words)[:10])))\n", "    df2 = df.copy()\n", "    df2[\"text\"] = df[\"text\"].apply(lambda x: \" \".join([w for w in x.split()\n", "                                                  if w not in rare_words]))\n", "    return df2\n", "\n", "def trim_expand(df, minlen=15, maxlen=256):\n", "    print(\"trimming / expanding\")\n", "    df2 = pd.DataFrame()\n", "    df2[\"text\"] = df[\"text\"].apply(lambda x: x[maxlen:])\n", "    df2[\"author\"] = df[\"author\"]\n", "    dfx = pd.concat([df, df2])\n", "    dfx = dfx[dfx[\"text\"].map(len) > minlen]\n", "    return dfx\n", "\n", "def lower(df):\n", "    df2 = df.copy()\n", "    df2[\"text\"] = df[\"text\"].apply(lambda x: \" \".join([w.lower() for w in x.split()]))\n", "    return df2\n", "\n", "def count_vectoriser(text, v=None):\n", "    print(\"vectorising\")\n", "    if not v:\n", "        v = CountVectorizer(stop_words=None, ngram_range=(1, 4))\n", "        v.fit(text)\n", "    else:\n", "        print(\"using {}\".format(v))\n", "    return v.transform(text), v"], "metadata": {"collapsed": true, "_uuid": "79b96f3a17f532c49fdf8c61a76b8bb0b8d27a3b", "_cell_guid": "33e4d922-5b80-476e-836b-cf50a9223adf"}, "cell_type": "code", "outputs": [], "execution_count": null}, {"source": ["### We can test various combinations of preprocessors by composing the functions"], "cell_type": "markdown", "metadata": {"_uuid": "0899368fee197b9918eed09c6512b6c208238cd0", "_cell_guid": "e44433d2-6f87-4c5a-a642-caa0c725f1d1"}}, {"source": ["# remember that the functions are applied in reverse order\n", "text_prep_train = funcy.compose(remove_rare,\n", "                                #stemmer,\n", "                                #remove_stopwords,\n", "                                #lower,\n", "                                extract_symbols)\n", "\n", "text_prep_test = funcy.compose(#stemmer,\n", "                               #remove_stopwords,\n", "                               #lower,\n", "                               extract_symbols)"], "metadata": {"collapsed": true, "_uuid": "d55f22724188135c3f800de77c76694c7e3ef1a6", "_cell_guid": "7b8c911e-2010-45db-b360-7f4f17ee64a6"}, "cell_type": "code", "outputs": [], "execution_count": null}, {"source": ["dftrain = text_prep_train(df)\n", "dftest = text_prep_test(dfte)"], "metadata": {"_uuid": "3e9fa39504c241668ce97326856ae2cc3020707f", "_cell_guid": "64880384-f042-4918-9dad-b84c747e66ff"}, "cell_type": "code", "outputs": [], "execution_count": null}, {"source": ["print(dftrain.shape, dftest.shape)"], "metadata": {"_uuid": "0bd01b463b6e7c55a5af00e43028a7b7d5426e2a", "_cell_guid": "a9cdcb0a-3378-45dc-bf4e-288bb6162cb6"}, "cell_type": "code", "outputs": [], "execution_count": null}, {"source": ["### Now we will extract the target variable (author), and take a validation set from the training set"], "cell_type": "markdown", "metadata": {"_uuid": "9bb3e4fb61403ec4d89c64e103245a32a5c943a3", "_cell_guid": "c13121d7-ee19-4a63-8e82-f083208f5549"}}, {"source": ["d = {'EAP': 0, 'HPL': 1, 'MWS': 2}\n", "dftrain[\"author\"] = dftrain[\"author\"].apply(lambda x: d[x])\n", "y = to_categorical(dftrain[\"author\"].values)"], "metadata": {"collapsed": true, "_uuid": "f63a715451f5c432a54b6a366a22ab3017a6b412", "_cell_guid": "6d00617e-decf-47c1-87b0-0f8a2a2a05e3"}, "cell_type": "code", "outputs": [], "execution_count": null}, {"source": ["# y is now a one-hot encoding of authors\n", "y[:4]"], "metadata": {"_uuid": "377c155b29c50e9babbdab68db82154d8c746865", "_cell_guid": "973820bb-e31f-4ee7-85b6-90c5e81cdc48"}, "cell_type": "code", "outputs": [], "execution_count": null}, {"source": ["Xtrain, Xval, ytrain, yval = train_test_split(dftrain[\"text\"].values, y)\n", "[a.shape for a in (Xtrain, Xval, ytrain, yval)]"], "metadata": {"_uuid": "d7018370a7bdd41208afed979af505322855f513", "_cell_guid": "6d1e443d-c7ed-48e2-8716-81b6f73b4fa1"}, "cell_type": "code", "outputs": [], "execution_count": null}, {"source": ["### Now tokenise the text"], "cell_type": "markdown", "metadata": {"_uuid": "7bfb3d00db5453143598af35b56600ab2c0662f7", "_cell_guid": "07171c95-6859-4d0d-8afb-c0bb1b6e1a21"}}, {"source": ["tokeniser = Tokenizer()\n", "tokeniser.fit_on_texts(Xtrain)"], "metadata": {"collapsed": true, "_uuid": "0e65e414d04c164ec2bb3fbce4a474eb78feb551", "_cell_guid": "1950a71f-c2ba-4fea-883d-f71aeb6cfd67"}, "cell_type": "code", "outputs": [], "execution_count": null}, {"source": ["print(np.mean([len(t) for t in Xtrain]))\n", "print(np.median([len(t) for t in Xtrain]))\n", "print(np.max([len(t) for t in Xtrain]))\n", "print(len([t for t in Xtrain if len(t) > 300]) / len(Xtrain))\n", "print(np.argmax([len(t) for t in Xtrain]))\n", "# so let's use 300 as the max length"], "metadata": {"_uuid": "1cbf53408b40f0dba4a01505f455d1d4f06cdf97", "_cell_guid": "2c03929f-aed6-42af-9624-13717a308954"}, "cell_type": "code", "outputs": [], "execution_count": null}, {"source": ["def tokenise(x, tokeniser, maxlen=256):\n", "    return pad_sequences(\n", "        sequences=tokeniser.texts_to_sequences(x),\n", "        maxlen=maxlen)"], "metadata": {"collapsed": true, "_uuid": "cf26345409f947da9758febea46c04461cba6fac", "_cell_guid": "20307913-b32b-4864-8b89-4bc36d277361"}, "cell_type": "code", "outputs": [], "execution_count": null}, {"source": ["X_train_tokens, X_val_tokens, X_test_tokens = (tokenise(x, tokeniser)\n", "                                               for x in (Xtrain, Xval, dftest[\"text\"].values))"], "metadata": {"collapsed": true, "_uuid": "073eeded3d5e5df9f07e2dccd125a14f253a9400", "_cell_guid": "d91686b6-ac71-47bf-86c9-2be7224263cb"}, "cell_type": "code", "outputs": [], "execution_count": null}, {"source": ["# this is what the longest phrase looks like after being tokenised;\n", "# shorted passages are padded with leading zeros\n", "longest = np.argmax([len(t) for t in Xtrain])\n", "X_train_tokens[longest]"], "metadata": {"_uuid": "0ddf6f21d3f33cd6fcd453eb57b86bd5746b4861", "_cell_guid": "e29d0851-75a9-4627-9b0b-4f2e2f911280"}, "cell_type": "code", "outputs": [], "execution_count": null}, {"source": ["### Let's try an implementation of FastText (see https://www.kaggle.com/nzw0301/simple-keras-fasttext-val-loss-0-31)"], "cell_type": "markdown", "metadata": {"_uuid": "8495ad37c6b5c52d31e164bf5e5a1d44f341ecb6", "_cell_guid": "5b8f52d0-83b0-48de-8ceb-1d42c6eab4a6"}}, {"source": ["input_dim = np.max(X_train_tokens) + 1\n", "embedding_dims = 15\n", "input_dim"], "metadata": {"_uuid": "861824c33718316affd022aa262736ec0db4f979", "_cell_guid": "3db44546-fd04-4a0c-9c6d-f9700995b535"}, "cell_type": "code", "outputs": [], "execution_count": null}, {"source": ["def build_model(input_dims, embedding_dims=20, optimiser=\"adam\"):\n", "    model = Sequential()\n", "    model.add(Embedding(input_dim=input_dims, output_dim=embedding_dims))\n", "    model.add(GlobalAveragePooling1D())\n", "    model.add(Dense(3, activation=\"softmax\"))\n", "    model.compile(loss=\"categorical_crossentropy\",\n", "                 optimizer=optimiser,\n", "                 metrics=[\"accuracy\"])\n", "    return model"], "metadata": {"collapsed": true, "_uuid": "2711975b96fb56cfc5b2416c41928f019e61a051", "_cell_guid": "aebfa32c-158f-4116-b2e8-b1ff3276fb36"}, "cell_type": "code", "outputs": [], "execution_count": null}, {"source": ["epochs = 50\n", "model = build_model(input_dim, embedding_dims)"], "metadata": {"_uuid": "8b774fff40bbc93948d9456245f4ccf084cd207c", "_cell_guid": "6d3101c8-379d-46fb-8f11-3fce49360b48"}, "cell_type": "code", "outputs": [], "execution_count": null}, {"source": ["data = model.fit(X_train_tokens, ytrain, batch_size=16, validation_data=(X_val_tokens, yval),\n", "                epochs=epochs, callbacks=[EarlyStopping(patience=2, monitor=\"val_loss\")])"], "metadata": {"_uuid": "d360100ae064de39db4dc49eb0b316df235583f3", "_cell_guid": "6b5f6d61-22ff-46dc-8d59-87dff7984f0d"}, "cell_type": "code", "outputs": [], "execution_count": null}, {"source": ["preds = model.predict_proba(X_val_tokens)"], "metadata": {"_uuid": "8af0f1404634baa017617bd6cadafe394d074c25", "_cell_guid": "a41350ed-f78d-4271-9c2c-ce6c94029bda"}, "cell_type": "code", "outputs": [], "execution_count": null}, {"source": ["print(metrics.log_loss(yval, preds))\n", "print(metrics.roc_auc_score(yval, preds))"], "metadata": {"_uuid": "69aac92dd8226e887268cd1c40be6fb47a00c895", "_cell_guid": "f97b734d-1b29-4789-bdcc-b3fef1569a66"}, "cell_type": "code", "outputs": [], "execution_count": null}, {"source": ["### How does this compare to Doc2Vec?"], "cell_type": "markdown", "metadata": {"_uuid": "b2fce3c4b56f846d28722abe89327d880b5029fa", "_cell_guid": "9fe8c3c7-e802-4561-abfd-a6ffee8feac2"}}, {"source": ["# Doc2Vec works with sentences, so first define some functions to split text into sentences\n", "def _tagger(sentence_n):\n", "    sentence, i = sentence_n[1], sentence_n[0]\n", "    return TaggedDocument(sentence.split(), [i])\n", "\n", "def tag_sentences(text, tokenizer):\n", "    tokens = tokenizer.tokenize(text)\n", "    return list(map(_tagger, enumerate(tokens)))"], "metadata": {"collapsed": true, "_uuid": "87350e3e63778ffbc8335dc85e78f46babe3bcc3", "_cell_guid": "762e23d3-684e-4fd5-a8a8-59cf55c07f1e"}, "cell_type": "code", "outputs": [], "execution_count": null}, {"source": ["df = train.copy()\n", "dfte = test.copy()"], "metadata": {"collapsed": true, "_uuid": "f8846c1134b6e2d7a42c78994c633e1d70cecedf", "_cell_guid": "14e342e5-eb82-431a-81ac-c70d6238b27b"}, "cell_type": "code", "outputs": [], "execution_count": null}, {"source": ["Xtrain, Xval, ytrain, yval = train_test_split(dftrain[\"text\"].values, y)\n", "[a.shape for a in (Xtrain, Xval, ytrain, yval)]"], "metadata": {"_uuid": "3b3e7bf7894551377f49e8746b6a89ae3ad92ecd", "_cell_guid": "c665ad77-d9ce-4b79-9b15-e5e83fe65f7d"}, "cell_type": "code", "outputs": [], "execution_count": null}, {"source": ["### first we need to build the model's vocabulary; we will use all of the training data to do that"], "cell_type": "markdown", "metadata": {"_uuid": "33dc32bc6ac0e5044f671a38cb36d7cd0f9dfbc3", "_cell_guid": "7e362766-2799-49db-8ecc-a1fd72ce980d"}}, {"source": ["# use the tagger to tag each phrase\n", "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n", "docs = tag_sentences(\" \".join(itertools.chain(Xtrain)), tokenizer)\n", "print(docs[:5])\n", "len(docs)"], "metadata": {"_uuid": "5a145e2f958c73c36589ce78bd5cefc14809558f", "_cell_guid": "f76dd2f6-f7bd-4d29-a4ee-f2456eef1d10"}, "cell_type": "code", "outputs": [], "execution_count": null}, {"source": ["# now we can build the model and add its vocabulary\n", "model = Doc2Vec(size=100, min_count=3, iter=1, window=8, workers=8)\n", "model.build_vocab(docs)"], "metadata": {"_uuid": "91dfbf16655391cdb8ddbc6ca4c1aeb0df3c24a5", "_cell_guid": "6d9f5bd3-c347-427f-b5b9-111d524ee305"}, "cell_type": "code", "outputs": [], "execution_count": null}, {"source": ["model.corpus_count"], "metadata": {"_uuid": "006b7bc2fb33e298ec3fadfda146df31a928be88", "_cell_guid": "34ea8e2b-2436-4650-bdee-a234a4ac385f"}, "cell_type": "code", "outputs": [], "execution_count": null}, {"source": ["# now we can train the model on the phrases in the training set\n", "n = len(Xtrain)\n", "i = 1\n", "for sentence in Xtrain:\n", "    if i % 500 == 0:\n", "        print(\"trained on {}/{} phrases\".format(i, n))\n", "    doc = tag_sentences(sentence, tokenizer)\n", "    # the loop below allows us to shuffle the words in the sentence after each epoch of training\n", "    for _ in range(25):\n", "        model.train(doc, total_examples=model.corpus_count, epochs=model.iter)\n", "        shuffle(doc)\n", "    i += 1\n", "print(\"done: {}\".format(model))\n", "# save the trained model\n", "with open(\"d2v.raw\", \"wb\") as f:\n", "    model.save(f)"], "metadata": {"_uuid": "439629e1d11d0601c046c14cb970efe0c86a6248", "_cell_guid": "1d7f049f-d99c-4c99-aab4-3fb73f6e3a29"}, "cell_type": "code", "outputs": [], "execution_count": null}, {"source": ["model = Doc2Vec.load(\"d2v.raw\")\n", "model.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\n", "model.wv.similar_by_vector(\"big\")"], "metadata": {"_uuid": "25984b3295a40f70cdb3c2301726be98b412cb00", "_cell_guid": "3360eeea-5783-42ee-96f2-c11301963b13"}, "cell_type": "code", "outputs": [], "execution_count": null}, {"source": ["# the model is now trained; we can use it to encode the phrases into vectors\n", "vecs_tr = np.array([model.infer_vector(phrase) for phrase in Xtrain])\n", "vecs_te = np.array([model.infer_vector(phrase) for phrase in Xval])\n", "print(vecs_tr.shape, vecs_te.shape)"], "metadata": {"_uuid": "b9e81ea0b6c3518c49605c78332efb9439083794", "_cell_guid": "f69e4b3d-ce26-4476-acd3-7b90c8508cbf"}, "cell_type": "code", "outputs": [], "execution_count": null}, {"source": ["print(ytrain.shape, yval.shape)"], "metadata": {"_uuid": "7d9fcd77dcbcdc47b70136a3b91208aa30d70199", "_cell_guid": "a0ff04e0-61bc-426b-abf5-fa0c62a3ac7a"}, "cell_type": "code", "outputs": [], "execution_count": null}, {"source": ["clf = RandomForestClassifier(n_estimators=150, n_jobs=-1)\n", "metrics.log_loss(yval, clf.fit(vecs_tr, ytrain).predict(vecs_te))"], "metadata": {"_uuid": "7519fe4f6296998870f2396bbfff332c54b4452b", "_cell_guid": "ffb70af2-4f84-4d03-9a64-7149086b968b"}, "cell_type": "code", "outputs": [], "execution_count": null}, {"source": ["fig = plt.figure(1, figsize=(8, 6))\n", "ax = Axes3D(fig, elev=-150, azim=110)\n", "X_reduced = PCA(n_components=3).fit_transform(vecs_tr)\n", "ax.scatter(X_reduced[:, 0], X_reduced[:, 1], X_reduced[:, 2], c=ytrain,\n", "           cmap=plt.cm.Blues, edgecolor='k', s=40)\n", "ax.set_title(\"First three PCA directions\")\n", "ax.set_xlabel(\"1st eigenvector\")\n", "ax.w_xaxis.set_ticklabels([])\n", "ax.set_ylabel(\"2nd eigenvector\")\n", "ax.w_yaxis.set_ticklabels([])\n", "ax.set_zlabel(\"3rd eigenvector\")\n", "ax.w_zaxis.set_ticklabels([])\n", "plt.show()"], "metadata": {"_uuid": "676120a6a915e9734eebc879be37d073091d9634", "_cell_guid": "4af338e4-7ca6-4e43-8b5e-6910074c20e8"}, "cell_type": "code", "outputs": [], "execution_count": null}, {"source": [], "metadata": {"collapsed": true, "_uuid": "e36d33a3acdf939962af6971fa3e4e9783f56488", "_cell_guid": "30f80db8-eaff-4a45-b5ae-2d85e87076a1"}, "cell_type": "code", "outputs": [], "execution_count": null}], "metadata": {"kernelspec": {"language": "python", "name": "python3", "display_name": "Python 3"}, "language_info": {"file_extension": ".py", "version": "3.6.3", "nbconvert_exporter": "python", "mimetype": "text/x-python", "name": "python", "pygments_lexer": "ipython3", "codemirror_mode": {"version": 3, "name": "ipython"}}}, "nbformat": 4}