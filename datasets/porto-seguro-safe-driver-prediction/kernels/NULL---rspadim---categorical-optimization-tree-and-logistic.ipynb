{"nbformat": 4, "cells": [{"cell_type": "markdown", "metadata": {"_uuid": "eb03d8e044458b5bdd864ee7a9d41db68cf00c96", "_cell_guid": "044726cd-d497-41b0-b8d7-0d3aa12b27b6"}, "source": ["Motivation:\n", "\n", "from https://www.kaggle.com/rspadim/categorical-variables-feature-tree-optimization/ comment by Nooh:\n", " \n", ">https://www.kaggle.com/rspadim/categorical-variables-feature-tree-optimization/comments#233465\n", ">\"Can you guide a little as what exactly this technique does? Its kinda new to me\"\n", "\n", "I will write what I was trying to do, and maybe give some idea to logistic regression optimization (but not sure if this is ok)\n", "\n", "---\n", "\n", "Consider you have two small dataset:\n", "\n", "    X=[1,2,3,4,5]\n", "    Y=[0,0,1,0,0]\n", "\n", "The problem is: **create a decision tree to reproduce the X->Y relation, consider that tree can only use >= operator, and have max_depth=infinity.**\n", "\n", "The most probable tree is:\n", "\n", "    if(x>=3): #first depth\n", "        if(x>=4): #second depth\n", "            y=0\n", "        else:     #second depth\n", "            y=1\n", "    else:     #first depth\n", "        y=0\n", "\n", "the problem now is: **create the decision tree with max_depth=1, and classification error=0 (overfit it)**. \n", "\n", "answer: **it's not possible without reordering X dataset, all tree will have an classification error**. ok... all trees:\n", "\n", "    if(x>=0):\n", "        y=0\n", "    else:\n", "        y=1\n", "\n", "    if(x>=1):\n", "        y=0\n", "    else:\n", "        y=1\n", "\n", "ok, i will stop here, you will never overfit this dataset with only one depth\n"]}, {"cell_type": "markdown", "metadata": {"_uuid": "60f16ad21ca5cf262dc1255806bbd38b8739c772", "_cell_guid": "0f02c303-0372-40b5-9c36-75ca406f2ddd"}, "source": ["in first overfitted tree (depth=2) you have a global if (x>=3), and a second depth with if (x>=4), the only way to do it with only one if is:\n", "\n", "change 'x=3' with 'x=1' or 'x=5'. my algorithm is not optimal (it don't converge fast and don't give the best answer allways), first I do a naive algorithm reordering by X features count (maybe by Y is better, but I did a naive algorithm, =) you can optimize it ) and if it didn't converge to 1 depth, run a permutation of all unique values of X, if X unique values length is too big (more than 6 values \u2243 721 permutations if I'm not wrong) I use a random sampling (with a limit of tries) to try new orders (good luck!!!). \n", "\n", "at each new order of X values you create a new tree, if tree depth is smaller (in this case depth=1) than initial tree (in this case depth=2) save the reorder dictionary.\n", "\n", "\n", "---\n", "\n", "\"naive\" algorithm part:\n", "\n", "    #group by X, count X values, sorting by count(X) asc\n", "    first_try=df.groupby(feature_col)[feature_col].count().sort_values(ascending=True)\n", "    l,values_dict=0,{}\n", "    for i in first_try.index:\n", "        values_dict[values[l]]=i\n", "        l+=1\n", "    model=getModel(classifier,tree_seed)\n", "    model.fit(df[feature_col].replace(values_dict).values.reshape(-1,1),df[target_col])\n", "\n", "what this will do?\n", "\n", "    X[1 -> 1, 2 -> 1, 3->1, 4-> 1, 5->1]\n", "\n", "no optimization here, probably tree depth = 2\n", "\n", "what we could do? permutation since it's a small dataset: [1,2,3,4,5], [1,2,3,5,4], [1,2,5,3,4] ... but we can do better, let's go to first best answer:\n", "\n", "    group by Y:\n", "    Y[0->{1,2,4,5},1->{3}]\n", "\n", "now, reorder X values\n", "0 values will be:\n", "\n", "    1->0, 2->1, 4-> 2, 5-> 3\n", "\n", "1 values will be:\n", "\n", "    3->4\n", "\n", "new dataset is:\n", "\n", "    X=[0,1,2,3,4]\n", "    Y=[0,0,0,0,1]\n", "\n", "create the new tree:\n", "\n", "    if(X>=4):\n", "        y=1\n", "    else:\n", "        y=0\n", "\n", "(i will talk more about it at end of this notebook, it's  just an idea about optimization with unbalanced data)"]}, {"cell_type": "markdown", "metadata": {"_uuid": "9d7a0dd544a8e9d75c0d5015cfe0a07b14b1d500", "_cell_guid": "fc77eb84-fab1-4aa3-8b37-d0b9638c3504"}, "source": ["---\n", "\n", "it's a model complexity minimization algorithm, \"minimize the (tree depth) subject to reordering categorical features\", now we have only 1 if(x>=4)\n", "\n", "now logistic regression...\n", "\n", "the problem is... how this make betters linear models? I'm not sure about my answer, BUT, from our friend @Eric Vos, \"this optimize linear models\", why? ordered data is easier to linear models fit =), when linear values are easy to separe with big weights. \n", "\n", "changing X values the new parameters of model Y=logit(weights), can be very small for Y=0 (X<=3) and very big to Y=1 (X>4), but you will see that X=4 is \"not good\", X=100000000 is \"better\", from logistic regression (wikipedia, but just to explain what happens in logistic classifier parameters):\n", "\n", "ln(pi / (1-pi) = B0 + B1*x1 + ....\n", "\n", "pi = 1/(1+exp(- B0 + B1*x1 + ...)\n", "\n", "y = 1 / (1+exp(-f(X))\n", "\n", "diff(y) = Y(1-y) * df/dX"]}, {"cell_type": "markdown", "metadata": {"_uuid": "1c6f504896039ce079b62d7b282b46d8b6e02844", "_cell_guid": "3d671bbc-d75c-490a-a15a-7619550148fb"}, "source": ["# Let's code =)"]}, {"source": ["import numpy as np\n", "import pandas as pd\n", "from sklearn.linear_model import LogisticRegression\n", "from sklearn import tree\n", "from sklearn.tree import DecisionTreeClassifier\n", "from sklearn.metrics import log_loss\n", "from graphviz import Source\n", "import matplotlib.pyplot as plt"], "cell_type": "code", "outputs": [], "metadata": {"collapsed": true, "_uuid": "07d6a2d75bc35cdf3c5be5b721432e3f4d1de8a8", "_cell_guid": "7d612434-f3a0-47ed-88a0-2b2497d5193e"}, "execution_count": null}, {"cell_type": "markdown", "metadata": {"_uuid": "4b74241e25cd6310be1a18a6beec285db1cb2e4f", "_cell_guid": "7c6df2ca-11bd-41b4-8f37-7423b73c3a25"}, "source": ["# DataSets:"]}, {"cell_type": "markdown", "metadata": {"_uuid": "0ccffe0cca2dac788cbfc6430386719a3ba5bd9a", "_cell_guid": "dc3ee0dc-ef59-44d7-9b89-7d530dbb367e"}, "source": ["I will use 5 datasets to show some things...\n", "\n", "1) is normal unsorted data\n", "\n", "2) sorted X, X=4 Y=1, all others Y=0\n", "\n", "3) one hot encode with only 1 Y=1\n", "\n", "4) one hot encode with 2 Y=1\n", "\n", "5) same from dataset 2 but with X=5 replaced by X=10000000"]}, {"source": ["# FIRST DATA SET\n", "X1=np.array([[1],[2],[3],[4],[5]])\n", "y1=np.array([[0],[0],[1],[0],[0]]) # with bad y=1 at x=3, we will get depth=2\n", "\n", "# REORDERED\n", "X2=np.array([[0],[1],[2],[3],[4]])\n", "y2=np.array([[0],[0],[0],[0],[1]]) # nice x -> y, we will get depth =1\n", "\n", "# TEST ONE HOT ENCODE\n", "X3=np.array([[1,0,0,0,0],[0,1,0,0,0],[0,0,1,0,0],[0,0,0,0,1],[0,0,0,0,1]])\n", "y3=np.array([[0],[0],[1],[0],[0]])\n", "\n", "# TEST ONE HOT ENCODE (x1=1 and x3=1)\n", "X4=np.array([[1,0,0,0,0],[0,1,0,0,0],[0,0,1,0,0],[0,0,0,0,1],[0,0,0,0,1]])\n", "y4=np.array([[1],[0],[1],[0],[0]])\n", "\n", "# TEST LINEAR MODELS:\n", "X5=np.array([[0],[1],[2],[3],[10000000]])  #nice X values, it's good to linear models\n", "y5=np.array([[0],[0],[0],[0],[1]]) # nice x->y"], "cell_type": "code", "outputs": [], "metadata": {"collapsed": true, "_uuid": "9f8e280d2c7351c662086863e83e15b43a8a4b83", "_cell_guid": "c16759c9-cb0d-4ca9-a3a7-bf789a5910b9"}, "execution_count": null}, {"cell_type": "markdown", "metadata": {"_uuid": "3c0cdcc0f6e8272cf0c8b85f51a9dfec03962820", "_cell_guid": "5f6e4bfc-e4a3-485b-81de-23d5dc8124f3"}, "source": ["# First, let's see decision trees:\n"]}, {"source": ["#http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n", "#    I will set some parameters, the main parameter is class_weight\n", "#\n", "#    criterion\n", "#       The function to measure the quality of a split. \n", "#       Supported criteria are \u201cgini\u201d for the Gini impurity and \n", "#       \u201centropy\u201d for the information gain. <- i like this, but let's use gini...\n", "#\n", "#    class_weight\n", "#        The \u201cbalanced\u201d mode uses the values of y to automatically adjust weights \n", "#        inversely proportional to class frequencies in the input data as \n", "#\n", "#        weight= n_samples / (n_classes * np.bincount(y))\n", "#        \n", "#    max_depth\n", "#        The maximum depth of the tree. If None, then nodes are expanded until all \n", "#        leaves are pure or until all leaves contain less than min_samples_split samples.\n", "\n", "model=DecisionTreeClassifier(criterion='gini',class_weight='balanced',max_depth=None)"], "cell_type": "code", "outputs": [], "metadata": {"collapsed": true, "_uuid": "f46ff8564c86cd67615c4e1991416cb09deec4ea", "_cell_guid": "ff285f57-965c-4d4e-a0c9-0c44c19ceeac"}, "execution_count": null}, {"cell_type": "markdown", "metadata": {"_uuid": "bf81d45b3a06ad24f11222f27ea10d3f106a1f1d", "_cell_guid": "31cb9565-b3b0-4909-b43a-22136fdf784d"}, "source": ["# Dataset 1 - default values"]}, {"source": ["# dataset 1\n", "model.fit(X1,y1)\n", "y_hat1=model.predict_proba(X1)[:,1]\n", "loss1 =log_loss(y1,y_hat1)\n", "print(\"dataset1: \")\n", "print('    depth:  ',model.tree_.max_depth)\n", "print('    proba:  ',y_hat1)\n", "print('    logloss:',loss1)\n", "#plot tree :)\n", "Source( tree.export_graphviz(model, out_file=None))"], "cell_type": "code", "outputs": [], "metadata": {"collapsed": true, "_uuid": "f3e29e92014e306f9c59f71fc47222b6ca259285", "_cell_guid": "b92aa342-d4ea-4ceb-a6aa-6f84d8c24b77"}, "execution_count": null}, {"cell_type": "markdown", "metadata": {"_uuid": "4ee57c04cf1fa102e109897ce7f8ec9d6bfd2e63", "_cell_guid": "884e4a64-c068-414a-9c99-18ad86d22fcb"}, "source": ["Here an important note...\n", "\n", "check samples: \n", "\n", "    samples=5\n", "    samples=2, samples=3 -> X values [1,2] , [3,4,5]\n", "    samples=1, samples=2 -> X values [3]   , [4,5]\n", "\n", "that's how tree see X values [1,2,3,4,5] / Y [0,0,1,0,0], it cut Y=1 in 3 leafs (box without arrows \u2243 final values) with 2 depths"]}, {"cell_type": "markdown", "metadata": {"_uuid": "8e27fbf0be79d34e1d565d19e6623f8d96a0cfac", "_cell_guid": "9ec85872-d558-40ea-8c1b-0599b154a059"}, "source": ["# Dataset 2 - ordered X categories"]}, {"source": ["# dataset 2\n", "model.fit(X2,y2)\n", "y_hat2=model.predict_proba(X2)[:,1]\n", "loss2 =log_loss(y2,y_hat2)\n", "print(\"dataset2: \")\n", "print('    depth:  ',model.tree_.max_depth)\n", "print('    proba:  ',y_hat1)\n", "print('    logloss:',loss1)\n", "#plot tree :)\n", "Source( tree.export_graphviz(model, out_file=None))"], "cell_type": "code", "outputs": [], "metadata": {"collapsed": true, "_uuid": "bf8b4bb36a4eb82d2306afefba02c7d4ca0f3043", "_cell_guid": "a758ab3e-7984-4718-a654-666e6ea4cbd2"}, "execution_count": null}, {"cell_type": "markdown", "metadata": {"_uuid": "5c3466431858c8c45ddc1256a3ae4dedca90aa54", "_cell_guid": "174af679-6ad6-4a5a-bf97-39b76b25ede3"}, "source": ["check samples: \n", "\n", "    samples=5\n", "    samples=4, samples=1 -> X values [0,1,2,3] , [4]\n", "\n", "that's how tree see X values [0,1,2,3,4], only one depth, 2 leafs"]}, {"cell_type": "markdown", "metadata": {"_uuid": "7adac21ed687bacc71c63a9b1a9f1c191985f0a8", "_cell_guid": "934bfcdd-5d78-46fd-9b6e-ca26f54f1d72"}, "source": ["# Dataset 5 (big X value)"]}, {"source": ["# dataset 5\n", "model.fit(X5,y5)\n", "y_hat5=model.predict_proba(X5)[:,1]\n", "loss5 =log_loss(y5,y_hat5)\n", "print(\"dataset5: \")\n", "print('    depth:  ',model.tree_.max_depth)\n", "print('    proba:  ',y_hat5)\n", "print('    logloss:',loss5)\n", "#plot tree :)\n", "Source( tree.export_graphviz(model, out_file=None))"], "cell_type": "code", "outputs": [], "metadata": {"collapsed": true, "_uuid": "34f23dab62e8aae5008708ff8968b68327dcb02d", "_cell_guid": "ae81908a-fb53-413c-a901-870b919ba0de"}, "execution_count": null}, {"cell_type": "markdown", "metadata": {"_uuid": "eecb742412e1f543540fa10df66b1d2b25f9a77d", "_cell_guid": "e5708e24-6226-4024-8f59-34d319c9d601"}, "source": ["same here, but X<= 50000000000000000000000000000000000000000000000 :P\n", "\n", "From: https://www.kaggle.com/c/porto-seguro-safe-driver-prediction/discussion/41161#231655\n", ">CPMP\n", ">xgboost is almost insensitive to monotonic feature transformations. What may really change xgboost behavior is a change in the order of values for a given feature.\n", "\n", "that's why @CPMP told it, values transformation in one feature column without mixing others features isn't good to trees, BUT, reordering is, it's right just remember that SIN() COS() functions can reorder it too, be carefull with \"Non Monotonic\" math functions =]"]}, {"cell_type": "markdown", "metadata": {"_uuid": "3fa39bca102178ca033e3aeb90d10c70be790c5c", "_cell_guid": "d4000a80-1d88-4c59-9284-c93fdb30020a"}, "source": ["# Decision tree with one hot encode"]}, {"cell_type": "markdown", "metadata": {"_uuid": "0fda0068b4646e9e37590730449e61d8e6c877ef", "_cell_guid": "793ff44e-e24e-448a-a7c5-b2ceae4923e6"}, "source": ["# Dataset 3 - one Y=1"]}, {"source": ["# dataset 3\n", "print('X=',X3)\n", "print('Y=',y3)\n", "model.fit(X3,y3)\n", "y_hat3=model.predict_proba(X3)[:,1]\n", "loss3 =log_loss(y3,y_hat3)\n", "print(\"dataset3: \")\n", "print('    depth:  ',model.tree_.max_depth)\n", "print('    proba:  ',y_hat3)\n", "print('    logloss:',loss3)\n", "#plot tree :)\n", "Source( tree.export_graphviz(model, out_file=None))\n"], "cell_type": "code", "outputs": [], "metadata": {"collapsed": true, "_uuid": "c9dcd0f7664503a9caaf3cfc660562985781438a", "_cell_guid": "6f70a394-bfc8-43cc-996d-ea42f292afd8"}, "execution_count": null}, {"cell_type": "markdown", "metadata": {"_uuid": "46310e07b2b970d7d98e525a2d19b283a7bf9249", "_cell_guid": "d8c2ad2c-8a03-4d4f-982f-2e7efb9d2444"}, "source": ["# Dataset 4 - two Y=1"]}, {"source": ["# dataset 4\n", "print('X=',X4)\n", "print('Y=',y4)\n", "model.fit(X4,y4)\n", "y_hat4=model.predict_proba(X4)[:,1]\n", "loss4 =log_loss(y4,y_hat4)\n", "print(\"dataset3: \")\n", "print('    depth:  ',model.tree_.max_depth)\n", "print('    proba:  ',y_hat3)\n", "print('    logloss:',loss3)\n", "#plot tree :)\n", "Source( tree.export_graphviz(model, out_file=None))\n"], "cell_type": "code", "outputs": [], "metadata": {"collapsed": true, "_uuid": "b1500d0a3fa88929c303863aac33ac2c4a6dc0da", "_cell_guid": "e1bd5ee1-e190-4ec9-828a-16e4d2047ff9"}, "execution_count": null}, {"cell_type": "markdown", "metadata": {"_uuid": "91a85ca4911a2040724c3dcfd70edb2899f80d4a", "_cell_guid": "08f89407-98c7-4708-a1a7-56b9146872a7"}, "source": ["Check that one hot encode, give depth=2 leafs=3, when we could use max depth =1, leafs=2, with sorted categorical variables\n", "\n", "That's why you should be carefull with OHE and decision trees!"]}, {"cell_type": "markdown", "metadata": {"_uuid": "f0c5d3edad12951140c42b16cdd6480fe91a8c5a", "_cell_guid": "a76b0a44-8932-4777-a2b2-64df2710182d"}, "source": ["let's see if decision tree fit Y values well? it must!"]}, {"source": ["#Y values\n", "plt.title('Dataset 1')\n", "plt.plot(y1,label='Y_true')\n", "plt.plot(y_hat1,label='y_hat1 - log loss:'+str(loss1))\n", "plt.legend()\n", "plt.show()\n", "\n", "plt.title('Dataset 2')\n", "plt.plot(y2,label='Y_true')\n", "plt.plot(y_hat2,label='y_hat2 - log loss:'+str(loss2))\n", "plt.legend()\n", "plt.show()\n", "\n", "plt.title('Dataset 3')\n", "plt.plot(y3,label='Y_true')\n", "plt.plot(y_hat3,label='y_hat3 - log loss:'+str(loss3))\n", "plt.legend()\n", "plt.show()\n", "\n", "plt.title('Dataset 4')\n", "plt.plot(y4,label='Y_true')\n", "plt.plot(y_hat4,label='y_hat4 - log loss:'+str(loss4))\n", "plt.legend()\n", "plt.show()\n", "\n", "plt.title('Dataset 5')\n", "plt.plot(y5,label='Y_true')\n", "plt.plot(y_hat5,label='y_hat5 - log loss:'+str(loss5))\n", "plt.legend()\n", "plt.show()"], "cell_type": "code", "outputs": [], "metadata": {"collapsed": true, "_uuid": "16ef61dd2e03fe0f25ec2a2f4f55da249de4689e", "_cell_guid": "ffdd19ca-e523-47b7-95f0-709fd38cdfae"}, "execution_count": null}, {"cell_type": "markdown", "metadata": {"_uuid": "bd17d301e11e4eb5bd1e674f928087bc78fa7c1b", "_cell_guid": "737cb3d8-7061-4b0b-a41d-663b058ccb0a"}, "source": ["Now, let's see what logistic regression do, I never tryed it before, let's try"]}, {"source": ["#http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n", "model=LogisticRegression()\n", "\n", "# try changing penalty and C hyperparameters :)"], "cell_type": "code", "outputs": [], "metadata": {"collapsed": true, "_uuid": "d7216d1f4ee34a61ffe53cc1cec77eff60156613", "_cell_guid": "bb847ae7-0183-41bd-86da-830176307817"}, "execution_count": null}, {"source": ["model.fit(X1,y1.ravel())\n", "y_hat1=model.predict_proba(X1)[:,1]\n", "loss1 =log_loss(y1,model.predict_proba(X1)[:,1])\n", "print(\"model1: \")\n", "print('    coefs:  ',model.coef_)\n", "print('    y_true: ',y1.ravel())\n", "print('    proba:  ',y_hat1)\n", "print('    logloss:',loss1)\n"], "cell_type": "code", "outputs": [], "metadata": {"collapsed": true, "_uuid": "4c50f68e630dafefe12ae82743b9ef3221a67365", "_cell_guid": "fc47fe92-918d-4020-aa00-dec29f0c6866"}, "execution_count": null}, {"source": ["model.fit(X2,y2.ravel())\n", "y_hat2=model.predict_proba(X2)[:,1]\n", "loss2 =log_loss(y2,model.predict_proba(X2)[:,1])\n", "print(\"model2: \")\n", "print('    coefs:  ',model.coef_)\n", "print('    y_true: ',y2.ravel())\n", "print('    proba:  ',y_hat2)\n", "print('    logloss:',loss2)\n", "\n"], "cell_type": "code", "outputs": [], "metadata": {"collapsed": true, "_uuid": "df72aaa650903aea8f332b36dd742f0906f29611", "_cell_guid": "e760a05d-97a3-497a-9762-433566109209"}, "execution_count": null}, {"source": ["model.fit(X3,y3.ravel())\n", "y_hat3=model.predict_proba(X3)[:,1]\n", "loss3 =log_loss(y3,model.predict_proba(X3)[:,1])\n", "print(\"model3: \")\n", "print('    coefs:  ',model.coef_)\n", "print('    y_true: ',y3.ravel())\n", "print('    proba:  ',y_hat3)\n", "print('    logloss:',loss3)\n", "\n"], "cell_type": "code", "outputs": [], "metadata": {"collapsed": true, "_uuid": "1b7efc2e52201adc2372d8ff77f28928481462f2", "_cell_guid": "0dba2c40-fc80-42c0-a988-9797c9a159f1"}, "execution_count": null}, {"source": ["model.fit(X4,y4.ravel())\n", "y_hat4=model.predict_proba(X4)[:,1]\n", "loss4 =log_loss(y4,model.predict_proba(X4)[:,1])\n", "print(\"model4: \")\n", "print('    coefs:  ',model.coef_)\n", "print('    y_true: ',y4.ravel())\n", "print('    proba:  ',y_hat4)\n", "print('    logloss:',loss4)\n"], "cell_type": "code", "outputs": [], "metadata": {"collapsed": true, "_uuid": "47a41fc8d9366334a012d2517beaedef7e96331e", "_cell_guid": "d50be9de-6866-4bd8-94bd-5170f69ce554"}, "execution_count": null}, {"source": ["model.fit(X5,y5)\n", "y_hat5=model.predict_proba(X5)[:,1]\n", "loss5 =log_loss(y5,model.predict_proba(X5)[:,1])\n", "print(\"model5: \")\n", "print('    coefs:  ',model.coef_)\n", "print('    y_true: ',y5.ravel())\n", "print('    proba:  ',y_hat5)\n", "print('    logloss:',loss5)\n", "\n"], "cell_type": "code", "outputs": [], "metadata": {"collapsed": true, "_uuid": "b25777e88e362c38eea32d7b0b49acfa6424a422", "_cell_guid": "04d4f747-09dc-496e-b140-19b83fffc165"}, "execution_count": null}, {"cell_type": "markdown", "metadata": {"_uuid": "3f8db4a43bf4c7e6e815eb75d888b22a1276461a", "_cell_guid": "e397d5c9-b7fd-4c0d-96a0-36f9753858ee"}, "source": ["Let's see some plots...\n", "\n", "# Dataset 1, 3 have same Y, but 3 have One Hot Encode (nice to logistic regression)"]}, {"source": ["plt.title('Dataset 1,3')\n", "plt.plot(y1,label='Y_true')\n", "plt.plot(y_hat1,label='y_hat1 - log loss:'+str(loss1))\n", "plt.plot(y_hat3,label='y_hat3 - log loss:'+str(loss3))\n", "plt.legend()\n", "plt.show()\n"], "cell_type": "code", "outputs": [], "metadata": {"collapsed": true, "_uuid": "50baaf1fe30afc2d6dd211111624b7953e32784d", "_cell_guid": "685c87cf-72c2-44be-9510-87cc1d68cf12"}, "execution_count": null}, {"cell_type": "markdown", "metadata": {"_uuid": "47ca7fdda654a5cc7cd776d7c5419c6e82f6b7bb", "_cell_guid": "b3591650-17f7-4bc1-ae51-4acf796a3693"}, "source": ["# Dataset 2,5 have same Y too\n", "\n", "but check how a BIG X value helps a lot!"]}, {"source": ["print('X2=',X2.ravel(),'Y2=',y2.ravel())\n", "print('X5=',X5.ravel(),'Y5=',y5.ravel())\n", "plt.title('Dataset 2,5')\n", "plt.plot(y2,label='Y_true')\n", "plt.plot(y_hat2,label='y_hat2 - log loss:'+str(loss2))\n", "plt.plot(y_hat5,label='y_hat5 - log loss:'+str(loss5))\n", "plt.legend()\n", "plt.show()\n"], "cell_type": "code", "outputs": [], "metadata": {"collapsed": true, "scrolled": false, "_uuid": "5b8d7f19e6d35ea6b7d907f7b4e22493d7b4748c", "_cell_guid": "3f7a49ea-84ae-4145-888d-a4037e885b74"}, "execution_count": null}, {"cell_type": "markdown", "metadata": {"_uuid": "58d990f7312918e2c8dfc0b20cfecd3a6afe665f", "_cell_guid": "14901ab1-46f6-4f81-9e31-9c44830ac2bb"}, "source": ["In this case, only sorting features isn't 100%, but we can use small X values to Y=0, and big to Y=1\n", "\n", "for example...  \n", "\n", "with a 1000 unique values X dataset, and from x=1 to x=500 we have y=0, and to x=10000 to x=10500 we have Y=1\n", "\n", "let's try..."]}, {"source": ["X1=list(range(0,1000))\n", "y1=[0]*500 +[1]*500\n", "\n", "X2_10000=list(range(10000,10500))\n", "X2_0    =list(range(0,500))\n", "X2=X2_0+X2_10000\n", "y2=y1\n", "\n", "print('X1=',X1)\n", "print('y1=',y1)\n", "print('\\n\\n-------------------------\\n\\n')\n", "print('X2=',X2)\n", "print('y2=',y2)"], "cell_type": "code", "outputs": [], "metadata": {"collapsed": true, "_uuid": "27e1915eac335d6ad72671787d1bba523854840e", "_cell_guid": "a8137ec2-e0ac-4025-b477-29254f88870a"}, "execution_count": null}, {"cell_type": "markdown", "metadata": {"_uuid": "216829eee0af79e6a6d5ee9261866dda448691e3", "_cell_guid": "8d94fc66-3011-4471-aa08-fab8fd3b9077"}, "source": ["Let's fit it with logistic regression"]}, {"source": ["X1_v=np.vstack(X1)\n", "model.fit(X1_v,y1)\n", "y_hat1=model.predict(X1_v)\n", "loss1 =log_loss(y1,y_hat1)\n", "print(\"model1: \")\n", "print('    coefs:  ',model.coef_)\n", "print('    y_true: ',y1)\n", "print('    proba:  ',y_hat1)\n", "print('    logloss:',loss1)\n", "\n", "X2_v=np.vstack(X2)\n", "model.fit(X2_v,y2)\n", "y_hat2=model.predict(X2_v)\n", "loss2 =log_loss(y2,y_hat2)\n", "print(\"model2: \")\n", "print('    coefs:  ',model.coef_)\n", "print('    y_true: ',y2)\n", "print('    proba:  ',y_hat2)\n", "print('    logloss:',loss2)\n"], "cell_type": "code", "outputs": [], "metadata": {"collapsed": true, "_uuid": "5b709d5e811ad289f1bf268b7868d9145903caa1", "_cell_guid": "1b869989-5bff-498c-81ed-730c37f1b30a"}, "execution_count": null}, {"source": ["plt.title('Dataset with X small and X big')\n", "plt.plot(y1,label='Y_true')\n", "plt.plot(y_hat1,label='y_hat1 - X small - log loss:'+str(loss1),alpha=.8)\n", "plt.plot(y_hat2,label='y_hat2 - X big - log loss:'+str(loss2),alpha=.2)\n", "plt.legend()\n", "plt.show()\n"], "cell_type": "code", "outputs": [], "metadata": {"collapsed": true, "_uuid": "307d9a7386b5bc28f6319787efd12e76f1c8a43e", "_cell_guid": "9a4fae85-9dc7-4023-8cbe-df768bc5efa3"}, "execution_count": null}, {"cell_type": "markdown", "metadata": {"_uuid": "cb29d6760588704cfe3e67bbc4f3d7f639cb76ef", "_cell_guid": "0b77f06a-0735-4f33-820c-58cbee0cd272"}, "source": ["Check that y_hat2 have better log loss!\n", "\n", "I think that's all =) good luck!"]}, {"cell_type": "markdown", "metadata": {"collapsed": true, "_uuid": "3f0e62736fd33d1a7151c21c1a8a59222e8822b1", "_cell_guid": "291673b8-8f11-4629-a312-f5ea1a5ae9cf"}, "source": ["---\n", "let's go back to unbalanced X-Y and optimization idea, just see what happen when try to implement some categorical optimization\n", "\n", "\n", "\n", "i will implement this new idea soon :), but must check problems with unbalanced data, example: X=3 Y->20% = 0, 80% = 1\n", "\n", "    x=[3,3,3,3,3,3,3,4]\n", "    y=[0,1,0,1,0,1,0,1]\n", "\n", "what's the best x values? maybe here an global optimization is better, must use try-and-error feedback... but some naive ideas:\n", "\n", "    x=3 => y=0 p()=3/7, y=1 p()=4/7\n", "    x=4 => y=1 p()=1\n", "\n", "maybe:\n", "\n", "    y=1\n", "\n", "but using sklearn we check this :\n", "\n", "    if(x>=4):\n", "        y=1\n", "    else:\n", "        if(x>=3): (y=0 p()=3/7, y=1 p()=4/7, more probability to y=1 than y=0)\n", "            y=3/7 (i was thinking this could be 1, but not... this should be 4/7 or 3/7, sklearn used 3/7... why? i don't know, but this reduce log loss!)\n", "        else:\n", "            we don't have more data... reduce this if \n", "\n", "reduce everything to\n", "\n", "    if(x>=4):\n", "        y=1\n", "    else:\n", "       y=3/7"]}, {"source": ["model=DecisionTreeClassifier(criterion='gini',class_weight='balanced',max_depth=None)\n", "x=np.vstack([3,3,3,3,3,3,3,4])\n", "y=[0,1,0,1,0,1,0,1]\n", "\n", "# dataset 3\n", "print('X=',x.ravel())\n", "print('Y=',y)\n", "model.fit(x,y)\n", "y_hat_both=model.predict_proba(x)\n", "y_hat_both2=y_hat_both.copy()\n", "y_hat_both2[:,0],y_hat_both2[:,1]=y_hat_both[:,1],y_hat_both[:,0] # inverse\n", "y_hat_both2[7][0]=0\n", "y_hat_both2[7][1]=1\n", "\n", "y_hat    =y_hat_both[:,1]\n", "y_hat_4_7=y_hat_both2[:,1]\n", "loss =log_loss(y,y_hat)\n", "loss1=log_loss(y,y_hat_4_7)\n", "print(\"X-Y: \")\n", "print('    depth:  ',model.tree_.max_depth)\n", "print('    y_true: ',y)\n", "print('    proba:  ',y_hat)\n", "print('    logloss:',loss)\n", "\n", "print(\"\\nWhat happen if use 4/7 instead of 3/7? let's see log loss only, but each metric give something different\")\n", "print('    proba:  ',y_hat_4_7)\n", "print('    logloss:',loss1)\n", "\n", "#plot tree :)\n", "Source( tree.export_graphviz(model, out_file=None))\n"], "cell_type": "code", "outputs": [], "metadata": {"collapsed": true, "_uuid": "6c5862d57ab840e662b29cddfde01942d6a79ee0", "_cell_guid": "88d30d41-5631-4ba6-a485-718406275c30"}, "execution_count": null}, {"source": ["# plot output\n", "plt.title('Unbalanced')\n", "plt.plot(y,label='Y_true')\n", "plt.plot(y_hat,label='y_hat - log loss:'+str(loss))\n", "plt.plot(y_hat_4_7,label='y_hat using 4/7 - log loss:'+str(loss1))\n", "plt.legend()\n", "plt.show()\n"], "cell_type": "code", "outputs": [], "metadata": {"collapsed": true, "_uuid": "62bcf1f4600d151c715f88080b5715fe2509161b", "_cell_guid": "f51b32e0-71b4-4334-b9d5-c2e488a57f54"}, "execution_count": null}, {"cell_type": "markdown", "metadata": {"collapsed": true, "_uuid": "0af041011cb1f856e9b4b91881bea3f0f9066b4f", "_cell_guid": "dccc918e-f45f-444d-bd02-23b79ef2267c"}, "source": ["Let's use this last tree, to check rock auc with different probabilities :)"]}, {"source": ["from sklearn.metrics import roc_auc_score\n", "print('roc=',roc_auc_score(y,y_hat)    ,', gini=',roc_auc_score(y,y_hat)*2-1)\n", "print('roc=',roc_auc_score(y,y_hat_4_7),', gini=',roc_auc_score(y,y_hat_4_7)*2-1)\n", "\n", "import scikitplot as skplt\n", "\n", "skplt.metrics.plot_roc_curve(y, y_hat_both)\n", "plt.show()\n", "skplt.metrics.plot_roc_curve(y, y_hat_both2)\n", "plt.show()"], "cell_type": "code", "outputs": [], "metadata": {"collapsed": true, "_uuid": "158b02951cc9e024c83cfa08019ccb755ed2bb37", "_cell_guid": "f6369c94-6a94-45f7-a900-0a166cd9b370"}, "execution_count": null}, {"source": [], "cell_type": "code", "outputs": [], "metadata": {"collapsed": true, "_uuid": "99c137b5a267fa0169e2563a6a66e390eebeb42f", "_cell_guid": "94916d51-03ca-4ea0-9969-42db49c285b8"}, "execution_count": null}], "metadata": {"language_info": {"codemirror_mode": {"version": 3, "name": "ipython"}, "nbconvert_exporter": "python", "version": "3.6.3", "pygments_lexer": "ipython3", "file_extension": ".py", "mimetype": "text/x-python", "name": "python"}, "kernelspec": {"name": "python3", "language": "python", "display_name": "Python 3"}}, "nbformat_minor": 1}