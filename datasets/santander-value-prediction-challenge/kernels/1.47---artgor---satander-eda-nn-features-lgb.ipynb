{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"markdown","source":"## General information\n\nThis kernel is dedicated to EDA of Santander Value Prediction Challenge competition as well as feature engineering and some modeling."},{"metadata":{"trusted":true,"_uuid":"090c781f1b07be8236a98a8b7e721ed1ad316073","collapsed":true},"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nplt.style.use('ggplot')\n\nfrom sklearn.neighbors import NearestNeighbors\nfrom sklearn.linear_model import Ridge\nimport lightgbm as lgb\nfrom sklearn.metrics import make_scorer\nfrom sklearn.model_selection import train_test_split, cross_val_score, KFold\nkf = KFold(n_splits=10)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"collapsed":true},"cell_type":"code","source":"train = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d757d2708e231b6ae410f76a8f7ee76eba1e3e68"},"cell_type":"markdown","source":"## Data overview"},{"metadata":{"trusted":true,"_uuid":"45131435d48970e3393f494f39b415de03b51e64","collapsed":true},"cell_type":"code","source":"train.shape, test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"86ae9d0729ee9ad269d2ebb339511d45601d1155","collapsed":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"608b3cf1199ffe8cadaf868430b6b84f2e6b31ce"},"cell_type":"markdown","source":"We have an interesting situation here: there are more features than samples in train. And test is 10x times larger than train. In will be difficult to train models adequately. So stacking and blending will be the ways to go..."},{"metadata":{"trusted":true,"_uuid":"d95f1734946f00555eb74c7f086839e58a589ce4","collapsed":true},"cell_type":"code","source":"plt.hist(train.target);\nplt.title('Target histogram.');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f69c3bc6e6f550ecd751bfb290059d45d20fcf59","collapsed":true},"cell_type":"code","source":"plt.hist(np.log1p(train.target));\nplt.title('Logarithm transformed target histogram.');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b274c33ee343dc70ceb324ae67c244fabae52e28"},"cell_type":"markdown","source":"Target variable is really skewered"},{"metadata":{"_uuid":"d69f8716f6542e817968a44fd2a5bce7ccb7b8de"},"cell_type":"markdown","source":"It is difficult to analyze anonymized features especially considering their number, but let's try.\n\nAt first let's see at the number of unique values in columns."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"f91367c9efa869589dfd9cd07e715c453734b5d8"},"cell_type":"code","source":"unique_values = [len(train[col].unique()) for col in train.columns]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"20a03275e9b84bdf6d703ed64fe4790ded6d39d7","collapsed":true},"cell_type":"code","source":"pd.Series(unique_values).quantile([0.25, 0.50, 0.75])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"653d1983ab4dd4416abd4ebed3dec1c306ed4de8","collapsed":true},"cell_type":"code","source":"pd.Series(unique_values).value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"79065f51966155bf18f2aee82ba39465de6dbf35","collapsed":true},"cell_type":"code","source":"train[[col for col in train.columns if len(train[col].unique()) == 1]].head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"514612ed6fa0db01190d1287afebb621be33343a"},"cell_type":"markdown","source":"25% of features have 7 or less unque values - I suppose most of them are categorical. And 256 features have all zero values. I'll drop them."},{"metadata":{"trusted":true,"_uuid":"5488048c82564062e99beb5721ed58ddde562a54","collapsed":true},"cell_type":"code","source":"train[[col for col in train.columns if len(train[col].unique()) == 2]].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"12b96ad1e9924e0072b837e831f03a7b14cdafd6","collapsed":true},"cell_type":"code","source":"zero_count = []\nfor col in [col for col in train.columns if len(train[col].unique()) == 2]:\n    zero_count.append([i[1] for i in list(train[col].value_counts().items()) if i[0] == 0][0])\n    \nprint('{0} features of 245 having 2 unique values have zeroes in 99% or more samples.'.format(len([i for i in zero_count if i >= 4459 * 0.99])))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"87bf6a3b97653de8092782bfc5345bdc0326cb03"},"cell_type":"markdown","source":"So most of these features are useless. In fact, let's repeat this analysis for all features."},{"metadata":{"trusted":true,"_uuid":"f5353094ab354e083db0bc1e14cb327523667e09","collapsed":true},"cell_type":"code","source":"zero_count = []\nfor col in train.columns[2:]:\n    zero_count.append([i[1] for i in list(train[col].value_counts().items()) if i[0] == 0][0])\n    \nprint('{0} features of 4491 have zeroes in 99% or more samples.'.format(len([i for i in zero_count if i >= 4459 * 0.99])))\nprint('{0} features of 4491 have zeroes in 98% or more samples.'.format(len([i for i in zero_count if i >= 4459 * 0.98])))\nprint('{0} features of 4491 have zeroes in 97% or more samples.'.format(len([i for i in zero_count if i >= 4459 * 0.97])))\nprint('{0} features of 4491 have zeroes in 96% or more samples.'.format(len([i for i in zero_count if i >= 4459 * 0.96])))\nprint('{0} features of 4491 have zeroes in 95% or more samples.'.format(len([i for i in zero_count if i >= 4459 * 0.95])))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"adc9f3541c3da80ce2e7da8f201366f3b2f04c50"},"cell_type":"markdown","source":"Well... this is a problem. Maybe having 95% zeroes isn't than bad, but features with 98-99% zeroes are very likely to be useless.\nI'll drop features where 98% or more samples are zeroes and will try some modelling."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"e8b936e368a7dce5830263914400581af8bbcec9"},"cell_type":"code","source":"cols_to_drop = [col for col in train.columns[2:] if [i[1] for i in list(train[col].value_counts().items()) if i[0] == 0][0] >= 4459 * 0.98]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ee07141357186b78004c3e26dbba1f51cff33a4c","collapsed":true},"cell_type":"code","source":"train.drop(cols_to_drop, axis=1, inplace=True)\ntest.drop(cols_to_drop, axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"781c671ba9e0547573ba70db14104aac372cfcf0"},"cell_type":"markdown","source":"## Modelling"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"bd7ef3434d24ae89339c7cee9e823415ab244737"},"cell_type":"code","source":"X = train.drop(['ID', 'target'], axis=1)\ny = train['target']\nX_test = test.drop('ID', axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"80a97caaf87a7bbeb22882112f02713bace27097"},"cell_type":"markdown","source":"### Ridge regression"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"5a4bc269e2bf6d9e0b28aef00e70670e1802c73b"},"cell_type":"code","source":"ridge = Ridge()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"437c1da831d2124785492f266812230eddf30639"},"cell_type":"code","source":"def rmsle(y_true, y_pred):\n    return np.sqrt(np.mean(np.power(np.log1p(y_true) - np.log1p(y_pred), 2)))\n\nrmsle_scorer = make_scorer(rmsle, greater_is_better=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2996e26f725476831740419ac40e5e38823e948c","collapsed":true},"cell_type":"code","source":"-cross_val_score(ridge, X, y, scoring=rmsle_scorer)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d9d0e382d02b6b7744ee840138a62073ca261d3b"},"cell_type":"markdown","source":"It isn't surprising that the result isn't great - ridge maybe unable to work with our features. Let's try LGB!"},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"3b185fb1cb9703b2ac93f957d880317464c6e8aa","collapsed":true},"cell_type":"code","source":"X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.20, random_state=42)\nparams = {'learning_rate': 0.01, 'max_depth': 6, 'boosting': 'gbdt', 'objective': 'regression', 'metric': ['rmse'], 'is_training_metric': True, 'seed': 19, 'num_leaves': 63, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 5}\nmodel = lgb.train(params, lgb.Dataset(X_train, label=y_train), 2000, lgb.Dataset(X_valid, label=y_valid), verbose_eval=50, early_stopping_rounds=20)\nprint('RMSLE on valid data: {0:.4}.'.format(rmsle(y_valid, model.predict(X_valid))))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"faf981946338d3fc5a9cb58c8139464c10eac866"},"cell_type":"markdown","source":"This is better. Now let's try log transformation on y."},{"metadata":{"trusted":true,"_uuid":"ed378f374f835b3c3cb3d1345050bd0c64448cb9","collapsed":true},"cell_type":"code","source":"X_train, X_valid, y_train, y_valid = train_test_split(X, np.log1p(y), test_size=0.20, random_state=42)\nparams = {'learning_rate': 0.01, 'max_depth': 6, 'boosting': 'gbdt', 'objective': 'regression', 'metric': ['rmse'], 'is_training_metric': True, 'seed': 19, 'num_leaves': 63, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 5}\nmodel = lgb.train(params, lgb.Dataset(X_train, label=y_train), 2000, lgb.Dataset(X_valid, label=y_valid), verbose_eval=50, early_stopping_rounds=20)\nprint('RMSLE on valid data: {0:.4}.'.format(rmsle(np.expm1(y_valid), np.expm1(model.predict(X_valid)))))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8a4ddba963e0697c5ecf71490a91f6e547afebdc"},"cell_type":"markdown","source":"This is better! Now we can try other parameters."},{"metadata":{"trusted":true,"_uuid":"9b1ce58163a5ff028af9be2b22ab443d99424bbb","collapsed":true},"cell_type":"code","source":"params = {'learning_rate': 0.01, 'max_depth': 13, 'boosting': 'gbdt', 'objective': 'regression_l2', 'metric': ['rmse'], 'is_training_metric': True, 'seed': 19, 'num_leaves': 26, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 5}\nmodel = lgb.train(params, lgb.Dataset(X_train, label=y_train), 2000, lgb.Dataset(X_valid, label=y_valid), verbose_eval=50, early_stopping_rounds=20)\nprint('RMSLE on valid data: {0:.4}.'.format(rmsle(np.expm1(y_valid), np.expm1(model.predict(X_valid)))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6ecd0716cdd72efa8f19a4a37280ba0c7613a0cb","collapsed":true},"cell_type":"code","source":"params = {'learning_rate': 0.01, 'max_depth': 13, 'boosting': 'rf', 'objective': 'regression_l2', 'metric': ['rmse'], 'is_training_metric': True, 'seed': 19, 'num_leaves': 256, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 5}\nmodel = lgb.train(params, lgb.Dataset(X_train, label=y_train), 2000, lgb.Dataset(X_valid, label=y_valid), verbose_eval=50, early_stopping_rounds=20)\nprint('RMSLE on valid data: {0:.4}.'.format(rmsle(np.expm1(y_valid), np.expm1(model.predict(X_valid)))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b19aba4c78170721d082b36db7ffb8904201f4fb","collapsed":true},"cell_type":"code","source":"params = {'learning_rate': 0.01, 'max_depth': 3, 'boosting': 'gbdt', 'objective': 'regression_l2', 'metric': ['rmse'], 'is_training_metric': True, 'seed': 19, 'num_leaves': 8, 'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 5}\nmodel = lgb.train(params, lgb.Dataset(X_train, label=y_train), 2000, lgb.Dataset(X_valid, label=y_valid), verbose_eval=50, early_stopping_rounds=20)\nprint('RMSLE on valid data: {0:.4}.'.format(rmsle(np.expm1(y_valid), np.expm1(model.predict(X_valid)))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"99fca22f28886558ab5735ac560392e34338ea42","collapsed":true},"cell_type":"code","source":"params = {'learning_rate': 0.01, 'max_depth': 13, 'boosting': 'gbdt', 'objective': 'regression', 'metric': ['rmse'], 'is_training_metric': True, 'seed': 19, 'num_leaves': 128, 'feature_fraction': 0.9,\n          'bagging_fraction': 0.8, 'bagging_freq': 5, 'num_threads': 16}\nmodel = lgb.train(params, lgb.Dataset(X_train, label=y_train), 2000, lgb.Dataset(X_valid, label=y_valid), verbose_eval=50, early_stopping_rounds=20)\nprint('RMSLE on valid data: {0:.4}.'.format(rmsle(np.expm1(y_valid), np.expm1(model.predict(X_valid)))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7dd2efc492e17b20736db47da3fede4e96778be9","collapsed":true},"cell_type":"code","source":"params = {'learning_rate': 0.02, 'max_depth': 13, 'boosting': 'gbdt', 'objective': 'regression', 'metric': 'rmse', 'is_training_metric': True, 'num_leaves': 12**2, 'feature_fraction': 0.9,\n          'bagging_fraction': 0.8, 'bagging_freq': 5,  'num_threads': 16}\nmodel = lgb.train(params, lgb.Dataset(X_train, label=y_train), 2000, lgb.Dataset(X_valid, label=y_valid), verbose_eval=50, early_stopping_rounds=20)\nprint('RMSLE on valid data: {0:.4}.'.format(rmsle(np.expm1(y_valid), np.expm1(model.predict(X_valid)))))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"63cc3752000de252d595f874d32575d603291302"},"cell_type":"markdown","source":"## NN features\nAnd now let's try to add some new features. I'll use NearestNeighbors model and find statistics on similar rows."},{"metadata":{"trusted":true,"_uuid":"d3178483dfb5c795db23b6870adbf9f7e6dce483","collapsed":true},"cell_type":"code","source":"df = pd.concat([X, X_test])\nX.shape, df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ec83840926bc7983080f19e5a4de07e2a0ec8a9d","collapsed":true},"cell_type":"code","source":"neigh = NearestNeighbors(5, n_jobs=-1)\nneigh.fit(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"803baa2ef668d46deb0cf42d4e78b37e535030da","collapsed":true},"cell_type":"code","source":"%%time\ndists, _ = neigh.kneighbors(X, n_neighbors=3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3821c8e6e5d2c3c184bb5447168ec04fa9da338b","collapsed":true},"cell_type":"code","source":"mean_dist = dists.mean(axis=1)\nmax_dist = dists.max(axis=1)\nmin_dist = dists.min(axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7be6fc3f3e0465746ffd292ac944be5f455807cc","collapsed":true},"cell_type":"code","source":"X_ = np.hstack((X, mean_dist.reshape(-1, 1), max_dist.reshape(-1, 1), min_dist.reshape(-1, 1)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"87482df0b0e3c6f168e4f33130b55e482a91daa3","collapsed":true},"cell_type":"code","source":"X_train, X_valid, y_train, y_valid = train_test_split(X_, np.log1p(y), test_size=0.20, random_state=42)\nparams = {'learning_rate': 0.02, 'max_depth': 13, 'boosting': 'gbdt', 'objective': 'regression', 'metric': 'rmse', 'is_training_metric': True, 'num_leaves': 12**2, 'feature_fraction': 0.9,\n          'bagging_fraction': 0.8, 'bagging_freq': 5,  'num_threads': 16}\nmodel = lgb.train(params, lgb.Dataset(X_train, label=y_train), 2000, lgb.Dataset(X_valid, label=y_valid), verbose_eval=50, early_stopping_rounds=20)\nprint('RMSLE on valid data: {0:.4}.'.format(rmsle(np.expm1(y_valid), np.expm1(model.predict(X_valid)))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"85e991d9451c09535ec159241218664dcf7d010c"},"cell_type":"code","source":"%%time\ntest_dists, _ = neigh.kneighbors(X_test, n_neighbors=3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"c815495ff9895bdc72de4c7b59cdbe8742b90cba"},"cell_type":"code","source":"test_mean_dist = test_dists.mean(axis=1)\ntest_max_dist = test_dists.max(axis=1)\ntest_min_dist = test_dists.min(axis=1)\nX_test_ = np.hstack((X_test, test_mean_dist.reshape(-1, 1), test_max_dist.reshape(-1, 1), test_min_dist.reshape(-1, 1)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"22c59cfb3cb5163e11294771e00c6bc55630f70d"},"cell_type":"code","source":"1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bb26333d2d3d523a270116e8df8a35c64be371c9","collapsed":true},"cell_type":"code","source":"%%time\nprediction = np.zeros((test.shape[0], 1))\nscore = []\nfor train_i, test_i in kf.split(X_):\n    print('Fold')\n    X_train = X_[train_i]\n    y_train = np.log1p(y)[train_i]\n    X_valid = X_[test_i]\n    y_valid = np.log1p(y)[test_i]\n    model = lgb.train(params, lgb.Dataset(X_train, label=y_train), 2000, lgb.Dataset(X_valid, label=y_valid), verbose_eval=1000, early_stopping_rounds=100)\n    pred = model.predict(X_test_).reshape(-1, 1)\n    prediction += np.expm1(pred)\n    score.append(model.best_score['valid_0']['rmse'])\nprint('Mean score: {:.6}. Std score: {:.6}'.format(np.mean(score), np.std(score)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"fbe230536a2a8a5e3292ea58199a72c1f19060a3"},"cell_type":"code","source":"sub = pd.read_csv('../input/sample_submission.csv')\nsub['target'] = prediction / 10\nsub.to_csv('lgb.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}