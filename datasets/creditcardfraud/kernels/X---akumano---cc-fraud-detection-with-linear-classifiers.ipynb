{"nbformat_minor": 1, "nbformat": 4, "cells": [{"outputs": [], "metadata": {"_uuid": "1d5d8af4e41d1972a2fa1340e4a0ce678c57f377", "_cell_guid": "025a102f-f798-4193-ba2a-0164fefca8f3"}, "cell_type": "markdown", "source": "# Credit Card Fraud Analysis\n\n## Data\nThe dataset is found [here](https://www.kaggle.com/dalpozz/creditcardfraud).\nIts description is found on the same page.\nSummary of the description:\n- 284807 credit card transactions among which 492 of them are fraudulent, i.e. 0.172%.\n- Predictors are Amount (transaction amount) and V1, V2, ... V28, which are principal components of the original data.\nThe original features are not available due to confidentiality issues.\n- Time (seconds elapsed between transactions) is also part of the data, but we did not include the variable in our models.\n\n## Approach\nSince the dataset is highly unbalanced, prediction accuracy is a meaningless metric.\nWe use F1 score for model evaluation.\nWe also record recall and precision scores in order to understand the tradeoff between the two scores.\nThe plot below shows an example of recall-precision tradeoff in a logistic regression model.\nThe vertical line is the class weight selected by cross-validation.\n\n![class weights vs scores](f1-vs-class-weight.png)\n\nThe models we consider are the following:\n\n- Ridge regression & classification by thresholding\n- Logistic regression with L1 regularization\n- Linear SVM classification.\n\nModels are evaluated in the following manner:\n0. Standardize the predictors.\n1. Split the dataset into a training set and test set.\n2. Parameter tuning on a model is done by 5-fold cross-validation on a training set.\n3. The model with tuned parameters is fit on the training set, followed by its evaluation on the testing set.\n4. Repeat the tuning and evaluation on different splits of the dataset (i.e. do cross-validation) to estimate the F1 score of the model. The estimated F1 score and accompanying recall and precision scores are reported in the Results section.\n\nEach model has two parameters: M, weight on positive class, and C, regularization strength. M is the ratio of weight on the positive class (class=1) to negative class (class=0), i.e. if M=m, then (weight on class 0):(weight on class 1) = 1:m.\nClass weights are necessary in our models as a remedy for unbalanced classes.\n\nThe parameter tuning step, which is embarassingly parallel, is run in parallel using `joblib`.\nMachine used: Acer S3-391 (Intel Core i3 (2nd Gen) 2367M / 1.4 GHz Dual-Core; DDR3 SDRAM 4GB)\n\n## Results\n\n### Ridge Classification\nRidge regression is ordinary least squares regression with L2 regularization.\nA ridge classifier uses a decision rule on a ridge regression result to convert it to binary outputs.\n\nF1 | Recall | Precision\n--- | --- | ---\n0.662578 | 0.886734 | 0.553262\n\n\n\n### Logistic Regression with L1 regularization\n\nF1 | Recall | Precision\n--- | --- | ---\n0.781691 | 0.802129 | 0.770161\n\nC varies between 0.144 and 0.00298, and M between 1.0 and 10.8.\n\nResults of L1-regularized regression indicate strong predictors.\nThe plot on the left shows that many cofficients are driven to zero with a stronger regularization.\nThe vertical line indicates the regularization constant chosen by cross-validation.\n\n![coefficients die out](logistic-L1-allvars-9vars.png)\n\nLogistic regression with L1 regularization on a training set indicates that ['V4', 'V11', 'V5', 'V19', 'V7', 'V28', 'V3', 'V17', 'V18'] are strong predictors.\n\nWe run logistic regression without penalty using these variables to obtain the following scores.\n\nF1 | Recall | Precision\n--- | --- | ---\n0.670403 | 0.699218 | 0.651430\n\nThe reduced model is faster to run, because of its small number of features, and a smaller parameter space to search, as it doesn't use regularization constant.\nIt is trained and tested in 19 mins for 50 parameters to search over, while the full model takes 8h for 200 parameters.\nA trade-off is that the reduced model does not have the same predictive perfermance as the full model.\nThe scores reported above is most likely upward-biased, because variable selection was done prior to parameter tuning without cross-validation.\nA proper method to select variables is to run a L1-regularized logistic regression after splitting the dataset into k-folds.\nAlthough variable selection adds extra complexity to model training, prediction and training on a new dataset can be done more efficiently once strong predictors are identified.\n\n### Linear SVM classification\nWe use `LinearSVC` of `sklearn` with `dual=False`.\n\nF1 | Recall | Precision\n--- | --- | ---\n0.821735 | 0.900472 | 0.763462\n\n### Summary of Results\nThe following table shows the results for our models together, with time to complete computation.\n\nClassifier | Predicted F1, Recall, and Precision Scores | Time to Train & Test (in min; grid search over 200 parameter pairs)\n--- | --- | --- \nRidge | 0.663, 0.887, 0.553 | 53\nLogistic | 0.783, 0.808, 0.768 | 480\nLinear SVM | 0.822, 0.900, 0.763 | 199\n\nIn all models, M varies between 1 and 25.\nRidge classifiers achieve high recall scores, though F1 score is comparatively low.\nAn advantage of ridge classification is that it trains faster than other methods; the time complexity of its training is on the same order as OLS regression, O(n\\*p^2).\n\nLinear SVM has the best performance measured in F1 score.\nThough not tested, all models, in theory, have same prediction time complexity, which is linear in the number of features.\n\n## Discussion\nTraining and testing of a model including a thorough parameter tuning takes from 6 hours to overnight, and this has been the bottleneck of progress in model evaluation.\nThe computation can be expedited with a faster parameter tuning scheme instead of grid search and a better hardware (my laptop has the specs of a chromebook).\nVariable selection is another means of reducing computation time, although we found that our model reduced via L1-regularized logistic regression does not preserve the perfermance of the full model.\n\nLinear SVM has the best predictive ability among the models tested.\nIt is, however, worth studying ridge classifier in more depth, since the model achieves high recall scores, and runs much faster.\nFurther work on ridge classifier should involve understanding the mechanism behind its recall-precision tradeoff to see if raising the precision score without recall taking a big hit; and applying results from variable selection via L1-regularized logistic regression. \n\n\n## Future work\n- Take a closer look at variable selection via L1-regularized logistic regression. Try other variable selection methods, such as forward stepwise selection, as well.\n- Tree-based classification\n- Speed up the parameter tuning step. Consider using RandomizedSearchCV of sklearn or Hyperopt.\n- Cost-sensitive analysis. Models should include penalty for not accurately classifying a fraudulent transaction with a high amount.\n\n\n## Code\nThe following is the linear SVM classifier.\nThis code can be made to do logistic regression by minor modifications, namely, by changing `LinearSVC` to `LogisticRegression` and changing parameters of `clf`.\nA large portion of the parameter tuning code can be replaced with `cross_val_score` from scikit-learn; however,\ncoding our own cross-validation allows for ", "execution_count": null}, {"outputs": [], "metadata": {"trusted": false, "_uuid": "e37233a8edea931a64b779dbef3268300c3cf067", "_cell_guid": "028437d9-e1a3-4109-aaa4-071267ddb3ca"}, "cell_type": "code", "source": "import pandas as pd\nimport scipy as sp\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split, cross_val_score, KFold\nfrom sklearn.linear_model import RidgeClassifier, RidgeClassifierCV, LogisticRegression, LogisticRegressionCV\nfrom sklearn.svm import LinearSVC\nfrom sklearn.metrics import f1_score, recall_score, precision_score\nfrom joblib import Parallel, delayed, load, dump\nimport tempfile, shutil, os\n\ndf = pd.read_csv(\"../input/creditcard.csv\")\nX = df.drop(['Time','Class'],axis=1)\ny = df['Class']\nX = (X - X.mean()) / X.std()\n\ndef make_weights(M):\n    return {0: 1, 1: M}\n\nimport warnings, sklearn\n#ignore warnings from f1_score and recall_score.\n#The methods return 0 when no positive class is available.\nwarnings.filterwarnings(\"ignore\", category=sklearn.exceptions.UndefinedMetricWarning)\n\nkC=30\nkM=10\ncs = sp.logspace(-6,-2,kC)\nms = sp.linspace(0,10,kM)\nclf = LinearSVC()\nk = 5\nkf = KFold(n_splits=k)\n\ndef param_eval_parallel(X, y, clf_class, cs, ms): #for each fold at the top-level\n        folder = tempfile.mkdtemp()\n        X_path = os.path.join(folder, 'X-data')\n        y_path = os.path.join(folder, 'y-data')\n        scores_path = os.path.join(folder, 'scores')\n        try:\n            scores = sp.memmap(scores_path, dtype=X.iat[0,0].dtype,\\\n                          shape=(kC,kM,3), mode='w+')\n            dump(X, X_path)\n            dump(y, y_path)\n            X = load(X_path, mmap_mode='r')\n            y = load(y_path, mmap_mode='r')\n            Parallel(n_jobs=2)(delayed(param_eval)(X, y, clf_class, ci, C, mi, M, scores) \\\n                for ci, C in enumerate(cs) for mi, M in enumerate(ms))\n            f1_scores = scores[:,:,0]\n            best_idx = sp.unravel_index(sp.argmax(f1_scores), f1_scores.shape)\n            return cs[best_idx[0]], ms[best_idx[1]]\n        finally:\n            try:\n                shutil.rmtree(folder)\n            except:\n                print(\"Failed to delete: \" + folder)\n                \ndef param_eval(X, y, clf_class, ci, C, mi, M, scores):\n        class_weight = make_weights(M)\n        if clf_class == LogisticRegression:\n            clf.set_params(penalty='l1', C=C, dual=False, class_weight=class_weight)\n        elif clf_class == RidgeClassifier:\n            clf.set_params(alpha=C, class_weight=class_weight)\n        elif clf_class == LinearSVC:\n            clf.set_params(C=C, dual=False, class_weight=class_weight)\n\n        f1_tmp = sp.empty(k)\n        recall_tmp = sp.empty(k)\n        precision_tmp = sp.empty(k)\n        kf = KFold(n_splits=k)\n        for j, (train_idx, test_idx) in enumerate(kf.split(y)):\n            #scores = cross_val_score(clf, X, y, scoring='f1', cv=5)\n            clf_fit = clf.fit(X.iloc[train_idx], y.iloc[train_idx])\n            y_pred_cv = clf_fit.predict(X.iloc[test_idx])\n            y_test_cv = y.iloc[test_idx]             \n            f1_tmp[j] = f1_score(y_pred_cv, y_test_cv)\n            recall_tmp[j] = recall_score(y_pred_cv, y_test_cv)\n            precision_tmp[j] = precision_score(y_pred_cv, y_test_cv)\n            #print(f1_tmp.mean(), recall_tmp.mean(), precision_tmp.mean())\n            scores[ci,mi,0] = f1_tmp.mean()\n            scores[ci,mi,1] = recall_tmp.mean()\n            scores[ci,mi,2] = precision_tmp.mean()\n\ndef model_eval(clf, kC, kM, k):\n    clf_class = clf.__class__\n    if clf_class == LogisticRegression:\n        cs = sp.logspace(-4,0,kC)\n        ms = sp.linspace(1,12,kM)\n    elif clf_class == RidgeClassifier:\n        cs = sp.linspace(1e-1, 1e4, kC) #alphas\n        ms = sp.linspace(1,50,kM)\n    elif clf_class == LinearSVC:\n        cs = sp.logspace(-5,-1,kC)\n        ms = sp.linspace(1,12,kM)\n    else:\n        raise ValueError(\"%s is not supported\" % clf.class_)\n                \n    f1_scores = sp.empty(k)\n    recall_scores = sp.empty(k)\n    precision_scores = sp.empty(k)\n    kf = KFold(n_splits=k, random_state=0)\n    for i, (train_idx, test_idx) in enumerate(kf.split(y)):\n        print(\"%d-th fold\" % i)\n        Xi, yi = X.iloc[train_idx], y.iloc[train_idx]\n        C,M = param_eval_parallel(Xi,yi,clf_class,cs,ms)\n        print(\"%d-th fold:  regularization = %f,  weight = %f\" \\\n             % (i, C, M))\n        class_weight = make_weights(M)\n        if clf_class == LogisticRegression:\n            clf.set_params(penalty='l1', C=C, dual=False, class_weight=class_weight)\n        elif clf_class == RidgeClassifier:\n            clf.set_params(alpha=C, class_weight=class_weight)\n        elif clf_class == LinearSVC:\n            clf.set_params(C=C, dual=False, class_weight=class_weight)\n\n        clf_fit = clf.fit(Xi,yi)\n        y_pred = clf_fit.predict(X.iloc[test_idx])\n        y_test = y.iloc[test_idx]\n        f1_scores[i] = f1_score(y_pred,y_test)\n        recall_scores[i] = recall_score(y_pred,y_test)\n        precision_scores[i] = precision_score(y_pred,y_test)\n    print(\"Predicted scores:\\n F1: %f;  Recall: %f;  Precision: %f\" \\\n        % (f1_scores.mean(), recall_scores.mean(), precision_scores.mean()))\n    return f1_scores, recall_scores, precision_scores\n", "execution_count": 4}, {"outputs": [], "metadata": {"trusted": false, "_uuid": "c0799e31d6f75048afab78c8f0656c4e0a89201b", "_cell_guid": "0dfa029c-119b-4dcc-9211-a781f84e56d8"}, "cell_type": "code", "source": "#Example\nkC=3 #regularization grid_num\nkM=2 #class weight grid_num  \nclf = LinearSVC()\nk = 2 # k-fold cross-validation\nmodel_eval(clf,kC,kM,k)", "execution_count": 5}], "metadata": {"language_info": {"pygments_lexer": "ipython3", "nbconvert_exporter": "python", "version": "3.6.1", "file_extension": ".py", "codemirror_mode": {"version": 3, "name": "ipython"}, "name": "python", "mimetype": "text/x-python"}, "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}}}