{"cells":[{"metadata":{"_uuid":"01f75fc2fc29b285ef80179d8ed19c4fa397c327"},"cell_type":"markdown","source":"## Rotten Tomatoes Sentiment Review - Two Step Classification\nIn this notebook i will use a Two Step Classifier, which first detects if a given phrase is negative, neutral or positive, and then proceeds to classify the exact label.\n\nThis architecture managed to beat simple linear models and simple neural network architectures during my tests."},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":false,"collapsed":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom functools import reduce\n\nfrom nltk.corpus import stopwords\nfrom nltk.collocations import BigramAssocMeasures, BigramCollocationFinder\n\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import LinearSVC","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"252dd7e944fd1a2219bafd30284b513c1237011e","trusted":false,"collapsed":true},"cell_type":"code","source":"def build_collocation_applier(set_colloc):\n    \"\"\"\n    Returns a function which applies the given set of collocations to a sentence\n    Parameters:\n    - set_colloc: Iterable of collocations, each of which is a tuple\n    \"\"\"\n    def apply_collocations(sentence):\n        \"\"\"\n        Transforms each collocation in the given sequence into a single word\n        \"\"\"\n        res = sentence.lower()\n        for b1,b2 in set_colloc:\n            res = res.replace(\"%s %s\" % (b1 ,b2), \"%s%s\" % (b1 ,b2))\n        return res\n    return apply_collocations\n\nclass TwoStepClassifier():\n    \"\"\"\n    This classifier will divide the 5-class problem into simpler problems:\n    Problem 1: Classify review between negative(0-1), neutral(2) and positive(3-4)\n    Problem 2: If review is negative, classify 0 or 1. If review is positive, classify 3 or 4\n    \"\"\"\n    def __init__(self, clf, clf_params = {}):\n        self.clf = clf\n        self.clf_params = clf_params\n        \n    def fit(self, X, Y):\n        Y_1 = Y.apply(lambda x : {0:0,1:0,2:2,3:4,4:4}[x])\n        self.clf_1 = self.clf(**self.clf_params).fit(X, Y_1)\n        self.clf_2 = self.clf(**self.clf_params).fit(X[Y<2,:], Y[Y<2])\n        self.clf_3 = self.clf(**self.clf_params).fit(X[Y>2,:], Y[Y>2])\n        \n    def predict(self, X):\n        Y = self.clf_1.predict(X)\n        Y[Y<2] = self.clf_2.predict(X[Y<2])\n        Y[Y>2] = self.clf_3.predict(X[Y>2])\n        return Y\n        \n    def score(X, Y):\n        return np.mean(self.predict(X) == Y)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"52ae89bd05ceeceb657cac1f2a876d72a36293b0"},"cell_type":"markdown","source":"## Data Prep\nHere we will read the dataset and apply some preprocessing\n\nWe are going to remove stopwords, replace collocations by a single word, and then we will vectorize the text using TfIdf"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false,"collapsed":true},"cell_type":"code","source":"df_train = pd.read_csv('../input/train.tsv', sep = '\\t')\ndf_test = pd.read_csv('../input/test.tsv', sep = '\\t')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"792ad4d7bfd0b4d44812fbf3b33575cc9fb85968"},"cell_type":"markdown","source":"In this line, we use a neat trick with the *reduce* operator to filter only the first apparition of each Sentence (the full Sentence)\n\nWe need such a dataset to find the most important collocations in the corpus, because if we used the raw dataset we would count a lot of repeated terms since each sentence appears multiple times"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"a7a29b7a7d39a889ed047ede2be87b6cfe773bd1"},"cell_type":"code","source":"df_train_unique = df_train.groupby('SentenceId').agg({'Phrase' : lambda x : reduce(lambda a, b: a, x)})","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2f70f29dfc0e7d004156c311671041b7e6d1e407","trusted":false,"collapsed":true},"cell_type":"code","source":"tokenizer = CountVectorizer(stop_words = stopwords.words('english')).build_analyzer()\ntokens = df_train_unique.Phrase.apply(tokenizer)\ntokens = tokens.apply(lambda x : ' '.join(x)).drop_duplicates().drop(3).reset_index(drop = True).apply(lambda x : x.split())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0e6514c33e853b2a5538924038a15bd1f5c21f60","trusted":false,"collapsed":true},"cell_type":"code","source":"bigram_measures = BigramAssocMeasures()\nfinder = BigramCollocationFinder.from_documents(tokens)\nfinder.apply_freq_filter(5)\ncolloc = finder.nbest(bigram_measures.pmi, 100)\napplier = build_collocation_applier(colloc)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ee79cb53efd0b95668c12c51f8588f0c7acbc5c1","trusted":false,"collapsed":true},"cell_type":"code","source":"tokens_colloc_train = df_train.Phrase.apply(tokenizer)\ntokens_colloc_train = tokens_colloc_train.apply(lambda x : applier(' '.join(x)))\ntokens_colloc_test = df_test.Phrase.apply(tokenizer)\ntokens_colloc_test = tokens_colloc_test.apply(lambda x : applier(' '.join(x)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"95caf6b283cbe7fa8759f72be9df1c26a2e83ca0"},"cell_type":"markdown","source":"I chose these parameters so i could fit the model pretty fast without losing accuracy"},{"metadata":{"_uuid":"0011459070b6acfec01238af24e54377b0f37a81","trusted":false,"collapsed":true},"cell_type":"code","source":"bow = TfidfVectorizer(binary = True,\n                      sublinear_tf = True,\n                      stop_words = stopwords.words('english'),\n                      ngram_range=(1,2),\n                      min_df = 10, max_df = 0.5, max_features = 5000)\nX_train = bow.fit_transform(tokens_colloc_train)\nX_test = bow.transform(tokens_colloc_test)\nY_train = df_train.Sentiment","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"752cadc5c42e2a573390716d2e5e2f47558e49e7"},"cell_type":"code","source":"clf = TwoStepClassifier(LogisticRegression)\nclf.fit(X_train.toarray(), Y_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"66b81d7fbdcbf79e4d7fa2b95eed0725bb0992f3","trusted":false,"collapsed":true},"cell_type":"code","source":"pred = pd.read_csv('../input/sampleSubmission.csv')\npred.Sentiment = clf.predict(X_test.toarray())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9bb436ba8dbfc1e698d4abbda35b9061d3091088","trusted":false,"collapsed":true},"cell_type":"code","source":"pred.to_csv('output.csv', index = False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}