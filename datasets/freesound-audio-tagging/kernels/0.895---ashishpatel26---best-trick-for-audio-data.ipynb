{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"#https://www.kaggle.com/fizzbuzz/beginner-s-guide-to-audio-data\n# Change this to True to replicate the result\nCOMPLETE_RUN = False\nimport numpy as np\nnp.random.seed(1001)\n\nimport os\nimport shutil\n\nimport IPython\nimport matplotlib\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom tqdm import tqdm_notebook\nfrom sklearn.cross_validation import StratifiedKFold\n\n%matplotlib inline\nmatplotlib.style.use('ggplot')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"collapsed":true},"cell_type":"code","source":"train = pd.read_csv(\"../input/freesound-audio-tagging/train.csv\")\ntest = pd.read_csv(\"../input/freesound-audio-tagging/sample_submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8b802e7ce40a3d1675722dc569d7b5c270aaa79f"},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fbab782779797ef7b06c3addf625a928890abbda"},"cell_type":"code","source":"print(\"Number of training examples=\", train.shape[0], \"  Number of classes=\", len(train.label.unique()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"824ee04ca5e7573df96c6e99807aa4adb0a60032"},"cell_type":"code","source":"print(train.label.unique())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a68918ca5c63f1c0660f5edf75a207867d10afc5"},"cell_type":"markdown","source":"# Distribution Category"},{"metadata":{"trusted":true,"_uuid":"5bb0125b486fc427d3958566e59f56b1fc721b00"},"cell_type":"code","source":"category_group = train.groupby(['label', 'manually_verified']).count()\nplot = category_group.unstack().reindex(category_group.unstack().sum(axis=1).sort_values().index)\\\n          .plot(kind='bar', stacked=True, title=\"Number of Audio Samples per Category\", figsize=(16,10))\nplot.set_xlabel(\"Category\")\nplot.set_ylabel(\"Number of Samples\");","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d9009fd3bd9a47481ced137c23779b5bf1bd937d"},"cell_type":"code","source":"print('Minimum samples per category = ', min(train.label.value_counts()))\nprint('Maximum samples per category = ', max(train.label.value_counts()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7d3d01410820a358a5eeacdb49ac7682aa6d2ac3"},"cell_type":"code","source":"import IPython.display as ipd  # To play sound in the notebook\nfname = '../input/freesound-audio-tagging/audio_train/' + '00044347.wav'   # Hi-hat\nipd.Audio(fname)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"18fb3f4f957b3b7f26df9fff7f1a7705296c66d1"},"cell_type":"code","source":"# Using wave library\nimport wave\nwav = wave.open(fname)\nprint(\"Sampling (frame) rate = \", wav.getframerate())\nprint(\"Total samples (frames) = \", wav.getnframes())\nprint(\"Duration = \", wav.getnframes()/wav.getframerate())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"60dbceb7dd798c1ad500d227e84a1e23b3f582d6"},"cell_type":"code","source":"# Using scipy\nfrom scipy.io import wavfile\nrate, data = wavfile.read(fname)\nprint(\"Sampling (frame) rate = \", rate)\nprint(\"Total samples (frames) = \", data.shape)\nprint(data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"84dd45894cdfdbee81b09606d411770d9a8472cf"},"cell_type":"code","source":"plt.plot(data, '-', );","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"14ec2628fb6e1c4d97e153f8a654a217d501ffb6"},"cell_type":"code","source":"plt.figure(figsize=(16, 4))\nplt.plot(data[:500], '.'); plt.plot(data[:500], '-');","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f41f07153e387aeacef09f5214c3d91ed0ce475d"},"cell_type":"markdown","source":"# Audio Length"},{"metadata":{"trusted":true,"_uuid":"a36909801ab69f740423931db44ad5a05d654e72"},"cell_type":"code","source":"train['nframes'] = train['fname'].apply(lambda f: wave.open('../input/freesound-audio-tagging/audio_train/' + f).getnframes())\ntest['nframes'] = test['fname'].apply(lambda f: wave.open('../input/freesound-audio-tagging/audio_test/' + f).getnframes())\n\n_, ax = plt.subplots(figsize=(16, 4))\nsns.violinplot(ax=ax, x=\"label\", y=\"nframes\", data=train)\nplt.xticks(rotation=90)\nplt.title('Distribution of audio frames, per label', fontsize=16)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0b5a85d4f2e42e74f53a02a3eee58c3560608b99"},"cell_type":"code","source":"fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(16,5))\ntrain.nframes.hist(bins=100, ax=axes[0])\ntest.nframes.hist(bins=100, ax=axes[1])\nplt.suptitle('Frame Length Distribution in Train and Test', ha='center', fontsize='large');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dbc94337f41ac6dd07bfa169cfb4f584359cbc76"},"cell_type":"code","source":"abnormal_length = [707364, 353682, 138474, 184338]\n\nfor length in abnormal_length:\n    abnormal_fnames = test.loc[test.nframes == length, 'fname'].values\n    print(\"Frame length = \", length, \" Number of files = \", abnormal_fnames.shape[0], end=\"   \")\n    fname = np.random.choice(abnormal_fnames)\n    print(\"Playing \", fname)\n    IPython.display.display(ipd.Audio( '../input/freesound-audio-tagging/audio_test/' + fname))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1f29ef292f20aaf06316d81e3271ada6e05f64fb"},"cell_type":"markdown","source":"# Model Building"},{"metadata":{"trusted":true,"_uuid":"a0b70f485a4e1a305a9d8d7c35cd0784cfe53414"},"cell_type":"code","source":"import librosa\nimport numpy as np\nimport scipy\nfrom keras import losses, models, optimizers\nfrom keras.activations import relu, softmax\nfrom keras.callbacks import (EarlyStopping, LearningRateScheduler,\n                             ModelCheckpoint, TensorBoard, ReduceLROnPlateau)\nfrom keras.layers import (Convolution1D, Dense, Dropout, GlobalAveragePooling1D, \n                          GlobalMaxPool1D, Input, MaxPool1D, concatenate)\nfrom keras.utils import Sequence, to_categorical","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"8eeb21f5764888dc36eed450322c2517aae8d8c4"},"cell_type":"code","source":"class Config(object):\n    def __init__(self,\n                 sampling_rate=16000, audio_duration=2, n_classes=41,\n                 use_mfcc=False, n_folds=12, learning_rate=0.0001, \n                 max_epochs=80, n_mfcc=20):\n        self.sampling_rate = sampling_rate\n        self.audio_duration = audio_duration\n        self.n_classes = n_classes\n        self.use_mfcc = use_mfcc\n        self.n_mfcc = n_mfcc\n        self.n_folds = n_folds\n        self.learning_rate = learning_rate\n        self.max_epochs = max_epochs\n\n        self.audio_length = self.sampling_rate * self.audio_duration\n        if self.use_mfcc:\n            self.dim = (self.n_mfcc, 1 + int(np.floor(self.audio_length/512)), 1)\n        else:\n            self.dim = (self.audio_length, 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"ea34b9c9ef92ecc4376d60b90f13cc5ec9ea20bb"},"cell_type":"code","source":"class DataGenerator(Sequence):\n    def __init__(self, config, data_dir, list_IDs, labels=None, \n                 batch_size=64, preprocessing_fn=lambda x: x):\n        self.config = config\n        self.data_dir = data_dir\n        self.list_IDs = list_IDs\n        self.labels = labels\n        self.batch_size = batch_size\n        self.preprocessing_fn = preprocessing_fn\n        self.on_epoch_end()\n        self.dim = self.config.dim\n\n    def __len__(self):\n        return int(np.ceil(len(self.list_IDs) / self.batch_size))\n\n    def __getitem__(self, index):\n        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n        return self.__data_generation(list_IDs_temp)\n\n    def on_epoch_end(self):\n        self.indexes = np.arange(len(self.list_IDs))\n\n    def __data_generation(self, list_IDs_temp):\n        cur_batch_size = len(list_IDs_temp)\n        X = np.empty((cur_batch_size, *self.dim))\n\n        input_length = self.config.audio_length\n        for i, ID in enumerate(list_IDs_temp):\n            file_path = self.data_dir + ID\n            \n            # Read and Resample the audio\n            data, _ = librosa.core.load(file_path, sr=self.config.sampling_rate,\n                                        res_type='kaiser_fast')\n\n            # Random offset / Padding\n            if len(data) > input_length:\n                max_offset = len(data) - input_length\n                offset = np.random.randint(max_offset)\n                data = data[offset:(input_length+offset)]\n            else:\n                if input_length > len(data):\n                    max_offset = input_length - len(data)\n                    offset = np.random.randint(max_offset)\n                else:\n                    offset = 0\n                data = np.pad(data, (offset, input_length - len(data) - offset), \"constant\")\n                \n            # Normalization + Other Preprocessing\n            if self.config.use_mfcc:\n                data = librosa.feature.mfcc(data, sr=self.config.sampling_rate,\n                                                   n_mfcc=self.config.n_mfcc)\n                data = np.expand_dims(data, axis=-1)\n            else:\n                data = self.preprocessing_fn(data)[:, np.newaxis]\n            X[i,] = data\n\n        if self.labels is not None:\n            y = np.empty(cur_batch_size, dtype=int)\n            for i, ID in enumerate(list_IDs_temp):\n                y[i] = self.labels[ID]\n            return X, to_categorical(y, num_classes=self.config.n_classes)\n        else:\n            return X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"e89e17fff22b5a215f3b5383b3ed6130b29a37a4"},"cell_type":"code","source":"def audio_norm(data):\n    max_data = np.max(data)\n    min_data = np.min(data)\n    data = (data-min_data)/(max_data-min_data+1e-6)\n    return data-0.5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"d15ac54d2c5c1dccda5d3a05366de7982cd5f729"},"cell_type":"code","source":"def get_1d_dummy_model(config):\n    \n    nclass = config.n_classes\n    input_length = config.audio_length\n    \n    inp = Input(shape=(input_length,1))\n    x = GlobalMaxPool1D()(inp)\n    out = Dense(nclass, activation=softmax)(x)\n\n    model = models.Model(inputs=inp, outputs=out)\n    opt = optimizers.Adam(config.learning_rate)\n\n    model.compile(optimizer=opt, loss=losses.categorical_crossentropy, metrics=['acc'])\n    return model\n\ndef get_1d_conv_model(config):\n    \n    nclass = config.n_classes\n    input_length = config.audio_length\n    \n    inp = Input(shape=(input_length,1))\n    x = Convolution1D(32, 9, activation=relu, padding=\"same\")(inp)\n    x = Convolution1D(32, 9, activation=relu, padding=\"same\")(x)\n    x = MaxPool1D(16)(x)\n    x = Dropout(rate=0.1)(x)\n    \n    x = Convolution1D(64, 3, activation=relu, padding=\"same\")(x)\n    x = Convolution1D(64, 3, activation=relu, padding=\"same\")(x)\n    x = MaxPool1D(4)(x)\n    x = Dropout(rate=0.1)(x)\n    \n    x = Convolution1D(64, 3, activation=relu, padding=\"same\")(x)\n    x = Convolution1D(64, 3, activation=relu, padding=\"same\")(x)\n    x = MaxPool1D(4)(x)\n    x = Dropout(rate=0.1)(x)\n    \n    x = Convolution1D(128, 3, activation=relu, padding=\"same\")(x)\n    x = Convolution1D(128, 3, activation=relu, padding=\"same\")(x)\n    x = MaxPool1D(4)(x)\n    x = Dropout(rate=0.1)(x)\n    \n    x = Convolution1D(256, 3, activation=relu, padding=\"same\")(x)\n    x = Convolution1D(256, 3, activation=relu, padding=\"same\")(x)\n    x = GlobalMaxPool1D()(x)\n    x = Dropout(rate=0.2)(x)\n\n    x = Dense(64, activation=relu)(x)\n    x = Dense(1028, activation=relu)(x)\n    out = Dense(nclass, activation=softmax)(x)\n\n    model = models.Model(inputs=inp, outputs=out)\n    opt = optimizers.Adam(config.learning_rate)\n\n    model.compile(optimizer=opt, loss=losses.categorical_crossentropy, metrics=['acc'])\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2a5b1c9c4a3c481de7ddb808f5e14ea5163e8b74","collapsed":true},"cell_type":"code","source":"LABELS = list(train.label.unique())\nlabel_idx = {label: i for i, label in enumerate(LABELS)}\ntrain.set_index(\"fname\", inplace=True)\ntest.set_index(\"fname\", inplace=True)\ntrain[\"label_idx\"] = train.label.apply(lambda x: label_idx[x])\nif not COMPLETE_RUN:\n    train = train[:2000]\n    test = test[:2000]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"7a33892c796b3c01285b5139331e08c62f936a06"},"cell_type":"code","source":"config = Config(sampling_rate=16000, audio_duration=2, n_folds=10, learning_rate=0.001)\nif not COMPLETE_RUN:\n    config = Config(sampling_rate=100, audio_duration=1, n_folds=2, max_epochs=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"847d5734d2d922fe9a5d37b6a2efe9513093ce9d","collapsed":true},"cell_type":"code","source":"PREDICTION_FOLDER = \"predictions_1d_conv\"\nif not os.path.exists(PREDICTION_FOLDER):\n    os.mkdir(PREDICTION_FOLDER)\nif os.path.exists('logs/' + PREDICTION_FOLDER):\n    shutil.rmtree('logs/' + PREDICTION_FOLDER)\n\nskf = StratifiedKFold(train.label_idx, n_folds=config.n_folds)\n\nfor i, (train_split, val_split) in enumerate(skf):\n    train_set = train.iloc[train_split]\n    val_set = train.iloc[val_split]\n    checkpoint = ModelCheckpoint('best_%d.h5'%i, monitor='val_loss', verbose=1, save_best_only=True)\n    early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=5)\n    tb = TensorBoard(log_dir='./logs/' + PREDICTION_FOLDER + '/fold_%d'%i, write_graph=True)\n\n    callbacks_list = [checkpoint, early, tb]\n    print(\"Fold: \", i)\n    print(\"#\"*50)\n    if COMPLETE_RUN:\n        model = get_1d_conv_model(config)\n    else:\n        model = get_1d_dummy_model(config)\n\n    train_generator = DataGenerator(config, '../input/freesound-audio-tagging/audio_train/', train_set.index, \n                                    train_set.label_idx, batch_size=64,\n                                    preprocessing_fn=audio_norm)\n    val_generator = DataGenerator(config, '../input/freesound-audio-tagging/audio_train/', val_set.index, \n                                  val_set.label_idx, batch_size=64,\n                                  preprocessing_fn=audio_norm)\n\n    history = model.fit_generator(train_generator, callbacks=callbacks_list, validation_data=val_generator,\n                                  epochs=config.max_epochs, use_multiprocessing=True, workers=6, max_queue_size=20)\n\n    model.load_weights('best_%d.h5'%i)\n\n    # Save train predictions\n    train_generator = DataGenerator(config, '../input/freesound-audio-tagging/audio_train/', train.index, batch_size=128,\n                                    preprocessing_fn=audio_norm)\n    predictions = model.predict_generator(train_generator, use_multiprocessing=True, \n                                          workers=6, max_queue_size=20, verbose=1)\n    np.save(PREDICTION_FOLDER + \"/train_predictions_%d.npy\"%i, predictions)\n\n    # Save test predictions\n    test_generator = DataGenerator(config, '../input/freesound-audio-tagging/audio_test/', test.index, batch_size=128,\n                                    preprocessing_fn=audio_norm)\n    predictions = model.predict_generator(test_generator, use_multiprocessing=True, \n                                          workers=6, max_queue_size=20, verbose=1)\n    np.save(PREDICTION_FOLDER + \"/test_predictions_%d.npy\"%i, predictions)\n\n    # Make a submission file\n    top_3 = np.array(LABELS)[np.argsort(-predictions, axis=1)[:, :3]]\n    predicted_labels = [' '.join(list(x)) for x in top_3]\n    test['label'] = predicted_labels\n    test[['label']].to_csv(PREDICTION_FOLDER + \"/predictions_%d.csv\"%i)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"4a731d7ab990e480cc74aee4e4c71c8dc2286dad"},"cell_type":"code","source":"import librosa\nSAMPLE_RATE = 44100\nfname = '../input/freesound-audio-tagging/audio_train/' + '00044347.wav'   # Hi-hat\nwav, _ = librosa.core.load(fname, sr=SAMPLE_RATE)\nwav = wav[:2*44100]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ea56ed5492a5864113c86e8bf17262f667907dc1","collapsed":true},"cell_type":"code","source":"mfcc = librosa.feature.mfcc(wav, sr = SAMPLE_RATE, n_mfcc=40)\nmfcc.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"149dee034fe321673736e004d824620ad77393bd","collapsed":true},"cell_type":"code","source":"plt.imshow(mfcc, cmap='hot', interpolation='nearest');","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f793965975fef92bccd8a555734466a1c2509ff7"},"cell_type":"markdown","source":"# Building Model Using MFCC"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"db02d96c673527f7d0d36abaa80d2e5f8151ad8f"},"cell_type":"code","source":"from keras.layers import (Convolution2D, GlobalAveragePooling2D, BatchNormalization, Flatten,\n                          GlobalMaxPool2D, MaxPool2D, concatenate, Activation)\nfrom keras.utils import Sequence, to_categorical\nfrom keras import backend as K","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"a1eacd48e3f43c731ce806b542d4ffb5bb104b4b"},"cell_type":"code","source":"def get_2d_dummy_model(config):\n    \n    nclass = config.n_classes\n    \n    inp = Input(shape=(config.dim[0],config.dim[1],1))\n    x = GlobalMaxPool2D()(inp)\n    out = Dense(nclass, activation=softmax)(x)\n\n    model = models.Model(inputs=inp, outputs=out)\n    opt = optimizers.Adam(config.learning_rate)\n\n    model.compile(optimizer=opt, loss=losses.categorical_crossentropy, metrics=['acc'])\n    return model\n\n\ndef get_2d_conv_model(config):\n    \n    nclass = config.n_classes\n    \n    inp = Input(shape=(config.dim[0],config.dim[1],1))\n    x = Convolution2D(32, (4,10), padding=\"same\")(inp)\n    x = BatchNormalization()(x)\n    x = Activation(\"relu\")(x)\n    x = MaxPool2D()(x)\n    \n    x = Convolution2D(32, (4,10), padding=\"same\")(x)\n    x = BatchNormalization()(x)\n    x = Activation(\"relu\")(x)\n    x = MaxPool2D()(x)\n    \n    x = Convolution2D(32, (4,10), padding=\"same\")(x)\n    x = BatchNormalization()(x)\n    x = Activation(\"relu\")(x)\n    x = MaxPool2D()(x)\n    \n    x = Convolution2D(32, (4,10), padding=\"same\")(x)\n    x = BatchNormalization()(x)\n    x = Activation(\"relu\")(x)\n    x = MaxPool2D()(x)\n\n    x = Flatten()(x)\n    x = Dense(64)(x)\n    x = BatchNormalization()(x)\n    x = Activation(\"relu\")(x)\n    out = Dense(nclass, activation=softmax)(x)\n\n    model = models.Model(inputs=inp, outputs=out)\n    opt = optimizers.Adam(config.learning_rate)\n\n    model.compile(optimizer=opt, loss=losses.categorical_crossentropy, metrics=['acc'])\n    return model","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"64fa28f8be2da17044cc4e81f7e688de407a1c04"},"cell_type":"markdown","source":"# Prepare Data"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"9b14c0b7d31bd28ca63f5c4367d759d967dd7ebf"},"cell_type":"code","source":"config = Config(sampling_rate=44100, audio_duration=2, n_folds=10, \n                learning_rate=0.001, use_mfcc=True, n_mfcc=40)\nif not COMPLETE_RUN:\n    config = Config(sampling_rate=44100, audio_duration=2, n_folds=2, \n                    max_epochs=1, use_mfcc=True, n_mfcc=40)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"98f506a677b91d92606e9cf8110a03db7de65291"},"cell_type":"code","source":"def prepare_data(df, config, data_dir):\n    X = np.empty(shape=(df.shape[0], config.dim[0], config.dim[1], 1))\n    input_length = config.audio_length\n    for i, fname in enumerate(df.index):\n        print(fname)\n        file_path = data_dir + fname\n        data, _ = librosa.core.load(file_path, sr=config.sampling_rate, res_type=\"kaiser_fast\")\n\n        # Random offset / Padding\n        if len(data) > input_length:\n            max_offset = len(data) - input_length\n            offset = np.random.randint(max_offset)\n            data = data[offset:(input_length+offset)]\n        else:\n            if input_length > len(data):\n                max_offset = input_length - len(data)\n                offset = np.random.randint(max_offset)\n            else:\n                offset = 0\n            data = np.pad(data, (offset, input_length - len(data) - offset), \"constant\")\n\n        data = librosa.feature.mfcc(data, sr=config.sampling_rate, n_mfcc=config.n_mfcc)\n        data = np.expand_dims(data, axis=-1)\n        X[i,] = data\n    return X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2fbfb6ca5d97f31aca3e80f90d76ff23ab6cb607","collapsed":true},"cell_type":"code","source":"X_train = prepare_data(train, config, '../input/freesound-audio-tagging/audio_train/')\nX_test = prepare_data(test, config, '../input/freesound-audio-tagging/audio_test/')\ny_train = to_categorical(train.label_idx, num_classes=config.n_classes)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"31ea17bc71b1c6e42e4731761172f13d78fc8778"},"cell_type":"markdown","source":"# Normalization"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"a64ee9442496321eb8029009ea9b90357fed41b1"},"cell_type":"code","source":"mean = np.mean(X_train, axis=0)\nstd = np.std(X_train, axis=0)\n\nX_train = (X_train - mean)/std\nX_test = (X_test - mean)/std","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2f477e616a5761de253dde0c6d6757dc271c0e3f"},"cell_type":"markdown","source":"# Training 2D Conv on MFCC"},{"metadata":{"trusted":true,"_uuid":"5afbc975204a9f3953e4d709112397b4c99a0315","collapsed":true},"cell_type":"code","source":"PREDICTION_FOLDER = \"predictions_2d_conv\"\nif not os.path.exists(PREDICTION_FOLDER):\n    os.mkdir(PREDICTION_FOLDER)\nif os.path.exists('logs/' + PREDICTION_FOLDER):\n    shutil.rmtree('logs/' + PREDICTION_FOLDER)\n\nskf = StratifiedKFold(train.label_idx, n_folds=config.n_folds)\nfor i, (train_split, val_split) in enumerate(skf):\n    K.clear_session()\n    X, y, X_val, y_val = X_train[train_split], y_train[train_split], X_train[val_split], y_train[val_split]\n    checkpoint = ModelCheckpoint('best_%d.h5'%i, monitor='val_loss', verbose=1, save_best_only=True)\n    early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=5)\n    tb = TensorBoard(log_dir='./logs/' + PREDICTION_FOLDER + '/fold_%i'%i, write_graph=True)\n    callbacks_list = [checkpoint, early, tb]\n    print(\"#\"*50)\n    print(\"Fold: \", i)\n    model = get_2d_conv_model(config)\n    history = model.fit(X, y, validation_data=(X_val, y_val), callbacks=callbacks_list, \n                        batch_size=64, epochs=config.max_epochs)\n    model.load_weights('best_%d.h5'%i)\n\n    # Save train predictions\n    predictions = model.predict(X_train, batch_size=64, verbose=1)\n    np.save(PREDICTION_FOLDER + \"/train_predictions_%d.npy\"%i, predictions)\n\n    # Save test predictions\n    predictions = model.predict(X_test, batch_size=64, verbose=1)\n    np.save(PREDICTION_FOLDER + \"/test_predictions_%d.npy\"%i, predictions)\n\n    # Make a submission file\n    top_3 = np.array(LABELS)[np.argsort(-predictions, axis=1)[:, :3]]\n    predicted_labels = [' '.join(list(x)) for x in top_3]\n    test['label'] = predicted_labels\n    test[['label']].to_csv(PREDICTION_FOLDER + \"/predictions_%d.csv\"%i)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4283b410e8588430c5ce3b12c2632d9da22da549"},"cell_type":"markdown","source":"# Ensembling 2D Conv Predictions"},{"metadata":{"trusted":true,"_uuid":"d12f321a5956424d04ad414ecda4f37a97237ebf","collapsed":true},"cell_type":"code","source":"pred_list = []\nfor i in range(10):\n    pred_list.append(np.load(\"../input/freesound-prediction-file/test_predictions_%d.npy\"%i))\nprediction = np.ones_like(pred_list[0])\nfor pred in pred_list:\n    prediction = prediction*pred\nprediction = prediction**(1./len(pred_list))\n# Make a submission file\ntop_3 = np.array(LABELS)[np.argsort(-prediction, axis=1)[:, :3]]\npredicted_labels = [' '.join(list(x)) for x in top_3]\ntest = pd.read_csv('../input/freesound-audio-tagging/sample_submission.csv')\ntest['label'] = predicted_labels\ntest[['fname', 'label']].to_csv(\"2d_conv_ensembled_submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"7b2e2bfbdb082ccb31afc984d4e355e056af5893"},"cell_type":"markdown","source":"# Ensembling 1D Conv and 2D Conv Predictions"},{"metadata":{"trusted":true,"_uuid":"1a5fd9c8c1d97f13d4b5fa72f646526341f04608","collapsed":true},"cell_type":"code","source":"pred_list = []\nfor i in range(10):\n    pred_list.append(np.load(\"../input/freesound-prediction-data-2d-conv-reduced-lr/test_predictions_%d.npy\"%i))\nfor i in range(10):\n    pred_list.append(np.load(\"../input/freesound-prediction-file/test_predictions_%d.npy\"%i))\nprediction = np.ones_like(pred_list[0])\nfor pred in pred_list:\n    prediction = prediction*pred\nprediction = prediction**(1./len(pred_list))\n# Make a submission file\ntop_3 = np.array(LABELS)[np.argsort(-prediction, axis=1)[:, :3]]\npredicted_labels = [' '.join(list(x)) for x in top_3]\ntest = pd.read_csv('../input/freesound-audio-tagging/sample_submission.csv')\ntest['label'] = predicted_labels\ntest[['fname', 'label']].to_csv(\"1d_2d_ensembled_submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"3dcfca3680d1f8b3cf17fba684531deaf58432ce"},"cell_type":"code","source":"\n\n\n\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}