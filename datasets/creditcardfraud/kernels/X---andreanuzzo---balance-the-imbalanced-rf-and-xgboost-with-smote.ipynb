{"cells":[{"metadata":{"_uuid":"3538ac4ddb8248fdfce9d05014ee7a7349cbcc2a"},"cell_type":"markdown","source":"# Fraud analysis: \n### Random Forest, XGBoost, OneClassSVM, Multivariate GMM and SMOTE, all in one cage against an imbalanced dataset."},{"metadata":{"_uuid":"feb14cf73017cd67e8ed59b3746b22bb7028e912","trusted":false,"collapsed":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2490ced3f5e64cb8ed4bcf3e907e43f6bdfbb02e"},"cell_type":"markdown","source":"## 1. Supervised learning tests"},{"metadata":{"_uuid":"9ddc8c24a9bb494660ac8476a5b19302ffa93d55"},"cell_type":"markdown","source":"I will now test a series of different machine learning models (no Neural Networks!) to see which one performs better, with some optimization here and there. "},{"metadata":{"_uuid":"43df64564cb373818d82c0b2a83066dcef61194f"},"cell_type":"markdown","source":"### 1.1 Data import, quick view and functions"},{"metadata":{"_uuid":"afae203dcf06eccae57b0a0ea45984eb9108ce79","trusted":false,"collapsed":true},"cell_type":"code","source":"df = pd.read_csv('../input/creditcard.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3289c056bf8351ef885d16abb65c8dc3ffea8733","trusted":false,"collapsed":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"181b821d52734c387bea3fd1bf8a485965d294fe"},"cell_type":"markdown","source":"I wonder if it should be treated as a data series rather than a table... "},{"metadata":{"_uuid":"dae84b43aa4779acb5b89a1f2137d895e90701d1","trusted":false,"collapsed":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"43232ffaef23d5612e274185efb739c325aa9d3b"},"cell_type":"markdown","source":"Check for NaNs"},{"metadata":{"_uuid":"3d5bbc84ce2350bae5e9053721c76080be68d456","trusted":false,"collapsed":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a061ee6869c16efdb138cfc41e614b44d64d93f5"},"cell_type":"markdown","source":"WOW! Seriously, no NaNs? \n\nOk, let's check for the classes distributions"},{"metadata":{"_uuid":"a61f8290df1f6d5df25d4e70b6a5f91fe0acfd10","trusted":false,"collapsed":true},"cell_type":"code","source":"print('Fraud \\n',df.Time[df.Class==1].describe(),'\\n',\n      '\\n Non-Fraud \\n',df.Time[df.Class==0].describe())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"68c4be4d327df1b07d216ccb807752ae5bd9836d"},"cell_type":"markdown","source":"Imbalanced dataset. Might be worth to work on upsampling/downsampling of the data, but I will try without it for the moment and hope I get good results. Now let's check which variable is more correlated with the fraudulent activities. "},{"metadata":{"_uuid":"82adb2df6ad8e01d34437bfbc80f329b8f607ebe","scrolled":true,"trusted":false,"collapsed":true},"cell_type":"code","source":"plt.figure(figsize=(12,30*4))\nimport matplotlib.gridspec as gridspec\nfeatures = df.iloc[:,0:30].columns\ngs = gridspec.GridSpec(30, 1)\nfor i, feature in enumerate(df[features]):\n    ax = plt.subplot(gs[i])\n    sns.distplot(df[feature][df.Class == 1], bins=50)\n    sns.distplot(df[feature][df.Class == 0], bins=50)\n    ax.set_xlabel('')\n    ax.set_title('Feature: ' + str(feature))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7d05bc05e0e082f6e996c247b3c5f4c25e421ad0"},"cell_type":"markdown","source":"Remove the features that do not have significantly different distributions between the two classes (i.e. will not contribute to our model)."},{"metadata":{"_uuid":"ce8b309791a1e919413224dda93d498f0aad1668","trusted":false,"collapsed":true},"cell_type":"code","source":"df2 = df.drop(['V15','V20','V22','V23','V25','V28', 'Time', 'Amount'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3c9c336f1e124c550702f82c55b6911e6ee1f077"},"cell_type":"markdown","source":"Data preparation and general functions for plots"},{"metadata":{"_uuid":"0bd86fbd5987e1ba770a0033f01e9fe8ad55f0e9","trusted":false,"collapsed":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\ndef plot_cm(classifier, predictions):\n    cm = confusion_matrix(y_test, predictions)\n    \n    plt.clf()\n    plt.imshow(cm, interpolation='nearest', cmap='RdBu')\n    classNames = ['Normal','Fraud']\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    tick_marks = np.arange(len(classNames))\n    plt.xticks(tick_marks, classNames, rotation=45)\n    plt.yticks(tick_marks, classNames)\n    s = [['TN','FP'], ['FN', 'TP']]\n    \n    for i in range(2):\n        for j in range(2):\n            plt.text(j,i, str(s[i][j])+\" = \"+str(cm[i][j]), \n                     horizontalalignment='center', color='White')\n    \n    plt.show()\n        \n    tn, fp, fn, tp = cm.ravel()\n\n    recall = tp / (tp + fn)\n    precision = tp / (tp + fp)\n    F1 = 2*recall*precision/(recall+precision)\n\n    print('Recall={0:0.3f}'.format(recall),'\\nPrecision={0:0.3f}'.format(precision))\n    print('F1={0:0.3f}'.format(F1))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d27167875d1253e1cc9824d455e14817b90e0e88","trusted":false,"collapsed":true},"cell_type":"code","source":"from sklearn.metrics import average_precision_score, precision_recall_curve\ndef plot_aucprc(classifier, scores):\n    precision, recall, _ = precision_recall_curve(y_test, scores, pos_label=0)\n    average_precision = average_precision_score(y_test, scores)\n\n    print('Average precision-recall score: {0:0.3f}'.format(\n          average_precision))\n\n    plt.plot(recall, precision, label='area = %0.3f' % average_precision, color=\"green\")\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('Recall')\n    plt.ylabel('Precision')\n    plt.title('Precision Recall Curve')\n    plt.legend(loc=\"best\")\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9de66416474650a228164e076f17f5993ad76e8c","trusted":false,"collapsed":true},"cell_type":"code","source":"# Splitting the dataset into the Training set and Test set\nfrom sklearn.model_selection import train_test_split\nX = df2.iloc[:,:-1]\ny = df2.iloc[:,-1]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f6bdc69159f6e3ec4f9f83ddcd95b191e2a38715"},"cell_type":"markdown","source":"## 1.2. Test a Random Forest model"},{"metadata":{"_uuid":"66830bd8921b121869fbf71de8d60a2b629d0f72","trusted":false,"collapsed":true},"cell_type":"code","source":"# Fitting Random Forest Classification to the Training set\nfrom sklearn.ensemble import RandomForestClassifier","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"66830bd8921b121869fbf71de8d60a2b629d0f72","trusted":false,"collapsed":true},"cell_type":"code","source":"pre = RandomForestClassifier(n_jobs=-1, random_state = 42,\n                             max_features= 'sqrt', \n                             criterion = 'entropy')\npre.fit(X_train, y_train)\n\n#Make predictions\ny_pred = pre.predict(X_test)\ntry:\n    scores = pre.decision_function(X_test)\nexcept:\n    scores = pre.predict_proba(X_test)[:,1]\n\n#Make plots\nplot_cm(pre, y_pred)\nplot_aucprc(pre, scores)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e3c42419390aa41dd70dd6aa9d4c3ac4ff0bc519"},"cell_type":"markdown","source":"The F-1 score is not that bad! Let's try to fine tune some parameters and see if we can improve that.\n\n*Note: since it takes too long for the Kaggle kernel, I executed it on my computer and here I am just showing the results of the GridSearchCV*"},{"metadata":{"_uuid":"eed4849b80211f9ebdf616e8888ceb19f42b4c12","scrolled":true,"trusted":false,"collapsed":true},"cell_type":"code","source":"#from sklearn.model_selection import GridSearchCV\n#param_grid = { \n#    'n_estimators': [10, 500],\n#    'max_features': ['auto', 'sqrt', 'log2'],\n#    'min_samples_leaf' : [len(X)//10000, len(X)//28000, \n#                          len(X)//50000, len(X)//100000]\n#}\n\n#CV_rfc = GridSearchCV(estimator=pre, \n#                      param_grid=param_grid, \n#                      scoring = 'f1',\n#                      cv=10, \n#                      n_jobs=10,\n#                      verbose=2,\n#                      pre_dispatch='2*n_jobs',\n#                      refit=False)\n#CV_rfc.fit(X_train, y_train)\n\n#CV_rfc.best_params_","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9705fb8afa40c8eea2ceff47ea2c8e179af68386","trusted":false,"collapsed":true},"cell_type":"code","source":"#rfc = RandomForestClassifier(n_jobs=-1, random_state = 42,\n#                             n_estimators=CV_rfc.best_params_['n_estimators'], \n#                             min_samples_leaf=CV_rfc.best_params_['min_samples_leaf'], \n#                             max_features= CV_rfc.best_params_['max_features'])\n\n#RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n#            max_depth=None, max_features='auto', max_leaf_nodes=None,\n#            min_impurity_split=1e-07, min_samples_leaf=2,\n#            min_samples_split=2, min_weight_fraction_leaf=0.0,\n#            n_estimators=500, n_jobs=-1, oob_score=False, random_state=42,\n#            verbose=0, warm_start=False)\n\n\nrfc = RandomForestClassifier(n_jobs=-1, random_state = 42,\n                             n_estimators=500, \n                             max_features='auto',\n                             min_samples_leaf=2,\n                             criterion = 'entropy')\n\nrfc.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5df5beb20f5e0b4e3b2aa882eb672a926cb377e7","trusted":false,"collapsed":true},"cell_type":"code","source":"#Make predictions\ny_pred = rfc.predict(X_test)\ntry:\n    scores = rfc.decision_function(X_test)\nexcept:\n    scores = rfc.predict_proba(X_test)[:,1]\n\n#Make plots\nplot_cm(rfc, y_pred)\nplot_aucprc(rfc, scores)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"213b141fad0dcc149977d73e23e36ec91081e070"},"cell_type":"markdown","source":"Yass! Nice increase! Now let's see if I can get any better with the most popular boosting algorithm..."},{"metadata":{"_uuid":"40146de5022c370475ca407b6d36ee2a0f17dd90"},"cell_type":"markdown","source":"## 1.3. Test a XGBoost model"},{"metadata":{"_uuid":"35a52da57e7a2b0b70c8473e19ad713cf2fa3462","trusted":false,"collapsed":true},"cell_type":"code","source":"# Fitting XGBoost to the Training set\nfrom xgboost import XGBClassifier\nxgb = XGBClassifier(random_state = 42, n_jobs = -1)\nxgb.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4a2e2f134261e03e737b257d056ebff5702296cd","collapsed":true,"trusted":false},"cell_type":"code","source":"#Make predictions\ny_pred = xgb.predict(X_test)\ntry:\n    scores = xgb.decision_function(X_test)\nexcept:\n    scores = xgb.predict_proba(X_test)[:,1]\n#Make plots\ny_pred = xgb.predict(X_test)\nplot_cm(xgb, y_pred)\nplot_aucprc(xgb, scores)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c3cb20be4afca569aec2faff9d635ab764ced733"},"cell_type":"markdown","source":"Ok, now we're talking. Any chances of getting better with optimization?"},{"metadata":{"_uuid":"37fa0f5dd5b7783b8b42e90497e8d7b356019ec2","collapsed":true,"trusted":false},"cell_type":"code","source":"fraud_ratio=y_train.value_counts()[1]/y_train.value_counts()[0]\nfrom sklearn.model_selection import GridSearchCV\nparam_grid = {'max_depth': [1,3,5], \n             'min_child_weight': [1,3,5], \n             'n_estimators': [100,200,500,1000], \n             'scale_pos_weight': [1, 0.1, 0.01, fraud_ratio]}","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c965476a0c41cc8d31d2054c2bf2f15a98111606","collapsed":true,"trusted":false},"cell_type":"code","source":"#CV_GBM = GridSearchCV(estimator = xgb, \n#                      param_grid = param_grid,\n#                      scoring = 'f1', \n#                      cv = 10, \n#                      n_jobs = -1,\n#                      refit = True)\n\n#CV_GBM.fit(X_train, y_train)\n\n#CV_GBM.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"f2ce3f9ff2dea4324db2901d378af6465001a059"},"cell_type":"code","source":"#optimized_GBM = XGBClassifier(n_jobs=-1, random_state = 42,\n#                             n_estimators=CV_GBM.best_params_['n_estimators'], \n#                             max_depth=CV_GBM.best_params_['max_depth'],\n#                             min_child_weight=CV_GBM.best_params_['min_child_weight'],\n#                             criterion = 'entropy')\noptimized_GBM = XGBClassifier(n_jobs=-1, random_state = 42,\n                             n_estimators=100, \n                             max_depth=1,\n                             min_child_weight=1,\n                             criterion = 'entropy',\n                             scale_pos_weight=fraud_ratio)\noptimized_GBM.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7a070daf6cb3b3fe628082fc31f34e5967dcfae7","collapsed":true,"trusted":false},"cell_type":"code","source":"#Make predictions\ny_pred = optimized_GBM.predict(X_test)\ntry:\n    scores = optimized_GBM.decision_function(X_test)\nexcept:\n    scores = optimized_GBM.predict_proba(X_test)[:,1]\n    \n#Make plots\nplot_cm(optimized_GBM, y_pred)\nplot_aucprc(optimized_GBM, scores)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7e18dbff4473bfbc699f4b434b9bf1c82bd576d5"},"cell_type":"markdown","source":"## 1.4. OneClassSVM?"},{"metadata":{"_uuid":"913816f909dd645d91dcb3dc2f8225db3cf7d811"},"cell_type":"markdown","source":"This should be, according to Scikit-learn tutorials, the best algorithm to infer anomalies in an imbalanced dataset. Let's give it a try."},{"metadata":{"_uuid":"6ac4cb433a9238878a48011600b34f5486e31e44","collapsed":true,"trusted":false},"cell_type":"code","source":"# Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)\n\n# Fitting SVM to the Training set\nfrom sklearn.svm import OneClassSVM\nclassifier = OneClassSVM(kernel=\"rbf\", random_state = 42)\nclassifier.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"50603a077e544c4df7c249c4a778b511d531ff77","collapsed":true,"trusted":false},"cell_type":"code","source":"#Make predictions\ny_pred = classifier.predict(X_test)\ny_pred = np.array([y==-1 for y in y_pred])\n\ntry:\n    scores = classifier.decision_function(X_test)\nexcept:\n    scores = classifier.predict_proba(X_test)[:,1]\n\n#Make plots\nplot_cm(classifier, y_pred)\nplot_aucprc(classifier, scores)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"21ed97069ccb67e7c5f9b725b7a60b4bde397254"},"cell_type":"markdown","source":"I don't really like these results, honestly... "},{"metadata":{"_uuid":"4eb58fa2cd2b9aba8f2f6555342d0712d45fd4dc"},"cell_type":"markdown","source":"## 1.5. Test a (Multivariate) GMM module"},{"metadata":{"_uuid":"7343b683ddd8cf146d2018bac6d4089068756f76"},"cell_type":"markdown","source":"Per Ng's lessons, we should divide the dataset into a training set with ONLY normal transactions, and validation and test set containing all the fraudulent transations. I will try at first without crossvalidation."},{"metadata":{"_uuid":"38774924e0fc0b85108bc723e4d1dd3f704d10ae","collapsed":true,"trusted":false},"cell_type":"code","source":"df3 = df2#.sample(frac = 0.1, random_state=42)\ntrain = df3[df3.Class==0].sample(frac=0.75, random_state = 42)\n\nX_train = train.iloc[:,:-1]\ny_train = train.iloc[:,-1]\n\nX_test = df3.loc[~df3.index.isin(X_train.index)].iloc[:,:-1]#.sample(frac=.50, random_state = 42)\ny_test = df3.loc[~df3.index.isin(y_train.index)].iloc[:,-1]#.sample(frac=.50, random_state = 42)\n\n#X_cval = df3.loc[~df3.index.isin(X_test.index)& ~df3.index.isin(X_train.index)].iloc[:,:-1]\n#y_cval = df3.loc[~df3.index.isin(y_test.index)& ~df3.index.isin(X_train.index)].iloc[:,-1]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"16c0eb21d7d15da1719f632bea8771174c8401f1","collapsed":true,"trusted":false},"cell_type":"code","source":"print('df3', df3.shape,'\\n',\n      'train',train.shape,'\\n',\n      'X_train',X_train.shape,'\\n',\n      'y_train',y_train.shape,'\\n',\n      'X_test',X_test.shape,'\\n',\n      'y_test',y_test.shape,'\\n', \n      #'X_val',X_cval.shape,'\\n',\n      #'y_val',y_cval.shape,'\\n'\n     )","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a83638c678179de1a68368c4901c47661d7cdf47","collapsed":true,"trusted":false},"cell_type":"code","source":"df3.shape[0] == train.shape[0] + X_test.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d1bcbba99f112c7a0abf95032e18239dfa96ddad","collapsed":true,"trusted":false},"cell_type":"code","source":"def covariance_matrix(X):\n    X = X.values\n    m, n = X.shape \n    tmp_mat = np.zeros((n, n))\n    mu = X.mean(axis=0)\n    for i in range(m):\n        tmp_mat += np.outer(X[i] - mu, X[i] - mu)\n    return tmp_mat / m","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"727ef169724e03e98621c0c01612a93b3654cbe9","collapsed":true,"trusted":false},"cell_type":"code","source":"cov_mat = covariance_matrix(X_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c8674cc5905427378a4f8b330f64a8a74d48cf26","collapsed":true,"trusted":false},"cell_type":"code","source":"cov_mat_inv = np.linalg.pinv(cov_mat)\ncov_mat_det = np.linalg.det(cov_mat)\ndef multi_gauss(x):\n    n = len(cov_mat)\n    return (np.exp(-0.5 * np.dot(x, np.dot(cov_mat_inv, x.T))) \n            / (2. * np.pi)**(n/2.) \n            / np.sqrt(cov_mat_det))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c6da5449f52589a041d0cd19f3cdab485dfd4d75","collapsed":true,"trusted":false},"cell_type":"code","source":"eps = min([multi_gauss(x) for x in X_train.values])\npredictions = np.array([multi_gauss(x) <= eps for x in X_test.values])\ny_test = np.array(y_test, dtype=bool)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"64b4944ef5a045afcdf0db108b717dc507b75c73","collapsed":true,"trusted":false},"cell_type":"code","source":"cm = confusion_matrix(y_test, predictions)\n\nplt.clf()\nplt.imshow(cm, interpolation='nearest', cmap='RdBu')\nclassNames = ['Normal','Fraud']\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\ntick_marks = np.arange(len(classNames))\nplt.xticks(tick_marks, classNames, rotation=45)\nplt.yticks(tick_marks, classNames)\ns = [['TN','FP'], ['FN', 'TP']]\n\nfor i in range(2):\n    for j in range(2):\n        plt.text(j,i, str(s[i][j])+\" = \"+str(cm[i][j]), \n                 horizontalalignment='center', color='White')\n\nplt.show()\n\ntn, fp, fn, tp = cm.ravel()\n\nrecall = tp / (tp + fn)\nprecision = tp / (tp + fp)\nF1 = 2*recall*precision/(recall+precision)\n\nprint(\"recall=\",recall,\"\\nprecision=\",precision)\nprint(\"F1=\",F1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ee53c88f1748d4bf41598a6f0e99333755ffc36b"},"cell_type":"markdown","source":"Adapting NG's code from MatLab"},{"metadata":{"_uuid":"9fff95c4656433e4409a397a2e96dd526bbcf29b","collapsed":true,"trusted":false},"cell_type":"code","source":"from scipy.stats import multivariate_normal\nfrom sklearn.metrics import f1_score\n\ndef feature_normalize(dataset):\n    mu = np.mean(dataset,axis=0)\n    sigma = np.std(dataset,axis=0)\n    return (dataset - mu)/sigma\n\ndef estimateGaussian(dataset):\n    mu = np.mean(dataset, axis=0)\n    sigma = np.cov(dataset.T)\n    return mu, sigma\n    \ndef multivariateGaussian(dataset,mu,sigma):\n    p = multivariate_normal(mean=mu, cov=sigma, allow_singular=True)\n    return p.pdf(dataset)\n\ndef selectThresholdByCV(probs,gt):\n    best_epsilon = 0\n    best_f1 = 0\n    f = 0\n    stepsize = (max(probs) - min(probs)) / 1000;\n    epsilons = np.arange(min(probs),max(probs),stepsize)\n    for epsilon in np.nditer(epsilons):\n        predictions = (probs < epsilon)\n        f = f1_score(gt, predictions, average = \"binary\")\n        if f > best_f1:\n            best_f1 = f\n            best_epsilon = epsilon\n    return best_f1, best_epsilon","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"da421aa6604ef2dbbbe54eaaf7c4c7105888b4be","collapsed":true,"trusted":false},"cell_type":"code","source":"#fit the model\nmu, sigma = estimateGaussian(X_train)\np = multivariateGaussian(X_train,mu,sigma)\n\np_cv = multivariateGaussian(X_test,mu,sigma)\nfscore, ep = selectThresholdByCV(p_cv,y_test)\noutliers = np.asarray(np.where(p < ep))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7bf25a124cf9c5151c3ddef9f22f3149c1cd34cc","collapsed":true,"trusted":false},"cell_type":"code","source":"predictions = np.array([p_cv <= ep]).transpose()\ny_test = np.array(y_test, dtype=bool)\n\ncm = confusion_matrix(y_test, predictions)\n\nplt.clf()\nplt.imshow(cm, interpolation='nearest', cmap='RdBu')\nclassNames = ['Normal','Fraud']\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\ntick_marks = np.arange(len(classNames))\nplt.xticks(tick_marks, classNames, rotation=45)\nplt.yticks(tick_marks, classNames)\ns = [['TN','FP'], ['FN', 'TP']]\n\nfor i in range(2):\n    for j in range(2):\n        plt.text(j,i, str(s[i][j])+\" = \"+str(cm[i][j]), \n                 horizontalalignment='center', color='White')\n\nplt.show()\n\ntn, fp, fn, tp = cm.ravel()\n\nrecall = tp / (tp + fn)\nprecision = tp / (tp + fp)\nF1 = 2*recall*precision/(recall+precision)\n\nprint(\"recall=\",recall,\"\\nprecision=\",precision)\nprint(\"F1=\",F1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7257538909f35eba222a9447c1d4fdc705fa925a"},"cell_type":"markdown","source":"## 2.  Working on imbalanced dataset: upsampling of the underrepresented class"},{"metadata":{"_uuid":"1690f6aecec7b5482013260de3c796e04dd484bf"},"cell_type":"markdown","source":"## 2.1 BalancedBaggingClassifier"},{"metadata":{"_uuid":"dbcdb9638e6a4f7f8787b807b3bc7b5afc2a9530"},"cell_type":"markdown","source":"The package imbalanced-learn (not yet part of the official scikitlearn) contains an imbalanced classifier which should be able, using a bagging method, to increase our prediction capabilities without resampling the dataset."},{"metadata":{"_uuid":"9bc3932b9f890b3ca7623962a8bec284e5263282"},"cell_type":"markdown","source":"First, let's reset our original dataset, without the unwanted features."},{"metadata":{"_uuid":"9de66416474650a228164e076f17f5993ad76e8c","trusted":false,"collapsed":true},"cell_type":"code","source":"# Splitting the dataset into the Training set and Test set\nfrom sklearn.model_selection import train_test_split\nX = df2.iloc[:,:-1]\ny = df2.iloc[:,-1]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"46b1270c04e2ff9a6eb8fa3175bb35caea7d5d50","collapsed":true,"trusted":false},"cell_type":"code","source":"from sklearn.ensemble import BaggingClassifier\nfrom imblearn.ensemble import BalancedBaggingClassifier\n\nfrom imblearn.metrics import classification_report_imbalanced\n\nbagging = BaggingClassifier(random_state=0)\nbalanced_bagging = BalancedBaggingClassifier(random_state=0)\n\nbagging.fit(X_train, y_train)\nbalanced_bagging.fit(X_train, y_train)\n\n#Make predictions\nprint('Classification of original dataset with Bagging (scikit-learn)')\ny_pred = bagging.predict(X_test)\ntry:\n    scores = bagging.decision_function(X_test)\nexcept:\n    scores = bagging.predict_proba(X_test)[:,1]\n\n#Make plots\nplot_cm(bagging, y_pred)\nplot_aucprc(bagging, scores)\n\n#Make predictions\nprint('Classification of original dataset with BalancedBagging (imbalanced-learn)')\ny_pred = balanced_bagging.predict(X_test)\ntry:\n    scores = balanced_bagging.decision_function(X_test)\nexcept:\n    scores = balanced_bagging.predict_proba(X_test)[:,1]\n\n#Make plots\nplot_cm(balanced_bagging, y_pred)\nplot_aucprc(balanced_bagging, scores)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"64d0f0b9abbee1dfffc3d451307b98a6d5fa5992"},"cell_type":"markdown","source":"So, the imbalanced-learn packages without resampling of the data allows us to have higher true positive numbers, but lowers the true negative, meaning that it's just mistakenly saying there are more frauds than in reality. Not good.  \nLet's try again, with SMOTE, which will produce synthetical samples of the under-represented class"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"b233c77c30016d4856fbe897f7f6865276d49940"},"cell_type":"code","source":"from imblearn.over_sampling import SMOTE\n\nsmote = SMOTE(random_state=42)\nX_res, y_res = smote.fit_sample(X, y)\n\nX_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size = 0.25, random_state = 42)\n\n#fit the best models so far\nxgb.fit(X_train, y_train)\nrfc.fit(X_train, y_train)\n\n#Make predictions\nprint('Classification of SMOTE-resampled dataset with XGboost')\ny_pred = xgb.predict(X_test)\ntry:\n    scores = xgb.decision_function(X_test)\nexcept:\n    scores = xgb.predict_proba(X_test)[:,1]\n#Make plots\ny_pred = xgb.predict(X_test)\nplot_cm(xgb, y_pred)\nplot_aucprc(xgb, scores)\n\n#Make predictions\nprint('Classification of SMOTE-resampled dataset with optimized RF')\ny_pred = rfc.predict(X_test)\ntry:\n    scores = rfc.decision_function(X_test)\nexcept:\n    scores = rfc.predict_proba(X_test)[:,1]\n\n#Make plots\nplot_cm(rfc, y_pred)\nplot_aucprc(rfc, scores)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d0584c47f5f65ae9fcd9a91b2837bdadb71b72ff"},"cell_type":"markdown","source":"WOW! So, if we now use this new RF classifier (which parameters were optimized on the resampled dataset) on the ORIGINAL dataset, we'll get our perfect fraud analysis prediction."},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"d3d1e455321ab446be6e97690787202320718bdd"},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 42)\n\n#Make predictions\ny_pred = rfc.predict(X_test)\ntry:\n    scores = rfc.decision_function(X_test)\nexcept:\n    scores = rfc.predict_proba(X_test)[:,1]\n\n#Make plots\nplot_cm(rfc, y_pred)\nplot_aucprc(rfc, scores)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d939bae171148e579e84b7274325d36d68c093ba"},"cell_type":"markdown","source":"\"Pretty cool huh?\""},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"7e682be7477c5e5499e84ae7a69ba277e98f5945"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}