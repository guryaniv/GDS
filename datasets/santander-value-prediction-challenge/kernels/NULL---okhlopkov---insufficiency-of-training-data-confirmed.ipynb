{"cells":[{"metadata":{"_uuid":"15a8721cfded800e1bce088f6e398b3267f4eb60"},"cell_type":"markdown","source":"My idea was to find out the **training samples with the same feature sets but with different target values**. That will help me to **remove the data ambiguity** and let my models perform better on the leaderboard. And I found some! \n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Reading the data\nimport pandas as pd \ntrain = pd.read_csv(\"../input/train.csv\", index_col='ID')\n\n# Remove target to simplify the calculation of duplicates\ntarget = train.pop(\"target\")\n\n# Find duplicate rows\nt = train.duplicated(keep=False)\nduplicated_indexes = t[t].index.values\nprint(\"Indexes of duplicated rows: {}\".format(duplicated_indexes))\n\n# Show target values for selected indexes\ntarget.loc[duplicated_indexes]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"The key idea is that we can't predict target variable for sure for these users. Moreover, we can't train on that data, because these users features are the same. \n\nThe possible ways to solve this problem are:\n* Ask competition organizers to provide Kagglers with more features (however, I'm sure they won't do it),\n* Drop these users from training dataset to remove the ambiguity (but our training dataset is already very small)\n* Leave only one user in the dataset and change the target variable for him.\n\nThere are two basic approaches to how we can choose the best target variable for the remaining user.\n\n* Just mean value:\n$$\\text{New target} = \\frac{10000000.0 + 20000000.0}{2} = 15000000$$\n\n* Logarythmic mean:\n$$\\text{New target} = \\exp{\\frac{\\log{10000000.0} + \\log{20000000.0}}{2}} \\approx 14142135.623730922$$\n\nThe implementation of the last approach:"},{"metadata":{"trusted":true,"_uuid":"6092a972407459a22ce7ddede7abe71a9b8d0a33","collapsed":true},"cell_type":"code","source":"import numpy as np\nfirst_ind, second_ind = duplicated_indexes[0], duplicated_indexes[1]\nnew_target_val = np.exp((np.log(target.loc[first_ind]) + np.log(target.loc[second_ind])) / 2)\n\ntarget = target.drop(first_ind)\ntarget[second_ind] = new_target_val\ntrain = train.drop(first_ind)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6bc9b4923efb61668802a2eaf998fc5a91efd7d9"},"cell_type":"markdown","source":"One of the possible continuations of this research is to try to remove the columns containing almost all zeros and look at the target spread of the obtained duplicate rows, if any."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}