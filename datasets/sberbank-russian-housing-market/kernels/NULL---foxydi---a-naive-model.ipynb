{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "a976b8f9-414d-9802-7573-18b4bb6010b0"
      },
      "source": [
        "A naive model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "5986d91a-19da-7400-8838-86f451c6c9d3"
      },
      "outputs": [],
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns # visualization\n",
        "import xgboost as xgb # GBDT\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor # randomForest, GradientBoosting\n",
        "\n",
        "from sklearn import preprocessing # categorical values\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.preprocessing import Imputer\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor # VIF\n",
        "\n",
        "from scipy.stats import norm\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import normalize\n",
        "from scipy import stats # PCA\n",
        "\n",
        "import warnings \n",
        "warnings.filterwarnings('ignore')\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "53368997-c612-6c2d-b76e-97d906bc4bfb"
      },
      "outputs": [],
      "source": [
        "df_train = pd.read_csv(\"train.csv\", parse_dates=['timestamp'])\n",
        "df_test = pd.read_csv(\"test.csv\", parse_dates=['timestamp'])\n",
        "df_macro = pd.read_csv(\"macro.csv\", parse_dates=['timestamp'])\n",
        "\n",
        "# truncate the extreme values in price_doc\n",
        "ulimit = np.percentile(df_train.price_doc.values, 99)\n",
        "llimit = np.percentile(df_train.price_doc.values, 1)\n",
        "df_train['price_doc'].ix[df_train['price_doc']>ulimit] = ulimit\n",
        "df_train['price_doc'].ix[df_train['price_doc']<llimit] = llimit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "eea596dc-44bb-5776-28ec-bb2819d51613"
      },
      "outputs": [],
      "source": [
        "y_train = df_train['price_doc'].values\n",
        "id_test = df_test['id']\n",
        "\n",
        "df_train.drop(['id', 'price_doc'], axis=1, inplace=True)\n",
        "df_test.drop(['id'], axis=1, inplace=True)\n",
        "\n",
        "# build df_all = (df_train+df_test).join(df_macro)\n",
        "num_train = len(df_train)\n",
        "df_all = pd.concat([df_train, df_test])\n",
        "df_all = pd.merge_ordered(df_all, df_macro, on='timestamp', how='left')\n",
        "print(df_all.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c8fbd4eb-f7fb-2b17-b4c7-9ca07eb1ae6a"
      },
      "outputs": [],
      "source": [
        "# add month-year count\n",
        "month_year = (df_all.timestamp.dt.month + df_all.timestamp.dt.year * 100)\n",
        "month_year_cnt_map = month_year.value_counts().to_dict()\n",
        "df_all['month_year_cnt'] = month_year.map(month_year_cnt_map)\n",
        "\n",
        "# add week-year count\n",
        "week_year = (df_all.timestamp.dt.weekofyear + df_all.timestamp.dt.year * 100)\n",
        "week_year_cnt_map = week_year.value_counts().to_dict()\n",
        "df_all['week_year_cnt'] = week_year.map(week_year_cnt_map)\n",
        "\n",
        "# add year\n",
        "df_all['year'] = df_all.timestamp.dt.year\n",
        "\n",
        "# add month of year\n",
        "df_all['month_of_year'] = df_all.timestamp.dt.month\n",
        "\n",
        "# add week of year\n",
        "df_all['week_of_year'] = df_all.timestamp.dt.weekofyear\n",
        "\n",
        "# add day-of-week\n",
        "df_all['day_of_week'] = df_all.timestamp.dt.dayofweek\n",
        "\n",
        "# age of building\n",
        "df_all['build_year'][df_all['build_year']==20052009] = 2005\n",
        "df_all['build_year'][df_all['build_year']==0] = df_all['build_year'][0]\n",
        "df_all['build_year'][df_all['build_year']==1] = df_all['build_year'][0]\n",
        "df_all['build_year'][df_all['build_year']==3] = df_all['build_year'][0]\n",
        "df_all['build_year'][df_all['build_year']==71] = df_all['build_year'][0]\n",
        "df_all['build_year'][df_all['build_year']==4965] = df_all['build_year'][0]\n",
        "df_all['build_year'][df_all['build_year']==20] = 2000\n",
        "df_all['build_year'][df_all['build_year']==215] = 2015\n",
        "df_all['age_of_building'] = df_all['year'] - df_all['build_year']\n",
        "\n",
        "# remove timestamp column (may overfit the model in train)\n",
        "df_all.drop(['timestamp'], axis=1, inplace=True)\n",
        "print(df_all.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c611d7aa-9b73-50e8-d1c8-99f764e88aaa"
      },
      "outputs": [],
      "source": [
        "# floor/max_floor ratio\n",
        "df_all['rel_floor'] = df_all['floor'] / df_all['max_floor'].astype(float)\n",
        "\n",
        "# num of floor from top \n",
        "df_all[\"floor_from_top\"] = df_all[\"max_floor\"] - df_all[\"floor\"]\n",
        "\n",
        "# kitchen-full ratio\n",
        "df_all['kitchen_full'] = df_all['kitch_sq'] / df_all['full_sq'].astype(float)\n",
        "df_all['kitchen_full'].ix[df_all['kitchen_full']<0] = 0\n",
        "df_all['kitchen_full'].ix[df_all['kitchen_full']>1] = 1\n",
        "\n",
        "# kitchen-living ratio\n",
        "df_all['kitchen_living'] = df_all['kitch_sq'] / df_all['life_sq'].astype(float)\n",
        "df_all['kitchen_living'].ix[df_all['kitchen_living']<0] = 0\n",
        "df_all['kitchen_living'].ix[df_all['kitchen_living']>1] = 1\n",
        "\n",
        "# add living ratio\n",
        "df_all['living_ratio'] = df_all['life_sq']/df_all['full_sq']\n",
        "df_all['living_ratio'].ix[df_all['living_ratio']<0] = 0\n",
        "df_all['living_ratio'].ix[df_all['living_ratio']>1] = 1\n",
        "\n",
        "# add non-living area\n",
        "df_all['non_living'] = df_all['full_sq'] - df_all['life_sq']\n",
        "\n",
        "# add living area per room\n",
        "df_all['living_room'] = df_all['life_sq'] / df_all['num_room']\n",
        "\n",
        "# apartment condition\n",
        "df_all['state'][df_all['state']==33] = 3\n",
        "\n",
        "# add preschool ratio\n",
        "df_all[\"ratio_preschool\"] = df_all[\"children_preschool\"] / df_all[\"preschool_quota\"].astype(\"float\")\n",
        "\n",
        "# add school ratio\n",
        "df_all[\"ratio_school\"] = df_all[\"children_school\"] / df_all[\"school_quota\"].astype(\"float\")\n",
        "\n",
        "print(df_all.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "064077ee-368a-ee62-9fcc-676b234a961c"
      },
      "outputs": [],
      "source": [
        "# numerical and categorical data types\n",
        "df_all_dtype=df_all.dtypes\n",
        "display_nvar = len(df_all.columns)\n",
        "df_all_dtype_dict = df_all_dtype.to_dict()\n",
        "df_all.dtypes.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "1e6ba4a3-b0d4-37f4-e8fe-248bcac6d43b"
      },
      "outputs": [],
      "source": [
        "# deal with categorical values\n",
        "for f in df_all.columns:\n",
        "    if df_all[f].dtype=='object' and f not in ['sub_area']:\n",
        "        print(f) # there should be 18 categorical variables\n",
        "        lbl = preprocessing.LabelEncoder()\n",
        "        lbl.fit(list(df_all[f].values.astype('str')) + list(df_all[f].values.astype('str')))\n",
        "        df_all[f] = lbl.transform(list(df_all[f].values.astype('str')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "5e31c74a-a00a-ee1c-93c6-0afc827706b7"
      },
      "outputs": [],
      "source": [
        "# convert to numpy values\n",
        "df_all.drop(['sub_area'], axis=1, inplace=True)\n",
        "X_all = df_all.values\n",
        "print(X_all.shape)\n",
        "\n",
        "X_train = X_all[:num_train]\n",
        "X_test = X_all[num_train:]\n",
        "\n",
        "df_columns = df_all.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "bd63846c-5e27-6e88-3d3b-4a47abc4a82d"
      },
      "outputs": [],
      "source": [
        "# use VIF\n",
        "\n",
        "class ReduceVIF(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, thresh=5.0, impute=True, impute_strategy='median'):\n",
        "        # From looking at documentation, values between 5 and 10 are \"okay\".\n",
        "        # Above 10 is too high and so should be removed.\n",
        "        self.thresh = thresh\n",
        "        \n",
        "        # The statsmodel function will fail with NaN values, as such we have to impute them.\n",
        "        # By default we impute using the median value.\n",
        "        # This imputation could be taken out and added as part of an sklearn Pipeline.\n",
        "        if impute:\n",
        "            self.imputer = Imputer(strategy=impute_strategy)\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        print('ReduceVIF fit')\n",
        "        if hasattr(self, 'imputer'):\n",
        "            self.imputer.fit(X)\n",
        "        return self\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "        print('ReduceVIF transform')\n",
        "        columns = X.columns.tolist()\n",
        "        if hasattr(self, 'imputer'):\n",
        "            X = pd.DataFrame(self.imputer.transform(X), columns=columns)\n",
        "        return ReduceVIF.calculate_vif(X, self.thresh)\n",
        "\n",
        "    @staticmethod\n",
        "    def calculate_vif(X, thresh=5.0):\n",
        "        # Taken from https://stats.stackexchange.com/a/253620/53565 and modified\n",
        "        dropped=True\n",
        "        while dropped:\n",
        "            # Loop repeatedly until we find that all columns within our dataset\n",
        "            # have a VIF value we're happy with.\n",
        "            variables = X.columns\n",
        "            dropped=False\n",
        "            vif = []\n",
        "            new_vif = 0\n",
        "            for var in X.columns:\n",
        "                new_vif = variance_inflation_factor(X[variables].values, X.columns.get_loc(var))\n",
        "                vif.append(new_vif)\n",
        "                if np.isinf(new_vif):\n",
        "                    break\n",
        "            max_vif = max(vif)\n",
        "            if max_vif > thresh:\n",
        "                maxloc = vif.index(max_vif)\n",
        "                print('Dropping %s with vif= %f' % (X.columns[maxloc],max_vif))\n",
        "                X = X.drop([X.columns.tolist()[maxloc]], axis=1)\n",
        "                dropped=True\n",
        "        return X\n",
        "    \n",
        "#transformer = ReduceVIF()\n",
        "#X = transformer.fit_transform(df_train[df_train.columns[-10:]], y_train)\n",
        "#X.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "11f40dfe-8a13-0f39-2b4b-d1ca56900ca7"
      },
      "outputs": [],
      "source": [
        "xgb_params = {\n",
        "    'eta': 0.05,\n",
        "    'max_depth': 5,\n",
        "    'subsample': 0.7,\n",
        "    'colsample_bytree': 0.7,\n",
        "    'objective': 'reg:linear',\n",
        "    'eval_metric': 'rmse',\n",
        "    'silent': 1\n",
        "}\n",
        "\n",
        "dtrain = xgb.DMatrix(X_train, y_train, feature_names=df_columns)\n",
        "dtest = xgb.DMatrix(X_test, feature_names=df_columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "a970d3f7-09fd-0288-2144-4cb0c06855e0"
      },
      "outputs": [],
      "source": [
        "cv_output = xgb.cv(xgb_params, dtrain, num_boost_round=200, early_stopping_rounds=20,\n",
        "    verbose_eval=50, show_stdv=False)\n",
        "cv_output[['train-rmse-mean', 'test-rmse-mean']].plot()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "6dcbe161-7a4f-5a33-9ad9-d88aab2b61fb"
      },
      "outputs": [],
      "source": [
        "num_boost_rounds = len(cv_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "6fd446c7-d660-048b-e7da-8b404c51a589"
      },
      "outputs": [],
      "source": [
        "model = xgb.train(dict(xgb_params, silent=0), dtrain, num_boost_round=num_boost_rounds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "3a05c4df-719e-5e29-a287-726dafcdfc12"
      },
      "outputs": [],
      "source": [
        "y_pred = model.predict(dtest)\n",
        "\n",
        "df_sub = pd.DataFrame({'id': id_test, 'price_doc': y_pred})\n",
        "\n",
        "df_sub.to_csv('sub.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ededca79-9c29-aa96-5d27-c3141a54731f"
      },
      "outputs": [],
      "source": [
        "X_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train, random_state=1848)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "2bdcb744-464f-c9f9-268c-624979a7a208"
      },
      "outputs": [],
      "source": [
        "# sklearn_boost = GradientBoostingRegressor(random_state=1849)\n",
        "# sklearn_boost.fit(X_train, y_train)\n",
        "# print('Training Error: {:.3f}'.format(1 - sklearn_boost.score(X_train, y_train)))\n",
        "# print('Validation Error: {:.3f}'.format(1 - sklearn_boost.score(X_validation, y_validation)))\n",
        "# %timeit sklearn_boost.fit(X_train, y_train.values.ravel())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c0b5b25c-739b-0541-6043-2ff31ff567c4"
      },
      "outputs": [],
      "source": [
        "# random_forest = RandomForestRegressor(random_state=1852)\n",
        "# random_forest.fit(X_train, y_train)\n",
        "# print('Training Error: {:.3f}'.format(1 - random_forest.score(X_train, y_train)))\n",
        "# print('Validation Error: {:.3f}'.format(1 - random_forest.score(X_validation, y_validation)))\n",
        "# %timeit random_forest.fit(X_train, y_train.values.ravel())"
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}