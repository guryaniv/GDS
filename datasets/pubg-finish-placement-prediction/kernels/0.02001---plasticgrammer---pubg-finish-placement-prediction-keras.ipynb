{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":false,"_kg_hide-output":false},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport itertools\nimport gc\nimport os\nimport sys\n\nsns.set_style('darkgrid')\nsns.set_palette('bone')\npd.options.display.float_format = '{:,.3f}'.format\n\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1b7c01604a8239f1222e7a7dc9d8562af10881f0","_kg_hide-output":false,"_kg_hide-input":true},"cell_type":"code","source":"def toTapleList(list1,list2):\n    return list(itertools.product(list1,list2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3519f56ce1cefbb965fc84b4890b5093f0f3d05f","_kg_hide-input":true,"_kg_hide-output":false},"cell_type":"code","source":"# Memory saving function credit to https://www.kaggle.com/gemartin/load-data-reduce-memory-usage\ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.\n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                #if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                #    df[col] = df[col].astype(np.float16)\n                #el\n                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        #else:\n            #df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB --> {:.2f} MB (Decreased by {:.1f}%)'.format(\n        start_mem, end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e220d486a8b54f0ae23d9136c40a7c6b428dd95b"},"cell_type":"markdown","source":"# Load data"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"%%time\ntrain = pd.read_csv('../input/train_V2.csv')\ntrain = reduce_mem_usage(train)\ntest = pd.read_csv('../input/test_V2.csv')\ntest = reduce_mem_usage(test)\nprint(train.shape, test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fbe4879dd44a73edffdc100d0158e551bf8edcb3","_kg_hide-input":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ed20e8129574689a336d934a5ee6ecbabcfb0186","_kg_hide-input":true},"cell_type":"code","source":"null_cnt = train.isnull().sum().sort_values()\nprint('null count:', null_cnt[null_cnt > 0])\n# dropna\ntrain.dropna(inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"10f263d9520b1041fea4184892e2c66ecbd7087f","scrolled":false,"_kg_hide-input":true},"cell_type":"code","source":"train.describe(include=np.number).drop('count').T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c4c2f216b4a5844210786103c0b53325d2621f87"},"cell_type":"markdown","source":"# Feature Engineering"},{"metadata":{"trusted":true,"_uuid":"f380c60d3e7f1d7c30ef85e003cef3e29044df74","scrolled":true},"cell_type":"code","source":"all_data = train.append(test, sort=False).reset_index(drop=True)\ndel train, test\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"669b837c115c19d61b40c79a7d3e7e72bb5e3474"},"cell_type":"markdown","source":"## new feature"},{"metadata":{"trusted":true,"_uuid":"5d077f08112b044cc360905361e366cdf1967271"},"cell_type":"code","source":"def fillInf(df, val):\n    numcols = df.select_dtypes(include='number').columns\n    cols = numcols[numcols != 'winPlacePerc']\n    df[df == np.Inf] = np.NaN\n    df[df == np.NINF] = np.NaN\n    for c in cols: df[c].fillna(val, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5bc388eab6eaba7077d715460011bc26199be2cb"},"cell_type":"code","source":"all_data['_totalDistance'] = all_data['rideDistance'] + all_data['walkDistance'] + all_data['swimDistance']\nall_data['_healthItems'] = all_data['heals'] + all_data['boosts']\nall_data['_headshotKillRate'] = all_data['headshotKills'] / all_data['kills']\nall_data['_killPlaceOverMaxPlace'] = all_data['killPlace'] / all_data['maxPlace']\nall_data['_killsOverWalkDistance'] = all_data['kills'] / all_data['walkDistance']\n#all_data['_killsOverDistance'] = all_data['kills'] / all_data['_totalDistance']\n#all_data['_walkDistancePerSec'] = all_data['walkDistance'] / all_data['matchDuration']\n\nfillInf(all_data, 0)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a463600c04fe5adb2d03c25eff03c7a7599f8d56"},"cell_type":"markdown","source":"## rank as percent"},{"metadata":{"trusted":true,"_uuid":"cee11a0d9059638278ac054edd0b352b75f7a644"},"cell_type":"code","source":"match = all_data.groupby('matchId')\nall_data['killsPerc'] = match['kills'].rank(pct=True).values\nall_data['killPlacePerc'] = match['killPlace'].rank(pct=True).values\nall_data['walkDistancePerc'] = match['walkDistance'].rank(pct=True).values\n#all_data['damageDealtPerc'] = match['damageDealt'].rank(pct=True).values\n\nall_data['walkPerc_killsPerc'] = all_data['walkDistancePerc'] / all_data['killsPerc']\n#all_data['walkPerc_kills'] = all_data['walkDistancePerc'] / all_data['kills']\n#all_data['kills_walkPerc'] = all_data['kills'] / all_data['walkDistancePerc']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"26244216aa5995036da9485d3d8188031c43a10d"},"cell_type":"markdown","source":"## drop feature"},{"metadata":{"trusted":true,"_uuid":"1765f2581a9f07e9616ced120cb3aafd6611015c"},"cell_type":"code","source":"#all_data.drop(['killStreaks','DBNOs'], axis=1, inplace=True)\nall_data.drop(['boosts','heals','revives','assists'], axis=1, inplace=True)\nall_data.drop(['headshotKills','roadKills','vehicleDestroys','teamKills'], axis=1, inplace=True)\nall_data.drop(['rideDistance','swimDistance','matchDuration'], axis=1, inplace=True)\nall_data.drop(['rankPoints','killPoints','winPoints'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f9ecb5ec4de85e60fde11b1fa9a8e8ec850094c6"},"cell_type":"markdown","source":"## grouping\n\n* need to predict the order of places for groups within each match.\n* train on group-level instead of the user-level"},{"metadata":{"trusted":true,"_uuid":"72b532cda47ca292bd0bfd5e2982f5d9aeab3401"},"cell_type":"code","source":"match = all_data.groupby(['matchId'])\ngroup = all_data.groupby(['matchId','groupId','matchType'])\n\n# target feature (max, min)\nagg_col = list(all_data.columns)\nexclude_agg_col = ['Id','matchId','groupId','matchType','maxPlace','numGroups','winPlacePerc']\nfor c in exclude_agg_col:\n    agg_col.remove(c)\nprint(agg_col)\n\n# target feature (sum)\nsum_col = ['kills','killPlace','damageDealt','walkDistance','_healthItems']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5318ce2a99f2960fef819a9793d939032e52c844"},"cell_type":"code","source":"''' match sum, match max, match mean, group sum\n'''\nmatch_data = pd.concat([\n    match.size().to_frame('m.players'), \n    match[sum_col].sum().rename(columns=lambda s: 'm.sum.' + s), \n    match[sum_col].max().rename(columns=lambda s: 'm.max.' + s),\n    match[sum_col].mean().rename(columns=lambda s: 'm.mean.' + s)\n    ], axis=1).reset_index()\nmatch_data = pd.merge(match_data, \n    group[sum_col].sum().rename(columns=lambda s: 'sum.' + s).reset_index())\nmatch_data = reduce_mem_usage(match_data)\n\nprint(match_data.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"270cbffd8766c34043d209258e64dbaee02a0a96"},"cell_type":"code","source":"''' ranking of kills and killPlace in each match\n'''\nminKills = all_data.sort_values(['matchId','groupId','kills','killPlace']).groupby(\n    ['matchId','groupId','kills']).first().reset_index().copy()\nfor n in np.arange(5):\n    c = 'kills_' + str(n) + '_Place'\n    nKills = (minKills['kills'] == n)\n    minKills.loc[nKills, c] = minKills[nKills].groupby(['matchId'])['killPlace'].rank().values\n    match_data = pd.merge(match_data, minKills[nKills][['matchId','groupId',c]], how='left')\n    match_data[c].fillna(0, inplace=True)\nmatch_data = reduce_mem_usage(match_data)\ndel minKills, nKills\n\nprint(match_data.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8ab91d60f601ab7bc4a997de22a7e60a0cd5ea07"},"cell_type":"code","source":"''' group mean, max, min\n'''\nall_data = pd.concat([\n    group.size().to_frame('players'),\n    group.mean(),\n    group[agg_col].max().rename(columns=lambda s: 'max.' + s),\n    group[agg_col].min().rename(columns=lambda s: 'min.' + s),\n    ], axis=1).reset_index()\nall_data = reduce_mem_usage(all_data)\n\nprint(all_data.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dbe45939855acb980439ea2f83ed7c6f2d87e95d"},"cell_type":"markdown","source":"## aggregate feature"},{"metadata":{"trusted":true,"_uuid":"a4bc443565321a07a9ce0c161ec8f9924b83d2d5"},"cell_type":"code","source":"numcols = all_data.select_dtypes(include='number').columns.values\nnumcols = numcols[numcols != 'winPlacePerc']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2dac1842b61335675367c7a4ac180c6b3dbc42bb"},"cell_type":"code","source":"''' match summary, max\n'''\nall_data = pd.merge(all_data, match_data)\ndel match_data\ngc.collect()\n\nall_data['enemy.players'] = all_data['m.players'] - all_data['players']\nfor c in sum_col:\n    all_data['enemy.' + c] = (all_data['m.sum.' + c] - all_data['sum.' + c]) / all_data['enemy.players']\n    #all_data['p.sum_msum.' + c] = all_data['sum.' + c] / all_data['m.sum.' + c]\n    #all_data['p.max_mmean.' + c] = all_data['max.' + c] / all_data['m.mean.' + c]\n    all_data['p.max_msum.' + c] = all_data['max.' + c] / all_data['m.sum.' + c]\n    all_data['p.max_mmax.' + c] = all_data['max.' + c] / all_data['m.max.' + c]\n    all_data.drop(['m.sum.' + c, 'm.max.' + c], axis=1, inplace=True)\n    \nfillInf(all_data, 0)\nprint(all_data.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d63ccf63773c949805683f5aee3c3699f1f3aeb7"},"cell_type":"code","source":"''' match rank\n'''\nmatch = all_data.groupby('matchId')\nmatchRank = match[numcols].rank(pct=True).rename(columns=lambda s: 'rank.' + s)\nall_data = reduce_mem_usage(pd.concat([all_data, matchRank], axis=1))\nrank_col = matchRank.columns\ndel matchRank\ngc.collect()\n\n# instead of rank(pct=True, method='dense')\nmatch = all_data.groupby('matchId')\nmatchRank = match[rank_col].max().rename(columns=lambda s: 'max.' + s).reset_index()\nall_data = pd.merge(all_data, matchRank)\nfor c in numcols:\n    all_data['rank.' + c] = all_data['rank.' + c] / all_data['max.rank.' + c]\n    all_data.drop(['max.rank.' + c], axis=1, inplace=True)\ndel matchRank\ngc.collect()\n\nprint(all_data.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dd2cfbf18321ea2e8d64c47a392c70c10556b7a2"},"cell_type":"markdown","source":"## killPlace rank of group and kills"},{"metadata":{"trusted":true,"_uuid":"a70258951066929394045448888ed0b5396d6c84"},"cell_type":"code","source":"''' TODO: incomplete\n''' \nkillMinorRank = all_data[['matchId','min.kills','max.killPlace']].copy()\ngroup = killMinorRank.groupby(['matchId','min.kills'])\nkillMinorRank['rank.minor.maxKillPlace'] = group.rank(pct=True).values\nall_data = pd.merge(all_data, killMinorRank)\n\nkillMinorRank = all_data[['matchId','max.kills','min.killPlace']].copy()\ngroup = killMinorRank.groupby(['matchId','max.kills'])\nkillMinorRank['rank.minor.minKillPlace'] = group.rank(pct=True).values\nall_data = pd.merge(all_data, killMinorRank)\n\ndel killMinorRank\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7ff6408d747855db53b14d6872aff2ba90521be3"},"cell_type":"markdown","source":"## drop constant feature"},{"metadata":{"trusted":true,"_uuid":"1e65a2de00d692f724ae1eaec6f9ef30ff9e9663"},"cell_type":"code","source":"# drop constant column\nconstant_column = [col for col in all_data.columns if all_data[col].nunique() == 1]\nprint('drop columns:', constant_column)\nall_data.drop(constant_column, axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4260c2df241609760f64056be3bd01615bae2a95"},"cell_type":"markdown","source":"## encode"},{"metadata":{"trusted":true,"_uuid":"ebd2851bb4f66edea40eeaf50042bb18a5379988"},"cell_type":"code","source":"'''\nsolo  <-- solo,solo-fpp,normal-solo,normal-solo-fpp\nduo   <-- duo,duo-fpp,normal-duo,normal-duo-fpp,crashfpp,crashtpp\nsquad <-- squad,squad-fpp,normal-squad,normal-squad-fpp,flarefpp,flaretpp\n'''\nmapper = lambda x: 'solo' if ('solo' in x) else 'duo' if ('duo' in x) or ('crash' in x) else 'squad'\nall_data['matchType'] = all_data['matchType'].apply(mapper)\n\nall_data = pd.concat([all_data, pd.get_dummies(all_data['matchType'], prefix='matchType')], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f7138c7995f98a6704d0f9952774917a075b7c14"},"cell_type":"code","source":"cols = [col for col in all_data.columns if col not in ['Id','matchId','groupId']]\nfor i, t in all_data.loc[:, cols].dtypes.iteritems():\n    if t == object:\n        all_data[i] = pd.factorize(all_data[i])[0]\n\nall_data = reduce_mem_usage(all_data)\nall_data.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8db28f0dcc9d2a0de93238de1a6eb76bb7f6331b"},"cell_type":"markdown","source":"# Predict"},{"metadata":{"trusted":true,"_uuid":"c9638c50680633d176596da0dbb085ca31bacf19"},"cell_type":"code","source":"X_train = all_data[all_data['winPlacePerc'].notnull()].reset_index(drop=True)\nX_test = all_data[all_data['winPlacePerc'].isnull()].drop(['winPlacePerc'], axis=1).reset_index(drop=True)\ndel all_data\ngc.collect()\n\nY_train = X_train.pop('winPlacePerc')\nX_test_grp = X_test[['matchId','groupId']].copy()\n\n# drop matchId,groupId\nX_train.drop(['matchId','groupId'], axis=1, inplace=True)\nX_test.drop(['matchId','groupId'], axis=1, inplace=True)\n\nX_train_cols = X_train.columns\n\nprint(X_train.shape, X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e777c101f7ce2cec11e284daabd4c1f94edae41d"},"cell_type":"code","source":"#print(pd.DataFrame([[val for val in dir()], [sys.getsizeof(eval(val)) for val in dir()]],\n#                   index=['name','size']).T.sort_values('size', ascending=False).reset_index(drop=True)[:10])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"725a31e7756fa677e26a5b4bd48ff756222f3513"},"cell_type":"code","source":"from keras import optimizers, regularizers\nfrom keras.callbacks import LearningRateScheduler, EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nfrom keras.layers import Dense, Dropout, BatchNormalization, PReLU\nfrom keras.models import load_model\nfrom keras.models import Sequential\n\ndef createModel():\n    model = Sequential()\n    model.add(Dense(512, kernel_initializer='he_normal', input_dim=X_train.shape[1], activation='relu'))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.2))\n\n    model.add(Dense(256, kernel_initializer='he_normal'))\n    model.add(PReLU(alpha_initializer='zeros', alpha_regularizer=None, alpha_constraint=None, shared_axes=None))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.2))\n\n    model.add(Dense(128, kernel_initializer='he_normal'))\n    model.add(PReLU(alpha_initializer='zeros', alpha_regularizer=None, alpha_constraint=None, shared_axes=None))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.1))\n\n    model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n\n    optimizer = optimizers.Adam(lr=0.005)\n    model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n    #model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['mae'])\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f67a96ebb513470dfb5ba116400eaaed38d14103"},"cell_type":"code","source":"def step_decay_schedule(initial_lr=1e-3, decay_factor=0.75, step_size=10, verbose=0):\n    ''' Wrapper function to create a LearningRateScheduler with step decay schedule. '''\n    def schedule(epoch):\n        return initial_lr * (decay_factor ** np.floor(epoch/step_size))\n    \n    return LearningRateScheduler(schedule, verbose)\n\nlr_sched = step_decay_schedule(initial_lr=0.001, decay_factor=0.97, step_size=1, verbose=1)\nearly_stopping = EarlyStopping(monitor='val_mean_absolute_error', mode='min', patience=10, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dc48e7700f42ba20351ac2ef4900fb8909cf6f6a"},"cell_type":"code","source":"from sklearn import preprocessing\nfrom tensorflow import set_random_seed\nnp.random.seed(42)\nset_random_seed(1234)\n\nscaler = preprocessing.StandardScaler().fit(X_train.astype(float))\nX_train = scaler.transform(X_train.astype(float))\nX_test = scaler.transform(X_test.astype(float))\n\nmodel = createModel()\nhistory = model.fit(\n        X_train, Y_train,\n        epochs=200,\n        batch_size=2**15,\n        validation_split=0.2,\n        callbacks=[lr_sched, early_stopping],\n        verbose=2)\npred = model.predict(X_test).ravel()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dcbae65430ef43f08591ccd200fe33f802d938a6","scrolled":true},"cell_type":"code","source":"# Plot training & validation loss values\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()\n\n# Plot training & validation mae values\nplt.plot(history.history['mean_absolute_error'])\nplt.plot(history.history['val_mean_absolute_error'])\nplt.title('Mean Abosulte Error')\nplt.ylabel('Mean absolute error')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"914305f240aa0ca6d0c6cd16954ccab4815e4bcb"},"cell_type":"markdown","source":"## feature importances"},{"metadata":{"trusted":true,"_uuid":"d30d3517efebdb6df8e2164b3ce36a879b6aa358"},"cell_type":"code","source":"'''\nfrom eli5.permutation_importance import get_score_importances\n\ndef score(X, y):\n    y_pred = model.predict(X).ravel()\n    return np.sum(np.abs(y - y_pred))\n\nbase_score, score_decreases = get_score_importances(score, X_train[:10000], Y_train[:10000])\nfeature_importances = np.mean(score_decreases, axis=0) * -1\n\nfeature_importances = 100.0 * (feature_importances / feature_importances.max())\nsorted_idx = np.argsort(feature_importances)\nsorted_idx = sorted_idx[len(feature_importances) - 30:]\npos = np.arange(sorted_idx.shape[0]) + .5\n\nplt.figure(figsize=(12,8))\nplt.barh(pos, feature_importances[sorted_idx], align='center')\nplt.yticks(pos, X_train_cols[sorted_idx])\nplt.xlabel('Relative Importance')\nplt.title('Variable Importance')\nplt.show()\n\nX_train_cols[np.argsort(feature_importances)[::-1]].values\n'''","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3f8e666941a6157dda493f36c8084e4f021ff454"},"cell_type":"markdown","source":"## alignment"},{"metadata":{"trusted":true,"_uuid":"605701ae5d89d21ac56ac3ddcf69175f936e3b18"},"cell_type":"code","source":"X_test = pd.read_csv('../input/test_V2.csv')\nX_test = X_test.groupby(['matchId','groupId','matchType']).first().reset_index()\nX_test = X_test[['matchId','groupId','matchType','numGroups','maxPlace','kills','killPlace']]\n\ngroup = X_test_grp.groupby(['matchId'])\nX_test_grp['winPlacePerc'] = pred\nX_test_grp['_rank.winPlacePerc'] = group['winPlacePerc'].rank(method='min')\nX_test = pd.merge(X_test, X_test_grp)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ae29276c07cdd5c255205eb4cae731bad1596521","scrolled":false},"cell_type":"code","source":"fullgroup = (X_test['numGroups'] == X_test['maxPlace'])\n\n# full group (201366) --> calculate from rank\nsubset = X_test.loc[fullgroup]\nX_test.loc[fullgroup, 'winPlacePerc'] = (subset['_rank.winPlacePerc'].values - 1) / (subset['maxPlace'].values - 1)\n\n# not full group (684872) --> align with maxPlace\nsubset = X_test.loc[~fullgroup]\ngap = 1.0 / (subset['maxPlace'].values - 1)\nnew_perc = np.around(subset['winPlacePerc'].values / gap) * gap  # half&up\nX_test.loc[~fullgroup, 'winPlacePerc'] = new_perc\n\nX_test['winPlacePerc'] = X_test['winPlacePerc'].clip(lower=0,upper=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5e95c4aab16d2954542f986adacbe2e8392037d3"},"cell_type":"code","source":"from tqdm import tqdm\n\n# credit to https://www.kaggle.com/nagroda100/pubg-submission-postprocessor/code\nprint(\"Checking for anomalies in the winPlacePerc - players with same number of kills should have scores in order of killPlace\")\n\ndo_correct = True\niteration_number = 1\n\nwhile do_correct & (iteration_number <= 1000):\n    X_test.sort_values(ascending=False, by=[\"matchId\",\"kills\",\"killPlace\",\"winPlacePerc\",\"groupId\"], inplace=True)\n    X_test[\"winPlacePerc_diff\"] = X_test[\"winPlacePerc\"].diff()\n    X_test[\"kills_diff\"] = X_test[\"kills\"].diff()\n    X_test[\"prev_matchId\"] = X_test[\"matchId\"].shift(1)\n    X_test[\"prev_groupId\"] = X_test[\"groupId\"].shift(1)\n    X_test[\"prev_winPlacePerc\"] = X_test[\"winPlacePerc\"].shift(1)\n\n    df_sub2 = X_test[(X_test[\"winPlacePerc_diff\"] < 0) \n                     & (X_test[\"kills_diff\"] == 0) \n                     & (X_test[\"matchId\"] == X_test[\"prev_matchId\"])]\n    anomalies_count = len(df_sub2)\n\n    print(\"Iteration \" + str(iteration_number) + \" Anomalies count: \" + str(anomalies_count))\n\n    changed_groups = list()\n\n    if anomalies_count > 0:\n        print()\n        print(\"Looking for pairs to change...\")\n\n        df_sub2[\"new_winPlacePerc\"] = df_sub2[\"winPlacePerc\"] \n\n        df_sub3 = pd.DataFrame()\n\n        for i in tqdm(range(1, min(15001, max(anomalies_count, 2))), \n                      desc=\"Identifying unique groups\", mininterval=10):\n            row = df_sub2.iloc[i - 1]\n            id_prev = str(row[\"prev_matchId\"]) + \"!\" + str(row[\"prev_groupId\"])\n            id_cur = str(row[\"matchId\"]) + \"!\" + str(row[\"groupId\"])\n            if (not id_prev in changed_groups) & (not id_cur in changed_groups):\n                changed_groups.append(id_prev)\n                changed_groups.append(id_cur)\n                df_sub3 = df_sub3.append({\"matchId\": row[\"matchId\"], \"groupId\": row[\"prev_groupId\"], \n                                          \"new_winPlacePerc\": row[\"winPlacePerc\"]}, \n                                         sort=False, ignore_index=True)\n                df_sub3 = df_sub3.append({\"matchId\": row[\"matchId\"], \"groupId\": row[\"groupId\"], \n                                          \"new_winPlacePerc\": row[\"prev_winPlacePerc\"]}, \n                                         sort=False, ignore_index=True)\n\n        df_sub3.drop_duplicates(inplace=True)\n        X_test = X_test.merge(df_sub3, on=[\"matchId\", \"groupId\"], how=\"left\")\n        notna = X_test[\"new_winPlacePerc\"].notna()\n        X_test.loc[notna, \"winPlacePerc\"] = X_test.loc[notna][\"new_winPlacePerc\"]\n        X_test.drop(labels=\"new_winPlacePerc\", axis=1, inplace=True)\n        del df_sub2\n        del df_sub3\n        df_sub2 = None\n        df_sub3 = None\n        gc.collect()\n    else:\n        do_correct = False\n\n    iteration_number = iteration_number + 1\n\nif do_correct:\n    print(\"Limit of iterations reached...\")\n\nprint(\"Finished correcting winPlacePerc\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2ab07f991a8d6cd17ba36c0510a2a507acb808f4"},"cell_type":"code","source":"# edge cases\nX_test.loc[X_test['maxPlace'] == 0, 'winPlacePerc'] = 0\nX_test.loc[X_test['maxPlace'] == 1, 'winPlacePerc'] = 1  # nothing\nX_test.loc[(X_test['maxPlace'] > 1) & (X_test['numGroups'] == 1), 'winPlacePerc'] = 0\nX_test['winPlacePerc'].describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"86209503fc7f68f37024f82057cf10819ef75634"},"cell_type":"markdown","source":"# Submit"},{"metadata":{"trusted":true,"_uuid":"7cd3bf14e3d8d03d9ebfa45826acbbb38cb421e4"},"cell_type":"code","source":"test = pd.read_csv('../input/test_V2.csv')\n\nsubmission = pd.merge(test, X_test[['matchId','groupId','winPlacePerc']])\nsubmission = submission[['Id','winPlacePerc']]\nsubmission.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2527e0ed0735c2b09a7f00880265d19da515eb74"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}