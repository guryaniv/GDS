{"metadata": {"language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "nbconvert_exporter": "python", "mimetype": "text/x-python", "pygments_lexer": "ipython3", "name": "python", "file_extension": ".py", "version": "3.6.1"}, "kernelspec": {"display_name": "Python 3", "name": "python3", "language": "python"}}, "nbformat": 4, "nbformat_minor": 0, "cells": [{"metadata": {"_uuid": "26879f7ba912cb6048fd80d1c0e47b7077c7b196", "collapsed": false, "_cell_guid": "dabff946-1a3e-40eb-a0e7-ebdc63406803", "_execution_state": "idle"}, "source": "I have had the idea of creating a comprehensive tutorial kernel accessible to everyone especially those who are new to Data Science, by keeping the narrative similar to thought process. Thanks to Kaggle for this great initiative.\n\nMy goals are:\n\n* **Creating a simple yet deep analysis of the data sets by incorporating features one by one to the analysis.** \n* **Performing proper feature engineering**\n* **Building predictive models and ensembles**\n\nFirst, import some generic notebook stuff and pay our tribute to the Great Taxi Driver :D", "outputs": [], "cell_type": "markdown", "execution_count": null}, {"metadata": {"_uuid": "54713c99327af54290b1d7753c3794c20d21e802", "collapsed": false, "trusted": false, "_cell_guid": "098d6d93-8c30-490a-9382-81890a6dc4f0", "_execution_state": "idle"}, "source": "from IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"  # for better interative experience\n\n%config InlineBackend.figure_format = 'retina'  # for high resolution plots", "outputs": [], "cell_type": "code", "execution_count": 2}, {"metadata": {"_uuid": "d864d272dc6a9ce301e20dbd70809a8e02269d5b", "collapsed": false, "_cell_guid": "1479ecee-ee5f-4e3e-83e0-eb2c26eb48d4", "_execution_state": "idle"}, "source": "![image](http://4.bp.blogspot.com/-ti7Cxqon1cg/TeQ4q9TKRBI/AAAAAAAAAzw/pJZpoOmh7dY/s1600/taxi_driver.jpg)", "outputs": [], "cell_type": "markdown", "execution_count": null}, {"metadata": {"_uuid": "9f84ab8ad3b0239913aa1a2f588acfd84cfd1b73", "trusted": false, "_cell_guid": "bcc0c0a1-3573-4b19-a96e-eec05f2718bc", "_execution_state": "idle"}, "source": "import warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport numpy as np\nimport pandas as pd\npd.options.display.max_columns = 100\n\nimport matplotlib.pyplot as plt\nplt.style.use('fivethirtyeight')\n%matplotlib inline\nimport seaborn as sns\nsns.set_style('ticks')\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))", "outputs": [], "cell_type": "code", "execution_count": 3}, {"metadata": {"_uuid": "478881a5409ea0a224a594039ccf0af29973c20f", "collapsed": false, "_cell_guid": "50a5311e-3cb0-4a6d-9127-9aa1df70add5", "_execution_state": "idle"}, "source": "# Exploratory Data Analysis\n\nWe have data sets with the following features\n\n* **id** - a unique identifier for each trip\n* **vendor_id** - a code indicating the provider associated with the trip record\n* **pickup_datetime** - date and time when the meter was engaged\n* **dropoff_datetime** - date and time when the meter was disengaged\n* **passenger_count** - the number of passengers in the vehicle (driver entered value)\n* **pickup_longitude** - the longitude where the meter was engaged\n* **pickup_latitude** - the latitude where the meter was engaged\n* **dropoff_longitude** - the longitude where the meter was disengaged\n* **dropoff_latitude** - the latitude where the meter was disengaged\n* **store_and_fwd_flag** - This flag indicates whether the trip record was held in vehicle memory before sending to the vendor because the vehicle did not have a connection to the server - Y=store and forward; N=not a store and forward trip\n* **trip_duration** - duration of the trip in seconds", "outputs": [], "cell_type": "markdown", "execution_count": null}, {"metadata": {"_uuid": "60dd689e6e3b573941d6165eb04039bbe3a21003", "collapsed": false, "_cell_guid": "428ebc9b-9e29-4784-99ed-a9988b99f81f", "_execution_state": "idle"}, "source": "Let's start reading the csv data by\n\n1. Specifying `parse_dates` and `infer_datetime_format` helps for faster reading\n\n2.  Generally using `category` types makes the memory usage lower, so we change the types of `vendor_id` and `store_and_fwd_flag` to `category` as follows", "outputs": [], "cell_type": "markdown", "execution_count": null}, {"metadata": {"_uuid": "f97045ae3e78928d1fde00b595f444bcfb215eb4", "collapsed": false, "trusted": false, "_cell_guid": "dd4c61e6-f2d0-4f43-8994-34ad74be730d", "_execution_state": "idle"}, "source": "%%time\ntrain = pd.read_csv(\"../input/train.csv\", index_col=\"id\", engine='python',\n                    parse_dates=['pickup_datetime', 'dropoff_datetime'],\n                    infer_datetime_format=True)\n\ntest = pd.read_csv(\"../input/test.csv\", index_col=\"id\", engine='python',\n                   parse_dates=['pickup_datetime'],\n                   infer_datetime_format=True)", "outputs": [], "cell_type": "code", "execution_count": 4}, {"metadata": {"_uuid": "a5ed6f5dd3d52c8ae488416d267a850cfe9bf9bc", "collapsed": false, "trusted": false, "_cell_guid": "7f889e0c-2aa2-4d85-81bc-e4f20ad1ca27", "_execution_state": "idle"}, "source": "# before changing the types, make sure train and test have the same categories. Otherwise,\n# we'll get into trouble at test time!\nprint(f\"train and test vendor_id unique values are the same: \\\n        {train.vendor_id.unique().sort() == test.vendor_id.unique().sort()}\")\nprint(f\"train and test store_and_fwd_flag unique values are the same: \\\n        {train.store_and_fwd_flag.unique().sort() == test.store_and_fwd_flag.unique().sort()}\")\n\ntrain['vendor_id'] = train['vendor_id'].astype('category')\ntrain['store_and_fwd_flag'] = train['store_and_fwd_flag'].astype('category')\n\ntest['vendor_id'] = test['vendor_id'].astype('category')\ntest['store_and_fwd_flag'] = test['store_and_fwd_flag'].astype('category')", "outputs": [], "cell_type": "code", "execution_count": 5}, {"metadata": {"_uuid": "03cb7b08f37da3cd9e7fb82e7de469617a4c8976", "collapsed": false, "_cell_guid": "51bb5f03-6e89-4c57-aa09-5feefaaa1406", "_execution_state": "idle"}, "source": "**Python 3.6** comes with [f-string](https://docs.python.org/3/whatsnew/3.6.html#whatsnew36-pep498) so I'll use this feature heavily throughout.\n\nBelow we can see some preliminary view of our datasets and we notice that `train` and `test` have no missing values.", "outputs": [], "cell_type": "markdown", "execution_count": null}, {"metadata": {"_uuid": "5af1d702033eaceb028b1fd1443348f50f22a4b6", "collapsed": false, "trusted": false, "_cell_guid": "740c2ede-1815-4d84-9397-ef6cd815367b", "_execution_state": "idle"}, "source": "print(f\"train shape: {train.shape}\")\nprint(\"=======================================================\")\nprint(f\"test shape: {test.shape}\")\nprint(\"=======================================================\")\nprint(\"train view:\")\ntrain.head()\nprint(\"test view:\")\ntest.head()\nprint(\"train/test columns difference:\")\ntrain.columns.difference(test.columns)\nprint(\"=======================================================\")\nprint(\"train info:\")\ntrain.info()\nprint(\"=======================================================\")\nprint(\"test info:\")\ntest.info()\nprint(\"=======================================================\")\nprint(f\"train feature descriptions:\")\ntrain.describe()\nprint(f\"test feature descriptions:\")\ntest.describe()\nprint(\"=======================================================\")\nprint(f\"train/test overlapping index: {train.index.intersection(test.index)}\")", "outputs": [], "cell_type": "code", "execution_count": 6}, {"metadata": {"_uuid": "446c8d814d26135066242bbb0967f4c194ac782c", "collapsed": false, "trusted": false, "_cell_guid": "dab9a186-fec2-440b-9c59-c2d524979828", "_execution_state": "idle"}, "source": "# Do NOT run! it'll take a long time\n#%%time\n#train_uniq = train.apply(lambda s: s.nunique())\n#test_uniq = test.apply(lambda s: s.nunique())\n\n#print(f\"Number of unique values per column in train: \\n {pd.DataFrame(train_uniq).T}\")\n#print(f\"Number of unique values per column in test: \\n {pd.DataFrame(test_uniq).T}\")", "outputs": [], "cell_type": "code", "execution_count": 7}, {"metadata": {"_uuid": "9a12be22878d25204a8e3c6c86c208ee3fca98ef", "collapsed": false, "_cell_guid": "071a93f8-793a-49f6-85fb-0bd609c3565e", "_execution_state": "idle"}, "source": "Outputing some simple statistics of the target variable `trip_duration` is pointing that there could exist ourliers in our data. So for now, I remove those data points with trip_duration values less that or greater than 1% and 99% quantiles using pandas `query` and f-string computation and plot the distribution of the resulted dataset.", "outputs": [], "cell_type": "markdown", "execution_count": null}, {"metadata": {"_uuid": "ac31904c334acc870d5a147643faa05e9817768e", "collapsed": false, "trusted": false, "_cell_guid": "642caad9-b808-4f46-90bf-95173b8765f6", "_execution_state": "idle"}, "source": "target = train.trip_duration\n\nprint(f\"trip_duration \\n \\\n      min: {target.min()} \\n \\\n      max: {target.max()} \\n \\\n      mode: {target.mode()[0]} \\n \\\n      mean: {target.mean()} \\n \\\n      median: {target.median()} \\n \\\n      1% quantile: {target.quantile(q=0.01)} \\n \\\n      99% quantile: {target.quantile(q=0.99)}\")", "outputs": [], "cell_type": "code", "execution_count": 8}, {"metadata": {"_uuid": "ee3415feb7b5e1efd73b77fb411088d6787afbb4", "collapsed": false, "trusted": false, "_cell_guid": "9d14492f-2dd3-4b21-a73a-27283e7bafac", "_execution_state": "idle"}, "source": "train_pure = train.query(f\"{target.quantile(q=0.01)} <= \\\n                            trip_duration <= {target.quantile(q=0.99)}\")\ntarget_pure = train_pure.trip_duration \nsns.distplot(target_pure)\nplt.xlabel(\"trip duration\")\nplt.title(\"Sample distribution of tip duration\");", "outputs": [], "cell_type": "code", "execution_count": 9}, {"metadata": {"_uuid": "0000cf74b2a750fa0442d370736924eeb6edab34", "collapsed": false, "_cell_guid": "b5f2266c-e2fa-4ee8-beb0-9a6cf6087c20", "_execution_state": "idle"}, "source": "Is `trip_duration` distribution likely to be log-normal? ", "outputs": [], "cell_type": "markdown", "execution_count": null}, {"metadata": {"_uuid": "2c2529ef3d1db66773f92a45735de1d9df4cba34", "collapsed": false, "trusted": false, "_cell_guid": "20c17a17-daf9-489d-bfb1-ab6fe663cee5", "_execution_state": "idle"}, "source": "sns.distplot(np.log(target_pure))\nplt.xlabel(\"log trip duration\")\nplt.title(\"Sample distribution of logarithm of tip duration\");", "outputs": [], "cell_type": "code", "execution_count": 10}, {"metadata": {"_uuid": "f51a813c3c18de56e5c5851ca4f1e4205c6cc602", "collapsed": false, "_cell_guid": "dcddc65a-2299-45c9-8d5b-2431bdb51fb3", "_execution_state": "idle"}, "source": "Let's use different normality tests to find the p-values", "outputs": [], "cell_type": "markdown", "execution_count": null}, {"metadata": {"_uuid": "23f87279d20ff4d06699f7962f60a5ed8dc003dd", "collapsed": false, "trusted": false, "_cell_guid": "d7b9b914-d163-4628-b197-ce9c30cdf2c5", "_execution_state": "idle"}, "source": "from scipy.stats import shapiro, anderson, kstest, normaltest\n\ntarget_pure_log = np.log(target_pure)  # target values are already positive\n\nprint(f\"p-value for Shapiro-Wilk test: {shapiro(target_pure_log)[1]}\")\n\nprint(f\"A 2-sided chi squared probability for the hypothesis test: \\\n        {normaltest(target_pure_log).pvalue}\")\n\nprint(f\"p-value for the Kolmogorov-Smirnov test: {kstest(target_pure, cdf='norm').pvalue}\")\n\nprint(f\"Anderson-Darling normality test: \\n \\\n        {anderson(target_pure_log, dist='norm')}\")", "outputs": [], "cell_type": "code", "execution_count": 11}, {"metadata": {"_uuid": "20fa6310db0cffde61b6c9e0c38b7a3dc8ba2abd", "collapsed": false, "_cell_guid": "18f6bef4-54cf-43dd-b275-f18143c65bdf", "_execution_state": "idle"}, "source": "So different tests are rejecting the log-normality assumption of the `target`. Note that p-values are probabilities of seeing data under null hypothesis (i.e. `target` comes from normal distribution).\n\nLet's take a look at the pair-wise plot of a sample of first 10000 data points colored with `vender_id` which we already changed its type to `category`.", "outputs": [], "cell_type": "markdown", "execution_count": null}, {"metadata": {"_uuid": "e3dbebc1955bc3095a406ef2c6ed4ea1de1996ce", "collapsed": false, "trusted": false, "_cell_guid": "f81e0909-3019-418b-84d4-c7d80a1b826b", "_execution_state": "idle"}, "source": "train_sample = train_pure[:10000]\nsns.pairplot(train_sample, hue='vendor_id');", "outputs": [], "cell_type": "code", "execution_count": 12}, {"metadata": {"_uuid": "00698d5efd6443e995a5d63b47248f8e12564ecc", "collapsed": false, "_cell_guid": "d9267ab6-1534-405c-8608-96cff23d5a51", "_execution_state": "idle"}, "source": "Plotting the correlations among continuous features shows for example that there's no correlation between the `passenger_count` and `trip_duration`.", "outputs": [], "cell_type": "markdown", "execution_count": null}, {"metadata": {"_uuid": "c6612c452bfca10d12b652e7186668ed7110e590", "collapsed": false, "trusted": false, "_cell_guid": "f75066f7-9019-44d5-af56-c3af11e73fcb", "_execution_state": "idle"}, "source": "train_corr = train_pure.corr()\nplt.figure(figsize=(12, 10))\nsns.heatmap(train_corr, annot=True)\nplt.title(\"Correlation plot\");", "outputs": [], "cell_type": "code", "execution_count": 13}, {"metadata": {"_uuid": "78d04d441a8601df5eb224667f746955ffd9803a", "collapsed": false, "_cell_guid": "bcfa9ef0-3157-47c1-b995-f02536dd9583", "_execution_state": "idle"}, "source": "## Categorical features first", "outputs": [], "cell_type": "markdown", "execution_count": null}, {"metadata": {"_uuid": "15504391af8ad2dc8cb6a6000581432c2b65085b", "collapsed": false, "_cell_guid": "f5a79944-2338-4279-92ab-e401b1828dc3", "_execution_state": "idle"}, "source": "We can plot our categorical features with `countplot` as follows", "outputs": [], "cell_type": "markdown", "execution_count": null}, {"metadata": {"_uuid": "1a99c6e70877c3bf0c1d4a2357d7979df7dfc532", "collapsed": false, "trusted": false, "_cell_guid": "636d5d29-0af5-427c-a79d-56d59af1f8e1", "_execution_state": "idle"}, "source": "print(f\"vendor_id value counts: \\n {train_pure.vendor_id.value_counts()}\")\nprint(f\"store_and_fwd_flag value counts: \\n {train_pure.store_and_fwd_flag.value_counts()}\")\n\nfig, ax = plt.subplots(1,2, figsize=(15, 7))\nsns.countplot(data=train_pure, x='vendor_id', ax=ax[0])\nsns.countplot(data=train_pure, x='store_and_fwd_flag', ax=ax[1])\nplt.show();", "outputs": [], "cell_type": "code", "execution_count": 14}, {"metadata": {"_uuid": "9e4d0bb0654f3ffc318ba981e5f2e8f2c9825a17", "collapsed": false, "_cell_guid": "9d54960a-d1a8-4742-ad52-6329ce6f40a5", "_execution_state": "idle"}, "source": "To plot distributions wrt to categorical features, we can use `boxplot` along with `swarmplot` to see the data points on top ", "outputs": [], "cell_type": "markdown", "execution_count": null}, {"metadata": {"_uuid": "a25f970c9e46a872183ba0b766edf545f43b4799", "collapsed": false, "trusted": false, "_cell_guid": "c6aae102-2757-498b-89dc-f3ed9add97bb", "_execution_state": "idle"}, "source": "plt.figure(figsize=(12, 10))\nax = sns.boxplot(x='vendor_id', y='trip_duration', data=train_pure[:1000])\nax = sns.swarmplot(x='vendor_id', y='trip_duration', data=train_pure[:1000], color='k');\nplt.xlabel(\"vendor id\")\nplt.ylabel(\"trip duration\")\nplt.show();", "outputs": [], "cell_type": "code", "execution_count": 15}, {"metadata": {"_uuid": "41c68ec2f98941f85e26b840c2ddcf38d269b16e", "collapsed": false, "_cell_guid": "eb920229-0ed4-4f47-b406-1a1de367f0fb", "_execution_state": "idle"}, "source": "Skewness in the distribution of `trip_duration` is evident in both `vendor_id`s above. To examine more, we can add `store_and_fwd_flag` and try to visualize the relations between `trip_duration` distribution and given `vendor_id` and `store_and_fwd_flag`. \n\n`FaceGrid` is a way to go! `FaceGrid` is a powerful way of plotting *conditional* dependencies between target/features. For example, the following is the (sample) conditional distribution\n\n$$P(\\text{trip_duration}|\\text{vendor_id, store_and_fwd_flag})$$\n\nof trip_duration given vendor_id and  store_and_fwd_flag.", "outputs": [], "cell_type": "markdown", "execution_count": null}, {"metadata": {"_uuid": "bf3a7f639a1f7245a5ad4d0519fcc18573330d33", "collapsed": false, "trusted": false, "_cell_guid": "ac05db7c-d748-46cb-a89b-f2910cbb5a04", "_execution_state": "idle"}, "source": "g = sns.FacetGrid(train_pure, row='vendor_id', hue='store_and_fwd_flag',\n                  aspect=3, size=2.5, margin_titles=True)\ng.map(sns.kdeplot, 'trip_duration', shade=True).add_legend()\nfor ax in g.axes.flat:\n    ax.yaxis.set_visible(False)\nsns.despine(left=True);", "outputs": [], "cell_type": "code", "execution_count": 16}, {"metadata": {"_uuid": "62c7bf4e0fcbe4cf52d0ec77412494e37ac619f1", "collapsed": false, "_cell_guid": "71fcddfa-5287-4a4a-be3c-f733856aeab5", "_execution_state": "idle"}, "source": "The first plot above, shows that the conditional (sample) distribution $$P(\\text{trip_duration}|\\text{vendor_id = 1, store_and_fwd_flag = Y})$$ has longer tail that $$P(\\text{trip_duration}|\\text{vendor_id = 1, store_and_fwd_flag = N})$$ and \n$$P(\\text{trip_duration}|\\text{vendor_id = 2, store_and_fwd_flag = Y})$$ is not present!", "outputs": [], "cell_type": "markdown", "execution_count": null}, {"metadata": {"_uuid": "7785048a44a3f91ad77e6bc919e923f7f1f41878", "collapsed": false, "_cell_guid": "9e304e5b-0510-488d-9cad-76f9bceb7de8", "_execution_state": "idle"}, "source": "## `passenger_count`", "outputs": [], "cell_type": "markdown", "execution_count": null}, {"metadata": {"_uuid": "4381df97acf98cf5c6d582c8939f5582d8c90434", "collapsed": false, "trusted": false, "_cell_guid": "1a98caee-66b5-4aa8-952c-f824c779d1df", "_execution_state": "idle"}, "source": "print(f\"unique values: {train_pure.passenger_count.unique()}\")\nprint(f\"number of unique values: {train_pure.passenger_count.nunique()}\")\n\nsns.barplot(x=\"passenger_count\", y=\"trip_duration\", data=train_pure, estimator=np.median);\nplt.xlabel(\"passenger count\")\nplt.ylabel(\"median of trip duration (seconds)\")\nplt.title(\"Median of trip duration barplot over passenger count\");", "outputs": [], "cell_type": "code", "execution_count": 17}, {"metadata": {"_uuid": "d128bdf23da2c05fc5cca9fe1889c2a0d8d9a656", "collapsed": false, "_cell_guid": "2298b09b-0d2a-4b5c-ac9c-964fe8b442e4", "_execution_state": "idle"}, "source": "Let's examine the conditional distribution of `trip_duration` given `passenger_count` and `vendor_id` \n\n$$P(\\text{trip_duration} | \\text{passenger_count, vendor_id})$$", "outputs": [], "cell_type": "markdown", "execution_count": null}, {"metadata": {"_uuid": "e852a8f18edce8df06d86a261bde59320feacd24", "collapsed": false, "trusted": false, "_cell_guid": "fa32ecdb-c0e5-48a6-ad28-5e8895e09ad5", "_execution_state": "idle"}, "source": "plt.figure(figsize=(16, 12))\ng = sns.FacetGrid(train_pure, col='passenger_count',\n                  col_wrap=3, hue='vendor_id',\n                  aspect=1, size=2, margin_titles=True)\ng.map(sns.kdeplot,  'trip_duration', shade=True).add_legend()\nfor ax in g.axes.flat:\n    ax.yaxis.set_visible(False)\nsns.despine(left=True);", "outputs": [], "cell_type": "code", "execution_count": 18}, {"metadata": {"_uuid": "fe6c5c0263613383a660885df234f1d8da112c95", "collapsed": false, "_cell_guid": "1d8f9400-e7ac-4af3-a841-726d86c6a495", "_execution_state": "idle"}, "source": "From above plots, we can see that the distribution of `trip_duration` given `passenger_count = 0` for both `vendor_id`s. This could show delays/traffics or taxi looking for passengers or even when the vehicle is stopped for some reasons. Other `passenger_count`s, from 1 to 5 seem consistent and 6, 8, 9 are getting a little weird. Btw where's 7?", "outputs": [], "cell_type": "markdown", "execution_count": null}, {"metadata": {"_uuid": "7036d11b15cf60e7cb29795debff25fa5aec7b21", "collapsed": false, "_cell_guid": "3ba4903e-dd22-49d2-918f-4af27b027907", "_execution_state": "idle"}, "source": "From various distributions of `trip_duration` we have seen so far, all are left skewed. So we'll use `median` instead of `mean`. \n\nNow, let's examine the medians of `trip_duration` per each `passenger_count` and `vendor_id` with margins as follows (note that pandas `pivot_table` is a generalization of `groupby`)", "outputs": [], "cell_type": "markdown", "execution_count": null}, {"metadata": {"_uuid": "cc2519066ff3e31f4daf531acc8c475ce9d78a78", "collapsed": false, "trusted": false, "_cell_guid": "795f48f8-c93d-44ee-86cb-eab66aace4e7", "_execution_state": "idle"}, "source": "train_trip_vendor = train_pure.pivot_table('trip_duration',\n                                           index='passenger_count', \n                                           columns='vendor_id',\n                                           aggfunc='median',\n                                           margins='All')\ntrain_trip_vendor", "outputs": [], "cell_type": "code", "execution_count": 19}, {"metadata": {"_uuid": "a3d20e156ebad989c9452a0637ed3935bc4bd10c", "collapsed": false, "_cell_guid": "34280eb0-42e2-4e2f-af97-d910b61ea54e", "_execution_state": "idle"}, "source": "From the table above, why `trip_duration` for `vendor_id=2` was taken almost twice the time than `vendor_id=1` when there's no passenger? moreover, `trip_duration` for `vendor_id=2` with `passenger_count=8` is significantly lower than the rest.", "outputs": [], "cell_type": "markdown", "execution_count": null}, {"metadata": {"_uuid": "c4e91a8028821574856217762eb8a3f5fcaec4dc", "collapsed": false, "_cell_guid": "dc27afde-e77f-45a5-abe0-b13d90780106", "_execution_state": "idle"}, "source": "Add `store_and_fwd_flag` variable and plot to see the behaviour `trip_duration` medians", "outputs": [], "cell_type": "markdown", "execution_count": null}, {"metadata": {"_uuid": "44b86f1daa7ec5f91316da6310eba1ede234f92a", "collapsed": false, "trusted": false, "_cell_guid": "b7a14db9-bef2-4e97-b745-1fb62e3e378a", "_execution_state": "idle"}, "source": "train_trip_vendor_flag = train_pure.pivot_table('trip_duration',\n                                                index='passenger_count', \n                                                columns=['vendor_id',\n                                                         'store_and_fwd_flag'],\n                                                aggfunc='median')\n\ntrain_trip_vendor_flag\n\ntrain_trip_vendor_flag.plot()\nplt.xlabel(\"passenger count\")\nplt.ylabel(\"median of trip_duration (seconds)\")\nplt.show()", "outputs": [], "cell_type": "code", "execution_count": 20}, {"metadata": {"_uuid": "d5616d5017b588a718ae9161062eb6690d41df8d", "collapsed": false, "_cell_guid": "093725cc-c6d7-4a39-9dd3-51f584fd4e01", "_execution_state": "idle"}, "source": "## pickup_datetime", "outputs": [], "cell_type": "markdown", "execution_count": null}, {"metadata": {"_uuid": "187e741f898517f921e036fbcb17f43f2db2e45f", "collapsed": false, "_cell_guid": "d729bca8-e503-4cbc-8ec5-deb3f7e32205", "_execution_state": "idle"}, "source": "It's time to incorporate datetime variables into our analysis slowly! In light of the above plot, we can find the medians of `trip_duration` with the added pickup hour quartiles as follows", "outputs": [], "cell_type": "markdown", "execution_count": null}, {"metadata": {"_uuid": "d94a1485ebad972debc246da12e71999f8ded14c", "collapsed": false, "trusted": false, "_cell_guid": "e3691d48-cad5-470d-816e-08184b3b4f48", "_execution_state": "idle"}, "source": "pickup_hour = pd.qcut(train_pure['pickup_datetime'].dt.hour, q=[0, .25, .5, .75, 1.])\ntrain_trip_vendor_flag = train_pure.pivot_table('trip_duration',\n                                                index='passenger_count', \n                                                columns=['vendor_id',\n                                                         'store_and_fwd_flag',\n                                                         pickup_hour],\n                                                aggfunc='median')\n\ntrain_trip_vendor_flag", "outputs": [], "cell_type": "code", "execution_count": 21}, {"metadata": {"_uuid": "5a6c91e45e0418cae71dcfb6b9531e71d6c05704", "collapsed": false, "_cell_guid": "5b78bca1-8698-440e-8744-ff721e86d1b5", "_execution_state": "idle"}, "source": "The above table indicates that the median of `trip_duration` between mid-night to 2pm is high *when taxis don't have any passengers* (`passenger_count = 0`). One explanations is it's likely taxis are spending more time looking for passengers between mid-night until 9am and traffic is intense from 9am - 2pm! \n\nAlso other values of `passenger_count` over time (increasing between mid-night to 2pm then decreasing after 2pm) could show that traffic is likely to blame for such behavior of medians. Moreover, 2pm seems to be rush hour in NY.", "outputs": [], "cell_type": "markdown", "execution_count": null}, {"metadata": {"_uuid": "8dfd8503fdfdad2d82d8c604818cac714e40e013", "collapsed": false, "trusted": false, "_cell_guid": "9a4e41b8-d674-454e-b13d-34f76d110f26", "_execution_state": "idle"}, "source": "train_pure['pickup_date'] = train_pure['pickup_datetime'].dt.date\ntrain_pure['pickup_time'] = train_pure['pickup_datetime'].dt.time\ntrain_pure['pickup_month'] = train_pure['pickup_datetime'].dt.month\ntrain_pure['pickup_day'] = train_pure['pickup_datetime'].dt.day\ntrain_pure['pickup_hour'] = train_pure['pickup_datetime'].dt.hour", "outputs": [], "cell_type": "code", "execution_count": 22}, {"metadata": {"_uuid": "d668bc412e61946a79dbdbd343f48acfff89ec5c", "collapsed": false, "trusted": false, "_cell_guid": "807025e6-8eef-44e8-9646-638b4833c250", "_execution_state": "idle"}, "source": "train_pkdt = train_pure.set_index('pickup_datetime')\n\nfig, ax = plt.subplots(2, 1, figsize=(12, 10))\ntrain_pkdt['trip_duration'].resample('M').median().plot(style='-', ax=ax[0])\ntrain_pkdt['trip_duration'].resample('W').median().plot(style='--', ax=ax[0])\ntrain_pkdt['trip_duration'].resample('D').median().plot(style=':', ax=ax[0])\ntrain_pkdt['trip_duration'].resample('H').median().plot(alpha=0.3, color='k', ax=ax[0])\nax[0].set_xlabel('')\nax[0].set_ylabel('median')\nax[0].set_title('Median trip durations over difference time intervals')\nax[0].legend(['Monthly', 'Weekly', 'Daily', 'Hourly'], loc='upper right')\n\ntrain_pkdt['trip_duration'].resample('D').count().plot(ax=ax[1])\nax[1].set_xlabel('pickup datetime')\nax[1].set_ylabel('count')\n\nfig.show();", "outputs": [], "cell_type": "code", "execution_count": 23}, {"metadata": {"_uuid": "8634bc459c2c45a01d546a33ce181cf05031641f", "collapsed": false, "_cell_guid": "e67e1563-9996-4906-ab84-58b7710b2765", "_execution_state": "idle"}, "source": "Time series above, clearly is showing the increase in hourly and daily `tip_duration` median or daily trip count in end of January 2016 which NY happened to be [hit by blizzard](https://www.weather.gov/okx/Blizzard_Jan2016) in that period.\n\nLet's look closer now", "outputs": [], "cell_type": "markdown", "execution_count": null}, {"metadata": {"_uuid": "9137b5d3a21ccbb24fd55b7735fcd51772a66731", "collapsed": false, "trusted": false, "_cell_guid": "0c2a2652-eb64-4cd6-a655-66973e69725a", "_execution_state": "idle"}, "source": "train_pkdt['trip_duration'].resample('M').median().plot(style='-')\ntrain_pkdt['trip_duration'].resample('W').median().plot(style='--')\ntrain_pkdt['trip_duration'].resample('D').median().plot(style=':')\nplt.xlabel('pickup datetime')\nplt.title('Median trip durations over difference time intervals')\nplt.legend(['Monthly', 'Weekly', 'Daily'], loc='best')\nplt.show();", "outputs": [], "cell_type": "code", "execution_count": 24}, {"metadata": {"_uuid": "c55c430ac1c4dc302a931de81937bea0881a29ff", "collapsed": false, "trusted": false, "_cell_guid": "5546e6a2-5c58-4f28-a5e6-8790e14af4c5", "_execution_state": "idle"}, "source": "", "outputs": [], "cell_type": "code", "execution_count": 24}, {"metadata": {"_uuid": "ad2826377799e0d0d92d62aa14a7634f07b19dde", "collapsed": false, "_cell_guid": "5d821bf7-82c5-4e28-ab5f-de84e409a3c5", "_execution_state": "idle"}, "source": "## Geo features", "outputs": [], "cell_type": "markdown", "execution_count": null}, {"metadata": {"_uuid": "9cc3eba44c1e61f209f89ea5185ddffce60292de", "collapsed": false, "trusted": false, "_cell_guid": "770b074a-1b6b-4f9a-9379-e15b59f16e04", "_execution_state": "idle"}, "source": "", "outputs": [], "cell_type": "code", "execution_count": null}, {"metadata": {"_uuid": "18a1efc72229d0532eeeb77fc08a66bb9d608437", "collapsed": false, "_cell_guid": "f622602e-6415-4248-9162-9e1f04195088", "_execution_state": "idle"}, "source": "# Feature Engineering", "outputs": [], "cell_type": "markdown", "execution_count": null}, {"metadata": {"_uuid": "0467f618c50b7e5c652965ccdb7e08d43098f6bb", "collapsed": false, "trusted": false, "_cell_guid": "02a68366-dc31-47e2-b384-eed48a02df3b", "_execution_state": "idle"}, "source": "", "outputs": [], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "6bdcdd2389c09297ede32ed35f937b8b9047c12c", "collapsed": false, "_cell_guid": "fb54c83f-4eb0-43f3-824d-90c5298a1ae9", "_execution_state": "idle"}, "source": "# Predictive Modeling", "outputs": [], "cell_type": "markdown", "execution_count": null}, {"metadata": {"_uuid": "7c725d30c378938311665181de666f4e11d67393", "collapsed": false, "trusted": false, "_cell_guid": "72f62c17-4e0a-4826-b165-2257f447beaa", "_execution_state": "idle"}, "source": "from fbprophet import Prophet", "outputs": [], "cell_type": "code", "execution_count": 25}, {"metadata": {"_uuid": "9eee82b701813da3110ba8c46d34a264f0ed5e56", "collapsed": false, "_execution_state": "idle"}, "source": "df_train = train_pkdt['trip_duration'].reset_index()\ndf_train.rename(columns={'pickup_datetime': 'ds', 'trip_duration': 'y'}, inplace=True)\n\ndf_test = test['pickup_datetime'].rename(columns={'pickup_datetime': 'ds'})", "outputs": [], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "dfc4b216d57ee4cc2b003a66816d766db8a4c487", "collapsed": false, "_execution_state": "idle"}, "source": "%%time\nprophet = Prophet(interval_width=0.95)\nprophet.fit(df_train.loc[:1000, :])", "outputs": [], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "248c4a0b03059361f5833828db4a6d10b6ee1e80", "collapsed": false, "_execution_state": "idle"}, "source": "", "outputs": [], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "c21cdf356c08dccfcd03366392ece757ca93f79b", "collapsed": false, "_execution_state": "idle"}, "source": "pred = prophet.predict(df_test.loc[:1000])", "outputs": [], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "2f813b59cbb659a7f66f65f26574204787c0587e", "collapsed": false, "_execution_state": "idle"}, "source": "df_test.head()\n\n", "outputs": [], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "4fcb23000fb826a4aa25e92e700f7bf6e40c34c2", "collapsed": false, "_execution_state": "idle"}, "source": "", "outputs": [], "execution_count": null, "cell_type": "code"}]}