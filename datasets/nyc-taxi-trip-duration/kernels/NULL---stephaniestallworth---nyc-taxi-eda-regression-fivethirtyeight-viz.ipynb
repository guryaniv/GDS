{"nbformat_minor": 1, "metadata": {"kernelspec": {"language": "python", "display_name": "Python 3", "name": "python3"}, "language_info": {"file_extension": ".py", "nbconvert_exporter": "python", "name": "python", "mimetype": "text/x-python", "codemirror_mode": {"version": 3, "name": "ipython"}, "pygments_lexer": "ipython3", "version": "3.6.1"}}, "nbformat": 4, "cells": [{"metadata": {"_uuid": "0fa4bd9753ca978f8ac64af38be86cea7be8976e", "_cell_guid": "42072836-14a5-45b3-97ce-0de308ac6bf7"}, "source": ["# **EXECUTIVE SUMMARY**    \n", "The objective of this kernel is to build a machine learning model that will predict taxicab trip durations based on 2016 NYC Yellow Cab trip record data.\n", "To that end, I will demonstrate four phases of the data science pipeline:\n", "1. **[Preprocessing](#prep) **: Clean and transform the data into a usable format for analysis.\n", "2. **[Exploratory Analysis](#eda)**: Perform exploratory analysis to identify the best features to be used for modeling.  Graphs have been stylized to replicate visualizations published by popular data science blog, \"FiveThirtyEight\".  A full tutorial can be found [here](https://www.dataquest.io/blog/making-538-plots/).\n", "2. **[Algorithm Development](#ml)**: Train, test, and refine various models to predict the target variable.  Given that our dependent variable `trip_duration` is a continuous outcome,  the regression algorithms to be protoyped are as follows:\n", " - [Multivarite Linear Regression](#linear)  \n", " - [Decision Tree](#tree)\n", " - [Random Forest](#random)\n", "3. **[Model Deployment](#deployment)**: Apply the best performing model to the test set for contest submission.\n", "\n", "\n", "# **ABOUT THE DATA **  \n", "The  dataset is based on the 2016 NYC Yellow Cab trip record data made available in Big Query on Google Cloud Platform.  Its variables are as follows:\n", "\n", "| **Variable Name** | **Description**|          \n", "| ------------------ |-------------|   \n", "| id      | a unique identifier for each trip |\n", "|vendor_id    | a code indicating the provider associated with the trip record     \n", "|pickup_datetime |  date and time when the meter was engaged|  \n", "|dropoff_datetime|  date and time when the meter was disengaged|  \n", "|passenger_count|  the number of passengers in the vehicle (driver entered value)|  \n", "|pickup_longitude | the longitude where the meter was engaged|  \n", "|pickup_latitude  |   the latitude where the meter was engaged|  \n", "|dropoff_longitude  |   the longitude where the meter was engaged|  \n", "|dropoff_latitude | the latitude where the meter was disengaged|\n", "|store_and_fwd_flag | This flag indicates whether the trip record was held in vehicle memory before sending to the vendor because the vehicle did not have a connection to the server - Y=store and forward; N=not a store and forward trip\n", "|trip_duration | duration of the trip in seconds\n", "\n"], "cell_type": "markdown"}, {"metadata": {"_uuid": "ac2a30a0c885251a47fefd183382ca37834e9d70", "_cell_guid": "d87c3684-3171-403d-a0f3-9501f5402c4f"}, "source": ["# <a id=\"prep\"></a>**1. DATA PRE-PROCESSING**\n", " First, the data will be loaded and cleaned into a usuable format for analysis. Specifically, I'll need to address:  \n", " - [missing data](#missing)\n", " - [outliers](#outliers)  \n", " - [data types](#types)\n", " - [feature engineering](#engineering)\n"], "cell_type": "markdown"}, {"metadata": {"collapsed": true, "_uuid": "ac5a176a69d36c0e68e8df908a51c76c258bfb18", "_cell_guid": "493c3f33-ed6a-469b-b8f0-56bcd4fbb13e"}, "outputs": [], "source": ["# Imports\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "%matplotlib inline\n", "import matplotlib.mlab as mlab\n", "import pandas as pd\n", "import seaborn as sns\n", "import sklearn\n", "import warnings\n", "warnings.filterwarnings(\"ignore\")\n", "\n", "# Settings\n", "import matplotlib\n", "matplotlib.style.use('fivethirtyeight')\n", "plt.rcParams['figure.figsize'] = (8.5, 5)\n", "plt.rcParams[\"patch.force_edgecolor\"] = True\n", "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n", "sns.mpl.rc(\"figure\", figsize=(8.5,5))\n", "\n", "# Read data\n", "train_data = pd.read_csv('../input/train.csv')\n", "\n", "# View data\n", "train_data.head()"], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "506153eb8a8b86227d02c437f739859fbe842a48", "_cell_guid": "af3d041e-78a8-4e20-86df-6666280f7650"}, "source": ["## <a id=\"missing\"></a> Data Pre-Processing: Missing Data \n", "After loading the data, I'll examine its struture to identify any missing observations that will need to be addressed."], "cell_type": "markdown"}, {"metadata": {"collapsed": true, "_uuid": "e049266cd21c1a8a29fcad91835d084c5cf07ff5", "_cell_guid": "0f483a32-e671-4b2e-8887-f5ae4940984b"}, "outputs": [], "source": ["# Data Shape\n", "print('Data Shape',train_data.shape)\n", "train_data.info()"], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "d0066cdec98ffb4eb9614ce4a4c922da18bc8c3f", "_cell_guid": "09bc5251-903b-4fed-a75c-d126d84c0ebf"}, "source": ["Based on the entry totals above, there are no missing observations  to be imputed. <br/>\n", " \n", "## <a id=\"outliers\"></a> Data Pre-Processing: Outliers\n", "Next, I will look at a statistical summary of the data to identify any obvious outliers."], "cell_type": "markdown"}, {"metadata": {"collapsed": true, "_uuid": "ae890048b40610c4e8c65f74b229225888379bf9", "_cell_guid": "eb57273d-f3ca-437a-9b6e-1bd02257225f"}, "outputs": [], "source": ["# Statistical summary\n", "train_data.describe().transpose()"], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "4e3f6b90b9045cec30881862f3d3a6e4a6bd73b8", "_cell_guid": "276a08e0-d64f-4085-aacb-dfb01b31d468"}, "source": ["**passenger_count**  \n", "The `passenger_count` variable has a minimum value of 0 passengers, which does not make sense in the context of this business case.  These observations are most likely errors and will need to removed from the dataset.    \n", "\n", "Another red flag is that `passenger_count` has a maximum value of 9 passengers -  highly unlikely for an NYC taxicab.   According to the NYC Taxi & Limousine Commission, the maximum number of people allowed in a yellow taxicab, by law, is five passengers (in a five passenger taxicab).  There are exceptions for passengers under the age of 7 who may sit on an adult's lap. However, it is unlikely that a full 5 person taxi cab would have that many children under the age of 7 on board to yield a passenger count as high as 9.   This observation is likely an error and will also to be removed from the dataset.   \n", "\n", "**Longitude and Latitude Coordinates**  \n", "Based on different coordinate estimates of New York City, the latitude and longitude ranges are as follows:  \n", "- Latitude is between 40.7128 and 40.748817\n", "- Longitude is between  - 74.0059 and  - 73.968285 \n", "\n", "The statisical summary of pick-up and drop-off coordinates show max and min observations that fall outside of the NYC city coordinate range.  I will exclude these data points as this analysis is  limited to New York City proper.\n", "\n", "**trip_duration**  \n", "Lastly, there are unusual observations present in our target variable, `trip_duration`.  A max trip_duration of 3526282.00 sec (~ 980 hours) is not a  realistic trip time - a clear indication that outliers are present in the data.  A systematic way to remove these outliers is to exclude all data points that are a specified number of standard deviations away from the mean.  In this case, I will remove `trip_duration` observations that are more than two standard deviations away from the mean duration time, 959.492 sec (~15.99 min)."], "cell_type": "markdown"}, {"metadata": {"collapsed": true, "_uuid": "c140a82989c6baea686d89b36edc250ab2494cf4", "_cell_guid": "4fa9462b-021a-48e6-9f0c-fe49e9d11857"}, "outputs": [], "source": ["# Remove passenger_count outliers\n", "train_data = train_data[train_data['passenger_count']>0]\n", "train_data = train_data[train_data['passenger_count']<9]\n", "\n", "# train_data = train_data[train_data['pickup_longitude'] <= -73.968285]\n", "# train_data = train_data[train_data['pickup_longitude'] >= -74.0059]\n", "# train_data = train_data[train_data['pickup_latitude'] <= 40.748817]\n", "# train_data = train_data[train_data['pickup_latitude'] >= 40.7128]\n", "# train_data = train_data[train_data['dropoff_longitude'] <= -73.968285]\n", "# train_data = train_data[train_data['dropoff_longitude'] >= -74.0059]\n", "# train_data = train_data[train_data['dropoff_latitude'] <= 40.748817]\n", "# train_data = train_data[train_data['dropoff_latitude'] >= 40.7128]\n", "\n", "# Remove coordinate outliers\n", "train_data = train_data[train_data['pickup_longitude'] <= -73.75]\n", "train_data = train_data[train_data['pickup_longitude'] >= -74.03]\n", "train_data = train_data[train_data['pickup_latitude'] <= 40.85]\n", "train_data = train_data[train_data['pickup_latitude'] >= 40.63]\n", "train_data = train_data[train_data['dropoff_longitude'] <= -73.75]\n", "train_data = train_data[train_data['dropoff_longitude'] >= -74.03]\n", "train_data = train_data[train_data['dropoff_latitude'] <= 40.85]\n", "train_data = train_data[train_data['dropoff_latitude'] >= 40.63]\n", "\n", "# Remove trip_duration outliers\n", "trip_duration_mean = np.mean(train_data['trip_duration'])\n", "trip_duration_std = np.std(train_data['trip_duration'])\n", "train_data = train_data[train_data['trip_duration']<=trip_duration_mean + 2*trip_duration_std]\n", "train_data = train_data[train_data['trip_duration']>= trip_duration_mean - 2*trip_duration_std]\n", "\n", "# Confirm removal\n", "train_data.describe().transpose()"], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "a1e940dcb7603807dcdb0b9e6b57f0ccac1971f8", "_cell_guid": "dc37746b-a369-44ee-b884-dbaf29c331e8"}, "source": ["## <a id=\"types\"></a> **Data Pre-Processing: Data Types**  \n", "Next, I'll do a quick review of the data types to confirm that the variables are being assigned correctly."], "cell_type": "markdown"}, {"metadata": {"collapsed": true, "_uuid": "11f45f56bba48028c657256a875f29bb8a4ededd", "_cell_guid": "02d43ae5-8f88-420e-bde2-726e0d2fa56d"}, "outputs": [], "source": ["train_data.info()"], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "8807cc3161ea073dbafb7276d00b524828099dfc", "_cell_guid": "18b7a934-5a45-4522-b29e-1799ee1a5c5f"}, "source": ["The pickup and dropoff timestamp variables are being treated as non-null objects.  These features should be specified as date objects to allow for easier feature engineering and analysis later on.  "], "cell_type": "markdown"}, {"metadata": {"collapsed": true, "_uuid": "bbe3ce67bcad0c2afb324e92814b38bda712922b", "_cell_guid": "4a7c4bc6-c156-4f74-827c-6436fda3f3be"}, "outputs": [], "source": ["# Convert timestamps to date objects\n", "train_data['pickup_datetime'] = pd.to_datetime(train_data.pickup_datetime) # Pickups\n", "train_data['dropoff_datetime'] = pd.to_datetime(train_data.dropoff_datetime) # Drop-offs\n", "\n", "# Confirm changes\n", "train_data.info()"], "execution_count": null, "cell_type": "code"}, {"metadata": {"collapsed": true, "_uuid": "377fdf69dd52e38cf028b3b3b7f6bc060fc9e3fa", "_cell_guid": "55ca34bc-1b3b-48ee-bcf8-f8969d785d92"}, "source": ["## <a id=\"engineering\"></a> **Data Pre-Processsing: Feature Engineering**  \n", "The `pickup_datetime` and `dropoff_datetime` variables both combine date and time observations into the same column.  I will delimit this date and time information into separate columns to allow for easier analysis downstream.    \n", "\n", "The hour and day of week a passenger is picked up may influence trip duration so I will extract these features from the `pickup_datetime` variable also.  This is not necessary for `dropoff_datetime` because, logically, the day and hour a passenger is dropped off would have no bearing on `trip_duration` because this is information is recorded *after* the trip is completed.   "], "cell_type": "markdown"}, {"metadata": {"collapsed": true, "_uuid": "00a822867dab07a30acaa6a105ff319d767a8d7e", "_cell_guid": "6865a536-294d-4a0c-ac0e-17745380b5d7"}, "outputs": [], "source": ["# Delimit pickup_datetime variable \n", "train_data['pickup_date'] = train_data['pickup_datetime'].dt.date # Extract date\n", "train_data['pickup_time'] = train_data['pickup_datetime'].dt.time # Extract time\n", "\n", "# Delimit dropoff_datetime variables\n", "train_data['dropoff_date'] = train_data['dropoff_datetime'].dt.date # Extract date\n", "train_data['dropoff_time'] = train_data['dropoff_datetime'].dt.time # Extract time\n", "\n", "# Additional pickup features\n", "train_data['pickup_month'] = train_data['pickup_datetime'].dt.month # Extract month\n", "# train_data['pickup_month'] = train_data.pickup_datetime.dt.to_period('M') # Extract yearmonth\n", "#train_data['pickup_YYYYMM'] = train_data['pickup_datetime'].apply(lambda x: x.strftime('%Y%m')) # Extract yearmonth\n", "train_data['pickup_hour'] = train_data['pickup_datetime'].dt.hour # Extract hour\n", "train_data['pickup_weekday'] = train_data['pickup_datetime'].dt.dayofweek # Extract day of week\n", "\n", "# Drop concatentated timestamp columns\n", "train_data.drop(['pickup_datetime'], axis = 1, inplace = True)\n", "train_data.drop(['dropoff_datetime'], axis = 1, inplace = True)\n", "\n", "# Confirm changes\n", "train_data.columns"], "execution_count": null, "cell_type": "code"}, {"metadata": {"collapsed": true, "_uuid": "a2e475447befa56e4874f28ebc36305a498d4ac8", "_cell_guid": "8daabb8c-ff31-43bc-8e4c-6d89ab8a4053"}, "source": ["# <a id=\"eda\"></a>**2.  EXPLORATORY DATA ANALYSIS**\n", "## Target Variable: trip_duration\n", "The target variable we are trying to predict is `trip_duration`. I'll need to examine its distribution to see if there are transformations that need to be applied.  \n"], "cell_type": "markdown"}, {"metadata": {"collapsed": true, "_uuid": "539acd3ba7402c80f7ec781ab1ce97f9ced46787", "_cell_guid": "08f0ee26-7804-4b4c-9c58-e34fd2b85306"}, "outputs": [], "source": ["# Mean distribution\n", "mu = train_data['trip_duration'].mean()\n", "\n", "# Std distribution\n", "sigma = train_data['trip_duration'].std()\n", "num_bins = 100\n", "\n", "# Histogram \n", "fig = plt.figure(figsize=(8.5, 5))\n", "n, bins, patches = plt.hist(train_data['trip_duration'], num_bins, normed=1,\n", "                           edgecolor = 'black', lw = 1, alpha = .40)\n", "# Normal Distribution\n", "y = mlab.normpdf(bins, mu, sigma)\n", "plt.plot(bins, y, 'r--', linewidth=2)\n", "plt.xlabel('trip_duration')\n", "plt.ylabel('Probability density')\n", "\n", "# Adding a title\n", "plt.title(r'$\\mathrm{Trip\\ duration\\ skewed \\ to \\ the \\ right:}\\ \\mu=%.3f,\\ \\sigma=%.3f$'%(mu,sigma))\n", "plt.grid(True)\n", "#fig.tight_layout()\n", "plt.show()\n", "\n", "# Statistical summary\n", "train_data.describe()[['trip_duration']].transpose()"], "execution_count": null, "cell_type": "code"}, {"metadata": {"collapsed": true, "_uuid": "841c83515bc0d71d0acea154fba4b7b5fad13039", "_cell_guid": "91db33b0-e3a3-4670-a50c-dde9a85a0d9c"}, "source": ["There is a clear indication that `trip_duration` is highly skewed to the right based on two key signals:  \n", "- Skewness value > 1.0\n", "- Long right tail\n", "\n", "The median `trip_duration` is only 655 seconds (~11 min).  However, there are `trip_duration` observations as high as 11,411 sec (3.12 hours) that were not removed previously as they are still within two standard deviations of the mean (our specified outlier cutoff).  As a result, these high `trip_duration` observations are skewing the distribution to the right.  \n", "\n", "Thus, applying the log transformation to `trip_duration` will normalize its distribution and reduce the influence of these high observations in the right tail.     \n", "\n", "## **Feature Variables**\n", "The remaining columns in our dataset are the 'feature variables'.  These are the variables that will be fed into our machine learning model to predict the dependent variable, `trip_duration`.   I will explore each one of these features to better understand the information it contains and what transformations are needed before we can proceed to the learning process.  "], "cell_type": "markdown"}, {"metadata": {"collapsed": true, "_uuid": "9bb474c7b5f608312884b700102d3b024f2ac9da", "_cell_guid": "fabdb9e8-470d-448d-95f0-96dadaac8950"}, "outputs": [], "source": ["# Feature names\n", "train_data.columns"], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "7a53461a14b74d17988015ce591d0871079dd32f", "_cell_guid": "60e2e744-b6e6-482d-b4ed-b7d9a6094b35"}, "source": ["** Id**  \n", "The `id` variable is a unique identifier of each trip.  I will explore how this feature varies over time (if at all)."], "cell_type": "markdown"}, {"metadata": {"collapsed": true, "_uuid": "e22d88812357b6c53fea85850957d74027efcc8f", "_cell_guid": "9aef49bc-c4c2-407e-a362-56de4f04ccbe"}, "outputs": [], "source": ["# Summarize total trips by day\n", "pickups_by_day = train_data.groupby('pickup_date').count()['id']\n", "\n", "# Create graph\n", "pickups_graph = pickups_by_day.plot(x = 'pickup_date', y = 'id', figsize = (8.5,5),legend = True)\n", "\n", "# Customize tick size\n", "pickups_graph.tick_params(axis = 'both', which = 'major', labelsize = 12)\n", "\n", "# Bold horizontal line at y = 0\n", "pickups_graph.axhline(y = 0, color = 'black', linewidth = 1.3, alpha = .7)\n", "\n", "# Customize tick labels of the y-axis\n", "#pickups_graph.set_yticklabels(labels = [-10, '2000   ', '4000   ', '6000   ', '8000   ', '10000   '])\n", "\n", "# Add an extra vertical line by tweaking the range of the x-axis\n", "pickups_graph.set_xlim(left = '2015-12-31', right = '2016-06-30')\n", "\n", "# Remove the label of the x-axis\n", "pickups_graph.xaxis.label.set_visible(False)\n", "\n", "# Add signature bar\n", "pickups_graph.text(x = '2015-12-15', # Adjusts left side of signature bar,has to be in same coordiantes as x-axis\n", "               y = -2500, \n", "               s = '    \u00a9KAGGLE                                          Source: NYC Taxi and Limousine Commission (TLC)   ', # copyright symbol ALT + 0169\n", "              fontsize = 14, color = '#f0f0f0', backgroundcolor = 'grey')\n", "\n", "# Adding a title and a subtitle\n", "pickups_graph.text(x = '2015-12-18', y = 11800,\n", "                   s = \"Dramatic drop in total trips in late January or early February\",\n", "                   fontsize = 20, weight = 'bold', alpha = .90)\n", "\n", "pickups_graph.text(x = '2015-12-18', y = 11000, \n", "                   s = 'Decline is isolated to a specific day so may be more than just seasonal effects.',\n", "                   fontsize = 14, alpha = .85)\n", "pickups_graph.text(x = '2016-01-27', y = 1500, s = 'What happened?',weight = 0, rotation = 0, backgroundcolor = '#f0f0f0', size = 14)\n", "plt.show()"], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "58d0296db4f19491338c7dd6a5487f317655657a", "_cell_guid": "05629aeb-4770-4056-a12a-2d9ac013eacf"}, "source": ["**Trip Id Over Time**  \n", "There is a unusual drop in the total number of `id` around late January.  At first glance, it is easy to assume that this could be just seasonality.   However, the decrease is much more drastic relative to other winter days before/after the drop and looks to be isolated around a single day.  Thus, a more plausible explanation for this outlier could be order entry error or some other extraneous event.  "], "cell_type": "markdown"}, {"metadata": {"collapsed": true, "_uuid": "d272c604e20fde7cd7b1e36423e4d3b65dfb1214", "_cell_guid": "56db7d1a-2d3b-4753-97fa-180918d04fb3"}, "outputs": [], "source": ["# Identify where drop occured\n", "train_data.groupby('pickup_date').count()['id'].sort_values(ascending = True)[[0]]"], "execution_count": null, "cell_type": "code"}, {"metadata": {"collapsed": true, "_uuid": "90c01e1e056e320e70cd5f8ba0a740c480611278", "_cell_guid": "216cca6f-e9df-4fd6-9902-7a1ea36ec377"}, "source": ["Upon further investigation, the drop occured on January 23, 2016 - the date of New York's first big snow storm of the year, where they were hit with 26.8 inches of snowfall.  Although there was a significant decline in the overall number of taxi rides, the median `trip_duration` of the rides that *were* given that day of 456.5 seconds does not seem to be out of the ordinary.  \n", "\n", "Although the `id` variable provides interesting insight about trips over time, the actual Id of a trip record will not be useful in predicting `trip_duration` in our alogrithm.  Thus, I will remove this feature when it comes time to train the model.  <br/><br/>\n", "\n", "**Vendor Id**  \n", "The `vendor_id` variable is a code indicating the provider associated with each trip record. I will examine if there is a provider that takes longer trips relative to the others. "], "cell_type": "markdown"}, {"metadata": {"collapsed": true, "_uuid": "64ed7617a38f75b91d9bf7c3db1e163d2d055a65", "_cell_guid": "a5071474-e8fa-41f8-a402-7a6701aef647"}, "outputs": [], "source": ["# Create boxplot\n", "plt.figure(figsize=(8.5,5))\n", "vendor_graph = sns.boxplot(x = 'vendor_id', y = 'trip_duration', data = train_data, \n", "                          palette = 'gist_rainbow', linewidth = 2.3)\n", "\n", "# Customize tick labels of the y-axis\n", "vendor_graph.set_yticklabels(labels = [-10, '0  ', '2000  ', '4000  ', '6000  ', '8000  ', '10000  ','12000 s'])\n", "\n", "# Bolding horizontal line at y = 0\n", "vendor_graph.axhline(y = 0, color = 'black', linewidth = 1.3, alpha = .70)\n", "\n", "# Remove the label of the x-axis\n", "vendor_graph.xaxis.label.set_visible(False)\n", "vendor_graph.yaxis.label.set_visible(False)\n", "\n", "# Add signature bar\n", "vendor_graph.text(x = -.66, # Adjusts left side of signature bar\n", "               y = -2500,  \n", "               s = '   \u00a9KAGGLE                                                 Source: NYC Taxi and Limousine Commission (TLC)   ', # copyright symbol ALT + 0169\n", "              fontsize = 14, color = '#f0f0f0', backgroundcolor = 'grey') \n", "\n", "# # Adding a title and a subtitle\n", "vendor_graph.text(x =-.66, y = 13800, s = \"Trip durations are similar between NYC taxi vendors\",\n", "               fontsize =20 , weight = 'bold', alpha = .90)\n", "vendor_graph.text(x = -.66, y = 13000.3, \n", "               s = 'Both have a median trip time ~650 seconds with many outliers',\n", "              fontsize = 14, alpha = .85)\n", "plt.show()\n", "\n", "# Statistical summary\n", "train_data.groupby('vendor_id')['trip_duration'].describe()\n"], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "7047955e01c654329108d956133ba545f68c0bb4", "_cell_guid": "8492f486-6224-4f79-8bfa-99bf3d1f0a31"}, "source": ["The median `trip_duration` is similar between the two vendors. However, it is worth nothing that each vendor also has a significant number of outliers beyond the upper fence.  That is, both have outliers that are greater than the upper quartile by at least 1.5x the interquartile range.   "], "cell_type": "markdown"}, {"metadata": {"_uuid": "5d8550d72c3da3f231c01d44496890bfd6c92959", "_cell_guid": "f587a448-eeee-4f97-9482-b22700b51cfd"}, "source": ["**store_and_fwd_flag**  \n", "The `store_and_fwd_flag` variable indicates whether the trip record was stored in vehicle memory before forwarding to the vendor because the vehicle did not have a direct connection to the server.  An immediate question that comes to mind is if these 'Store and Forward' trips are contributing to the `trip_duration` outliers for the two vendors noted above.  "], "cell_type": "markdown"}, {"metadata": {"collapsed": true, "_uuid": "2e5ec6d094578a8c0121a88e2938633e5c24bb52", "_cell_guid": "e7041d76-6d9f-4872-bcf6-db2d58961e7a"}, "outputs": [], "source": ["# Create boxplot\n", "plt.figure(figsize=(8.5,5))\n", "vendor_graph = sns.boxplot(x = 'vendor_id', y = 'trip_duration', data = train_data, \n", "                          orient = 'v',color = 'lightgrey', linewidth = 2.3)\n", "plt.setp(vendor_graph.artists, alpha = 0.5)\n", "\n", "# Create strip plot\n", "sns.stripplot(data = train_data, x = 'vendor_id', y = 'trip_duration', jitter = 1, size = 5,\n", "             edgecolor = 'black', linewidth = .2,palette = 'gist_rainbow_r',hue = 'store_and_fwd_flag')\n", "\n", "# Customize tick size\n", "vendor_graph.tick_params(axis = 'both', which = 'major', labelsize = 12)\n", "\n", "# Customize tick labels of the y-axis\n", "vendor_graph.set_yticklabels(labels = [-10, '0  ', '2000  ', '4000  ', '6000  ', '8000  ', '10000  ','12000 s'])\n", "\n", "# Bolding horizontal line at y = 0\n", "vendor_graph.axhline(y = 0, color = 'black', linewidth = 1.3, alpha = .70)\n", "\n", "# Remove the label of the x-axis\n", "vendor_graph.xaxis.label.set_visible(False)\n", "vendor_graph.yaxis.label.set_visible(False)\n", "\n", "# Add signature bar\n", "vendor_graph.text(x = -.66, # Adjusts left side of signature bar\n", "               y = -2500,  \n", "               s = '   \u00a9KAGGLE                                                 Source: NYC Taxi and Limousine Commission (TLC)   ', # copyright symbol ALT + 0169\n", "              fontsize = 14, color = '#f0f0f0', backgroundcolor = 'grey') \n", "\n", "# Adding a title and a subtitle\n", "vendor_graph.text(x =-.66, y = 13800, s = 'Store-and-forward trips found for Vendor 1 only',\n", "               fontsize =20 , weight = 'bold', alpha = .90)\n", "vendor_graph.text(x = -.66, y = 13000.3, \n", "               s = 'However, server connection does not have much bearing on the high number of outliers',\n", "              fontsize = 14, alpha = .85)\n", "# Format legend\n", "vendor_graph.legend(title = 'store_and_fwd_flag', bbox_to_anchor = (.80,1),loc = 2, fontsize=12)\n", "plt.show()\n", "\n", "# Statistical summary\n", "train_data.groupby(['vendor_id','store_and_fwd_flag'])['store_and_fwd_flag'].count().unstack().fillna(0)\n"], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "771c1633726b84cafe1f7fc16023d2aecd4c1dea", "_cell_guid": "3d1e1442-7f11-4338-b24f-471c4126ea62"}, "source": ["Only Vendor 1 had trip records that were stored and forwarded to the vendor rather than directly onto the server.  Intially, I thought these 'Store and Forward' trips would be reponsible for most of that vendor's outliers, but these trips account for only a small portion of Vendor 1 observations.  Most of the outliers are \"normal\" trips that were directly stored onto the Vendor server.  Thus, this `store_and_fwd_flag`  may not be informative in predicting trip_duration times.   <br/> <br/>\n", "\n", "**Passenger Count**  \n", " The `passenger_count` variable is the number of passengers in the vehicle as inputed by the driver.  The assumption is that trips with more passengers are inherently longer due to more stops, but I will explore this variable to test this assumption."], "cell_type": "markdown"}, {"metadata": {"collapsed": true, "_uuid": "84a332eee26f6ba55ba58f7d4c4038157bde39b4", "_cell_guid": "60851d87-6d57-468c-bda0-ee0ba396ed06"}, "outputs": [], "source": ["# Settings\n", "import matplotlib\n", "matplotlib.style.use('fivethirtyeight')\n", "\n", "# Create boxplot\n", "plt.figure(figsize=(8.5,5))\n", "passenger_graph = sns.boxplot(x = 'passenger_count', y = 'trip_duration', data = train_data, \n", "                          palette = 'gist_rainbow', linewidth = 2.3)\n", "\n", "# Customize tick size\n", "passenger_graph.tick_params(axis = 'both', which = 'major', labelsize = 12)\n", "\n", "# Customize tick labels of the y-axis\n", "passenger_graph.set_yticklabels(labels = [-10, '0  ', '2000  ', '4000  ', '6000  ', '8000  ', '10000  ','12000 s'])\n", "\n", "# Bolding horizontal line at y = 0\n", "passenger_graph.axhline(y = 0, color = 'black', linewidth = 1.3, alpha = .70)\n", "\n", "# Add an extra vertical line by tweaking the range of the x-axis\n", "#month_graph.set_xlim(left = -1, right = 6)\n", "\n", "# Remove the label of the x-axis\n", "passenger_graph.xaxis.label.set_visible(False)\n", "passenger_graph.yaxis.label.set_visible(False)\n", "\n", "# Add signature bar\n", "passenger_graph.text(x = -1.1, # Adjusts left side of signature bar\n", "               y = -2500,  \n", "               s = '   \u00a9KAGGLE                                                 Source: NYC Taxi and Limousine Commission (TLC)   ', # copyright symbol ALT + 0169\n", "              fontsize = 14, color = '#f0f0f0', backgroundcolor = 'grey') \n", "\n", "# Alternative signature bar\n", "# fte_graph.text(x = 1967.1, y = -6.5,\n", "#               s = '________________________________________________________________________________________________________________',\n", "#               color = 'grey', alpha = .70)\n", "# fte_graph.text(x = 1966.1, y = -9,\n", "#               s ='   \u00a9DATAQUEST                                                                               Source: National Center for Education Statistics   ', # copyright symbol ALT + 0169\n", "#               fontsize = 14, color = 'grey', alpha = .7)\n", "\n", "# # Adding a title and a subtitle\n", "passenger_graph.text(x =-1.05, y = 13800, s = \"Passenger count does not have much effect on trip duration\",\n", "               fontsize =20 , weight = 'bold', alpha = .90)\n", "passenger_graph.text(x = -1.05, y = 13000.3, \n", "               s = 'Median trip times remain similar despite more passengers being aboard',\n", "              fontsize = 14, alpha = .85)\n", "plt.show()\n", "\n", "# Statistical summary\n", "train_data.groupby('passenger_count')['trip_duration'].describe().transpose()"], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "9c432b51362308fba5c4d4b539bbdc318e91b57a", "_cell_guid": "69a70467-16dc-4b6a-a1c9-7f7105f4414d"}, "source": ["Surpringly, the median `trip_duration` does not vary much as passenger_count increases.  Trips with just one passenger seem to have more outliers than other trips.   <br/> <br/>\n", "\n", "**Trip Duration by Pickup Hour and Day**  \n", "I suspect that trips are longer on the weekends due to higher traffic levels, but will need to explore if there are other times during the week `trip duration` is higher than average.  "], "cell_type": "markdown"}, {"metadata": {"collapsed": true, "scrolled": false, "_uuid": "7a2a068734b6dc1c71000d4e2ec5a48f7f61a0a7", "_cell_guid": "e48abdd0-75f4-4376-82c6-353cbbd2dd2a"}, "outputs": [], "source": ["# Trips by Hour and Day of Week\n", "trip_duration_median = train_data['trip_duration'].median()\n", "plt.figure(figsize=(8.5,5))\n", "pickup_hourday = train_data.groupby(['pickup_hour','pickup_weekday'])['trip_duration'].median().unstack()\n", "hourday_graph = sns.heatmap(pickup_hourday[pickup_hourday>trip_duration_median],\n", "                                   lw = .5, annot = True, cmap = 'GnBu', fmt = 'g',annot_kws = {\"size\":10} )\n", "# Customize tick label size\n", "hourday_graph.tick_params(axis = 'both', which = 'major', labelsize = 10)\n", "\n", "# Customize tick labels of the y-axis\n", "hourday_graph.set_xticklabels(labels = ['Mon', 'Tue', 'Wed','Thu','Fri','Sat','Sun'])\n", "\n", "# Bolding horizontal line at y = 0\n", "hourday_graph.axhline(y = 0, color = 'black', linewidth = 1.3, alpha = .70)\n", "\n", "# Remove the label of the x-axis\n", "hourday_graph.xaxis.label.set_visible(False)\n", "\n", "# Add signature bar\n", "hourday_graph.text(x = -.8,  y = -4,\n", "                   s = ' \u00a9KAGGLE                                          Source: NYC Taxi and Limousine Commission (TLC)   ',\n", "fontsize = 14, color = '#f0f0f0', backgroundcolor = 'grey') \n", "\n", "# # Adding a title and a subtitle\n", "hourday_graph.text(x =-.8, y = 27, s = \"Trip durations vary greatly depending on day of week\",\n", "               fontsize =20 , weight = 'bold', alpha = .90)\n", "hourday_graph.text(x =-.8, y = 25.5, \n", "               s = 'Median trip times longest during office hours and weekend nights',\n", "              fontsize = 14, alpha = .85)\n", "\n", "# plt.ylabel('pickup_hour (military time)')\n", "# plt.xlabel('pickup_weekday (Mon - Sun)')\n", "# plt.title('Median Trip Duration by Pickup Hour and Day of Week')\n", "plt.show()"], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "e0e8c2f2bcf51fa627f112931e39dc2b5219b09f", "_cell_guid": "46e1acde-c4d6-4546-b63c-be6f8095f73f"}, "source": ["Trips tend much longer than the median `trip_duration` of 655 seconds during the following parts of the week: \n", "- **Monday - Thursday Office Hours: ** 8:00 am  -  6:00 pm  \n", "- **Thursday, Friday, Saturday Nights: ** 6:00 pm - midnight\n", "- **Early Saturday & Sunday Mornings: **12:00 am  - 1:00 am  \n", "- **Sunday Afternoons:** 2:00 pm and 4:00 pm<br/><br/>\n", "\n", "**Trip Duration by Month**    \n", "Next, I'll examine if `trip_duration` varies by month due to seasonality.  \n", "\n"], "cell_type": "markdown"}, {"metadata": {"collapsed": true, "_uuid": "3bfadc2fb5652a8fda42ac2bafbdb966e76b734c", "_cell_guid": "a38e31cb-1b33-46e8-a7e2-6b199ec7e5ea"}, "outputs": [], "source": ["# Box plot of pickups by month\n", "import matplotlib\n", "matplotlib.style.use('fivethirtyeight')\n", "\n", "# Create boxplot\n", "plt.figure(figsize=(8.5,5))\n", "month_graph = sns.boxplot(x = 'pickup_month', y = 'trip_duration', data = train_data, \n", "                          palette = 'gist_rainbow', linewidth = 2.3)\n", "\n", "# Customize tick size\n", "month_graph.tick_params(axis = 'both', which = 'major', labelsize = 12)\n", "\n", "# Customize tick labels of the y-axis\n", "month_graph.set_yticklabels(labels = [-10, '0  ', '2000  ', '4000  ', '6000  ', '8000  ', '10000  ','12000 s'])\n", "\n", "# Bolding horizontal line at y = 0\n", "month_graph.axhline(y = 0, color = 'black', linewidth = 1.3, alpha = .70)\n", "\n", "\n", "# Add an extra vertical line by tweaking the range of the x-axis\n", "#month_graph.set_xlim(left = -1, right = 6)\n", "\n", "# Remove the label of the x-axis\n", "month_graph.xaxis.label.set_visible(False)\n", "month_graph.yaxis.label.set_visible(False)\n", "\n", "# Add signature bar\n", "month_graph.text(x = -1.1, # Adjusts left side of signature bar\n", "               y = -2500,  \n", "               s = '   \u00a9KAGGLE                                                 Source: NYC Taxi and Limousine Commission (TLC)   ', # copyright symbol ALT + 0169\n", "              fontsize = 14, color = '#f0f0f0', backgroundcolor = 'grey') \n", "\n", "# Alternative signature bar\n", "# fte_graph.text(x = 1967.1, y = -6.5,\n", "#               s = '________________________________________________________________________________________________________________',\n", "#               color = 'grey', alpha = .70)\n", "# fte_graph.text(x = 1966.1, y = -9,\n", "#               s ='   \u00a9DATAQUEST                                                                               Source: National Center for Education Statistics   ', # copyright symbol ALT + 0169\n", "#               fontsize = 14, color = 'grey', alpha = .7)\n", "\n", "# # Adding a title and a subtitle\n", "month_graph.text(x =-1.05, y = 13800, s = \"Month of transaction has minimal effect on trip duration\",\n", "               fontsize =20 , weight = 'bold', alpha = .90)\n", "month_graph.text(x = -1.05, y = 13000.3, \n", "               s = 'Median trip times hover around ~650 seconds throughout the year',\n", "              fontsize = 14, alpha = .85)\n", "plt.show()\n", "\n", "# Statistical summary\n", "train_data.groupby('pickup_month')['trip_duration'].describe().transpose()"], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "e56b521cc90bb24aaf33adabfb8d5e47be294c70", "_cell_guid": "8d385012-e5b5-4f95-9649-166cb653f47e"}, "source": ["June has the highest median `trip_duration` overall, but only slightly.  Median trip times seem to hover around the 10-12 minute mark and do not vary much from month-to-month.  This may be an indication that this month feature will not be helpful in predicting our target variable, `trip_duration`.  <br/><br/>\n", "**Plot Rides**   \n", "Next, I will plot the pickup and drop off points of each taxi ride.  "], "cell_type": "markdown"}, {"metadata": {"collapsed": true, "_uuid": "08f1707d4f40e59a9a803020cecea145ba20e06a", "_cell_guid": "11c2993d-7385-46ea-b4d8-354aec95ebc8"}, "outputs": [], "source": ["longitude = list(train_data.pickup_longitude) + list(train_data.dropoff_longitude)\n", "latitude = list(train_data.pickup_latitude) + list(train_data.dropoff_latitude)\n", "plt.figure(figsize = (10,8))\n", "plt.plot(longitude,latitude,'.',alpha = .40, markersize = .8)\n", "plt.title('Trip Plots')\n", "plt.show()"], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "8b16197ea4c42d77892a2bba0b085e688547b51e", "_cell_guid": "2c27e4fd-8b34-4c00-9b4d-1c6fb4c02090"}, "source": ["**Plot by Neighborhood**  \n", "Using the KMeans alogirthm, we can cluster the data points into the different neighborhoods of NYC.  "], "cell_type": "markdown"}, {"metadata": {"collapsed": true, "_uuid": "446b6eec3c3f527247fa741072a37d03a9b66077", "_cell_guid": "d59831df-4e8e-480a-b4ed-c05e5e769a23"}, "outputs": [], "source": ["# Create data frame of coordinates\n", "loc_df = pd.DataFrame()\n", "loc_df['longitude'] = longitude\n", "loc_df['latitude'] = latitude\n", "\n", "# Clusters of New York\n", "from sklearn.cluster import KMeans\n", "kmeans = KMeans(n_clusters=15, random_state=2, n_init = 10).fit(loc_df)\n", "loc_df['label'] = kmeans.labels_\n", "loc_df = loc_df.sample(200000)\n", "plt.figure(figsize = (12,7))\n", "for label in loc_df.label.unique():\n", "    plt.plot(loc_df.longitude[loc_df.label == label],loc_df.latitude[loc_df.label == label],'.', alpha = 0.8, markersize = 0.8)\n", "plt.title('Clusters of New York')\n", "plt.show()"], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "52423c8dc8b5ad107a0ca3981f4d0c96b512b54d", "_cell_guid": "b514e566-d46e-451f-a0d2-588e6a6be8cc"}, "source": ["**Relationships Between Variables**  \n", "Now that each feature has been explored individually, I will examine how they relate to the target variable as well as each other.  Variables that are highly correlated (correlation cofficient >.70) are likely to convey redudant information and can be removed from the dataset.  Reducing the data's dimentionality in this way will make the data easier to work with and allow for better model results.  "], "cell_type": "markdown"}, {"metadata": {"collapsed": true, "_uuid": "f0c06c053e0daec380db8babea54747974825920", "_cell_guid": "c813e62f-834b-48fa-bf91-972e1810bcde"}, "outputs": [], "source": ["# Correlations to trip_duration\n", "corr = train_data.select_dtypes(include = ['float64', 'int64']).iloc[:, 1:].corr()\n", "cor_dict = corr['trip_duration'].to_dict()\n", "del cor_dict['trip_duration']\n", "print(\"List the numerical features in decending order by their correlation with trip_duration:\\n\")\n", "for ele in sorted(cor_dict.items(), key = lambda x: -abs(x[1])):\n", "    print(\"{0}: {1}\".format(*ele))\n", "    \n", "# Correlation matrix heatmap\n", "corrmat = train_data.corr()\n", "plt.figure(figsize=(12, 7))\n", "\n", "# Number of variables for heatmap\n", "k = 76\n", "cols = corrmat.nlargest(k, 'trip_duration')['trip_duration'].index\n", "cm = np.corrcoef(train_data[cols].values.T)\n", "\n", "# Generate mask for upper triangle\n", "mask = np.zeros_like(cm, dtype=np.bool)\n", "mask[np.triu_indices_from(mask)] = True\n", "\n", "sns.set(font_scale=1)\n", "sns.heatmap(cm, mask=mask, cbar=True, annot=True, square=True,\\\n", "                 fmt='.2f',annot_kws={'size': 12}, yticklabels=cols.values,\\\n", "                 xticklabels=cols.values, cmap = 'coolwarm',lw = .1)\n", "plt.show() "], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "fcc8a5b2c89c7f4e1aaa0384d9836a0fec3b1c2a", "_cell_guid": "0f59ba24-7b42-4cc4-bace-b1e75f5d9062"}, "source": ["**Correlations**  \n", "Not suprisingly, the correlation coefficient of the coordinate features: `pickup_latitude`, `pickup_longitude`, `dropoff_latitude`, `dropoff_longitude`, indicates a linear relationship exists between them.  However, this correlation is under .50 and is not strong enough to merit removal from the data set.    \n", "\n", "There is a weak positive correlation between the longitude variables and `trip_duration`.  There is also a weak negative correlation betwen the latitude variables and `trip_duration`.    \n", "\n", "Otherwise, there doesn't appear to be much a of a *linear* relationship between our target variable and the remaining features.    \n", "\n", "# <a id=\"ml\"></a>**3. ALGORITHM DEVELOPMENT**  \n", "Now that I have a better sense of the dataset's features and target variable, I will need to make sure the data is \"model ready\" before prototyping a machine learning algorithm.  Namely, I'll need to confirm that:\n", "- there is no missing data\n", "- all categorical features are encoded as numerical\n", "- unnecessary features are removed"], "cell_type": "markdown"}, {"metadata": {"collapsed": true, "_uuid": "c0696a1c4315a896235ed3182437cac67de97884", "_cell_guid": "f43d3f8e-8a0b-4d3b-b81f-3bec5de8ed2f"}, "outputs": [], "source": ["# Check for categorical variables\n", "train_data.head()"], "execution_count": null, "cell_type": "code"}, {"metadata": {"collapsed": true, "_uuid": "6393fa86d4667ed0cb6b6938b06ff0e66cc4553f", "_cell_guid": "bd694924-ce18-4b9a-a073-d9b2d7c2125d"}, "outputs": [], "source": ["# Encode categorical variables\n", "train_data['store_and_fwd_flag'] = train_data['store_and_fwd_flag'].map({'N':0,'Y':1})"], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "39a440b48171ff7916bfbcc25e4cf7309eae3a34", "_cell_guid": "a69c5865-8c69-4b46-ba91-8c2fb233e7ea"}, "source": ["**Drop Unnecessary Features**  \n", "The machine learning algorithm will not be able to accept dates and times as inputs so the following features will be removed prior to training: `pickup_date`, `pickup_time`, `dropoff_date`,  and `dropoff_time`.  Instead, this information will be represented by the `pickup_month` and `pickup_hour` features that were engineered during pre-processing."], "cell_type": "markdown"}, {"metadata": {"collapsed": true, "_uuid": "e03ffa4952f5aa4f2aabbf6cf400ac6fcaf7a237", "_cell_guid": "cf24f173-befe-4944-a520-3fa481d811a9"}, "outputs": [], "source": ["# Remove unnecessary features\n", "train_data.drop(['pickup_date','pickup_time','dropoff_date', 'dropoff_time','id'], \n", "                axis = 1, inplace = True)"], "execution_count": null, "cell_type": "code"}, {"metadata": {"collapsed": true, "_uuid": "f3fc0a3cb97e6c94ff3dc4253d26919dc5308d29", "_cell_guid": "dc25fbef-ad65-401a-b49f-771775dad861"}, "outputs": [], "source": ["train_data.columns"], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "1c7ed80da180aaea71737a1470521b5bf5dffb2e", "_cell_guid": "3f12ec74-e1e1-447b-9571-070dabe46984"}, "source": ["Now that data is model-ready, we are finally ready for the fun part - building models! First, I will split the data into training and test sets.  Next, I will feed these sets into a number of Regression algorithms to determine which learner is the most performant to use for the Kaggle submission.  "], "cell_type": "markdown"}, {"metadata": {"collapsed": true, "_uuid": "1a1b52645e71677d621e97efb17646d720d0fe62", "_cell_guid": "058c46c8-ceea-44ff-bf3c-20b67dc2571a"}, "outputs": [], "source": ["# Split\n", "# Create matrix of features\n", "X = train_data[['vendor_id', 'passenger_count', 'pickup_longitude',\n", "       'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude',\n", "       'store_and_fwd_flag','pickup_month', 'pickup_hour',\n", "       'pickup_weekday']] # double brackets!\n", "\n", "# Create array of target variable \n", "y = train_data['trip_duration']\n", "\n", "# Create train and test sets\n", "from sklearn.model_selection import train_test_split\n", "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "125102cca4541927f2d2bbc206c37aed6cb2eac3", "_cell_guid": "5311b97f-3ed9-4507-b32b-2aba58d81b1d"}, "source": ["## <a id=\"linear\"></a>  Multivariate Linear Regression\n", "First, I will try [Linear Regression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) as it is the most widely-known and understood regression learner.  However, from the outset, it is important to underscore the assumptions we're making when using this model:\n", "1. **Linear Relationship:** The relationship between the independent and dependent variables is linear.\n", "2. **Multivariate Normality:** The variables (features) are normally distributed.  If not, a non-linear transformation (e.g., log-transformation) may be needed to fix the issue.  \n", "3. **No or Little Multicollinearity:** The independent variables (features) are not highly correlated with each other.\n", "4. **No Auto-Corrlation:** Residuals are independent of one another (i.e., outcome is independent of a previous outcome).\n", "5.  **Homoscedasticity:** Residuals are equal across the regression line.\n", "\n", "These assumptions may or may not be appropriate for this particular data set.  We can test these assumptions by seeing how well the Linear Regression performs.  <br\\>  \n", "\n", "[More Information About Assumptions](http://www.statisticssolutions.com/assumptions-of-linear-regression/)\n", "\n", "During the training process, I will apply `GridSearchCV()` to use the best parameters possible to train the model. For additional information see links below:\n", "\n", "[Documentation](http://scikitlearn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)  \n", "[Scoring Parameters](http://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter)  \n", "[GridSearchCV Regression](https://stats.stackexchange.com/questions/153131/gridsearchcv-regression-vs-linear-regression-vs-stats-model-ols)\n"], "cell_type": "markdown"}, {"metadata": {"collapsed": true, "_uuid": "18669801212814499694385f59e831d3cfd79b0e", "_cell_guid": "67fae35f-594a-4cd7-b08b-6bd149f226a8"}, "outputs": [], "source": ["#  Import model\n", "from sklearn.linear_model import LinearRegression\n", "\n", "#  Instantiate model object\n", "lreg = LinearRegression()\n", "\n", "# Fit to training data\n", "lreg.fit(X_train,y_train)\n", "print(lreg)\n", "\n", "# Predict\n", "y_pred_lreg = lreg.predict(X_test)\n", "\n", "# Score It\n", "from sklearn import metrics\n", "print('\\nLinear Regression Performance Metrics')\n", "print('R^2=',metrics.explained_variance_score(y_test,y_pred_lreg))\n", "print('MAE:',metrics.mean_absolute_error(y_test,y_pred_lreg))\n", "print('MSE:',metrics.mean_squared_error(y_test,y_pred_lreg))\n", "print('RMSE:',np.sqrt(metrics.mean_squared_error(y_test,y_pred_lreg)))"], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "27ef7e0900675c7c082b6b4c22ad923b5cae7ea0", "_cell_guid": "5556b6ce-e4d2-43a9-9a8f-dfc0d4cf9f0f"}, "source": ["According to the R-squared, only 23%  of the variation in the dependent variable is explained by this model.   Such a low score is an indication that the relationship between the features and independent variable may be better explained with a non-linear model.  \n", "\n", "## <a id=\"tree\"></a>  **Decision Tree**\n", "There are a number of non-linear regressors to choose from, but I will implement a [Decision Tree Regressor](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html) first for three primary reasons:\n", "-  it is the easiest to interpret\n", "-  does not require feature scaling\n", "-  computationally less expensive than other methods\n"], "cell_type": "markdown"}, {"metadata": {"collapsed": true, "_uuid": "ef5485cbc27cba3a503a08dd69fb6f66c3b9b7e2", "_cell_guid": "85f257f3-ef58-410e-a522-3e2d91d2c317"}, "outputs": [], "source": ["# Fit\n", "# Import model\n", "from sklearn.tree import DecisionTreeRegressor\n", "\n", "# Instantiate model object\n", "dtree = DecisionTreeRegressor()\n", "\n", "# Fit to training data\n", "dtree.fit(X_train,y_train)\n", "print(dtree)\n", "\n", "# Predict\n", "y_pred_dtree = dtree.predict(X_test)\n", "\n", "# Score It\n", "from sklearn import metrics\n", "print('\\nDecision Tree Regression Performance Metrics')\n", "print('R^2=',metrics.explained_variance_score(y_test,y_pred_dtree))\n", "print('MAE:',metrics.mean_absolute_error(y_test,y_pred_dtree))\n", "print('MSE:',metrics.mean_squared_error(y_test,y_pred_dtree))\n", "print('RMSE:',np.sqrt(metrics.mean_squared_error(y_test,y_pred_dtree)))"], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "05d998bc366dba2e2d8698893d7bd01d45686c36", "_cell_guid": "a02c2103-9609-4521-88d2-4554a7bd064b"}, "source": ["## <a id=\"random\"></a> **Random Forest **\n", "[Random Forest](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)"], "cell_type": "markdown"}, {"metadata": {"collapsed": true, "_uuid": "4fd60c01f64c7d3b908bb25341135123af297b23", "_cell_guid": "a652864e-f6cb-49cf-88c6-65d429a02a1b"}, "outputs": [], "source": ["# Fit \n", "# Import model\n", "from sklearn.ensemble import RandomForestRegressor \n", "\n", "# Instantiate model object\n", "rforest = RandomForestRegressor(n_estimators = 20, n_jobs = -1)\n", "\n", "# Fit to training data\n", "rforest = rforest.fit(X_train,y_train)\n", "print(rforest)\n", "\n", "# Predict\n", "y_pred_rforest = rforest.predict(X_test)\n", "\n", "# Score It\n", "from sklearn import metrics\n", "print('\\nRandom Forest Regression Performance Metrics')\n", "print('R^2 =',metrics.explained_variance_score(y_test,y_pred_rforest))\n", "print('MAE',metrics.mean_absolute_error(y_test, y_pred_rforest))\n", "print('MSE',metrics.mean_squared_error(y_test, y_pred_rforest))\n", "print('RMSE',np.sqrt(metrics.mean_squared_error(y_test, y_pred_rforest)))"], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "efe79b603f2a1b43a21d0c86bbf2add5af20a894", "_cell_guid": "dc67ca3a-f27a-4f86-b669-90681dd19e89"}, "source": ["#  <a id=\"deployment\"></a> 4. MODEL DEPLOYMENT \n", "I've decided to use the Random Forest algorithm on my final Kaggle Submission.  But first, I'll need to pre-process the test data in the same manner as above before feeding it into the model.   "], "cell_type": "markdown"}, {"metadata": {"collapsed": true, "_uuid": "d4085c213591d671b2d95f316f4fb50f4ea9e118", "_cell_guid": "fcf5ee33-9619-4785-919d-6dff1d01b639"}, "outputs": [], "source": ["# Load test data\n", "test_data = pd.read_csv('../input/test.csv')\n", "\n", "# Test data info\n", "test_data.info()\n", "\n", "# Test data shape\n", "print('shape',test_data.shape)"], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "9fcb7db3aea77ce1772b564af071817d36ab9c8e", "_cell_guid": "65b51891-cbe7-47aa-bd4c-6e138890144d"}, "source": ["## **Feature Engineering**"], "cell_type": "markdown"}, {"metadata": {"collapsed": true, "_uuid": "b4485daf60d1cee192aa274d04321051169f8db6", "_cell_guid": "37493cb3-a905-486b-bc83-ec254a40011f"}, "outputs": [], "source": ["# Convert timestamps to date objects\n", "test_data['pickup_datetime'] = pd.to_datetime(test_data.pickup_datetime) # Pickups\n", "\n", "# Delimit pickup_datetime variable \n", "test_data['pickup_date'] = test_data['pickup_datetime'].dt.date # Extract date\n", "test_data['pickup_time'] = test_data['pickup_datetime'].dt.time # Extract time\n", "\n", "# Additional pickup features\n", "test_data['pickup_month'] = test_data['pickup_datetime'].dt.month # Extract month\n", "\n", "#train_data['pickup_YYYYMM'] = train_data['pickup_datetime'].apply(lambda x: x.strftime('%Y%m')) # Extract yearmonth\n", "test_data['pickup_hour'] = test_data['pickup_datetime'].dt.hour # Extract hour\n", "test_data['pickup_weekday'] = test_data['pickup_datetime'].dt.dayofweek # Extract day of week\n", "\n", "# Encode categorical variables\n", "test_data['store_and_fwd_flag'] = test_data['store_and_fwd_flag'].map({'N':0,'Y':1})\n"], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "00277633fef6d8788d20ac596c674ce09f357d2b", "_cell_guid": "0280074e-3da6-480e-91ac-31d85d1bf0d2"}, "source": ["## ** Kaggle Submission**"], "cell_type": "markdown"}, {"metadata": {"collapsed": true, "_uuid": "7c95697739dbb666f1cebc070f7bdab0472d4cd2", "_cell_guid": "93d0a116-bf63-45c0-b32c-c1394614d8d1"}, "outputs": [], "source": ["# Create new matrix of features from test data\n", "X_test= test_data[['vendor_id', 'passenger_count', 'pickup_longitude',\n", "       'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude',\n", "       'store_and_fwd_flag','pickup_month', 'pickup_hour',\n", "       'pickup_weekday']]\n", "\n", "# Feed features into random forest\n", "y_pred= rforest.predict(X_test)"], "execution_count": null, "cell_type": "code"}, {"metadata": {"collapsed": true, "_uuid": "6c3ca865153c8f58fd8634acbf3ed6aa84061edf", "_cell_guid": "4f5ea9bb-9e44-47be-8e2c-dd1df72cc8aa"}, "outputs": [], "source": ["# Create contest submission\n", "submission = pd.DataFrame({\n", "    'Id':test_data['id'],\n", "    'trip_duration': y_pred\n", "})\n", "submission.to_csv('mytaxisubmission.csv',index = False)"], "execution_count": null, "cell_type": "code"}]}