{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "4b43dabc-3dad-ac97-dc5a-1658eb40c169"
      },
      "source": [
        "# leaf classification problem\n",
        "\n",
        "### The training set contains 99 species and 10 samples for each"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "3e91873f-868b-b0ed-0c4c-c38b2d81d85a"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "f0887d92-985d-af80-90de-a673f0529c5b"
      },
      "outputs": [],
      "source": [
        "train_df = pd.read_csv('../input/train.csv')\n",
        "train_df.info()\n",
        "copy_df = train_df.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "d48cf74d-61a7-22df-39a4-bdbc966ae818"
      },
      "source": [
        "## Now we're done preperation. Time to do some data wrangling (mapping species into indices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "90804b14-4e95-fe96-2dbb-5367064981c5"
      },
      "outputs": [],
      "source": [
        "species = train_df['species'].unique()\n",
        "species.sort()\n",
        "spe_dict = dict(enumerate(species))\n",
        "inv_spe_dict = {v: k for k,v in spe_dict.items()}\n",
        "train_df['species_index'] = train_df['species'].map(inv_spe_dict).astype(int)\n",
        "train_df = train_df.drop(['species','id'],axis = 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "8eacdc32-a26c-be3f-9f34-604ce59c21f5"
      },
      "source": [
        "## Done data wrangling. Start training CLF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "bd369de0-2a4c-48a8-436b-2db40b0cdc91"
      },
      "outputs": [],
      "source": [
        "train_data = train_df.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ffd80508-e215-a60d-6329-596d4c1c9618"
      },
      "outputs": [],
      "source": [
        "#Essntially, 'lbfgs' methond alreday did a great job on data set smaller than 1,000(so, a bigger training set seems\n",
        "#like a overkill). Over 1,000, we'd like to use 'adam' but 'adam' works so badly.\n",
        "\n",
        "#You can do it over and over again in order to get a larger training set.\n",
        "a = np.random.permutation(train_data)\n",
        "b = np.random.permutation(train_data)\n",
        "train_data = np.vstack((a,b))\n",
        "train_data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "e30fca64-c9e8-0e8e-fc06-4acf47fa7bdc"
      },
      "outputs": [],
      "source": [
        "#since we have dropped the 'specie'&'id', the features starts at col 0. The last col is the labels.\n",
        "X,y = train_data[:,:-1],train_data[:,-1]\n",
        "scaler = StandardScaler().fit(X)\n",
        "X = scaler.transform(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "7d82fcbf-c1f4-0b53-029f-fd42a7cd87d2"
      },
      "outputs": [],
      "source": [
        "model = MLPClassifier(hidden_layer_sizes=(150,),activation='logistic',solver='lbfgs',alpha=0.001\n",
        "                      ,max_iter=200,early_stopping=True,validation_fraction=0.2,\n",
        "                      learning_rate='adaptive',tol=1e-8,random_state=1).fit(X,y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "0201af8f-f1cf-ab7d-35c3-15e72f101846"
      },
      "outputs": [],
      "source": [
        "def cv(a,b,model):\n",
        "    cv_X = train_data[a:b,:-1]\n",
        "    cv_X = scaler.transform(cv_X)\n",
        "    cv_y = train_data[a:b,-1]\n",
        "    cv_p1_y = model.predict(cv_X)\n",
        "    cv_p2_y = model.predict_log_proba(cv_X)\n",
        "    print(accuracy_score(cv_y,cv_p1_y))\n",
        "    print(cv_y)\n",
        "    print(cv_p1_y)\n",
        "#print(cv_p2_y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "4fca503f-c022-dce7-6bd9-a97996f4bc46"
      },
      "outputs": [],
      "source": [
        "cv(200,220,model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "6244408a-79a0-365b-c72f-6dad039ccd34"
      },
      "outputs": [],
      "source": [
        "#Testing overfitting/underfitting of our origional training set\n",
        "d = np.random.permutation(train_df.values)\n",
        "Xp = d[:,:-1]\n",
        "Xp = scaler.transform(Xp)\n",
        "yp = d[:,-1]\n",
        "ypp = model.predict(Xp)\n",
        "#cv_p2_y = model.predict_log_proba(cv_X)\n",
        "print(accuracy_score(yp,ypp))\n",
        "#print(cv_y)\n",
        "#print(cv_p1_y)\n",
        "#print(cv_p2_y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "a419e53a-97f8-58c4-c0ec-e8087e64ba6e"
      },
      "source": [
        "## Let's go predict, shall we?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "084303b2-7787-6510-5eb2-82b9a0dfb016"
      },
      "outputs": [],
      "source": [
        "test_df = pd.read_csv('../input/test.csv')\n",
        "index = test_df.pop('id')\n",
        "test_data = test_df.values\n",
        "\n",
        "test_X = test_data\n",
        "test_X = scaler.transform(test_X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "de8f6d91-5f54-7c27-6a42-4df575ef0375"
      },
      "outputs": [],
      "source": [
        "predict = model.predict(test_X)\n",
        "predict_proba = model.predict_proba(test_X)\n",
        "predict_proba"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "13a3f317-0801-f23d-8b77-249c9b726856"
      },
      "outputs": [],
      "source": [
        "#print(species.tolist())\n",
        "species_list = species.tolist()\n",
        "result = pd.DataFrame(predict_proba,index = index, columns = species_list)\n",
        "# It seems like this competition scores higher for these probability results(well, unless you can figure\n",
        "# out the true answers completely \u00af\\_(\u30c4) _/\u00af)\n",
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "876ea75b-c99d-7a8f-46d3-2198ea63fcf7"
      },
      "outputs": [],
      "source": [
        "fp = open('submission.csv','w')\n",
        "fp.write(result.to_csv())"
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}