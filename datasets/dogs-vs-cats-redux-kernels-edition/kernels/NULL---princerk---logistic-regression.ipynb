{"metadata": {"kernelspec": {"language": "python", "name": "python3", "display_name": "Python 3"}, "language_info": {"mimetype": "text/x-python", "pygments_lexer": "ipython3", "version": "3.6.1", "file_extension": ".py", "name": "python", "nbconvert_exporter": "python", "codemirror_mode": {"name": "ipython", "version": 3}}}, "nbformat": 4, "nbformat_minor": 1, "cells": [{"outputs": [], "cell_type": "code", "metadata": {"collapsed": true, "_cell_guid": "f8babf22-df7d-4138-b572-2489fc680821", "_uuid": "c655d92a826af5c2dcf71f7d6c96ddf97a06e9ec"}, "execution_count": null, "source": ["# This Python 3 environment comes with many helpful analytics libraries installed\n", "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n", "# For example, here's several helpful packages to load in \n", "\n", "import numpy as np # linear algebra\n", "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n", "from scipy import ndimage, misc\n", "# Input data files are available in the \"../input/\" directory.\n", "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n", "import os\n", "import glob\n", "import matplotlib.pyplot as plt\n", "from random import shuffle\n", "\n", "from PIL import Image\n", "%matplotlib inline\n", "# Any results you write to the current directory are saved as output.\n"]}, {"outputs": [], "cell_type": "code", "metadata": {"collapsed": true, "_cell_guid": "500e12b4-7cee-48f6-a884-e59defa94d9a", "_uuid": "a5fedad247bdf102ca63913802052434da66064a"}, "execution_count": null, "source": ["trainlist = glob.glob('../input/train/*')\n", "\n", "train_catlist = [imagename for imagename in trainlist if 'cat' in imagename]\n", "train_doglist = [imagename for imagename in trainlist if 'dog' in imagename]\n", "\n", "# take 200 samples of each cat and dogs\n", "m_trainlist = train_catlist[:1000]\n", "m_trainlist.extend(train_doglist[:1000])\n", "shuffle(m_trainlist)\n", "\n", "m_testlist = train_catlist[-200:]\n", "m_testlist.extend(train_doglist[-200:])\n", "shuffle(m_testlist)\n", "\n", "\n", "# shuffle each train and test set\n"]}, {"outputs": [], "cell_type": "code", "metadata": {"_cell_guid": "a486ff71-4eb6-4672-bacc-81faa484b538", "_uuid": "7e530afe59e0a29667b9130d64af73e8ca15a268"}, "execution_count": null, "source": ["\n", "#trainlist = trainlist[:10]\n", "#testlist = testlist[:10]\n", "x_train_orig = np.array([np.array(misc.imresize(ndimage.imread(imagename, mode='RGB'), (256,256,3))) for imagename in m_trainlist])\n", "x_test_orig = np.array([np.array(misc.imresize(ndimage.imread(imagename, mode='RGB'), (256,256,3))) for imagename in m_testlist])\n", "y_train_orig = np.array([0 if 'cat' in imagename else 1 for imagename in m_trainlist]).reshape((-1, 1)).T\n", "y_test_orig = np.array([0 if 'cat' in imagename else 1 for imagename in m_testlist]).reshape((-1, 1)).T\n", "print(x_train_orig.shape)\n", "print(x_test_orig.shape)\n", "plt.imshow(x_train_orig[1])"]}, {"outputs": [], "cell_type": "code", "metadata": {"scrolled": false, "_cell_guid": "70e57c5c-67c8-4c77-bb7d-a6626f760625", "_uuid": "945c8244668a265c3df6215be9b905f844c1d851"}, "execution_count": null, "source": ["x_train_flatten = x_train_orig.reshape(x_train_orig.shape[0], -1).T\n", "x_test_flatten = x_test_orig.reshape(x_test_orig.shape[0], -1).T\n", "print (\"x_train_flatten shape: \" + str(x_train_flatten.shape))\n", "print (\"y_train_orig shape: \" + str(y_train_orig.shape))\n", "print (\"x_test_flatten shape: \" + str(x_test_flatten.shape))\n", "print (\"y_test_orig shape: \" + str(y_test_orig.shape))\n", "print (\"sanity check after reshaping: \" + str(x_train_flatten[0:5,0]))\n"]}, {"outputs": [], "cell_type": "code", "metadata": {"collapsed": true, "_cell_guid": "03dd614c-f1bd-4146-b351-e219cbc14fc5", "_uuid": "e1a7a8b34c399ef446403e65a4203b1207355515"}, "execution_count": null, "source": ["x_train = x_train_flatten / 255\n", "x_test = x_test_flatten / 255\n", "y_train = y_train_orig\n", "y_test = y_test_orig"]}, {"outputs": [], "cell_type": "code", "metadata": {"collapsed": true, "_cell_guid": "c3782d47-f2e0-444a-b96f-3b01d12a10d6", "_uuid": "6ad3c24f66eddad205e233637eb3c987058edaae"}, "execution_count": null, "source": ["def sigmoid(z):\n", "    \"\"\"\n", "    Arguments:\n", "    z -- A scalar or numpy array of any size.\n", "\n", "    Return:\n", "    s -- sigmoid(z)\n", "    \"\"\"\n", "    s = 1/(1 + np.exp(-z))\n", "    return s"]}, {"outputs": [], "cell_type": "code", "metadata": {"collapsed": true, "_cell_guid": "f74a7642-cad8-49bd-befb-d3043753ef52", "_uuid": "b15a441a0a5512dbfecb839b5d0b3a4b970abeb9"}, "execution_count": null, "source": ["def initialize_with_zeros(dim):\n", "    \"\"\"\n", "    This function creates a vector of zeros of shape (dim, 1) for w and initializes b to 0.\n", "    \n", "    Argument:\n", "    dim -- size of the w vector we want (or number of parameters in this case)\n", "    \n", "    Returns:\n", "    w -- initialized vector of shape (dim, 1)\n", "    b -- initialized scalar (corresponds to the bias)\n", "    \"\"\"\n", "    \n", "    w = np.zeros((dim, 1))\n", "    b = 0\n", "    \n", "    assert(w.shape == (dim, 1))\n", "    assert(isinstance(b, float) or isinstance(b, int))\n", "    \n", "    return w, b"]}, {"outputs": [], "cell_type": "code", "metadata": {"collapsed": true, "_cell_guid": "d70b6b03-a93d-486e-9cb6-46925a5dc550", "_uuid": "24f1017c15f559029b24df612c18fe59743da191"}, "execution_count": null, "source": ["def propagate(w, b, X, Y):\n", "    \"\"\"\n", "    Arguments:\n", "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n", "    b -- bias, a scalar\n", "    X -- data of size (num_px * num_px * 3, number of examples)\n", "    Y -- true \"label\" vector (containing 0 if cat, 1 if dog) of size (1, number of examples)\n", "\n", "    Return:\n", "    cost -- negative log-likelihood cost for logistic regression\n", "    dw -- gradient of the loss with respect to w, thus same shape as w\n", "    db -- gradient of the loss with respect to b, thus same shape as b\n", "    \"\"\"\n", "    \n", "    m = X.shape[1]\n", "    \n", "    # FORWARD PROPAGATION (FROM X TO COST)\n", "   \n", "    A = sigmoid(np.dot(w.T, X) + b)                                     # compute activation\n", "    cost = -1/m * np.sum(Y * np.log(A) + (1 - Y) * np.log(1 - A))\n", "   \n", "    \n", "    # BACKWARD PROPAGATION (TO FIND GRAD)\n", "   \n", "    dw = 1/m * np.dot(X, (A -Y).T)\n", "    db = 1/m * np.sum(A-Y)\n", "\n", "\n", "    assert(dw.shape == w.shape)\n", "    assert(db.dtype == float)\n", "    cost = np.squeeze(cost)\n", "    assert(cost.shape == ())\n", "    \n", "    grads = {\"dw\": dw,\n", "             \"db\": db}\n", "    \n", "    return grads, cost"]}, {"outputs": [], "cell_type": "code", "metadata": {"collapsed": true, "_cell_guid": "5fecd540-576e-477e-acaf-405be60b1c5d", "_uuid": "053c16894e5381d21b426e73b027a7e954c6d0e3"}, "execution_count": null, "source": ["def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n", "    \"\"\"\n", "    This function optimizes w and b by running a gradient descent algorithm\n", "    \n", "    Arguments:\n", "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n", "    b -- bias, a scalar\n", "    X -- data of shape (num_px * num_px * 3, number of examples)\n", "    Y -- true \"label\" vector (containing 0 if cat, 1 if dog), of shape (1, number of examples)\n", "    num_iterations -- number of iterations of the optimization loop\n", "    learning_rate -- learning rate of the gradient descent update rule\n", "    print_cost -- True to print the loss every 100 steps\n", "    \n", "    Returns:\n", "    params -- dictionary containing the weights w and bias b\n", "    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n", "    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n", "    \n", "    Tips:\n", "    You basically need to write down two steps and iterate through them:\n", "        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n", "        2) Update the parameters using gradient descent rule for w and b.\n", "    \"\"\"\n", "    \n", "    costs = []\n", "    \n", "    for i in range(num_iterations):\n", "        \n", "        \n", "        # Cost and gradient calculation (\u2248 1-4 lines of code)\n", "        grads, cost = propagate(w, b, X, Y)\n", "        \n", "        # Retrieve derivatives from grads\n", "        dw = grads[\"dw\"]\n", "        db = grads[\"db\"]\n", "        \n", "        w = w - learning_rate * dw\n", "        b = b - learning_rate * db\n", "        \n", "        # Record the costs\n", "        if i % 100 == 0:\n", "            costs.append(cost)\n", "        \n", "        # Print the cost every 100 training examples\n", "        if print_cost and i % 100 == 0:\n", "            print (\"Cost after iteration %i: %f\" %(i, cost))\n", "    \n", "    params = {\"w\": w,\n", "              \"b\": b}\n", "    \n", "    grads = {\"dw\": dw,\n", "             \"db\": db}\n", "    \n", "    return params, grads, costs"]}, {"outputs": [], "cell_type": "code", "metadata": {"collapsed": true, "_cell_guid": "f2005cf0-5b75-4a9c-ad2a-db5acaa14c77", "_uuid": "ebf6ed49736d8c0b888dfd011e5657bae6a3b86d"}, "execution_count": null, "source": ["def predict(w, b, X):\n", "    '''\n", "    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)\n", "    \n", "    Arguments:\n", "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n", "    b -- bias, a scalar\n", "    X -- data of size (num_px * num_px * 3, number of examples)\n", "    \n", "    Returns:\n", "    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X\n", "    '''\n", "    \n", "    m = X.shape[1]\n", "    Y_prediction = np.zeros((1,m))\n", "    w = w.reshape(X.shape[0], 1)\n", "    \n", "    # Compute vector \"A\" predicting the probabilities of a cat being present in the picture\n", "    ### START CODE HERE ### (\u2248 1 line of code)\n", "    A = sigmoid(np.dot(w.T, X) + b)\n", "    ### END CODE HERE ###\n", "    for i in range(A.shape[1]):\n", "        \n", "        # Convert probabilities A[0,i] to actual predictions p[0,i]\n", "        ### START CODE HERE ### (\u2248 4 lines of code)\n", "        if A[0][i] > 0.5:\n", "            Y_prediction[0][i] = 1\n", "        ### END CODE HERE ###\n", "    \n", "    assert(Y_prediction.shape == (1, m))\n", "    \n", "    return Y_prediction"]}, {"outputs": [], "cell_type": "code", "metadata": {"collapsed": true, "_cell_guid": "0fb592ce-68ef-49b9-8da9-2e6e03a9cecf", "_uuid": "641d1413814f4554f2502a3d2e626b0cb07f9e33"}, "execution_count": null, "source": ["def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False):\n", "    \"\"\"\n", "    Builds the logistic regression model by calling the function you've implemented previously\n", "    \n", "    Arguments:\n", "    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n", "    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n", "    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n", "    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n", "    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n", "    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n", "    print_cost -- Set to true to print the cost every 100 iterations\n", "    \n", "    Returns:\n", "    d -- dictionary containing information about the model.\n", "    \"\"\"\n", "    \n", "    ### START CODE HERE ###\n", "    \n", "    # initialize parameters with zeros (\u2248 1 line of code)\n", "    w, b = initialize_with_zeros(X_train.shape[0])\n", "\n", "    # Gradient descent (\u2248 1 line of code)\n", "    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)\n", "    \n", "    # Retrieve parameters w and b from dictionary \"parameters\"\n", "    w = parameters[\"w\"]\n", "    b = parameters[\"b\"]\n", "    \n", "    # Predict test/train set examples (\u2248 2 lines of code)\n", "    Y_prediction_test = predict(w, b, X_test)\n", "    Y_prediction_train = predict(w, b, X_train)\n", "\n", "    ### END CODE HERE ###\n", "\n", "    # Print train/test Errors\n", "    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n", "    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n", "\n", "    \n", "    d = {\"costs\": costs,\n", "         \"Y_prediction_test\": Y_prediction_test, \n", "         \"Y_prediction_train\" : Y_prediction_train, \n", "         \"w\" : w, \n", "         \"b\" : b,\n", "         \"learning_rate\" : learning_rate,\n", "         \"num_iterations\": num_iterations}\n", "    \n", "    return d"]}, {"outputs": [], "cell_type": "code", "metadata": {"_cell_guid": "bf6818ee-def8-43c7-995e-4cde7e10026c", "_uuid": "fafb64af9bbf7ae1d61b8b6a5c38b8042f621c8e"}, "execution_count": null, "source": ["d = model(x_train, y_train, x_test, y_test, num_iterations = 2000, learning_rate = 0.001, print_cost = True)"]}]}