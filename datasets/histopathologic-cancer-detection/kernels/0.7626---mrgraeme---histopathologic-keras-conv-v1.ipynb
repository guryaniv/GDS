{"cells":[{"metadata":{"scrolled":true,"trusted":true,"_uuid":"c6e6d840e0f78d060b39cebaf4f8cfeea1af4471"},"cell_type":"code","source":"from keras.preprocessing.image import ImageDataGenerator\nfrom keras.models import Sequential\nfrom keras.models import model_from_json\nfrom keras.layers import Conv2D, MaxPooling2D, MaxPool2D\nfrom keras.layers import Activation, Dropout, Flatten, Dense, BatchNormalization\nfrom keras.optimizers import RMSprop, Adam\nfrom keras import backend as K\nimport pandas as pd\nimport os\nimport numpy as np","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6a485aed512946a8e5b28fae2cc746043f191ff6"},"cell_type":"code","source":"# dimensions of our images.\nimg_width, img_height = 96, 96\nvalidation_split = 0.1\n\ntrain_data_dir = '../input/train'\ntest_data_dir = '../input/test'\n\ntrain_df = pd.read_csv('../input/train_labels.csv')\ntrain_df['filename'] = train_df['id'] + \".tif\"\ntrain_df['class'] = train_df['label']\n\ntest_df = pd.DataFrame({'filename':os.listdir(test_data_dir)})\n\nnb_train_samples = train_df.shape[0] - train_df.shape[0]*validation_split\nnb_validation_samples = nb_train_samples*validation_split\nnb_test_samples = test_df.shape[0]\n\nepochs = 2\nbatch_size = 35\n\nif K.image_data_format() == 'channels_first':\n    input_shape = (3, img_width, img_height)\nelse:\n    input_shape = (img_width, img_height, 3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4b689fe6c6b98596a0fb5fde9c9351959cfa0e0a"},"cell_type":"code","source":"# this is the augmentation configuration we will use for training\ntrain_datagen = ImageDataGenerator(\n    #preprocessing_function=lambda x:(x - x.mean()) / x.std() if x.std() > 0 else x,\n    rescale=1. / 255,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    featurewise_center=True, \n    featurewise_std_normalization=True,\n    zca_whitening=True,\n    vertical_flip=True,\n    validation_split=validation_split)\n\n# this is the augmentation configuration we will use for testing:\n# only rescaling\ntest_datagen = ImageDataGenerator(rescale=1. / 255)\n\ntrain_generator = train_datagen.flow_from_dataframe(\n    dataframe = train_df,\n    directory = train_data_dir,\n    target_size = (img_width, img_height),\n    shuffle=True,\n    batch_size=batch_size,\n    subset=\"training\",\n    class_mode = 'binary')\n\nvalid_generator=train_datagen.flow_from_dataframe(\n    dataframe=train_df,\n    directory=train_data_dir,\n    target_size = (img_width, img_height),\n    shuffle=True,\n    batch_size=batch_size,\n    subset=\"validation\",\n    class_mode = 'binary')\n\ntest_generator = test_datagen.flow_from_dataframe(\n    dataframe = test_df,\n    directory = test_data_dir,\n    target_size = (img_width, img_height),\n    shuffle=False,\n    batch_size=batch_size,\n    class_mode = None)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6374887f953b2840bd0d806f9d7edb8e2c616810"},"cell_type":"code","source":"kernel_size = (3,3)\npool_size= (2,2)\nfirst_filters = 32\nsecond_filters = 64\nthird_filters = 128\n\ndropout_conv = 0.3\ndropout_dense = 0.5\n\nmodel = Sequential()\nmodel.add(Conv2D(first_filters, kernel_size, activation = 'relu', input_shape = (img_width, img_height, 3)))\nmodel.add(Conv2D(first_filters, kernel_size, use_bias=False))\nmodel.add(BatchNormalization())\nmodel.add(Activation(\"relu\"))\nmodel.add(MaxPool2D(pool_size = pool_size)) \nmodel.add(Dropout(dropout_conv))\n\nmodel.add(Conv2D(second_filters, kernel_size, use_bias=False))\nmodel.add(BatchNormalization())\nmodel.add(Activation(\"relu\"))\nmodel.add(Conv2D(second_filters, kernel_size, use_bias=False))\nmodel.add(BatchNormalization())\nmodel.add(Activation(\"relu\"))\nmodel.add(MaxPool2D(pool_size = pool_size))\nmodel.add(Dropout(dropout_conv))\n\nmodel.add(Conv2D(third_filters, kernel_size, use_bias=False))\nmodel.add(BatchNormalization())\nmodel.add(Activation(\"relu\"))\nmodel.add(Conv2D(third_filters, kernel_size, use_bias=False))\nmodel.add(BatchNormalization())\nmodel.add(Activation(\"relu\"))\nmodel.add(MaxPool2D(pool_size = pool_size))\nmodel.add(Dropout(dropout_conv))\n\n#model.add(GlobalAveragePooling2D())\nmodel.add(Flatten())\nmodel.add(Dense(256, use_bias=False))\nmodel.add(BatchNormalization())\nmodel.add(Activation(\"relu\"))\nmodel.add(Dropout(dropout_dense))\nmodel.add(Dense(1, activation = \"sigmoid\"))\n\n# Compile the model\nmodel.compile(Adam(0.01), loss = \"binary_crossentropy\", metrics=[\"accuracy\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"009b2a3edbad843c1208452d05a6c904e3ba00f9"},"cell_type":"code","source":"from keras.callbacks import EarlyStopping, ReduceLROnPlateau\nearlystopper = EarlyStopping(monitor='val_loss', patience=2, verbose=1, restore_best_weights=True)\nreducel = ReduceLROnPlateau(monitor='val_loss', patience=1, verbose=1, factor=0.1)\n\nmodel.fit_generator(\n    train_generator,\n    steps_per_epoch=nb_train_samples // batch_size,\n    epochs=epochs,\n    validation_data=valid_generator,\n    validation_steps=valid_generator.n//valid_generator.batch_size,\n    callbacks=[reducel, earlystopper])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"85e39fa8779108514fa0c9c4d7df05c0cec8f622"},"cell_type":"code","source":"model.evaluate_generator(generator=valid_generator,  steps= nb_validation_samples / batch_size, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7cda6c6b0f4a274481b6aa44e9ae654f95f89805"},"cell_type":"code","source":"# serialize model to JSON\nmodel_json = model.to_json()\nwith open(\"model.json\", \"w\") as json_file:\n    json_file.write(model_json)\n\n# serialize weights to HDF5\nmodel.save_weights(\"model.h5\")\nprint(\"Saved model to disk\")\n\n## load json and create model\n#json_file = open('model.json', 'r')\n#loaded_model_json = json_file.read()\n#json_file.close()\n#loaded_model = model_from_json(loaded_model_json)\n## load weights into new model\n#loaded_model.load_weights(\"model.h5\")\n#print(\"Loaded model from disk\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4773ac6f804cf9524893dbaf3f90d88385dfc11d"},"cell_type":"code","source":"test_generator.reset()\n\npred=model.predict_generator(test_generator, steps= nb_test_samples / batch_size, verbose=1)\nprint('=== raw preds ===')\nprint(pred[0:10])\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"26a538fca1baa3724ebdad1bda0c50a0aae0ee33"},"cell_type":"code","source":"\npred_list = [int(round(pred[i][0])) for i in range(0, pred.shape[0])]\npred_list\n\nfilenames=test_generator.filenames\nfilenames = [f.split(sep='.')[0] for f in filenames]\nresults=pd.DataFrame({\"id\":filenames,\n                      \"label\":pred_list})\nresults.to_csv(\"results.csv\",index=False)\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}