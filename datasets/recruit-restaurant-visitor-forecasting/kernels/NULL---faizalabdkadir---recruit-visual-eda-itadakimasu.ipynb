{"nbformat": 4, "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3", "language": "python"}, "language_info": {"file_extension": ".py", "name": "python", "version": "3.6.3", "codemirror_mode": {"name": "ipython", "version": 3}, "nbconvert_exporter": "python", "mimetype": "text/x-python", "pygments_lexer": "ipython3"}}, "cells": [{"metadata": {"_kg_hide-input": true, "_cell_guid": "503267bd-e00a-4ba2-b41f-68cf7192a3bc", "_uuid": "43d9edd3d8a8b7768e7250ab1795f517d8ebfe0f"}, "outputs": [], "cell_type": "code", "source": ["import numpy as np\n", "import pandas as pd\n", "from sklearn import ensemble, neighbors, linear_model, metrics, preprocessing\n", "from datetime import datetime\n", "import glob, re\n", "import time, datetime\n", "from datetime import timedelta\n", "\n", "import matplotlib.pyplot as plt\n", "plt.style.use('fivethirtyeight')\n", "import seaborn as sns\n", "color = sns.color_palette()\n", "\n", "%matplotlib inline\n", "\n", "import warnings\n", "warnings.filterwarnings(\"ignore\")\n", "\n", "# from JdPaletto & the1owl1\n", "# JdPaletto - https://www.kaggle.com/jdpaletto/surprised-yet-part2-lb-0-503?scriptVersionId=1867420\n", "# the1owl1 - https://www.kaggle.com/the1owl/surprise-me\n", "start1 =time.time()\n", "data = {\n", "    'tra': pd.read_csv('../input/air_visit_data.csv'),\n", "    'as': pd.read_csv('../input/air_store_info.csv'),\n", "    'hs': pd.read_csv('../input/hpg_store_info.csv'),\n", "    'ar': pd.read_csv('../input/air_reserve.csv'),\n", "    'hr': pd.read_csv('../input/hpg_reserve.csv'),\n", "    'id': pd.read_csv('../input/store_id_relation.csv'),\n", "    'tes': pd.read_csv('../input/sample_submission.csv'),\n", "    'hol': pd.read_csv('../input/date_info.csv').rename(columns={'calendar_date':'visit_date'})\n", "    }\n", "\n", "data['hr'] = pd.merge(data['hr'], data['id'], how='inner', on=['hpg_store_id'])# bring air id to hpd reserve data\n", "\n", "print('Training data....',data['tra'].shape)\n", "print('Unique store id in training data',len(data['tra']['air_store_id'].unique()))\n", "print('Id data....',data['id'].shape)\n", "print('Air store data....',data['as'].shape,'& unique-',data['as']['air_store_id'].unique().shape)\n", "print('Hpg store data....',data['hs'].shape,'& unique-',data['hs']['hpg_store_id'].unique().shape)\n", "print('Air reserve data....',data['ar'].shape,'& unique-',data['ar']['air_store_id'].unique().shape)\n", "print('Hpg reserve data....',data['hr'].shape,'& unique-',data['hr']['air_store_id'].unique().shape)\n", "      \n", "#converting datetime to date for reservation data\n", "for df in ['ar','hr']:\n", "    data[df]['visit_datetime'] = pd.to_datetime(data[df]['visit_datetime'])\n", "    data[df]['visit_datetime'] = data[df]['visit_datetime'].dt.date\n", "    data[df]['reserve_datetime'] = pd.to_datetime(data[df]['reserve_datetime'])\n", "    data[df]['reserve_datetime'] = data[df]['reserve_datetime'].dt.date\n", "    \n", "    #calculate reserve time difference and summarizing ar,hr to date\n", "    data[df]['reserve_datetime_diff'] = data[df].apply(\n", "        lambda r: (r['visit_datetime'] - r['reserve_datetime']).days, axis=1)\n", "    data[df] = data[df].groupby(['air_store_id','visit_datetime'], as_index=False)[[\n", "        'reserve_datetime_diff', 'reserve_visitors']].sum().rename(columns={'visit_datetime':'visit_date'})\n", "\n", "#breaking down dates on training data & summarizing \n", "data['tra']['visit_date'] = pd.to_datetime(data['tra']['visit_date'])\n", "data['tra']['dow'] = data['tra']['visit_date'].dt.weekday_name\n", "data['tra']['year'] = data['tra']['visit_date'].dt.year\n", "data['tra']['month'] = data['tra']['visit_date'].dt.month\n", "data['tra']['visit_date'] = data['tra']['visit_date'].dt.date\n", "\n", "#extracting store id and date info from test data\n", "data['tes']['air_store_id'] = data['tes']['id'].map(lambda x: '_'.join(x.split('_')[:2]))\n", "data['tes']['visit_date'] = data['tes']['id'].map(lambda x: str(x).split('_')[2])\n", "data['tes']['visit_date'] = pd.to_datetime(data['tes']['visit_date'])\n", "data['tes']['dow'] = data['tes']['visit_date'].dt.weekday_name\n", "data['tes']['year'] = data['tes']['visit_date'].dt.year\n", "data['tes']['month'] = data['tes']['visit_date'].dt.month\n", "data['tes']['visit_date'] = data['tes']['visit_date'].dt.date\n", "\n", "#extract unique stores based on test data and populate dow 1 to 6\n", "unique_stores = data['tes']['air_store_id'].unique()#extract unique stores id from test data\n", "#populating unique stores to dow\n", "store_7days = pd.concat([pd.DataFrame({'air_store_id': unique_stores, 'dow': [i]*len(unique_stores)}) \n", "                    for i in range(7)], axis=0, ignore_index=True).reset_index(drop=True)\n", "store_sum = pd.DataFrame({'air_store_id': unique_stores})\n", "\n", "# mapping train data dow to stores(test data) - min, mean, median, max, count \n", "tmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)[\n", "    'visitors'].sum().rename(columns={'visitors':'total_visitors'})\n", "store_7days = pd.merge(store_7days, tmp, how='left', on=['air_store_id','dow']) \n", "tmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)[\n", "    'visitors'].mean().rename(columns={'visitors':'mean_visitors'})\n", "store_7days = pd.merge(store_7days, tmp, how='left', on=['air_store_id','dow'])\n", "tmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)[\n", "    'visitors'].median().rename(columns={'visitors':'median_visitors'})\n", "store_7days = pd.merge(store_7days, tmp, how='left', on=['air_store_id','dow'])\n", "tmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)[\n", "    'visitors'].max().rename(columns={'visitors':'max_visitors'})\n", "store_7days = pd.merge(store_7days, tmp, how='left', on=['air_store_id','dow'])\n", "tmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)[\n", "    'visitors'].count().rename(columns={'visitors':'count_observations'})\n", "store_7days = pd.merge(store_7days, tmp, how='left', on=['air_store_id','dow']) \n", "# map stores(test) to store genre and location detail\n", "store_7days = pd.merge(store_7days, data['as'], how='left', on=['air_store_id']) \n", "# Encoding categories Air _genre and air area\n", "#lbl = preprocessing.LabelEncoder()\n", "#stores['air_genre_name'] = lbl.fit_transform(stores['air_genre_name'])\n", "#stores['air_area_name'] = lbl.fit_transform(stores['air_area_name'])# to drop categorical for algo use later\n", "\n", "data['hol']['visit_date'] = pd.to_datetime(data['hol']['visit_date'])\n", "#data['hol']['day_of_week'] = lbl.fit_transform(data['hol']['day_of_week'])\n", "data['hol']['visit_date'] = data['hol']['visit_date'].dt.date\n", "\n", "train = pd.merge(data['tra'], data['hol'], how='left', on=['visit_date']) \n", "test = pd.merge(data['tes'], data['hol'], how='left', on=['visit_date']) \n", "train = pd.merge(train, stores, how='left', on=['air_store_id','dow']) \n", "test = pd.merge(test, stores, how='left', on=['air_store_id','dow'])\n", "\n", "for df in ['ar','hr']:\n", "    train = pd.merge(train, data[df], how='left', on=['air_store_id','visit_date']) \n", "    test = pd.merge(test, data[df], how='left', on=['air_store_id','visit_date'])\n", "\n", "col = [c for c in train if c not in ['id', 'air_store_id','visit_date','visitors']]\n", "train = train.fillna(0) #change to one for algo training\n", "test = test.fillna(0)\n", "#df=df.rename(columns = {'two':'new_name'})\n", "train=train.rename(columns = {'reserve_datetime_diff_x':'reserve_datetime_diff_air','reserve_visitors_x':'reserve_visitors_air',\n", "                             'reserve_datetime_diff_y':'reserve_datetime_diff_hpg','reserve_visitors_y':'reserve_visitors_hpg'})\n", "train['v_no_reservation']=train['visitors']-train['reserve_visitors_air']-train['reserve_visitors_hpg']\n", "print(round(time.time()-start1,4))"], "execution_count": null}, {"metadata": {}, "outputs": [], "cell_type": "code", "source": ["store_sum"], "execution_count": null}, {"metadata": {"_cell_guid": "a83ef944-2b40-4ba8-9392-f679c8f0d85b", "_uuid": "56edd4becedfe57f9039be1c0b63c57c656bf972"}, "outputs": [], "cell_type": "code", "source": ["data['as'].head()"], "execution_count": null}, {"metadata": {"_cell_guid": "b245bd03-8b5c-480b-831b-fe06bcd8ba48", "_uuid": "4ad35af53937878df99446cacadf83791251c470"}, "outputs": [], "cell_type": "code", "source": ["data['as']['air_genre_name'].value_counts()"], "execution_count": null}, {"metadata": {"_cell_guid": "e745c414-c1fc-4cc0-a2ae-67a9ff5e3364", "_uuid": "3ded2d3c8f24ed28479e139b77ee47cb45e25736"}, "outputs": [], "cell_type": "code", "source": ["data['hs'].head()"], "execution_count": null}, {"metadata": {"_cell_guid": "fd49d304-e441-4fdf-a7a9-06b4024c6321", "_uuid": "56778071a58740c98228691dbe4c7a1eb90b7ff6"}, "outputs": [], "cell_type": "code", "source": ["data['hs']['hpg_genre_name'].value_counts()"], "execution_count": null}, {"metadata": {"_cell_guid": "b9a9b452-a4b3-4331-9b4a-46df6d730767", "_uuid": "ed1166b2033f1251e275ded4eee7f93bd99e8aaa"}, "outputs": [], "cell_type": "code", "source": ["data['hr'].head()"], "execution_count": null}, {"metadata": {}, "outputs": [], "cell_type": "code", "source": [], "execution_count": null}, {"metadata": {"_cell_guid": "7dacdf78-0e77-42bb-9968-419e6ad0f5d0", "_uuid": "aef8cc92adb3360b2035aca62e569048b98e831f"}, "outputs": [], "cell_type": "code", "source": ["test1=pd.merge(data['tra'],data['id'], how='left', on='air_store_id')\n", "test1['hpg_store_id'].notnull().sum()\n", "#test1.head()"], "execution_count": null}, {"metadata": {"_cell_guid": "7ec22a27-3da2-4009-9581-9a66ce5a69cf", "_uuid": "3bb5d734cedbaf7e9face6ed7e2c32952a0c9159"}, "outputs": [], "cell_type": "code", "source": ["import folium\n", "from folium.plugins import MarkerCluster\n", "\n", "location =store_7days.groupby(['air_store_id','air_genre_name'])['latitude','longitude'].mean().reset_index()\n", "\n", "\n", "lbl = preprocessing.LabelEncoder()\n", "store1['air_genre_name'] = lbl.fit_transform(store1['air_genre_name'])\n", "\n", "data = np.array([location['latitude'] ,location['longitude'],location['air_genre_name']]).T\n", "map1 = folium.Map(location=[39, 139.6917], \n", "                        tiles = \"Stamentoner\",# width=1000, height=500,\n", "                        zoom_start = 5)\n", "marker_cluster = MarkerCluster(data).add_to(map1)\n", "map1"], "execution_count": null}, {"metadata": {"collapsed": true, "_cell_guid": "63694baa-64d4-4e79-894b-b60bf1854930", "_uuid": "cad9626ccab328b812ad0b97873efb4de7a42e17"}, "outputs": [], "cell_type": "code", "source": ["stored2.head()"], "execution_count": null}, {"metadata": {"collapsed": true, "_cell_guid": "5065cad0-caf1-4187-bce9-f1cd41a00924", "_uuid": "589739239f096171fd6a1af31e29904df11bf8e8"}, "outputs": [], "cell_type": "code", "source": ["import folium\n", "import os\n", "from folium import plugins\n", "N = 100\n", "\n", "data = np.array(\n", "    [\n", "        np.random.uniform(low=35, high=60, size=N),  # Random latitudes in Europe.\n", "        np.random.uniform(low=-12, high=30, size=N),  # Random longitudes in Europe.\n", "        range(N),  # Popups texts are simple numbers.\n", "    ]\n", ").T\n", "\n", "m = folium.Map([45, 3], zoom_start=4)\n", "\n", "plugins.MarkerCluster(data).add_to(m)\n", "\n", "#m.save(os.path.join('results', 'Plugins_1.html'))\n", "\n", "m"], "execution_count": null}, {"metadata": {"collapsed": true, "_cell_guid": "8613edcf-497d-493a-8f24-f0d85e83bf3a", "_uuid": "00135ec3fc9c3c78a5d5a7539a91d368ba05b302"}, "outputs": [], "cell_type": "code", "source": [], "execution_count": null}, {"metadata": {"collapsed": true, "_cell_guid": "dbb94a40-377a-4db6-aac0-80adcdc3456a", "_uuid": "dafa6ff60bd040f2164fb8633f1489d99f15a667"}, "outputs": [], "cell_type": "code", "source": [], "execution_count": null}, {"metadata": {"_cell_guid": "d03fd87a-2400-49b1-8927-3d5d5eda59f0", "_uuid": "2d1debdcf66e97c87574e839931310cb0b3ca21c"}, "cell_type": "markdown", "source": []}, {"metadata": {"_cell_guid": "26febeac-7ca5-44ef-a2c8-6473e7bfa693", "_uuid": "05ca2eeb2168b1a65eeb47a7c141228c9b3385cf"}, "cell_type": "markdown", "source": ["Visualising holidays in Japan"]}, {"metadata": {"_kg_hide-input": true, "collapsed": true, "_cell_guid": "04e35d40-bda8-43b1-ab1a-d602f59751e7", "_uuid": "6412fd3dbd5ffa03c43e8e20373270b5963905b2"}, "outputs": [], "cell_type": "code", "source": ["print('Total visitors - ',train['visitors'].sum())\n", "print('Total reservation air - ',train['reserve_visitors_air'].sum())\n", "print('Total reservation hpg - ',train['reserve_visitors_hpg'].sum())\n"], "execution_count": null}, {"metadata": {"collapsed": true, "_cell_guid": "dfcb6de4-9138-4e3d-938f-b6bbf267174e", "_uuid": "0b091b0426b29869fadcfc0d473fa560ad3a9aa9"}, "outputs": [], "cell_type": "code", "source": ["f,ax = plt.subplots(1,1,figsize=(15,1))\n", "x=data['hol']['visit_date']\n", "y=data['hol']['holiday_flg']\n", "plt.plot(x,y, color='m')\n", "plt.show()"], "execution_count": null}, {"metadata": {"_kg_hide-input": true, "collapsed": true, "_cell_guid": "1d44bab9-9c51-401e-8268-6b8af1ab9477", "_uuid": "b05ffb62bad55d37547496ea6b66f43dd4c7ce74"}, "outputs": [], "cell_type": "code", "source": ["#Visitor each day\n", "f,ax = plt.subplots(1,1,figsize=(15,4.5))\n", "plt1 = train.groupby(['visit_date'], as_index=False).agg({'visitors': np.sum})\n", "plt1=plt1.set_index('visit_date')\n", "plt1.plot(color='c', kind='area', ax=ax)\n", "plt.ylabel(\"Sum of Visitors\")\n", "plt.title(\"Visitor each day\")"], "execution_count": null}, {"metadata": {"_cell_guid": "58db06ff-05d3-49ca-acd4-fabf6db9d645", "_uuid": "6af3e04dfbc647ce22a972d1ae18432ec44a0895"}, "cell_type": "markdown", "source": ["Visitors based on days of the week for the last 1 year"]}, {"metadata": {"_kg_hide-input": true, "collapsed": true, "_cell_guid": "2ef62caa-b054-4ff3-bc17-9db24a3f1775", "_uuid": "86368b476c9a02456950433faa2e4b899e1568a6"}, "outputs": [], "cell_type": "code", "source": ["max_date=max(train['visit_date'])\n", "one_year = datetime.timedelta(days=364)\n", "year_ago= max_date - one_year\n", "train2=train[train['visit_date']>year_ago]\n", "pvt=pd.pivot_table(train2, index=['dow'], columns='month',values='visitors',aggfunc=[np.mean],fill_value=0)\n", "pvt2=pd.pivot_table(train2, index=['dow'], columns='month',values='visitors',aggfunc=[np.median],fill_value=0)\n", "pvt3=pd.pivot_table(train2, index=['dow'], columns='month',values='visitors',aggfunc=[np.max],fill_value=0)\n", "pvt4=pd.pivot_table(train2, index=['dow'], columns='month',values='visitors',aggfunc=[np.sum],fill_value=0)\n", "f, ax=plt.subplots(2,2, figsize=(15,8))\n", "sns.heatmap(pvt, ax=ax[0,0],cmap='cool')\n", "sns.heatmap(pvt2, ax=ax[0,1],cmap='cool')\n", "sns.heatmap(pvt3, ax=ax[1,0],cmap='cool')\n", "sns.heatmap(pvt4, ax=ax[1,1],cmap='cool')\n", "ax[0,0].set_title('Mean Visitors')\n", "ax[0,1].set_title('Median Visitors')\n", "ax[1,0].set_xlabel('Max Visitors', fontsize=13)\n", "ax[1,1].set_xlabel('Total Visitors', fontsize=13)\n", "#plt.xlabel(\"Month\")"], "execution_count": null}, {"metadata": {"collapsed": true, "_cell_guid": "8cd3d5af-4b68-4a6c-bd5f-049f4c9a9c20", "_uuid": "3372a17e4143509a9800930012370113fe34bb1b"}, "outputs": [], "cell_type": "code", "source": ["plt1 = train.groupby(['visit_date'], as_index=False).agg({'visitors': np.sum})"], "execution_count": null}, {"metadata": {"_kg_hide-input": true, "collapsed": true, "_cell_guid": "a4cb5b61-4e1b-4114-9187-d2d4f5e737e1", "_uuid": "e3c759b07084577655d0ec83fbc61cdde8bc5864"}, "outputs": [], "cell_type": "code", "source": ["plt.style.use('fivethirtyeight')\n", "plt1=train['visitors'].value_counts().reset_index().sort_index()\n", "fig,ax = plt.subplots()\n", "ax.bar(plt1['index'] ,plt1['visitors'],color='darkturquoise')\n", "fig.set_size_inches(12,4, forward=True)\n", "ax.set_xlim(0, 150)\n", "ax.set_title(\"PAX Frequency\")\n", "ax.set_ylabel('Counts')\n", "ax.set_xlabel('Number of PAX per visit')"], "execution_count": null}, {"metadata": {"collapsed": true, "_cell_guid": "93afa8c3-7f28-442c-99bf-2db1f0a0ac12", "_uuid": "9fe030e5de6c5aeaac9ea40cdae636921f558edd"}, "outputs": [], "cell_type": "code", "source": ["import numpy as np\n", "import pandas as pd\n", "from sklearn import ensemble, neighbors, linear_model, metrics, preprocessing\n", "from datetime import datetime\n", "import glob, re\n", "import time, datetime\n", "from datetime import timedelta\n", "data = {\n", "    'ar': pd.read_csv('../input/air_reserve.csv'),\n", "    'hr': pd.read_csv('../input/hpg_reserve.csv')}"], "execution_count": null}, {"metadata": {"collapsed": true, "_cell_guid": "bc501d2d-4354-4b47-be99-fd9a611928f8", "_uuid": "d372c505116648631ad8464a7d7811b925436faf"}, "outputs": [], "cell_type": "code", "source": ["'''data['ar']['visit_datetime'] = pd.to_datetime(data['ar']['visit_datetime'])\n", "data['ar']['reserve_datetime'] = pd.to_datetime(data['ar']['reserve_datetime'])\n", "data['ar']['visit_hour']=data['ar']['visit_datetime'].apply(lambda x: x.time().hour)\n", "data['ar']['dow_visit'] = data['ar']['visit_datetime'].dt.dayofweek\n", "data['ar']['month'] = data['ar']['visit_datetime'].dt.month\n", "data['ar']['reserve_day'] = data['ar'].apply(\n", "    lambda r: (r['visit_datetime'] - r['reserve_datetime']).days, axis=1)'''\n", "\n", "data['hr']['visit_datetime'] = pd.to_datetime(data['hr']['visit_datetime'])\n", "data['hr']['reserve_datetime'] = pd.to_datetime(data['hr']['reserve_datetime'])\n", "data['hr']['visit_hour']=data['hr']['visit_datetime'].apply(lambda x: x.time().hour)\n", "data['hr']['dow_visit'] = data['hr']['visit_datetime'].dt.dayofweek\n", "data['hr']['month'] = data['hr']['visit_datetime'].dt.month\n", "data['hr']['reserve_day'] = data['hr'].apply(\n", "    lambda r: (r['visit_datetime'] - r['reserve_datetime']).days, axis=1)\n", "\n", "data['hr'].head()\n"], "execution_count": null}, {"metadata": {"collapsed": true, "_cell_guid": "0c0a42e6-bcbe-407a-a22b-75c99d3afe4d", "_uuid": "65c3c4bd464f13c7e53fe3e896c5f95b8d130290"}, "outputs": [], "cell_type": "code", "source": [], "execution_count": null}, {"metadata": {"collapsed": true, "_cell_guid": "a940a9b2-70b7-4887-bdad-6f2a29116c78", "_uuid": "c168174960859e94e53e4e9408dd6e2ddcc41bbf"}, "outputs": [], "cell_type": "code", "source": [], "execution_count": null}], "nbformat_minor": 1}