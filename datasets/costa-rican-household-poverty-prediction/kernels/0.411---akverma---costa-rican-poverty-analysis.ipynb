{"cells":[{"metadata":{"_uuid":"a6c93c136402fe987beb8f356cf168c8b67ef389","trusted":true},"cell_type":"code","source":"%matplotlib inline\n\nimport os\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport lightgbm\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import f1_score, make_scorer\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.impute import SimpleImputer\nfrom matplotlib import pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f12fc720aa2514565c2179363238b5a4e4fb06d2","trusted":false},"cell_type":"code","source":"# To remove the limit on the number of rows displayed by pandas\npd.set_option(\"display.max_rows\", None)\n\n# Read csv files in pandas dataframe\ntestDf = pd.read_csv('../input/test.csv')\ntrainDf = pd.read_csv('../input/train.csv')\nprint(\"Training dataset basic information\")\nprint(\"Rows: {}\".format(len(trainDf)))\nprint(\"Columns: {}\".format(len(trainDf.columns)))\ntrainDf.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ac474022d02b916f956c114aa890e4062c367ffc","trusted":false},"cell_type":"code","source":"print(\"Test dataset basic information\")\nprint(\"Rows: {}\".format(len(testDf)))\nprint(\"Columns: {}\".format(len(testDf.columns)))\ntestDf.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"247875e5b353522dc8184b8d9e8ec9a0af49aa0c","trusted":false},"cell_type":"code","source":"# Add null Target column to test\ntestDf['Target'] = np.nan\ndata = trainDf.append(testDf, ignore_index = True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7ea4267a49b81d6abb2c5675fa026bbbcf1c85cf"},"cell_type":"markdown","source":"#### Exploratory Data Analysis (EDA)\n1. Find missing values\n2. Find outliers\n3. Find incosistent values\n4. Remove correlated features\n5. Feature engineering\n6. Feature scaling\n7. Impute missing values"},{"metadata":{"_uuid":"564bee57d7bb506335a2bf84ce64da7f235e05bf"},"cell_type":"markdown","source":"### 1. Find and fix missing feature values"},{"metadata":{"_uuid":"c705dc8a74a77a28131fbe8a71e77b94b605ce13","trusted":false},"cell_type":"code","source":"# 1. Find missing values in training and test dataset\ndef findColumnsWithNan(df):\n    cols = df.columns[df.isna().any()]\n    print(\"Number of columns with Nan: {}\".format(len(cols)))\n    print(\"Column names: {}\".format(cols))\n    print(\"-\" * 80)\n    for col in cols:\n        print(\"Column: [{}] missing {} values.\".format(col, len(df[df[col].isna() == True])))\n\nprint(\"Analysis of training dataset...\")\nfindColumnsWithNan(trainDf)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"184be6479eee5a9350d5b8a3c6f33be7a4bfd19e","trusted":false},"cell_type":"code","source":"print()\nprint(\"Analysis of test dataset...\")\nfindColumnsWithNan(testDf)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b77aa532bb952be3b00feddfa8de04b1b04e5232"},"cell_type":"markdown","source":"#### 1.1. Fix missing values of v2a1 \nIt means Monthly rent payment. To find what Nan means, v2a1 is compared with 'tipovivi' feature which gives information whether the house is rented/completely paid off etc. <br>\n\ntipovivi1 =1 own and fully paid house <br>\ntipovivi2 =1 own,  paying in installments\" <br>\ntipovivi3 =1 rented <br>\ntipovivi4 =1 precarious <br>\ntipovivi5 =1 other(assigned,  borrowed)\" <br>\n\n'v2a1' is replaced with 0, wherever tipovivi1=1, and all other missing values are left which will be imputed later. This means that if the house is fully owned by household then they don't pay any rent."},{"metadata":{"_uuid":"497cdf28a2a99d0d6953c0104a3f79b99f460070","trusted":false},"cell_type":"code","source":"data.loc[(data['tipovivi1'] == 1) & (data['v2a1'].isna()), 'v2a1'] = 0\nprint(\"Missing values after replacing: {}\".format(len(data.loc[data['v2a1'].isna()])))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5181716bf3bdcc1b27b1fdd339104a5874b579fd"},"cell_type":"markdown","source":"#### 1.2. Fix missing values of v18q1\nIt means number of tablets household owns. After careful analysis of household members, it can concluded that NaN means household does not own a tablet. We replace NaN with 0."},{"metadata":{"_uuid":"0f83b39a0513ba6a44041d35e37e6ed4c32f6267","trusted":false},"cell_type":"code","source":"data.loc[data['v18q1'].isna(), 'v18q1'] = 0\nprint(\"Missing values after replacing: {}\".format(len(data.loc[data['v18q1'].isna()])))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ff28ab169629451f25edc2409fa3d49e1e6bbffe"},"cell_type":"markdown","source":"#### 1.3 Fix missing values of rez_esc\nIt means years behind in school. From the discussions on Kaggle, it can be concluded that this value is defined only for people whose age is between 7 and 19. So the missing values can be updated to 0 using this criteria. Age of an individual is in the column appropriately named 'age'."},{"metadata":{"_uuid":"69a88d48ef5a8391c4498a143226f37a692666ec","trusted":false},"cell_type":"code","source":"data.loc[(data['age'] < 7) & (data['rez_esc'].isna()), 'rez_esc'] = 0\ndata.loc[(data['age'] > 19) & (data['rez_esc'].isna()), 'rez_esc'] = 0\nprint(\"Missing values after replacing: {}\".format(len(data.loc[data['rez_esc'].isna()])))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e6633f73c88ea57a61e713f73b0949f5bf614a6c"},"cell_type":"markdown","source":"#### 1.4 Fix missing values of meaneduc\nIt means average years of education for adults (18+). This implies that if the age of an individual is less than 18 and the value is NaN, then we can replace it with 0. Other NaN are left to be imputed."},{"metadata":{"_uuid":"59917077bec73878d3a687deb72eb64e3e82d400","trusted":false},"cell_type":"code","source":"data.loc[data['age'] < 19 & data['meaneduc'].isna(), 'meaneduc'] = 0\nprint(\"Missing values after replacing: {}\".format(len(data.loc[data['meaneduc'].isna()])))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7624cc8d5e481a5b5287916a6e146bc4266dd5e7"},"cell_type":"markdown","source":"#### 1.5 Fix missing values of SQBmeaned\nIt means square of the mean years of education of adults (>=18) in the household. It is highly correlated with feature 'age' and there is no real need of it. Hence, this feature is dropped from the dataset."},{"metadata":{"_uuid":"11e95b1cfba125f988bca316c71d6dc9cdb67ef8","trusted":false},"cell_type":"code","source":"data.drop('SQBmeaned', inplace=True, axis=1)\nprint(\"Total number of columns left: {}\".format(len(data.columns)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"814123fc3a0363a15b5f05f6711142b112662a0f"},"cell_type":"markdown","source":"### 2. Find outliers\nInter-quartile range(IQR) is used to identify outliers in the dataset. IQR is the difference between the 75th and 25th percentile of the data. It is measure of dispersion along the lines of standard deviation. During this analysis, features were found which had incosistent values like integer and boolean string (yes/no) mixed together. These needs to be removed and is the main focus of the next step."},{"metadata":{"_uuid":"07b11b9d3e1a043fda6ea0ce441f9b6cf6337539","trusted":false},"cell_type":"code","source":"for cols in data.columns[1:]:\n    if cols in ['idhogar', 'dependency', 'edjefe', 'edjefa']:\n        continue\n    percentile75 = np.percentile(data[cols].fillna(0), 75)\n    percentile25 = np.percentile(data[cols].fillna(0), 25)\n    threshold = (percentile75 - percentile25) * 1.5\n    lower, upper = (percentile25 - threshold), (percentile75 + threshold)\n    # identify outliers\n    outliers = data.loc[(data[cols] < lower) & (data[cols] > upper)]\n    if len(outliers) > 0:\n        print('Feature: {}. Identified outliers: {}'.format(cols, len(outliers)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ddf695e450561151e298c2d7172d69614c64a2cb"},"cell_type":"markdown","source":"### 3. Find incosistent values\nWhen finding outliers, following three features ('dependency', 'edjefe', 'edjefa') were found to have incosistent values. To take care of this, 'yes' is replaced with 1 and 'no' is replaced with 0. Also, to make sure each value in the feature are of the same data type, features are converted to float."},{"metadata":{"_uuid":"425769a1c6dbc5556b67866cd7f6d10f8a4c0ed8","trusted":false},"cell_type":"code","source":"for col in ['dependency', 'edjefe', 'edjefa']:\n    data.loc[data[col] == 'yes', col] = 1.0\n    data.loc[data[col] == 'no', col] = 0.0\n    data[col] = pd.to_numeric(data[col])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8a78f9d59d4725457496eb1d0a33ad8496141c9c"},"cell_type":"markdown","source":"### 4. Remove correlated features\nHighly correlated feature pairs are redundant, one of the pairs is selected to be removed."},{"metadata":{"_uuid":"edcac70dc1afe87c61c71893b41a3b194b498ac1","trusted":false},"cell_type":"code","source":"corrMat = data.corr()\nplt.figure(figsize=(30, 10))\nsns.heatmap(corrMat.iloc[:10, :10])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6a1385d08a11f4b70da61ea7fb6883f38fdd0629","trusted":false},"cell_type":"code","source":"def featuresToDrop(corrMatrix):\n    \"\"\"\n    To remove correlated features, used this gem of a code from here:\n    https://chrisalbon.com/machine_learning/feature_selection/drop_highly_correlated_features\n    \"\"\"\n    # Select upper triangle of correlation matrix\n    upper = corrMatrix.where(np.triu(np.ones(corrMatrix.shape), k=1).astype(np.bool))\n\n    # Find index of feature columns with correlation greater than 0.95\n    return [column for column in upper.columns if any(upper[column] > 0.95)]\n\ntoDrop = featuresToDrop(corrMat)\ndata.drop(toDrop, inplace=True, axis=1)\nprint(\"Correlated features which are dropped: {}\".format(toDrop))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"743c8b16abb49187c1cd5a3de35c0a219a027e1c"},"cell_type":"markdown","source":"### 5. Feature engineering\nDefinition: \"Feature engineering is the process of using domain knowledge of the data to create features that make machine learning algorithms work.\"\n\n5.1 Add aggregated features (min, max, std, sum)\n5.2 Add features per household\n5.3 Explore features"},{"metadata":{"_uuid":"57c7160f6f6c04ac946c8880b2d1cf2b4a9c4bce","trusted":false},"cell_type":"code","source":"features = list(data.drop(columns = ['Id', 'idhogar', 'Target']).columns)\naggDf = data.drop(columns='Target').groupby('idhogar').agg(['min', 'max', 'sum', 'count', 'std'])\n# Rename the columns\nnew_col = []\nfor c in aggDf.columns.levels[0]:\n    for stat in aggDf.columns.levels[1]:\n        new_col.append('{}-{}'.format(c, stat))\n        \naggDf.columns = new_col\ntoDrop = featuresToDrop(aggDf.corr())\naggDf.drop(toDrop, inplace=True, axis=1)\ndata = data.merge(aggDf, on='idhogar', how ='left')\nprint('Training feature shape: ', data.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"60d0e5193fba6d15f6e2f741d2cac6733029bd51","trusted":false},"cell_type":"code","source":"data['phones-per-capita'] = data['qmobilephone'] / data['tamviv']\ndata['tablets-per-capita'] = data['v18q1'] / data['tamviv']\ndata['rooms-per-capita'] = data['rooms'] / data['tamviv']\ndata['rent-per-capita'] = data['v2a1'] / data['tamviv']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"267c96d16a7b8f5f766d76a6e1f1cd2e9140098b"},"cell_type":"markdown","source":"### 6. Feature imputing and scaling\nScaling means to transform data in such a way that they fit within a range say 0-100 or 0-1. We will be using min-max scaler to transform the feature to be in the range 0-1."},{"metadata":{"_uuid":"e69be8df35b32f9c9370345458db21e82ccfb3dc","trusted":false},"cell_type":"code","source":"# Labels for training\ntrainTarget = np.array(list(data[data['Target'].notnull()]['Target'].astype(np.uint8)))\nsubmission = data.loc[data['Target'].isnull(), 'Id'].to_frame()\n\n# Extract the training data\ntrainData = data[data['Target'].notnull()].drop(columns = ['Id', 'idhogar', 'Target'])\ntestData = data[data['Target'].isnull()].drop(columns = ['Id', 'idhogar', 'Target'])\n\n# Impute training and test data\nimputer = SimpleImputer(missing_values=np.nan, strategy='median')\ntrainData = imputer.fit_transform(trainData)\ntestData = imputer.transform(testData)\n\n# Scale training and test data\nscaler = MinMaxScaler()\ntrainData = scaler.fit_transform(trainData)\ntestData = scaler.transform(testData)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4ce01c3c25b38413d4b3bf017a2d01f696e0203a"},"cell_type":"markdown","source":"### 7. Machine Learning model"},{"metadata":{"_uuid":"d5e6bfd7d4f1ac1ad88cceac25a06b4456295931","trusted":false},"cell_type":"code","source":"model = lightgbm.LGBMClassifier(max_depth=-1, learning_rate=0.1, objective='multiclass',\n                             random_state=None, silent=True, metric='None', \n                             n_jobs=4, n_estimators=5000, class_weight='balanced',\n                             colsample_bytree =  0.93, min_child_samples = 95, num_leaves = 14, subsample = 0.96)\nkfold = 5\nkf = StratifiedKFold(n_splits=kfold, shuffle=True)\n\npredicts_result = []\nfor idx, (train_index, test_index) in enumerate(kf.split(trainData, trainTarget)):\n    print(\"Fold: {}\".format(idx))\n    X_train, X_val = trainData[train_index], trainData[test_index]\n    y_train, y_val = trainTarget[train_index], trainTarget[test_index]\n    model.fit(X_train, y_train, verbose=100)\n    predicts_result.append(model.predict(testData))\nsubmission['Target'] = np.array(predicts_result).mean(axis=0).round().astype(int)\nsubmission.to_csv('submission.csv', index=False)\nprint(\"Completed!\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}