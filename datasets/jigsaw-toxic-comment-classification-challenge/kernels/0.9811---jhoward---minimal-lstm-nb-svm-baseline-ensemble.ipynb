{"cells":[{"metadata":{"_uuid":"6c82c5ade1ea2ebe3115b7acdbcb5eb01b8dc604","_cell_guid":"d1aed5c1-2284-4aa0-9bdd-8b3a87f10455"},"cell_type":"markdown","source":"## Introduction"},{"metadata":{"_uuid":"c8b9bb50ea3e069843370be618e835cd53a2dc5e","_cell_guid":"5a738023-69cc-4acc-a92b-53c338dd1ab3"},"cell_type":"markdown","source":"There are two very different strong baselines currently in the kernels for this competition:\n    \n- An *LSTM* model, which uses a recurrent neural network to model state across each text, with no feature engineering\n- An *NB-SVM* inspired model, which uses a simple linear approach on top of naive bayes features\n\nIn theory, an ensemble works best when the individual models are as different as possible. Therefore, we should see that even a simple average of these two models gets a good result. Let's try it! First, we'll load the outputs of the models (in the Kaggle Kernels environment you can add these as input files directly from the UI; otherwise you'll need to download them first)."},{"metadata":{"_uuid":"d3acc5945691f3ac94b41a99efd00d2f794b28d9","collapsed":true,"_cell_guid":"4f49e4ea-a226-4d61-921b-db33c41453a8","trusted":true},"cell_type":"code","source":"import numpy as np, pandas as pd\n\nf_lstm = '../input/improved-lstm-baseline-glove-dropout/submission.csv'\nf_nbsvm = '../input/nb-svm-strong-linear-baseline/submission.csv'","execution_count":3,"outputs":[]},{"metadata":{"_uuid":"12511977319d644ed8a07af913b0f8e00b14f22f","_cell_guid":"55a8085b-7a52-4c20-975d-4f6fe0dc7202","trusted":true},"cell_type":"code","source":"p_lstm = pd.read_csv(f_lstm)\np_nbsvm = pd.read_csv(f_nbsvm)","execution_count":4,"outputs":[]},{"metadata":{"_uuid":"756837877eacd2ab5312623640c66e156a75c1fe","_cell_guid":"74113048-4055-4ead-98b8-a003b6b075bd"},"cell_type":"markdown","source":"Now we can take the average of the label columns."},{"metadata":{"_uuid":"d3a16c6bb660eb600e68c11c03bd2d841b7515f4","collapsed":true,"_cell_guid":"c736d49f-de3b-46f5-b4c3-04a26ac98d31","trusted":true},"cell_type":"code","source":"label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\np_res = p_lstm.copy()\np_res[label_cols] = (p_nbsvm[label_cols] + p_lstm[label_cols]) / 2","execution_count":5,"outputs":[]},{"metadata":{"_uuid":"807d5f936cc49ccff2e4edc28287be06b008beb1","_cell_guid":"d4359384-5657-4660-8fd9-6b395bb7c2a6"},"cell_type":"markdown","source":"And finally, create our CSV."},{"metadata":{"_uuid":"30905be21b812c74c8bb1e3469134b226cbcfc78","collapsed":true,"_cell_guid":"5f73aafe-81a9-4149-b879-717fee5f1814","trusted":true},"cell_type":"code","source":"p_res.to_csv('submission.csv', index=False)","execution_count":6,"outputs":[]},{"metadata":{"_uuid":"648c18c7829885667bbb79a4ed95b82cb5cc620d","_cell_guid":"d66ecdcc-6339-48d5-94ad-519187078972"},"cell_type":"markdown","source":"As we hoped, when we submit this to Kaggle, we get a great result - much better than the individual scores of the kernels we based off. This is currently the best Kaggle kernel submission that runs within the kernels sandbox!"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"80a0e88a262fcc70d89fc2a8a4c9c061a7dad4e3"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}