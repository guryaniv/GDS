{"cells": [{"outputs": [], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "f81e624bdf59455964460f6b7cc228ff07a6ec0c", "_cell_guid": "25501782-da4a-4dae-b70e-5af0175f71e3"}, "source": ["import random\n", "import numpy as np # linear algebra\n", "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n", "import tensorflow as tf\n", "from IPython.display import clear_output, Image, display, HTML\n", "from matplotlib import pyplot"]}, {"outputs": [], "execution_count": null, "cell_type": "code", "metadata": {"scrolled": true, "_uuid": "5f6e47bbb2703382c04861bbb323e33d8276c041", "_cell_guid": "e44dcda5-d2be-4f79-b15d-b46b201d1c3c"}, "source": ["# Read data\n", "data = pd.read_csv(\"../input/creditcard.csv\")\n", "auto_encoder_variables = [s for s in data.columns if \"V\" in s]\n", "\n", "# Print first 10\n", "data.loc[1:10,auto_encoder_variables]"]}, {"metadata": {}, "cell_type": "markdown", "source": ["Train non-linear (sigmoid activation function) auto-encoder neural network with 2 hidden layers, 1st being input vector weights and 2nd being 2 dimensional weight embeddings. Minimize squared error by adjusting/optimising weights in opposite direction of error-fraction (according to learning rate) multiplied activation function derivative. This done by backpropagation of errors over mini batches of training data."]}, {"outputs": [], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "01c17f0eb15b8287434c32a047b48b8dd4342900", "_cell_guid": "dd5ae21d-ce2d-45a7-bfe5-a874795fed7b"}, "source": ["# Training Parameters\n", "learning_rate = 0.01\n", "num_steps = 100\n", "batch_size = 100\n", "display_step = 10\n", "\n", "# Network Parameters\n", "num_input = len(auto_encoder_variables)\n", "num_hidden_1 = len(auto_encoder_variables) # 1st layer num features\n", "num_hidden_2 = 2 # 2nd layer num features (the latent dim)\n", "\n", "# tf Graph input\n", "X = tf.placeholder(\"float\", [None, num_input])\n", "\n", "weights = {\n", "    'encoder_h1': tf.Variable(tf.random_normal([num_input, num_hidden_1])),\n", "    'encoder_h2': tf.Variable(tf.random_normal([num_hidden_1, num_hidden_2])),\n", "    'decoder_h1': tf.Variable(tf.random_normal([num_hidden_2, num_hidden_1])),\n", "    'decoder_h2': tf.Variable(tf.random_normal([num_hidden_1, num_input])),\n", "}\n", "biases = {\n", "    'encoder_b1': tf.Variable(tf.random_normal([num_hidden_1])),\n", "    'encoder_b2': tf.Variable(tf.random_normal([num_hidden_2])),\n", "    'decoder_b1': tf.Variable(tf.random_normal([num_hidden_1])),\n", "    'decoder_b2': tf.Variable(tf.random_normal([num_input])),\n", "}\n", "\n", "# Building the encoder\n", "def encoder(x):\n", "    # Encoder Hidden layer with sigmoid activation #1\n", "    layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights['encoder_h1']),\n", "                                   biases['encoder_b1']))\n", "    # Encoder Hidden layer with sigmoid activation #2\n", "    layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, weights['encoder_h2']),\n", "                                   biases['encoder_b2']))\n", "    return layer_2\n", "\n", "\n", "# Building the decoder\n", "def decoder(x):\n", "    # Decoder Hidden layer with sigmoid activation #1\n", "    layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights['decoder_h1']),\n", "                                   biases['decoder_b1']))\n", "    # Decoder Hidden layer with sigmoid activation #2\n", "    layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, weights['decoder_h2']),\n", "                                   biases['decoder_b2']))\n", "    return layer_2\n", "\n", "# Construct model\n", "encoder_op = encoder(X)\n", "decoder_op = decoder(encoder_op)\n", "\n", "# Prediction\n", "y_pred = decoder_op\n", "# Targets (Labels) are the input data.\n", "y_true = X\n", "\n", "# Define loss and optimizer, minimize the squared error\n", "loss = tf.reduce_mean(tf.pow(y_true - y_pred, 2))\n", "optimizer = tf.train.RMSPropOptimizer(learning_rate).minimize(loss)\n", "\n", "# Initialize the variables (i.e. assign their default value)\n", "init = tf.global_variables_initializer()\n", "\n", "encoded = None\n", "\n", "# Start Training\n", "# Start a new TF session\n", "with tf.Session() as sess:\n", "    # Run the initializer\n", "    sess.run(init)\n", "    # Training\n", "    for i in range(1, num_steps+1):\n", "        x_batch = data[auto_encoder_variables].sample(batch_size)\n", "        # Run optimization op (backprop) and cost op (to get loss value)\n", "        _, l = sess.run([optimizer, loss], feed_dict={X: x_batch})\n", "        # Display logs per step\n", "        if i % display_step == 0 or i == 1:\n", "            print('Step %i: Minibatch Loss: %f' % (i, l))\n", "    \n", "    # Encode data using auto-encoder neural network\n", "    encoded = pd.DataFrame(\n", "        sess.run(encoder_op, feed_dict={X: data[auto_encoder_variables]}),\n", "        columns = [\"X\", \"Y\"]\n", "    )"]}, {"outputs": [], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "e1ee795e0d388b2f72828dea3394b477358b1fb6", "_cell_guid": "dc85765d-823f-41f6-ac1a-86e996d50028"}, "source": ["# Print 10 first encoded variables\n", "encoded.loc[1:10,:]"]}, {"metadata": {}, "cell_type": "markdown", "source": ["Visualize auto-encoded data and color points by fraud (Yes/No)"]}, {"outputs": [], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "1490359be38b5eae3217993875d892c24ecfb295", "_cell_guid": "3ce20f31-6572-4412-882d-277ee12a66e1"}, "source": ["pyplot.scatter(\n", "    encoded[\"X\"],\n", "    encoded[\"Y\"],\n", "    c=data[\"Class\"],\n", "    s=0.005\n", ")\n", "\n", "pyplot.show()"]}, {"outputs": [], "execution_count": null, "cell_type": "code", "metadata": {"collapsed": true, "_uuid": "e3497ee01c8a5a1eac2e9d3539b78f30fca931b2", "_cell_guid": "87001dd1-cbf2-4a4e-ba32-bb36c6be12d2"}, "source": []}], "nbformat": 4, "metadata": {"language_info": {"pygments_lexer": "ipython3", "version": "3.6.3", "mimetype": "text/x-python", "codemirror_mode": {"version": 3, "name": "ipython"}, "file_extension": ".py", "name": "python", "nbconvert_exporter": "python"}, "kernelspec": {"display_name": "Python 3", "name": "python3", "language": "python"}}, "nbformat_minor": 1}