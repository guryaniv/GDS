{"cells":[{"metadata":{"scrolled":true,"trusted":true,"collapsed":true,"_uuid":"a6833dcb1682f9e0509b51cece6a82c78b829510"},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\n\n# required for visualizing plots directly in Jupyter Notebook\n%matplotlib inline","execution_count":13,"outputs":[]},{"metadata":{"_uuid":"9a882c087a5a73b45ba94e1c05b7a5b4b73574e2"},"cell_type":"markdown","source":"First let's read the training data and explore it"},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"686409f43e6caf433c64277bb6b033317324c1a3"},"cell_type":"code","source":"# reading the training dataset\ntrain = pd.read_csv('../input/sales_train.csv')\n\n# explore the train dataset structure\ntrain.head()","execution_count":19,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"6f41cfee8865dfe427acbb1bc9e5c8cea38e749c"},"cell_type":"code","source":"train.tail()","execution_count":3,"outputs":[]},{"metadata":{"_uuid":"30f705f1a2855a8f0fd5dd88a98ee9daf9b8e211"},"cell_type":"markdown","source":"We see that the column **'date'** can be converted to a datetime index for our dataframe. However I do not have to do that as my data already has a column **'date_block_num'** which shows the month in which the given sell occured. I will keep the column as it will not harm having it but I do not plan to use it for my prediction.\n\nAn interesting observation is that I have a negavite amount in my **'item_cnt_day'** column. I assume that this was actually delivered to the shop. If I was to aggregate the column **'item_cnt_day'** based on **['date_block_num', 'shop_id', 'item_id']** these negative values will actually decrease my total sales which will lead me into a wrong conclusions. Thus I will filter my data only for the positive values of **'item_cnt_day'**.\n\nThe first thing which I will explore it is to find the total number of sales per shop per month. This will give me information about the general trend of sales and also help me identify if all the shops are still open."},{"metadata":{"scrolled":false,"trusted":false,"_uuid":"5d1946ca23de53512c9f3e4ad2ef68226c6bdaa2"},"cell_type":"code","source":"# filtering the train dataset only on the SALES data\nfilter_1 = train['item_cnt_day'] > 0\n\n# applying the filter and creating new dataframe\nfiltered_1 = train[filter_1]\n\n# setting a plot size\nplt.rcParams['figure.figsize'] = (40.0, 20.0)\n\n# 2D plot for total sales/month per shops\nsns.heatmap(filtered_1.pivot_table(values='item_cnt_day', \n                                   index='shop_id', \n                                   columns='date_block_num', \n                                   aggfunc='sum', \n                                   dropna=True\n                                  ), \n            linewidths=1, \n            annot=True, \n            annot_kws={\"size\": 10}\n           )","execution_count":4,"outputs":[]},{"metadata":{"_uuid":"60dcc062270f50d08e44026a2433a541eb88ccbc"},"cell_type":"markdown","source":"From the plot I can observe that several shops appear to be closed in the last month **'date_block_num' == 33**. This might be a long shot but I would rather predict **0** sales regardless of the **item_id** for these shops in my test dataset than to apply any sort of learning algorithm on them. Thus I will reduce my sales dataset only to these shops which *(in my opininon)* are still open."},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"0a610ec2f68c9829d914f3fbee45eecc65681268"},"cell_type":"code","source":"# list of shops which were operational during the last month 'date_block_num' == 33\nfilter_2 = filtered_1[(filtered_1['date_block_num'] == 33)]['shop_id'].unique()\n\n# subset of the sales dataset based on filter_1\nfiltered_2 = filtered_1[filtered_1['shop_id'].isin(filter_2)]","execution_count":5,"outputs":[]},{"metadata":{"_uuid":"05e40122fdebdebf138db56f5a266e3d698d975b"},"cell_type":"markdown","source":"Let's explore the total number of sales again, this time on the filtered data"},{"metadata":{"trusted":false,"_uuid":"147bc6eb30bb6d2788f3feff652abb4fe644f525"},"cell_type":"code","source":"# setting a plot size\nplt.rcParams['figure.figsize'] = (40.0, 20.0)\n\n# 2D plot for total sales/month per shops\nsns.heatmap(filtered_2.pivot_table(values='item_cnt_day', \n                                   index='shop_id', \n                                   columns='date_block_num', \n                                   aggfunc='sum', \n                                   dropna=True\n                                  ), \n            linewidths=1, \n            annot=True, \n            annot_kws={\"size\": 10}\n           )","execution_count":6,"outputs":[]},{"metadata":{"_uuid":"93556cb64d310cf3f5a50eb1099c24a48a015869"},"cell_type":"markdown","source":"The plot already looks good in term of patter recognition however I will make it even more clear"},{"metadata":{"trusted":false,"_uuid":"fc87a5ae38f0ba15aabdcfbd2b803d9768eabe99"},"cell_type":"code","source":"# defining a list with the month names for a period of 99 year (99 is more or less an arbitrary number)\nmonths = ['January','February','March','April','May','June','July','August','September','October','November','December']*99\n\n# setting a plot size\nplt.rcParams['figure.figsize'] = (20.0, 10.0)\n\n# 2D plot for total sales/month per shops\nsns.heatmap(filtered_2.pivot_table(values='item_cnt_day', \n                                   index='shop_id', \n                                   columns='date_block_num', \n                                   aggfunc='sum', \n                                   dropna=True\n                                  ), \n            linewidths=1, \n            xticklabels=months[0:34]\n           )","execution_count":7,"outputs":[]},{"metadata":{"_uuid":"5ed098efde1d1efdda54baa485d5185f7ab88d7c"},"cell_type":"markdown","source":"From this plot we can observe that there are 3 shops **[9,20,36]** which are open only in October. I believe there is a reason for this e.g. high demand due to public holidays in Russia or whatsoever but I am not really interested in the exact details behind this. I will simply filter my dataset once again and remove those shops. As you may have guessed so far I will predict **0** for those shops as my objective is to estimate the sales for **November 2015**."},{"metadata":{"trusted":false,"_uuid":"bfae51dc4755c4fced3c9364e6d5df27cd220940"},"cell_type":"code","source":"# list of shops which were operational in 'December'\nfilter_3 = filtered_2[(filtered_2['date_block_num'] == 23)]['shop_id'].unique()\n\n# subset of the sales dataset based on filter_1\nfiltered_3 = filtered_2[filtered_2['shop_id'].isin(filter_3)]\n\n# defining a list with the month names for a period of 99 year (99 is more or less an arbitrary number)\nmonths = ['January','February','March','April','May','June','July','August','September','October','November','December']*99\n\n# setting a plot size\nplt.rcParams['figure.figsize'] = (20.0, 10.0)\n\n# 2D plot for total sales/month per shops\nsns.heatmap(filtered_3.pivot_table(values='item_cnt_day', \n                                   index='shop_id', \n                                   columns='date_block_num', \n                                   aggfunc='sum', \n                                   dropna=True\n                                  ), \n            linewidths=1, \n            xticklabels=months[0:34]\n           )\n\n# rotating the shop_id labels on the plot\nplt.yticks(rotation='horizontal')","execution_count":8,"outputs":[]},{"metadata":{"_uuid":"eb848c57cb9f4dd4e12ca3dccfafb699872ea0eb"},"cell_type":"markdown","source":"Now I have (what I believe to be) all the shops which need an actual prediction. I will start by predicting the total number of sales for each shop and later I will break (distribute) this number amoung the different **item_id**'s."},{"metadata":{"trusted":false,"_uuid":"164329c95cd47c615cc735a60724cf8785a71200"},"cell_type":"code","source":"# assigning the pivot table into a variable\nfiltered_3_pivoted = filtered_3.pivot_table(values='item_cnt_day', \n                                   index='shop_id', \n                                   columns='date_block_num', \n                                   aggfunc='sum', \n                                   dropna=True\n                                  )\n\n# converting the pivot table into a dataframe\nfiltered_3_records = pd.DataFrame(filtered_3_pivoted.to_records())\n\n# transposing the dataframe\nfiltered_3_records = filtered_3_records.transpose()\n\n# assiginig the correct column names\nfiltered_3_records.columns = filtered_3_records.loc['shop_id'].astype('str')\n\n# ---------------- converting 2.0 into 2 and etc.\nnew_names = []\n\nfor name in filtered_3_records.columns:\n    new_names.append(name.split('.')[0])\n\nfiltered_3_records.columns = new_names\n\n# removing the 'shop_id' row\nfiltered_3_records.drop('shop_id', axis=0, inplace=True)\n\n# replacing the NaN values with 0.0\nfiltered_3_records.fillna(value=0.0, inplace=True)\n\n# converting the index to dtype='int64'\nfiltered_3_records.index = filtered_3_records.index.astype('int64')\n\n# exploring the dataframe\nfiltered_3_records.head()","execution_count":9,"outputs":[]},{"metadata":{"_uuid":"8f7366c729d70b4fe0524af42ce47e185cca7914"},"cell_type":"markdown","source":"What we have now is a dataframe where each row represents the **'date_block_num'** , each column the **shop_id** and the values inside are the total number of sales for that combination.\n\nIn order for you to get the sense of timeseries data I need to produce one more plot"},{"metadata":{"trusted":false,"_uuid":"6bf4e3a621f99d4402913efb3f0e22e03e32b14d"},"cell_type":"code","source":"# showing time-history of total sales for shop 31\nfiltered_3_records['31'].plot()","execution_count":10,"outputs":[]},{"metadata":{"_uuid":"6baeda808fbfb9b1d24e42c2d1e6109637119f7e"},"cell_type":"markdown","source":"If we were to explore the ETS (error,trend,seasonality) of this data we can easily look at its seasonal decomposition provided by the statsmodels package."},{"metadata":{"scrolled":false,"trusted":false,"_uuid":"02ac41c37895fa3ccb875cc70d2fc37670ea2a57"},"cell_type":"code","source":"plt.rcParams['figure.figsize'] = (13, 13)\n\nfiltered_3_records['date'] = pd.date_range(start='2013-01-01', end='2015-11-01', freq='M')\n\nfiltered_3_records = filtered_3_records.set_index('date')\n\ndecomposed = sm.tsa.seasonal_decompose(filtered_3_records['31'], freq=12, model='multiplicative').plot()","execution_count":17,"outputs":[]},{"metadata":{"_uuid":"6037ffd53e102f5829976848af70cd4e3a76980e"},"cell_type":"markdown","source":"Using the data from the plot above I could predict the future sales by extrapolating the trend component and multiplying it with the corresponding seasonal value. However the error component (residuals) I cannot estimate as they tend to follow a random pattern. Just for reference here is a histogram of the residuals:"},{"metadata":{"trusted":false,"_uuid":"127cba183fd095b809037e28b8d63002affa12d8"},"cell_type":"code","source":"residuals = sm.tsa.seasonal_decompose(filtered_3_records['31'], freq=12, model='multiplicative').resid\n\nplt.rcParams['figure.figsize'] = (5, 5)\n\nplt.hist(residuals.dropna(), bins=3)","execution_count":35,"outputs":[]},{"metadata":{"_uuid":"d1c34d325f331ae8a4726e1cd2298787e678d8bc"},"cell_type":"markdown","source":"For forecasting I will use the fbprophet package provided by Facebook. "},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"0ab66e166ff132072ca323f39f68f297b646d172"},"cell_type":"code","source":"from fbprophet import Prophet ","execution_count":38,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"81dd7a0d994b562f06842e2057922998d8fdce6b"},"cell_type":"code","source":"df = pd.DataFrame()\n\ndf['ds'] = filtered_3_records['31'].index\ndf['y'] = filtered_3_records['31'].values\n\nm = Prophet().fit(df)\nfuture = m.make_future_dataframe(periods=1, freq='M')\nfcst = m.predict(future)\nm.plot(fcst)\nplt.axvline(x='2015-10-31', color='red')","execution_count":62,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"1e4b2b6293f1e7c3bb8e0af9200bc898e69942a0"},"cell_type":"code","source":"fcst.head()","execution_count":68,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"23d0f5318d3c7243e11df1bf5a6e090b79674ff4"},"cell_type":"code","source":"fcst[['yhat_lower','yhat','yhat_upper']].loc[32:34]","execution_count":55,"outputs":[]},{"metadata":{"_uuid":"abc00e4a8c0953d6e92e0f41efd597d716d0eebf"},"cell_type":"markdown","source":"The next thing I will do is to create a wrapper function which will return the prediction for a given store"},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"b557f1326b9f81053894f4dae7b356de0fb27ac2"},"cell_type":"code","source":"def prediction(series):\n    \n    # create an empty dataframe\n    df = pd.DataFrame()\n\n    # assign columns\n    df['ds'] = series.index\n    df['y'] = series.values\n    \n    # make the actual prediction for the next month\n    m = Prophet(weekly_seasonality=False, daily_seasonality=False).fit(df)\n    future = m.make_future_dataframe(periods=1, freq='M')\n    fcst = m.predict(future)\n    \n    # return the predicted value for the last month\n    return fcst['yhat'].loc[34]","execution_count":85,"outputs":[]},{"metadata":{"collapsed":true,"scrolled":true,"trusted":false,"_uuid":"35897e2251d6fadc744d056a7ccfc7c05b2e4854"},"cell_type":"code","source":"# initializing an empty dictionary to store the predicted values\npredicted_sales = {}\n\nfor column in filtered_3_records.columns:\n    \n    # extracting a single column of the dataframe into a series\n    # passing the series to our wrapper function\n    predicted_value = prediction(filtered_3_records[column])\n    \n    # storing the predicted value into a dictionary\n    predicted_sales[column] = predicted_value","execution_count":86,"outputs":[]},{"metadata":{"_uuid":"4ec96377f314c20c42939b349553794a1260635d"},"cell_type":"markdown","source":"Once I have the dictionary filled in I will convert it to a dataframe which I will later add to my existing dataframe."},{"metadata":{"trusted":false,"_uuid":"66b4c9d8e746525061606c65ff546c40dac24b06"},"cell_type":"code","source":"df = pd.DataFrame.from_dict(predicted_sales, orient='index')\ndf = df.transpose()\n\n# adding a new row to our dataframe\nfiltered_3_records = filtered_3_records.append(df, ignore_index=True)\n\n# checking the last 5 rows of the dataframe\nfiltered_3_records.tail()","execution_count":131,"outputs":[]},{"metadata":{"_uuid":"3b68086b1bf80e9e44ed61c0bfd5af1dfe8e25de"},"cell_type":"markdown","source":"Now I have the total sales for November I just need to decompose the sales amoung the different **'item_id'** items. The way to do that is to explore the historical data for november."},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"23fc69948453ee1afa03ac74c0eb9490a0c2d607"},"cell_type":"code","source":"# subsetting the dataframe to November 2013\nNovember_2013 = filtered_3[filtered_3['date_block_num'] == 10]\n\n# subsetting the dataframe to November 2014\nNovember_2014 = filtered_3[filtered_3['date_block_num'] == 22]","execution_count":134,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"8ef08b60808372eb6185cfd616bdf4eb2d9128f7"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"09cb733b5b9bffc8bfdcd5b1e5b04e1ed52c9812"},"cell_type":"code","source":"# creating a pivot table \nNovember_2013_pivoted = November_2013.pivot_table(index='shop_id', columns='item_id', values='item_cnt_day', aggfunc='sum', fill_value=0)\n\n# converting the pivot table into a dataframe\nNovember_2013_records = pd.DataFrame(November_2013_pivoted.to_records())\n\n# setting an index for the dataframe\nNovember_2013_records.set_index('shop_id', inplace=True)\n\n# normalizing the dataframe\n# here a convert the number of sales per item to a fraction of the total sales for the shops\nNovember_2013_records = November_2013_records.div(November_2013_records.sum(axis=1), axis=0)\n\nNovember_2013_records.head()","execution_count":183,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"6b7c47555731c6a30a67b903521e29ee05dc3d6b"},"cell_type":"code","source":"# creating a pivot table \nNovember_2014_pivoted = November_2014.pivot_table(index='shop_id', columns='item_id', values='item_cnt_day', aggfunc='sum', fill_value=0)\n\n# converting the pivot table into a dataframe\nNovember_2014_records = pd.DataFrame(November_2014_pivoted.to_records())\n\n# setting an index for the dataframe\nNovember_2014_records.set_index('shop_id', inplace=True)\n\n# normalizing the dataframe\n# here a convert the number of sales per item to a fraction of the total sales for the shops\nNovember_2014_records = November_2014_records.div(November_2014_records.sum(axis=1), axis=0)\n\nNovember_2014_records.head()","execution_count":184,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"f11bebb4a21cb0f7da0086c9322f10f13f2909c8"},"cell_type":"code","source":"# merging the two dataframes and in the same time taking the mean for the columns with common names\n# https://stackoverflow.com/questions/50312018/merge-two-dataframes-in-pandas-by-taking-the-mean-between-the-columns\nNovember_concat = pd.concat([November_2013_records, November_2014_records], axis=1).groupby(axis=1, level=0).mean()\n\n# softmax row-wise\nNovember_concat = November_concat.div(November_concat.sum(axis=1), axis=0)\n\n# converting to number of sales\nseries = filtered_3_records.loc[34]\nseries.index = series.index.astype('int64')\nseries.index.name = 'shop_id'\n\nNovember_concat = November_concat.mul(series, axis=0)\n\n# replace NaN values with 0.0\nNovember_concat.fillna(0.0, inplace=True)\n\n# rounding the sales to whole numbers\nNovember_concat = November_concat.round(decimals=0)\n\nNovember_concat.head()","execution_count":265,"outputs":[]},{"metadata":{"_uuid":"aaac42281d611688093746a7693b387003c29b63"},"cell_type":"markdown","source":"Now I have my lookup table, which I intend to use directly for prediction.\n\nLet's start by loading the test dataset:"},{"metadata":{"_kg_hide-output":false,"collapsed":true,"trusted":true,"_uuid":"9e398ea285d00a6c6fd0e44ec664caff6d1229bd"},"cell_type":"code","source":"test = pd.read_csv('../input/test.csv', index_col='ID')","execution_count":21,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"65bf28ddb646c4ac91328f7d79eade4a35064e50"},"cell_type":"code","source":"test.head()","execution_count":22,"outputs":[]},{"metadata":{"_uuid":"cc139388181c181c21a34ed07084cda241c38872"},"cell_type":"markdown","source":"Now I will construct a helper function which will lookup through my lookup dataframe and assign a value for each row of the test dataset:"},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"671de24348f6410fa1019be4289d27ab69a765b2"},"cell_type":"code","source":"def lookup(df, row):\n    '''\n    function which returns the total number of sales of given item_id for given shop_id\n    '''\n    try:\n        value = df.loc[row['shop_id']][row['item_id']]\n    except:\n        # KeyError -> store_id missing\n        # IndexError -> item_id missing\n        # for both cases makes sense to predict 0 as either the store is closed or the given item was not sold even once\n        # in the given store in the past Novembers\n        value = 0\n    \n    return value\n\n# apply the lookup function to each row of the test dataframe and store the result in 'prediction' column\ntest['item_cnt_month'] = test.apply(lambda row: lookup(November_concat,row), axis=1)","execution_count":279,"outputs":[]},{"metadata":{"_uuid":"67bd88a908a07e4deb57d7639dc6da032cf64c39"},"cell_type":"markdown","source":"As the competition rules state, my results have to be cliped on a range of [0,20]. So the final step would be that:"},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"b8ee1a9d9c03d27423c6d561d2707549735ad682"},"cell_type":"code","source":"test['item_cnt_month'].clip(lower=0, upper=20, inplace=True)","execution_count":280,"outputs":[]},{"metadata":{"_uuid":"e25c536391c8400f9884c1c45b452f3e486ea94a"},"cell_type":"markdown","source":"For submission I need to have only two columns **['ID','item_cnt_month']**"},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"f2bbf9f9478d16cdc7ec7ee3f9826bf729f8e0fe"},"cell_type":"code","source":"submission = test.drop(['shop_id','item_id'], axis=1)\n\nsubmission.to_csv('submission-01.csv')","execution_count":281,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}