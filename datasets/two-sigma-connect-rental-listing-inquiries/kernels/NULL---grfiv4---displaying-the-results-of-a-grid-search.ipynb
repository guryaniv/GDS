{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "257d3a73-fbdc-9c57-8064-30f743e15fff"
      },
      "source": [
        "# Displaying the results of a Grid Search\n",
        "\n",
        "`sklearn.model_selection.GridSearchCV` and    \n",
        "`sklearn.model_selection.RandomizedSearchCV` \n",
        "\n",
        "are very useful ways to parametrize an estimator. This notebook shows a function that displays the results in a helpful way."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ab3f7178-87ef-68cd-5e43-b997ba642fbd"
      },
      "outputs": [],
      "source": [
        "import numpy  as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn import svm, datasets\n",
        "from sklearn.model_selection import GridSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "65ab888b-9f0c-dd27-6a88-dd871f891f99"
      },
      "outputs": [],
      "source": [
        "def GridSearch_table_plot(grid_clf, param_name,\n",
        "                          num_results=15,\n",
        "                          negative=True,\n",
        "                          graph=True,\n",
        "                          display_all_params=True):\n",
        "\n",
        "    '''Display grid search results\n",
        "\n",
        "    Arguments\n",
        "    ---------\n",
        "\n",
        "    grid_clf           the estimator resulting from a grid search\n",
        "                       for example: grid_clf = GridSearchCV( ...\n",
        "\n",
        "    param_name         a string with the name of the parameter being tested\n",
        "\n",
        "    num_results        an integer indicating the number of results to display\n",
        "                       Default: 15\n",
        "\n",
        "    negative           boolean: should the sign of the score be reversed?\n",
        "                       scoring = 'neg_log_loss', for instance\n",
        "                       Default: True\n",
        "\n",
        "    graph              boolean: should a graph be produced?\n",
        "                       non-numeric parameters (True/False, None) don't graph well\n",
        "                       Default: True\n",
        "\n",
        "    display_all_params boolean: should we print out all of the parameters, not just the ones searched for?\n",
        "                       Default: True\n",
        "\n",
        "    Usage\n",
        "    -----\n",
        "\n",
        "    GridSearch_table_plot(grid_clf, \"min_samples_leaf\")\n",
        "\n",
        "                          '''\n",
        "    from matplotlib      import pyplot as plt\n",
        "    from IPython.display import display\n",
        "    import pandas as pd\n",
        "\n",
        "    clf = grid_clf.best_estimator_\n",
        "    clf_params = grid_clf.best_params_\n",
        "    if negative:\n",
        "        clf_score = -grid_clf.best_score_\n",
        "    else:\n",
        "        clf_score = grid_clf.best_score_\n",
        "    clf_stdev = grid_clf.cv_results_['std_test_score'][grid_clf.best_index_]\n",
        "    cv_results = grid_clf.cv_results_\n",
        "\n",
        "    print(\"best parameters: {}\".format(clf_params))\n",
        "    print(\"best score:      {:0.5f} (+/-{:0.5f})\".format(clf_score, clf_stdev))\n",
        "    if display_all_params:\n",
        "        import pprint\n",
        "        pprint.pprint(clf.get_params())\n",
        "\n",
        "    # pick out the best results\n",
        "    # =========================\n",
        "    scores_df = pd.DataFrame(cv_results).sort_values(by='rank_test_score')\n",
        "\n",
        "    best_row = scores_df.iloc[0, :]\n",
        "    if negative:\n",
        "        best_mean = -best_row['mean_test_score']\n",
        "    else:\n",
        "        best_mean = best_row['mean_test_score']\n",
        "    best_stdev = best_row['std_test_score']\n",
        "    best_param = best_row['param_' + param_name]\n",
        "\n",
        "    # display the top 'num_results' results\n",
        "    # =====================================\n",
        "    display(pd.DataFrame(cv_results) \\\n",
        "            .sort_values(by='rank_test_score').head(num_results))\n",
        "\n",
        "    # plot the results\n",
        "    # ================\n",
        "    scores_df = scores_df.sort_values(by='param_' + param_name)\n",
        "\n",
        "    if negative:\n",
        "        means = -scores_df['mean_test_score']\n",
        "    else:\n",
        "        means = scores_df['mean_test_score']\n",
        "    stds = scores_df['std_test_score']\n",
        "    params = scores_df['param_' + param_name]\n",
        "\n",
        "    # plot\n",
        "    if graph:\n",
        "        plt.figure(figsize=(8, 8))\n",
        "        plt.errorbar(params, means, yerr=stds)\n",
        "\n",
        "        plt.axhline(y=best_mean + best_stdev, color='red')\n",
        "        plt.axhline(y=best_mean - best_stdev, color='red')\n",
        "        plt.plot(best_param, best_mean, 'or')\n",
        "\n",
        "        plt.title(param_name + \" vs Score\\nBest Score {:0.5f}\".format(clf_score))\n",
        "        plt.xlabel(param_name)\n",
        "        plt.ylabel('Score')\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "2daee773-268a-f293-2767-95f3142b6227"
      },
      "outputs": [],
      "source": [
        "iris = datasets.load_iris()\n",
        "\n",
        "grid_clf = GridSearchCV(estimator  = svm.SVC(), \n",
        "                        param_grid = {'C': range(1,10)}, \n",
        "                        cv         = 10)\n",
        "\n",
        "_ = grid_clf.fit(iris.data, iris.target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "e9398782-7762-1e7b-f938-9d86fc906009"
      },
      "outputs": [],
      "source": [
        "GridSearch_table_plot(grid_clf, \"C\", negative=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "3edf3c14-4b6e-6419-b0c7-ec80f7d31687"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}