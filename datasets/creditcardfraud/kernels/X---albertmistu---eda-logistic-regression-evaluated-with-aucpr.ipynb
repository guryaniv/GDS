{"cells": [{"outputs": [], "execution_count": null, "cell_type": "code", "metadata": {"collapsed": true, "_cell_guid": "70d67bf1-4a48-2a4e-9c5f-27c5ee906da6", "_execution_state": "idle", "_uuid": "fe932a53b4e867ae092c8eabaf21b384eb328eee"}, "source": ["# This Python 3 environment comes with many helpful analytics libraries installed\n", "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n", "# For example, here's several helpful packages to load in \n", "import numpy as np \n", "import pandas as pd \n", "# Input data files are available in the \"../input/\" directory.\n", "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n", "from subprocess import check_output\n", "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n", "# Any results you write to the current directory are saved as output."]}, {"source": ["Read database. Show data information: we can see that we do not have any missing values. \n", "We have 28 principal components from the PCA (already applied in dataset)"], "cell_type": "markdown", "metadata": {"_cell_guid": "c8dc2c69-8d28-4c77-80fe-f31dd9ce3654", "_uuid": "074f858e09f3458d19830d1cb71498cbf03d3a60"}}, {"outputs": [], "execution_count": null, "cell_type": "code", "metadata": {"_cell_guid": "0816445d-6fdc-4fdc-aa20-f47ce9981661", "collapsed": true, "_uuid": "dd9b021d7919c3be7ce4d8ee48f97a07a4a6de15"}, "source": ["df = pd.read_csv('../input/creditcard.csv')\n", "print(df.info())"]}, {"source": ["**Exploratory data analysis:\n", "**First I plotted separately the amount of the transactions vs. time for fraudulent and non-fraudulent as well as the distributions of amounts for each case. The goal was to see if fraudulent transactions have higher or lower amounts (on average) than non-fraudulents and if they follow any temporal pattern.\n"], "cell_type": "markdown", "metadata": {"_cell_guid": "0726ba17-cc5f-4fb6-adee-b1ce9502c091", "_uuid": "4520b2ee60b66572e8bafe478b7b68f155c0800a"}}, {"outputs": [], "execution_count": null, "cell_type": "code", "metadata": {"collapsed": true, "_cell_guid": "d595df43-e102-41cb-dd0d-ba53aa960ace", "_execution_state": "idle", "_uuid": "85d994e3256992019165bf57aaed07dcc9352e9b"}, "source": ["import matplotlib.pyplot as plt\n", "fig, ax = plt.subplots(1, 3)\n", "ax[0].scatter(df['Time'][df['Class'] == 0], df['Amount'][df['Class']==0], color='b' )\n", "ax[0].scatter(df['Time'][df['Class'] == 1], df['Amount'][df['Class']==1], color='r' , marker='.')\n", "ax[0].legend(['non fraudulent', 'fraudulent'], loc='best')\n", "ax[1].hist( df['Amount'][df['Class']==0], 100, facecolor='b', alpha=0.5, label=\"Distribution of amounts for non-fraudulent \")\n", "ax[2].hist( df['Amount'][df['Class']==1], 100, facecolor='r', ec=\"black\", lw=0.5, alpha=0.5, label=\"Distribution of amounts for fraudulent\")\n", "ax[0].set_xlabel(\"Time\")\n", "ax[0].set_ylabel(\"Amount\")\n", "ax[1].set_xlabel(\"Amount\")\n", "ax[1].set_ylabel(\"Frequency\")\n", "ax[2].set_xlabel(\"Amount\")\n", "ax[2].set_ylabel(\"Frequency\")\n", "ax[1].set_title('non-fraudulent')\n", "ax[2].set_title('fraudulent')\n", "fig.subplots_adjust(left=0, right=2, bottom=0, top=1, hspace=0.05, wspace=0.5)"]}, {"source": ["We can see that all fraudulent amounts are below 2000 euros. The majority of non-fraudulent namounts are also below 2000 euros, although there are several large amounts. This is a sign that the amount won't be a very important variable to detect a non-fraudulent transaction.\n", "Let's analyze the correlation matrix. We know that all the PCs are going to have correlation = 0 because the PCA procedure obtains uncorrelated components. What we want to analyze here is which of the components are more correlated with the class, the time and amount."], "cell_type": "markdown", "metadata": {"_cell_guid": "a129959b-ab9c-487e-8248-9ce099255a07", "_uuid": "6b87a067258df13ccca2b9b07dc3ae88ae332b2b"}}, {"outputs": [], "execution_count": null, "cell_type": "code", "metadata": {"_cell_guid": "3a6f167f-c5a7-6d71-fe1a-59f4faa5c856", "collapsed": true, "_uuid": "88d69f0664fd280d7c1d2828a7e7c7750659683f"}, "source": ["import seaborn as sns\n", "corr=df.corr()\n", "mask=np.zeros_like(corr)\n", "mask[np.triu_indices_from(mask)] = True\n", "with sns.axes_style(\"white\"):\n", "    ax = sns.heatmap(corr, xticklabels=corr.columns.values, yticklabels=corr.columns.values, mask=mask, vmax=.6, square=True)\n", "sns.plt.show()"]}, {"source": ["We can see that the amount of the transaction or the time is not correlated with the class. The most positively correlated PC with the class is V11 and V4, while the most negative correlated are V17 and V14. We will try to use this four PCs for the first simple model and then try more complicated models to detect the non-fraudulent transactions\n", "Finally, we evaluate the distributions of the four most correlated features and the two least correlated for both fraudulent and non-fraudulent cases."], "cell_type": "markdown", "metadata": {"_cell_guid": "8496d450-882c-46fa-a901-507b9552f602", "_uuid": "e9415d81b54b85096018389e8473e0a7fb1080ad"}}, {"outputs": [], "execution_count": null, "cell_type": "code", "metadata": {"_cell_guid": "8b1479b1-ab0f-4ec3-9314-129a291e5da1", "collapsed": true, "_uuid": "d91b407bb2983708f1c3ec04ca1cbf2200276d88"}, "source": ["import matplotlib.gridspec as gridspec\n", "features=['V17','V14', 'V11', 'V4', 'V15', 'V13']\n", "nplots=np.size(features)\n", "plt.figure(figsize=(15,4*nplots))\n", "gs = gridspec.GridSpec(nplots,1)\n", "for i, feat in enumerate(features):\n", "    ax = plt.subplot(gs[i])\n", "    sns.distplot(df[feat][df.Class==1], bins=30)\n", "    sns.distplot(df[feat][df.Class==0],bins=30)\n", "    ax.legend(['fraudulent', 'non-fraudulent'],loc='best')\n", "    ax.set_xlabel('')\n", "    ax.set_title('Distribution of feature: ' + feat)"]}, {"source": ["We can see that the division between the two classes is almost impossible if we use V15 and V13\n", "\n", "As the description says, only 0.172% of the total instances are frauds (492 out of 284.807). \n", "Thus the dataset is highly unbalanced. To evaluate the accuracy of our classifier we will use the AUPRC.\n", "Now that we have done some exploratory analysis, we will start by building simple models with the dataset as it is. After that, we will try to oversample the 'fraud' class in order to balance the dataset and finally we will build more complicated models. I will perform a comparison of the model performances.\n", "\n", "Now let's evaluate the range of values of the features and their average + std\n"], "cell_type": "markdown", "metadata": {"_cell_guid": "282af93b-ab9d-49dc-82be-c02b89bf48a2", "collapsed": true, "_uuid": "cdee2570da6e640ce614aa754e4e79a71a725ff5"}}, {"outputs": [], "execution_count": null, "cell_type": "code", "metadata": {"_cell_guid": "0807c7c3-29a3-48e2-9d20-4b19b1b5989b", "collapsed": true, "_uuid": "3d0ffb3fd2eaad5273f52243406bbb01745f578a"}, "source": ["df.boxplot()\n", "plt.ylim((-5,350))\n", "locs, labels = plt.xticks()\n", "plt.setp(labels, rotation=45)"]}, {"source": ["We can see that 'amount' and 'time' need to be scaled... We will include a MinMax() scaler\n", "to scale all features from 0-1. Note this is not needed if we use DecisionTrees or RandomForests...\n", "\n", "Finally, we will train a simple LogisticRegression model to see what the AUCPR is. Note this is not a very smart model for several reasons:\n", "First, the data is highly unbalanced. Second, we are assuming a linear classification problem. Finally, we are not performing any kind of feature selection.\n", "A smarter model will be applied in another kernel, where I will apply GMM to the non-fraudulent cases and detect anomalies by comparing the fraudulent test cases to the trained GMM. "], "cell_type": "markdown", "metadata": {"_cell_guid": "4ccb2296-86c3-4486-9e1c-19934b0e8996", "_uuid": "480eac5c43f209c5f17fcf316fe8f6a669fbb0c9"}}, {"outputs": [], "execution_count": null, "cell_type": "code", "metadata": {"_cell_guid": "412c35e2-12f8-4061-8caa-ff9b6e306195", "collapsed": true, "_uuid": "fffbe8ba6ce7442debf31fddd1276833021a1c27"}, "source": ["from sklearn.linear_model import LogisticRegression\n", "from sklearn.model_selection import GridSearchCV, cross_val_score\n", "from sklearn.feature_selection import SelectKBest, f_regression\n", "from sklearn.preprocessing import MinMaxScaler\n", "from sklearn.pipeline import Pipeline\n", "from sklearn.model_selection import KFold\n", "from sklearn.metrics import classification_report\n", "from sklearn.metrics import confusion_matrix\n", "from sklearn.model_selection import train_test_split\n", "\n", "X_train, X_test, y_train, y_test = train_test_split(df.drop(['Class'],axis=1), df['Class'], test_size=0.2, random_state=0, stratify = df['Class'])\n", "print('Percentage of fraudulent in train ='+str(sum(y_train)/np.size(X_train,0)*100)+'%')\n", "print('Percentage of fraudulent in test ='+str(sum(y_test)/np.size(X_test,0)*100)+'%')\n", "\n", "#Now perform cross validation with grid search to find optimal parameter for the model.\n", "#Note we apply this on training data. \n", "kfold = KFold(n_splits=3, random_state=1)     #Create 10-CV split object    \n", "model=LogisticRegression()\n", "pipe_lm = Pipeline([('minmax',MinMaxScaler()), ('lmodel',model )])\n", "param_grid = [{'lmodel__C': [0.01, 0.1, 1.0]}]\n", "clf = GridSearchCV(pipe_lm, param_grid, cv=kfold, scoring='average_precision')                #Nested-3fold-CV\n", "outer_average_precission = cross_val_score(clf, X_train,y_train, scoring = 'average_precision', cv=kfold)              #Outer-10fold-CV\n", "print('3-fold CV average AUCPR: %.3f +/- %.3f' % ( outer_average_precission.mean(), outer_average_precission.std()))    \n", "\n", "#Fit the model with optimal parameter using all the training data to evaluate feature importance.                 \n", "clf.fit(X_train, y_train)\n", "lm_best_alpha=LogisticRegression(C=clf.best_params_['lmodel__C'], random_state=1)\n", "pipe_lm_best = Pipeline([('minmax',MinMaxScaler()),  ('lmodel',lm_best_alpha )])\n", "pipe_lm_best.fit(X_train,y_train)\n", "feat_labels = df.columns[0:30]\n", "importances=lm_best_alpha.coef_\n", "indices = np.argsort(importances)[::-1]\n", "importances=importances[0]\n", "plt.figure()\n", "plt.title('Logistic Regression coefficients')\n", "plt.bar(range(np.size(feat_labels)),importances[indices[0]],color='lightblue',align='center')\n", "plt.xticks(range(np.size(feat_labels)),feat_labels[indices[0]], rotation=90)\n", "plt.tight_layout()\n", "plt.show()"]}, {"source": ["We can see on the results above that the logistic regression coefficients for V4, V11, V13 and V12 are the highest (in absolute value). This means that they are more important for this model. Note this is not surprising since we saw in the EDA that these were the features that gave most separable distributions between fraudulent and non-fraudulent. \n", "Finally, let's evaluate with more detail the classifier performance (AUPRC + PR curve + Precision + Recall) to see what should we do about the unbalance issue"], "cell_type": "markdown", "metadata": {"_cell_guid": "4742e9b9-7f0e-486d-add1-f7c50e258097", "collapsed": true, "_uuid": "9a511d9f33b954e30b8e08c3cc222d6f73f9d5b8"}}, {"outputs": [], "execution_count": null, "cell_type": "code", "metadata": {"_cell_guid": "87ebc6a0-d969-44c3-803d-cac7d5e866d2", "collapsed": true, "_uuid": "76cbb806cbffdf7e29db0405fde34ba94277bbbf"}, "source": ["from sklearn.metrics import average_precision_score\n", "from sklearn.metrics import precision_recall_curve\n", "\n", "means = clf.cv_results_['mean_test_score']\n", "stds = clf.cv_results_['std_test_score']\n", "for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n", "    print(\"%0.3f (+/-%0.03f) for %r\"\n", "          % (mean, std * 2, params))\n", "print(clf.best_params_)\n", "clf.best_estimator_.fit(X_train,y_train)\n", "y_pred=clf.best_estimator_.predict(X_test)\n", "print('Classification report')\n", "print(classification_report(y_test,y_pred))\n", "print('Test AUCPR = ' + str(average_precision_score(y_test, y_pred)))\n", "\n", "precision, recall, _ = precision_recall_curve(y_test, y_pred)\n", "plt.step(recall, precision, color='b', alpha=0.2, where='post')\n", "plt.fill_between(recall, precision, step='post', alpha=0.2, color='b')\n", "plt.xlabel('Recall')\n", "plt.ylabel('Precision')\n", "plt.ylim([0.0, 1.05])\n", "plt.xlim([0.0, 1.0])\n", "plt.title('2-class Precision-Recall curve: AUC={0:0.2f}'.format( average_precision_score(y_test, y_pred)))"]}, {"outputs": [], "execution_count": null, "cell_type": "code", "metadata": {"_cell_guid": "88d022e4-2c24-4605-bd57-71006e5f5cab", "collapsed": true, "_uuid": "c050e95fb761ffad50b7178033527e72ef68cdfa"}, "source": ["#STILL WORKING ON THIS KERNEL. CHECK FOR UPDATES"]}], "nbformat": 4, "metadata": {"kernelspec": {"language": "python", "display_name": "Python 3", "name": "python3"}, "_is_fork": false, "_change_revision": 0, "language_info": {"codemirror_mode": {"version": 3, "name": "ipython"}, "file_extension": ".py", "pygments_lexer": "ipython3", "mimetype": "text/x-python", "nbconvert_exporter": "python", "version": "3.6.3", "name": "python"}}, "nbformat_minor": 1}