{"cells": [{"source": ["# This Python 3 environment comes with many helpful analytics libraries installed\n", "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n", "# For example, here's several helpful packages to load in \n", "#reference:\n", "#https://www.kaggle.com/sudalairajkumar/simple-exploration-notebook-zillow-prize/notebook\n", "import numpy as np # linear algebra\n", "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n", "import matplotlib.pyplot as plt\n", "import seaborn as sns\n", "color = sns.color_palette()\n", "\n", "%matplotlib inline\n", "\n", "# Input data files are available in the \"../input/\" directory.\n", "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n", "\n", "from subprocess import check_output\n", "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n", "\n", "# Any results you write to the current directory are saved as output."], "outputs": [], "execution_count": null, "metadata": {"_cell_guid": "142265d9-eb75-4f61-8ca2-0ea4a951784f", "_uuid": "1ed71497a84ad2ce4fde665256692a001e115434"}, "cell_type": "code"}, {"source": ["train_df = pd.read_csv(\"../input/train_2016_v2.csv\", parse_dates=[\"transactiondate\"])\n", "train_df.shape"], "outputs": [], "execution_count": null, "metadata": {"_cell_guid": "89953174-8876-4b6d-8191-4c30e975c553", "_uuid": "0893414b8556865f4e1ed763947e03961c41204d"}, "cell_type": "code"}, {"source": ["train_df.head()"], "outputs": [], "execution_count": null, "metadata": {"_cell_guid": "dbd5e805-75cf-408f-958d-abb9143cd79e", "_uuid": "03a171068a9dc23b5a52f3650cb87e0154da8287"}, "cell_type": "code"}, {"source": ["train_df.dtypes"], "outputs": [], "execution_count": null, "metadata": {"_cell_guid": "9f1d8c10-2ca7-48ca-bb92-7758dd2a55ad", "_uuid": "46e4513cf20d578e7b4994462068ab55d85dc36a"}, "cell_type": "code"}, {"source": ["**Logerror:\n", "Target variable for this competition is \"logerror\" field. So let us do some analysis on this field first.**"], "metadata": {"_cell_guid": "a124c4a5-0d98-47e9-a113-446a7bc9eb39", "_uuid": "243c9bd1e1a9345117d632daad6624d81319a882", "collapsed": true}, "cell_type": "markdown"}, {"source": ["plt.figure(figsize=(8,6))\n", "plt.scatter(range(train_df.shape[0]), np.sort(train_df.logerror.values))\n", "plt.xlabel('index', fontsize=12)\n", "plt.ylabel('logerror', fontsize=12)\n", "plt.show()"], "outputs": [], "execution_count": null, "metadata": {"_cell_guid": "058bc426-e6ae-45d2-ae1c-f070ed786ed8", "_uuid": "2e2029e9da8901b7a0680b071990bc231680ccdb"}, "cell_type": "code"}, {"source": ["**This looks nice with some outliers at both the ends.!**"], "metadata": {}, "cell_type": "markdown"}, {"source": ["**these outliers are the most interesting datapoints.  It looks like this is where the Zillow algorithm fails, so if we can predict these failures it would mean a huge score increase.****"], "metadata": {}, "cell_type": "markdown"}, {"source": ["#replacing outliers with the values for the 1st and 99th percentile\n", "ulimit = np.percentile(train_df.logerror.values, 99)\n", "llimit = np.percentile(train_df.logerror.values, 1)\n", "train_df['logerror'].loc[train_df['logerror']>ulimit] = ulimit\n", "train_df['logerror'].loc[train_df['logerror']<llimit] = llimit\n", "\n", "plt.figure(figsize=(12,8))\n", "sns.distplot(train_df.logerror.values, bins=50, kde=False)\n", "plt.xlabel('logerror', fontsize=12)\n", "plt.show()"], "outputs": [], "execution_count": null, "metadata": {}, "cell_type": "code"}, {"source": ["#let us explore the date field. Let us first check the number of transactions in each month.\n", "train_df['transaction_month'] = train_df['transactiondate'].dt.month\n", "cnt_srs = train_df['transaction_month'].value_counts()\n", "plt.figure(figsize=(12,6))\n", "sns.barplot(cnt_srs.index, cnt_srs.values, alpha=0.8, color=color[3])\n", "plt.xticks(rotation='vertical')\n", "plt.xlabel('Month of transaction', fontsize=12)\n", "plt.ylabel('Number of Occurrences', fontsize=12)\n", "plt.show()"], "outputs": [], "execution_count": null, "metadata": {}, "cell_type": "code"}, {"source": ["#let's see parcelid\n", "(train_df['parcelid'].value_counts().reset_index())['parcelid'].value_counts()"], "outputs": [], "execution_count": null, "metadata": {}, "cell_type": "code"}, {"source": ["**So most of the parcel ids are appearing only once in the dataset.**"], "metadata": {}, "cell_type": "markdown"}, {"source": ["#Now let us explore the properties_2016 file\n", "prop_df = pd.read_csv(\"../input/properties_2016.csv\",low_memory=False)\n", "prop_df.shape"], "outputs": [], "execution_count": null, "metadata": {}, "cell_type": "code"}, {"source": ["prop_df.head()"], "outputs": [], "execution_count": null, "metadata": {}, "cell_type": "code"}, {"source": ["#check missing data in properties\n", "missing_df = prop_df.isnull().sum(axis=0).reset_index()\n", "missing_df.columns = ['column_name', 'missing_count']\n", "missing_df = missing_df.loc[missing_df['missing_count']>0]\n", "missing_df = missing_df.sort_values(by='missing_count')\n", "\n", "ind = np.arange(missing_df.shape[0])\n", "width = 0.9\n", "fig, ax = plt.subplots(figsize=(12,18))\n", "rects = ax.barh(ind, missing_df.missing_count.values, color='orange')\n", "ax.set_yticks(ind)\n", "ax.set_yticklabels(missing_df.column_name.values, rotation='horizontal')\n", "ax.set_xlabel(\"Count of missing values\")\n", "ax.set_title(\"Number of missing values in each column\")\n", "plt.show()"], "outputs": [], "execution_count": null, "metadata": {}, "cell_type": "code"}, {"source": ["#Let us explore the latitude and longitude variable to begin with\n", "plt.figure(figsize=(12,12))\n", "sns.jointplot(x=prop_df.latitude.values, y=prop_df.longitude.values, size=10)\n", "plt.ylabel('Longitude', fontsize=12)\n", "plt.xlabel('Latitude', fontsize=12)\n", "plt.show()"], "outputs": [], "execution_count": null, "metadata": {}, "cell_type": "code"}, {"source": ["#So let us merge the two files and then carry out our analysis.\n", "train_df = pd.merge(train_df, prop_df, on='parcelid', how='left')\n", "train_df.head()"], "outputs": [], "execution_count": null, "metadata": {}, "cell_type": "code"}, {"source": ["#check the dtypes of different types of variable\n", "pd.options.display.max_rows = 65\n", "dtype_df = train_df.dtypes.reset_index()\n", "dtype_df.columns = [\"Count\", \"Column Type\"]\n", "dtype_df"], "outputs": [], "execution_count": null, "metadata": {}, "cell_type": "code"}, {"source": ["#count the dtypes\n", "dtype_df.groupby(\"Column Type\").aggregate('count').reset_index()"], "outputs": [], "execution_count": null, "metadata": {}, "cell_type": "code"}, {"source": ["#check the number of Nulls in this new merged dataset\n", "missing_df = train_df.isnull().sum(axis=0).reset_index()\n", "missing_df.columns = ['column_name', 'missing_count']\n", "missing_df['missing_ratio'] = missing_df['missing_count'] / train_df.shape[0]\n", "missing_df.loc[missing_df['missing_ratio']>0.999]"], "outputs": [], "execution_count": null, "metadata": {}, "cell_type": "code"}, {"source": ["**let us first take the 'float' variables alone and then get the correlation with the target variable to see how they are related.**"], "metadata": {}, "cell_type": "markdown"}, {"source": ["# Let us just impute the missing values with mean values to compute correlation coefficients\n", "mean_values = train_df.mean(axis=0)\n", "train_df_new = train_df.fillna(mean_values, inplace=True)\n", "\n", "# Now let us look at the correlation coefficient of each of these variables\n", "x_cols = [col for col in train_df_new.columns if col not in ['logerror'] if train_df_new[col].dtype=='float64']\n", "\n", "labels = []\n", "values = []\n", "for col in x_cols:\n", "    labels.append(col)\n", "    values.append(np.corrcoef(train_df_new[col].values, train_df_new.logerror.values)[0,1])\n", "corr_df = pd.DataFrame({'col_labels':labels, 'corr_values':values})\n", "corr_df = corr_df.sort_values(by='corr_values')\n", "    \n", "ind = np.arange(len(labels))\n", "width = 0.9\n", "fig, ax = plt.subplots(figsize=(12,40))\n", "rects = ax.barh(ind, np.array(corr_df.corr_values.values), color='y')\n", "ax.set_yticks(ind)\n", "ax.set_yticklabels(corr_df.col_labels.values, rotation='horizontal')\n", "ax.set_xlabel(\"Correlation coefficient\")\n", "ax.set_title(\"Correlation coefficient of the variables\")\n", "#autolabel(rects)\n", "plt.show()"], "outputs": [], "execution_count": null, "metadata": {}, "cell_type": "code"}, {"source": ["**There are few variables at the top of this graph without any correlation values. I guess they have only one unique value and hence no correlation value. Let us confirm the same.**"], "metadata": {}, "cell_type": "markdown"}, {"source": ["corr_zero_cols = ['assessmentyear', 'storytypeid', 'pooltypeid2', 'pooltypeid7', 'pooltypeid10', 'poolcnt', 'decktypeid', 'buildingclasstypeid']\n", "for col in corr_zero_cols:\n", "    print(col, len(train_df_new[col].unique()))"], "outputs": [], "execution_count": null, "metadata": {}, "cell_type": "code"}, {"source": ["#Let us take the variables with high correlation values and then do some analysis on them\n", "corr_df_sel = corr_df.loc[(corr_df['corr_values']>0.02) | (corr_df['corr_values'] < -0.01)]\n", "corr_df_sel"], "outputs": [], "execution_count": null, "metadata": {}, "cell_type": "code"}, {"source": ["cols_to_use = corr_df_sel.col_labels.tolist()\n", "\n", "temp_df = train_df[cols_to_use]\n", "corrmat = temp_df.corr(method='spearman')\n", "f, ax = plt.subplots(figsize=(8, 8))\n", "\n", "# Draw the heatmap using seaborn\n", "sns.heatmap(corrmat, vmax=1., square=True)\n", "plt.title(\"Important variables correlation map\", fontsize=15)\n", "plt.show()"], "outputs": [], "execution_count": null, "metadata": {}, "cell_type": "code"}, {"source": ["#Let us seee how the finished square feet 12 varies with the log error\n", "col = \"finishedsquarefeet12\"\n", "ulimit = np.percentile(train_df[col].values, 99.5)\n", "llimit = np.percentile(train_df[col].values, 0.5)\n", "train_df[col].loc[train_df[col]>ulimit] = ulimit\n", "train_df[col].loc[train_df[col]<llimit] = llimit\n", "\n", "plt.figure(figsize=(12,12))\n", "sns.jointplot(x=train_df.finishedsquarefeet12.values, y=train_df.logerror.values, size=10, color=color[4])\n", "plt.ylabel('Log Error', fontsize=12)\n", "plt.xlabel('Finished Square Feet 12', fontsize=12)\n", "plt.title(\"Finished square feet 12 Vs Log error\", fontsize=15)\n", "plt.show()"], "outputs": [], "execution_count": null, "metadata": {}, "cell_type": "code"}, {"source": ["**Seems the range of logerror narrows down with increase in finished square feet 12 variable. Probably larger houses are easy to predict**"], "metadata": {}, "cell_type": "markdown"}, {"source": ["#Calculated finished square feet:\n", "col = \"calculatedfinishedsquarefeet\"\n", "ulimit = np.percentile(train_df[col].values, 99.5)\n", "llimit = np.percentile(train_df[col].values, 0.5)\n", "train_df[col].loc[train_df[col]>ulimit] = ulimit\n", "train_df[col].loc[train_df[col]<llimit] = llimit\n", "\n", "plt.figure(figsize=(12,12))\n", "sns.jointplot(x=train_df.calculatedfinishedsquarefeet.values, y=train_df.logerror.values, size=10, color=color[5])\n", "plt.ylabel('Log Error', fontsize=12)\n", "plt.xlabel('Calculated finished square feet', fontsize=12)\n", "plt.title(\"Calculated finished square feet Vs Log error\", fontsize=15)\n", "plt.show()"], "outputs": [], "execution_count": null, "metadata": {}, "cell_type": "code"}, {"source": ["#Bathroom Count:\n", "plt.figure(figsize=(12,8))\n", "sns.countplot(x=\"bathroomcnt\", data=train_df)\n", "plt.ylabel('Count', fontsize=12)\n", "plt.xlabel('Bathroom', fontsize=12)\n", "plt.xticks(rotation='vertical')\n", "plt.title(\"Frequency of Bathroom count\", fontsize=15)\n", "plt.show()"], "outputs": [], "execution_count": null, "metadata": {}, "cell_type": "code"}, {"source": ["#check how the log error changes based on this\n", "plt.figure(figsize=(12,8))\n", "sns.boxplot(x=\"bathroomcnt\", y=\"logerror\", data=train_df)\n", "plt.ylabel('Log error', fontsize=12)\n", "plt.xlabel('Bathroom Count', fontsize=12)\n", "plt.xticks(rotation='vertical')\n", "plt.title(\"How log error changes with bathroom count?\", fontsize=15)\n", "plt.show()"], "outputs": [], "execution_count": null, "metadata": {}, "cell_type": "code"}, {"source": ["#Bedroom count:\n", "plt.figure(figsize=(12,8))\n", "sns.countplot(x=\"bedroomcnt\", data=train_df)\n", "plt.ylabel('Frequency', fontsize=12)\n", "plt.xlabel('Bedroom Count', fontsize=12)\n", "plt.xticks(rotation='vertical')\n", "plt.title(\"Frequency of Bedroom count\", fontsize=15)\n", "plt.show()"], "outputs": [], "execution_count": null, "metadata": {}, "cell_type": "code"}, {"source": ["train_df['bedroomcnt'].loc[train_df['bedroomcnt']>7] = 7\n", "plt.figure(figsize=(12,8))\n", "sns.violinplot(x='bedroomcnt', y='logerror', data=train_df)\n", "plt.xlabel('Bedroom count', fontsize=12)\n", "plt.ylabel('Log Error', fontsize=12)\n", "plt.show()"], "outputs": [], "execution_count": null, "metadata": {}, "cell_type": "code"}, {"source": ["col = \"taxamount\"\n", "ulimit = np.percentile(train_df[col].values, 99.5)\n", "llimit = np.percentile(train_df[col].values, 0.5)\n", "train_df[col].loc[train_df[col]>ulimit] = ulimit\n", "train_df[col].loc[train_df[col]<llimit] = llimit\n", "\n", "plt.figure(figsize=(12,12))\n", "sns.jointplot(x=train_df['taxamount'].values, y=train_df['logerror'].values, size=10, color='g')\n", "plt.ylabel('Log Error', fontsize=12)\n", "plt.xlabel('Tax Amount', fontsize=12)\n", "plt.title(\"Tax Amount Vs Log error\", fontsize=15)\n", "plt.show()"], "outputs": [], "execution_count": null, "metadata": {}, "cell_type": "code"}, {"source": ["#YearBuilt:\n", "from ggplot import *\n", "ggplot(aes(x='yearbuilt', y='logerror'), data=train_df) + \\\n", "    geom_point(color='steelblue', size=1) + \\\n", "    stat_smooth()"], "outputs": [], "execution_count": null, "metadata": {}, "cell_type": "code"}, {"source": ["#let us see how the logerror varies with respect to latitude and longitude\n", "ggplot(aes(x='latitude', y='longitude', color='logerror'), data=train_df) + \\\n", "    geom_point() + \\\n", "    scale_color_gradient(low = 'red', high = 'blue')"], "outputs": [], "execution_count": null, "metadata": {}, "cell_type": "code"}, {"source": ["#Let us take the variables with highest positive correlation and highest negative correlation to see if we can see some visible patterns\n", "ggplot(aes(x='finishedsquarefeet12', y='taxamount', color='logerror'), data=train_df) + \\\n", "    geom_point(alpha=0.7) + \\\n", "    scale_color_gradient(low = 'pink', high = 'blue')"], "outputs": [], "execution_count": null, "metadata": {}, "cell_type": "code"}, {"source": ["**Now let us build a non-linear model to get the important variables by building Extra Trees model**"], "metadata": {}, "cell_type": "markdown"}, {"source": ["train_y = train_df['logerror'].values\n", "cat_cols = [\"hashottuborspa\", \"propertycountylandusecode\", \"propertyzoningdesc\", \"fireplaceflag\", \"taxdelinquencyflag\"]\n", "train_df = train_df.drop(['parcelid', 'logerror', 'transactiondate', 'transaction_month']+cat_cols, axis=1)\n", "feat_names = train_df.columns.values\n", "\n", "from sklearn import ensemble\n", "model = ensemble.ExtraTreesRegressor(n_estimators=25, max_depth=30, max_features=0.3, n_jobs=-1, random_state=0)\n", "model.fit(train_df, train_y)\n", "\n", "## plot the importances ##\n", "importances = model.feature_importances_\n", "std = np.std([tree.feature_importances_ for tree in model.estimators_], axis=0)\n", "indices = np.argsort(importances)[::-1][:20]\n", "\n", "plt.figure(figsize=(12,12))\n", "plt.title(\"Feature importances\")\n", "plt.bar(range(len(indices)), importances[indices], color=\"r\", yerr=std[indices], align=\"center\")\n", "plt.xticks(range(len(indices)), feat_names[indices], rotation='vertical')\n", "plt.xlim([-1, len(indices)])\n", "plt.show()"], "outputs": [], "execution_count": null, "metadata": {}, "cell_type": "code"}, {"source": ["**Seems \"tax amount\" is the most importanct variable followed by \"structure tax value dollar count\" and \"land tax value dollor count\"**"], "metadata": {}, "cell_type": "markdown"}, {"source": ["import xgboost as xgb\n", "xgb_params = {\n", "    'eta': 0.05,\n", "    'max_depth': 8,\n", "    'subsample': 0.7,\n", "    'colsample_bytree': 0.7,\n", "    'objective': 'reg:linear',\n", "    'silent': 1,\n", "    'seed' : 0\n", "}\n", "dtrain = xgb.DMatrix(train_df, train_y, feature_names=train_df.columns.values)\n", "model = xgb.train(dict(xgb_params, silent=0), dtrain, num_boost_round=50)\n", "\n", "# plot the important features #\n", "fig, ax = plt.subplots(figsize=(12,18))\n", "xgb.plot_importance(model, max_num_features=50, height=0.8, ax=ax)\n", "plt.show()"], "outputs": [], "execution_count": null, "metadata": {}, "cell_type": "code"}, {"source": ["**Using xgboost, the important variables are: 'structured tax value dollar count' followed by 'latitude' and 'calculated finished square feet'**"], "metadata": {}, "cell_type": "markdown"}, {"source": ["# Parameters\n", "XGB_WEIGHT = 0.6500\n", "BASELINE_WEIGHT = 0.0056\n", "BASELINE_PRED = 0.0115\n", "\n", "import xgboost as xgb\n", "from sklearn.preprocessing import LabelEncoder\n", "import lightgbm as lgb\n", "import gc"], "outputs": [], "execution_count": null, "metadata": {"collapsed": true}, "cell_type": "code"}, {"source": ["Reference:https://www.kaggle.com/aharless/xgb-w-o-outliers-lgb-with-outliers-combined/code"], "metadata": {}, "cell_type": "markdown"}, {"source": ["print( \"\\nReading data from disk ...\")\n", "prop = pd.read_csv('../input/properties_2016.csv',low_memory=False)\n", "train = pd.read_csv(\"../input/train_2016_v2.csv\",low_memory=False)"], "outputs": [], "execution_count": null, "metadata": {}, "cell_type": "code"}, {"source": ["print( \"\\nProcessing data for LightGBM ...\" )\n", "for c, dtype in zip(prop.columns, prop.dtypes):\n", "    if dtype == np.float64:\n", "        prop[c] = prop[c].astype(np.float32)\n", "\n", "df_train = train.merge(prop, how='left', on='parcelid')\n", "df_train.fillna(df_train.median(),inplace = True)\n", "\n", "x_train = df_train.drop(['parcelid', 'logerror', 'transactiondate', 'propertyzoningdesc', \n", "                         'propertycountylandusecode', 'fireplacecnt', 'fireplaceflag'], axis=1)\n", "y_train = df_train['logerror'].values\n", "print(x_train.shape, y_train.shape)"], "outputs": [], "execution_count": null, "metadata": {}, "cell_type": "code"}, {"source": ["train_columns = x_train.columns\n", "\n", "for c in x_train.dtypes[x_train.dtypes == object].index.values:\n", "    x_train[c] = (x_train[c] == True)\n", "\n", "del df_train; gc.collect()\n", "\n", "x_train = x_train.values.astype(np.float32, copy=False)\n", "d_train = lgb.Dataset(x_train, label=y_train)"], "outputs": [], "execution_count": null, "metadata": {"collapsed": true}, "cell_type": "code"}, {"source": ["params = {}\n", "params['max_bin'] = 10\n", "params['learning_rate'] = 0.0021\n", "params['boosting_type'] = 'gbdt'\n", "params['objective'] = 'regression'\n", "params['metric'] = 'l1'          \n", "params['sub_feature'] = 0.5      \n", "params['bagging_fraction'] = 0.85 \n", "params['bagging_freq'] = 40\n", "params['num_leaves'] = 512        \n", "params['min_data'] = 500         \n", "params['min_hessian'] = 0.05     \n", "params['verbose'] = 0\n", "\n", "print(\"\\nFitting LightGBM model ...\")\n", "clf = lgb.train(params, d_train, 430)\n", "\n", "del d_train; gc.collect()\n", "del x_train; gc.collect()\n", "\n", "print(\"\\nPrepare for LightGBM prediction ...\")\n", "print(\"   Read sample file ...\")\n", "sample = pd.read_csv('../input/sample_submission.csv')\n", "print(\"   ...\")\n", "sample['parcelid'] = sample['ParcelId']\n", "print(\"   Merge with property data ...\")\n", "df_test = sample.merge(prop, on='parcelid', how='left')\n", "print(\"   ...\")\n", "del sample, prop; gc.collect()\n", "print(\"   ...\")\n", "x_test = df_test[train_columns]\n", "print(\"   ...\")\n", "del df_test; gc.collect()\n", "print(\"   Preparing x_test...\")\n", "for c in x_test.dtypes[x_test.dtypes == object].index.values:\n", "    x_test[c] = (x_test[c] == True)\n", "print(\"   ...\")\n", "x_test = x_test.values.astype(np.float32, copy=False)\n", "\n", "print(\"\\nStart LightGBM prediction ...\")\n", "# num_threads > 1 will predict very slow in kernal\n", "clf.reset_parameter({\"num_threads\":1})\n", "p_test = clf.predict(x_test)\n", "\n", "del x_test; gc.collect()\n", "\n", "print( \"\\nUnadjusted LightGBM predictions:\" )\n", "print( pd.DataFrame(p_test).head() )"], "outputs": [], "execution_count": null, "metadata": {}, "cell_type": "code"}, {"source": ["print( \"\\nPreparing results for write ...\" )\n", "y_pred=[]\n", "\n", "for i,predict in enumerate(p_test):\n", "    y_pred.append(str(round(predict,4)))\n", "y_pred=np.array(y_pred)\n", "\n", "properties = pd.read_csv('../input/properties_2016.csv')\n", "\n", "output = pd.DataFrame({'ParcelId': properties['parcelid'].astype(np.int32),\n", "        '201610': y_pred, '201611': y_pred, '201612': y_pred,\n", "        '201710': y_pred, '201711': y_pred, '201712': y_pred})\n", "# set col 'ParceID' to first col\n", "cols = output.columns.tolist()\n", "cols = cols[-1:] + cols[:-1]\n", "output = output[cols]\n", "from datetime import datetime\n", "\n", "print( \"\\nWriting results to disk ...\" )\n", "output.to_csv('sub{}.csv'.format(datetime.now().strftime('%Y%m%d_%H%M%S')), index=False)\n", "\n", "print( \"\\nFinished ...\" )"], "outputs": [], "execution_count": null, "metadata": {}, "cell_type": "code"}, {"source": [], "outputs": [], "execution_count": null, "metadata": {"collapsed": true}, "cell_type": "code"}], "nbformat_minor": 1, "nbformat": 4, "metadata": {"kernelspec": {"name": "python3", "language": "python", "display_name": "Python 3"}, "language_info": {"nbconvert_exporter": "python", "mimetype": "text/x-python", "version": "3.6.3", "file_extension": ".py", "name": "python", "pygments_lexer": "ipython3", "codemirror_mode": {"name": "ipython", "version": 3}}}}