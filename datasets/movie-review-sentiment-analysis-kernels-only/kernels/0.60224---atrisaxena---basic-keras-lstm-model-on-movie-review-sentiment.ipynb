{"cells":[{"metadata":{"_uuid":"7a95fee4f00c17b3b41f7f7215ffae877126599e"},"cell_type":"markdown","source":"**General information**\nIn this kernel I'll work with data from Movie Review Sentiment Analysis Playground Competition.\n\nThis is a very basic notebook for begineers who wants to get into LSTM keras. I have created the basic model after that you can improve your model."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"scrolled":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"486a30f24f0816cb0121bd8a6d087f261589aee4"},"cell_type":"markdown","source":"**Reading the dataset**"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"_kg_hide-input":false,"collapsed":true},"cell_type":"code","source":"train = pd.read_csv('../input/train.tsv', sep=\"\\t\")\ntest = pd.read_csv('../input/test.tsv', sep=\"\\t\")\nsub = pd.read_csv('../input/sampleSubmission.csv', sep=\",\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fc2ec2d3d85f1dbf28b915984427f2024a43d37a","scrolled":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"71639cdb54fd855befbf70f3e6dea14e64ef61cb"},"cell_type":"markdown","source":"**Importing libraries required**\nWe are using keras with tensorflow as backend."},{"metadata":{"trusted":true,"_uuid":"b88ad23bc8799aa8f9518774533e7310aa1423a1"},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing import text, sequence\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\nfrom sklearn.model_selection import train_test_split\nfrom keras.utils.np_utils import to_categorical\nimport re","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6ba69df47ace5fa32b2684fd755fc63db45579ce"},"cell_type":"markdown","source":"I am filtering the Phrases so only valid texts and words remain. Then, I define the number of max features as 2000 and use Tokenizer to vectorize and convert text into Sequences so the Network can deal with it as input."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"1f406a46ab39b70db73464db0b54959be57d97d7"},"cell_type":"code","source":"train['Phrase'] = train['Phrase'].apply(lambda x: x.lower())\ntrain['Phrase'] = train['Phrase'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x)))\n\ntest['Phrase'] = test['Phrase'].apply(lambda x: x.lower())\ntest['Phrase'] = test['Phrase'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"262bf3a36f2ecda964d930fab623aed0df74d651"},"cell_type":"code","source":"print(set(train.Sentiment)) #Output Labels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"dc230645420ab1e3bfa80690d8c502a6e26bd39e"},"cell_type":"code","source":"max_fatures = 2000\ntokenizer = Tokenizer(num_words=max_fatures,filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n                                   lower=True,split=' ')\ntokenizer.fit_on_texts(train['Phrase'].values)\nX = tokenizer.texts_to_sequences(train['Phrase'].values)\nX = pad_sequences(X)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e4d14fb00e31fe184b0851c65408892c9fe34b92"},"cell_type":"markdown","source":"**Creating the model**\nNext, I compose the LSTM Network. Note that embed_dim, lstm_out, batch_size, droupout_x variables are hyperparameters, their values are somehow intuitive, can be and must be played with in order to achieve good results. Please also note that I am using softmax as activation function. The reason is that our Network is using categorical crossentropy, and softmax is just the right activation method for that."},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"34f48af85f040cc9d3a7aaf4a075be2d88a46965"},"cell_type":"code","source":"embed_dim = 128\nlstm_out = 196\n\nmodel = Sequential()\nmodel.add(Embedding(max_fatures, embed_dim,input_length = X.shape[1]))\n#model.add(SpatialDropout1D(0.4))\nmodel.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\nmodel.add(Dense(5,activation='softmax'))\nmodel.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\nprint(model.summary())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1adf131e82bf8f0334fa886849a514c5485e063f"},"cell_type":"markdown","source":"Hereby I declare the train and test dataset."},{"metadata":{"trusted":true,"_uuid":"9ed0edc005062d355c4c23c80d498d1beab40e5f","scrolled":true},"cell_type":"code","source":"Y = pd.get_dummies(train['Sentiment']).values\nX_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.20, random_state = 42)\nprint(X_train.shape,Y_train.shape)\nprint(X_test.shape,Y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0dae1737ac81bf708b4c2a1907fb390f491ff678"},"cell_type":"markdown","source":"Here we train the Network."},{"metadata":{"trusted":true,"_uuid":"28bca6fae3a0378ee84b73e0804892c85ff8a024","scrolled":true},"cell_type":"code","source":"batch_size = 32\nmodel.fit(X_train, Y_train, epochs = 10, batch_size=batch_size, verbose = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6bb65d6b3f72c364b675c34c5cec55db6b90bdd2"},"cell_type":"code","source":"validation_size = 1200\n\nX_validate = X_test[-validation_size:]\nY_validate = Y_test[-validation_size:]\nX_test = X_test[:-validation_size]\nY_test = Y_test[:-validation_size]\nscore,acc = model.evaluate(X_test, Y_test, verbose = 2, batch_size = batch_size)\nprint(\"score: %.2f\" % (score))\nprint(\"acc: %.2f\" % (acc))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9b45329864c27016ddb97383f86eacc79dd9f307"},"cell_type":"code","source":"x_test = test['Phrase'].values\nprint(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"480c55aa93929ec914e7255bb26c48ee7ea9e5ce"},"cell_type":"code","source":"x_test_tokenized = tokenizer.texts_to_sequences(x_test)\nx_testing = sequence.pad_sequences(x_test_tokenized, maxlen=45)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"98e6f12ca283d5495710a5b55f11425462840944"},"cell_type":"code","source":"y_testing = model.predict(x_testing, verbose = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"9f0c700a2f5ecca1065740aae655913dafec6fb0"},"cell_type":"code","source":"predictions = np.round(np.argmax(y_testing, axis=1)).astype(int)\nsub['Sentiment'] = predictions\nsub.to_csv(\"submission_result.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5f0329b3f4ecd8368facea22df4eeb511a713713"},"cell_type":"markdown","source":"For improvement you can make a bigger network model. And also use the whole dataset and submit the prediction, you will get good accuracy. \nIf you like Applause. Thank you."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}