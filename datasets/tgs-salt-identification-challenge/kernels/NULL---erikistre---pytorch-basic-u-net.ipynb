{"cells":[{"metadata":{"trusted":true,"collapsed":true,"_uuid":"b39ab12e7b7eba628dec9ce00a4f2a27a705c7b6"},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport os\nfrom glob import glob\nimport sys\nimport random\n\nfrom tqdm import tqdm_notebook\nfrom skimage.io import imread, imshow\nfrom skimage.transform import resize\nfrom sklearn.metrics import jaccard_similarity_score\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.datasets as dsets\nfrom torch.autograd import Variable","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5ee18d5936d6a2607d33abd504e6311c0a921415"},"cell_type":"markdown","source":"This kernel demonstrates a quick and naive implementation of a U-Net in Pytorch, trained on a GPU. Enjoy!"},{"metadata":{"_uuid":"fb1e790282ddfcdc8278bf64f1e81a33be457f80"},"cell_type":"markdown","source":"# Image Preparation"},{"metadata":{"_uuid":"2fa28f12f7609ffc6d1297f6a782019f694367c7"},"cell_type":"markdown","source":"The following image preparation was taken almost verbatim from another excellent kernel of the TGS Salt Identification challenge: https://www.kaggle.com/jesperdramsch/intro-to-seismic-salt-and-how-to-geophysics. I take no credit for it.  "},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"beedb640f57af97717385ff8ec99cc142c7d9e93"},"cell_type":"code","source":"# Set some parameters# Set s \nim_width = 128\nim_height = 128\nim_chan = 1\npath_train = '../input/train'\npath_test = '../input/test'\n\ntrain_path_images = os.path.abspath(path_train + \"/images/\")\ntrain_path_masks = os.path.abspath(path_train + \"/masks/\")\n\ntest_path_images = os.path.abspath(path_test + \"/images/\")\ntest_path_masks = os.path.abspath(path_test + \"/masks/\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"29fc52ae42d8aeb76930c3b82cc15e6acd634f12"},"cell_type":"code","source":"train_path_images_list = glob(os.path.join(train_path_images, \"*.png\"))\ntrain_path_masks_list = glob(os.path.join(train_path_masks, \"*.png\"))\ntest_path_images_list = glob(os.path.join(test_path_images, \"*.png\"))\ntest_path_masks_list = glob(os.path.join(test_path_masks, \"*.png\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e670c59db84bbf3d86c8e2578981c0bdaae7e515"},"cell_type":"code","source":"ids= ['1f1cc6b3a4','5b7c160d0d','6c40978ddf','7dfdf6eeb8','7e5a6e5013']\nplt.figure(figsize=(20,10))\nfor j, img_name in enumerate(ids):\n    q = j+1\n    img = imread(train_path_images + \"/\" + img_name + '.png')\n    img_mask = imread(train_path_masks + \"/\" + img_name + '.png')\n    \n    plt.subplot(1,2*(1+len(ids)),q*2-1)\n    plt.imshow(img)\n    plt.subplot(1,2*(1+len(ids)),q*2)\n    plt.imshow(img_mask)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"537a4e8dfa5f91b51592fbf824b0e38bd970cf14"},"cell_type":"code","source":"train_ids = next(os.walk(train_path_images))[2]\ntest_ids = next(os.walk(test_path_images))[2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ebb320590dd3cf378cf5a2c0a16936dc2892dc00","scrolled":true},"cell_type":"code","source":"# Get and resize train images and masks\nX_train = np.zeros((len(train_ids), im_height, im_width, im_chan), dtype=np.uint8)\nY_train = np.zeros((len(train_ids), im_height, im_width, 1), dtype=np.bool_)\nprint('Getting and resizing train images and masks ... ')\nsys.stdout.flush()\nfor n, id_ in tqdm_notebook(enumerate(train_ids), total=len(train_ids)):\n    img = imread(path_train + '/images/' + id_)\n    x = resize(img, (128, 128, 1), mode='constant', preserve_range=True)\n    X_train[n] = x\n    mask = imread(path_train + '/masks/' + id_)\n    Y_train[n] = resize(mask, (128, 128, 1), \n                        mode='constant', \n                        preserve_range=True)\n\nprint('Done!')","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"46addeafe206c4db101c6b7db1c42495e9d8a6ec"},"cell_type":"code","source":"# Check if training data looks all right\nix = random.randint(0, len(train_ids))\nplt.imshow(np.dstack((X_train[ix],X_train[ix],X_train[ix])))\nplt.show()\ntmp = np.squeeze(Y_train[ix]).astype(np.float32)\nplt.imshow(np.dstack((tmp,tmp,tmp)))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a2299409f91d264e71476650a63ad33c4b30ce65"},"cell_type":"markdown","source":"# Prepare Images for Pytorch"},{"metadata":{"_uuid":"3e82ae1e260b6ec62b162440b377eb99d28dc4fe"},"cell_type":"markdown","source":"The following is what allows us to easily use our with Pytorch. We create a class with the following methods which then allows us to use a DataLoader."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"f3adcb99bace1da1a7de566bd56e4460bb39d040"},"cell_type":"code","source":"# https://stackoverflow.com/questions/50052295/how-do-you-load-images-into-pytorch-dataloader\nclass saltIDDataset(torch.utils.data.Dataset):\n\n    def __init__(self,preprocessed_images,train=True, preprocessed_masks=None):\n        \"\"\"\n        Args:\n            text_file(string): path to text file\n            root_dir(string): directory with all train images\n        \"\"\"\n        self.train = train\n        self.images = preprocessed_images\n        if self.train:\n            self.masks = preprocessed_masks\n\n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, idx):\n        image = self.images[idx]\n        mask = None\n        if self.train:\n            mask = self.masks[idx]\n        return (image, mask)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"37479278d3363425cbbb2d25f7f3a4d921981343"},"cell_type":"code","source":"X_train_shaped = X_train.reshape(-1, 1, 128, 128)/255\nY_train_shaped = Y_train.reshape(-1, 1, 128, 128)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"fa161620d0e907dc5edfac0a73575658cfeac2a2"},"cell_type":"code","source":"X_train_shaped = X_train_shaped.astype(np.float32)\nY_train_shaped = Y_train_shaped.astype(np.float32)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2d613106c66bd63d5be01ee8878321ff3a14c251"},"cell_type":"markdown","source":"We set a random seed for reproducibility."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"d0b66fcd005e4070d5eb1a8968eba6bf6855653c"},"cell_type":"code","source":"torch.cuda.manual_seed_all(4200)\nnp.random.seed(133700)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"6c2b36a1ba7815fbdd0179e2292ec568326bd3cc"},"cell_type":"code","source":"indices = list(range(len(X_train_shaped)))\nnp.random.shuffle(indices)\n\nval_size = 1/10\nsplit = np.int_(np.floor(val_size * len(X_train_shaped)))\n\ntrain_idxs = indices[split:]\nval_idxs = indices[:split]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2282e6f4e73e935cf78095623df1201344c995da"},"cell_type":"markdown","source":"You may need to tweak your batch_size based on how much memory you have on your GPU."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"8b5d48c4c9bf942642ea9c07af5c82737cb198da"},"cell_type":"code","source":"salt_ID_dataset_train = saltIDDataset(X_train_shaped[train_idxs], \n                                      train=True, \n                                      preprocessed_masks=Y_train_shaped[train_idxs])\nsalt_ID_dataset_val = saltIDDataset(X_train_shaped[val_idxs], \n                                      train=True, \n                                      preprocessed_masks=Y_train_shaped[val_idxs])\n\nbatch_size = 16\n\ntrain_loader = torch.utils.data.DataLoader(dataset=salt_ID_dataset_train, \n                                           batch_size=batch_size, \n                                           shuffle=True)\n\nval_loader = torch.utils.data.DataLoader(dataset=salt_ID_dataset_val, \n                                           batch_size=batch_size, \n                                           shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b6c433dc8bd61454583cc840c641beca9e283c5e"},"cell_type":"markdown","source":"# Define U-Net Model"},{"metadata":{"_uuid":"e3a7512b8e70a4be3d722adc3b1d9e63fc3508b6"},"cell_type":"markdown","source":"Here's the meat of the kernel, where we define our U-Net architecture. See also this excellent kernel from an older challenge: https://www.kaggle.com/mlagunas/naive-unet-with-pytorch-tensorboard-logging."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"9b191779e4ff3ec17ebee2767833d34b273b3515"},"cell_type":"code","source":"class double_conv(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n        super(double_conv, self).__init__()\n        self.conv = nn.Sequential(\n                    nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size,\n                              stride=stride, padding=padding),\n                    nn.BatchNorm2d(out_channels),\n                    nn.ReLU(inplace=True),\n                    nn.Conv2d(out_channels, out_channels, kernel_size=kernel_size,\n                              stride=stride, padding=padding),\n                    nn.BatchNorm2d(out_channels),\n                    nn.ReLU(inplace=True))\n        \n    def forward(self, x):\n        x = self.conv(x)\n        return x\n        \nstart_fm = 16\n\nclass Unet(nn.Module):\n    \n    def __init__(self):\n        super(Unet, self).__init__()\n        \n        # Input 128x128x1\n        \n        #Contracting Path\n        \n        #(Double) Convolution 1        \n        self.double_conv1 = double_conv(1, start_fm, 3, 1, 1)\n        #Max Pooling 1\n        self.maxpool1 = nn.MaxPool2d(kernel_size=2)\n        \n        #Convolution 2\n        self.double_conv2 = double_conv(start_fm, start_fm * 2, 3, 1, 1)\n        #Max Pooling 2\n        self.maxpool2 = nn.MaxPool2d(kernel_size=2)\n        \n        #Convolution 3\n        self.double_conv3 = double_conv(start_fm * 2, start_fm * 4, 3, 1, 1)\n        #Max Pooling 3\n        self.maxpool3 = nn.MaxPool2d(kernel_size=2)\n        \n        #Convolution 4\n        self.double_conv4 = double_conv(start_fm * 4, start_fm * 8, 3, 1, 1)\n        #Max Pooling 4\n        self.maxpool4 = nn.MaxPool2d(kernel_size=2)\n        \n        #Convolution 5\n        self.double_conv5 = double_conv(start_fm * 8, start_fm * 16, 3, 1, 1)\n        \n        #Transposed Convolution 4\n        self.t_conv4 = nn.ConvTranspose2d(start_fm * 16, start_fm * 8, 2, 2)\n        # Expanding Path Convolution 4 \n        self.ex_double_conv4 = double_conv(start_fm * 16, start_fm * 8, 3, 1, 1)\n        \n        #Transposed Convolution 3\n        self.t_conv3 = nn.ConvTranspose2d(start_fm * 8, start_fm * 4, 2, 2)\n        #Convolution 3\n        self.ex_double_conv3 = double_conv(start_fm * 8, start_fm * 4, 3, 1, 1)\n        \n        #Transposed Convolution 2\n        self.t_conv2 = nn.ConvTranspose2d(start_fm * 4, start_fm * 2, 2, 2)\n        #Convolution 2\n        self.ex_double_conv2 = double_conv(start_fm * 4, start_fm * 2, 3, 1, 1)\n        \n        #Transposed Convolution 1\n        self.t_conv1 = nn.ConvTranspose2d(start_fm * 2, start_fm, 2, 2)\n        #Convolution 1\n        self.ex_double_conv1 = double_conv(start_fm * 2, start_fm, 3, 1, 1)\n        \n        # One by One Conv\n        self.one_by_one = nn.Conv2d(start_fm, 1, 1, 1, 0)\n        #self.final_act = nn.Sigmoid()\n        \n        \n    def forward(self, inputs):\n        # Contracting Path\n        conv1 = self.double_conv1(inputs)\n        maxpool1 = self.maxpool1(conv1)\n\n        conv2 = self.double_conv2(maxpool1)\n        maxpool2 = self.maxpool2(conv2)\n\n        conv3 = self.double_conv3(maxpool2)\n        maxpool3 = self.maxpool3(conv3)\n\n        conv4 = self.double_conv4(maxpool3)\n        maxpool4 = self.maxpool4(conv4)\n            \n        # Bottom\n        conv5 = self.double_conv5(maxpool4)\n        \n        # Expanding Path\n        t_conv4 = self.t_conv4(conv5)\n        cat4 = torch.cat([conv4 ,t_conv4], 1)\n        ex_conv4 = self.ex_double_conv4(cat4)\n        \n        t_conv3 = self.t_conv3(ex_conv4)\n        cat3 = torch.cat([conv3 ,t_conv3], 1)\n        ex_conv3 = self.ex_double_conv3(cat3)\n\n        t_conv2 = self.t_conv2(ex_conv3)\n        cat2 = torch.cat([conv2 ,t_conv2], 1)\n        ex_conv2 = self.ex_double_conv2(cat2)\n        \n        t_conv1 = self.t_conv1(ex_conv2)\n        cat1 = torch.cat([conv1 ,t_conv1], 1)\n        ex_conv1 = self.ex_double_conv1(cat1)\n        \n        one_by_one = self.one_by_one(ex_conv1)\n        \n        return one_by_one","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4ae043fa624089989d91d70c861d75b671605ea8"},"cell_type":"markdown","source":"We define a BCEWithLogitsLoss since we're comparing pixel by pixel. In addition, we didn't include a final sigmoid activation as this loss function includes a sigmoid for us."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"0dcff9b9f9b0647498e86b951e2a7a68fc15b60d"},"cell_type":"code","source":"model = Unet()\nmodel.cuda();\n\ncriterion = nn.BCEWithLogitsLoss()\n\nlearning_rate = 1e-3\n\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"aee5ba20f7c11b4f5b74c466d780f7994587bff6"},"cell_type":"code","source":"mean_train_losses = []\nmean_val_losses = []\nfor epoch in range(12):\n    train_losses = []\n    val_losses = []\n    for images, masks in train_loader:        \n        images = Variable(images.cuda())\n        masks = Variable(masks.cuda())\n        \n        outputs = model(images)        \n        \n        loss = criterion(outputs, masks)\n        train_losses.append(loss.data)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n    for images, masks in val_loader:\n        images = Variable(images.cuda())\n        masks = Variable(masks.cuda())\n        \n        outputs = model(images)\n        loss = criterion(outputs, masks)\n        val_losses.append(loss.data)\n    \n    mean_train_losses.append(np.mean(train_losses))\n    mean_val_losses.append(np.mean(val_losses))\n    # Print Loss\n    print('Epoch: {}. Train Loss: {}. Val Loss: {}'.format(epoch+1, np.mean(train_losses), np.mean(val_losses)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3365a08f4096383eb21ef58dcff09443b4bac8cc"},"cell_type":"markdown","source":"We note that around 11-13 epochs is when we start to worry about overfitting to our training data as we see a rise in our validation loss."},{"metadata":{"trusted":true,"_uuid":"1d6cc863b8f502180d3a64eb9e048f0e04836dac"},"cell_type":"code","source":"train_loss_series = pd.Series(mean_train_losses)\nval_loss_series = pd.Series(mean_val_losses)\ntrain_loss_series.plot(label=\"train\")\nval_loss_series.plot(label=\"validation\")\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8253ee0a4ddd545dcfaeb5c7e515773810c507c1"},"cell_type":"markdown","source":"Finally we compute our IOU score for various thresholds. "},{"metadata":{"trusted":true,"_uuid":"fbf4dfd55f3e0a3e47c16e0c3f0fb9561860d1fe"},"cell_type":"code","source":"y_pred_true_pairs = []\nfor images, masks in val_loader:\n    images = Variable(images.cuda())\n    y_preds = model(images)\n    for i, _ in enumerate(images):\n        y_pred = y_preds[i] \n        y_pred = torch.sigmoid(y_pred)\n        y_pred = y_pred.cpu().data.numpy()\n        y_pred_true_pairs.append((y_pred, masks[i].numpy()))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"30cb551ef5b855bd8acca043a196793047de0128"},"cell_type":"markdown","source":"We use a method to calculate the IOU score as found in this kernel here: https://www.kaggle.com/leighplt/goto-pytorch-fix-for-v0-3.\n"},{"metadata":{"trusted":true,"_uuid":"082bfd105ec00b430dd4c5f06855d3daa346a329"},"cell_type":"code","source":"# https://www.kaggle.com/leighplt/goto-pytorch-fix-for-v0-3\nfor threshold in np.linspace(0, 1, 11):\n    \n    ious = []\n    for y_pred, mask in y_pred_true_pairs:\n        prediction = (y_pred > threshold).astype(int)\n        iou = jaccard_similarity_score(mask.flatten(), prediction.flatten())\n        ious.append(iou)\n        \n    accuracies = [np.mean(ious > iou_threshold)\n                 for iou_threshold in np.linspace(0.5, 0.95, 10)]\n    print('Threshold: %.1f, Metric: %.3f' % (threshold, np.mean(accuracies)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"247a8e0cd194ee6568b0f01c4e4046a7e5f36902"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}