{"cells":[{"metadata":{"_uuid":"37c9de8a512aabd4df9fb11d5c2369cfb5c3386e"},"cell_type":"markdown","source":"Neural Network with convolution over previous applications and bureau credit reports.\n\nThis notebook attempts to predict the defaulters in the competition with a neural network without aggregating data. Appart from the main input with details on the current loan application the details of previous activity will be fed with two auxiliary inputs for both Bureau Credit Reports and Previous Applications.\n\nPreprocessing is limited to standarization, categorical encodings and building the tensors that will feed the neural network. Other than those transformation steps there is no feature engeneering and no aggregation of input tables.\n\nAt this point only Current Application, Bureau and Previous Application data is included. This model does not include Installments, Credit Card Balance, Pos Cash Balance or Bureau Balance details."},{"metadata":{"trusted":true,"_uuid":"93225952641ca37101f71d82fabfeaf35e44baba","collapsed":true},"cell_type":"code","source":"\nimport pandas as pd\nimport numpy as np\nimport zipfile\nimport random\n\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.preprocessing import LabelEncoder  \n#Used for category encoding. Will use embedding layers in Keras instead of doing one hot encoding\n\nfrom sklearn import metrics #Using the AUC from sklear at the end of each epoch\n\n\nfrom keras.layers import (Reshape, Lambda, Dropout, Input, Embedding, Dense, Conv1D, Conv2D, \n                          concatenate, Flatten)\n                         \nfrom keras.models import Model, load_model\n\nfrom keras import regularizers, backend, optimizers\n\nfrom keras.backend import expand_dims, relu\n\nfrom keras import backend as K\nimport tensorflow as tf\n\n#These two are needed to desplay the network\nfrom keras.utils.vis_utils import model_to_dot, plot_model\nfrom IPython.display import Image\n\nimport h5py\n\nimport pickle\n\n#Memory cleanup after last use of large datasets \nimport gc\n\nimport matplotlib.pyplot as plt\n\n\n#For the first run should be set to true. The program will save the pre processed data in a few h5py and pickle files. \n#set it to False for all subsequent executions. The data will be loaded from the preprocessed files. \n#I am not sure the generated files persist between runs of the notebook in Kaggle. I just left it in case you download and run locally.\np_rebuild_datasets = True\n\n#Setting to values other than zero allows to test with smaller subset of data. For testing purposes it can be set for 50k\np_subsample_lines = 0\n\nk_datasets_prefix = \"../input/\"    \n\ntrain_data_file = \"application_train.csv\"\npredict_data_file = \"application_test.csv\"\nbureau_data_file = \"bureau.csv\"\nbureau_balance_data_file = \"bureau_balance.csv\"\npos_cash_balance_data_file = \"POS_CASH_balance.csv\"\ncredit_card_balance_data_file = \"credit_card_balance.csv\"\nprevious_application_data_file = \"previous_application.csv\"\ninstallments_payments_data_file = \"installments_payments.csv\"\n\n#This string will be used to identify auxiliary files.\np_data_model_version = \"KerasNN_PAConv_SimpleV5\"\n\np_objective = \"TARGET\"\np_index = \"SK_ID_CURR\"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"948e234033cb1e16936255d8adf3d9a76b8b846f"},"cell_type":"markdown","source":"This function takes care of the categorical encoding and standarization for non categorical data. Setting combined to true and passing two DF to it allows for combined treatment of train/test data."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"8fdc12cb1a66c190fc1df9d1dbd02457bd7086d0"},"cell_type":"code","source":"def preproc_ds(df1, df2,combined=True,keys=None):\n    if combined:\n        df = df1.append(df2, ignore_index=True)[data.columns.tolist()]\n        categoricals = df.columns[df.dtypes==\"O\"]\n        for col in df.columns[df.dtypes!=\"O\"]:\n            if col != \"TARGET\" and not col in keys:\n                mean = df[col].mean()\n                std = df[col].std()\n                df1[col] = (df1[col].fillna(mean) - mean) / std\n                df2[col] = (df2[col].fillna(mean) - mean) / std\n                \n        print(\"combined\")\n        for col in df.columns[df.dtypes==\"O\"]:\n            print(col)\n            le = LabelEncoder()\n            le.fit(df[col].fillna('PreProcEmpty'))\n            print(le.classes_)\n            df1[col] = le.transform(df1[col].fillna('PreProcEmpty'))\n            df2[col] = le.transform(df2[col].fillna('PreProcEmpty'))\n\n        return(df1, df2, categoricals)\n    else:\n        print(\"single\")\n        categoricals = df1.columns[df1.dtypes==\"O\"]\n        for col in df1.columns[df1.dtypes!=\"O\"]:\n            if col != \"TARGET\" and not col in keys:\n                mean = df1[col].mean()\n                std = df1[col].std()\n                df1[col] = (df1[col].fillna(mean) - mean) / std\n                \n        for col in df1.columns[df1.dtypes==\"O\"]:\n            print(col)\n            le = LabelEncoder()\n            le.fit(df1[col].fillna('PreProcEmpty'))\n            print(le.classes_)\n            df1[col] = le.transform(df1[col].fillna('PreProcEmpty'))\n        return(df1,categoricals)\n    \n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"b1ab76b29d8622cbf0479b4464b1d49d8146b74f"},"cell_type":"code","source":"def read_df(filename,subdir,index): \n    df = pd.read_csv(subdir + filename).sort_values(by=index)\n    if index != None:\n        df.set_index(index, inplace=False)\n    return(df)\n    \n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c311a2c1779b6cc2e3bd7d584631ab8c170185f1"},"cell_type":"markdown","source":"The network will have a main input (IE application_train.csv) and two additional inputs for previous products (Previous Application and Bureau Data). \n\nThe main input will have an ordinary input matrix of the form (samples, features)\n\nThe two additional inputs have a many-to-one relationship with the main dataset, hence the format will be the following: (samples, products, features). The additional dimension, products, identifies the previous applications or bureau reports.\n\nA similar approach can be used and extend this to the next level of detail (credit card balances, installment payments, etc. ). That last level is not implemented in this notebook.\n\nThe following function, generate_conv_tensor_simple, builds the additional input tensors as an ndarray. The number of products to consider for each sample is capped (at 24) because the tensor needs a predefined size for convolutions. \n\nAfter building the input matrices and tensors they will be stored in H5Py format which Keras can use. Using ndarrays directly was unfeasible in terms of memory (especially when attempting to implement the next level of detail and include data from the more detailed sources). Alternatively a generator/yield scheme could be used but resulted much slower than the h5py method.\n\nThe input data frames to the following function need to be pre sorted so as to iterate only once on the samples in the main data source and each detailed source (Previous Application and Bureau)."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"aff18c04d7b10c1a85b3038c1d04f97153dbe8cd"},"cell_type":"code","source":"def generate_conv_tensor_simple(X,X_index, products,PA_cp):\n\n    PA = PA_cp[PA_cp[p_index].isin(X[p_index])]\n    st_products = 0\n    samples = len(X)\n    PA_length = len(PA)\n    max_len_detected = 0\n    max_prod_detected = 0\n    nPA_in_vars = len(PA.iloc[0])\n    PA_nX_tensor = np.zeros((samples,products,nPA_in_vars))\n    X_sample=0                                                 \n    x_progress_tic = samples/100\n    x_progress = 0\n    chunks_PA = (1,products,nPA_in_vars-2)\n    PA_iloc = 0\n    for X_iloc in range(samples):\n        X_row = X.iloc[X_iloc,:]\n        PA_row = PA.iloc[PA_iloc,:]\n        product_n = 0        \n        while PA_row[X_index] == X_row[X_index]:         \n            if (product_n < products):\n                PA_nX_tensor[X_iloc,product_n,:]= PA_row           \n                st_products = st_products + 1 \n            product_n = product_n + 1\n            PA_iloc = PA_iloc + 1\n            if PA_iloc >= PA_length: break\n            PA_row = PA.iloc[PA_iloc,:]\n        x_progress = x_progress + 1\n        if x_progress > x_progress_tic:\n            x_progress_tic = x_progress_tic + samples/100\n            prog = \"\\r\" + str(x_progress/samples*100) \n            print(prog, end=\"\", flush=True)\n            \n    print(\"Products \",st_products)\n    return(PA_nX_tensor[:,:,2:],chunks_PA)   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"3deae5045d2abf7ddfbfc9989f71953c34cdf547"},"cell_type":"code","source":"\ndef pickledump(filename,dumpobj):    \n    outfile = open(filename,'wb')\n    pickle.dump(dumpobj,outfile, protocol=4)\n    outfile.close()\n\ndef pickleload(filename):    \n    print(\"Cargando pickle : \", filename)\n    infile = open(filename,'rb')\n    dumpobj = pickle.load(infile)\n    infile.close()\n    return(dumpobj)\n\ndef h5dump(dataset,array,hp5yf,chunks):\n    return(hp5yf.create_dataset(dataset, data=array, compression=\"lzf\",chunks=chunks))\n\ndef build_dump_tensor(X,p_index,p_products,h5py_dataset,PA,h5pyfilename):\n    print(\"Saving :\", h5py_dataset)\n    ts_pa,chunks_pa = generate_conv_tensor_simple(X,p_index, p_products,PA)\n    h5pyf = h5py.File(h5pyfilename, \"a\", driver = \"core\")\n    ts_pa = h5dump(h5py_dataset, ts_pa,h5pyf,chunks_pa)\n    h5pyf.flush()\n    h5pyf.close()\n    del h5pyf, ts_pa\n    gc.collect()\n    return()\n","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"fd9f1fbe74d17116e623445a50cda887f3f9f76e","collapsed":true},"cell_type":"code","source":"#The number of previous applications/bureau reports by sample is limited to 24\np_products = 24\n\nh5py_file_filename = \"h5py_\"+ p_data_model_version + \".hdf5\" #to store train/test/unclassified main input\nh5py_file_bd_filename = \"h5py_\"+ p_data_model_version + \"_bd.hdf5\" #to store train/test/unclassified bureau data\nh5py_file_pa_filename = \"h5py_\"+ p_data_model_version + \"_pa.hdf5\"  #to store train/test/unclassified previous applications\n\nif p_rebuild_datasets == True:  #Generate all pre-processed datasets\n    \n\n##### MAIN DATA\n# Read train/unclassified main data\n    data = read_df(train_data_file,k_datasets_prefix,p_index)\n    data_unclassified = read_df(predict_data_file,k_datasets_prefix,p_index)\n\n# Normalize + categoricals preprocess for main data\n    data, data_unclassified, data_categs = preproc_ds(data, data_unclassified,True,[p_index,\"TARGET\"])\n\n# This sorting is NOT mandatory since it will be resorted after train/test split\n    data = data.sort_values(by=[p_index], ascending = True)\n\n# This sorting is important for the tensor generation to work correctly      \n    data_unclassified = data_unclassified.sort_values(by=[p_index], ascending = True)\n    \n# Save ID sorted list to match the predicted probabilities in the end\n    data_unclassified[\"SK_ID_CURR\"].to_csv(\"./\" + p_data_model_version + \"unclassified_ids.csv\", header=True, index=None, sep=',', mode='w')\n    \n# Columns are re arranged so that categorical data is in the end. \n# Solving embeddings in the NN this will be simpler in this way\n    data = pd.concat([data.drop(data_categs,axis=1),data[data_categs]], axis=1)\n    data_unclassified = pd.concat([data_unclassified.drop(data_categs,axis=1),data_unclassified[data_categs]], axis=1)    \n\n    \n##### BUREAU DATA    \n# Read + Normaulize + Pre-Process Bureau Data    \n    X_bd, bd_categs = preproc_ds(read_df(bureau_data_file,k_datasets_prefix,[p_index,\"SK_ID_BUREAU\"]),None,False,[p_index,\"SK_ID_BUREAU\"])\n\n# This sorting is important for the tensor generation to work correctly   \n    X_bd = X_bd.sort_values(by=[p_index, 'SK_ID_BUREAU','DAYS_CREDIT' ], ascending=[1,1,1])\n    \n# Columns are re arranged so that categorical data is in the end. \n# Solving embeddings in the NN this will be simpler in this way\n    X_bd = pd.concat([X_bd.drop(bd_categs,axis=1),X_bd[bd_categs]], axis=1)\n    \n##### PREVIOUS APPLICATION DATA    \n    X_pa, pa_categs = preproc_ds(read_df(previous_application_data_file,k_datasets_prefix,[p_index,\"SK_ID_PREV\"]),None,False,[p_index,\"SK_ID_PREV\"])\n\n# This sorting is important for the tensor generation to work correctly   \n    X_pa = X_pa.sort_values(by=[p_index, 'SK_ID_PREV' ], ascending=[1,1])\n\n# Columns are re arranged so that categorical data is in the end. \n# Solving embeddings in the NN this will be simpler in this way    \n    X_pa = pd.concat([X_pa.drop(pa_categs,axis=1),X_pa[pa_categs]], axis=1)\n    \n# If need to do a quick test you can use the p_subsample_lines parameter to limit the dataset to a smaller portion. \n# Since the dataset has been sorted by ID it should only be used to check the logic is working properly\n    if p_subsample_lines != 0:    \n        data = data.iloc[:p_subsample_lines]\n\n    \n    X_predict = data_unclassified\n\n# use the smallest of 10% or 10ksamples for test    \n    if len(data)> 100000: test_size = 10000 \n    else: test_size = 0.10\n\n# split data. No need to use \"the answer to life, the universe and everything\" as random_state ;)    \n    X_train, X_test = train_test_split(data, test_size=test_size, random_state=42, shuffle = True)\n\n# This sorting is important for the tensor generation to work correctly  \n    X_train = X_train.sort_values(by=[p_index], ascending = True)\n    y_train = X_train[p_objective]\n    X_train = X_train.drop(p_objective, axis=1)\n\n# This sorting is important for the tensor generation to work correctly      \n    X_test = X_test.sort_values(by=[p_index], ascending = True)\n    y_test = X_test[p_objective]\n    X_test = X_test.drop(p_objective, axis=1)\n\n# Save preprocessed data for the main input data and category lists\n    h5py_file = h5py.File(h5py_file_filename, \"a\", driver = \"core\")    \n    X_train_h5 = h5dump(\"pd_\"+ p_data_model_version +\"_X_train\",X_train,h5py_file,(5,X_train.shape[1]),)\n    X_test_h5 = h5dump(\"pd_\"+ p_data_model_version +\"_X_test\",X_test,h5py_file,(5,X_test.shape[1]))\n    X_predict_h5 = h5dump(\"pd_\"+ p_data_model_version +\"_X_predict\",X_predict,h5py_file,(5,X_predict.shape[1]))\n    y_train_h5 = h5dump(\"pd_\"+ p_data_model_version +\"_y_train\",y_train,h5py_file,None)\n    y_test_h5 = h5dump(\"pd_\"+ p_data_model_version +\"_y_test\",y_test,h5py_file,None)\n    X_categs = pickledump(\"pk_\"+ p_data_model_version +\"_X_categs\",data_categs)\n\n\n    h5py_file.flush()\n    h5py_file.close()\n\n#Invoke the tensor build and save to H5PY for Bureau data in its three splits (train, Test and unclassified)    \n    build_dump_tensor(X_test, p_index, p_products,\"pd_\"+ p_data_model_version +'_bd_test',X_bd, h5py_file_bd_filename)\n    build_dump_tensor(X_predict, p_index, p_products,\"pd_\"+ p_data_model_version +'_bd_predict',X_bd, h5py_file_bd_filename)\n    build_dump_tensor(X_train, p_index, p_products,\"pd_\"+ p_data_model_version +'_bd',X_bd, h5py_file_bd_filename)\n\n#Invoke the tensor build and save to H5PY for Previous Application data in its three splits (train, Test and unclassified)    \n    build_dump_tensor(X_test,p_index, p_products,\"pd_\"+ p_data_model_version +'_pa_test',X_pa,h5py_file_pa_filename)\n    build_dump_tensor(X_predict, p_index, p_products,\"pd_\"+ p_data_model_version +'_pa_predict',X_pa,h5py_file_pa_filename)\n    build_dump_tensor(X_train, p_index, p_products,\"pd_\"+ p_data_model_version +\"_pa\",X_pa,h5py_file_pa_filename)\n\n#Save category list for embedding processing within NN\n    pickledump(\"pk_\"+ p_data_model_version +\"_bd_categs\",bd_categs)\n    pickledump(\"pk_\"+ p_data_model_version +\"_pa_categs\",pa_categs)       \n        \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"574645e9f4d0fce89728985217ccfce194d44c64","collapsed":true},"cell_type":"code","source":"# Reload from h5py all data. Makes sense for subsequent runs where data is not generated again\nh5py_file = h5py.File(\"h5py_\"+ p_data_model_version + \".hdf5\", \"r\")\nh5py_file_bd = h5py.File(h5py_file_bd_filename, \"r\")\nh5py_file_pa = h5py.File(h5py_file_pa_filename, \"r\")\n\nX_train = h5py_file[\"pd_\"+ p_data_model_version +\"_X_train\"][:,1:]\nX_train_labels = h5py_file[\"pd_\"+ p_data_model_version +\"_X_train\"][:,:1]\nX_test = h5py_file[\"pd_\"+ p_data_model_version +\"_X_test\"][:,1:]\nX_test_labels = h5py_file[\"pd_\"+ p_data_model_version +\"_X_test\"][:,:1]\nX_predict = h5py_file[\"pd_\"+ p_data_model_version +\"_X_predict\"][:,1:]\n\ny_train = h5py_file[\"pd_\"+ p_data_model_version +\"_y_train\"]\ny_test = h5py_file[\"pd_\"+ p_data_model_version +\"_y_test\"]\nX_categs = pickleload(\"pk_\"+ p_data_model_version +\"_X_categs\")\n\nX_bd_tensor = h5py_file_bd[\"pd_\"+ p_data_model_version +\"_bd\"]\nX_test_bd_tensor = h5py_file_bd[\"pd_\"+ p_data_model_version +\"_bd_test\"]\nX_predict_bd_tensor = h5py_file_bd[\"pd_\"+ p_data_model_version +\"_bd_predict\"]\nX_bd_categs = pickleload(\"pk_\"+ p_data_model_version +\"_bd_categs\")\n\nX_pa_tensor = h5py_file_pa[\"pd_\"+ p_data_model_version +\"_pa\"]\nX_test_pa_tensor = h5py_file_pa[\"pd_\"+ p_data_model_version +\"_pa_test\"]\nX_predict_pa_tensor = h5py_file_pa[\"pd_\"+ p_data_model_version +\"_pa_predict\"]\nX_pa_categs = pickleload(\"pk_\"+ p_data_model_version +\"_pa_categs\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"c4bf0238b9c5dc984b54f424e26078b4d61f553b"},"cell_type":"code","source":"#Determine features for all three input matrices/tensors\npa_in_vars = X_pa_tensor.shape[2]\nbd_in_vars = X_bd_tensor.shape[2]\nmain_in_vars = X_train.shape[1]\n\ntotal_samples = len(X_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f2f959f3b25a337df5c62332fd22d6c650d2808d"},"cell_type":"markdown","source":"The following custom callback calculates the AUC at the end of each epoch, deals with early stopping and saves the weights to disk if the auc is the best so far. Next to that there is an auc_m custom metric that calculates the same within training but tends to differ a bit from the sklearn numbers. "},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"d8e10accdddd2ea9131ed110177ae37830133c56"},"cell_type":"code","source":"from keras.callbacks import Callback\n\n\nclass roc_callback(Callback):\n    def __init__(self,training_data,validation_data,early_stopping=False, patience=10, min_delta=0, checkpoint_file = None):\n        self.x_val = validation_data[0]\n        self.y_val = validation_data[1]\n        self.auc_val = []\n        self.early_stopping = early_stopping\n        self.patience = patience\n        self.min_delta = min_delta\n        self.wait = 0\n        self.stopped_epoch = 0\n        self.monitor_op = np.greater\n        self.checkpoint_file = checkpoint_file\n\n\n    def on_train_begin(self, logs={}):\n        self.wait = 0\n        self.stopped_epoch = 0\n        self.best = 0\n        return\n\n    def on_train_end(self, logs={}):\n        return\n\n    def on_epoch_begin(self, epoch, logs={}):\n        return\n\n    def on_epoch_end(self, epoch, logs={}):\n        y_pred_val = self.model.predict(self.x_val)\n        fpr, tpr, thresholds = metrics.roc_curve(self.y_val, y_pred_val, pos_label=1)\n        auc_val=metrics.auc(fpr, tpr)\n        self.auc_val.append(auc_val)\n        print('\\rSKLearn roc-auc_val: %s' % str(round(auc_val,4)),end=100*' '+'\\n')\n        if self.early_stopping == True:\n            current = auc_val\n\n            if self.monitor_op(current-self.min_delta,self.best):\n                self.best = current\n                self.wait = 0\n                if not self.checkpoint_file is None:\n                    self.model.save_weights(self.checkpoint_file)\n                    print(\"Saved checkpoint: \", self.checkpoint_file)\n            else:\n                self.wait += 1\n                if self.wait >= self.patience:\n                    self.stopped_epoch = epoch\n                    self.model.stop_training = True\n            print(\"Early Stopping Best: \",str(round(self.best,5)), \"Epochs w/o improvement: \", self.wait, \"Must score \", str(round(self.best+self.min_delta,5)), \"before  \",self.patience-self.wait,\" Epochs to extend\",'\\n' )\n        return\n\n    def on_batch_begin(self, batch, logs={}):\n        return\n\n    def on_batch_end(self, batch, logs={}):\n        return\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"2b0950e3d6366abe4fddabfa3095ba19d2d4abdd"},"cell_type":"code","source":"#This makes an aproximate AUC score calculation within the epoch\ndef auc_m(y_true, y_pred):   \n    ptas = tf.stack([binary_PTA(y_true,y_pred,k) for k in np.linspace(0, 1, 1000)],axis=0)\n    pfas = tf.stack([binary_PFA(y_true,y_pred,k) for k in np.linspace(0, 1, 1000)],axis=0)\n    pfas = tf.concat([tf.ones((1,)) ,pfas],axis=0)\n    binSizes = -(pfas[1:]-pfas[:-1])\n    s = ptas*binSizes\n    return K.sum(s, axis=0)\n\n#-----------------------------------------------------------------------------------------------------------------------------------------------------\n# PFA, prob false alert for binary classifier\ndef binary_PFA(y_true, y_pred, threshold=K.variable(value=0.5)):\n    y_pred = K.cast(y_pred >= threshold, 'float32')\n    # N = total number of negative labels\n    N = K.sum(1 - y_true)\n    # FP = total number of false alerts, alerts from the negative class labels\n    FP = K.sum(y_pred - y_pred * y_true)    \n    return FP/N\n#-----------------------------------------------------------------------------------------------------------------------------------------------------\n# P_TA prob true alerts for binary classifier\ndef binary_PTA(y_true, y_pred, threshold=K.variable(value=0.5)):\n    y_pred = K.cast(y_pred >= threshold, 'float32')\n    # P = total number of positive labels\n    P = K.sum(y_true)\n    # TP = total number of correct alerts, alerts from the positive class labels\n    TP = K.sum(y_pred * y_true)    \n    return TP/P\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"1f41385b7e2c58486672d2aad52e3cab44caeab8"},"cell_type":"code","source":"#Custom function that allows cropping of tensors within keras\ndef crop(dimension, start, end=None):\n    if not end is None:        \n        def func(x):\n            if dimension == 0:\n                return x[start: end]\n            if dimension == 1:\n                return x[:, start: end]\n            if dimension == 2:\n                return x[:, :, start: end]\n            if dimension == 3:\n                return x[:, :, :, start: end]\n            if dimension == 4:\n                return x[:, :, :, :, start: end]\n    else:\n        def func(x):\n            if dimension == 0:\n                return x[start]\n            if dimension == 1:\n                return x[:, start]\n            if dimension == 2:\n                return x[:, :, start]\n            if dimension == 3:\n                return x[:, :, :, start]\n            if dimension == 4:\n                return x[:, :, :, :, start]            \n    return Lambda(func)\n\ndef gen_conv_branch(name,  # name of layer\n                    products, # number of prev applications or bureau reports as max\n                    pa_filters, # number of filters in first convolution\n                    pa_filters1, # number of filters in next convolution\n                    pa_filters2, # number of filters in last convolution\n                    lc, # Size of dense layer after convolutions\n                    lc2, # Size of final dense layer before integrating with main input\n                    features_pa, # number of features in prev applications or bureau reports\n                    regu, # regularization parameter for convolutions and dense layers\n                    pa_categs, # list of categorical fields\n                    inputs, # list of input layers to this moment. function will append and return with the input for the new input\n                    NN_data, # list with training data (h5py arrays). function will append and return with training data fot this branch\n                    NN_val_data, # list with testing data (h5py arrays). function will append and return with testing data fot this branch\n                    data_pa, # h5py object with train data\n                    data_test_pa):   # h5py object with test data\n    \n\n        # Create input\n        b_pa_input = Input(shape=(products,features_pa),name=name+\"_PA_main\", dtype='float32') \n\n        # Append new input to list of inputs already created\n        inputs.append(b_pa_input)\n        \n        #Append train data to list of data already created\n        NN_data.append(data_pa)\n\n        #Append test data to list of data already created       \n        NN_val_data.append(data_test_pa)\n\n        #Number of non categorical features\n        noncat_features = int(b_pa_input.shape[2]-len(pa_categs))\n\n        #Splits from the input the part that has non categorical data. \n        # Data has (Sample, Product, Feature). This function will keep (Sample, Product, 0 to number of non-cat features)\n        x_noncat_pa = crop(2,0,noncat_features)(b_pa_input)\n#       x_noncat_pa = Reshape((products,noncat_features,))(x_noncat_pa)\n        \n        x_cat_pa = [x_noncat_pa] # initialize list of concatenation after dealing with embeddings\n\n        \n        #the following block creates the embedding layer for each categorical feature. \n        #It will crop each feature out of the input layer, determine the cardinality, create the embedding layer,\n        #appends the resulting embedded layer to the list x_cat_pa (non-categoricals + categoricals)\n        #and concatenate it all in a single result\n        i=0 \n        if len(pa_categs) > 0:\n            for categ in pa_categs:\n                cat_layer_pa = crop(2,noncat_features+i )(b_pa_input)\n                cardinality = len(np.unique(data_pa[:,:,-len(pa_categs)+i]))\n                target_cardinality = int(cardinality ** 0.25)\n                cat_layer_pa = Embedding(cardinality, target_cardinality)(cat_layer_pa)\n                cat_layer_pa = Reshape((products,target_cardinality,))(cat_layer_pa)         \n                x_cat_pa.append(cat_layer_pa)\n                i+=1\n            b_pa_x= concatenate(x_cat_pa)\n        else:\n            b_pa_x= b_pa_input\n        \n        #The resulting tensor after embeddings still has the structure (sample, products, features)\n        #it will pass through three 1D convolution with a kernel of size 1. \n        #The input features represent channels in the convolution. The idea behind using a convolution is to  \n        #have a common \"logic\" for all instances of products\n        \n        b_pa_x = Conv1D(filters=pa_filters,\n                      kernel_size=(1),\n                      input_shape=(products,features_pa),\n                      activation=\"relu\",\n                      kernel_regularizer=regularizers.l2(regu),\n                      strides=1)(b_pa_x)\n        b_pa_x = Conv1D(filters=pa_filters1,\n              kernel_size=(1),\n              input_shape=(products,pa_filters),\n              activation=\"relu\",\n              kernel_regularizer=regularizers.l2(regu),\n              strides=1)(b_pa_x)\n        b_pa_x = Conv1D(filters=pa_filters2,\n              kernel_size=(1),\n              input_shape=(products,pa_filters1),\n              activation=\"relu\",\n              kernel_regularizer=regularizers.l2(regu),\n              strides=1)(b_pa_x)\n        \n        # The result id flattened and passed through two dense layers before connecting with the other inputs\n\n        b_pa_x = Flatten()(b_pa_x)\n        b_pa_x = Dropout(0.3)(b_pa_x)\n\n        b_pa_x = Dense(lc, activation=\"relu\",kernel_initializer='he_normal',\n            kernel_regularizer=regularizers.l2(regu))(b_pa_x)  \n        b_pa_x = Dropout(0.3)(b_pa_x)\n        \n        b_pa_x = Dense(lc2, activation=\"relu\",kernel_initializer='he_normal',\n            kernel_regularizer=regularizers.l2(regu))(b_pa_x)        \n\n        return(b_pa_x, inputs, NN_data, NN_val_data)\n\n\n    \ndef plot_roc(model,test_data,test_labels):\n\n    prediction = model.predict(test_data)\n    fpr, tpr, thresholds = metrics.roc_curve(test_labels, prediction, pos_label=1)\n    auc=metrics.auc(fpr, tpr)\n    plt.figure()\n    lw = 2\n    plt.plot(fpr, tpr, color='darkorange',\n             lw=lw, label='ROC curve '+ str(round(auc,6)))\n    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic example')\n    plt.legend(loc=\"lower right\")\n    fig = plt.gcf()\n    plt.show()\n    fig.savefig('./work/'+p_data_model_version+'_ROC.png')\n    return(auc)    \n    ","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"160e6365e34d503d1a26c851c6fc2d3cef6678da","collapsed":true},"cell_type":"code","source":"#Set the weights of the classess based on the aproximate proportions of positive/negative samples\nclass_weight = {0: 1.,\n                1: 16.}\n\n#Regularization for convolution branches\nregu=10**-7.56\n#Redularization for main dense layers\nregm=10**-4.2\n#Learning rate\nlearn_rate = 10**-2.38\n#Generally early stopping is activated before\nepochs=50\n#Set to lower size if out of GPU memory\nbatch_size = 2**12\n\n\n#Input for main data file\nmain_input = Input(shape=(main_in_vars,),name='main_input', dtype='float32') #dtype='int32',\n\n#Initialize inputs and data lists\nmodel_inputs = [main_input]\nNNtrain_data = [X_train]\nNNtest_data = [X_test]\n\n\n\n#Number of non categorical features\nnoncat_features = int(main_input.shape[1]-len(X_categs))\n\n#Splits from the input the part that has non categorical data. \n#Data has (Sample, Feature). This function will keep (Sample, 0 to number of non-cat features)\nx_noncat = crop(1,0,noncat_features)(main_input)\nx_cat = [x_noncat]\n\n#the following block creates the embedding layer for each categorical feature. \n#It will crop each feature out of the input layer, determine the cardinality, create the embedding layer,\n#appends the resulting embedded layer to the list x_cat_pa (non-categoricals + categoricals)...\ni=0\nfor categ in X_categs:\n    cat_layer = crop(1,noncat_features+i )(main_input)\n    cardinality = len(np.unique(X_train[:,-len(X_categs)+i]))\n    target_cardinality = int(cardinality ** 0.25)   \n    cat_layer = Embedding(cardinality, target_cardinality)(cat_layer)\n    cat_layer = Reshape((target_cardinality,))(cat_layer)         \n    x_cat.append(cat_layer)\n    i+=1\n\n#...and concatenate it all in a single result\nx= concatenate(x_cat)\n\n\n#First fully connected layer\nx = Dense(90, activation=\"relu\", kernel_initializer='he_normal',\n        kernel_regularizer=regularizers.l2(regm))(x)\nx = Dropout(0.5)(x)\n\n\n#Generate convolution branch for bureau data\nbd_x, model_inputs, NNtrain_data, NNtest_data  = gen_conv_branch(\"bd_input\",p_products, \n                                                  90, 60, 10,\n                                                  60, 20, bd_in_vars,regu, \n                                                  X_bd_categs,\n                                                  model_inputs, NNtrain_data, NNtest_data,\n                                                  X_bd_tensor, X_test_bd_tensor)\n\n#concatenate the result of the bureau data conv branch to the rest\nx = concatenate([bd_x, x])\n\n#Generate convolution branch for Previous Application\npcb_x, model_inputs, NNtrain_data, NNtest_data = gen_conv_branch(\"pa_input\",p_products, \n                                                  90, 60, 10,\n                                                  60,20,pa_in_vars,regu, \n                                                  X_pa_categs,\n                                                  model_inputs, NNtrain_data, NNtest_data,\n                                                  X_pa_tensor, X_test_pa_tensor)\nx = concatenate([pcb_x, x])\n\n\n#add two fully connected layers\nx = Dense(150, activation=\"relu\",kernel_initializer='he_normal',\n    kernel_regularizer=regularizers.l2(regm))(x)\nx = Dropout(0.5)(x)\n\nx = Dense(30, activation=\"relu\",kernel_initializer='he_normal',\n    kernel_regularizer=regularizers.l2(regm))(x)     \n\nmain_output = Dense(1, activation='sigmoid', name='main_output')(x)\nmodel = Model(inputs=model_inputs, outputs=main_output)\n\n#custom function to correctly measure AUC, deal with early stopping and save training progress\nroc_cbk = roc_callback(training_data=None,validation_data=(NNtest_data, y_test),\n                       early_stopping= True, patience=8, min_delta=0.0001, \n                       checkpoint_file = './'+p_data_model_version+'_weights.hdf5')   \ncallbacks_list = [roc_cbk]\n\n#Compile model\nmodel.compile(optimizer=optimizers.Adam(lr=learn_rate), loss='binary_crossentropy', metrics=[auc_m])\n","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"4c3d42ce118d1a9f4d498bbc85d2db08060489da","collapsed":true},"cell_type":"code","source":"#Train and plot evolution of AUC\nfitlog = model.fit(NNtrain_data, y_train, validation_data=(NNtest_data, y_test),\n                   epochs=epochs, batch_size=batch_size, \n                   class_weight=class_weight, callbacks=callbacks_list, shuffle=\"batch\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"96d9a3545ecdfd1a359d6f3e8522dcc291b9904f","collapsed":true},"cell_type":"code","source":"#Plot training evolution\nauc = fitlog.history[\"auc_m\"]\nplt.figure(figsize=(12,10))\nplt.plot(auc)\nplt.plot(roc_cbk.auc_val)\nplt.ylim([0.73, 0.85])\nplt.title('AUC')\nplt.ylabel('AUC')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper right')\nplt.legend('test', loc='upper right')\nplt.plot(np.array(range(len(auc))),np.full(len(auc),np.mean(auc[-4:])))\nplt.plot(np.array(range(len(roc_cbk.auc_val))),np.full(len(roc_cbk.auc_val),np.mean(roc_cbk.auc_val[-4:])))\nplt.show()\n\nprint(\"Best AUC: \",max(roc_cbk.auc_val), \" at epoch: \", np.argmax(roc_cbk.auc_val))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"0ba3cadd09e429d22a42b519def48543c46fcf0a"},"cell_type":"code","source":"#Build prediction input data list\nNNpredict_data = [X_predict, X_predict_bd_tensor, X_predict_pa_tensor ]\n\n#load best weights from checkpoint file\nmodel.load_weights('./'+p_data_model_version+'_weights.hdf5')\n\n#Compute prediction\nprediction = model.predict(NNpredict_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"c8a87dd2b740448dbefdc2b11759f055e8e72745"},"cell_type":"code","source":"#create submission file\nsubmission = pd.read_csv(\"./\" + p_data_model_version + \"unclassified_ids.csv\", sep=',')\nsubmission[\"TARGET\"] = prediction","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"9415e0aa985f2f52c30691c09a709a431bc316e5"},"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"2a3d40019edb206715ec2eb664b843ce013619e6"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{},"version_major":2,"version_minor":0}}},"nbformat":4,"nbformat_minor":1}