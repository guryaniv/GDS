{"cells":[{"metadata":{"_cell_guid":"fb618020-d1ef-43b8-914f-fa870e433505","_uuid":"5cbb8c69ffee2153f7cd4f12318f5797da813c49"},"cell_type":"markdown","source":"There are several things we will be doing here. Be warned:\n\n![TL;DR](https://m.popkey.co/3c4432/1Z7Mx.gif)\n\n\nFirst, we will do some feature engineering on \"categorical\" variables (note that I am legally obligated to put that word in quotation marks since, on the surface, they are all numerical variables). I will advertise [__MLBox__](https://github.com/AxeldeRomblay/MLBox) as it will help us with feature engineering. This seems like an excellent ML package, and even though I would not want a single ML package doing everything while I'm just watching, it is undeniable that there are lots of useful tools in it. The one we will use is its [__categorical encoder__](http://mlbox.readthedocs.io/en/latest/features.html#categorical-features). Originally, I wrote this script with [__entity embedding__](https://arxiv.org/abs/1604.06737) as my strategy of choice. We all know what happens with best laid plans ...\n\n\n![Best laid plans](https://i.imgur.com/f8sAmnn.gif)\n\n\nOn my GTX 1080 the entity embedding learning took 3 minutes, while on Kaggle it was going for solid  52 minutes during peak hours. So I went with [__random projection__](https://en.wikipedia.org/wiki/Random_projection) instead for the sake of time, but I do encourage you to uncomment the line below that calls entity embedding and give it a try locally.\n\nNext, we will use these new features as an input for [__XGBoost upsampling__](https://www.kaggle.com/ogrellier/xgb-classifier-upsampling-lb-0-283). That script is very fast so it stands a chance of finishing several runs in an hour, and I like the idea as well. I have left all of the original comments from that script intact, which also give credit to other Kagglers from whom @[olivier](https://www.kaggle.com/ogrellier) has borrowed.\n\nPlease read the comment section in that script and @olivier's though on a variety of topics, including the potential for overfitting. Though we are not using his target encoding method here, the same disclaimer applies.\n\nThe idea is to do several quick Bayesian optimization runs with relatively high learing rate (0.1) in order to find the best parameters. Once we have the parameters, proper XGBoost training and prediction are done for higher number of iterations and with lower learning rate (0.02). You can explore other Bayesian optimization ideas [__here__](https://www.kaggle.com/tilii7/bayesian-optimization-of-xgboost-parameters)."},{"metadata":{"_cell_guid":"0a133530-24be-4239-ab65-3e1eb75349ac","_uuid":"42a121b1acb1c5ec7c1644184394b1504bd4347f","collapsed":true,"trusted":true},"cell_type":"code","source":"# coding: utf-8\n# The next line is needed for python 2.7 ; probably not for python 3\nfrom __future__ import print_function\n\nimport numpy as np\nimport pandas as pd\nimport gc\nimport warnings\nfrom bayes_opt import BayesianOptimization\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import log_loss, matthews_corrcoef, roc_auc_score\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\nimport gc\nfrom numba import jit\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import MinMaxScaler\nimport time\nfrom datetime import datetime\nfrom mlbox.encoding import Categorical_encoder as CE\n\n@jit\ndef eval_gini(y_true, y_prob):\n    \"\"\"\n    Original author CPMP : https://www.kaggle.com/cpmpml\n    In kernel : https://www.kaggle.com/cpmpml/extremely-fast-gini-computation\n    \"\"\"\n    y_true = np.asarray(y_true)\n    y_true = y_true[np.argsort(y_prob)]\n    ntrue = 0\n    gini = 0\n    delta = 0\n    n = len(y_true)\n    for i in range(n-1, -1, -1):\n        y_i = y_true[i]\n        ntrue += y_i\n        gini += y_i * delta\n        delta += 1 - y_i\n    gini = 1 - 2 * gini / (ntrue * (n - ntrue))\n    return gini\n\ndef gini_xgb(preds, dtrain):\n    labels = dtrain.get_label()\n    gini_score = eval_gini(labels, preds)\n    return [('gini', gini_score)]\n\ndef add_noise(series, noise_level):\n    return series * (1 + noise_level * np.random.randn(len(series)))\n\ndef timer(start_time=None):\n    if not start_time:\n        start_time = datetime.now()\n        return start_time\n    elif start_time:\n        thour, temp_sec = divmod(\n            (datetime.now() - start_time).total_seconds(), 3600)\n        tmin, tsec = divmod(temp_sec, 60)\n        print('\\n Time taken: %i hours %i minutes and %s seconds.' % (thour, tmin, round(tsec, 2)))\n\ndef scale_data(X, scaler=None):\n    if not scaler:\n        scaler = MinMaxScaler(feature_range=(-1, 1))\n        scaler.fit(X)\n    X = scaler.transform(X)\n    return X, scaler\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"24ca964e-faa3-4796-a6c0-18e03f2db181","_uuid":"8c8c9574e4383194f7715df05253705c8610f312"},"cell_type":"markdown","source":"Here we define cross-validation variables that are used for parameter search. Each parameter has its own line, so it is easy to comment something out if you wish. Keep in mind that in such a case you must comment out the matching lines in optimization and explore sections below. I commented out *max_delta_step, subsample and colsample_bytree* and assigned them fixed values. This was done after noticing interesting patterns for alpha, lambda and scale_pos_weight in [__this script__](https://www.kaggle.com/aharless/xgboost-cv-lb-284). So I included them in optimization even though I believe that the above-mentioned script is over-fitting. Feel free to uncomment the lines and optimize 9 instead of 6 variables, but keep in mind that you will need much larger number of initial and optimization points to do that properly.\n\nNote that the learning rate (\"eta\") is set to 0.1 below. That is done so we can learn the parameters quickly (without going over 200 XGBoost iterations on average). __Here is a tip: change n_estimators below from 200 to 300-400 and see if that gives a better score during optimization -- it will take longer, though.__"},{"metadata":{"_cell_guid":"e353f5c8-804b-41eb-8dcb-90768a8c71ca","_uuid":"8d1180db3bc66d0d6e47a179ef01183d30bdde74","collapsed":true,"trusted":true},"cell_type":"code","source":"# Comment out any parameter you don't want to test\ndef XGB_CV(\n          max_depth,\n          gamma,\n          min_child_weight,\n#          max_delta_step,\n#          subsample,\n#          colsample_bytree\n          scale_pos_weight,\n          reg_alpha,\n          reg_lambda\n         ):\n\n    global GINIbest\n\n    n_splits = 5\n    n_estimators = 200\n    folds = StratifiedKFold(n_splits=n_splits, random_state=1001)\n    xgb_evals = np.zeros((n_estimators, n_splits))\n    oof = np.empty(len(trn_df))\n    sub_preds = np.zeros(len(sub_df))\n    increase = True\n    np.random.seed(0)\n\n    for fold_, (trn_idx, val_idx) in enumerate(folds.split(target, target)):\n        trn_dat, trn_tgt = trn_df.iloc[trn_idx], target.iloc[trn_idx]\n        val_dat, val_tgt = trn_df.iloc[val_idx], target.iloc[val_idx]\n\n#\n# Define all XGboost parameters\n#\n        clf = XGBClassifier(n_estimators=n_estimators,\n                            max_depth=int(max_depth),\n                            objective=\"binary:logistic\",\n                            learning_rate=0.1,\n#                            subsample=max(min(subsample, 1), 0),\n#                            colsample_bytree=max(min(colsample_bytree, 1), 0),\n#                            max_delta_step=int(max_delta_step),\n                            max_delta_step=1,\n                            subsample=0.8,\n                            colsample_bytree=0.8,\n                            gamma=gamma,\n                            reg_alpha=reg_alpha,\n                            reg_lambda=reg_lambda,\n                            scale_pos_weight=scale_pos_weight,\n                            min_child_weight=min_child_weight,\n                            nthread=4)\n\n        # Upsample during cross validation to avoid having the same samples\n        # in both train and validation sets\n        # Validation set is not up-sampled to monitor overfitting\n        if increase:\n            # Get positive examples\n            pos = pd.Series(trn_tgt == 1)\n            # Add positive examples\n            trn_dat = pd.concat([trn_dat, trn_dat.loc[pos]], axis=0)\n            trn_tgt = pd.concat([trn_tgt, trn_tgt.loc[pos]], axis=0)\n            # Shuffle data\n            idx = np.arange(len(trn_dat))\n            np.random.shuffle(idx)\n            trn_dat = trn_dat.iloc[idx]\n            trn_tgt = trn_tgt.iloc[idx]\n\n        clf.fit(trn_dat, trn_tgt,\n                eval_set=[(trn_dat, trn_tgt), (val_dat, val_tgt)],\n                eval_metric=gini_xgb,\n                early_stopping_rounds=None,\n                verbose=False)\n\n        # Find best round for validation set\n        xgb_evals[:, fold_] = clf.evals_result_[\"validation_1\"][\"gini\"]\n        # Xgboost provides best round starting from 0 so it has to be incremented\n        best_round = np.argsort(xgb_evals[:, fold_])[::-1][0]\n\n    # Compute mean score and std\n    mean_eval = np.mean(xgb_evals, axis=1)\n    std_eval = np.std(xgb_evals, axis=1)\n    best_round = np.argsort(mean_eval)[::-1][0]\n\n    print(' Stopped after %d iterations with val-gini = %.6f +- %.6f' % ( best_round, mean_eval[best_round], std_eval[best_round]) )\n    if ( mean_eval[best_round] > GINIbest ):\n        GINIbest = mean_eval[best_round]\n\n    return mean_eval[best_round]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"7c75002c-22ef-462e-be82-9ccdf48e80eb","_uuid":"35645233c34693ed83218fae84cc75587db9bb1c"},"cell_type":"markdown","source":"I explained above why I went with random projection over entity embedding, but I encourage you to give the latter a try. I suggest you save the files with learned embeddings, so next time you just open them and skip the learning part.\n\nWe are dropping all __ps_calc__ variables."},{"metadata":{"_cell_guid":"a732a005-0170-4c17-80b2-a343e1862275","_uuid":"bd86ee869753e3b2a60349422ededea9d15429b4","collapsed":true,"trusted":true},"cell_type":"code","source":"GINIbest = -1.\n\n#ce = CE(strategy='random_projection', verbose=True)\nce = CE(strategy='entity_embedding', verbose=True)\n\nstart_time = timer(None)\n\ntrain_loader = pd.read_csv('../input/train.csv', dtype={'target': np.int8, 'id': np.int32})\ntrain = train_loader.drop(['target', 'id'], axis=1)\nprint('\\n Shape of raw train data:', train.shape)\ncol_to_drop = train.columns[train.columns.str.startswith('ps_calc_')]\ntrain.drop(col_to_drop, axis=1, inplace=True)\ntarget = train_loader['target']\ntrain_ids = train_loader['id'].values\n\ntest_loader = pd.read_csv('../input/test.csv', dtype={'id': np.int32})\ntest = test_loader.drop(['id'], axis=1)\nprint(' Shape of raw test data:', test.shape)\ntest.drop(col_to_drop, axis=1, inplace=True)\ntest_ids = test_loader['id'].values\n\n#n_train = train.shape[0]\n#train_test = pd.concat((train, test)).reset_index(drop=True)\ncol_to_embed = train.columns[train.columns.str.endswith('_cat')].astype(str).tolist()\nembed_train = train[col_to_embed].astype(np.str)\nembed_test = test[col_to_embed].astype(np.str)\ntrain.drop(col_to_embed, axis=1, inplace=True)\ntest.drop(col_to_embed, axis=1, inplace=True)\n\nprint('\\n Learning random projections - this will take less time than entity embedding ...')\n#print('\\n Learning entity embedding - this will take a while ...')\nce.fit(embed_train, target)\nembed_enc_train = ce.transform(embed_train)\nembed_enc_test = ce.transform(embed_test)\ntrn_df = pd.concat((train, embed_enc_train), axis=1)\nsub_df = pd.concat((test, embed_enc_test), axis=1)\nprint('\\n Shape of processed train data:', trn_df.shape)\nprint(' Shape of processed test data:', sub_df.shape)\n\ntimer(start_time)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"1f2f9b50-648f-4983-ab39-e91c186b9027","_uuid":"a23989cf37c4d031ec1eefe5fce7af00114f37b9"},"cell_type":"markdown","source":"Several things are worth noting here. First, the effective range of max_depth is 2-6. Since in that range the overfitting is less likely, I was brave enough to top *gamma* and *min_child_weight* at 5. All of this is done for the sake of time. However, a proper way would be to allow max_depth to be 8 (or even 10), in which case *gamma* and *min_child_weight* should be topping at 10 or so.\n\n*If you decide to uncomment the remaining three parameters here, the same must be done above in XGB_CV section.*"},{"metadata":{"_cell_guid":"6205264b-a383-4573-8a9e-444ff21caa6d","_uuid":"0a6bfe804173ac2f4591ccebb3c56e81f868da8c"},"cell_type":"markdown","source":"We are doing a little trick here. Since it is highly unlikely that 5-6 parameter search runs would be able to identify anything remotely close to optimal parameters, I am giving us a head-start by providing two parameter combinations that are known to give good scores.\n\nNote that these are specifically for random projection encoding. If you go with entity embedding, you'll want to delete this section and uncomment the whole paragraph underneath it."},{"metadata":{"_cell_guid":"3b5f748c-4a14-47eb-8c63-80db29f652b6","_uuid":"4109068a0793c6c6c0b14ac7f1095247e5246e02","trusted":true},"cell_type":"code","source":"XGB_BO = BayesianOptimization(XGB_CV, {\n                                     'max_depth': (2, 6.99),\n                                     'gamma': (0.1, 5),\n                                     'min_child_weight': (0, 5),\n                                     'scale_pos_weight': (1, 5),\n                                     'reg_alpha': (0, 10),\n                                     'reg_lambda': (1, 10),\n#                                     'max_delta_step': (0, 5),\n#                                     'subsample': (0.4, 1.0),\n#                                     'colsample_bytree' :(0.4, 1.0)\n                                    })\n\n#XGB_BO.explore({\n#              'max_depth':            [4, 4],\n#              'gamma':                [0.1511, 2.7823],\n#              'min_child_weight':     [2.4073, 2.6086],\n#              'scale_pos_weight':     [2.2281, 2.4993],\n#              'reg_alpha':            [8.0702, 6.9874],\n#              'reg_lambda':           [2.0126, 3.9598],\n#              'max_delta_step':       [1, 1],\n#              'subsample':            [0.8, 0.8],\n#              'colsample_bytree':     [0.8, 0.8],\n#              })\n\n# If you go with entitiy embedding, these are good starting points\n#XGB_BO.explore({\n#              'max_depth':            [4, 4],\n#              'gamma':                [2.8098, 2.1727],\n#              'min_child_weight':     [4.1592, 4.8113],\n#              'scale_pos_weight':     [2.4450, 1.7195],\n#              'reg_alpha':            [2.8601, 7.6995],\n#              'reg_lambda':           [6.5563, 2.6879],\n#              })\n","execution_count":1,"outputs":[]},{"metadata":{"_cell_guid":"154c018d-936f-40b7-ae3b-58eac23bf7f7","_uuid":"78a5c19e805a5efe53aae290c90662153dd2ee04"},"cell_type":"markdown","source":"We are doing only one random guess of parameters, which makes a total of 3 when combined with two exploratory groups above. Afterwards, only 2 optimization runs are done.\n\nA total number of random points (from **.explore** section + init_points) should be at least 10-15. I would consider 20 if you decide to include more than 6 parameters. n_iter should be in 30+ range to do proper parameter optimization."},{"metadata":{"_cell_guid":"f3fa4483-5410-4ee1-b5bc-a95b56ef9013","_uuid":"e6f40de99c5367b9b2f99b267c2d6bfce64ba0ae","collapsed":true,"trusted":true},"cell_type":"code","source":"print('-'*126)\n\nstart_time = timer(None)\nwith warnings.catch_warnings():\n    warnings.filterwarnings('ignore')\n    XGB_BO.maximize(init_points=1, n_iter=2, acq='ei', xi=0.0)\ntimer(start_time)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"a6b19e7b-6272-4c7e-bffe-d3c7e49a8c03","_uuid":"75b02617c1dd9041c60c6c27e1405e657f0b591d"},"cell_type":"markdown","source":"Here we print the summary and create a CSV file with grid results."},{"metadata":{"_cell_guid":"d353dc81-a869-4759-9191-7471de46070a","_uuid":"73b31c14afbf82b48101ed323952a439cf0072cd","collapsed":true,"trusted":true},"cell_type":"code","source":"print('-'*126)\nprint('\\n Final Results')\nprint(' Maximum XGBOOST value: %f' % XGB_BO.res['max']['max_val'])\nprint(' Best XGBOOST parameters: ', XGB_BO.res['max']['max_params'])\ngrid_file = 'Bayes-gini-5fold-XGB-target-enc-run-04-v1-grid.csv'\nprint(' Saving grid search parameters to %s' % grid_file)\nXGB_BO.points_to_csv(grid_file)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"6c9b60e7-93a0-4ab0-82ff-9221143b7a46","_uuid":"3fd9c895bbace20a56baf5dade545351f2a1533d"},"cell_type":"markdown","source":"Finally, we do the last XGBoost upsampling, but this time with larger **n_estimators** and smaller **learning_rate**. You should do 1000 for n_estimators even if you don't touch the learning rate. If you lower the learning rate further, definitely increase n_estimators to 1500-2000."},{"metadata":{"_cell_guid":"a5670fd1-196a-4373-b577-da7fed9e475a","_uuid":"bb96c111541d1582e004d9cf8f8e57f3f0c4d831","collapsed":true,"trusted":true},"cell_type":"code","source":"\nmax_depth = int(XGB_BO.res['max']['max_params']['max_depth'])\ngamma = XGB_BO.res['max']['max_params']['gamma']\nmin_child_weight = XGB_BO.res['max']['max_params']['min_child_weight']\n#max_delta_step = int(XGB_BO.res['max']['max_params']['max_delta_step'])\n#subsample = XGB_BO.res['max']['max_params']['subsample']\n#colsample_bytree = XGB_BO.res['max']['max_params']['colsample_bytree']\nscale_pos_weight = XGB_BO.res['max']['max_params']['scale_pos_weight']\nreg_alpha = XGB_BO.res['max']['max_params']['reg_alpha']\nreg_lambda = XGB_BO.res['max']['max_params']['reg_lambda']\n\nstart_time = timer(None)\nprint('\\n Making final prediction - this will take a while ...')\nn_splits = 5\nn_estimators = 800\nfolds = StratifiedKFold(n_splits=n_splits, random_state=1001)\nimp_df = np.zeros((len(trn_df.columns), n_splits))\nxgb_evals = np.zeros((n_estimators, n_splits))\noof = np.empty(len(trn_df))\nsub_preds = np.zeros(len(sub_df))\nincrease = True\nnp.random.seed(0)\n\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(target, target)):\n    trn_dat, trn_tgt = trn_df.iloc[trn_idx], target.iloc[trn_idx]\n    val_dat, val_tgt = trn_df.iloc[val_idx], target.iloc[val_idx]\n\n    clf = XGBClassifier(n_estimators=n_estimators,\n                        max_depth=max_depth,\n                        objective=\"binary:logistic\",\n                        learning_rate=0.02,\n#                        subsample=subsample,\n#                        colsample_bytree=colsample_bytree,\n#                        max_delta_step=max_delta_step,\n                        subsample=0.8,\n                        colsample_bytree=0.8,\n                        max_delta_step=1,\n                        gamma=gamma,\n                        min_child_weight=min_child_weight,\n                        reg_alpha=reg_alpha,\n                        reg_lambda=reg_lambda,\n                        scale_pos_weight=scale_pos_weight,\n                        nthread=4)\n    # Upsample during cross validation to avoid having the same samples\n    # in both train and validation sets\n    # Validation set is not up-sampled to monitor overfitting\n    if increase:\n        # Get positive examples\n        pos = pd.Series(trn_tgt == 1)\n        # Add positive examples\n        trn_dat = pd.concat([trn_dat, trn_dat.loc[pos]], axis=0)\n        trn_tgt = pd.concat([trn_tgt, trn_tgt.loc[pos]], axis=0)\n        # Shuffle data\n        idx = np.arange(len(trn_dat))\n        np.random.shuffle(idx)\n        trn_dat = trn_dat.iloc[idx]\n        trn_tgt = trn_tgt.iloc[idx]\n\n    clf.fit(trn_dat, trn_tgt,\n            eval_set=[(trn_dat, trn_tgt), (val_dat, val_tgt)],\n            eval_metric=gini_xgb,\n            early_stopping_rounds=None,\n            verbose=False)\n\n    # Keep feature importances\n    imp_df[:, fold_] = clf.feature_importances_\n\n    # Find best round for validation set\n    xgb_evals[:, fold_] = clf.evals_result_[\"validation_1\"][\"gini\"]\n    # Xgboost provides best round starting from 0 so it has to be incremented\n    best_round = np.argsort(xgb_evals[:, fold_])[::-1][0]\n\n    # Predict OOF and submission probas with the best round\n    oof[val_idx] = clf.predict_proba(val_dat, ntree_limit=best_round)[:, 1]\n    # Update submission\n    sub_preds += clf.predict_proba(sub_df, ntree_limit=best_round)[:, 1] / n_splits\n\n    # Display results\n    print(\"Fold %2d : %.6f @%4d / best score is %.6f @%4d\"\n          % (fold_ + 1,\n             eval_gini(val_tgt, oof[val_idx]),\n             n_estimators,\n             xgb_evals[best_round, fold_],\n             best_round))\n\nprint(\"Full OOF score : %.6f\" % eval_gini(target, oof))\n\n# Compute mean score and std\nmean_eval = np.mean(xgb_evals, axis=1)\nstd_eval = np.std(xgb_evals, axis=1)\nbest_round = np.argsort(mean_eval)[::-1][0]\n\nprint(\"Best mean score : %.6f + %.6f @%4d\"\n      % (mean_eval[best_round], std_eval[best_round], best_round))\n\nbest_gini = round(mean_eval[best_round], 6)\nimportances = sorted([(trn_df.columns[i], imp) for i, imp in enumerate(imp_df.mean(axis=1))],\n                     key=lambda x: x[1])\n\nfor f, imp in importances[::-1]:\n    print(\"%-34s : %10.4f\" % (f, imp))\n\ntimer(start_time)\n\nfinal_df = pd.DataFrame(test_ids, columns=['id'])\nfinal_df['target'] = sub_preds\n\nnow = datetime.now()\nsub_file = 'submission_5fold-xgb-upsampling-target-enc-01_' + str(best_gini) + '_' + str(now.strftime('%Y-%m-%d-%H-%M')) + '.csv'\nprint('\\n Writing submission: %s' % sub_file)\nfinal_df.to_csv(sub_file, index=False, float_format=\"%.9f\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}