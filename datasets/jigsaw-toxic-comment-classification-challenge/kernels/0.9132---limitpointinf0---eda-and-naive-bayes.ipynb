{"cells":[{"metadata":{"_uuid":"ae72475f0209930dd1947f72890558f0c17b746c"},"cell_type":"markdown","source":"## Import necessary modules and define helper functions"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport re\nimport string\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.tokenize import word_tokenize, sent_tokenize\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.model_selection import cross_val_score\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef clean_ColText(data, col, stem=True):\n    \"\"\"Takes dataframe and column name and returns a dataframe with cleaned strings in the form of a list of word tokens. \n    Stemming is an option.\"\"\"\n    df = data.copy()\n    df[col] = df[col].map(lambda x: re.sub('\\s+', ' ', x).strip())\n    table = str.maketrans('', '', string.punctuation)\n    df[col] = df[col].map(lambda x: x.translate(table)) #remove punctuation\n    df[col] = df[col].map(lambda x: x.lower()) #lowercase\n    df[col] = df[col].apply(word_tokenize) #tokenize\n    stop_words = set(stopwords.words('english'))\n    df[col] = df[col].map(lambda x: [y for y in x if not y in stop_words]) #remove stop words\n    df[col] = df[col].map(lambda x: [y for y in x if y not in [\"’\",\"’\",\"”\",\"“\",\"‘\",\"—\"]]) #remove smart quotes and other non alphanums\n    if stem:\n        porter = PorterStemmer()\n        df[col] = df[col].map(lambda x: [porter.stem(y) for y in x])\n        return df\n    return df\n\ndef stemIt(data, col):\n    df = data.copy()\n    porter = PorterStemmer()\n    df[col] = df[col].map(lambda x: [porter.stem(y) for y in x])\n    return df\n\ndef plot_wordcloud(text, title=None, max = 1000, size=(10,5), title_size=16):\n    \"\"\"plots wordcloud\"\"\"\n    wordcloud = WordCloud(max_words=max).generate(text)\n    plt.figure(figsize=size)\n    plt.title(title, size=title_size)\n    plt.imshow(wordcloud, interpolation='bilinear')\n    plt.axis(\"off\")\n    \ndef getNBModel(data, predictor, label, alpha=0.1):\n    y = data[label]\n    model = MultinomialNB(alpha=alpha)\n    scores = cross_val_score(model, X, y, cv=5)\n    model = model.fit(X, y)\n    return (label, model, scores) \n\nbp = '../input/'\nprint(os.listdir(bp))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a3ce49a19c462617a595a4dea1cabf6b62cd0896"},"cell_type":"markdown","source":"## Import data to dataframe"},{"metadata":{"trusted":true,"_uuid":"095dbf178cf3697917d64aed12d7faae4c4bb464","_kg_hide-input":false,"collapsed":true},"cell_type":"code","source":"df = pd.read_csv(bp + 'train.csv')\n#df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"abe94e84b9a0b7c9c4f768c40b40e2290955e65e"},"cell_type":"markdown","source":"## Clean comment_text"},{"metadata":{"trusted":true,"_uuid":"209082c13161c0b766712847ca382613186f2cbd","_kg_hide-input":false,"collapsed":true},"cell_type":"code","source":"df = clean_ColText(df, 'comment_text', stem=False)\n#df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f6f2ed7b5bc28e1f6551615e8e0272d327884e46"},"cell_type":"markdown","source":"## EDA on most toxic comments with total score >= 5"},{"metadata":{"trusted":true,"_uuid":"79feae66b0eb9a49e87ae331b70aad975fd36e8e","_kg_hide-input":false,"collapsed":true},"cell_type":"code","source":"#df_eda = df.copy()\n#df_eda['total'] = df_eda.iloc[:,2:].sum(axis=1)\n#df_eda[df_eda['total'] >=5].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7ba865d01e8a23efb5950f1e04620d243d233b04","_kg_hide-input":false,"collapsed":true},"cell_type":"code","source":"#txt = ' '.join(sum(list(df_eda[df_eda['total'] >=5]['comment_text']), []))\n#plot_wordcloud(txt, title='Toxic Comments', size=(10,5))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ca4dbbe40919398e7b2b75cca9569f4002324def"},"cell_type":"markdown","source":"## Look for strong correlations between different labels"},{"metadata":{"_kg_hide-input":false,"trusted":true,"_uuid":"963ff4fb63c05f63688931153dbe194e3f385134"},"cell_type":"code","source":"targets = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\ncorr = df[targets].corr()\nplt.figure(figsize=(20,10))\nsns.heatmap(corr, mask=np.zeros_like(corr, dtype=np.bool), cmap=sns.diverging_palette(220, 10, as_cmap=True),\n            square=True)\nplt.title(\"Toxic Comment Correlation between Types\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"793f94181213bb9df7b66de193929fb2e543d72e"},"cell_type":"markdown","source":"## Build model for each label and display cross validation scores"},{"metadata":{"_kg_hide-input":false,"trusted":true,"_uuid":"d53cd6d68447c4441090f966945451c70744b770"},"cell_type":"code","source":"df['text'] = df['comment_text'].map(lambda x: ' '.join(x))\nX = df[['text']]\ncount_vect = CountVectorizer()\ntfidf_transformer = TfidfTransformer()\nX = count_vect.fit_transform(X.text)\nX = tfidf_transformer.fit_transform(X)\n\nmodels_stats = []\nfor t in targets:\n    m_s = getNBModel(df, X, t)\n    models_stats.append(m_s)\n    print(m_s)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"trusted":true,"scrolled":true,"_uuid":"20ef1fd2cfc2298ccd5839ed89609f58287a5391","collapsed":true},"cell_type":"code","source":"df_test = pd.read_csv(bp + 'test.csv')\ndf_test = clean_ColText(df_test, 'comment_text', stem=False)\ndf_test['text'] = df_test['comment_text'].map(lambda x: ' '.join(x))\n\nX_test = df_test[['text']]\nX_test = count_vect.transform(X_test.text)\nX_test = tfidf_transformer.transform(X_test)\n\nfor x in models_stats:\n    df_test[x[0]] = x[1].predict_proba(X_test)[:,1]\n#df_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cec8f7f7aa0b262b51647bd11c6ea7b950c0de0f","_kg_hide-input":false,"collapsed":true},"cell_type":"code","source":"submit = df_test.drop(['comment_text', 'text'], axis=1)\nsubmit.to_csv('results.csv', index=False)\nprint('wrote to csv')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}