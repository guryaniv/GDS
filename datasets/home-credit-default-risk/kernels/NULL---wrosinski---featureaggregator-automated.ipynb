{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import gc\nimport glob\nimport os\nimport time\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\nfrom sklearn.model_selection import KFold, StratifiedKFold\ngc.enable()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"32b154eb84c106a5fd76581a005412b5d98be776"},"cell_type":"markdown","source":"## Automating Feature Engineering\n\nBuilding on top of _FeatureAggregator_, it is quite easy to automate the process of feature engineering through sampling aggregates. This way, with each run different features will be created, using different combinations of aggregates themselves or using sampled combinations of column aggregation, where each column has it's own aggregates sampled.\nFirst approach can thus be called __batch aggregation__ and the second __selected aggregation__, as was the case in _FeatureAggregator_ itself.\n\nWhole process requires only a few additional elements:\n1. List of all possible aggregates to be used in sampling. Either two different sets can be used for categorical and numerical variables or just one, from which aggregates will be chosen at random for both types of features. Because in case of __selected__ aggregation aggregates are sampled independently for each column, only one list of aggregates has to be chosen. __Batch__ aggregation can be done using two different lists. \n2. Parameter specifying whether __batch__ or __selected__ aggregation should be done, in this case it's the `use_selected` one.\n\nSteps of the process itself are as follows:\n1. Select aggregates to sample from and choose which type of aggregation should be used.\n2. Load the data.\n3. Preprocess the data, if needed.\n4. Sample aggregates:\n    1. __Batch aggregation__ - sample lists of aggregates for categorical and numerical variables. This can be done either once for all DFs or sampled once for every DF.\n    2. __Selected aggregation__ - sample combinations of columns/aggregates for each DF. Sampling for each DF is mandatory, as a subset of columns to be aggregated is also chosen at random and every DF contains different columns.\n5. Perform aggregations on each DF that should be aggregated.\n6. Merge aggregated DFs onto main train/test table.\n7. Train model.\n8. Predict and output submission or save predictions.\n\nAll those steps can be repeated **n** times and every time a different set of features should be created.\nIn fact, as an exercise, it would be nice to wrap the whole process in one big function (or not) and set a simple loop over it to perform the aggregations __n__ times.\n\n### Why is this useful?\n\nDifferent sets of features can capture different relationships between the target and the training features. On the other hand, performing all possible aggregations and training a model with a huge set of features may decrease it's accuracy. Number of models trained on different sets of features should perform better, when their predictions are averaged or stacking is used.\n\nAnother useful thing is the fact that this process almost does not require human supervision. Given enough computational resources, hundreds of models can be trained in an automated manner, all their parameters and predictions saved and in the end one submission generated.\n\nIf ones has a lot of features they consider really important, they can be added explicitly to a DataFrame in a very simple way and new random features will be created on top of those already used. Or one can even at random select a subset of their own important features and perform aggregations on them. \n\nFunctions themselves are pretty much data agnostic, as they only need a DF containing features and a list of aggregates. Using them on Home Credit data is just an example :).\n\nWhat is more, the random features themselves do not score so bad either, a random runs statistics:\n\n- batch: Mean KFold AUC: 0.7820\n- selected: Mean KFold AUC: 0.7843"},{"metadata":{"_uuid":"aa445d6ca770fadf0c8e548af520efc00a2e9dc5"},"cell_type":"markdown","source":"### 0. Set parameters"},{"metadata":{"trusted":true,"_uuid":"d75d22907014a3b2b6fcf7d39db815f9a72ecc01"},"cell_type":"code","source":"# Define whether all columns should be processed with sampled aggregates (False)\n# or sample columns and sample aggregates for each of them using selected\n# aggregations (True).\nuse_selected = True\n\n\n# Set data source.\ndata_src = '../input/'\n\n\n# 'mad' is very slow, so the second list excludes it.\naggs_all_num = ['mean', 'median', 'min', 'max', 'count', 'std', 'sem', 'sum', 'mad']\n\n# Default lists of aggregates to sample from, _num for numerical variables.\n# _cat for categorical variables.\naggs_medium_num = ['mean', 'median', 'min', 'max', 'count', 'std', 'sem', 'sum']\naggs_all_cat = ['mean', 'std', 'sum', 'median', 'count']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"123557cd8870ddbb2cb8271f16af1454de5650f3"},"cell_type":"markdown","source":"### Define functions, which will be used below to keep the script clean:"},{"metadata":{"trusted":true,"_uuid":"8f6eeeb69497a18636022bba94270855425ae682"},"cell_type":"code","source":"class FeatureAggregator(object):\n\n    \"\"\"Feature aggregator - automated feature aggregation method.\n    Two ways of usage, either selected aggregations can be applied onto\n    numerical and categorical columns or specific combinations of aggregates\n    can be set for each column.\n\n    # Arguments:\n        df: (pandas DataFrame), DataFrame to create features from.\n        aggregates_cat: (list), list containing aggregates for\n            categorical features\n        aggregates_num: (list), list containing aggregates for\n            numerical features.\n\n    \"\"\"\n\n    def __init__(self,\n                 df,\n                 aggregates_cat=['mean', 'std'],\n                 aggregates_num=['mean', 'std', 'sem', 'min', 'max']):\n\n        self.df = df.copy()\n        self.aggregates_cat = aggregates_cat\n        self.aggregates_num = aggregates_num\n\n    def process_features_batch(self,\n                               categorical_columns=None,\n                               categorical_int_columns=None,\n                               numerical_columns=None,\n                               to_group=['SK_ID_CURR'], prefix='BUREAU'):\n        \"\"\"Process, group features in batch.\n\n        # Arguments:\n            categorical_columns: (list), list of categorical columns, which need\n            to be label-encoded (factorized).\n            categorical_int_columns: (list), list of categorical columns, which\n            are already of integer type.\n            numerical_columns: (list), list of numerical columns.\n            to_group: (list), list of columns to group by.\n            prefix: (string), prefix for columns names.\n\n        # Returns:\n            df_cat/df_num: (pandas DataFrame), DataFrame with aggregated columns.\n\n        \"\"\"\n\n        assert isinstance(\n            to_group, list), 'Variable to group by must be of type list.'\n\n        if categorical_columns is not None:\n            assert len(categorical_columns) > 0, 'No columns to encode.'\n            self.categorical_features_factorize(categorical_columns)\n            df_cat = self.create_aggregates_set(\n                columns=categorical_columns,\n                aggregates=self.aggregates_cat,\n                to_group=to_group, prefix=prefix)\n            print('\\nAggregated df_cat shape: {}'.format(df_cat.shape))\n            return df_cat\n\n        if categorical_int_columns is not None:\n            assert len(categorical_int_columns) > 0, 'No columns to encode.'\n            df_cat = self.create_aggregates_set(\n                columns=categorical_int_columns,\n                aggregates=self.aggregates_cat,\n                to_group=to_group, prefix=prefix)\n            print('\\nAggregated df_cat int shape: {}'.format(df_cat.shape))\n            return df_cat\n\n        if numerical_columns is not None:\n            assert len(numerical_columns) > 0, 'No columns to encode.'\n            df_num = self.create_aggregates_set(\n                columns=numerical_columns,\n                aggregates=self.aggregates_num,\n                to_group=to_group, prefix=prefix)\n            print('\\nAggregated df_num shape: {}'.format(df_num.shape))\n            return df_num\n\n        return\n\n    def process_features_selected(self,\n                                  aggregations,\n                                  categorical_columns=None,\n                                  to_group=['SK_ID_CURR'], prefix='BUREAU'):\n        \"\"\"Process, group features for selected combinations of aggregates\n        and columns.\n\n        # Arguments:\n            categorical_columns: (list), list of categorical columns, which need\n            to be label-encoded (factorized).\n            to_group: (list), list of columns to group by.\n            prefix: (string), prefix for columns names.\n\n        # Returns:\n            df_agg: (pandas DataFrame), DataFrame with aggregated columns.\n\n        \"\"\"\n\n        assert isinstance(\n            to_group, list), 'Variable to group by must be of type list.'\n\n        if categorical_columns is not None:\n            # Provide categorical_columns argument if some features need to be factorized.\n            self.categorical_features_factorize(categorical_columns)\n\n        df_agg = self.create_aggregates_set(\n            aggregations=aggregations,\n            to_group=to_group, prefix=prefix)\n\n        print('\\nAggregated df_agg shape: {}'.format(df_agg.shape))\n\n        return df_agg\n\n    def create_aggregates_set(self,\n                              aggregations=None,\n                              columns=None,\n                              aggregates=None,\n                              to_group=['SK_ID_CURR'],\n                              prefix='BUREAU'):\n        \"\"\"Create selected aggregates.\n\n        # Arguments:\n            aggregations: (dict), dictionary specifying aggregates for selected columns.\n            columns: (list), list of columns to group for batch aggregation.\n            aggregates: (list), list of aggregates to apply on columns argument\n            for batch aggregation.\n            to_group: (list), list of columns to group by.\n            prefix: (string), prefix for columns names.\n\n        # Returns:\n            df_agg: (pandas DataFrame), DataFrame with aggregated columns.\n\n        \"\"\"\n\n        assert isinstance(\n            to_group, list), 'Variable to group by must be of type list.'\n\n        if aggregations is not None:\n            print('Selected aggregations:\\n{}\\n.'.format(aggregations))\n            df_agg = self.df.groupby(\n                to_group).agg(aggregations)\n\n        if columns is not None and aggregates is not None:\n            print('Batch aggregations on columns:\\n{}\\n.'.format(columns))\n            df_agg = self.df.groupby(\n                to_group)[columns].agg(aggregates)\n\n        df_agg.columns = pd.Index(['{}_{}_{}'.format(\n            prefix, c[0], c[1].upper()) for c in df_agg.columns.tolist()])\n        df_agg = df_agg.reset_index()\n\n        return df_agg\n\n    def get_column_types(self):\n        \"\"\"Select categorical (to be factorized), categorical integer and numerical\n        columns based on their dtypes. This facilitates proper grouping and aggregates selection for\n        different types of variables.\n        Categorical columns needs to be factorized, if they are not of\n        integer type.\n\n        # Arguments:\n            self.df: (pandas DataFrame), DataFrame to select variables from.\n\n        # Returns:\n            categorical_columns: (list), list of categorical columns which need factorization.\n            categorical_columns_int: (list), list of categorical columns of integer dtype.\n            numerical_columns: (list), list of numerical columns.\n        \"\"\"\n\n        categorical_columns = [\n            col for col in self.df.columns if self.df[col].dtype == 'object']\n        categorical_columns_int = [\n            col for col in self.df.columns if self.df[col].dtype == 'int']\n        numerical_columns = [\n            col for col in self.df.columns if self.df[col].dtype == 'float']\n\n        categorical_columns = [\n            x for x in categorical_columns if 'SK_ID' not in x]\n        categorical_columns_int = [\n            x for x in categorical_columns_int if 'SK_ID' not in x]\n\n        print('DF contains:\\n{} categorical object columns\\n{} categorical int columns\\n{} numerical columns.\\n'.format(\n            len(categorical_columns), len(categorical_columns_int), len(numerical_columns)))\n\n        return categorical_columns, categorical_columns_int, numerical_columns\n\n    def categorical_features_factorize(self, categorical_columns):\n        \"\"\"Factorize categorical columns, which are of non-number dtype.\n\n        # Arguments:\n            self.df: (pandas DataFrame), DataFrame to select variables from.\n            Transformation is applied inplace.\n\n        \"\"\"\n\n        print('\\nCategorical features encoding: {}'.format(categorical_columns))\n\n        for col in categorical_columns:\n            self.df[col] = pd.factorize(self.df[col])[0]\n\n        print('Categorical features encoded.\\n')\n\n        return\n\n    def check_and_save_file(self, df, filename, dst='../input/'):\n        \"\"\"Utility function to check if there isn't a file with the same name already.\n\n        # Arguments:\n            df: (pandas DataFrame), DataFrame to save.\n            filename: (string), filename to save DataFrame with.\n\n        \"\"\"\n\n        filename = '{}{}.pkl'.format(dst, filename)\n        if not os.path.isfile(filename):\n            print('Saving: {}'.format(filename))\n            df.to_pickle('{}'.format(filename))\n        return\n\n\ndef feature_aggregator_on_df(df,\n                             aggregates_cat,\n                             aggregates_num,\n                             to_group,\n                             prefix,\n                             suffix='basic',\n                             save=False,\n                             categorical_columns_override=None,\n                             categorical_int_columns_override=None,\n                             numerical_columns_override=None):\n    \"\"\"Wrapper for FeatureAggregator to process dataframe end-to-end using batch aggregation.\n    It takes lists of aggregates for categorical and numerical features, which are created for\n    selected column (to_group), by which data is grouped. In addition to that, prefix and suffix can\n    be provided to facilitate column naming.\n    _override arguments can be used if only selected subset of each type of columns should\n    be aggregated. If those are not provided, FeatureAggregator processes all columns for each type.\n\n        # Arguments:\n            aggregates_cat: (list), list of aggregates to apply to categorical features.\n            aggregates_num: (list), list of aggregates to apply to numerical features.\n            to_group: (list), list of columns to group by.\n            prefix: (string), prefix for column names.\n            suffix: (string), suffix for filename.\n            save: (boolean), whether to save processed DF.\n            categorical_columns_override: (list), list of categorical columns\n            to override default, inferred list.\n            categorical_int_columns_override: (list), list of categorical integer\n            columns to override default, inferred list.\n            numerical_columns_override: (list), list of numerical columns\n            to override default, inferred list.\n\n        # Returns:\n            to_return: (list of pandas DataFrames), DataFrames with aggregated columns,\n            one for each type of column types. This is due to the fact that not every\n            raw dataframe may contain all types of columns.\n\n        \"\"\"\n\n    assert isinstance(aggregates_cat, list), 'Aggregates must be of type list.'\n    assert isinstance(aggregates_num, list), 'Aggregates must be of type list.'\n\n    t = time.time()\n    to_return = []\n\n    column_base = ''\n    for i in to_group:\n        column_base += '{}_'.format(i)\n\n    feature_aggregator_df = FeatureAggregator(\n        df=df,\n        aggregates_cat=aggregates_cat,\n        aggregates_num=aggregates_num)\n\n    print('DF prefix: {}, suffix: {}'.format(prefix, suffix))\n    print('Categorical aggregates - {}'.format(aggregates_cat))\n    print('Numerical aggregates - {}'.format(aggregates_num))\n\n    df_cat_cols, df_cat_int_cols, df_num_cols = feature_aggregator_df.get_column_types()\n\n    if categorical_columns_override is not None:\n        print('Overriding categorical_columns.')\n        df_cat_cols = categorical_columns_override\n    if categorical_columns_override is not None:\n        print('Overriding categorical_int_columns.')\n        df_cat_int_cols = categorical_int_columns_override\n    if categorical_columns_override is not None:\n        print('Overriding numerical_columns.')\n        df_num_cols = numerical_columns_override\n\n    if len(df_cat_cols) > 0:\n        df_curr_cat = feature_aggregator_df.process_features_batch(\n            categorical_columns=df_cat_cols,\n            to_group=to_group, prefix=prefix)\n        if save:\n            feature_aggregator_df.check_and_save_file(\n                df_curr_cat, '{}_cat_{}_{}'.format(prefix, column_base, suffix))\n        to_return.append(df_curr_cat)\n        del df_curr_cat\n        gc.collect()\n\n    if len(df_cat_int_cols) > 0:\n        df_curr_cat_int = feature_aggregator_df.process_features_batch(\n            categorical_int_columns=df_cat_int_cols,\n            to_group=to_group, prefix=prefix)\n        if save:\n            feature_aggregator_df.check_and_save_file(\n                df_curr_cat_int, '{}_cat_int_{}_{}'.format(prefix, column_base, suffix))\n        to_return.append(df_curr_cat_int)\n        del df_curr_cat_int\n        gc.collect()\n\n    if len(df_num_cols) > 0:\n        df_curr_num = feature_aggregator_df.process_features_batch(\n            numerical_columns=df_num_cols,\n            to_group=to_group, prefix=prefix)\n        if save:\n            feature_aggregator_df.check_and_save_file(\n                df_curr_num, '{}_num_{}_{}'.format(prefix, column_base, suffix))\n        to_return.append(df_curr_num)\n        del df_curr_num\n        gc.collect()\n\n    print('\\nTime it took to create features on df: {:.3f}s'.format(\n        time.time() - t))\n\n    return to_return\n\n\ndef feature_aggregator_on_df_selected(df,\n                                      aggregations,\n                                      to_group,\n                                      prefix,\n                                      suffix='basic',\n                                      save=False):\n    \"\"\"Wrapper for FeatureAggregator to process dataframe end-to-end using selected\n    aggregates/columns combinations.\n    It takes dictionary of aggregates/columns combination for selected features,\n    which are created for selected column (to_group), by which data is grouped.\n    In addition to that, prefix and suffix can be provided to facilitate column naming.\n\n        # Arguments:\n            aggregations: (dict), dictionary containing combination of columns/aggregates.\n            to_group: (list), list of columns to group by.\n            prefix: (string), prefix for column names.\n            suffix: (string), suffix for filename.\n            save: (boolean), whether to save processed DF.\n\n        # Returns:\n            to_return: (list of pandas DataFrames), DataFrames with aggregated columns,\n            one for each type of column types. This is due to the fact that not every\n            raw dataframe may contain all types of columns.\n\n        \"\"\"\n\n    assert isinstance(\n        to_group, list), 'Variable to group by must be of type list.'\n\n    t = time.time()\n    to_return = []\n\n    column_base = ''\n    for i in to_group:\n        column_base += '{}_'.format(i)\n\n    feature_aggregator_df = FeatureAggregator(df=df)\n\n    print('DF prefix: {}, suffix: {}'.format(prefix, suffix))\n\n    df_cat_cols, df_cat_int_cols, df_num_cols = feature_aggregator_df.get_column_types()\n\n    if len(df_cat_cols) > 0:\n        df_aggs = feature_aggregator_df.process_features_selected(\n            aggregations=aggregations,\n            categorical_columns=df_cat_cols,\n            to_group=to_group,\n            prefix=prefix)\n    else:\n        df_aggs = feature_aggregator_df.process_features_selected(\n            aggregations=aggregations,\n            to_group=to_group,\n            prefix=prefix)\n\n    if save:\n        feature_aggregator_df.check_and_save_file(\n            df_aggs, '{}_selected_{}_{}'.format(prefix, column_base, suffix))\n\n    to_return.append(df_aggs)\n    del df_aggs\n    gc.collect()\n\n    print('\\nTime it took to create features on df: {:.3f}s'.format(\n        time.time() - t))\n\n    return to_return\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7b5a6005fd6cd483c12a408da8cbda91b30c3e3b"},"cell_type":"code","source":"def load_data(data_src):\n    \n    start_time = time.time()\n    \n    train = pd.read_csv('{}application_train.csv'.format(data_src)) \n    test = pd.read_csv('{}application_test.csv'.format(data_src))\n    print('Train and test tables loaded.')\n    \n    bureau = pd.read_csv('{}bureau.csv'.format(data_src))\n    bureau_bal = pd.read_csv('{}bureau_balance.csv'.format(data_src))\n    print('Bureau data loaded.')\n    \n    prev = pd.read_csv('{}previous_application.csv'.format(data_src))\n    print('Previous applications data loaded.')\n    \n    cred_card_bal = pd.read_csv('{}credit_card_balance.csv'.format(data_src))\n    print('Credit card balance loaded.')\n    \n    pos_cash_bal = pd.read_csv('{}POS_CASH_balance.csv'.format(data_src))\n    print('POS cash balance loaded.')\n    \n    ins = pd.read_csv('{}installments_payments.csv'.format(data_src))\n    print('Installments data loaded.')\n    \n    print('Time it took to load all the data: {:.4f}s\\n'.format(time.time() - start_time))\n    \n    return train, test, bureau, bureau_bal, prev, cred_card_bal, pos_cash_bal, ins\n\n\ndef sample_aggregates(aggs_cat_all, aggs_num_all):\n    \n    \"\"\"\n    Sample aggregates for categorical and numerical variables.\n    \n    # Arguments:\n        aggs_cat_all: (list), list of aggregates to sample from for categorical variables.\n        aggs_num_all: (list), list of aggregates to sample from for numerical variables.\n\n    # Returns:\n        aggs_cat: (list), list of selected aggregates for categorical variables.\n        aggs_num: (list), list of selected aggregates for categorical variables.\n    \"\"\"\n    \n    print('\\nSample aggregates for numerical and categorical variables.')\n    # Sample number of aggregates for numerical and categorical variables.\n    num_sampled_cat_aggregates = np.random.randint(1, len(aggs_cat_all))\n    num_sampled_num_aggregates = np.random.randint(1, len(aggs_num_all))\n    \n    # Sample aggregates for categorical variables from aggs_cat_all.\n    # Their number is equal to num_sampled_cat_aggregates.\n    aggs_cat = np.random.choice(aggs_cat_all, num_sampled_cat_aggregates, replace=False).tolist()\n    print('Selected aggregates for categorical variables: {}\\n'.format(aggs_cat))\n\n    # Sample aggregates for numerical variables from aggs_num_all.\n    # Their number is equal to num_sampled_num_aggregates.\n    aggs_num = np.random.choice(aggs_num_all, num_sampled_num_aggregates, replace=False).tolist()\n    print('Selected aggregates for numerical variables: {}'.format(aggs_num))\n    \n    return aggs_cat, aggs_num\n\n\ndef sample_selected_aggregations(df, aggs_list):\n    \n    \"\"\"\n    Sample combinations of columns/aggregates for selected aggregations.\n    \n    # Arguments:\n        aggs_list: (list), list of aggregates to sample from for all variables.\n\n    # Returns:\n        selected_aggregates: (dict), dictionary, where each key is a columns having it's own\n        list of aggregates.\n    \"\"\"\n    \n    \n    # Sample number of columns to aggregate in df.\n    # It is assumed that number of columns sampled will be higher than a half of all columns.\n    sampled_num_columns = np.random.randint(np.ceil(len(df.columns) * 0.5), (len(df.columns)))\n    # Sample columns from df, their number is equal to sampled_num_columns.\n    sampled_columns = np.random.choice(df.columns, sampled_num_columns, replace=False)\n    print('\\nSample aggregates for {} columns.'.format(len(sampled_columns)))\n\n    selected_aggregates = {}\n\n    # For each chosen column, select number of aggregates for it with sampled_column_num_aggregates.\n    # Then choose aggregates from aggs_list\n    # Save each entry into a dictionary which will specify aggregations in DF, where column is the key\n    # and value is a list of aggregates for this column.\n    for i in sampled_columns:\n        sampled_column_num_aggregates = np.random.randint(1, len(aggs_list))\n        sampled_column_aggregates = np.random.choice(aggs_list, sampled_column_num_aggregates, replace=False).tolist()\n        selected_aggregates[i] = sampled_column_aggregates\n        \n    print('Selected aggregations:\\n{}\\n'.format(selected_aggregates))\n    \n    return selected_aggregates\n\n\ndef categorical_features_factorize(X):\n\n    categorical_feats = [col for col in X.columns if X[col].dtype == 'object']\n    print('Categorical features encoding: {}'.format(categorical_feats))\n\n    for col in categorical_feats:\n        X[col] = pd.factorize(X[col])[0]\n\n    print('Categorical features encoded.\\n')\n\n    return X\n\n\ndef run_kfold_lgbm(X_train,\n                   y_train,\n                   X_test,\n                   model_params,\n                   n_folds=5,\n                   seed=1337):\n    \n    \n    # Prepare KFold split, Stratified works well in this competition.\n    # Parametrize it's seed to enable easy change of splits.\n    kf = StratifiedKFold(\n        n_splits=n_folds, shuffle=True, random_state=seed)\n    \n    # Subset features to eliminate irrelevant ones.\n    X_train = X_train[good_features]\n    X_test = X_test[good_features]\n    \n    # Assert that both train and test have the same set of columns.\n    assert np.all(X_train.columns == X_test.columns), '\\\n    Train and test sets must have the same set of columns.'\n\n    # Create oof sets for prediction storage.\n    # Create gbm_history for storage of best AUC per fold.\n    oof_train = np.zeros((X_train.shape[0]))\n    oof_test = np.zeros((X_test.shape[0], n_folds))\n    \n    gbm_history = {}\n    folds_auc = []\n\n    # Helper variable to index oof\n    i = 0\n    \n    for train_index, valid_index in kf.split(X=X_train, y=y_train):\n        assert len(np.intersect1d(train_index, valid_index)) == 0, '\\\n        Train and test indices must not overlap.'\n        \n        print('Running on fold: {}'.format(i + 1))\n\n        # Create train and validation sets based on KFold indices.\n        X_tr = X_train.iloc[train_index]\n        X_val = X_train.iloc[valid_index]\n        y_tr = y_train.iloc[train_index]\n        y_val = y_train.iloc[valid_index]\n\n        dtrain = lgb.Dataset(X_tr, y_tr)\n        dvalid = lgb.Dataset(X_val, y_val, reference=dtrain)\n\n        # Train LightGBM model, it's parameters can be changed easily\n        # through model_params function variable.\n        gbm = lgb.train(\n            params=model_params,\n            train_set=dtrain,\n            evals_result=gbm_history,\n            num_boost_round=10000,  # change to 10000 for proper training.\n            valid_sets=[dtrain, dvalid],\n            early_stopping_rounds=200,\n            verbose_eval=100)\n\n        # Predict validation and test data and store them in oof sets.\n        oof_train[valid_index] = gbm.predict(\n            X_val, num_iteration=gbm.best_iteration)\n        oof_test[:, i] = gbm.predict(X_test, num_iteration=gbm.best_iteration)\n        \n        # Show best AUC per fold based on GBM training history.\n        best_fold_auc = np.max(gbm_history['valid_1']['auc'])\n        folds_auc.append(best_fold_auc)\n        print('Best fold GBM AUC: {:.4f}\\n'.format(best_fold_auc))\n        \n        i += 1\n        \n    print('Mean KFold AUC: {:.4f}'.format(np.asarray(folds_auc).mean()))\n\n    return oof_train, oof_test","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"94dc385fc3f3f585bf0ac465bae25113dc5f2738"},"cell_type":"markdown","source":"### 1. Load data, all tables"},{"metadata":{"trusted":true,"_uuid":"4bd520cfc6204fe4cc758ff8107d49b39953368f"},"cell_type":"code","source":"train, test, bureau, bureau_bal, prev, cred_card_bal, pos_cash_bal, ins = load_data(data_src)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8e8e7a0183198fdb68b3615704e512714d21c024"},"cell_type":"markdown","source":"### 2. Do subtle processing to the tables:\n\n1. Drop unnecessary ID columns\n2. Replace values not making sense\n3. Create a few new features\n\nThanks to other kernels authors for those ideas (2. and 3.) !"},{"metadata":{"trusted":true,"_uuid":"4f9e7e2ca55c5a149649d5d89195163d084ba9a4"},"cell_type":"code","source":"ins = ins.drop(['SK_ID_PREV'], axis=1)\nprev = prev.drop(['SK_ID_PREV'], axis=1)\ncred_card_bal = cred_card_bal.drop(['SK_ID_PREV'], axis=1)\npos_cash_bal = pos_cash_bal.drop(['SK_ID_PREV'], axis=1)\n\n\nins['PAYMENT_PERC'] = ins['AMT_PAYMENT'] / ins['AMT_INSTALMENT']\nins['PAYMENT_DIFF'] = ins['AMT_INSTALMENT'] - ins['AMT_PAYMENT']\nins['DPD'] = ins['DAYS_ENTRY_PAYMENT'] - ins['DAYS_INSTALMENT']\nins['DBD'] = ins['DAYS_INSTALMENT'] - ins['DAYS_ENTRY_PAYMENT']\nins['DPD'] = ins['DPD'].apply(lambda x: x if x > 0 else 0)\nins['DBD'] = ins['DBD'].apply(lambda x: x if x > 0 else 0)\n\nprev['DAYS_FIRST_DRAWING'].replace(365243, np.nan, inplace= True)\nprev['DAYS_FIRST_DUE'].replace(365243, np.nan, inplace= True)\nprev['DAYS_LAST_DUE_1ST_VERSION'].replace(365243, np.nan, inplace= True)\nprev['DAYS_LAST_DUE'].replace(365243, np.nan, inplace= True)\nprev['DAYS_TERMINATION'].replace(365243, np.nan, inplace= True)\nprev['APP_CREDIT_PERC'] = prev['AMT_APPLICATION'] / prev['AMT_CREDIT']\n\n\n# Concatenate train and test.\nX = pd.concat([train, test], ignore_index=True, sort=False)\n\n# Encode categorical features in concatenated DF.\nX = categorical_features_factorize(X)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dcea07123a646f96d023c200fab680d89e8ea16c"},"cell_type":"markdown","source":"### 3. Sample aggregations:\n\n- If selected aggregations shall be used - for each DF select columns and their aggregates.\n- If batch aggregations shall be used - sample aggregates to use for all DFs."},{"metadata":{"trusted":true,"_uuid":"91c5838c3495b9220b9e41e1af7dc0368e8e07a1"},"cell_type":"code","source":"if use_selected:\n    selected_aggregates_bureau_bal = sample_selected_aggregations(bureau_bal, aggs_all_cat)\n    selected_aggregates_bureau = sample_selected_aggregations(bureau, aggs_all_cat)\n    selected_aggregates_ins = sample_selected_aggregations(ins, aggs_all_cat)\n    selected_aggregates_prev = sample_selected_aggregations(prev, aggs_all_cat)\n    selected_aggregates_cred_card = sample_selected_aggregations(cred_card_bal, aggs_all_cat)\n    selected_aggregates_pos_cash_bal = sample_selected_aggregations(pos_cash_bal, aggs_all_cat)\nelse:\n    aggs_cat, aggs_num = sample_aggregates(aggs_medium_num, aggs_all_cat)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ea50119a20cd66974a7d81318c497bed1fb638ef"},"cell_type":"markdown","source":"### 4. Perform aggregations and merge processed DF onto train/test:"},{"metadata":{"trusted":true,"_uuid":"a1a214e6bb2afe8de207ea1a153abbf0ee510c48"},"cell_type":"code","source":"if use_selected:\n    bureau_bal_dfs = feature_aggregator_on_df_selected(\n        bureau_bal,\n        selected_aggregates_bureau_bal,\n        to_group=['SK_ID_BUREAU'],\n        prefix='bureau_bal', suffix='basic_selected', save=False)\n\n    bureau_ = bureau.merge(\n        bureau_bal_dfs[0],\n        how='left',\n        on='SK_ID_BUREAU', copy=False)\n    bureau_dfs = feature_aggregator_on_df_selected(\n        bureau_,\n        selected_aggregates_bureau,\n        to_group=['SK_ID_CURR'],\n        prefix='bureau', suffix='basic_selected', save=False)\n\n\n    ins_dfs = feature_aggregator_on_df_selected(\n        ins,\n        selected_aggregates_ins,\n        to_group=['SK_ID_CURR'],\n        prefix='ins', suffix='basic_selected', save=False)\n\n\n    prev_dfs = feature_aggregator_on_df_selected(\n        prev,\n        selected_aggregates_prev,\n        to_group=['SK_ID_CURR'],\n        prefix='prev', suffix='basic_selected', save=False)\n\n\n    cred_card_bal_dfs = feature_aggregator_on_df_selected(\n        cred_card_bal,\n        selected_aggregates_cred_card,\n        to_group=['SK_ID_CURR'],\n        prefix='cred_card_bal', suffix='basic_selected', save=False)\n\n\n    pos_cash_bal_dfs = feature_aggregator_on_df_selected(\n        pos_cash_bal,\n        selected_aggregates_pos_cash_bal,\n        to_group=['SK_ID_CURR'],\n        prefix='pos_cash_bal', suffix='basic_selected', save=False)\n\n\n    X = X.merge(bureau_dfs[0], how='left', on='SK_ID_CURR', copy=False)\n    X = X.merge(ins_dfs[0], how='left', on='SK_ID_CURR', copy=False)\n    X = X.merge(prev_dfs[0], how='left', on='SK_ID_CURR', copy=False)\n    X = X.merge(cred_card_bal_dfs[0], how='left', on='SK_ID_CURR', copy=False)\n    X = X.merge(pos_cash_bal_dfs[0], how='left', on='SK_ID_CURR', copy=False)\n    \nelse:\n    \n    bureau_bal_dfs = feature_aggregator_on_df(\n        bureau_bal, aggs_cat, aggs_num, ['SK_ID_BUREAU'], 'bureau_bal', 'basic', save=False)\n\n    bureau_ = bureau.merge(bureau_bal_dfs[0], how='left', on='SK_ID_BUREAU', copy=False)\n    bureau_ = bureau_.merge(bureau_bal_dfs[1], how='left', on='SK_ID_BUREAU', copy=False)\n    bureau_dfs = feature_aggregator_on_df(\n        bureau_, aggs_cat, aggs_num, ['SK_ID_CURR'], 'bureau', 'basic', save=False)\n\n\n    ins_dfs = feature_aggregator_on_df(\n        ins, aggs_cat, aggs_num, ['SK_ID_CURR'], 'ins', 'basic', save=False)\n\n\n    prev_dfs = feature_aggregator_on_df(\n        prev, aggs_cat, aggs_num, ['SK_ID_CURR'], 'prev', 'basic', save=False)\n\n\n    cred_card_bal_dfs = feature_aggregator_on_df(\n        cred_card_bal, aggs_cat, aggs_num, ['SK_ID_CURR'], 'cred_card_bal', 'basic', save=False)\n\n\n    pos_cash_bal_dfs = feature_aggregator_on_df(\n        pos_cash_bal, aggs_cat, aggs_num, ['SK_ID_CURR'], 'pos_cash_bal', 'basic', save=False)\n\n\n    X = X.merge(bureau_dfs[0], how='left', on='SK_ID_CURR', copy=False)\n    X = X.merge(bureau_dfs[1], how='left', on='SK_ID_CURR', copy=False)\n    X = X.merge(bureau_dfs[2], how='left', on='SK_ID_CURR', copy=False)\n\n    X = X.merge(ins_dfs[0], how='left', on='SK_ID_CURR', copy=False)\n    X = X.merge(ins_dfs[1], how='left', on='SK_ID_CURR', copy=False)\n\n    X = X.merge(prev_dfs[0], how='left', on='SK_ID_CURR', copy=False)\n    X = X.merge(prev_dfs[1], how='left', on='SK_ID_CURR', copy=False)\n    X = X.merge(prev_dfs[2], how='left', on='SK_ID_CURR', copy=False)\n\n    X = X.merge(cred_card_bal_dfs[0], how='left', on='SK_ID_CURR', copy=False)\n    X = X.merge(cred_card_bal_dfs[1], how='left', on='SK_ID_CURR', copy=False)\n    X = X.merge(cred_card_bal_dfs[2], how='left', on='SK_ID_CURR', copy=False)\n\n    X = X.merge(pos_cash_bal_dfs[0], how='left', on='SK_ID_CURR', copy=False)\n    X = X.merge(pos_cash_bal_dfs[1], how='left', on='SK_ID_CURR', copy=False)\n    X = X.merge(pos_cash_bal_dfs[2], how='left', on='SK_ID_CURR', copy=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d92623de30a5e81a0ea09e5e7bf1f3f588775d8f"},"cell_type":"markdown","source":"### 5. Make train/test ready to train model:"},{"metadata":{"trusted":true,"_uuid":"953c22194af72b2a13823a70bb4f0f7b52952bf6"},"cell_type":"code","source":"# Split data into train and test once again, based on availability of TARGET variable.\nX_train = X[X['TARGET'].notnull()]\nX_test = X[X['TARGET'].isnull()]\n\n# Select TARGET and create a new variable for it, useful for model training.\ny_train = X_train.TARGET\n\n# Remove X (concatenated DF), as it will not be needed anymore.\ndel X\ngc.collect()\n\n# Select only features relevant to the model, do not use ID or index ones!\ngood_features = [x for x in X_train.columns if x not in ['TARGET','SK_ID_CURR','SK_ID_BUREAU','SK_ID_PREV','index']]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e792b87db735c95ec16369939a79ed5cb39f4cd2"},"cell_type":"markdown","source":"### 6. Train the LGBM model:"},{"metadata":{"trusted":true,"_uuid":"7d4d432cdc187513e1e8cafa3a9fa19d60714ba0"},"cell_type":"code","source":"gbm_params = {\n    'objective': 'binary',\n    'boosting_type': 'gbdt',\n    'nthread': 6,\n    'learning_rate': 0.05,  # 02,\n    'num_leaves': 20,\n    'colsample_bytree': 0.9497036,\n    'subsample': 0.8715623,\n    'subsample_freq': 1,\n    'max_depth': 8,\n    'reg_alpha': 0.041545473,\n    'reg_lambda': 0.0735294,\n    'min_split_gain': 0.0222415,\n    'min_child_weight': 60, # 39.3259775,\n    'seed': 0,\n    'verbose': -1,\n    'metric': 'auc',\n}\n\n\noof_train, oof_test = run_kfold_lgbm(X_train, y_train, X_test, gbm_params)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"14f6e5e9d0ac257eaf537391f23c106546943170"},"cell_type":"markdown","source":"### 7. Output submission:"},{"metadata":{"_uuid":"b1a82ac54a7c5b20c9cc7ff7ac952b968c8679f5","trusted":true},"cell_type":"code","source":"# Take mean of fold predictions for the test data.\nsubmission_preds = oof_test.mean(axis=1)\n\n# Prepare submission format and save it.\nsubmission_df = X_test[['SK_ID_CURR']].copy()\nsubmission_df['TARGET'] = submission_preds\nsubmission_df[['SK_ID_CURR', 'TARGET']].to_csv('submission.csv', index= False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}