{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f1f49cda30dc112441753df61c2a58df1b66c4e2"},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3c00d993dc1ee290a2dbd20830f57caff708f36b"},"cell_type":"code","source":"train.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a28a51b0e9d815d36819bec423d80b9b53ebc294"},"cell_type":"code","source":"rowsum = train.iloc[:,2:].sum(axis=1)\ntrain['clean'] = (rowsum==0)\ntrain['clean'].sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9ac65a120e9be7e166b85c3482569262463a7293"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fb3564a3c8a446687c2c905e2900baa63d4f1be4"},"cell_type":"code","source":"total = train.iloc[:,2:].sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1d61c966a08d31a450cd700e97bf8c006901a6e5"},"cell_type":"code","source":"total","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"05743d82e0e120d7eeb82ac0c449628b86c34761"},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns \nimport plotly.plotly as py\nimport plotly.graph_objs as go\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"739864363a2b16d7d74e9c8b3cfb38ced88f8ef3"},"cell_type":"code","source":"plt.figure(figsize=(10,8))\nsns.barplot(total.index, total.values, palette= 'dark' )\nplt.title('Class Frequency')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c87c6c4a1603a96da4d9e8da706cc1d8db921eb7"},"cell_type":"markdown","source":"<h3> Multi-tagging </h3>"},{"metadata":{"trusted":true,"_uuid":"37a277b2e8d2a5f62d5b20ac0b7a12e810be49b4"},"cell_type":"code","source":"multi_tags = rowsum.value_counts()\nmulti_tags","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9e53df91a072328c45ce7d4cc200878a12d33313"},"cell_type":"code","source":"plt.figure(figsize=(8,4))\nax = sns.barplot(multi_tags.index, multi_tags.values,palette='dark')\nplt.title('Number of multi-tags in a comment') ; plt.xlabel('Number of tags'); plt.ylabel('Number of comments');\nrects = ax.patches\nlabels = multi_tags.values\nfor rect, label in zip(rects, labels):\n    height = rect.get_height()\n    ax.text(rect.get_x() + rect.get_width()/2, height + 5, label, ha='center', va='bottom')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2f772cfaca316b3c1af6948e47448a50061855e2"},"cell_type":"code","source":"temp = train.iloc[:,2:-1]\ncorr = temp.corr()\nplt.figure(figsize=(10,8))\n\nsns.heatmap(corr,annot= True,xticklabels=corr.columns.values,yticklabels=corr.columns.values,)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c26a799bd86de05182052c1d9184000544864892"},"cell_type":"code","source":"import scipy.stats as ss\ndef cramers_corrected_stat(confusion_matrix):\n    \"\"\" calculate Cramers V statistic for categorial-categorial association.\n        uses correction from Bergsma and Wicher, \n        Journal of the Korean Statistical Society 42 (2013): 323-328\n    \"\"\"\n    chi2 = ss.chi2_contingency(confusion_matrix)[0]\n    n = confusion_matrix.sum().sum()\n    phi2 = chi2/n\n    r,k = confusion_matrix.shape\n    phi2corr = max(0, phi2 - ((k-1)*(r-1))/(n-1))    \n    rcorr = r - ((r-1)**2)/(n-1)\n    kcorr = k - ((k-1)**2)/(n-1)\n    return np.sqrt(phi2corr / min( (kcorr-1), (rcorr-1)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0aee9921b40a1bd009051bc686e97d9fdf14016f"},"cell_type":"code","source":"col1=\"toxic\"\ncol2=\"severe_toxic\"\nconfusion_matrix = pd.crosstab(temp[col1], temp[col2])\nprint(\"Confusion matrix between toxic and severe toxic:\")\nprint(confusion_matrix)\nnew_corr=cramers_corrected_stat(confusion_matrix)\nprint(\"The correlation between Toxic and Severe toxic using Cramer's stat=\",new_corr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"59722896dfe58f3fbb445c58b9c0bea629b54bd9"},"cell_type":"code","source":"print(\"Some examples : \\n\")\nprint(\"Toxic : \\n\")\nprint(\"\\n1.  \"+train[train.toxic ==1].iloc[4,1])\nprint(\"\\nSevere Toxic : \\n\")\nprint(\"\\n1.  \"+train[train.severe_toxic ==1].iloc[4,1])\nprint(\"\\nThreat : \\n\")\nprint(\"\\n1.  \"+train[train.threat ==1].iloc[4,1])\nprint(\"\\nObscene : \\n\")\nprint(\"\\n1.  \"+train[train.obscene ==1].iloc[4,1])\nprint(\"\\nIdentity Hate : \\n\")\nprint(\"\\n1.  \"+train[train.identity_hate ==1].iloc[4,1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4ae01c5372c3f08905f3c79b7ca6d5fbcea1ff0d"},"cell_type":"code","source":"from wordcloud import WordCloud ,STOPWORDS\nfrom PIL import Image\nimport matplotlib_venn as venn","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5fd5beb8903a4add8adf340574177523fa45df90"},"cell_type":"code","source":"stopword=set(STOPWORDS)\nsubset=train[train.clean==True]\ntext=subset.comment_text.values\nwc= WordCloud(background_color=\"black\",max_words=2000,stopwords=stopword)\nwc.generate(\" \".join(text))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4c6acf66f49082e0df350e65c8854f03f7edbe1b"},"cell_type":"code","source":"\nplt.figure(figsize=(12,8))\nplt.axis(\"off\")\nplt.title(\"Words frequented in Clean Comments\", fontsize=20)\nplt.imshow(wc.recolor(colormap= 'viridis' , random_state=17), alpha=0.98)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b624be1a5b17d20a6f9491c6681dbf16f624f62f"},"cell_type":"code","source":"subset=train[train.severe_toxic==True]\ntext=subset.comment_text.values\nwc= WordCloud(background_color=\"black\",max_words=2000,stopwords=stopword)\nwc.generate(\" \".join(text))\nplt.figure(figsize=(12,8))\nplt.axis(\"off\")\nplt.title(\"Words frequented in Severe Toxic Comments\", fontsize=20)\nplt.imshow(wc.recolor(colormap= 'viridis' , random_state=17), alpha=0.98)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"89e71c7a0282c8a808604f6b109d816a64d7e31b"},"cell_type":"code","source":"subset=train[train.threat==True]\ntext=subset.comment_text.values\nwc= WordCloud(background_color=\"black\",max_words=2000,stopwords=stopword)\nwc.generate(\" \".join(text))\nplt.figure(figsize=(12,8))\nplt.axis(\"off\")\nplt.title(\"Words frequented in Threat Comments\", fontsize=20)\nplt.imshow(wc.recolor(colormap= 'viridis' , random_state=17), alpha=0.98)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"69001235b884d9edd35940dac9b56322415ab04e"},"cell_type":"code","source":"data = pd.concat([train.iloc[:,0:2], test.iloc[:,0:2]])\ndata = data.reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2b5e376341f390f8834573c0da17b7c3da6e273f"},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"14602b2d697d252cb199d07777357b936d948799"},"cell_type":"code","source":"import string\nimport re    #for regex\nimport nltk\nfrom nltk.corpus import stopwords\nimport spacy\nfrom nltk import pos_tag\nfrom nltk.stem.wordnet import WordNetLemmatizer \nfrom nltk.tokenize import word_tokenize\n# Tweet tokenizer does not split at apostophes which is what we want\nfrom nltk.tokenize import TweetTokenizer   \n\n\n#FeatureEngineering\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, HashingVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.base import BaseEstimator, ClassifierMixin\nfrom sklearn.utils.validation import check_X_y, check_is_fitted\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nfrom sklearn.metrics import log_loss\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ce253652fa14308c2d1fabcbefc3be30fcc628f4"},"cell_type":"code","source":"data['count_sent'] = data[\"comment_text\"].apply(lambda x : len(re.findall(\"\\n\",str(x))) + 1 )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"aa301bb87a35f4fabcc0d1e835ea57f235e84ccf"},"cell_type":"code","source":"data['count_words'] = data[\"comment_text\"].apply(lambda x : len(str.split(x))) \n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"01009968217a744f505c7f691b45f7d913fbc314"},"cell_type":"code","source":"data['count_unique_words'] = data[\"comment_text\"].apply(lambda x : len(set(str.split(x)))) \ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1983058c5f94bc3b6a29282122ea35ae500686db"},"cell_type":"code","source":"data['count_letters'] = data['comment_text'].apply(lambda x : len(str(x)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"47b4ccfab6b2fb6ba0dba3eefe1d93d5ab6efb4b"},"cell_type":"code","source":"data['count_puntuations'] = data['comment_text'].apply( lambda x : len([p for p in str(x) if p in string.punctuation]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"784bcfcedb56fb9b0eee6f9c0728e0bbb1be1900"},"cell_type":"code","source":"data['count_word_upper'] = data[\"comment_text\"].apply(lambda x : len([i for i in str(x) if i.isupper()   ]) )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f76350e8fb5b2d95ff1fa5e288cdaa9f914b31bb"},"cell_type":"code","source":"data['count_words_title'] = data['comment_text'].apply(lambda x : len([j for j in str(x) if j.istitle() ]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bb4a57538f0281dc2c4d75ed8f177cc23e237690"},"cell_type":"code","source":"eng_stopwords = set(stopwords.words(\"english\"))\ndata['count_stopwords'] = data['comment_text'].apply(lambda x : len([i for i in str(x).lower().split() if i in eng_stopwords])) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3493b0005a1307bd641273a0039760dd1c390d71"},"cell_type":"code","source":"data['mean_word_length'] = data[\"comment_text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d04b99296fe8564a8716b759eaab4c5d3eddd012"},"cell_type":"code","source":"data['word_unique_percent']=data['count_unique_words']*100/data['count_words']\ndata['punct_percent']=data['count_puntuations']*100/data['count_words']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a4ede4d8522de51675b32c83604dcbb0d9a81b06"},"cell_type":"code","source":"data['count_exclamation_marks'] = data['comment_text'].apply(lambda comment: comment.count('!'))\ndata['count_question_marks'] = data['comment_text'].apply(lambda comment: comment.count('?'))\ndata['count_symbols'] = data['comment_text'].apply(\n    lambda comment: sum(comment.count(w) for w in '*&$%'))\ndata['count_smilies'] = data['comment_text'].apply(\n    lambda comment: sum(comment.count(w) for w in (':-)', ':)', ';-)', ';)')))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5d9a6240a9c1b6e108d2e885ff0fe36d32b2c7f4"},"cell_type":"code","source":"data['ip']=data[\"comment_text\"].apply(lambda x: re.findall(\"\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\",str(x)))\n#count of ip addresses\ndata['count_ip']=data[\"ip\"].apply(lambda x: len(x))\n\n#links\ndata['link']=data[\"comment_text\"].apply(lambda x: re.findall(\"http://.*com\",str(x)))\n#count of links\ndata['count_links']=data[\"link\"].apply(lambda x: len(x))\n\n#article ids\ndata['article_id']=data[\"comment_text\"].apply(lambda x: re.findall(\"\\d:\\d\\d\\s{0,5}$\",str(x)))\ndata['article_id_flag']=data.article_id.apply(lambda x: len(x))\n\n#username\n##              regex for     Match anything with [[User: ---------- ]]\n# regexp = re.compile(\"\\[\\[User:(.*)\\|\")\ndata['username']=data[\"comment_text\"].apply(lambda x: re.findall(\"\\[\\[User(.*)\\|\",str(x)))\n#count of username mentions\ndata['count_usernames']=data[\"username\"].apply(lambda x: len(x))\n#check if features are created\n#df.username[df.count_usernames>0]\n\n# Leaky Ip\ncv = CountVectorizer()\ncount_feats_ip = cv.fit_transform(data[\"ip\"].apply(lambda x : str(x)))\n\n\n# Leaky usernames\n\ncv = CountVectorizer()\ncount_feats_user = cv.fit_transform(data[\"username\"].apply(lambda x : str(x)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"306027d31847119b83b965f50298cda46b42700e"},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5a28ee9e87d5a815e46ccadcaa9be4f233e4055e"},"cell_type":"code","source":"data.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bfdedbcf793c42c0f46b532b33a0eaae4dcb9cd6"},"cell_type":"code","source":"def clean_text(text):\n    text = text.lower()\n    text = re.sub(r\"what's\", \"what is \", text)\n    text = re.sub(r\"\\'s\", \" \", text)\n    text = re.sub(r\"\\'ve\", \" have \", text)\n    text = re.sub(r\"can't\", \"cannot \", text)\n    text = re.sub(r\"n't\", \" not \", text)\n    text = re.sub(r\"i'm\", \"i am \", text)\n    text = re.sub(r\"\\'re\", \" are \", text)\n    text = re.sub(r\"\\'d\", \" would \", text)\n    text = re.sub(r\"\\'ll\", \" will \", text)\n    text = re.sub(r\"\\'scuse\", \" excuse \", text)\n    text = re.sub('\\W', ' ', text)\n    text = re.sub('\\s+', ' ', text)\n    text = text.strip(' ')\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"95ee9957920b5b87064eab0f4346b7bf8908a0c6"},"cell_type":"code","source":"data['comment_text'] = data['comment_text'].map(lambda com : clean_text(com))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"acb5c175c4fd6818d49377b685c0fc79c405e1c7"},"cell_type":"code","source":"data['comment_text'].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3f68500dd694f22b5257afec6e92451eb648c28c"},"cell_type":"code","source":"train_feats=data.iloc[0:len(train),]\ntest_feats=data.iloc[len(train):,]\n#join the tags\ntrain_tags=train.iloc[:,2:]\ntrain_feats=pd.concat([train_feats,train_tags],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ff358f3a581bd5fe7dc0592e65a3bf7954d3831b"},"cell_type":"code","source":"train_feats.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"540e58bc1d3ad8a89d8ed85fbcd12c5ddce9ab9c"},"cell_type":"code","source":"X = train_feats.comment_text\ntest_X = test_feats.comment_text\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0bccb6e688453a07053a6003ca049c6af39fe032"},"cell_type":"code","source":"print(X.shape, test_X.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"49db7acbdb87ad29d1459afc8daff9b149ce5e59"},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features=5000,stop_words='english')\nvect","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"55baa130b480da559e8100fb5b825273d993429b"},"cell_type":"code","source":"X_dtm = vect.fit_transform(X)\n# examine the document-term matrix created from X_train\nX_dtm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d276b0ff8e2dc41b85e9d53333b27ffcc26bd428"},"cell_type":"code","source":"test_X_dtm = vect.transform(test_X)\n# examine the document-term matrix from X_test\ntest_X_dtm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a042adfd8ecc113a42e425e20802c0e7f7f4bf56"},"cell_type":"code","source":"cols_target = ['count_sent', 'count_words', 'count_unique_words',\n       'count_letters', 'count_puntuations', 'count_word_upper',\n       'count_words_title', 'count_stopwords', 'mean_word_length',\n       'word_unique_percent', 'punct_percent', 'count_exclamation_marks',\n       'count_question_marks', 'count_symbols', 'count_smilies']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"78d787c463a1b45f58f1ec61ba2a944b8a37162b"},"cell_type":"code","source":"class NbSvmClassifier(BaseEstimator, ClassifierMixin):\n    def __init__(self, C=1.0, dual=False, n_jobs=1):\n        self.C = C\n        self.dual = dual\n        self.n_jobs = n_jobs\n\n    def predict(self, x):\n        # Verify that model has been fit\n        check_is_fitted(self, ['_r', '_clf'])\n        return self._clf.predict(x.multiply(self._r))\n\n    def predict_proba(self, x):\n        # Verify that model has been fit\n        check_is_fitted(self, ['_r', '_clf'])\n        return self._clf.predict_proba(x.multiply(self._r))\n\n    def fit(self, x, y):\n        # Check that X and y have correct shape\n        y = y.values\n        x, y = check_X_y(x, y, accept_sparse=True)\n\n        def pr(x, y_i, y):\n            p = x[y==y_i].sum(0)\n            return (p+1) / ((y==y_i).sum()+1)\n\n        self._r = sparse.csr_matrix(np.log(pr(x,1,y) / pr(x,0,y)))\n        x_nb = x.multiply(self._r)\n        self._clf = LogisticRegression(C=self.C, dual=self.dual, n_jobs=self.n_jobs).fit(x_nb, y)\n        return self","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"03359456818d030af9ed1c2d7f8afad0146a3fac"},"cell_type":"code","source":"target_x=train_feats[cols_target]\n# target_x\n\nTARGET_COLS=['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\ntarget_y=train_tags[TARGET_COLS]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0c1c33fa31920470dab688c56861747a0e49d22f"},"cell_type":"code","source":"print(\"Using only Indirect features\")\nmodel = LogisticRegression(C=3)\nX_train, X_valid, y_train, y_valid = train_test_split(target_x, target_y, test_size=0.33, random_state=2018)\ntrain_loss = []\nvalid_loss = []\nimportance=[]\npreds_train = np.zeros((X_train.shape[0], len(y_train)))\npreds_valid = np.zeros((X_valid.shape[0], len(y_valid)))\nfor i, j in enumerate(TARGET_COLS):\n    print('Class:= '+j)\n    model.fit(X_train,y_train[j])\n    preds_valid[:,i] = model.predict_proba(X_valid)[:,1]\n    preds_train[:,i] = model.predict_proba(X_train)[:,1]\n    train_loss_class=log_loss(y_train[j],preds_train[:,i])\n    valid_loss_class=log_loss(y_valid[j],preds_valid[:,i])\n    print('Trainloss=log loss:', train_loss_class)\n    print('Validloss=log loss:', valid_loss_class)\n    importance.append(model.coef_)\n    train_loss.append(train_loss_class)\n    valid_loss.append(valid_loss_class)\nprint('mean column-wise log loss:Train dataset', np.mean(train_loss))\nprint('mean column-wise log loss:Validation dataset', np.mean(valid_loss))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"86a2cb2296595dc4007f11b128b392415c883998"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"31d15bccd827267c6334b2f6138d73cae30551d5"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ee372957c2e44138922d44c6607a4a71e5f28e77"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"afabb522b1768e7ba2bab6c7a1c26206c663156f"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}