{"cells":[
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "# this version gave a 0.96xxx overall score\n# special handling of data was required\n# 03/01/2016\n\nimport os\nimport sys\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.cross_validation import train_test_split, cross_val_score\nfrom sklearn import preprocessing\nfrom sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve\n\nsys.path.append(\"C:/Continuum/Anaconda3/Lib/site-packages/xgboost-master/python-package/\")\nsys.path.append(\"C:/Continuum/Anaconda3/Lib/site-packages/xgboost-master/python-package/xgboost/\")\nsys.path.append(\"C:/Continuum/Anaconda3/Lib/site-packages/xgboost-master/windows/x64/Release/\")\n\nimport xgboost as xgb\nimport pickle\nimport matplotlib.pyplot as plt\n\nfrom time import time\n\npd.set_option('display.max_rows',9999)\n\nseed = 999\nt0 = time()\n#train = pd.read_csv(\"../input/train.csv\")\n#test = pd.read_csv(\"../input/test.csv\")\nd1 = pd.read_csv(\"../input/train.csv\", parse_dates=[1])\n\ny = d1['QuoteConversion_Flag']\ndel d1['QuoteConversion_Flag']\n\nd1['year'] = d1['Original_Quote_Date'].dt.year\nd1['month'] = d1['Original_Quote_Date'].dt.month\nd1['wkday'] = d1['Original_Quote_Date'].dt.dayofweek\n\ndel d1['Original_Quote_Date']\n\ntot_features = d1.columns\n\nmissing_data_list = []\n\nfor v in tot_features:\n    n1 = d1[v].isnull().sum()\n    if (n1 > 0):\n#       print(v, n1)\n        missing_data_list.append(v)\n\n#print(missing_data_list)\n\nunique_keys = []\n\nfor v in missing_data_list:\n    w = d1[v].unique()\n    unique_keys.append(w)\n    \nn1 = len(missing_data_list)\n\nd1['PropertyField29'].fillna(-1, inplace=True)\n\nd1['PersonalField7'].fillna('-1', inplace=True)\nd1['PersonalField84'].fillna(0, inplace=True)\n\nd1['PropertyField3'].fillna('-1', inplace=True)\nd1['PropertyField4'].fillna('-1', inplace=True)\nd1['PropertyField32'].fillna('-1', inplace=True)\nd1['PropertyField34'].fillna('-1', inplace=True)\nd1['PropertyField36'].fillna('-1', inplace=True)\nd1['PropertyField38'].fillna('-1', inplace=True)\n\nprint('*' * 80)\n\nprint('reading test...')\n\nd2 = pd.read_csv(\"../input/test.csv\", parse_dates=[1])\n\nprint(d2.shape)\n\nd2['year'] = d2['Original_Quote_Date'].dt.year\nd2['month'] = d2['Original_Quote_Date'].dt.month\nd2['wkday'] = d2['Original_Quote_Date'].dt.dayofweek\n\ndel d2['Original_Quote_Date']\n\nd2['PropertyField5'] = np.where(d2['PropertyField5'].isnull(), 'Y', d2['PropertyField5'])\nd2['PropertyField30'] = np.where(d2['PropertyField30'].isnull(), 'N', d2['PropertyField30'])\n\ntot_features = d2.columns\n\nmissing_data_list = []\n\nfor v in tot_features:\n    n1 = d2[v].isnull().sum()\n    if (n1 > 0):\n        print(v, n1)\n        missing_data_list.append(v)\n\nprint(missing_data_list)\n\nunique_keys = []\n\nfor v in missing_data_list:\n    w = d2[v].unique()\n    unique_keys.append(w)\n    \nn1 = len(missing_data_list)\n\nd2['PropertyField29'].fillna(-1, inplace=True)\n\nd2['PersonalField7'].fillna('-1', inplace=True)\nd2['PersonalField84'].fillna(0, inplace=True)\n\nd2['PropertyField3'].fillna('-1', inplace=True)\nd2['PropertyField4'].fillna('-1', inplace=True)\nd2['PropertyField32'].fillna('-1', inplace=True)\nd2['PropertyField34'].fillna('-1', inplace=True)\nd2['PropertyField36'].fillna('-1', inplace=True)\nd2['PropertyField38'].fillna('-1', inplace=True)\n\nprint('*' * 80)\n\nprint(\"Converting objects into numbers...\")\n\nobject_features = d1.select_dtypes(include=['object']).columns\n\nfor v in object_features:\n    LE1 = preprocessing.LabelEncoder()\n    LE1.fit( list(d1[v].values) + list(d2[v].values) )\n    d1[v] = LE1.transform( list(d1[v].values) )\n    d2[v] = LE1.transform( list(d2[v].values) )\n    d1[v] = d1[v].astype('float')\n    d2[v] = d2[v].astype('float')\n    print('object',v,'completed...',sep=' ')\n\nd1['QuoteConversion_Flag'] = y\n\ndel y\n\nrng = np.random.RandomState(31337)\n\nn10 = len(d1.columns)\n\nxx = np.array(np.arange(1,n10-1),dtype='int64')\n\ntrain1 = d1\n\ntrain1.data = d1[ xx ]\ntrain1.target = d1[ [n10-1] ]\ntrain1.feature_names = d1.columns[ xx ]\n\n# split train-test\nX_train1, X_test, y_train1, y_test = train_test_split(train1.data,\n                                                    train1.target,\n                                                    test_size=0.04,\n                                                    random_state=1)\n                                                    \n# split train-calibration\n                                                    \nX_train, X_cal, y_train, y_cal = train_test_split(X_train1,\n                                                    y_train1,\n                                                    test_size=0.04,\n                                                    random_state=1)\n\n\nnames = feature_names = d1.columns[ xx ]\n\nprint(\".......names\")\nprint(names)\n\nprint('_' * 80)\n\nprint(\"Parallel Parameter optimization\")\n          \nparam_dist = {'bst:max_depth':8,\n              'bst:eta':0.90,\n              'n_estimators': 600,\n              'objective':'binary:logistic',\n              'nthread':4,\n              'eval_metric':'auc',\n              'verbose':True}\n\nif __name__ == \"__main__\":\n\n    y_train = np.array(y_train).ravel()  #xgb.DMatrix('y_train')\n    y_cal = np.array(y_cal).ravel()  #xgb.DMatrix('y_train')\n    y_test  = np.array(y_test).ravel()\n    \n    dtrain = xgb.DMatrix(X_train[names], label=y_train)\n    dvalid = xgb.DMatrix(X_cal[names], label=y_cal)\n    dtest = xgb.DMatrix(X_test[names], label=y_test)\n    \n    evallist = [(dvalid,'eval'), (dtrain,'train')]\n    early_stopping_rounds = 10 #100\n    \n    clf = xgb.train(param_dist,\n                    dtrain,\n                    early_stopping_rounds,\n                    evallist)\n  \n    \nprint('Proceeding to final calculations...')\n\nprint(\"Going into predicting...\")\n\ny_train_pred = clf.predict(dtrain)\n\ny_cal_pred = clf.predict(dvalid)\n\ny_test_pred = clf.predict(dtest)\n\nprint(np.corrcoef(np.array(y_cal).ravel(), y_cal_pred))\n\nprint(np.corrcoef(np.array(y_test).ravel(), y_test_pred))\n\ny_test_pred_clf = clf.predict(dtest)\n\nfpr_test_clf, tpr_test_clf, _ = roc_curve(y_test, y_test_pred_clf)\n\ny_cal_pred_clf = clf.predict(dvalid)\n\nfpr_cal_clf, tpr_cal_clf, _ = roc_curve(y_cal, y_cal_pred_clf)\n\nplt.figure(1)\nplt.plot([0, 1], [0, 1], 'k--')\n\nplt.plot(fpr_cal_clf, tpr_cal_clf, label='cal')\nplt.plot(fpr_test_clf, tpr_test_clf, label='test')\n\nplt.xlabel('False positive rate')\nplt.ylabel('True positive rate')\nplt.title('ROC curve')\nplt.legend(loc='best')\n\nt1 = time()\n\nprint((t1-t0)/60,' min')\n\npickle.dump(clf, open(\"xgb_clf_fit_01a.pkl\", \"wb\"))\n\nnames0 = d2.columns\n\nnames1 = names0[1:]\n\nfor v in names1:\n  d2[v] = d2[v].astype('float')\n\nfeature_names = names1\n\nprint('feature_names')\nprint(feature_names)\n\nprint('*' * 80)\nprint('loading model...')\n#bst1 = pickle.load(open(\"xgb_clf_fit_01a.pkl\", \"rb\"))\n\nprint('predicting...')\nd2_test = xgb.DMatrix(d2[feature_names])\npreds1 = clf.predict(d2_test)\npreds = preds1\n\nresult = pd.DataFrame({'QuoteNumber': pd.Series(d2['QuoteNumber']),\n                       'QuoteConversion_Flag': pd.Series(preds)},\n                       columns=['QuoteNumber','QuoteConversion_Flag'])\n\nprint('writing to file...')                       \nresult.to_csv('draft0712.csv', index=False)\nprint('time elapsed=',(time()-t0)/60,'min',sep=' ')\n\nprint(chr(7) * 3)\n\nplt.show()\n\n\n\n"
 }
],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}}, "nbformat": 4, "nbformat_minor": 0}