{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nfrom nltk.stem.porter import PorterStemmer\nimport nltk\nimport os\nprint(os.listdir(\"../input\"))\n\nimport collections\nimport math\nimport os\nimport errno\nimport random\nimport zipfile\n\nimport numpy as np\nfrom six.moves import urllib\nfrom six.moves import xrange \nimport tensorflow as tf\n\n# Any results you write to the current directory are saved as output.","execution_count":23,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"../input/train.csv\")[:20000]","execution_count":26,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"f4b8462db4d583a335aca6119c43233d3a8d5593"},"cell_type":"code","source":"text =  \" \".join(train['comment_text'])\ntext = text.split()","execution_count":27,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"30c76c8a616e8c3cd12243dfc8b10792abb5f748"},"cell_type":"code","source":"from collections import Counter\n\ndef create_counts(vocab_size=50000):\n\n    # Begin adding vocab counts with Counter\n    vocab = [] + Counter(text).most_common(vocab_size )\n    \n    # Turn into a numpy array\n    vocab = np.array([word for word, _ in vocab])\n    \n    dictionary = {word: code for code, word in enumerate(vocab)}\n    data = np.array([dictionary.get(word, 0) for word in text])\n    return data,vocab\n\nvocab_size = 50000\n\ndata,vocabulary = create_counts(vocab_size=vocab_size)\n\n\ndata_index = 0\n\ndef generate_batch(batch_size, num_skips, skip_window):\n    global data_index\n    assert batch_size % num_skips == 0\n    assert num_skips <= 2 * skip_window\n    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n    span = 2 * skip_window + 1  # [ skip_window target skip_window ]\n    buffer = collections.deque(maxlen=span)\n    if data_index + span > len(data):\n        data_index = 0\n    buffer.extend(data[data_index:data_index + span])\n    data_index += span\n    for i in range(batch_size // num_skips):\n        target = skip_window  # target label at the center of the buffer\n        targets_to_avoid = [skip_window]\n        for j in range(num_skips):\n            while target in targets_to_avoid:\n                target = random.randint(0, span - 1)\n            targets_to_avoid.append(target)\n            batch[i * num_skips + j] = buffer[skip_window]\n            labels[i * num_skips + j, 0] = buffer[target]\n    if data_index == len(data):\n        buffer[:] = data[:span]\n        data_index = span\n    else:\n        buffer.append(data[data_index])\n        data_index += 1\n  # Backtrack a little bit to avoid skipping words in the end of a batch\n    data_index = (data_index + len(data) - span) % len(data)\n    return batch, labels","execution_count":28,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"900e6c544ccc1603ad0a86dd947780c79a13ef71"},"cell_type":"code","source":"# Size of the bath\nbatch_size = 128\n\n# Dimension of embedding vector\nembedding_size = 150\n\n# How many words to consider left and right (the bigger, the longer the training)\nskip_window = 1       \n\n# How many times to reuse an input to generate a label\nnum_skips = 2        \n\n# We pick a random validation set to sample nearest neighbors. Here we limit the\n# validation samples to the words that have a low numeric ID, which by\n# construction are also the most frequent.\n\n# Random set of words to evaluate similarity on.\nvalid_size = 16   \n\n# Only pick dev samples in the head of the distribution.\nvalid_window = 100  \nvalid_examples = np.random.choice(valid_window, valid_size, replace=False)\n\n# Number of negative examples to sample.\nnum_sampled = 64   \n\n# Model Learning Rate\nlearning_rate = 0.01\n\n# How many words in vocab\nvocabulary_size = 50000","execution_count":29,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b1040ecd808ccce356cce486c4691b4ca12396b9"},"cell_type":"code","source":"tf.reset_default_graph()\n\n# Input data.\ntrain_inputs = tf.placeholder(tf.int32, shape=[None])\ntrain_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\nvalid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n\n# Look up embeddings for inputs.\ninit_embeds = tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0)\nembeddings = tf.Variable(init_embeds)\n\nembed = tf.nn.embedding_lookup(embeddings, train_inputs)\n\n# Construct the variables for the NCE loss\nnce_weights = tf.Variable(tf.truncated_normal([vocabulary_size, embedding_size],stddev=1.0 / np.sqrt(embedding_size)))\nnce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n\n# Compute the average NCE loss for the batch.\n# tf.nce_loss automatically draws a new sample of the negative labels each\n# time we evaluate the loss.\nloss = tf.reduce_mean(\n    tf.nn.nce_loss(nce_weights, nce_biases, train_labels, embed,\n                   num_sampled, vocabulary_size))\n\n# Construct the Adam optimizer\noptimizer = tf.train.AdamOptimizer(learning_rate=1.0)\ntrainer = optimizer.minimize(loss)\n\n# Compute the cosine similarity between minibatch examples and all embeddings.\nnorm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), axis=1, keep_dims=True))\nnormalized_embeddings = embeddings / norm\nvalid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)\nsimilarity = tf.matmul(valid_embeddings, normalized_embeddings, transpose_b=True)\n\n# Add variable initializer.\ninit = tf.global_variables_initializer()\n\ngpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.9)\n\n #Usually needs to be quite large to get good results, \n# training takes a long time!\nnum_steps = 200\n\nwith tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:\n    sess.run(init)\n    average_loss = 0\n    for step in range(num_steps):\n         \n        batch_inputs, batch_labels = generate_batch(batch_size, num_skips, skip_window)\n        feed_dict = {train_inputs : batch_inputs, train_labels : batch_labels}\n\n        # We perform one update step by evaluating the training op (including it\n        # in the list of returned values for session.run()\n        empty, loss_val = sess.run([trainer, loss], feed_dict=feed_dict)\n        average_loss += loss_val\n\n        if step % 1000 == 0:\n            if step > 0:\n                average_loss /= 1000\n            # The average loss is an estimate of the loss over the last 1000 batches.\n            print(\"Average loss at step \", step, \": \", average_loss)\n            average_loss = 0\n            \n    final_embeddings = normalized_embeddings.eval()","execution_count":30,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"1dad074b17bf49c3a692f1e3dca18cabe755d928"},"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline\n\ndef plot_with_labels(low_dim_embs, labels):\n    assert low_dim_embs.shape[0] >= len(labels), \"More labels than embeddings\"\n    plt.figure(figsize=(18, 18))  #in inches\n    for i, label in enumerate(labels):\n        x, y = low_dim_embs[i,:]\n        plt.scatter(x, y)\n        plt.annotate(label,\n                     xy=(x, y),\n                     xytext=(5, 2),\n                     textcoords='offset points',\n                     ha='right',\n                     va='bottom')","execution_count":31,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eb42053035ba5b30069ed17a5bc53dc55b5a01e0"},"cell_type":"code","source":"from  sklearn.manifold import TSNE\ntsne = TSNE(perplexity=30, n_components=2, init='pca', n_iter=5000)\nplot_only = 2000\nlow_dim_embs = tsne.fit_transform(final_embeddings[:plot_only,:])\nlabels = [vocabulary[i] for i in range(plot_only)]\nplot_with_labels(low_dim_embs, labels)","execution_count":32,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7681227c48e0e3f68d0e8800d24ad03f7bc5fde7"},"cell_type":"code","source":"plot_with_labels(low_dim_embs, labels)\nplt.xlim(-10,10)\nplt.ylim(-10,10)","execution_count":33,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3d6e04a3f9063c0e6fb6a6c849ab3db0b930c476"},"cell_type":"code","source":"plot_with_labels(low_dim_embs, labels)\nplt.xlim(-20,20)\nplt.ylim(-20,20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"a375711abb0d70563208a06981bae28b256e064f"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}