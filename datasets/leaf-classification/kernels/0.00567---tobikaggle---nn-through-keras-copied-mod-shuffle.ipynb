{"cells":[{"metadata":{"_cell_guid":"75bd814e-9fa4-070c-aa71-bc4b18106296","_uuid":"d691e5a10a38813781a6670bdcc8f1ac56d58f61"},"cell_type":"markdown","source":"## Using Neural Networks through Keras"},{"metadata":{"_cell_guid":"224f9801-741c-7d4a-8946-5034d310583b","_uuid":"405812520c27a32267158cf99627f1af1a694021"},"cell_type":"markdown","source":"Copied from Kaggle itself - see if I make it better !  \nUpdated for 2018:  runs 77 seconds on the CPU and  13 seconds on the cloud NVIDIA Tesla K80.  \nFinal accuracy: 99.49%  \n"},{"metadata":{"_cell_guid":"93768274-a451-f111-440c-cf929716d679","_uuid":"a741cfe18d2f794ca3813c8df5f8db2640b87b15","trusted":true,"collapsed":true},"cell_type":"code","source":"## Measure execution time, becaus Kaggle cloud fluctuates  \nimport time\nstart = time.time()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"42859ad0-e281-758d-dc97-1453c170df10","_uuid":"edb56756d92f8962f8e2fc25237128f8da97827a","trusted":true},"cell_type":"code","source":"## Importing standard libraries\n%pylab inline\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"472dc525-28ef-8909-6daf-14db89d02736","_uuid":"b42e73569d873684f7c922c45c94e456b38628b5","trusted":true},"cell_type":"code","source":"## Importing sklearn libraries\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedShuffleSplit","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f1a31d09-8fb8-1b9a-e2d5-3af7f1b9a395","_uuid":"de649cdab15eb8856307ca0697d09ddc6473de82","trusted":true},"cell_type":"code","source":"## Keras Libraries for Neural Networks\n\nfrom keras.models import Sequential\nfrom keras.layers import merge\nfrom keras.layers import Dense, Dropout, Activation, Flatten\nfrom keras.layers.advanced_activations import PReLU\nfrom keras.layers import Convolution2D, MaxPooling2D\nfrom keras.utils.np_utils import to_categorical\nfrom keras.callbacks import EarlyStopping","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"4cda25b4-3b84-99b1-2535-bbf258413407","_uuid":"14ce1665ce8b94afe45dd5c20184a35f27dee3b9","trusted":true,"collapsed":true},"cell_type":"code","source":"## Read data from the CSV file\ndata = pd.read_csv('../input/train.csv')\nparent_data = data.copy()    ## Always a good idea to keep a copy of original data\nID = data.pop('id')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"260f0d07-9d76-942b-4c3b-cd68e6607581","_uuid":"7dd3df19cd5e25a74ef318919b35022b18a5cbac","trusted":true},"cell_type":"code","source":"data.shape\ndata.describe()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"70a39b8b-c802-0b1c-ec80-31f2fca5b113","_uuid":"d7ab698e7132c5aa7ea3cfd530ded103949008e3","trusted":true},"cell_type":"code","source":"## Since the labels are textual, so we encode them categorically\n\ny = data.pop('species')\ny = LabelEncoder().fit(y).transform(y)\nprint(y.shape)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"4d6decaa-a33e-ccd3-a31d-e9a25b7f41a8","_uuid":"cf5bcfb61f01badb923fa578c9170385c551cde5","trusted":true},"cell_type":"code","source":"## Most of the learning algorithms are prone to feature scaling\n## Standardising the data to give zero mean =)\nfrom sklearn import preprocessing\nX = preprocessing.MinMaxScaler().fit(data).transform(data)\nX = StandardScaler().fit(data).transform(data)\n## normalizing does not help here; l1 and l2 allowed\n## X = preprocessing.normalize(data, norm='l1')\nprint(X.shape)\nX","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"2f5c24c6-ca68-f5d8-5e1e-277455b2bb13","_uuid":"31de99f4eea65baf07e60184dca438eabea56d6c","trusted":true},"cell_type":"code","source":"## We will be working with categorical crossentropy function\n## It is required to further convert the labels into \"one-hot\" representation\nfrom keras import utils as np_utils\ny_cat = to_categorical(y)\nprint(y_cat.shape)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"cbd61c0d-b225-2a84-21c5-f31e2f6e02c8","_uuid":"c482c7d7f6009613f314390fe31770d1ae6120e8","trusted":true},"cell_type":"code","source":"## retain class balances\nsss = StratifiedShuffleSplit(n_splits=10, test_size=0.2,random_state=12345)\ntrain_index, val_index = next(iter(sss.split(X, y)))\nx_train, x_val = X[train_index], X[val_index]\ny_train, y_val = y_cat[train_index], y_cat[val_index]\nprint(\"x_train dim: \",x_train.shape)\nprint(\"x_val dim:   \",x_val.shape)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"820fb857-a7db-2279-a228-c799dddbf110","_uuid":"0b0ef207e25ba2471a2d95470b4e2e2097230e89","trusted":true},"cell_type":"code","source":"## Developing a layered model for Neural Networks\n## Input dimensions should be equal to the number of features\n## We used softmax layer to predict a uniform probabilistic distribution of outcomes\n## https://keras.io/initializations/ ;glorot_uniform, glorot_normal, lecun_uniform, orthogonal,he_normal\n\nmodel = Sequential()\nmodel.add(Dense(768,input_dim=192,  kernel_initializer='glorot_normal', activation='tanh'))\nmodel.add(Dropout(0.4))\n\nmodel.add(Dense(768, activation='tanh'))\nmodel.add(Dropout(0.4))\n\nmodel.add(Dense(99, activation='softmax'))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b8cd5713-e6b6-7248-2ac9-34e8c4b13be2","_uuid":"93f514fa920e864f7dd5fa3b0cc51cabfd42f7cd","trusted":true,"collapsed":true},"cell_type":"code","source":"## Error is measured as categorical crossentropy or multiclass logloss\n## Adagrad, rmsprop, SGD, Adadelta, Adam, Adamax, Nadam\n\nmodel.compile(loss='categorical_crossentropy',optimizer='rmsprop', metrics = [\"accuracy\"])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"05e4916c-17cc-693d-2784-e913b103d063","_uuid":"5ce9fce44291d0508513d337123757ef36a7bb75","trusted":true,"collapsed":true},"cell_type":"code","source":"## Fitting the model on the whole training data with early stopping\nearly_stopping = EarlyStopping(monitor='val_loss', patience=300)\n\nhistory = model.fit(x_train, y_train,batch_size=192,epochs=2500 ,verbose=0,\n                    validation_data=(x_val, y_val),callbacks=[early_stopping])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"991bf2ab-a9af-baaa-3f47-805839e13ee3","_uuid":"9041b5ddbbba48b10edea987bf0d57cc6688160b","trusted":true},"cell_type":"code","source":"## we need to consider the loss for final submission to leaderboard\n## print(history.history.keys())\nprint('val_acc: ',max(history.history['val_acc']))\nprint('val_loss: ',min(history.history['val_loss']))\nprint('train_acc: ',max(history.history['acc']))\nprint('train_loss: ',min(history.history['loss']))\n\nprint()\nprint(\"train/val loss ratio: \", min(history.history['loss'])/min(history.history['val_loss']))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b48ee871-13de-0b1e-438d-d6e2937e17f8","_uuid":"08fd0f925b560cf07de2232a18ddbb4387da31e2","trusted":true},"cell_type":"code","source":"## summarize history for loss\n## Plotting the loss with the number of iterations\nplt.semilogy(history.history['loss'])\nplt.semilogy(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"08739222-9ca7-05ea-eabc-743002cc7327","_uuid":"f00b8235dbbb682a70480d0ec93ae954eb50b490","trusted":true},"cell_type":"code","source":"## Plotting the error with the number of iterations\n## With each iteration the error reduces smoothly\nplt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"6060cdf0-cba4-0bd9-b0ee-d75dd15f0462","_uuid":"5872a5d04b352373b997c52ce3d140c6dee3d5fa","trusted":true,"collapsed":true},"cell_type":"code","source":"## read test file\ntest = pd.read_csv('../input/test.csv')\nindex = test.pop('id')\n\n## we need to perform the same transformations from the training set to the test set\ntest = preprocessing.MinMaxScaler().fit(test).transform(test)\ntest = StandardScaler().fit(test).transform(test)\nyPred = model.predict_proba(test)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"5c52f1c2-ecfa-f3a3-f3b3-5fa0197995d0","_uuid":"c214b6dbce1b1075e2313a76aa988a678ba8d543","trusted":true,"collapsed":true},"cell_type":"code","source":"## Converting the test predictions in a dataframe as depicted by sample submission\nyPred = pd.DataFrame(yPred,index=index,columns=sort(parent_data.species.unique()))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"fe58a1d0-7e94-24df-ceb8-0ef876379677","_uuid":"ca903ac28a2e49d779d83b3113cb53400f3113df","trusted":true},"cell_type":"code","source":"## write submission to file\nfp = open('submission_nn_kernel.csv','w')\nfp.write(yPred.to_csv())\n\n## print run time\nend = time.time()\nprint(round((end-start),2), \"seconds\")","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"10bf679e-a4e5-72ac-e7a6-f0652a4a3891","_uuid":"541d0046483cf45f672621c9372a257f520ac3c5"},"cell_type":"markdown","source":"---------\n\nEarlier` we used a 4 layer network but the result came out to be overfitting the test set. We dropped the count of neurones in the network and also restricted the number of layers to 3 so as to keep it simple.\nInstead of submitting each test sample as a one hot vector we submitted each samples as a probabilistic distribution over all the possible outcomes. This \"may\" help reduce the penalty being exercised by the multiclass logloss thus producing low error on the leaderboard! ;)\nAny suggestions are welcome!"},{"metadata":{"_cell_guid":"dfe70334-1f60-e1c6-6ca8-90ebe626a5af","_uuid":"f64115aa6803536335761b8397cfec94ac06a741","trusted":true,"collapsed":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}