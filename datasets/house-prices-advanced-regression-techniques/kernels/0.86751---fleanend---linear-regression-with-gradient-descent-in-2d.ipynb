{"cells":[{"metadata":{"_uuid":"bca3362019b8ea6ca0394054e358d7b1269c4eb6","_cell_guid":"cc689557-4eeb-460a-a329-b4dcac37edcb"},"cell_type":"markdown","source":"# Handling and  Visualising Data\n\nLet's start by playing a bit with the dataset, first and foremost load all the necessary libraries, and then let's take a look at the data"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","scrolled":true,"trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport matplotlib.pyplot as plt\nimport math\nfrom scipy import stats\ndf = pd.read_csv('../input/train.csv')\ndf = df[df['LotArea']<30000] # remove outliers\ndf.head()","execution_count":1,"outputs":[]},{"metadata":{"_uuid":"13abc2f3283574afe0857db6ff2e7b7c6dbba3dd","_cell_guid":"93495de2-a445-4d67-8a03-f28f3d5bd415"},"cell_type":"markdown","source":"As we can see, whe have an Id, some features of different nature and at last our label, SalePrice. As we're going to use Gradient Descent in 2D I'll choose just one of our features to build a linear regression upon, in this instance LotArea.\nHaving chosen our x and y, we can start normalising our feature and labels to have similar ranges, a process known as scaling.\n\nWith a normalised x and a normalised y, we can plot the first scatter, just to visualise our dataset.\n"},{"metadata":{"_uuid":"0374248ad00548ef182433a2f49c2dada846dae7","_cell_guid":"cfc34a89-8cad-4d21-8c8f-edde20a39f7d","trusted":true},"cell_type":"code","source":"x = df['LotArea']\nmax_x = x.max()\nmean_x = x.mean()\nx_norm = list(map(lambda elem: (elem-mean_x)/max_x,list(x)))\ny = df['SalePrice']\nmax_y = y.max()\nmean_y = y.mean()\ny_norm = list(map(lambda elem: (elem-mean_y)/max_y,list(y)))\n\nplt.figure(figsize=(10,10))\nplt.scatter(x_norm,y_norm, alpha=0.6)\nplt.show()","execution_count":2,"outputs":[]},{"metadata":{"_uuid":"f068ec829dabc94a69587dde8d66c739ef53aa4a","_cell_guid":"18155c20-bfcb-4f14-ab62-517aab0eec70"},"cell_type":"markdown","source":"Well I can't say much about our result, but maybe our model will!\nLet's start by defining our cost function and its derivative\n\n# Fitting the data a.k.a. training the model\n\nLet's define some numbers such as **n**, the size of the dataset, **err**, the cost function and its derivative and **der**, its gradient. \nWe also initialise the regressor coefficients **a** and **b**, and set the hyperparameters **alpha** and **max_iter**, which are, respectively, the step of the gradient descent and the number of iterations computed before stopping the algorithm.\n\nWe define the cycle where the gradient descent is actually computed and then plot the error for each iteration"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"n = len(x)\na = 0.01 # [a,b] also known as theta\nb = 1\nalpha = 0.5\nmax_iter = 3000\n\ndef err (a,b,x,y): # J(theta) = 1/2n * (y - theta dot x + theta_0)^2\n    est_y = list(map(lambda elem: a*elem+b,x)) # theta dot x + theta_0\n    err = np.subtract(np.array(est_y),np.array(y)) # y^ - y\n    err_2 = np.power(err,2)\n    return (1/(2*n)) * sum(err_2)\n\ndef der(a,b,x,y): # gradient of J(theta) = [1/n * (y - theta dot x + theta_0) dot x, 1/n*(y - theta dot x + theta_0) dot 1]\n    est_y = list(map(lambda elem: a*elem+b,x)) # theta dot x + theta_0\n    err = np.subtract(np.array(est_y),np.array(y)) # y^ - y\n    return (1/n * np.dot(err,x),1/n*sum(err))\n\nerr_iter = []\nfor i in range(0,max_iter): \n    deriv = der(a,b,x_norm,y_norm)\n    a -= alpha*deriv[0]\n    b -= alpha*deriv[1]\n    err_iter.append(math.sqrt(err(a,b,x_norm,y_norm)))\n    \nindex = list(range(0,max_iter))\n#plt.figure(figsize=(10,10))\nplt.plot(index,err_iter)\nplt.show()\n","execution_count":3,"outputs":[]},{"metadata":{"_uuid":"e752161b622ffa8f3e592656d9bad808a9201d96","_cell_guid":"76e4cb92-ffd0-4f7a-88bd-5fb97844fd12"},"cell_type":"markdown","source":"As you can see the error decreases super fast at first and then flattens nicely"},{"metadata":{"_uuid":"804779219fe4b229ceda901f2d17f10ed22a22a8","_cell_guid":"91a607ad-d982-4dc5-b05d-416600fc1919","trusted":true},"cell_type":"code","source":"def y_from_x(x): # theta dot x + theta_0\n    return a*x+b\nplt.figure(figsize=(10,10))\n\nplt.scatter(x_norm,y_norm, alpha=0.6)\nplt.plot([0,1],[y_from_x(0),y_from_x(1)])\nplt.show()","execution_count":4,"outputs":[]},{"metadata":{"_uuid":"0ad4d5f86e6622d28354b713a93e6a3dfa8c5a69","_cell_guid":"d701d032-16a0-4a0b-8e28-9ba759b5ae69"},"cell_type":"markdown","source":"And this is the result! A fine linear regressor for an unforgiving dataset, as you can see, the predictions might leave you unsatisfied, let's review our R squared and the p-value"},{"metadata":{"_uuid":"0619755cd1445d06e1f4f85f296d9f010e8bb897","_cell_guid":"89262b2f-3d75-48fb-8e17-2444f0ce95d2","trusted":true},"cell_type":"code","source":"# R^2 is ESS / TSS, Explained sum of squares over total sum of squares\ny_avg=sum(y_norm)/len(y_norm)\ny_hat = np.apply_along_axis(y_from_x, 0, x_norm)\nESS = np.sum(np.add(y_hat, -1*y_avg)**2)\nTSS = np.sum(np.add(y_norm, -1*y_avg)**2)\n\ndegf = len(y_norm)\n\nse = math.sqrt(np.sum(np.add(y_norm,-y_hat)**2)/degf) / math.sqrt(np.sum(np.add(x_norm,-sum(x_norm)/len(x_norm))**2))\n\nslope, intercept, r_value, p_value, std_err = stats.linregress(x_norm,y_norm)\n\ntscore = (a - 0)/(se)\np = stats.t.sf(tscore,df=degf)\nprova = stats.t.ppf(p_value,df=degf)\nprint(\"R Squared:\", ESS/TSS)\nprint(\"p-value\", p)","execution_count":5,"outputs":[]},{"metadata":{"_uuid":"fa27b343e7ea1eef4523baebbd3be6ddaeca7ef3","_cell_guid":"c26123be-a21e-449c-987b-922834c211d7"},"cell_type":"markdown","source":"Nice stat"},{"metadata":{"_uuid":"fb5f440641a404d4d053b38977eedaf4ab995f7b","_cell_guid":"980f9c0b-7668-4d2f-a945-68d2942620ae","collapsed":true,"trusted":true},"cell_type":"code","source":"df_2 = pd.read_csv('../input/test.csv')\nx_test = df_2['LotArea']\ny_test = list(map(y_from_x,x))\noutput = pd.DataFrame(df_2['Id'])\noutput['SalePrice'] = pd.Series(y_test)\noutput.to_csv(\"output.csv\", index=False)","execution_count":6,"outputs":[]}],"metadata":{"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}