{"cells":[{"metadata":{"trusted":false,"_uuid":"ef5c81b6d3cce4fea2aa56edda00e2743a3b543c"},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns \nimport matplotlib.pyplot as plt\n\nfrom nltk.tokenize import word_tokenize\nfrom nltk import ngrams\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.corpus import stopwords\n\nimport string\n\nfrom collections import Counter, defaultdict\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix  \nfrom sklearn.metrics import accuracy_score\n\nfrom pyemd import emd\nimport gensim\nfrom gensim.similarities import WmdSimilarity\nfrom gensim.models import Word2Vec\nfrom gensim import corpora\nimport gensim.downloader as api\nfrom gensim.matutils import softcossim\n\nfrom scipy.spatial.distance import cosine,cityblock,jaccard,canberra,euclidean,minkowski,braycurtis\n\nfrom fuzzywuzzy import fuzz\nfrom tqdm import tqdm_notebook\n\nimport xgboost as xgb\n\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"2f1de8e34446023946ca9be762944d6aeedbebc5"},"cell_type":"code","source":"df = pd.read_csv(\"train.csv\")\ndf = df.dropna(how=\"any\").reset_index(drop=True)\ndf_test_total = pd.read_csv(\"test.csv\")\n#df = df.head(30)\ndf_test = df_test_total[:200000]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"3cb1668f7a7d3b4d190a0d7bf31e3e6b88596458"},"cell_type":"code","source":"# EDA\nis_dup = df['is_duplicate'].value_counts()\nprint (is_dup)\nplt.figure(figsize=(8,4))\nsns.barplot(is_dup.index, is_dup.values, alpha=0.8)\nplt.ylabel('No of Occurrences', fontsize=12)\nplt.xlabel('Is Duplicate', fontsize=12)\nplt.show()\nis_dup / is_dup.sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"fdd427b8299d24fdf7c067ecfc2f7ec9044c33d8"},"cell_type":"code","source":"# length of the questions\n\ndf['q1_word_len'] = df['question1'].str.split().str.len()\ndf['q2_word_len'] = df['question2'].str.split().str.len()\ndf['q1_char_len'] = df['question1'].str.len()\ndf['q2_char_len'] = df['question2'].str.len()\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"00460466f0a1dfc0be8fbb056abfb724e0df4356"},"cell_type":"code","source":"# test data\n\ndf_test['q1_word_len'] = df_test['question1'].str.split().str.len()\ndf_test['q2_word_len'] = df_test['question2'].str.split().str.len()\ndf_test['q1_char_len'] = df_test['question1'].str.len()\ndf_test['q2_char_len'] = df_test['question2'].str.len()\ndf_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"d970bda4987b960d958a56ab3982d5f58c383a99"},"cell_type":"code","source":"# Plot of words\n\ncnt_words = df['q1_word_len'] + df['q2_word_len']\ncnt_words = cnt_words.value_counts()\nplt.figure(figsize=(18,6))\nsns.barplot(cnt_words.index, cnt_words.values, alpha=0.8)\nplt.ylabel('No of Occurrences', fontsize=12)\nplt.xlabel('No of words in the question', fontsize=12)\nplt.xticks(rotation='vertical')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"f5989e96e7f033a9161b02bec9d40e82688e9ff9"},"cell_type":"code","source":"# Plot of characters\n\ncnt_chars = df['q1_char_len'] + df['q2_char_len']\ncnt_chars = cnt_chars.value_counts()\nplt.figure(figsize=(18,6))\nsns.barplot(cnt_chars.index, cnt_chars.values, alpha=0.8)\nplt.ylabel('No of Occurrences', fontsize=12)\nplt.xlabel('No of chars in the question', fontsize=12)\nplt.xticks(rotation='vertical')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"8e3f35236c5e1997cf3dc88f921d1aa18e7ee989"},"cell_type":"code","source":"# Feature Extraction on grams\nques_stopwords = set(stopwords.words('english')) |  set(string.punctuation) \nstemmer = PorterStemmer()\n\ndef feature_extraction(row):\n    que1 = str(row['question1'])\n    que2 = str(row['question2'])\n    out_list = []\n    # get unigram features #\n    unigrams_que1 = [stemmer.stem(word) for word in que1.lower().split() if word not in ques_stopwords]\n    unigrams_que2 = [stemmer.stem(word) for word in que2.lower().split() if word not in ques_stopwords]\n    common_unigrams_len = len(set(unigrams_que1).intersection(set(unigrams_que2)))\n    common_unigrams_ratio = float(common_unigrams_len) / max(len(set(unigrams_que1).union(set(unigrams_que2))),1)\n    out_list.extend([common_unigrams_len, common_unigrams_ratio])\n\n    # get bigram features #\n    bigrams_que1 = [i for i in ngrams(unigrams_que1, 2)]\n    bigrams_que2 = [i for i in ngrams(unigrams_que2, 2)]\n    common_bigrams_len = len(set(bigrams_que1).intersection(set(bigrams_que2)))\n    common_bigrams_ratio = float(common_bigrams_len) / max(len(set(bigrams_que1).union(set(bigrams_que2))),1)\n    out_list.extend([common_bigrams_len, common_bigrams_ratio])\n\n    # get trigram features #\n    trigrams_que1 = [i for i in ngrams(unigrams_que1, 3)]\n    trigrams_que2 = [i for i in ngrams(unigrams_que2, 3)]\n    common_trigrams_len = len(set(trigrams_que1).intersection(set(trigrams_que2)))\n    common_trigrams_ratio = float(common_trigrams_len) / max(len(set(trigrams_que1).union(set(trigrams_que2))),1)\n    out_list.extend([common_trigrams_len, common_trigrams_ratio])\n    return out_list","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"d4916589cecd6742663768cbcacf03f6925ce7ba"},"cell_type":"code","source":"# Use grams_feature_extraction to extract features \n\ndf['common_grams'] = df.apply(lambda row: feature_extraction(row), axis=1)\ncolumns = ['common_unigrams_len', 'common_unigrams_ratio', \n  'common_bigrams_len', 'common_bigrams_ratio', \n  'common_trigrams_len', 'common_trigrams_ratio']\ndf1 = pd.DataFrame(df['common_grams'].tolist(), columns=columns)\ndf = pd.concat([df,df1], axis=1)\ndf.drop('common_grams', axis=1, inplace=True) \ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"d78fee6b42bde4f72b2316525d7724c50b602628"},"cell_type":"code","source":"# Use grams_feature_extraction to extract features for test data\n\ndf_test['common_grams'] = df_test.apply(lambda row: feature_extraction(row), axis=1)\ncolumns = ['common_unigrams_len', 'common_unigrams_ratio', \n  'common_bigrams_len', 'common_bigrams_ratio', \n  'common_trigrams_len', 'common_trigrams_ratio']\ndf1 = pd.DataFrame(df_test['common_grams'].tolist(), columns=columns)\ndf_test = pd.concat([df_test,df1], axis=1)\ndf_test.drop('common_grams', axis=1, inplace=True) \ndf_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"2beb09ae6ea2291364f932ceb12dac80d6be258e"},"cell_type":"code","source":"df.to_csv('df_new', index=False, encoding='utf-8')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"523ed912e1ce8962a208fbebf627df7498b873a8"},"cell_type":"code","source":"df_test.to_csv('df_new_test', index=False, encoding='utf-8')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"8662aa8e3a0853b8b406d953c9278017fcdbbe06"},"cell_type":"code","source":"df = pd.read_csv(\"df_new\")\ndf_test = pd.read_csv(\"df_new_test\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"9772a7480605ddf66d465a15596b6ca80999d51f"},"cell_type":"code","source":"# EXA on unigrams\ncnt_srs = df['common_unigrams_len'].value_counts()\nplt.figure(figsize=(12,6))\nsns.barplot(cnt_srs.index, cnt_srs.values, alpha=0.8)\nplt.ylabel(\"No of Occurrences\", fontsize=12)\nplt.xlabel(\"Common unigrams count\", fontsize=12)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"ad322bf8e4c91dad2b1e97a94a85c41775426618"},"cell_type":"code","source":"# EDA on the feature\n\nplt.figure(figsize=(12,6))\nsns.boxplot(x='is_duplicate', y='common_unigrams_len', data=df)\nplt.xlabel('Is duplicate', fontsize=12)\nplt.ylabel('Common unigrams count', fontsize=12)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"d166434a501d589a5b30ce55dc816cd9c9f380a5"},"cell_type":"code","source":"# EDA on unigram ratio\nplt.figure(figsize=(12,6))\nsns.boxplot(x='is_duplicate', y='common_unigrams_ratio', data=df)\nplt.xlabel(\"Is duplicate\", fontsize=12)\nplt.ylabel(\"Unigram common ratio\")\nplt.show","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"347eb419dcbcbd0d703398814c9686930ab77bb0"},"cell_type":"code","source":"n = 10\nsns.pairplot(df[['q1_char_len','q2_char_len','q1_word_len','q2_word_len','is_duplicate','common_unigrams_len']][0:n])                \n                ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"e111143a300f80dfc50677df11e8da50f8852358"},"cell_type":"code","source":"col_mask=df.isnull().any(axis=0) \n#col_mask\nrow_mask=df.isnull().any(axis=1)\nrow_mask\ndf = df.dropna()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"3f7a3f8d82e4b49518f6a2eb739395a7f43d38c1"},"cell_type":"code","source":"scaler = MinMaxScaler().fit(df[['q1_word_len','q2_word_len','q1_char_len','q2_char_len','common_unigrams_len', 'common_unigrams_ratio', \n  'common_bigrams_len', 'common_bigrams_ratio', 'common_trigrams_len', 'common_trigrams_ratio']])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"7e40cd5fe47421c03cdfaee91c19526a14506871"},"cell_type":"code","source":"X = scaler.transform(df[['q1_word_len','q2_word_len','q1_char_len','q2_char_len','common_unigrams_len', 'common_unigrams_ratio', \n  'common_bigrams_len', 'common_bigrams_ratio', 'common_trigrams_len', 'common_trigrams_ratio']])\ny = df['is_duplicate']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n\nX_train.shape, X_test.shape, y_train.shape, y_test.shape\n\nclf = LogisticRegression()\ngrid = {\n    'C': [1e-6, 1e-3, 1e0],\n    'penalty': ['l1','l2']\n}\ncv = GridSearchCV(clf, grid, scoring='neg_log_loss', n_jobs=1, verbose=1)\ncv.fit(X_train, y_train)\nprint(cv.best_params_)\nprint(cv.best_estimator_.coef_)\ncv.error_score\ny_pred = cv.predict(X_test)\ncf_matrix = confusion_matrix(y_test, y_pred)\nprint(cf_matrix)\nprint(y_pred)\nprint('accuracy of logistic regression classifier for train set: {:.2f}'.format(cv.score(X_test,y_test)))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"711812043296fc35c8dda5f5abe84af0de6fad51"},"cell_type":"code","source":"# Graph based features\n\n\nques = pd.concat([df[['question1', 'question2']], \\\n    df_test_total[['question1', 'question2']]], axis=0).reset_index(drop='index')\n\nq_dict = defaultdict(set)\nfor i in range(ques.shape[0]):\n    q_dict[ques.question1[i]].add(ques.question2[i])\n    q_dict[ques.question2[i]].add(ques.question1[i])\n\n    \ndef q1_freq(row):\n    return (len(q_dict[row['question1']]))\n\n\ndef q2_freq(row):\n    return (len(q_dict[row['question2']]))\n\n\ndef q1_q2_intersect(row):\n    return (len(\n        set(q_dict[row['question1']]).intersection(\n            set(q_dict[row['question2']]))))\n\ndf['q1_q2_intersect'] = df.apply(q1_q2_intersect, axis=1, raw=True)\ndf['q1_freq'] = df.apply(q1_freq, axis=1, raw=True)\ndf['q2_freq'] = df.apply(q2_freq, axis=1, raw=True)\n\n# test data\ndf_test['q1_q2_intersect'] = df_test.apply(q1_q2_intersect, axis=1, raw=True)\ndf_test['q1_freq'] = df_test.apply(q1_freq, axis=1, raw=True)\ndf_test['q2_freq'] = df_test.apply(q2_freq, axis=1, raw=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"6466a98341517fa34f00136505ab93d9bb5a9192"},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"65caa17081f93f42ef807ce5b0454939d2e55ed7"},"cell_type":"code","source":"df_test.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"7d4f1c22e5d5b43a281173a1901f79b4198b0559"},"cell_type":"code","source":"df.to_csv('df_new', index=False, encoding='utf-8')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"baebdf6c55b0916a828518d42dbf0ca4bd37e4b2"},"cell_type":"code","source":"df_test.to_csv('df_test_new', index=False, encoding='utf-8')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"00ce0e4afbebb3e187850d215b134d506a105d32"},"cell_type":"code","source":"df = pd.read_csv(\"df_new\")\ndf_test = pd.read_csv(\"df_test_new\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"bf2cb58b500c1027d85f6ec3a44cfc09d467c029"},"cell_type":"code","source":"col_mask=df.isnull().any(axis=0) \nprint(col_mask)\ncol_test_mask=df_test.isnull().any(axis=0) \nprint(col_test_mask)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"e5a2d4f7a3c48535c95d0ac96db67176399a3cb5"},"cell_type":"code","source":"df_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"1187d0b98ec5fcb2d3751eaafcaa59fc6da7e22e"},"cell_type":"code","source":"cnt_srs = df['q1_q2_intersect'].value_counts()\n\nplt.figure(figsize=(12,6))\nsns.barplot(cnt_srs.index, np.log1p(cnt_srs.values), alpha=0.8)\nplt.xlabel('Q1-Q2 neighbour instersection count', fontsize=12)\nplt.ylabel('Log of Number of Occurrences', fontsize=12)\nplt.xticks(rotation='vertical')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"dd5df2cf515631c135a38e5254ade20e9efcfefb"},"cell_type":"code","source":"grouped_df = df.groupby('q1_q2_intersect')['is_duplicate'].aggregate(np.mean).reset_index()\nplt.figure(figsize=(12,8))\nsns.pointplot(grouped_df['q1_q2_intersect'].values,\n             grouped_df['is_duplicate'].values, alpha=0.8)\nplt.ylabel('Mean is duplicate', fontsize=12)\nplt.xlabel('Q1-Q2 neighbor intersection count', fontsize=12)\nplt.xticks(rotation='vertical')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"20899e5ed92738a8e9ca1b6a74f9d9940d7734a2"},"cell_type":"code","source":"pvt_df = df.pivot_table(index='q1_freq', columns='q2_freq', values='is_duplicate')\nplt.figure(figsize=(12,12))\nsns.heatmap(pvt_df)\nplt.title(\"Mean is_duplicate across q1 and q2 fequency\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"20b8e7e8e8ef0e17792587177635c91bb665eadc"},"cell_type":"code","source":"cols_to_use = ['q1_q2_intersect', 'q1_freq', 'is_duplicate']\ntemp_df = df[cols_to_use]\ncorrmat = temp_df.corr(method='spearman')\nf, ax = plt.subplots(figsize=(8,8))\n\n#Heatmap\nsns.heatmap(corrmat, vmax=1., square=True)\nplt.title(\"Leaky variables correlation map\", fontsize=15)\nplt.show()\n\ncorr_mat = df[cols_to_use].corr()\ncorr_mat.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"0b782c1f07c142ffcc570fac29f85a3122382917"},"cell_type":"code","source":"scaler = MinMaxScaler().fit(df[['q1_word_len','q2_word_len','q1_char_len','q2_char_len','common_unigrams_len', 'common_unigrams_ratio', \n  'common_bigrams_len', 'common_bigrams_ratio', 'common_trigrams_len', 'common_trigrams_ratio','q1_q2_intersect', 'q1_freq', 'q2_freq']])\nX = scaler.transform(df[['q1_word_len','q2_word_len','q1_char_len','q2_char_len','common_unigrams_len', 'common_unigrams_ratio', \n  'common_bigrams_len', 'common_bigrams_ratio', 'common_trigrams_len', 'common_trigrams_ratio','q1_q2_intersect', 'q1_freq', 'q2_freq']])\ny = df['is_duplicate']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n\nX_train.shape, X_test.shape, y_train.shape, y_test.shape\n\nclf = LogisticRegression()\ngrid = {\n    'C': [1e-6, 1e-3, 1e0],\n    'penalty': ['l1','l2']\n}\ncv = GridSearchCV(clf, grid, scoring='neg_log_loss', n_jobs=1, verbose=1)\ncv.fit(X_train, y_train)\nprint(cv.best_params_)\nprint(cv.best_estimator_.coef_)\ncv.error_score\ny_pred = cv.predict(X_test)\ncf_matrix = confusion_matrix(y_test, y_pred)\nprint(cf_matrix)\nprint('accuracy of logistic regression classifier for train set: {:.2f}'.format(cv.score(X_test,y_test)))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"cbdb478133c6ee724841163198d3fa880fffa965"},"cell_type":"code","source":"scaler = MinMaxScaler().fit(df_test[['q1_word_len','q2_word_len','q1_char_len','q2_char_len','common_unigrams_len', 'common_unigrams_ratio', \n  'common_bigrams_len', 'common_bigrams_ratio', 'common_trigrams_len', 'common_trigrams_ratio','q1_q2_intersect', 'q1_freq', 'q2_freq']])\nX_test = scaler.transform(df_test[['q1_word_len','q2_word_len','q1_char_len','q2_char_len','common_unigrams_len', 'common_unigrams_ratio', \n  'common_bigrams_len', 'common_bigrams_ratio', 'common_trigrams_len', 'common_trigrams_ratio','q1_q2_intersect', 'q1_freq', 'q2_freq']])\n\ntest_pred = cv.predict(X_test)\nprint(test_pred)\ndf_test.info()\n#df.to_csv('df_new', index=False, encoding='utf-8')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"6fab22038d42b05cc2d2dcfa0e17b53390632e9b"},"cell_type":"code","source":"df1 = pd.DataFrame(test_pred, columns=columns)\nsubmission = pd.concat([df_test['test_id'],df1], axis=1)\nsubmission.head()\ncol_mask=submission.isnull().any(axis=0) \ncol_mask","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"ee0f0afa69ed037a8f639e6eb2b5946e829abff6"},"cell_type":"code","source":"is_dup = submission['is_duplicate'].value_counts()\nplt.figure(figsize=(8,4))\nsns.barplot(is_dup.index, is_dup.values, alpha=0.8)\nplt.ylabel('No of Occurrences', fontsize=12)\nplt.xlabel('Is Duplicate', fontsize=12)\nplt.show()\n\nsubmission.to_csv('submission.csv', index=False, encoding='utf-8')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"45901f7965c2974e58f8916a13c9e46f3772e598"},"cell_type":"code","source":"norm_model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"d4b9441210900b3701c84507bbe05226112f5e33"},"cell_type":"code","source":"def wmd(q1, q2):\n    q1 = str(q1).lower().split()\n    q2 = str(q2).lower().split()\n    stop_words = stopwords.words('english')\n    q1 = [w for w in q1 if w not in stop_words]\n    q2 = [w for w in q2 if w not in stop_words]\n    return model.wmdistance(q1, q2)\n\ndef norm_wmd(q1, q2):\n    q1 = str(q1).lower().split()\n    q2 = str(q2).lower().split()\n    stop_words = stopwords.words('english')\n    q1 = [w for w in q1 if w not in stop_words]\n    q2 = [w for w in q2 if w not in stop_words]\n    return norm_model.wmdistance(q1, q2)\n\ndef sent2vec(s):\n    words = str(s).lower()\n    words = word_tokenize(words)\n    stop_words = stopwords.words('english')\n    words = [w for w in words if not w in stop_words]\n    words = [w for w in words if w.isalpha()]\n    M = []\n    for w in words:\n        try:\n            M.append(model[w])\n        except:\n            continue\n    M = np.array(M)\n    v = M.sum(axis=0)\n    return v / np.sqrt((v ** 2).sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"521a97b31878b9eae38b4062d19b88c9fcb56e56"},"cell_type":"code","source":"norm_model.init_sims(replace=True)\ndf['norm_wmd'] = df.apply(lambda x: norm_wmd(x['question1'], x['question2']), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"377c9130e21f04e069d4aef85a91f54d98856f40"},"cell_type":"code","source":"# Advanced Features\ndf['fuzz_ratio'] = df.apply(lambda x: fuzz.ratio(str(x['question1']), str(x['question2'])), axis=1)\ndf['fuzz_partial_ratio'] = df.apply(lambda x: fuzz.partial_ratio(str(x['question1']), str(x['question2'])), axis=1)\ndf['fuzz_partial_token_set_ratio'] = df.apply(lambda x: fuzz.partial_token_set_ratio(str(x['question1']), str(x['question2'])), axis=1)\ndf['fuzz_partial_token_sort_ratio'] = df.apply(lambda x: fuzz.partial_token_sort_ratio(str(x['question1']), str(x['question2'])), axis=1)\ndf['fuzz_token_set_ratio'] = df.apply(lambda x: fuzz.token_set_ratio(str(x['question1']), str(x['question2'])), axis=1)\ndf['fuzz_token_sort_ratio'] = df.apply(lambda x: fuzz.token_sort_ratio(str(x['question1']), str(x['question2'])), axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"f490d9cb76e7fea8500faea486812554d2d4848f"},"cell_type":"code","source":"# Advanced Features\ndf_test['fuzz_ratio'] = df_test.apply(lambda x: fuzz.ratio(str(x['question1']), str(x['question2'])), axis=1)\ndf_test['fuzz_partial_ratio'] = df_test.apply(lambda x: fuzz.partial_ratio(str(x['question1']), str(x['question2'])), axis=1)\ndf_test['fuzz_partial_token_set_ratio'] = df_test.apply(lambda x: fuzz.partial_token_set_ratio(str(x['question1']), str(x['question2'])), axis=1)\ndf_test['fuzz_partial_token_sort_ratio'] = df_test.apply(lambda x: fuzz.partial_token_sort_ratio(str(x['question1']), str(x['question2'])), axis=1)\ndf_test['fuzz_token_set_ratio'] = df_test.apply(lambda x: fuzz.token_set_ratio(str(x['question1']), str(x['question2'])), axis=1)\ndf_test['fuzz_token_sort_ratio'] = df_test.apply(lambda x: fuzz.token_sort_ratio(str(x['question1']), str(x['question2'])), axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"c5738a1a7b1b3ebc5352f1311ca51bb27fbf95a1"},"cell_type":"code","source":"question1_vectors = np.zeros((df.shape[0], 300))\nfor i, q in enumerate(tqdm_notebook(df.question1.values)):\n    question1_vectors[i, :] = sent2vec(q)\n    \nquestion2_vectors  = np.zeros((df.shape[0], 300))\nfor i, q in enumerate(tqdm_notebook(df.question2.values)):\n    question2_vectors[i, :] = sent2vec(q)\n\ndf['cosine_distance'] = [cosine(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors), np.nan_to_num(question2_vectors))]\ndf['cityblock_distance'] = [cityblock(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors), np.nan_to_num(question2_vectors))]\ndf['jaccard_distance'] = [jaccard(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors), np.nan_to_num(question2_vectors))]\ndf['canberra_distance'] = [canberra(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors), np.nan_to_num(question2_vectors))]\ndf['euclidean_distance'] = [euclidean(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors), np.nan_to_num(question2_vectors))]\ndf['minkowski_distance'] = [minkowski(x, y, 3) for (x, y) in zip(np.nan_to_num(question1_vectors), np.nan_to_num(question2_vectors))]\ndf['braycurtis_distance'] = [braycurtis(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors), np.nan_to_num(question2_vectors))]\n\ndf = df[pd.notnull(df['cosine_distance'])]\ndf = df[pd.notnull(df['jaccard_distance'])]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"93ed14f9e95c4dd04e0d586ec29f0cffda8a089f"},"cell_type":"code","source":"# test dataframe\nquestion1_vectors = np.zeros((df_test.shape[0], 300))\nfor i, q in enumerate(tqdm_notebook(df_test.question1.values)):\n    question1_vectors[i, :] = sent2vec(q)\n    \nquestion2_vectors  = np.zeros((df_test.shape[0], 300))\nfor i, q in enumerate(tqdm_notebook(df_test.question2.values)):\n    question2_vectors[i, :] = sent2vec(q)\n\ndf_test['cosine_distance'] = [cosine(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors), np.nan_to_num(question2_vectors))]\ndf_test['cityblock_distance'] = [cityblock(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors), np.nan_to_num(question2_vectors))]\ndf_test['jaccard_distance'] = [jaccard(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors), np.nan_to_num(question2_vectors))]\ndf_test['canberra_distance'] = [canberra(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors), np.nan_to_num(question2_vectors))]\ndf_test['euclidean_distance'] = [euclidean(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors), np.nan_to_num(question2_vectors))]\ndf_test['minkowski_distance'] = [minkowski(x, y, 3) for (x, y) in zip(np.nan_to_num(question1_vectors), np.nan_to_num(question2_vectors))]\ndf_test['braycurtis_distance'] = [braycurtis(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors), np.nan_to_num(question2_vectors))]\n\ndf_test = df_test[pd.notnull(df_test['cosine_distance'])]\ndf_testf = df_test[pd.notnull(df_test['jaccard_distance'])]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"125e10784bde63e25442e2b8034cca6e9bf221ba"},"cell_type":"code","source":"df.drop(['question1', 'question2'], axis=1, inplace=True)\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"1a01a65c09b073480370f95934c7c62889fe89d3"},"cell_type":"code","source":"df_test.drop(['question1', 'question2'], axis=1, inplace=True)\ndf_test.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"fe3a300057a31c24e196a2e10d1997bb3b5129ba"},"cell_type":"code","source":"X = df.loc[:, df.columns != 'is_duplicate']\ny = df.loc[:, df.columns == 'is_duplicate']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"212818f065b69facbf311a65f76d3dc8f3404459"},"cell_type":"code","source":"scaler = MinMaxScaler().fit(df)\nX = scaler.transform(df)\ny = df['is_duplicate']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n\nX_train.shape, X_test.shape, y_train.shape, y_test.shape\n\nclf = LogisticRegression()\ngrid = {\n    'C': [1e-6, 1e-3, 1e0],\n    'penalty': ['l1','l2']\n}\ncv = GridSearchCV(clf, grid, scoring='neg_log_loss', n_jobs=1, verbose=1)\ncv.fit(X_train, y_train)\nprint(cv.best_params_)\nprint(cv.best_estimator_.coef_)\ncv.error_score\ny_pred = cv.predict(X)\ncf_matrix = confusion_matrix(y, y_pred)\nprint(cf_matrix)\ny_pred = cv.predict(X)\ny_pred\nprint('accuracy of logistic regression classifier for train set: {:.2f}'.format(cv.score(X,y)))\ntest_pred = cv.predict(df_test)\nprint(test_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"c13e3744b2076b96e74e56300d77e2cba9abe1d5"},"cell_type":"code","source":"model = xgb.XGBClassifier(max_depth=50, n_estimators=80, learning_rate=0.1, colsample_bytree=.7, gamma=0, reg_alpha=4, objective='binary:logistic', eta=0.3, silent=1, subsample=0.8).fit(X_train, y_train.values.ravel()) \nprediction = model.predict(X_test)\ncm = confusion_matrix(y_test, prediction)  \nprint(cm)  \nprint('Accuracy', accuracy_score(y_test, prediction))\nprint(classification_report(y_test, prediction))\ntest_pred = model.predict(df_test)\nprint(test_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"0f40871383c1b515d0f51f7792f4a7829aaa690d"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"c42f531d21d1c773b0cc111459241d2aab1e754e"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":1}