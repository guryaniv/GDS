{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "9df1ac46-a694-f128-e21a-1d125935b9e8"
      },
      "source": [
        "I am still puzzling on this contest\n",
        "IMHO there is no 'perfect' solution in that sense, that all submission that have a 'bugfree' should be good...\n",
        "\n",
        "Take for instance this SVD attack to the problem\n",
        "*Do you train a 'full matrix' on one copy of  the titles.words* or do you train a matrix taking with 10 copies columnwize of the matrix such that  you get a resolution of 60 axis, instead of the '6-axis solution' ?\n",
        "*Do you use that result on the titles* ? Or do you use that result on the content ? the answer changes each time, and all should be accounted as good..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ba0f4ab9-c447-3871-6809-8f2b4c1dc44e"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "import string\n",
        "\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import itertools \n",
        "import csv\n",
        "import collections\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "sns.set_context(\"paper\")\n",
        "%matplotlib inline\n",
        "\n",
        "RES_DIR = \"../input/\"\n",
        "# Load train data (skips the content column)\n",
        "def load_train_data():\n",
        "    categories = ['cooking', 'robotics', 'travel', 'crypto', 'diy', 'biology']\n",
        "    train_data = []\n",
        "    for cat in categories:\n",
        "        data = pd.read_csv(\"{}{}.csv\".format(RES_DIR, cat), usecols=['id', 'title', 'tags'])\n",
        "        data['category'] = cat\n",
        "        train_data.append(data)\n",
        "    \n",
        "    return pd.concat(train_data)\n",
        "train_data = load_train_data()\n",
        "#import the test data\n",
        "test = pd.read_csv(\"../input/test.csv\")\n",
        "train_data.head()\n",
        "test.head()\n",
        "uri_re = r'(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:\\'\".,<>?\u00ab\u00bb\u201c\u201d\u2018\u2019]))'\n",
        "\n",
        "def stripTagsAndUris(x):\n",
        "    if x:\n",
        "        # BeautifulSoup on content\n",
        "        soup = BeautifulSoup(x, \"html.parser\")\n",
        "        # Stripping all <code> tags with their content if any\n",
        "        if soup.code:\n",
        "            soup.code.decompose()\n",
        "        # Get all the text out of the html\n",
        "        text =  soup.get_text()\n",
        "        # Returning text stripping out all uris\n",
        "        return re.sub(uri_re, \"\", text)\n",
        "    else:\n",
        "        return \"\"\n",
        "# This could take a while\n",
        "train_data[\"title\"] = train_data[\"title\"].map(stripTagsAndUris)\n",
        "test[\"content\"] = test[\"content\"].map(stripTagsAndUris)\n",
        "train_data.head()\n",
        "test.head()\n",
        "def removePunctuation(x):\n",
        "    # Lowercasing all words\n",
        "    x = x.lower()\n",
        "    # Removing non ASCII chars\n",
        "    x = re.sub(r'[^\\x00-\\x7f]',r' ',x)\n",
        "    # Removing (replacing with empty spaces actually) all the punctuations\n",
        "    return re.sub(\"[\"+string.punctuation+\"]\", \" \", x)\n",
        "\n",
        "train_data[\"title\"] = train_data[\"title\"].map(removePunctuation)\n",
        "test[\"title\"] = test[\"title\"].map(removePunctuation)\n",
        "test[\"content\"] = test[\"content\"].map(removePunctuation)\n",
        "\n",
        "train_data.head()\n",
        "test.head()\n",
        "\n",
        "stops = set(stopwords.words(\"english\"))\n",
        "def removeStopwords(x):\n",
        "    # Removing all the stopwords\n",
        "    filtered_words = [word for word in x.split() if word not in stops]\n",
        "    return \" \".join(filtered_words)\n",
        "\n",
        "train_data[\"title\"] = train_data[\"title\"].map(removeStopwords)\n",
        "test[\"title\"] = test[\"title\"].map(removeStopwords)\n",
        "test[\"content\"] = test[\"content\"].map(removeStopwords)\n",
        "\n",
        "\n",
        "# Summary about tags\n",
        "tag_lists = [t.split() for t in train_data['tags'].values]\n",
        "tag_lists2 = [t.split() for t in train_data['title'].values]\n",
        "all_tags = list(itertools.chain(*tag_lists,*tag_lists2))\n",
        "tag_list_size = np.array([len(x) for x in tag_lists])\n",
        "print(\"\"\"The corpus is composed by {} questions. Overall {} tags have been used, of which {} unique ones. \n",
        "Average number of tags per question {:.2f} (min={}, max={}, std={:.2f})\"\"\".format(\n",
        "    len(train_data),\n",
        "    len(all_tags), len(set(all_tags)),\n",
        "    tag_list_size.mean(), \n",
        "    min(tag_list_size), max(tag_list_size),\n",
        "    tag_list_size.std()))\n",
        "\n",
        "# Utility function to return top occuring tags in the passed df\n",
        "def get_top_tags(df, n=None):\n",
        "    itag_lists = [t.split() for t in df['tags'].values]\n",
        "    itag_lists2 = [t.split() for t in df['title'].values]\n",
        "    tags = list(itertools.chain(*itag_lists,*itag_lists2))\n",
        "    top_tags = collections.Counter(list(tags)).most_common(n)\n",
        "    tags, count = zip(*top_tags)\n",
        "    return tags, count\n",
        "# Utility function to return top occuring tags in the passed df\n",
        "\n",
        "# Created DataFrame indexed on tags\n",
        "tags_df = pd.DataFrame(index=set(itertools.chain(*tag_lists,*tag_lists2)))\n",
        "# For each category create a column and update the flag to tag count\n",
        "for i, (name, group) in enumerate(train_data.groupby('category')):\n",
        "    tags_df[name] = 0\n",
        "    tmp_index, count = get_top_tags(group)\n",
        "    tmp = pd.Series(count, index=tmp_index)\n",
        "    tags_df[name].update(tmp)\n",
        "# Number of categories for which a tag appeared at least 1 time\n",
        "tags_df['categories_appears'] = tags_df.apply(lambda x: x.astype(bool).sum(), axis=1)\n",
        "tags_df['categories_appears'].value_counts()\n",
        "# viewing the table of tags\n",
        "\n",
        "A=tags_df.drop('categories_appears',axis=1)\n",
        "#lets try to , smooth big numbers,normalize \n",
        "#A = np.log(A+1)\n",
        "#B = (A - A.mean()) / (A.max() - A.min())\n",
        "B=A\n",
        "from scipy.cluster.vq import whiten\n",
        "B=whiten(A)\n",
        "print(B)\n",
        "from numpy.linalg import inv\n",
        "U,s,V=np.linalg.svd(B,full_matrices=False)\n",
        "# reconstruct\n",
        "S=np.diag(s)\n",
        "\n",
        "iS=inv(S)\n",
        "US=np.dot(U,iS)\n",
        "\n",
        "# A fill up with US matrix\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "03323f30-eac2-fc44-fd50-92a22b9ce30c"
      },
      "outputs": [],
      "source": [
        "US=np.dot(U,iS)\n",
        "US_df=pd.DataFrame(data=US, index=A.index, columns=A.columns)\n",
        "print(US_df.shape)\n",
        "# with this simple math i know all the relations between all the tags and the documents\n",
        "# \n",
        "#learn how to use dataframes...  and yes the algorithm knows extreme tourism antarctica has something to do with travel...\n",
        "df1=US_df['extreme-tourism':'extreme-tourism':]\n",
        "df2=US_df['antarctica':'antarctica':]\n",
        "frames = [df1,df2]\n",
        "Qtemp=pd.concat(frames).sum()\n",
        "print(np.dot(Qtemp,V)/np.dot(np.abs(Qtemp),np.abs(V)))\n",
        "\n",
        "columns = ['biology','cooking','crypto','diy','robotics','travel']\n",
        "#,'categories_appears']\n",
        "data = {'biology': [0],'cooking': [0],'crypto': [0],'diy': [0],'robotics': [0],'travel': [0],'categories_appears': [0]}\n",
        "newDF = pd.DataFrame(data, columns=columns,index = ['blanco'])\n",
        "\n",
        "\n",
        "print(test.head())\n",
        "for xyz in range (1,10):\n",
        "    zoekwoorden=test['title'][xyz]\n",
        "    tempspl = zoekwoorden.strip().split()\n",
        "    Qtemp=newDF\n",
        "    for sword in tempspl:\n",
        "        if sword in US_df.index:\n",
        "           #print(US_df.loc[sword:sword,:])\n",
        "           Qtemp=Qtemp.append(US_df.loc[sword:sword,:])\n",
        "    #print(Qtemp)\n",
        "    #print(Qtemp.sum())\n",
        "    simila=np.dot(Qtemp.sum(),V)/np.dot(np.abs(Qtemp.sum()),np.abs(V))*100\n",
        "    #print(simila)\n",
        "    tempprnt=''\n",
        "    for xyb in range(0,5):\n",
        "        if simila[xyb]>85:\n",
        "            #or simila[xyb]==np.amax(simila[0:5]):\n",
        "            tempprnt+=columns[xyb]+' '\n",
        "    print('title map:',xyz,tempprnt)\n",
        "\n",
        "for xyz in range (1,10):\n",
        "    zoekwoorden=test['content'][xyz]\n",
        "    tempspl = zoekwoorden.strip().split()\n",
        "    Qtemp=newDF\n",
        "    for sword in tempspl:\n",
        "        if sword in US_df.index:\n",
        "           #print(US_df.loc[sword:sword,:])\n",
        "           Qtemp=Qtemp.append(US_df.loc[sword:sword,:])\n",
        "    #print(Qtemp)\n",
        "    #print(Qtemp.sum())\n",
        "    simila=np.dot(Qtemp.sum(),V)/np.dot(np.abs(Qtemp.sum()),np.abs(V))*100\n",
        "    #print(simila)\n",
        "    tempprnt=''\n",
        "    for xyb in range(0,5):\n",
        "        if simila[xyb]>85:\n",
        "            #or simila[xyb]==np.amax(simila[0:5]):\n",
        "            tempprnt+=columns[xyb]+' '\n",
        "    print('content mapp:',xyz,tempprnt)\n",
        "    \n",
        "for xyz in range (1,10):\n",
        "    zoekwoorden=test['content'][xyz]+' '+test['title'][xyz]\n",
        "    tempspl = zoekwoorden.strip().split()\n",
        "    Qtemp=newDF\n",
        "    for sword in tempspl:\n",
        "        if sword in US_df.index:\n",
        "           #print(US_df.loc[sword:sword,:])\n",
        "           Qtemp=Qtemp.append(US_df.loc[sword:sword,:])\n",
        "    #print(Qtemp)\n",
        "    #print(Qtemp.sum())\n",
        "    simila=np.dot(Qtemp.sum(),V)/np.dot(np.abs(Qtemp.sum()),np.abs(V))*100\n",
        "    #print(simila)\n",
        "    tempprnt=''\n",
        "    for xyb in range(0,5):\n",
        "        if simila[xyb]>85:\n",
        "            #or simila[xyb]==np.amax(simila[0:5]):\n",
        "            tempprnt+=columns[xyb]+' '\n",
        "    print('content/title mapp:',xyz,tempprnt)"
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}