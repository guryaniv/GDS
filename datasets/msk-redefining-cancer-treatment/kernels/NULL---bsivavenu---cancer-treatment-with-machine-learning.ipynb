{"cells":[{"metadata":{"_uuid":"d2327b8f664e3f8b51e2b59af1c6664754048316"},"cell_type":"markdown","source":"Hello all, im a nebie here and i'll  try to explain you what i've learned so far in an understable way.\n\nPlease upvote at the end if you like my kernel and encourage me.\n\n![](https://78.media.tumblr.com/0a56b418334765ec595a0982fe25aac3/tumblr_ouloa3CUT41wq17fxo3_400.gif)"},{"metadata":{"_uuid":"b4be0228d158e724f0391e4ee235e6317728b992","_cell_guid":"4db280c5-4a6d-4e45-9dda-b9462f795a19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":26,"outputs":[]},{"metadata":{"_uuid":"6ebcfce8581025824e5d811eceee55facaa674b2","collapsed":true,"_cell_guid":"2d7aaff5-c01d-428c-9981-dfdc82ce0592","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ncolor = sns.color_palette()\n\n%matplotlib inline\n\npd.options.mode.chained_assignment = None\npd.options.display.max_columns = 999","execution_count":27,"outputs":[]},{"metadata":{"_uuid":"8d2ae3ab4a1e2099b2abe778a595e4ba054b5278","collapsed":true,"_cell_guid":"4428f502-8463-437a-8c76-286dc67489af","trusted":true},"cell_type":"code","source":"train_variants_df = pd.read_csv(\"../input/training_variants\")\ntest_variants_df = pd.read_csv(\"../input/test_variants\")\ntrain_text_df = pd.read_csv(\"../input/training_text\", sep=\"\\|\\|\", engine='python', header=None, skiprows=1, names=[\"ID\",\"Text\"])\ntest_text_df = pd.read_csv(\"../input/test_text\", sep=\"\\|\\|\", engine='python', header=None, skiprows=1, names=[\"ID\",\"Text\"])\n","execution_count":28,"outputs":[]},{"metadata":{"_uuid":"b15686b63e430e3569f513fe30cd88e4fe183ecb","_cell_guid":"1023df92-3c1f-4118-bb65-43ebfe2a9bef","trusted":true},"cell_type":"code","source":"train_text_df.shape,test_text_df.shape","execution_count":29,"outputs":[]},{"metadata":{"_uuid":"d36578c7b9c463e95155ab37c40f5e5ecc62c1fc","_cell_guid":"16ed665c-4a31-4ad4-a064-b4619011c253","trusted":true},"cell_type":"code","source":"train_variants_df.shape,test_variants_df.shape","execution_count":30,"outputs":[]},{"metadata":{"_uuid":"7883661f9c69ca1d9d86c61ae3d33534725e8c28","_cell_guid":"9a9dbd03-8a69-4ec5-954c-3f27a565aa74","trusted":true},"cell_type":"code","source":"train_variants_df.head(3)","execution_count":31,"outputs":[]},{"metadata":{"_uuid":"960b3db1297ff6fc12cd22000574d380d2cd8e45","_cell_guid":"85dd207a-0015-4a4a-963b-a7c6d9800a0a","trusted":true},"cell_type":"code","source":"train_text_df.head(3)","execution_count":32,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1a3040ef3cc367a8746a3611ec64fe56f6d8f5fe"},"cell_type":"code","source":"gene_group = train_variants_df.groupby(\"Gene\")['Gene'].count()\nminimal_occ_genes = gene_group.sort_values(ascending=True)[:10]\nprint(\"Genes with maximal occurences\\n\", gene_group.sort_values(ascending=False)[:10])\nprint(\"\\nGenes with minimal occurences\\n\", minimal_occ_genes)","execution_count":33,"outputs":[]},{"metadata":{"_uuid":"ef25579bd6ba8c7ab616377fb9e3d9b1b5371ed0","_cell_guid":"b6c93e6f-ce2b-4ce2-8ee1-d10f271889a2","trusted":true},"cell_type":"code","source":"test_variants_df.head(3)","execution_count":34,"outputs":[]},{"metadata":{"_uuid":"a9692254a62e5c80d7c30a2f79fad550e6d98b07","_cell_guid":"cd4c0e9e-e5ab-4019-9f61-8b11767409f6","trusted":true},"cell_type":"code","source":"test_text_df.head(3)","execution_count":35,"outputs":[]},{"metadata":{"_uuid":"19a31a053208e018718b177354afe745df43dd30","_cell_guid":"502544e3-8959-4215-b566-4270c1fb871d","_kg_hide-output":false,"trusted":true},"cell_type":"code","source":"train_text_df.Text[0]","execution_count":36,"outputs":[]},{"metadata":{"_uuid":"4db952e18c5ea638e3d813e9d981c42ea343aa63","_cell_guid":"9b4780e5-d83d-476d-ba52-903d8587420b","trusted":true},"cell_type":"code","source":"train_variants_df.Class.unique()","execution_count":37,"outputs":[]},{"metadata":{"_uuid":"44e5ff03478f4ce6404451033de5093df7afb946","_cell_guid":"366b4fdd-86d7-447f-bf0c-7174e26c8844","trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,5))\nsns.countplot(train_variants_df.Class,data = train_variants_df)","execution_count":38,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2257dc980341d73fed82917fe616ab61eaeb8f59"},"cell_type":"code","source":"print(len(train_variants_df.Gene.unique()))","execution_count":39,"outputs":[]},{"metadata":{"_uuid":"3cde1c9c7e44340d07ffc51c7d3fb86163a7d1b2","_cell_guid":"170eace8-92f4-4152-b7af-ca5988c73235","trusted":true},"cell_type":"code","source":"train_df = pd.merge(train_text_df,train_variants_df,on = 'ID')\nprint(train_df.shape)\ntrain_df.head(3)","execution_count":40,"outputs":[]},{"metadata":{"_uuid":"444c6151759d5eec241da233be26bcf7a418caa4","trusted":true},"cell_type":"code","source":"test_df = pd.merge(test_text_df,test_variants_df,on = 'ID')\nprint(test_df.shape)\ntest_df.head(3)\n","execution_count":41,"outputs":[]},{"metadata":{"_uuid":"5d1be124ca827ba4f05f31d503bbbe1dee49e682"},"cell_type":"markdown","source":"This is multi class classification problem and number of classes are total 9. we have to predicat the classes probabalitie  for particular Id. Now we'll see how the submission file should be."},{"metadata":{"_uuid":"64704897e2f223d2227fa98b5d3f179264111dc8","trusted":true},"cell_type":"code","source":"submission_file = pd.read_csv(\"../input/submissionFile\")\nsubmission_file.head()","execution_count":42,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"33fd07778d8cad2612e4172fdbdeb656d6ce6adb"},"cell_type":"code","source":"train_df.isnull().sum()","execution_count":43,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"c2c19637ced344f4b3932a5509bb9d5c32d3dd24"},"cell_type":"code","source":"train_df.dropna(inplace=True)","execution_count":44,"outputs":[]},{"metadata":{"_uuid":"3d3d1271d9689f0fc307b7c1e70e76f4df29618a","trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntrain ,test = train_test_split(train_df,test_size=0.2) \nnp.random.seed(0)\ntrain.head()","execution_count":45,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"8d21e8976b8c5aeb674b4db937d06f1cd59c2dd3","trusted":true},"cell_type":"code","source":"X_train = train['Text'].values\nX_test = test['Text'].values\ny_train = train['Class'].values\ny_test = test['Class'].values","execution_count":46,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6dfb84a4bebfcf31e44ea5a2f717879895658418"},"cell_type":"code","source":"train.isnull().sum()","execution_count":47,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"54698e7b06e30f4acbc9342efa44c86c5567d9b4","trusted":true},"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn import svm\nfrom sklearn.ensemble import RandomForestClassifier,ExtraTreesClassifier\nimport xgboost as xgb\nimport lightgbm as lgb","execution_count":48,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"1496f93a39bc8630fc6702988f9f8b36ad11db06","trusted":true},"cell_type":"code","source":"svc = svm.LinearSVC()\nrfc = RandomForestClassifier()\netrc = ExtraTreesClassifier()\nxgbc = xgb.XGBClassifier()\nlgbc = lgb.LGBMClassifier()\nclf = [svc,rfc]\n# ,etrc,xgbc,lgbc","execution_count":49,"outputs":[]},{"metadata":{"_uuid":"f68957c0d9b9032bd13e7506cf2e9aefe93595dd","trusted":true},"cell_type":"code","source":"\nfor i in clf:\n        text_clf = Pipeline([('vect', TfidfVectorizer(lowercase=True,stop_words='english',encoding='utf-8')),('tfidf', TfidfTransformer()),('clf', i)])\n        text_clf = text_clf.fit(X_train,y_train)\n        y_test_predicted = text_clf.predict(X_test)\n        acc = np.mean(y_test_predicted == y_test)\n        print('accuracy of :',str(i),'is: ',acc )","execution_count":50,"outputs":[]},{"metadata":{"_uuid":"65df661b5f8adc746d91f3c5b4d2e68a69ba60a8"},"cell_type":"markdown","source":"Note here i consider only train['Text'] values only into consideration while training but not other features like Gene.\n\n* Here i considered word vectors (tfidf) in determining class and same used for training the model.so if i supply the same words in test so it will determine the particular class instead of considereing Gene and other features which is wrong. Though we got 60% here, this is not generalized model. what are your thoughts let me in the comments."},{"metadata":{"_uuid":"d749efbedc85aa58bb74d8fa93290f08d6e8c07a"},"cell_type":"markdown","source":"Further we'll apply some NLP concepts here and get the most out of the huge text data we have. To be honest to solve this problem one should have domain knowledge in this particulat field. Even though we dont have that biology realted knowledge we'll try to implement some NLP techniques like wordembendings to get the important information."},{"metadata":{"_uuid":"918b2312f1156a0c49156fc32c15e6c28deb4c43","_cell_guid":"78f19a0e-ec76-4155-87a4-73b77fcc52f8"},"cell_type":"markdown","source":"More to come soon so  watch this space.\n\n**Please encourage me by upvoting**\n\nThank you.\n\n![](https://media.giphy.com/media/cyoN6pC6kek2A/giphy.gif)"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}