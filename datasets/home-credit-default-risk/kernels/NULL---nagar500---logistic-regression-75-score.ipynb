{"cells":[{"metadata":{"_uuid":"a6824da0667111357c0865742f04fb10ee52ee47"},"cell_type":"markdown","source":"- <a href='#1'>1. Problem Statement </a>  \n- <a href='#2'>2. Reading the data</a>\n- <a href='#3'>3. Feature Engineering</a>\n    - <a href='#3-1'>3.1 Creating new feature for bureau</a>\n    - <a href='#3-2'> 3.2 Function to count and normalize values of categorical variables </a>\n- <a href='#4'>4. Grouping the data</a>\n- <a href='#5'>5. Exploratory Data Analysis</a>\n       - <a href='#5-1'>5.1  Analyzing Target Variable</a>\n     - <a href='#5-2'>5.2  Visualizing basic info of the applicant </a>\n      - <a href='#5-3'>5.3 Client accompanied by ? </a>\n- <a href='#6'>6. Merging the data</a>     \n- <a href='#7'>7. Combining Training and Testing data</a>     \n- <a href='#7'>8. Feature Engineering Continued</a>     \n     -<a href='#8_1'>8.1. Deleting features </a>\n     -<a href='#8_2'>8.2  Handling Missing Values </a>\n      -<a href='#8_3'>8.3 Scaling Numerical Features </a>\n      -<a href='#8_3'>8.4 Converting into Categorical </a>\n   \n- <a href='#9'>9.Modelling</a>     "},{"metadata":{"_uuid":"b357fa29ec56d7bc265d3ff641ed56eb84090f6d"},"cell_type":"markdown","source":"> # <a id='1'>1. Problem Statement</a>"},{"metadata":{"_uuid":"cd0ff3f769b834bada40a5fe4f013d6a479d1869"},"cell_type":"markdown","source":"Home Credit strives to broaden financial inclusion for the unbanked population by providing a positive and safe borrowing experience. In order to make sure this underserved population has a positive loan experience, Home Credit makes use of a variety of alternative data--including telco and transactional information--to predict their clients' repayment abilities.\n\n**Evalutaion**  - Area under the ROC Curve\n\n**Data  ** -   the problem has 7 files. \n\n* **application_train/application_test**: the main training and testing data with information about each loan application at Home Credit. Every loan has its own row and is identified by the feature SK_ID_CURR.  \n* **bureau **: All client's previous credits provided by other financial institutions that were reported to Credit Bureau (for clients who have a loan in our sample). Each previous credit has its own row in bureau, but one loan in the application data can have multiple previous credits."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n# import for plotting \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5dd1eaae5623c9ecdf8fde81b505b4ed293e049b"},"cell_type":"markdown","source":">  # <a id='2'>2. Reading the Data</a>"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"app_train = pd.read_csv('../input/application_train.csv')\napp_test = pd.read_csv('../input/application_test.csv')\nbureau = pd.read_csv('../input/bureau.csv')\nbureau_balance = pd.read_csv('../input/bureau_balance.csv')\npos_cash_balance = pd.read_csv('../input/POS_CASH_balance.csv')\n\nprevious_app = pd.read_csv('../input/previous_application.csv')\ninstallments_payments = pd.read_csv('../input/installments_payments.csv')\ncredit_card_balance = pd.read_csv('../input/credit_card_balance.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"47a6124df95306a1b400f19aa01402ecb62cf566"},"cell_type":"code","source":"print(app_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"77f0b6f1ff903279cd7c012897d71c5527605ca7"},"cell_type":"markdown","source":"> # <a id='3'>3. Feature Engineering</a>"},{"metadata":{"_uuid":"1abd668104bee54e84b172693b6783c22fcca178"},"cell_type":"markdown","source":"## <a id='3-1'>3.1 Creating new feature for bureau</a>"},{"metadata":{"trusted":true,"_uuid":"d74d5a5f546adea6bbfd50f5526028203e59542a"},"cell_type":"code","source":"# Groupby the client id (SK_ID_CURR), count the number of previous loans, and rename the column\nprevious_loan_counts = bureau.groupby('SK_ID_CURR', as_index=False)['SK_ID_BUREAU'].count().rename(columns = {'SK_ID_BUREAU': 'previous_loan_counts'})\nprevious_loan_counts.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"835f505d2f0d2c902ba1ffd299854abe3c7ceb94"},"cell_type":"markdown","source":"## <a id='3-2'>3.2 Function to count and normalize values of categorical variables </a>"},{"metadata":{"trusted":true,"_uuid":"8a2e9b38eb335eb43bb08a82002566b082e1e1bc"},"cell_type":"code","source":"def normalize_categorical(df, group_var, col_name):\n    \n    \"\"\"Computes counts and normalized counts for each observation\n    of `group_var` for each unique category in every categorical variable\n    \n    Parameters \n    ----------\n    df - DataFrame for which we will calculate count\n    \n    group_var  = string\n        The variable by which to group the dataframe. For each unique\n        value of this variable, the final dataframe will have one row\n        \n    col_name = string\n            Variable added to the front of column names to keep track of columns\n            \n            \"\"\"\n    # select the categorical columns\n    categorical = pd.get_dummies(df.select_dtypes('object'))\n    \n    # Make sure to put the identifying id on the column\n    categorical[group_var] = df[group_var]\n    \n    # Groupby the group var and calculate the sum and mean\n    categorical = categorical.groupby(group_var).agg(['sum', 'mean'])                                              \n    \n    column_names = []\n    \n    # Iterate through the columns in level 0\n    for var in categorical.columns.levels[0]:\n        # Iterate through the stats in level 1\n        for stat in ['count', 'count_norm']:\n            # Make a new column name\n            column_names.append('%s_%s_%s' % (col_name, var, stat))\n    \n    categorical.columns = column_names\n    \n    return categorical\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"593480adaaa613827e7c2ed8ed9fb2976c49606b"},"cell_type":"code","source":"bureau_counts = normalize_categorical(bureau, group_var = 'SK_ID_CURR', col_name = 'bureau')\nbureau_counts.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c2b47c5a19e1bdeb1425ae89a39349de38702f09"},"cell_type":"markdown","source":"> # <a id='4'>4 Grouping the data </a>"},{"metadata":{"trusted":true,"_uuid":"619f4b8f739cb4132746c3f9943b8045f607e609"},"cell_type":"code","source":"# Grouping data  so  that we can merge all the files in 1 dataset\n\ndata_bureau_agg=bureau.groupby(by='SK_ID_CURR').mean()\ndata_credit_card_balance_agg=credit_card_balance.groupby(by='SK_ID_CURR').mean()\ndata_previous_application_agg=previous_app.groupby(by='SK_ID_CURR').mean()\ndata_installments_payments_agg=installments_payments.groupby(by='SK_ID_CURR').mean()\ndata_POS_CASH_balance_agg=pos_cash_balance.groupby(by='SK_ID_CURR').mean()\n\ndata_bureau_agg.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e31324180323876a48f6bfd7bdcde0fdadeaab38"},"cell_type":"markdown","source":"> # <a id='5'>5. Exploratory Data Exploration</a>"},{"metadata":{"_uuid":"c4e0371d3067eb20e22f1f409c3b04911ef91e0e"},"cell_type":"markdown","source":"## <a id='5-2'>5.2  Visualizing basic info of the applicant </a>"},{"metadata":{"trusted":true,"_uuid":"18a300eefd78b49d7d5c5da2cf228b6b22ce453c"},"cell_type":"code","source":"# we will be plotting gender, occupation, has car, has flat  \n\nplt.figure(1)\nplt.subplot(221)\napp_train['CODE_GENDER'].value_counts(normalize=True).plot.bar(figsize=(20,10), title= 'Gender')\n\nplt.subplot(222)\napp_train['FLAG_OWN_CAR'].value_counts(normalize=True).plot.bar(title= 'Own Car?')\n\nplt.subplot(223)\napp_train['CNT_CHILDREN'].value_counts(normalize=True).plot.bar(title= 'Count Children')\n\nplt.subplot(224)\napp_train['FLAG_OWN_REALTY'].value_counts(normalize=True).plot.bar(figsize=(24,6), title= 'Has Realty?')\n\n\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4fd2b1c4b6573c13b822b92aaef535041823603a"},"cell_type":"markdown","source":"# Inference - \n1. We see that most of the applicants were female and without any children.\n2. An interesting fact is that most of the applicants owned a realty but not a car. \n"},{"metadata":{"_uuid":"f52d76ec17f3cd35a08066061d86257efa2ca896"},"cell_type":"markdown","source":"## <a id='5-3'>5.3 Client accompanied by ? </a>"},{"metadata":{"trusted":true,"_uuid":"95cf085c6414ce553bda1ecb35648da2eb052ec1"},"cell_type":"code","source":"plt.figure(2)\n\nplt.subplot(321)\napp_train['NAME_TYPE_SUITE'].value_counts(normalize=True).plot.bar(figsize=(20,20), title= 'Accompanient')\n\nplt.subplot(322)\napp_train[\"NAME_CONTRACT_TYPE\"].value_counts(normalize=True).plot.pie(figsize=(20,20), title='Loan Type')\n\nplt.subplot(323)\napp_train[\"NAME_FAMILY_STATUS\"].value_counts(normalize=True).plot.pie(figsize=(20,20), title='Family status of applicants')\n\nplt.subplot(324)\napp_train[\"OCCUPATION_TYPE\"].value_counts(normalize=True).plot.bar(figsize=(20,20), title='Occupation')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"381f4392ae5d362c9ca8924db4771b0ead4e2a6d"},"cell_type":"markdown","source":"## <a id='5-3'>5.3  Loan is replayed or not? </a>"},{"metadata":{"_uuid":"9c8276ce2dbdcef2c61093cd69eacc086200dc45"},"cell_type":"markdown","source":"In Progress"},{"metadata":{"_uuid":"535c94e68729c98ea2341d1e586aa88fea61d118"},"cell_type":"markdown","source":"> # <a id='6'>6. Merging the data</a>"},{"metadata":{"trusted":true,"_uuid":"31caadcce9dca883497da8bf6ed221a1f5f41530"},"cell_type":"code","source":"def merge(df):\n    df = df.join(data_bureau_agg, how='left', on='SK_ID_CURR', lsuffix='1', rsuffix='2') \n    df = df.join(bureau_counts, on = 'SK_ID_CURR', how = 'left')\n    df = df.merge(previous_loan_counts, on = 'SK_ID_CURR', how = 'left')\n    df = df.join(data_credit_card_balance_agg, how='left', on='SK_ID_CURR', lsuffix='1', rsuffix='2')    \n    df = df.join(data_previous_application_agg, how='left', on='SK_ID_CURR', lsuffix='1', rsuffix='2')   \n    df = df.join(data_installments_payments_agg, how='left', on='SK_ID_CURR', lsuffix='1', rsuffix='2') \n    \n    return df\n\ntrain = merge(app_train)\ntest = merge(app_test)\ndisplay(train.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"115ce085078c451ad9c38bd2079c000eb6701363"},"cell_type":"code","source":"print(train.shape)\nprint(test.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7d99f241fd2e9f1b6de96df8d8f468953bcf7c3e"},"cell_type":"markdown","source":"> # <a id='7'>7. Combining training and testing data</a>"},{"metadata":{"trusted":true,"_uuid":"8a960e0285293edccb5fe58db4ff66944ce1525f"},"cell_type":"code","source":"#combining the data\nntrain = train.shape[0]\nntest = test.shape[0]\n\ny_train = train.TARGET.values\n\n#train_df = train_df.drop\n\nall_data = pd.concat([train, test]).reset_index(drop=True)\nall_data.drop(['TARGET'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a84b333d3fd460b2415abfbdedd294535da72a74"},"cell_type":"markdown","source":"> # <a id='8'>8. Feature Engineering Continued</a>"},{"metadata":{"trusted":true,"_uuid":"8c5602d4fc8e82589dda908c26e35f6d771f1e55"},"cell_type":"code","source":"# Now we will convert days employed and days registration and days id publish to a positive no. \ndef correct_birth(df):\n    \n    df['DAYS_BIRTH'] = round((df['DAYS_BIRTH'] * (-1))/365)\n    return df\n\ndef convert_abs(df):\n    df['DAYS_EMPLOYED'] = abs(df['DAYS_EMPLOYED'])\n    df['DAYS_REGISTRATION'] = abs(df['DAYS_REGISTRATION'])\n    df['DAYS_ID_PUBLISH'] = abs(df['DAYS_ID_PUBLISH'])\n    df['DAYS_LAST_PHONE_CHANGE'] = abs(df['DAYS_LAST_PHONE_CHANGE'])\n    return df\n\n# Now we will fill misisng values in OWN_CAR_AGE. \n#Most probably there will be missing values if the person does not own a car. So we will fill with 0\n\ndef missing(df):\n    \n    features = ['previous_loan_counts','NONLIVINGAPARTMENTS_MEDI', 'NONLIVINGAPARTMENTS_AVG','NONLIVINGAREA_MEDI','OWN_CAR_AGE']\n    \n    for f in features:\n        df[f] = df[f].fillna(0 )\n    return df\n\ndef transform_app(df):\n    df = correct_birth(df)\n    df = convert_abs(df)\n    df = missing(df)\n    return df\n\n   \n\nall_data = transform_app(all_data)\n\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3a6055689140c328f7a64395af2fe717202f33eb"},"cell_type":"code","source":"# counting no of phones given by the company and delete the irrelevant features\nall_data['NO_OF_CLIENT_PHONES'] = all_data['FLAG_MOBIL'] + all_data['FLAG_EMP_PHONE'] + all_data['FLAG_WORK_PHONE']\nall_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"927d152f15996c3fa2c72f8a9fcc824e15b5fef5"},"cell_type":"code","source":"# add a feature to determine if client's permanent city does not match with contact/work city\nall_data['FLAG_CLIENT_OUTSIDE_CITY'] = np.where((all_data['REG_CITY_NOT_WORK_CITY']==1) & (all_data['REG_CITY_NOT_LIVE_CITY']==1),1,0)\nall_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1f907d1d7e4ed526dbdf62ed88005a9d9ac962ee"},"cell_type":"code","source":" # add a feature to determine if client's permanent city does not match with contact/work region\nall_data['FLAG_CLIENT_OUTSIDE_REGION'] = np.where((all_data['REG_REGION_NOT_LIVE_REGION']==1) & (all_data['REG_REGION_NOT_WORK_REGION']==1),1,0)\nall_data.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0dafd2c1e4b2c5c60114e80daca7753c0fa8bfea"},"cell_type":"markdown","source":" ## <a id='8'>8.1. Deleting features</a>"},{"metadata":{"trusted":true,"_uuid":"0911e041f374a471ed1f6d5fe8fffefcf098cd29"},"cell_type":"code","source":"# deleting useless features\ndef delete(df):\n   # useless=['FLAG_MOBIL', 'FLAG_EMP_PHONE' ,'FLAG_WORK_PHONE','REG_CITY_NOT_WORK_CITY','REG_CITY_NOT_LIVE_CITY','REG_REGION_NOT_LIVE_REGION','REG_REGION_NOT_WORK_REGION']\n    #for feature in useless:\n     return df.drop(['FLAG_MOBIL', 'FLAG_EMP_PHONE' ,'FLAG_WORK_PHONE','REG_CITY_NOT_WORK_CITY','REG_CITY_NOT_LIVE_CITY','REG_REGION_NOT_LIVE_REGION','REG_REGION_NOT_WORK_REGION'], axis=1)\ndef transform(df):\n   # df = convert_abs(df)\n    df = delete(df)\n   \n    return df\n\nall_data = transform(all_data)\nall_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d77a960c80551da5257053276c073605b2856466"},"cell_type":"code","source":"# delete Ids\n\ndef delete_id(df):\n    return df.drop(['SK_ID_CURR', 'SK_ID_PREV','SK_ID_BUREAU'], axis = 1)\n\nall_data = delete_id(all_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"36434d8ca67b47ac64657e604d7c09c7c265ee71"},"cell_type":"code","source":"all_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ff9cde80e7b8c2412ce3adde81d1364544097991"},"cell_type":"code","source":"print(all_data.columns)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"602f6ec0c36bcbc407ea70ccaea169680005c417"},"cell_type":"markdown","source":"## <a id='8-2'>8.2  Handling Missing Values </a>"},{"metadata":{"trusted":true,"_uuid":"91e4ff918f45d5e84fc0e5f2acf18702d74d7475"},"cell_type":"code","source":"# handling missing values\n\ndef miss_numerical(df):\n    \n    features = ['previous_loan_counts','NONLIVINGAPARTMENTS_MEDI', 'NONLIVINGAPARTMENTS_AVG','NONLIVINGAREA_MEDI','OWN_CAR_AGE']\n    numerical_features = all_data.select_dtypes(exclude = [\"object\"] ).columns\n    #print(numerical_features)\n    for f in numerical_features:\n        #print(f)\n        if f not in features:\n            df[f] = df[f].fillna(df[f].median())\n      \n    return df\n\ndef miss_categorical(df):\n    \n    categorical_features = all_data.select_dtypes(include = [\"object\"]).columns\n    \n    for f in categorical_features:\n        df[f] = df[f].fillna(df[f].mode()[0])\n        \n    return df\n\ndef transform_feature(df):\n    df = miss_numerical(df)\n    df = miss_categorical(df)\n    #df = fill_cabin(df)\n    return df\n\nall_data = transform_feature(all_data)\n\n\nall_data.head()\n        ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5349f37ffa0b641d1f20c85b9ccb6fe3749ed23a"},"cell_type":"markdown","source":"## <a id='8-3'>8.3 Scaling Numerical Features </a>"},{"metadata":{"trusted":true,"_uuid":"dec691cd4fdb5c642e4f354845cca052fae95167"},"cell_type":"code","source":"# Scaling the data \n\nfrom sklearn.preprocessing import MinMaxScaler\n\ndef encoder(df):\n    scaler = MinMaxScaler()\n    numerical = all_data.select_dtypes(exclude = [\"object\"]).columns\n    features_transform = pd.DataFrame(data= df)\n    features_transform[numerical] = scaler.fit_transform(df[numerical])\n    display(features_transform.head(n = 5))\n    return df\n\nall_data = encoder(all_data)\n\n#display(all_data.head())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1bf4a405c94494e409011e7f6838fb86a0e75a47"},"cell_type":"markdown","source":"## <a id='8-4'>8.4 Converting into categorical features </a>"},{"metadata":{"trusted":true,"_uuid":"ce8d7adace8a63ef075bde2c716e91881edca026"},"cell_type":"code","source":"# Converting into categorical features\n\n# Create a label encoder object\nfrom sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\nle_count = 0\n\n\n# Iterate through the columns\nfor col in all_data:\n    if all_data[col].dtype == 'object':\n        # If 2 or fewer unique categories\n        if len(list(all_data[col].unique())) <= 2:\n            # Train on the training data\n            le.fit(all_data[col])\n            # Transform both training and testing data\n            all_data[col] = le.transform(all_data[col])\n            #test[col] = le.transform(test[col])\n            \n            # Keep track of how many columns were label encoded\n            le_count += 1\n           \nprint('%d columns were label encoded.' % le_count)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"48ed76f0cc7b3bf6df16b4c0d44f6ba53139a139"},"cell_type":"code","source":"# dummy variables\nall_data = pd.get_dummies(all_data)\n\ndisplay(all_data.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c2065b7b588b7bf69df8b6adeb37d91a9c3de79b"},"cell_type":"markdown","source":"> # <a id='9'>9. Modelling</a>"},{"metadata":{"trusted":true,"_uuid":"0624780a1f480762332a7de5098ffc2f928a6d62"},"cell_type":"code","source":"### Splitting features\ntrain = all_data[:ntrain]\ntest = all_data[ntrain:]\n\nprint(\"Training shape\", train.shape)\nprint(\"Testing shape\", test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1368520ec18e921a4802757c6ca213168b242392"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, Y_train, Y_test = train_test_split(train, y_train, test_size = 0.3, random_state = 200)\nprint(\"X Training shape\", X_train.shape)\nprint(\"X Testing shape\", X_test.shape)\nprint(\"Y Training shape\", Y_train.shape)\nprint(\"Y Testing shape\", Y_test.shape)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bc839f8b1ab54603dfbe2c95681e895eee8700b5"},"cell_type":"code","source":"from sklearn.metrics import make_scorer\nfrom sklearn.metrics import confusion_matrix, precision_recall_curve, roc_curve, auc, log_loss\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.grid_search import GridSearchCV\n\nlogreg = LogisticRegression(random_state=0, class_weight='balanced', C=100)\nlogreg.fit(X_train, Y_train)\nY_pred = logreg.predict_proba(X_test)[:,1]\n\n#Y_pred_proba = logreg.predict_proba(X_test)\n\nprint('Train/Test split results:')\n#print(logreg.__class__.__name__+\" accuracy is %2.3f\" % accuracy_score(Y_test, Y_pred))\nprint(\"ROC\",  roc_auc_score(Y_test, Y_pred))\n#print(logreg.__class__.__name__+\" auc is %2.3f\" % auc(fpr, tpr))\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b4cbdcd721b0b7225e2be3c96893ea08d4e319dc"},"cell_type":"code","source":"pred_test = logreg.predict_proba(test)\n#print(\"ROC\",  roc_auc_score(Y_test, pred_test))\nsubmission = pd.read_csv('../input/sample_submission.csv')\n\nsubmission['SK_ID_CURR']=app_test['SK_ID_CURR']\nprint(len(app_test['SK_ID_CURR']))\nsubmission['TARGET']=pred_test\n#converting to csv\n#print(submission['TARGET'])\npd.DataFrame(submission, columns=['SK_ID_CURR','TARGET'],index=None).to_csv('homecreditada.csv')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}