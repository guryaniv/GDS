{"metadata": {"kernelspec": {"language": "python", "name": "python3", "display_name": "Python 3"}, "language_info": {"nbconvert_exporter": "python", "file_extension": ".py", "pygments_lexer": "ipython3", "codemirror_mode": {"version": 3, "name": "ipython"}, "version": "3.6.1", "mimetype": "text/x-python", "name": "python"}}, "nbformat": 4, "nbformat_minor": 1, "cells": [{"cell_type": "markdown", "metadata": {"collapsed": true, "_uuid": "cc648e17950d0a8b65c89f70a09250280d77d56f", "_cell_guid": "437c4cfb-3177-4e38-a2ba-9d44101b7eb3"}, "source": ["# Forewords\n", "\n", "For this notebook, I wanted to try something different than most of what I could see with this dataset so far. Just for you to know the background, I am a full time banker and leasure time self-proclamed data scientist (or I guess, just a banker). So what I am trying here, is to bring some financial knowledge to make something out of this dataset.\n", "\n", "Most of the people who tried to played with these data took the stance of trying to predict future movements from past prices. While this seems natural, empirical evidences show that this is actually a difficult, if not impossible, task (this is what we call the weak form market efficiency hypothesis).\n", "\n", "This notebook is a WIP that I copy-pasted from an existing ipynb file stored on my computer (for some reason, I could not make an import on Kaggle). I'll try to maintain both version up to date.\n", "\n", "Enjoy!!\n", "\n", "# Introduction\n", "## Objectives\n", "\n", "The aim of this notebook is to demonstrate how machine learning can be used to enhance the stock picking process for a value oriented fund using the backward looking P/E for equity.\n", "\n", "The first section of this notebook is used to import libraries, retrieve data and define functions that might be useful later. The second section picks a bunch of securities using the traditional/naive P/E approach. The last part picks securities using various machine learning techniques.\n", "\n", "## Reminder on Value vs Growth Funds\n", "\n", "Investment funds can be split in a wide range of categories, depending on theirs assets, strategies and so on. Traditional (i.e. non-hedge funds) equity funds, are often described as \"value\", \"blend\" or \"growth\", depending on the type of equity they target. This toponomy is used for instance by <a href=http://www.morningstar.com/>Morningstar</a> for their funds ranking.\n", "\n", "In the value style, the aim of the fund manager is to outperform a benchmark by selecting stocks that she believes to be undervalued. On the contrary, growth oriented managers will try to pick stocks with a high potential for growth, despite the fact that they may not be cheaper relative to the market (the high price paid would be then compensated by the growth in price).\n", "\n", "Both styles have their merits and may perform well in different market conditions. The goal here is not define which style is better, but to assume that, provided that a value style has been chosen, to determine which stocks should be picked.\n", "\n", "## Financial Multiples and the P/E\n", "\n", "The use of multiples is widespread in the financial sector. It is a tool that allows (among other things) to screen a broad range of stock to select those that might be considered as good investment opportunities. One such multiple is the P/E or \"price-to-earnings\" ratio. As its name indicates, it is a measure of the price divided by earnings (per share) of the company. Multiplying both the denominator and the numerator by the number of shares, this formula can be rewritten using the market capitalization and the earnings of the full company.\n", "\n", "$$P/E = {\\mbox{Price per share}\\over{\\mbox{Earnings per Share}}} = {\\mbox{Market Capitalization}\\over{\\mbox{Earnings}}}$$\n", "\n", "From now on, I will use \"Price\" and \"Earnings\" in the forumla, but the reader should understand them as \"per share\" or globally, depending on the context. I may also use the abbreviation \"EPS\" instead of earnings per share.\n", "\n", "This simple formula overlooks the practical difficulties associated with the use of P/E. Indeed, this formula says nothing about the timeframe of the numerator and denominator. Traditionally, two different measures are used:\n", "\n", "$$P/E_\\mbox{forward looking} = {{Price_{t_0}}\\over{Earnings_{t_{1f}}}}$$\n", "$$P/E_\\mbox{backward looking} = {{Price_{t_0}}\\over{Earnings_{t_0}}}$$\n", "\n", "Where $t_0$ refers to the \"current\" time and $t_{1f}$ the forecasted value one year forward.\n", "As one could imagine, using the forward looking P/E is more cumbersome than the backward looking P/E as it requires more inputs in order to forecast the one year ahead earnings of the company. Thus when used in a screening process, the backward P/E is easier to use. This measure is less \"precise\" in the sense that it does not incorporate estimations of future earnings of the company, but it is more stable and does not depend on the model used to forecast the year ahead earnings. The reader should however note that, provided that a methodology has been defined to estime the year ahead earnings, the content of this notebook could be replaced by the forward looking P/E without much difficulties.\n", "\n", "In the remainder of this notebook, unless otherwise specified, the term \"P/E\" will refer to the backward looking P/E."]}, {"cell_type": "markdown", "metadata": {"_uuid": "5d4471083e2cdd50fab424d9b5709002157a7453", "_cell_guid": "d20d9bda-473c-4791-9971-13645ff3cc3a"}, "source": ["# Environment Setup"]}, {"cell_type": "markdown", "metadata": {"_uuid": "13ef0a6b456c60c49a0410fbbd99f1e7342c70b5", "_cell_guid": "53628bb5-691d-4479-8c0b-a9eaa34bcedf"}, "source": ["## Libraries\n", "This notebook use the traditional python libraries for data processing and statistical learning (numpy, pandas, scikit-learn). To vizualise graphs, I am mostly using the pandas' built-in for convenience, and seaborn for purely esthetical reason (i.e. I like the final results of seaborn graphs). I am using their conventional names to import them in the namespace.\n"]}, {"cell_type": "code", "metadata": {"collapsed": true, "_uuid": "29b8133e3320a6d68bafc535c6576ef14f26329f", "_cell_guid": "76a14d28-9344-411d-92e0-2f1086488dc4"}, "outputs": [], "source": ["%matplotlib inline\n", "\n", "#Basic data manipulation\n", "import numpy as np\n", "import pandas as pd\n", "\n", "#Tools\n", "import datetime as dt\n", "\n", "#IPython manager\n", "from IPython.display import display\n", "\n", "#Graphs\n", "import seaborn as sns\n", "import matplotlib.pyplot as plt\n", "\n", "#Machine learning\n", "from sklearn.neighbors import NearestNeighbors"], "execution_count": null}, {"cell_type": "markdown", "metadata": {"_uuid": "edfd23e587b7a527e03701a1bd309ab3bcbd8b20", "_cell_guid": "a8d833d9-7ecd-4916-8b2c-46cb19213c40"}, "source": ["## Data"]}, {"cell_type": "code", "metadata": {"collapsed": true, "_uuid": "43cc161a7e0039109cde70aee2113cf64b774c42", "_cell_guid": "a7eef694-8dfe-4bac-b917-19a2724f4a42"}, "outputs": [], "source": ["fundamentals = pd.read_csv('../input/fundamentals.csv',index_col = 0)\n", "prices = pd.read_csv('../input/prices.csv')\n", "securities = pd.read_csv('../input/securities.csv',index_col=0)\n", "sectors = securities['GICS Sector'].unique()\n", "sub_industry = securities['GICS Sub Industry'].unique()\n", "accounts = fundamentals.columns.values[2:]"], "execution_count": null}, {"cell_type": "markdown", "metadata": {"_uuid": "6b43db9884c63cb6f63a8da3b3075dab3d3b5c88", "_cell_guid": "89c2fe07-b30a-4280-82e0-b4fba86064a6"}, "source": ["## Helper Functions"]}, {"cell_type": "code", "metadata": {"collapsed": true, "_uuid": "64e8f91dedb992fecc7fdebf73f34c33d183f9b6", "_cell_guid": "d3f3c12d-e263-45f7-bfa2-525f211ae76d"}, "outputs": [], "source": ["def set_year(period):\n", "    \"\"\"\n", "    Take a date as a string formatted such as in the fundamentals dataframe and returns its year as an integer\n", "    \n", "    Parameters\n", "    ----------\n", "    period: Date as '%Y-%m-%d'\n", "    \n", "    Returns\n", "    -------\n", "    year: Year in the 'Period Ending' column\n", "    \"\"\"\n", "    x = dt.datetime.strptime(period,'%Y-%m-%d')\n", "    if x.month >= 6:        #If the reporting month is after June, return the reported year\n", "        return x.year\n", "    else:                   #Else, return the year before\n", "        return x.year - 1\n", "\n", "def get_publication_date(period):\n", "    \"\"\"\n", "    Returns the estimated publication date as a datetime object, assuming a 90 days delay from the reporting day\n", "    \n", "    Parameters\n", "    ----------\n", "    period: Reporting date string as '%Y-%m-%d'\n", "    \n", "    Returns\n", "    --------\n", "    published: Publication date as datetime object\n", "    \n", "    \"\"\"\n", "    x = dt.datetime.strptime(period,'%Y-%m-%d') + dt.timedelta(days=90)\n", "    \n", "    return x"], "execution_count": null}, {"cell_type": "markdown", "metadata": {"_uuid": "0358d7c88428e22b41ddc03395afd7638021b7a2", "_cell_guid": "557944fe-4f07-41b4-babf-6f342f796c3a"}, "source": ["# Traditional P/E Stock Picking\n", "## Introduction\n", "\n", "Prior to optimize the screening process, this section will demonstrate how the non-optimized process works.\n", "\n", "First, let us recall that the P/E can be obtained this way:\n", "\n", "$$P/E_\\mbox{backward looking} = {{Price_{t_{0}}}\\over{Earnings_{t_{0}}}}$$\n", "\n", "Naturally, if $Earnings_{t_{0}} < 0$, then this measure is of no use in trying to pick the most undervalued stocks. Therefore, these datapoint will be removed ex post.\n", "\n", "The reader should also understand that the amount of earnings per share is meaningless in itself. Indeed, a higher value is not necessarily better for the investor. For instance, if we have an EPS of USD 10.- with a share price of USD 100.-, this is equivalent to an EPS of USD 100.- with a share price of USD 1000.- (excluding effects on liquidity and other matters which are not discussed here).\n", "\n", "One difficulty which has not been discussed yet is the ambiguity surronding the $t_{0}$ concept. A delay of several months exists between the end of the period of financial statements and their official publications, mostly due to the auditing process. Companies may or may not discuss these figures before their publications (with the necessary caution to avoid material non-public disclosures and misrepresentations) or emit profit warning. \n", "\n", "While these dataset offers a broad range of data, the scope of the 'fundamentals' dataset is related to audited SEC fillings, so I will assume that {t_0} refers here to the day when the financial statements are publicly released and I will update the formula to:\n", "\n", "$$P/E_\\mbox{backward looking} = {{Price_{\\mbox{relase date}}}\\over{Earnings_{\\mbox{relase date - 3months}}}}$$\n", "\n", "## Earnings per Share Data Exploration\n", "\n", "My first goal when taking into account EPS is to remove datapoints where $EPS_t < 0$ and have a global idea of the figures I have. This last point is only to make sure they make sense. One more time, the amount of EPS in itself is not representative of how well a company is doing as it is biased by the stock price. The main reason to do this is that an extremely large value would be doubtful."]}, {"cell_type": "code", "metadata": {"collapsed": true, "_uuid": "eec2f290c3dc60fc0c43b0ce3be4debd9c8565db", "_cell_guid": "09247265-8d65-4b24-87d4-acd71771f6a9"}, "outputs": [], "source": ["df_eps = fundamentals[['Ticker Symbol','Period Ending','Earnings Per Share']]\n", "df_eps['Year'] = df_eps['Period Ending'].map(set_year)   #This replace values in Period Ending by their closest year\n", "df_eps = df_eps[df_eps['Year'] > 2011] #Just keep year from 2012\n", "\n", "df_eps[[col for col in df_eps.columns if col not in ['Period Ending','Year']]].describe() #I do not need stats about 'Period Ending'"], "execution_count": null}, {"cell_type": "code", "metadata": {"collapsed": true, "_uuid": "b63ab14352598f7f80332a2c1516b55c5b09d2ee", "_cell_guid": "5c2fd115-5e71-4912-9304-41741423bd85"}, "outputs": [], "source": ["_ = [col for col in df_eps.columns if col != 'Period Ending']\n", "sns.violinplot(data=df_eps[_],x='Year',y='Earnings Per Share')"], "execution_count": null}, {"cell_type": "markdown", "metadata": {"_uuid": "d2b461c742aef890dd45436236515e821de315ca", "_cell_guid": "0823e15a-9e62-4009-817e-a65da51d8409"}, "source": ["Both the description of EPS and the graphs indicates that no extremely large values are present. While there seems to be unusually high value, at this stage, they still might be explained by a high share value.\n", "\n", "I can now remove negative EPS values."]}, {"cell_type": "code", "metadata": {"collapsed": true, "_uuid": "1cdb83e055dab814390ea15f0e49009c561ba14a", "_cell_guid": "7f87859c-7305-4986-8177-302afd016a9d"}, "outputs": [], "source": ["df_eps = df_eps[df_eps['Earnings Per Share'] > 0.10] #Remove any data where EPS is below 1cts"], "execution_count": null}, {"cell_type": "markdown", "metadata": {"_uuid": "9cc54cf95b31ffefdda8c079b4fa92e1cd90480d", "_cell_guid": "fdd3f8f0-bfb4-4e11-b382-6431ce9e4794"}, "source": ["## Computing the P/E Ratio\n", "\n", "Before to go any further, I will remove companies that are not present in both the `prices` and `fundamentals` dataframes."]}, {"cell_type": "code", "metadata": {"collapsed": true, "_uuid": "5127364526c78b744539f2db7055da42e3d8e93f", "_cell_guid": "899c72c0-d126-4208-acc4-9ed88fa68d2a"}, "outputs": [], "source": ["_ = np.array(fundamentals['Ticker Symbol'].unique())\n", "companies = [symb for symb in prices['symbol'].unique() if symb in fundamentals['Ticker Symbol'].unique()]\n", "\n", "df_prices = prices[prices['symbol'].isin(companies)]\n", "df_eps = df_eps[df_eps['Ticker Symbol'].isin(companies)]\n", "\n", "#This is a quick summary of the 447 remaining stocks\n", "securities.loc[companies].head()"], "execution_count": null}, {"cell_type": "markdown", "metadata": {"_uuid": "b1f473457c8bd805d60a32255db5ec3352c821bf", "_cell_guid": "65ad30c0-982a-4172-ab44-841f5a17df44"}, "source": ["The objective now is to retrieve the prices for these securities at the publication date. Although the actual publication date is not available from these data, I will assume that it takes 90 days between the end of the reporting period and the actual publication date. This is a rule of thumb using past experiences and knowledge from friends working in the Big 4.\n", "\n", "As such, pandas does not allow to merge on \"approximate\" date. Therefore, I will use a trick that I found on <a href=\"https://stackoverflow.com/questions/33421551/how-to-merge-two-data-frames-based-on-nearest-date\">Stack Overflow</a>. The basic idea is to use a KNN algorithem, with $k=1$.\n", "\n", "I will then refine these results by setting the price retrieved to NA should the nearest date be off from the reporting date by more than one month (I would then consider that this data is not available).\n", "\n", "The following steps being rather time-consuming, I splitted them in different cells."]}, {"cell_type": "code", "metadata": {"collapsed": true, "_uuid": "66cfe568d273dfe8d62ec6a6f995a71ae2a0e8fc", "_cell_guid": "da15c766-5446-4f24-9b77-4af997c64bf3"}, "outputs": [], "source": ["def clean_prices_date(d):\n", "    try:\n", "        return dt.datetime.strptime(d,'%Y-%m-%d 00:00:00')\n", "    except:\n", "        return dt.datetime.strptime(d,'%Y-%m-%d')\n", "\n", "def find_nearest(group, match, groupname):\n", "    match = match[match[groupname] == group.name]\n", "    nbrs = NearestNeighbors(1).fit(match['date'].values[:, None])\n", "    dist, ind = nbrs.kneighbors(group['Published'].values[:, None])\n", "\n", "    group['KeyDate'] = group['Published']\n", "    group['ActualDate'] = match['date'].values[ind.ravel()]\n", "    return group\n", "\n", "df_eps['Published'] = df_eps['Period Ending'].map(get_publication_date)\n", "df_prices['date'] = df_prices['date'].map(clean_prices_date)\n", "\n", "df_eps_mod = df_eps.groupby('Ticker Symbol').apply(find_nearest, df_prices, 'symbol')\n"], "execution_count": null}, {"cell_type": "code", "metadata": {"collapsed": true, "_uuid": "ab7b52d04ff3508087a40efc894babfadca77cfb", "_cell_guid": "483ded8a-f5de-4bc0-aead-26ab54b318ea"}, "outputs": [], "source": ["df_prices_mod = df_prices[[col for col in df_prices.columns]]\n", "df_prices_mod.rename(columns={'symbol':'Ticker Symbol','date':'ActualDate'},inplace=True)\n", "df_merged = pd.merge(df_eps_mod,df_prices_mod,on=['ActualDate','Ticker Symbol'])\n"], "execution_count": null}, {"cell_type": "markdown", "metadata": {"_uuid": "6a781c8a1d862beb32fe6bd9ac08afa5d95458d1", "_cell_guid": "aa5ba1e1-1096-44ff-b733-afd405a9e441"}, "source": ["We can now compute the P/E ratio for our companies by simply dividing the columns. Concerning the price to use, we have the choice among four values:\n", "<ul>\n", "<li>The opening price</li>\n", "<li>The closing price</li>\n", "<li>The max price of the day</li>\n", "<li>The min price of the day</li>\n", "</ul>\n", "\n", "Whichever is chosen, the result would be arbitrary. I opted to take the closing price here as it is often used by investment fund to compute the daily NAV."]}, {"cell_type": "code", "metadata": {"collapsed": true, "_uuid": "9901e0121ede130fba974eda26238f4bee60e405", "_cell_guid": "2a9af030-22a6-41ae-8a7f-418b556f7fcc"}, "outputs": [], "source": ["df_pe = df_merged[['Ticker Symbol','Year','Earnings Per Share','close']]\n", "df_pe['PE'] = df_pe['close'] / df_pe['Earnings Per Share']"], "execution_count": null}, {"cell_type": "markdown", "metadata": {"_uuid": "6f368b4d4aa4e493d23c17a5e95ebc1f718a1b8d", "_cell_guid": "a5a90d07-9505-475a-ab63-915551cfdb8e"}, "source": ["The computation of the P/E ratio is now complete. In order to have better view of the results, I will split these P/E by sector. As it will be discussed later, companies in the same sector/industry are expected to have similar P/E ratios."]}, {"cell_type": "code", "metadata": {"collapsed": true, "_uuid": "f3453163593004af366b57749e3f4dbb324497d2", "_cell_guid": "5e2dd04e-75c6-4595-ae02-04994ddc42be"}, "outputs": [], "source": ["_ = securities.loc[df_pe['Ticker Symbol'],'GICS Sector']\n", "_.index = df_pe.index\n", "df_pe_sector = pd.concat([df_pe,_],axis=1)"], "execution_count": null}, {"cell_type": "code", "metadata": {"collapsed": true, "_uuid": "ed5019016a335b607efc1d0820e0573df037462d", "_cell_guid": "8ed5a5a3-b967-41e9-99e6-fbd370de794a"}, "outputs": [], "source": ["yearsplt = [2014,2015]\n", "g = sns.violinplot(data=df_pe_sector[df_pe_sector['Year'].isin(yearsplt)][['PE','GICS Sector','Year']],\n", "                   x='GICS Sector',y='PE',hue='Year',split=True)\n", "plt.xticks(rotation=90)"], "execution_count": null}, {"cell_type": "markdown", "metadata": {"_uuid": "0cbc3b7da37dad837c6705384c5f7f35eb76be5b", "_cell_guid": "a5afd9e2-09e2-423a-bb24-56b7b40877fe"}, "source": ["The tables below summarize the breakdown of P/E ratios at the sub-industry level."]}, {"cell_type": "code", "metadata": {"collapsed": true, "_uuid": "d358e8c115390106b3bf309e9c6d65802a9897bb", "_cell_guid": "684593d3-8635-4aa5-a7cc-05927834aca3"}, "outputs": [], "source": ["_ = securities.loc[df_pe['Ticker Symbol'],'GICS Sub Industry']\n", "_.index = df_pe.index\n", "df_pe_subindustry = pd.concat([df_pe,_],axis=1)"], "execution_count": null}, {"cell_type": "code", "metadata": {"collapsed": true, "_uuid": "948eb275ff1c82ac2317c9f7f36919589fbe12c3", "_cell_guid": "db509b9b-a27c-4e93-955d-2980fb527b19"}, "outputs": [], "source": ["pd.pivot_table(data=df_pe_subindustry[['GICS Sub Industry','PE','Year']],\n", "               index='GICS Sub Industry',columns='Year',values='PE').sort_values(2015,ascending=False)"], "execution_count": null}, {"cell_type": "markdown", "metadata": {"_uuid": "e431b186b8e0eeb9e16a95b7c3b86b89196592ab", "_cell_guid": "a191d18b-4709-40f6-9a3e-bb50e8ad0e0b"}, "source": ["# Predicting P/E Ratios\n", "\n", "Now I obtained the P/E through the traditional means, it is time to add some machine learning on top of that in order to see which companies are out of track.\n", "\n", "In order to predict what should be the correct P/E, let us go back to some theory. The P/E ratio for a given company can be modeled using the Gordon Growth models. This model assume that:\n", "<ol>\n", "<li>The growth rate of earnings is constant over time</li>\n", "<li>The discount rate applied to a companies cash flow is constant</li>\n", "<li>The relevant cash flow for valuation is dividends</li>\n", "</ol>\n", "\n", "While this model is well suited for large, mature companies, it is less so for startups and tech companies with a high/multi-staged growth rate. Nevertheless, it gives us some indications on features that might be relevant to predict the P/E ratio.\n", "\n", "Assuming that dividends are discounted at a rate $K_e$ (for cost of equity) with a growth rate $g$, the price of an action is the following:\n", "\n", "$$P_{t_0}={{Dividends_{t_1}}\\over{K_e - g}}$$\n", "\n", "Using the fact that the growth rate for dividends is constant (thus $Dividends_{t_0} = Dividends_{t_1} * (1 + g)$), this formula can be rewritten as follow:\n", "\n", "$$P_{t_0}={{Dividends_{t_0} * (1 + g)}\\over{K_e - g}}$$\n", "\n", "Finally, we can compute the price of an action as:\n", "\n", "$$P_{t_0}={{Earnings_{t_0} * b * (1 + g)}\\over{K_e - g}}$$\n", "where $b = {{Dividends_{t_0}}\\over{Earnings_{t_0}}} = \\mbox{Dividends distribution rate}$\n", "\n", "This leads to a ex-ante backward looking P/E ratio of:\n", "\n", "$${P/E}_{t_0} = {{b * (1 + g)}\\over{K_e - g}}$$\n", "\n", "We can now estimate the dividend distribution rate using the data in the \"fundamentals\" table. A simple accounting relationship gives us:\n", "$$\\mbox{Retained Earnings}_{t_1} = \\mbox{Retained Earnings}_{t_0} + \\mbox{Net Income}_{t_1} - Dividends_{t_1}$$\n", "\n", "Thus:\n", "\n", "$$Dividends_{t_1} = (\\mbox{Retained Earnings}_{t_0} - \\mbox{Retained Earnings}_{t_1}) + \\mbox{Net Income}_{t_1} $$\n", "\n", "Here, the dividends account both for actual dividends and share repurchases. Please note that retained earnings are part of the balance sheet and should therefore be treated as a \"stock\" while the net incomes and dividends are both included in the P&L and are therefore considered as a flow. The $t_0$ and $t_1$ indices have been chosen here such as they represent the actual year of financial statements in which they appear.\n", "\n", "We can then compute the dividend distribution rate:\n", "\n", "$$b = 1 + {{(\\mbox{Retained Earnings}_{t_0} - \\mbox{Retained Earnings}_{t_1})}\\over{\\mbox{Net Income}_{t_1}}}$$"]}, {"cell_type": "code", "metadata": {"collapsed": true, "_uuid": "4078c8cf76b72cdb9a11694a2745a32e11943983", "_cell_guid": "709696dc-9e28-409e-8f5e-a7e46f17ea79"}, "outputs": [], "source": ["#df_eps[['Stock Symbol','Year','Retained Earnings']]\n", "df_re_ni = fundamentals[['Ticker Symbol','Period Ending','Net Income','Retained Earnings']]\n", "df_re_ni['Year'] = df_re_ni['Period Ending'].map(set_year)\n", "df_re_ni = df_re_ni[df_re_ni['Year']>2011]\n", "_ = df_re_ni[['Ticker Symbol','Year','Retained Earnings']]\n", "x = pd.pivot_table(data=_,values='Retained Earnings',columns='Year',index='Ticker Symbol')\n", "re0 = x[[x for x in range(2012,2016)]]/1000000\n", "re1 = x[[x for x in range(2013,2017)]]/1000000\n", "re0.columns = re1.columns\n", "delta_re = re0 - re1\n", "_ = df_re_ni[['Ticker Symbol','Year','Net Income']]\n", "ni = pd.pivot_table(data=_,values='Net Income',columns='Year',index='Ticker Symbol')/1000000\n", "df_b = 1+delta_re.divide(ni[delta_re.columns])# - re\n", "df_b.columns = ['b_'+str(x) for x in df_b.columns]\n", "pt_pe = pd.pivot_table(data=df_pe,columns='Year',values='PE',index='Ticker Symbol')\n", "pt_pe.columns = ['pe_'+str(x) for x in pt_pe.columns]\n", "_ = pd.concat([pt_pe,df_b],axis=1)[['pe_2014','b_2014']]\n", "_ = pd.concat([_,securities.loc[_.index][['GICS Sector']]],axis=1)\n", "_ = _[(_['pe_2014'] < 60) & (_['b_2014'] < 1) & (_['b_2014'] > 0)]\n", "\n", "sns.lmplot(data=_,x='b_2014',y='pe_2014',\n", "           hue='GICS Sector',col='GICS Sector',col_wrap=3, ci=None, truncate=True,\n", "           sharex = False)\n", "#sns.lmplot(data=df_b,x='',y='')"], "execution_count": null}, {"cell_type": "markdown", "metadata": {"_uuid": "6037c18c1af65087fb76b02b2693ac16d6d9ae47", "_cell_guid": "5100774b-3264-4b83-8ce7-0478bcbb3b24"}, "source": ["Without digging further, some of these graphs would suggest that the theory does not pass the smell test. However, the following comments provide some perspectives:\n", "<ul>\n", "<li>First of all, recall that we are looking only at one of several components of a very simplified model. The fact that some of these graphs display an upward sloping trend is encouraging in itself.</li>\n", "<li>Long standing sectors such as the industrial, real estate and financial sectors which can now be considered as mature fit the model more or less neatly in this model.</li>\n", "<li>IT and health care are growing sector (the first one for obvious reasons and the second due to aging of the population). The fact that they exhibit a downward sloping curve is not alarming.</li>\n", "<li>This is a simple hypothesis that worth investing further, but the flat curve for the energy sector could be explained by the discrepencies between companies specialized in shale vs traditional energy produces.</li>\n", "<li>Materials and consumer discretionary are harder to explain.</li>\n", "</ul>"]}]}