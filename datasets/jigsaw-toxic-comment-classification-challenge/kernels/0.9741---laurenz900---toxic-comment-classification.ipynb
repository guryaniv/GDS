{"cells":[{"metadata":{"collapsed":true,"_uuid":"07c656f167aea06499c014f65b9d10c34ec50806","_cell_guid":"dfa27cfc-d8b4-4231-a6fd-6b589191393a","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.feature_selection import SelectPercentile\nfrom sklearn.feature_selection import chi2\nfrom sklearn.model_selection import cross_val_score\nimport matplotlib.pyplot as plt\nfrom scipy.sparse import hstack\nfrom scipy import stats\n\nfrom subprocess import check_output\n#print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n","execution_count":1,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"bb0c2fab49b00740a286d14029cd862a10606f7b","_cell_guid":"398cb59d-83a5-44b6-acda-41d4006004cc","trusted":true},"cell_type":"code","source":"df_train = pd.read_csv('../input/train.csv')\ndf_predict = pd.read_csv('../input/test.csv')","execution_count":2,"outputs":[]},{"metadata":{"scrolled":true,"_uuid":"c751937b57a0c54bd2ff418ab2fb6026ca7b2ea1","_cell_guid":"bd933e91-b2b3-4edd-b82a-efb2ea4a2778","trusted":true},"cell_type":"code","source":"# Generating additional new features and cleaning text with RegEx\n\ndef add_features(df):\n    # new column for exclamation mark\n    df['ex_mark'] = df['comment_text'].str.findall('\\!+')\n    df['ex_mark'] = df['ex_mark'].apply(lambda x: len(x))\n    df['ex_mark'][ df['ex_mark']>  df['ex_mark'].quantile(.9)] = df['ex_mark'].quantile(.9) #remove outsiders\n    \n    # new column for question mark\n    df['qu_mark'] = df['comment_text'].str.findall('\\?+')\n    df['qu_mark'] = df['qu_mark'].apply(lambda x: len(x))\n    df['qu_mark'][ df['qu_mark']>  df['qu_mark'].quantile(.9)] = df['qu_mark'].quantile(.9) #remove outsiders\n    \n    # new column for *\n    df['star_mark'] = df['comment_text'].str.findall('\\*+')\n    df['star_mark'] = df['star_mark'].apply(lambda x: len(x))\n\n    # new columns for smileys\n    smileys_good = r'((:|;|X)-?(\\)|P|D))\\W'\n    smileys_bad =  r'((:|;)-?(\\())\\W'\n    df['smileys_good'] = df['comment_text'].str.extract(smileys_good, expand=True)[0].fillna(0)\n    df['smileys_bad'] = df['comment_text'].str.extract(smileys_bad, expand=True)[0].fillna(0)\n\n    df['smileys_good'][df['smileys_good']!=0] = 1\n    df['smileys_bad'][df['smileys_bad']!=0] = 1\n    \n    # new column link_count\n    df['link_count'] = df['comment_text'].str.findall(r'\\wwww\\.')\n    df['link_count'] = df['link_count'].apply(lambda x: len(x))\n    \n    # new column quote_count\n    df['quote_count'] = df['comment_text'].str.findall(r'(\\'+|\\\"+)')\n    df['quote_count'] = df['quote_count'].apply(lambda x: len(x))\n    df['quote_count'][ df['quote_count']>  df['quote_count'].mean()*2] = df['quote_count'].mean()*2\n    \n    # new column comma_count\n    df['comma_count'] = df['comment_text'].str.findall(r'\\,+')\n    df['comma_count'] = df['comma_count'].apply(lambda x: len(x))\n    df['comma_count'][ df['comma_count']>  df['comma_count'].mean()*2] = df['comma_count'].mean()*2\n    \n    \n    # cleaning text\n    df['comment_text'] = df['comment_text'].str.replace(r'a*h+a+h+a+', 'haha')\n    df['comment_text'] = df['comment_text'].str.replace(r'a+hh+', 'ahh')\n    df['comment_text'] = df['comment_text'].str.replace(r'(l+o+l+\\s?)+', 'lol')\n    df['comment_text'] = df['comment_text'].str.replace(r'a+b+c\\w*', 'abc')\n    df['comment_text'] = df['comment_text'].str.replace(r'a+r+g+h+', 'argh')\n    df['comment_text'] = df['comment_text'].str.replace(r'a+w+e+s+o+m+e+', 'awesome')\n    df['comment_text'] = df['comment_text'].str.replace(r'\\ba*f+u+c*k*\\b', 'fuck')\n    df['comment_text'] = df['comment_text'].str.replace(r'aa+ww+', 'aww')\n    df['comment_text'] = df['comment_text'].str.replace(r'y+e*a+y+', 'yeah')\n    df['comment_text'] = df['comment_text'].str.replace(r'y+e+a+h+', 'yeah')\n    df['comment_text'] = df['comment_text'].str.replace(r'y+e{2,}s{2,}', 'yeah')\n    df['comment_text'] = df['comment_text'].str.replace(r'ass', 'azz')\n    \n    # replace char repetitions\n    df['comment_text'] = df['comment_text'].str.replace(r'(.)\\1+', r\"\\1\")\n        \n    return df\n    \ndf_train = add_features(df_train)\ndf_predict = add_features(df_predict)\n\ndf_train.describe()","execution_count":3,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"e12d36bf843bd2a0d80d487c5cd12c6eac735d23","_cell_guid":"a5eb9859-ef7a-4a2a-9210-3697dc7d1209","trusted":true},"cell_type":"code","source":"# Fit the vectorizer to whole data set\nall_text = pd.concat([df_train['comment_text'], df_predict['comment_text']])\n# Here comparing binarized scores with Tfidf scores\nbin_vect = TfidfVectorizer(min_df=4, ngram_range=(1,2), stop_words='english', lowercase=True, binary=True).fit(all_text)\nvect = TfidfVectorizer(min_df=4, ngram_range=(1,2), stop_words='english', lowercase=True, binary=False).fit(all_text)","execution_count":4,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"938c6a39f173492fe7421d465177b5b99932a5eb","_cell_guid":"7b320be9-9119-4217-a09b-920874a6a936","trusted":true},"cell_type":"code","source":"# Split data into X and Y\nX_train = df_train['comment_text']\nX_predict = df_predict['comment_text']\nY = df_train[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']]\n\n# transform the documents in the training data to a sparse matrix\nX_train_vectorized = vect.transform(X_train)\nX_train_bin_vectorized = bin_vect.transform(X_train)\nX_predict_vectorized = vect.transform(X_predict)","execution_count":5,"outputs":[]},{"metadata":{"scrolled":true,"_uuid":"a98dd92d847a17de4e08af095542d077bd7cccad","_cell_guid":"a395c8a5-2304-4804-a2b5-e96ab32e1cd6","trusted":true},"cell_type":"code","source":"# plot influence of Chi2 dimensionality reduction\ndef plot_model():\n    lr = LogisticRegression()\n    percentiles = (60, 70, 80, 90)\n    best_scores = []\n    \n    for y_col in Y.columns:\n        score_maxs, score_means, score_mins = [], [], []\n        bin_score_maxs, bin_score_means, bin_score_mins = [], [], []\n\n        for percentile in percentiles:\n            # fit chi2-Filter to binary values and apply filter to both X-sets\n            chi2_filter = SelectPercentile(chi2, percentile)\n            X_train_bin_vectorized_new = chi2_filter.fit_transform(X_train_bin_vectorized, Y[y_col])\n            X_train_vectorized_new = chi2_filter.transform(X_train_vectorized)\n            # calculate cross-validation scores for filtered sets\n            bin_scores = cross_val_score(lr, X_train_bin_vectorized_new, Y[y_col], n_jobs=1)\n            scores = cross_val_score(lr, X_train_vectorized_new, Y[y_col], n_jobs=1)\n            bin_score_maxs.append(bin_scores.max())\n            bin_score_means.append(bin_scores.mean())\n            bin_score_mins.append(bin_scores.min())\n            score_maxs.append(scores.max())\n            score_means.append(scores.mean())\n            score_mins.append(scores.min())\n        #plot results \n        fig, ax = plt.subplots()\n        ax.plot(percentiles, bin_score_means, label='binary')\n        ax.fill_between(percentiles, bin_score_maxs, bin_score_mins, alpha=.5)\n        ax.plot(percentiles, score_means, label='cont.')\n        ax.fill_between(percentiles, score_maxs, score_mins, alpha=.5)\n        plt.legend()\n        plt.title(y_col)\n        plt.xlabel('Percentile')\n        plt.ylabel('Prediction rate')\n        plt.show()\n\nplot_model()\n        \n# fit and apply Chi²-Filter\nchi2_filter = SelectPercentile(chi2, 65)\nX_train_filtered = chi2_filter.fit_transform(X_train_vectorized, Y)\nX_predict_filtered = chi2_filter.transform(X_predict_vectorized)\n","execution_count":6,"outputs":[]},{"metadata":{"_kg_hide-output":true,"_uuid":"5df4d0943ccefd3dc0f368e1c46e1c843c80903a","_kg_hide-input":false,"_cell_guid":"b8afb682-ec88-47de-95ff-3de841c6deb3","trusted":true},"cell_type":"code","source":"# add additional features features\n\ndef plot_model2():\n    add_features = ['ex_mark', 'smileys_good', 'smileys_bad', 'star_mark',\n                        'qu_mark', 'comma_count', 'quote_count']\n\n    for y_col in Y.columns:\n        scores_mean, scores, scores_diff = [], [], []\n        score0 = np.mean(cross_val_score(LogisticRegression(), X_train_filtered, Y[y_col], n_jobs=1))\n        for feature in add_features:\n            train_features =  hstack([X_train_filtered, np.array(df_train[feature].astype('int64'))[:,None]])\n            cv_score = cross_val_score(LogisticRegression(), train_features, Y[y_col], n_jobs=1)\n            scores_mean.append(np.mean(cv_score))\n            scores_diff.append((np.max(cv_score)-np.min(cv_score))/2)\n            scores.append(cv_score)\n\n        fig, ax = plt.subplots()\n        ax.bar(add_features, scores_mean, alpha=0.8, yerr=scores_diff)\n        ax.plot(add_features, np.full([len(add_features)], score0), 'k')\n        plt.ylim(score0-np.abs(score0-np.min(scores))*1.2, score0+np.abs(score0-np.max(scores))*1.5)\n        plt.xticks(rotation='vertical')\n        plt.title(y_col)\n        plt.xlabel('Percentile')\n        plt.ylabel('feature')\n        plt.show()\n    \nplot_model2()\n\n# adding ex_mark as additional feature\ntrain_features =  hstack([X_train_filtered, np.array(df_train['ex_mark'].astype('int64'))[:,None]])\npredict_features =  hstack([X_predict_filtered, np.array(df_predict['ex_mark'].astype('int64'))[:,None]])","execution_count":7,"outputs":[]},{"metadata":{"scrolled":true,"_uuid":"bb38c81fba37b09e4127075ddc7f97cfaaedce00","_cell_guid":"0b89911a-20a3-4c04-9d06-0faea1db9ea1","trusted":true},"cell_type":"code","source":"#predict Y\n#additional tuning of hyperparameters with GridsearchCV\n\nmodel = LogisticRegression()\nparams = {'C':[1]}\n\nY_predicted = pd.DataFrame()\nY_predicted['id'] = df_predict['id']\nscores = []\n\nfor y_col in Y.columns:\n    gsCV = GridSearchCV(model, params, scoring=\"roc_auc\").fit(train_features, Y[y_col])\n    scoreX = np.max(gsCV.cv_results_['mean_test_score'])\n    scores.append(scoreX)\n    Y_predicted[y_col] = gsCV.predict_proba(predict_features)[:,1]\n    print(y_col + ':' + str(scoreX))\nprint('mean score: ' + str(np.mean(scores)))","execution_count":8,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"13f04b4f5bfccfd84099a5bd65dbbefee0761f3e","_cell_guid":"9c5bf9e2-1fab-4f98-8d9b-a60cc166220a","trusted":false},"cell_type":"code","source":"submission = Y_predicted\nsubmission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}