{"cells": [{"source": "The purpose of this kernel is to illustrate the usefulness of stacking, and using online algorithms to deal with the massive feature space after creating our features, with different loss functions and penalty functions.", "outputs": [], "cell_type": "markdown", "execution_count": null, "metadata": {"_execution_state": "idle", "collapsed": false, "_cell_guid": "aae4e318-0a56-4466-86b5-ccdbfb112594", "_uuid": "08198b6e6cf69310e7bb30cc04618306d25a1ce6"}}, {"source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.", "outputs": [], "cell_type": "code", "execution_count": null, "metadata": {"_execution_state": "idle", "trusted": false, "_cell_guid": "30d8f742-6029-40af-8642-d8562bdde4d9", "_uuid": "485c31f7d021637b9106ff734d5c0a75413ca383"}}, {"source": "First we load our data", "outputs": [], "cell_type": "markdown", "execution_count": null, "metadata": {"_execution_state": "idle", "collapsed": false, "_cell_guid": "6817392b-294b-4a6b-af13-14dc0c651979", "_uuid": "74a6c20126e108e86a3a25aff43c2e7f2845a931"}}, {"source": "train = pd.read_csv('../input/train.csv', delimiter = ',' )\ntest = pd.read_csv('../input/test.csv', delimiter = ',' )\n\ndf_sub = pd.DataFrame( columns = ['ID', 'y'] )\n\ndf_sub['ID'] = test.pop('ID')\n\ntrain_labels = np.log1p( train.pop('y') )\ntrain.drop('ID', axis = 1, inplace = True)", "outputs": [], "cell_type": "code", "execution_count": null, "metadata": {"_execution_state": "idle", "trusted": false, "collapsed": false, "_cell_guid": "06483bf8-1fb6-4780-a24e-ee08a211bdd1", "_uuid": "ba1dd579798d0b0fc8810968d9b861dc5cd9b918"}}, {"source": "This takes each category piece of information, and binds them together into distinct pairs.  This should aid our models in the prediction process. \n\nWe also filter out low frequency categorial values.\n\nFirst, we normalize all numeric features", "outputs": [], "cell_type": "markdown", "execution_count": null, "metadata": {"_execution_state": "idle", "collapsed": false, "_cell_guid": "fb84af6c-d71b-40ee-b3b4-8d24b8f7b01e", "_uuid": "3cd0838771d09c0699dfafb61a2c6b983925a0bf"}}, {"source": "#Normalize numeric features\ndef normdf( train, test ):\n\n    from scipy.stats import skew\n\n    numeric_feats = train.dtypes[train.dtypes != \"object\"].index\n\n    for col in numeric_feats:\n        cardinality_train = len(np.unique(train[col]))\n        cardinality_test = len(np.unique(test[col]))\n\n        if cardinality_train == 1 | cardinality_test == 1:\n            train.drop(col, axis = 1, inplace = True)\n            test.drop(col, axis = 1, inplace = True)\n\n    numeric_feats = train.dtypes[train.dtypes != \"object\"].index\n\n    skewed_feats = train[numeric_feats].apply(lambda x: skew(x.dropna())) #compute skewness\n    left_skewed_feats = skewed_feats[skewed_feats > 0.75].index\n    right_skewed_feats = skewed_feats[skewed_feats < -0.75].index\n\n    train[left_skewed_feats] = np.log1p(train[left_skewed_feats])\n    test[left_skewed_feats] = np.log1p(test[left_skewed_feats])\n\n    train[right_skewed_feats] = np.expm1(train[right_skewed_feats])\n    test[right_skewed_feats] = np.expm1(test[right_skewed_feats])\n\n    return train, test", "outputs": [], "cell_type": "code", "execution_count": null, "metadata": {"_execution_state": "idle", "trusted": false, "collapsed": false, "_cell_guid": "e671681f-a29c-437f-86d8-bc0e5498311e", "_uuid": "245fb8b79aa757d65f80e209522dd6d21a44020e"}}, {"source": "Now we prep our data.", "outputs": [], "cell_type": "markdown", "execution_count": null, "metadata": {"_execution_state": "idle", "collapsed": false, "_cell_guid": "96420ceb-27ee-482c-ab6f-35bdd2dec3b2", "_uuid": "f99ae53f4020c8d37fe93649210d494c8a66a280"}}, {"source": "def dataPrep( df ):\n\n    cat_cols = df.select_dtypes(['object']).columns\n\n    n = len(cat_cols)\n\n    print ('\\nConcat string columns')\n\n    #This concat string columns together, first in twos and then in threes\n    for i in range(n):\n\n        col1 = cat_cols[i]\n\n        for j in range(i+1,n):\n\n            col2 = cat_cols[j]\n\n            new_col = col1 + '_' + col2\n\n            df[new_col] = df[col1].str.cat(df[col2], sep = '_')\n\n    cat_columns = df.select_dtypes(['object']).columns\n\n    df_cat = df[cat_columns]\n\n    df_cat = pd.get_dummies( df_cat )\n\n    #Remove low frequency cat columns\n    sums = df_cat.sum(axis = 0)\n\n    l_bound = 0.2*df.shape[0]\n    u_bound = 0.8*df.shape[0]\n\n    to_remove = sums[sums < l_bound].index.values\n    df_cat.drop(to_remove, axis = 1, inplace = True)\n\n    to_remove = sums[sums > u_bound].index.values\n    df_cat.drop(to_remove, axis = 1, inplace = True)\n    \n    df.drop(cat_columns, axis=1, inplace = True)\n\n    df = pd.concat( [df, df_cat], axis = 1 )\n\n    return df", "outputs": [], "cell_type": "code", "execution_count": null, "metadata": {"_execution_state": "idle", "trusted": false, "collapsed": false, "_cell_guid": "f3927f9f-9bbc-49e3-a6e9-78845fcbf237", "_uuid": "3d344a8792b008f1640e91b83ee62f82c6b2181b"}}, {"source": "train, test = normdf( train.copy(), test.copy() )\n\nprint ('Create training dataset')\n\ntrain = dataPrep( train )\n\nprint ('\\nCreate testing dataset')\n\ntest = dataPrep( test )\n\ncols = list( set(train.columns) & set(test.columns) )\n\ntrain = train[cols]\ntest = test[cols]", "outputs": [], "cell_type": "code", "execution_count": null, "metadata": {"_execution_state": "idle", "trusted": false, "collapsed": false, "_cell_guid": "898f46a0-32b6-4727-b9c0-69348a7c5a47", "_uuid": "f173a1f2e5147abbc10876f9054f1693ef7c27de"}}, {"source": "This is to group our variables", "outputs": [], "cell_type": "markdown", "execution_count": null, "metadata": {"_execution_state": "idle", "collapsed": false, "_cell_guid": "5b7322c2-03df-44fe-a980-361f1f6d9d5d", "_uuid": "96f3124e4bbaeeaf3e00aa44740ccba2b61d94b0"}}, {"source": "from sklearn.decomposition import PCA, FastICA\nfrom sklearn.random_projection import GaussianRandomProjection\nfrom sklearn.random_projection import SparseRandomProjection\nfrom sklearn.decomposition import TruncatedSVD\n\nn_comp = 12\n\n    # tSVD\ntsvd = TruncatedSVD(n_components=n_comp, random_state=420)\ntsvd_results_train = tsvd.fit_transform(train)\ntsvd_results_test = tsvd.transform(test)\n\n    # PCA\npca = PCA(n_components=n_comp, random_state=420)\npca2_results_train = pca.fit_transform(train)\npca2_results_test = pca.transform(test)\n\n    # ICA\nica = FastICA(n_components=n_comp, random_state=420)\nica2_results_train = ica.fit_transform(train)\nica2_results_test = ica.transform(test)\n\n    # GRP\ngrp = GaussianRandomProjection(n_components=n_comp, eps=0.1, random_state=420)\ngrp_results_train = grp.fit_transform(train)\ngrp_results_test = grp.transform(test)\n\n    # SRP\nsrp = SparseRandomProjection(n_components=n_comp, dense_output=True, random_state=420)\nsrp_results_train = srp.fit_transform(train)\nsrp_results_test = srp.transform(test)\n\nfor i in range(1, n_comp + 1):\n    train['pca_' + str(i)] = pca2_results_train[:, i - 1]\n    test['pca_' + str(i)] = pca2_results_test[:, i - 1]\n\n    train['ica_' + str(i)] = ica2_results_train[:, i - 1]\n    test['ica_' + str(i)] = ica2_results_test[:, i - 1]\n\n    train['tsvd_' + str(i)] = tsvd_results_train[:, i - 1]\n    test['tsvd_' + str(i)] = tsvd_results_test[:, i - 1]\n\n    train['grp_' + str(i)] = grp_results_train[:, i - 1]\n    test['grp_' + str(i)] = grp_results_test[:, i - 1]\n\n    train['srp_' + str(i)] = srp_results_train[:, i - 1]\n    test['srp_' + str(i)] = srp_results_test[:, i - 1]\n\n\nprint ('Feature Space Before: ' + str( train.shape[1] ) )\n\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.linear_model import LassoCV\n\nfeat_sel = SelectFromModel( LassoCV( cv = 5, fit_intercept = False ) )\n\nfeat_sel.fit( train, train_labels )\n\ntrain = feat_sel.transform( train )\n\ntest = feat_sel.transform( test )\n\nprint ('Feature Space After: ' + str( train.shape[1] ) )", "outputs": [], "cell_type": "code", "execution_count": null, "metadata": {"_execution_state": "idle", "trusted": false, "collapsed": false, "_cell_guid": "da307d3f-9d4d-446b-bdf1-b7aa8c8fbbfd", "_uuid": "a4bb4440e37de01e3105c6b62acd90dc31c6a45a"}}, {"source": "This is to create a custom boosting method, with a linear model to find the optimal weights for our decision regression trees from boosting method.", "outputs": [], "cell_type": "markdown", "execution_count": null, "metadata": {"_execution_state": "idle", "collapsed": false, "_cell_guid": "dee99ff4-5fc5-4aa6-bd73-31300a2e7fcf", "_uuid": "59810a1dc19788121d0555972ceb49751a58cf2b"}}, {"source": "def optGBM(train, test, train_labels):\n    \n    from sklearn.model_selection import GridSearchCV\n    from sklearn.ensemble import GradientBoostingRegressor\n    \n    param_grid = {\n        'max_depth': range(4,7),\n        'min_samples_split' : range(3,7),\n        'min_samples_leaf' : range(2,6)\n    }\n    \n    \n    from sklearn.linear_model import ElasticNetCV\n    \n    l1_list = np.arange(0.15, 1.0, 0.15)\n    \n    reg = ElasticNetCV( l1_ratio = l1_list, cv = 5, normalize = True )\n    \n    gbm = GradientBoostingRegressor()\n    \n    gbm_cv = GridSearchCV( gbm, param_grid, cv = 5, \n                          scoring = 'r2', n_jobs = -1, verbose = 2)\n    \n    gbm_cv.fit(train, train_labels)\n    \n    train_pred = np.zeros( ( train.shape[0], 100) )\n    test_pred = np.zeros( ( test.shape[0], 100) )\n    \n    for i in range( 100 ):\n        est = gbm_cv.best_estimator_.estimators_[i, 0]\n        \n        train_pred[:, i] = est.predict( train )\n        test_pred[:, i] = est.predict( test )\n        \n    reg.fit(train_pred, train_labels)\n    \n    return reg.predict( train_pred ), reg.predict( test_pred )", "outputs": [], "cell_type": "code", "execution_count": null, "metadata": {"_execution_state": "idle", "trusted": false, "collapsed": false, "_cell_guid": "60d74d6f-fa33-4c73-9e02-e160b2bdbf83", "_uuid": "05dfee99ed611af14c7b95c3932b9daeab755527"}}, {"source": "We largely use linear models to create solutions for our stack, with a few methods from support vector machines.", "outputs": [], "cell_type": "markdown", "execution_count": null, "metadata": {"_execution_state": "idle", "collapsed": false, "_cell_guid": "d5085ea7-332c-4743-936d-2d999873e5bf", "_uuid": "10b4244419eab0154104cb3fd5f89e6ae355436e"}}, {"source": "def createStackPred( train, test, train_labels) :\n\n    #Base Learners\n    from sklearn.ensemble import ExtraTreesRegressor, AdaBoostRegressor\n    from sklearn.linear_model import ElasticNetCV, LassoLarsCV, LassoLarsIC, LassoCV\n    from sklearn.svm import LinearSVR, SVR, NuSVR\n\n    \n    l1_list = np.arange(0.15,1.0,0.15)\n\n    print (\"\\n\\tFitting our training model\")\n\n    #Lists of regressors\n\n    line_list = [\n                ExtraTreesRegressor( n_estimators = 100, n_jobs = -1 ),\n                AdaBoostRegressor( loss = 'linear'),\n                AdaBoostRegressor( loss = 'square'),\n                LassoLarsIC( criterion = 'bic', fit_intercept = False ),\n                LassoLarsIC( criterion = 'aic', fit_intercept = False ),\n                LinearSVR( loss = 'epsilon_insensitive', fit_intercept = False),\n                LinearSVR( loss = 'squared_epsilon_insensitive', fit_intercept = False),\n                SVR(),\n                NuSVR()\n                ]\n\n    line_n = len( line_list )\n\n    n = line_n\n\n    #Sets up the arrays to store the predictions\n\n    test_pred = np.zeros( ( test.shape[0], n) )\n    train_pred = np.zeros( ( train.shape[0], n) )\n    \n    for i in range(line_n):\n       \n        print (\"\\n\\t\\tAt regression model: \" + str(i + 1) )\n        est = line_list[i]\n        \n        est.fit( train, train_labels )\n        \n        train_pred[:,i] = est.predict( train )\n        test_pred[:,i] = est.predict( test )\n\n    print (\"\\n\\tFitting... Done\")\n\n    return train_pred, test_pred", "outputs": [], "cell_type": "code", "execution_count": null, "metadata": {"_execution_state": "idle", "trusted": false, "collapsed": false, "_cell_guid": "c0a0827a-c64b-40b1-83d5-c43e96ad1e48", "_uuid": "b9c03ea9f026f4ed6e890c15c29ffdeff35a2f01"}}, {"source": "We finally create our stacker.", "outputs": [], "cell_type": "markdown", "execution_count": null, "metadata": {"_execution_state": "idle", "collapsed": false, "_cell_guid": "3f452d8e-02e7-4fb1-b48b-2d005abe76d7", "_uuid": "35112972ad6d83b9fffa93f6abe46bd50f31e063"}}, {"source": "#This makes a linear relationship between the initial predictions of the train labels with the actual\n#, and projections that onto the testing labels\ndef stacker(train_df, test_df, train_labels ) :\n\n    import matplotlib.pyplot as plt\n\n    print (\"\\nCreating our stacks of predictions\")\n\n    train_pred, test_pred = createStackPred( train_df, test_df, train_labels)\n\n    plt.figure( figsize = (10, 10) )\n    \n    colormap = plt.cm.gist_ncar\n    plt.gca().set_color_cycle([colormap(i) for i in np.linspace(0, 0.9, train_pred.shape[1] + 1)])\n\n    print (\"\\nCreating our averaging systems\")\n\n    for i in range(train_pred.shape[1]):\n        plt.scatter( train_pred[:,i], train_labels, label = 'Regression Model_' + str(i+1) )\n\n    train_pred, test_pred = optGBM(train_pred, test_pred, train_labels)\n\n    plt.scatter( train_pred, train_labels, label = 'Stacked' )\n\n    plt.legend()\n\n    plt.show()\n\n    return np.expm1( test_pred )", "outputs": [], "cell_type": "code", "execution_count": null, "metadata": {"_execution_state": "idle", "trusted": false, "collapsed": false, "_cell_guid": "78af8690-ffd9-4d8d-a484-1005c60bec01", "_uuid": "1fa8bb2534c2d06cddec8ff58f5f376fae83922c"}}, {"source": "df_sub['y'] = stacker( train, test, train_labels )", "outputs": [], "cell_type": "code", "execution_count": null, "metadata": {"_execution_state": "idle", "trusted": false, "collapsed": false, "_cell_guid": "2bf3dd1b-0b2d-4665-82bc-b64c5f82c625", "_uuid": "eed2104bf59e4f5d11e2373f7210882f415ef2f5"}}, {"source": "df_sub.to_csv('m_bendz.csv', index = False)", "outputs": [], "cell_type": "code", "execution_count": null, "metadata": {"_execution_state": "idle", "trusted": false, "collapsed": false, "_cell_guid": "9274c27c-808c-47ed-ad9d-4831af620a58", "_uuid": "4ab98dc5b2dd4478a93ebeab7690ee8717e3a47a"}}, {"outputs": [], "source": "df_sub['y']", "execution_count": null, "cell_type": "code", "metadata": {"collapsed": false, "_execution_state": "idle", "_uuid": "751df772741c22ff7a9253264d855480db493d47"}}, {"source": "This shows the overall usefulness of the stacked predictions, and using a linear model to properly choose the best weights to give to each of the predictions from the stack.  I only used a small handful of models.  I am sure others can whip up more elaborate models for their stack.\n\nIf you think this is useful, or interesting, please upvote.  Thank you.", "outputs": [], "cell_type": "markdown", "execution_count": null, "metadata": {"_execution_state": "idle", "collapsed": false, "_cell_guid": "f1d67653-eac8-4bb4-a5ee-35e5128e69df", "_uuid": "dee18cdb2c27ea51a99beafaa3a5d8a52ff8f6a4"}}], "metadata": {"language_info": {"name": "python", "version": "3.6.1", "mimetype": "text/x-python", "file_extension": ".py", "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "codemirror_mode": {"name": "ipython", "version": 3}}, "kernelspec": {"name": "python3", "language": "python", "display_name": "Python 3"}}, "nbformat": 4, "nbformat_minor": 0}