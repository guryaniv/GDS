{"cells":[{"metadata":{"_uuid":"09329512f3362136c518df077e2bff40afc5ce4d"},"cell_type":"markdown","source":"# An idiots guide to a not horrible score.  Forget exploratory data analysis, forget parameter tuning, just get something that works sort of well.  Let's do it in less than 30 lines of code."},{"metadata":{"_uuid":"eb41ee3ab8ccbff21e4b4466ade0c5256cd9414a"},"cell_type":"markdown","source":"Load some packages and the data.  Create a numpy array called X with the training set features and a numpy array called y with the target values.  Create a numpy array with the test set features.  Let's log (base e) transform the target variable since it is vast in scale."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\ndataset = pd.read_csv('../input/train.csv')\ntestset = pd.read_csv('../input/test.csv')\n\nX = dataset.iloc[:, 2:].values\ny = dataset.iloc[:, 1].values\nX_Test = testset.iloc[:,1:].values\ny = np.log(y)","execution_count":1,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"Let's forget all about EDA.  Other people showed that there are no missing values, everything is numeric, and no wild outliers.  Let's just get rid of any features with no variance.  This will include variables with all the same value."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"25f83f79a04d29a01fa66843994cf8f59452b8b2"},"cell_type":"code","source":"from sklearn.feature_selection import VarianceThreshold\nfeature_selector = VarianceThreshold()\nX = feature_selector.fit_transform(X)\nX_Test = feature_selector.transform(X_Test)","execution_count":2,"outputs":[]},{"metadata":{"_uuid":"e975a5cd6aaf84112e8da643529fedd574baebcb"},"cell_type":"markdown","source":"Now let's get to making a regressor.  A good place to start is XGBoost.  Let's just make most of the paramters at their defaults.  Maybe use 300 estimators.  Instantiate regressor, fit model, bada boom, bada bing."},{"metadata":{"trusted":true,"_uuid":"0f682af00a7c59452157854383b1fcff741331e1","collapsed":true},"cell_type":"code","source":"from xgboost import XGBRegressor\nregressor = XGBRegressor(n_estimators=300)\nregressor.fit(X, y)","execution_count":3,"outputs":[]},{"metadata":{"_uuid":"b404756f1c74c4c8d499f563cab3801079739bbc"},"cell_type":"markdown","source":"Okee dokee.  Now let's make predictions on the test data with our not-so-fancy model.  Don't forget to exponentiate them since the target values were log transformed."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"5967390797fb260d39e065f79249f9c1f25c5368"},"cell_type":"code","source":"results = regressor.predict(X_Test)\nresults = np.exp(results)","execution_count":4,"outputs":[]},{"metadata":{"_uuid":"c76b7d6e01668008df1f81f5d3533c3f67bdead6"},"cell_type":"markdown","source":"Boooooooya!  All thats left to do is write the results to a file and submit it.  Don't ya' just love pandas?"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"5d62bb707aeb0533d88eb516d63b74ebf08e263e"},"cell_type":"code","source":"submission = pd.DataFrame()\nsubmission['ID'] = testset['ID']\nsubmission['target'] = results\nsubmission.to_csv('submission.csv', index=False)","execution_count":8,"outputs":[]},{"metadata":{"_uuid":"b5fb6cb9d1b74322c86e76486d37595c3f6ea0f2"},"cell_type":"markdown","source":"Hey, that's 21 lines of code.  Not so bad."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}