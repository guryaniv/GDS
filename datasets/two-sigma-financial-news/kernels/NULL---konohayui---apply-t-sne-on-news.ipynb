{"cells":[{"metadata":{"_uuid":"ee50a2f3fecf6683f161273e1224e09d1f7a0f96"},"cell_type":"markdown","source":"This is an extention of this [notebook](https://www.kaggle.com/konohayui/two-sigma-market-news-data-eda)\n\nThe analysis in this notebook follows [How to mine newsfeed data and extract interactive insights in Python](https://ahmedbesbes.com/how-to-mine-newsfeed-data-and-extract-interactive-insights-in-python.html)."},{"metadata":{"_uuid":"b137c61bbad7f756e76267c14c5c1c92834754cb"},"cell_type":"markdown","source":"## t-Stochastic Neighbor Embedding (t-SNE)\n\nt-SNE is an non-linear visualizing technique that visualizes high dimentional data by mapping datapoints into 2 or 3-dimention. \n\nAdvantages using t-SNE:\n1. It is able to convert a high dimentional data set into a matrix of pairwise similarities. \n2. It can well capture most of the local structure of high-dimention data while revealing the globle structure."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np, pandas as pd, seaborn as sns, matplotlib.pyplot as plt\nimport warnings, time, gc\nfrom plotly import tools\nimport plotly.plotly as py\nimport plotly.graph_objs as go\nfrom plotly.offline import init_notebook_mode, iplot\n\ninit_notebook_mode(connected = True)\ncolor = sns.color_palette(\"Set2\")\nwarnings.filterwarnings(\"ignore\")\n%matplotlib inline\n\nfrom kaggle.competitions import twosigmanews\n\npd.options.mode.chained_assignment = None\npd.options.display.max_columns = 999\n\nenv = twosigmanews.make_env()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"market_train, news_train = env.get_training_data()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7dd5966ef3038a2588c36d925a05936983293a76"},"cell_type":"code","source":"del market_train; gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4a719a52ddbcbfb2fd372d0ac4d6692be60e942c"},"cell_type":"markdown","source":"### An Overview on News Data"},{"metadata":{"trusted":true,"_uuid":"14e66b2a9f1c2a527c73189899299153d8652d67"},"cell_type":"code","source":"news_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5061b5f37d42f50fc044672d882593cafc368f14"},"cell_type":"code","source":"news_train[\"headline_len\"] = news_train[\"headline\"].apply(lambda x: len(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b3feaff6d34d2652e7c8703913b8e5b5c129bee7"},"cell_type":"code","source":"news_train[\"headline_len\"].hist(figsize = (15, 5), bins = 100)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"128a981ae4a844fabc2836ea4672b7605633d073"},"cell_type":"markdown","source":"* About $30\\%$ headlines' length are over $100$ \n* A little have zero length. This implies that there are empty headlines."},{"metadata":{"trusted":true,"_uuid":"a0f69077e87099dbcba3c8b884ee515a301282c2","scrolled":true},"cell_type":"code","source":"max_len = max(news_train[\"headline_len\"])\ntemp = news_train[news_train[\"headline_len\"] == max_len][\"headline\"]\nfor t in temp:\n    print(t)\n    print()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"031328b84dd87682c8ad2685c5b8a96437eb924a"},"cell_type":"code","source":"temp = news_train[news_train[\"headline_len\"] == 0]\ntemp.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9f3a76c353c36cf0b0a77001ed13f0d7dd513d67"},"cell_type":"code","source":"# Drop rows with empty headlines\nnews_train.drop(news_train[news_train[\"headline_len\"] == 0].index, \n                inplace = True)\nnews_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"76918de96e5634411b33fdcf4cbd7101b962df5b"},"cell_type":"code","source":"from wordcloud import WordCloud\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation\n\nfrom nltk.tokenize import word_tokenize, sent_tokenize, TweetTokenizer \nfrom nltk.stem import WordNetLemmatizer, PorterStemmer\nfrom nltk.corpus import stopwords\nfrom string import punctuation\n\nimport re\nfrom functools import reduce\n\nstop_words = set(stopwords.words(\"english\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9aeca7d2fe2bb19728f98b7cd6e8b578b965437a"},"cell_type":"code","source":"#from:https://github.com/RenatoBMLR/nlpPy/tree/master/src\n\nclass TextDataset():\n\n    def __init__(self, df, lang = 'english'):\n\n        self.data = df\n\n        self.tokenizer = TweetTokenizer()\n        self.stop_words = set(stopwords.words(lang))\n        self.lemmatizer = WordNetLemmatizer()\n        self.ps = PorterStemmer()\n        \n    def _get_tokens(self, words):    \n        return [word.lower() for word in words.split()]\n    \n    def _removeStopwords(self, words):\n        # Removing all the stopwords\n        return [word for word in words if word not in self.stop_words]\n\n    def _removePonctuation(self, words):\n        return re.sub(r'[^\\w\\s]', '', words)\n\n    def _lemmatizing(self, words):\n        #Lemmatizing\n        return [self.lemmatizer.lemmatize(word) for word in words]\n\n    def _stemming(self, words):\n        #Stemming\n        return [self.ps.stem(word) for word in words]\n\n\n    def process_data(self, col = 'content', remove_pontuation=True, remove_stopw = True, lemmalize = False, stem = False):\n\n        self.data = self.data.drop_duplicates(subset=col, keep=\"last\")\n        \n        proc_col = col\n        if remove_pontuation:\n            proc_col = col + '_data'\n            self.data[proc_col] = self.data[col].apply(lambda x: self._removePonctuation(x) )\n        \n        # get tokens of the sentence\n        self.data[proc_col] = self.data[proc_col].apply(lambda x: self._get_tokens(x))\n        if remove_stopw:\n            self.data[proc_col] = self.data[proc_col].apply(lambda x: self._removeStopwords(x)) \n        if lemmalize:\n            self.data[proc_col] = self.data[proc_col].apply(lambda x: self._lemmatizing(x) )\n        if stem:\n            self.data[proc_col] = self.data[proc_col].apply(lambda x: self._stemming(x))\n\n        self.data['nb_words'] = self.data[proc_col].apply(lambda x: len(x))\n        self.proc_col = proc_col\n        \n    def __len__(self):\n        return len(self.data)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"da5f6ea979322dd194987b560e878dbd5ac64718"},"cell_type":"markdown","source":"**Due to the limit of RAM, we will limit the analysis to news of the last month in 2016 (aka, 2016/12/01-2016/12/31)**"},{"metadata":{"trusted":true,"_uuid":"db4184a65933015676b2efccf7575e7c7ee3c0c8"},"cell_type":"code","source":"partial_news = news_train[news_train[\"time\"] >= \"2016-12-01\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9ae91e3df237658a67ec5c88f7d162bcec05e2ee"},"cell_type":"code","source":"text_tokens = TextDataset(partial_news)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"18230ef3b1aa8ce98166e3acce59e7422777cde0"},"cell_type":"code","source":"text_tokens.process_data(col = \"headline\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f8ecf9d5749714b02758962ca6261ab92255484d"},"cell_type":"code","source":"text_tokens.data[\"headline_data\"].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"88cdef412aa8c7980f9f64ea5ab25cef1a78170a"},"cell_type":"code","source":"partial_news[\"tokens\"] = text_tokens.data[\"headline_data\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9f85667603e62af1ed69806c68d254dba5965e2a"},"cell_type":"code","source":"partial_news.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a1f86150bf4a1ba75971279eb5a1767e7fc37ccf"},"cell_type":"markdown","source":"Some tokens are filled with **NA** because of duplicates and the ones been kept are on the last; thus, we fill those **NA** from below."},{"metadata":{"trusted":true,"_uuid":"b2d7ba4b2e7eeea54c2c62fe41394d0f3dae8a2b"},"cell_type":"code","source":"partial_news.fillna(method = \"bfill\", inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5131e49d2433315f82e4651a164819d316cd2097"},"cell_type":"markdown","source":"## Tf-Idf\n\n> tf-idf stands for term frequencey-inverse document frequency. It's a numerical statistic intended to reflect how important a word is to a document or a corpus (i.e a collection of documents)."},{"metadata":{"trusted":true,"_uuid":"cca3e921f1dc65947e1dac404cc79fb222667c75"},"cell_type":"code","source":"tf_idf_vec = TfidfVectorizer(min_df = 3, \n                             max_features = 100000, \n                             analyzer = \"word\",\n                             ngram_range = (1, 2),\n                             stop_words = \"english\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"47f5653c0adb04ae32ecba87a7a7ec369896d607"},"cell_type":"code","source":"tf_idf = tf_idf_vec.fit_transform(list(partial_news[\"tokens\"].map(lambda tokens: \" \".join(tokens))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2e6e9b54b1068cb88bbcda8e8613ec5a8d274f75"},"cell_type":"code","source":"tfidf_df = dict(zip(tf_idf_vec.get_feature_names(), tf_idf_vec.idf_))\ntfidf_df = pd.DataFrame(columns = [\"tfidf\"]).from_dict(dict(tfidf_df), orient = \"index\")\ntfidf_df.columns = [\"tfidf\"]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"76bbafb333b42d4b0506846a72019aeaaef31d27"},"cell_type":"markdown","source":"### Visualize the distribution of the tf-idf scores "},{"metadata":{"trusted":true,"_uuid":"f3c3b231043bf35f0263f4e7b343008ec57d813c"},"cell_type":"code","source":"tfidf_df.tfidf.hist(bins = 25, figsize = (15, 5))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"98c7e46c21378d5d2a1c1eb67bcac284ec2e0faa"},"cell_type":"markdown","source":"Examples of less meaningful words:"},{"metadata":{"trusted":true,"_uuid":"5d30c19bd3d8ba9ceffef6b4e4346db0ec8ad8de"},"cell_type":"code","source":"tfidf_df.sort_values(by = [\"tfidf\"], ascending = True).head(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"67f66d32dec093d1448a16761fd9b39bf5066a02"},"cell_type":"markdown","source":"Examples of more meaningful words:"},{"metadata":{"trusted":true,"_uuid":"0973eb892cdaa1a726e92f86157780963b1ec229"},"cell_type":"code","source":"tfidf_df.sort_values(by=[\"tfidf\"], ascending = False).head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"73ac06f25e0635e768e045c0355a8d5827f198ae"},"cell_type":"code","source":"tf_idf.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"59543739f0cd1a2f052c2b4643c683d3c70895ca"},"cell_type":"markdown","source":"As we see from above, the tf-idf documents contain over 50,000 features. In order to visualize the document, we need to reduce the dimension to 2 or 3. To achive this goal, we do the following steps:\n1. Apply Singular Value Decomposition (SVD) to reduce the dimension to a much lower dimension (for example, 50. Due to the limit of kernel, we do 30 in this case)\n2. Apply t-SNE to reduce the dimention to 2    "},{"metadata":{"trusted":true,"_uuid":"3ea7b2193061de1e04918314e632058a95462cf1"},"cell_type":"code","source":"from sklearn.decomposition import TruncatedSVD\n\nsvd = TruncatedSVD(n_components = 30, random_state = 32)\nsvd_tfidf = svd.fit_transform(tf_idf)\n\nsvd_tfidf.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"26a7c2a98bcfea766d69e4e5320189acd6552338","_kg_hide-output":true},"cell_type":"code","source":"from sklearn.manifold import TSNE\n\ntsne_model = TSNE(n_components = 2, verbose = 1, random_state = 32, n_iter = 500)\ntsne_tfidf = tsne_model.fit_transform(svd_tfidf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"65f7b27b6846db9bca4035fd64725a4a2273829b"},"cell_type":"code","source":"tsne_tfidf.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"609f1198d6cc94df03e356a03274ca9693f508c8"},"cell_type":"markdown","source":"As we can from above, we now have 2 features after applying t-SNE.\nTo visualize t-SNE, we use *Bokeh*"},{"metadata":{"trusted":true,"_uuid":"a592e79a8b6c5098db5c6ec366d485b55a92dfca"},"cell_type":"code","source":"import bokeh.plotting as bp\nfrom bokeh.models import HoverTool, BoxSelectTool\nfrom bokeh.models import ColumnDataSource\nfrom bokeh.plotting import figure, show, output_notebook, reset_output\nfrom bokeh.palettes import d3\nimport bokeh.models as bmo\nfrom bokeh.io import save, output_file","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f2b541fac9e4051f152bc703c9301a8b54b93461"},"cell_type":"code","source":"tsne_tfidf_df = pd.DataFrame(tsne_tfidf)\ntsne_tfidf_df.columns = [\"x\", \"y\"]\ntsne_tfidf_df[\"asset_name\"] = partial_news[\"assetName\"].values\ntsne_tfidf_df[\"headline\"] = partial_news[\"headline\"].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b4ab5027c5d5dad991bc4702b9a16c6a2d77fb8d"},"cell_type":"code","source":"tsne_tfidf_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"faca65847240d18cb8894f5a8a93635e56cf96fc"},"cell_type":"code","source":"output_notebook()\nplot_tfidf = bp.figure(plot_width = 700, plot_height = 600, \n                       title = \"tf-idf clustering of stock market news\",\n                       tools = \"pan, wheel_zoom, box_zoom, reset, hover, previewsave\",\n                       x_axis_type = None, y_axis_type = None, min_border = 1)\n\n# palette = d3[\"Category10\"][len(tsne_tfidf_df[\"asset_name\"].unique())]\n# color_map = bmo.CategoricalColorMapper(factors = tsne_tfidf_df[\"asset_name\"].map(str).unique(), \n#                                        palette = palette)\n\nplot_tfidf.scatter(x = \"x\", y = \"y\", \n#                    color = {\"field\": \"asset_name\", \"transform\": color_map}, \n#                    legend = \"asset_name\",\n                   source = tsne_tfidf_df,\n                   alpha = 0.7)\nhover = plot_tfidf.select(dict(type = HoverTool))\nhover.tooltips = {\"headline\": \"@headline\", \"asset_name\": \"@asset_name\"}\n\nshow(plot_tfidf)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8e41fcc2599d69deb45e720552d8dd0ce85a5c75"},"cell_type":"markdown","source":"## K-Means\nK-means cluster is an another technique to group data by their *similarity*. In this case, we use Euclidean distance to calculate the *similarity*."},{"metadata":{"trusted":true,"_uuid":"ac21210cbc6ed839b13d121245d5290a59200a46"},"cell_type":"code","source":"from sklearn.cluster import MiniBatchKMeans\n\nkmeans_model = MiniBatchKMeans(n_clusters = 50, # don't have time to find the best number\n                               init = \"k-means++\",\n                               n_init =  1,\n                               init_size = 1000, \n                               batch_size = 1000, \n                               verbose = 0, \n                               max_iter = 1000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2d66ba97983896b0e33fd189c55452b02fe8f721"},"cell_type":"code","source":"kmeans = kmeans_model.fit(tf_idf)\nkmeans_clusters = kmeans.predict(tf_idf)\nkmeans_distances = kmeans.transform(tf_idf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d29c310447b22cc983e8d52b86e05d7a8b4abf32","_kg_hide-output":true},"cell_type":"code","source":"tsne_kmeans = tsne_model.fit_transform(kmeans_distances)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4d3f66ebe389a69a1b46b1cea27874bfd3fcfa09"},"cell_type":"code","source":"tsne_kmeans_df = pd.DataFrame(tsne_kmeans)\ntsne_kmeans_df.columns = [\"x\", \"y\"]\ntsne_kmeans_df[\"cluster\"] = kmeans_clusters\ntsne_kmeans_df[\"asset_name\"] = partial_news[\"assetName\"].values\ntsne_kmeans_df[\"headline\"] = partial_news[\"headline\"].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e14e1bc9117178c39ec2c98a3a26dd72fa7e1a12"},"cell_type":"code","source":"colormap = np.array([\"#6d8dca\", \"#69de53\", \"#723bca\", \"#c3e14c\", \"#c84dc9\", \"#68af4e\", \"#6e6cd5\", \"#e3be38\", \n                     \"#4e2d7c\", \"#5fdfa8\", \"#d34690\", \"#3f6d31\", \"#d44427\", \"#7fcdd8\", \"#cb4053\", \"#5e9981\",\n                     \"#803a62\", \"#9b9e39\", \"#c88cca\", \"#e1c37b\", \"#34223b\", \"#bdd8a3\", \"#6e3326\", \"#cfbdce\", \n                     \"#d07d3c\", \"#52697d\", \"#194196\", \"#d27c88\", \"#36422b\", \"#b68f79\", \"#00ffff\", \"#33ff33\",\n                     \"#ffff99\", \"#99ff33\", \"#ff6666\", \"#666600\", \"#99004c\", \"#808080\", \"#a80a0a\", \"#a4924c\",\n                     \"#4a8e92\", \"#92734a\", \"#7d4097\", \"#4b4097\", \"#c0c0c0\", \"#409794\", \"#1a709b\", \"#a7dcf6\",\n                     \"#b1a7f6\", \"#eea7f6\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2c3c755785e89cc11b1f728ec99bfbf0961a746d"},"cell_type":"code","source":"plot_kmeans = bp.figure(plot_width = 700, plot_height = 600, \n                       title = \"k-means clustering of stock market news\",\n                       tools = \"pan, wheel_zoom, box_zoom, reset, hover, previewsave\",\n                       x_axis_type = None, y_axis_type = None, min_border = 1)\n\nsource = ColumnDataSource(data = dict(x = tsne_kmeans_df[\"x\"], y = tsne_kmeans_df[\"y\"],\n                                      color = colormap[kmeans_clusters],\n                                      headline = tsne_kmeans_df[\"headline\"],\n                                      asset_name = tsne_kmeans_df[\"asset_name\"],\n                                      cluster = tsne_kmeans_df[\"cluster\"]))\n\nplot_kmeans.scatter(x = \"x\", y = \"y\", color = \"color\", source = source)\nhover = plot_kmeans.select(dict(type = HoverTool))\nhover.tooltips = {\"headline\": \"@headline\", \"asset_name\": \"@asset_name\", \"cluster\": \"@cluster\"}\nshow(plot_kmeans)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1027e91b44dac232203610820ccbe7b4b4c107cd"},"cell_type":"markdown","source":"### Latent Dirichlet Allocation (LDA)\n>  LDA, a generative probabilistic model for collections of discrete data such as text corpra. LDA is a three-level hierachical Baysian model, in which each item of a collection is modeled as a finite mixture over an underlying set of topics. Each topic is, in turn, modeled as an inifinte mixture over an underlying set of topic probabilities. \n\nUnlike k-means clustering, LDA is capable to illustrate documents with multible topics"},{"metadata":{"trusted":true,"_uuid":"c1a6a1f6d50da06bfae050b4aec295d16e81cc4c"},"cell_type":"code","source":"cv = CountVectorizer(min_df = 2,\n                     max_features = 100000,\n                     analyzer = \"word\",\n                     ngram_range = (1, 2),\n                     stop_words = \"english\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9a79d0f0c78d7e42285b4e78167cc0dc8db61086"},"cell_type":"code","source":"count_vectors = cv.fit_transform(partial_news[\"headline\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"96dd05c56ab36959fba64ff2f806f7a14ba75a79"},"cell_type":"code","source":"lda_model = LatentDirichletAllocation(n_components = 20, \n                                      # we choose a small n_components for time convenient\n                                      learning_method = \"online\",\n                                      max_iter = 20,\n                                      random_state = 32)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b411f8e5a851b4065749ca49e16f353c53cd855a"},"cell_type":"code","source":"news_topics = lda_model.fit_transform(count_vectors)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f21199c78b950778f185d527401964c1a7d22781"},"cell_type":"code","source":"n_top_words = 10\ntopic_summaries = []\ntopic_word = lda_model.components_\nvocab = cv.get_feature_names()\n\nfor i, topic_dist in enumerate(topic_word):\n    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n    topic_summaries.append(\" \".join(topic_words))\n    print(\"Topic {}: {}\".format(i, \" | \".join(topic_words)))","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true,"_uuid":"023523e849f3b895fc14b8e8137a9c70347e55e7"},"cell_type":"code","source":"tsne_lda = tsne_model.fit_transform(news_topics)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c5a69dce789620fd35004731699859857c5b21b0"},"cell_type":"code","source":"news_topics = np.matrix(news_topics)\ndoc_topics = news_topics/news_topics.sum(axis = 1)\n\nlda_keys = []\nfor i, tweet in enumerate(partial_news[\"headline\"]):\n    lda_keys += [doc_topics[i].argmax()]\n    \ntsne_lda_df = pd.DataFrame(tsne_lda, columns = [\"x\", \"y\"])\ntsne_lda_df[\"headline\"] = partial_news[\"headline\"].values\ntsne_lda_df[\"asset_name\"] = partial_news[\"assetName\"].values\ntsne_lda_df[\"topics\"] = lda_keys\ntsne_lda_df[\"topics\"] = tsne_lda_df[\"topics\"].map(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6c952ca6051db16452310dc839cd480616f8a058"},"cell_type":"code","source":"plot_lda = bp.figure(plot_width = 700, plot_height = 600, \n                    title = \"LDA topics of stock market news\",\n                    tools = \"pan, wheel_zoom, box_zoom, reset, hover, previewsave\",\n                    x_axis_type = None, y_axis_type = None, min_border = 1)\n\nsource = ColumnDataSource(data = dict(x = tsne_lda_df[\"x\"], y = tsne_lda_df[\"y\"],\n                         color = colormap[lda_keys],\n                         headline = tsne_lda_df[\"headline\"],\n                         asset_name = tsne_lda_df[\"asset_name\"],\n                         topics = tsne_lda_df[\"topics\"]))\n\nplot_lda.scatter(x = \"x\", y = \"y\", color = \"color\", source = source)\nhover = plot_lda.select(dict(type = HoverTool))\nhover.tooltips = {\"headline\": \"@headline\", \"asset_name\": \"@asset_name\", \"topics\": \"@topics\"}\nshow(plot_lda)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4534060871b526059fbf2daeaaa64ffadeced807"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c6e7f9e8f2c12741769842ba1c2d6aee4648baa9"},"cell_type":"markdown","source":"### To Be Continued..."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}