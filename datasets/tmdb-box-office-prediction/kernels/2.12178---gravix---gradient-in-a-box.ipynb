{"cells":[{"metadata":{"trusted":false,"_uuid":"b799e41169df2336b36da7dca90a2cdc4a14d58a"},"cell_type":"code","source":"import pandas as pd\npd.set_option('display.max_columns', 100)\npd.set_option('display.max_rows', 100)\n\nimport numpy as np\n\nimport os\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport ast\n\nimport re\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\nimport xgboost as xgb\n\nfrom collections import Counter\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"d29ea452b1dc18e9a85d7edae515678cc450d5af"},"cell_type":"code","source":"print(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"5ec21b1eb08e960d4bb780c3ef932084d057c70b"},"cell_type":"code","source":"train = pd.read_csv(\"../input/train.csv\")\ntest = pd.read_csv(\"../input/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"322d5ace26656c47d98d02259bb49d38da2e84fa"},"cell_type":"code","source":"train.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false,"_uuid":"2dbfdbfb13cb2cf870ba062b8058a5ff121026b4"},"cell_type":"code","source":"test.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0c84945a76ba8c7cbbc084afb7d8f7521f232f67"},"cell_type":"markdown","source":"Transforming dictionary columns to proper format :"},{"metadata":{"trusted":false,"_uuid":"78580f37f2e7fdde9eebdc5e82901118b6f776d4"},"cell_type":"code","source":"dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n\ndef text_to_dict(df):\n    for column in dict_columns:\n        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n    return df\n        \ntrain = text_to_dict(train)\ntest = text_to_dict(test)\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"322dcf783f984f5813308caad6e2b31237bc8c19"},"cell_type":"markdown","source":"Extracting categories from selected dictionary columnns : "},{"metadata":{"trusted":false,"_uuid":"3c5a7a63b94d60197133e70a276703351ee1d218"},"cell_type":"code","source":"def build_category_list(x, field, feature):\n    regex = re.compile('[^0-9a-zA-Z_]')\n    category_list = \"\"\n    \n    for d in x:\n        new_category = regex.sub('', d[field].lower().replace(\" \",\"_\"))\n        category_list += \" \" + new_category\n    return category_list.strip()\n\n\ntarget_fields = {'belongs_to_collection': 'name', 'genres': 'name',\n                 'production_countries': 'iso_3166_1', 'production_companies': 'name',\n                 'spoken_languages': 'iso_639_1', 'Keywords': 'name', 'cast':'name',\n                 'crew':'name'\n                }\n\ntrain['crew_copy'] = train['crew']\ntest['crew_copy'] = test['crew']\n\ntrain['cast_copy'] = train['cast']\ntest['cast_copy'] = test['cast']\n\n\nfor k,v in target_fields.items():\n    print(k)\n    train[k] = train[k].apply(lambda x: build_category_list(x, v, k))\n    test[k] = test[k].apply(lambda x: build_category_list(x, v, k)) \n    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"80322928c71ab0840a7e31b3da3c6577f36fe3c5"},"cell_type":"code","source":"thresholds = {'belongs_to_collection': 0, 'genres': 0,\n                 'production_countries': 10, 'production_companies': 10,\n                 'spoken_languages': 10, 'Keywords': 10, 'cast': 10, 'crew': 10\n                }\n\ndef streamline(x, kept):\n    streamlined = \"\"\n    for w in x.split(\" \"):\n        if w in kept:\n            streamlined = streamlined + \" \" + w\n    return streamlined.strip()\n\nfor k,v in thresholds.items():\n    print(k)\n    c = Counter(\" \".join(train[k]).split(\" \"))\n    print(\"Initial:\", len(c))\n    kept = [w for w,nb in c.items() if nb > v]\n    print(\"Kept:\", len(kept))\n    print(\"\")\n    train[k] = train[k].apply(lambda x: streamline(x, kept))\n    test[k] = test[k].apply(lambda x: streamline(x, kept))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1cb3d5389db7c88cad5b1b94b50ac04e7d6f71aa"},"cell_type":"markdown","source":"For cast and crew we select only key roles :"},{"metadata":{"trusted":false,"_uuid":"6a2eec3a71684da97e159a95d8ea8aa54cccaf14"},"cell_type":"code","source":"def build_category_list_with_roles(x, v, rv):\n    regex = re.compile('[^0-9a-zA-Z_]')\n    category_list = \"\"\n    for d in x:\n        if d[v['role_field']] != rv:\n            pass\n        else:\n            if category_list == \"\":\n                new_category = regex.sub('', d[v['field']].lower().replace(\" \",\"_\"))\n                category_list += \" \" + new_category\n    return category_list.strip()  \n    \ntarget_fields = {'cast_copy':{'field':'name', 'role_field':'order', 'role_values':[0,1,2,3,4,5]}, \n                 'crew_copy':{'field': 'name', 'role_field': 'job',\n                         'role_values':['Director', 'Producer',\n                                        'Executive Producer', 'Writer', 'First Assistant Director',\n                                        'Associate Producer', 'Director of Photography'\n                                       ]\n                        }\n                }\n\n\nadditional_label_encoding_columns = []\n\nfor k,v in target_fields.items():\n    print(k)\n    for rv in v['role_values']:\n        striped_rv = str(rv).lower().replace(' ','_')\n        additional_label_encoding_columns.append(k + '_' + striped_rv)\n        train[k + '_' + striped_rv] = train[k].apply(lambda x: build_category_list_with_roles(x, v, rv))\n        test[k + '_' + striped_rv] = test[k].apply(lambda x: build_category_list_with_roles(x, v, rv))\n    \n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"110b4d9a7cbc21964b7305c2cec68c477d2fa0dd"},"cell_type":"markdown","source":"Filling nan values :"},{"metadata":{"trusted":false,"_uuid":"52ff4fe23979408c185dcb0becaefd22fad15dd6"},"cell_type":"code","source":"fillna_columns = {'release_date':'mode',\n                  'status':'mode',\n                  'belongs_to_collection': 'none',\n                  'runtime': 'mode'}\n\nfor k,v in fillna_columns.items():\n    if v == 'mode':\n        fill = train[k].mode()[0]\n    else:\n        fill = v\n    print(k, ': ', fill)\n    train[k] = train[k].fillna(value = fill)\n    test[k] = test[k].fillna(value = fill)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fb17bdc263a88aed4a775dc154f847bae2402573"},"cell_type":"markdown","source":"Adding a few features :"},{"metadata":{"trusted":false,"_uuid":"d6eab3698baab075b5220898378d55b4e5955061"},"cell_type":"code","source":"def extract_nb_within_collection(r):\n    regex = re.compile('[^0-9a-zA-Z_]')\n    original_title = regex.sub('', r['original_title'].lower().replace(\" \",\"_\"))\n    \n    if r['is_part_of_collection'] == 0:\n        return 0\n    else:\n        if (r['belongs_to_collection'] == original_title + '_collection') or (r['belongs_to_collection'] == original_title):\n            return 1\n        else:\n            regex = re.compile('[^0-9]')\n            probable_number = regex.sub('', r['original_title'])\n            if probable_number == '' or int(probable_number) > 5:\n                return 0\n            else:\n                return probable_number\n\ndef feature_addition(df):\n    \n    df['release_year'] = df.release_date.apply(lambda x: x[-2:]).astype('int')\n    df['release_month'] = df.release_date.apply(lambda x: x.split('/')[0]).astype('int')\n    df['release_quarter'] = df.release_month % 4 + 1\n    \n    df['budget'] = df.budget / 1000000\n    \n    df['nb_spoken_languages'] = df.spoken_languages.apply(lambda r: len(r.split(' ')))\n    df['nb_words_overview'] = df.overview.apply(lambda x: len(str(x).split(' ')) )\n    df['nb_production_companies'] = df.production_companies.apply(lambda x: len(x.split(' ')) )\n    df['nb_production_countries'] = df.production_countries.apply(lambda x: len(x.split(' ')) )\n    df['nb_cast'] = df.cast.apply(lambda x: len(x.split(' ')) )\n    df['nb_crew'] = df.crew.apply(lambda x: len(x.split(' ')) )\n    df['nb_keywords'] = df.Keywords.apply(lambda x: len(x.split(' ')) )\n    df['nb_words_title'] = df.title.apply(lambda x: len(str(x).split(' ')) )\n    df['nb_words_tagline'] = df.tagline.apply(lambda x: len(str(x).split(' ')) )\n    \n    df['nb_words_original_title'] = df.original_title.apply(lambda x: len(x.split(' ')) )\n    \n    df['has_original_title'] = (df.title == df.original_title).astype('int')\n\n    df['has_homepage'] = 1 - df.homepage.isna().astype('int')\n    df['homepage_base'] = df.homepage.apply(lambda x: str(x).split('//')[-1].split('/')[0].split('www.')[-1].split('.')[0])\n    df['homepage_extension'] = df.homepage.apply(lambda x: str(x).split('//')[-1].split('/')[0].split('www.')[-1].split('.')[-1]).fillna(value = '')\n\n    df['is_part_of_collection'] = 1 - (df.belongs_to_collection == '').astype('int')\n    df['nb_within_collection'] =  df.apply(lambda r: extract_nb_within_collection(r), axis = 1).astype('int')\n    \n    return df\n                                                \ntrain = feature_addition(train)\ntest = feature_addition(test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ab43b36549f8c5558134a127b0d84ebd44b0b56d"},"cell_type":"markdown","source":"Label encoding selected features :"},{"metadata":{"trusted":false,"_uuid":"d45dfbbc46a86066eb8e9b2aaaa888f6b3d8d225"},"cell_type":"code","source":"columns_to_categorize = ['belongs_to_collection', 'status', 'original_language', 'homepage_base', 'homepage_extension']\ncolumns_to_categorize += additional_label_encoding_columns\n\nfor c in columns_to_categorize:\n    print(c)\n    le = LabelEncoder()\n    le.fit_transform(train[c])\n    test[c] = test[c].map(lambda s: 'unknown' if s not in le.classes_ else s)\n    le.classes_ = np.append(le.classes_, 'unknown')\n    train[c] = le.transform(train[c])\n    test[c] = le.transform(test[c])\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9bdf377edbf2263cb9c71a49cbb36ab505eb95cd"},"cell_type":"markdown","source":"Removing unused columns : "},{"metadata":{"trusted":false,"_uuid":"05bb80cdb51cde3a4819a9634628419a7385ecd6"},"cell_type":"code","source":"submission = pd.DataFrame(test['id'])\n\nremoved_columns = ['id', 'homepage', 'imdb_id', 'original_title', 'spoken_languages',\n                   'overview', 'poster_path', 'tagline', 'title',\n                  'release_date', 'crew_copy', 'cast_copy']\n\n\ntrain.drop(removed_columns, axis = 1, inplace = True)\ntest.drop(removed_columns, axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"180be87b8bdb2d5796c293838ab4772f76dd5fbf"},"cell_type":"markdown","source":"Vectorizing selected columns : "},{"metadata":{"trusted":false,"_uuid":"d4c70aa996d0c70e772c1667049b4bf0e2d4745f"},"cell_type":"code","source":"features_to_vectorize = ['genres', 'production_countries', 'production_companies', 'Keywords', 'cast', 'crew']\n\n\nfor f in features_to_vectorize:\n    print(f)\n    vectorizer = TfidfVectorizer(use_idf = False)\n    vectorized_features = vectorizer.fit_transform(train[f])\n    vectorized_features_names = [f + '_' + v for v in vectorizer.get_feature_names()]\n\n    vectorized_features_sparse = pd.SparseDataFrame([ pd.SparseSeries(vectorized_features[i].toarray().ravel()) \n                              for i in np.arange(vectorized_features.shape[0]) ], columns = vectorized_features_names)\n\n    train = pd.concat([train, vectorized_features_sparse], axis = 1)\n    \n    vectorized_features = vectorizer.transform(test[f])\n    vectorized_features_sparse = pd.SparseDataFrame([ pd.SparseSeries(vectorized_features[i].toarray().ravel()) \n                              for i in np.arange(vectorized_features.shape[0]) ], columns = vectorized_features_names)\n\n    test = pd.concat([test, vectorized_features_sparse], axis = 1)\n    \n    train.drop(f, inplace = True, axis = 1)\n    test.drop(f, inplace = True, axis = 1)\n    ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"636a46b683f31f4cc51e9c5e15a4190809b14b7c"},"cell_type":"markdown","source":"Transforming revenue to log for log rmse :"},{"metadata":{"trusted":false,"_uuid":"701df37a74488d266fdc2cafbea03fbf4950ddde"},"cell_type":"code","source":"train['revenue'] = np.log1p(train['revenue'] )","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c9e64030cfa1de0c19ae235c6b890c2eb1462fc1"},"cell_type":"markdown","source":"Train test split : "},{"metadata":{"trusted":false,"_uuid":"ab023183cb0e08f00b38ec73df57f08ddae25cae"},"cell_type":"code","source":"target_column = 'revenue'\n\ntrain_set, validate_set = train_test_split(train, test_size = 0.2, random_state = 1)\n\nx_train = train_set.drop([target_column], axis = 1).copy()\ny_train = train_set[target_column].copy()\n\nx_validate = validate_set.drop([target_column], axis = 1).copy()\ny_validate = validate_set[target_column].copy()\n\nx_total = train.drop([target_column], axis = 1).copy()\ny_total = train[target_column].copy()\n\nx_test = test.copy()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"89f0eddc4c9904f26410cd9f30f6460ed77b9efe"},"cell_type":"markdown","source":"LGBM with preliminary params optimization (hyperopt) :"},{"metadata":{"trusted":false,"_uuid":"f68cf601cf7d4699a7be6ddd3e1328a1b6007408"},"cell_type":"code","source":"import lightgbm as lgb\n\nparams_lgb = {'drop_rate': [0.09777484320779173], 'feature_fraction': [0.6087324102659581],\n              'lambda_l1': [0.03915143495854047], 'lambda_l2': [26.68081917087524],\n              'learning_rate': [0.013231541159028165],\n              'max_drop': [67.0], 'min_data_in_leaf': [1.0],\n              'num_leaves': [32.0], 'num_trees': [1370.0]}\n\nparams_lgb = {k:v[0] for k,v in params_lgb.items()}\n\n\nlg = lgb.LGBMRegressor(\n                        objective = 'regression',\n                        metric = 'rmse',\n                        early_stopping_round = 50,\n                        drop_rate = params_lgb['drop_rate'],\n                        feature_fraction = params_lgb['feature_fraction'],\n                        lambda_l1 = params_lgb['lambda_l1'],\n                        lambda_l2 = params_lgb['lambda_l2'],\n                        learning_rate = params_lgb['learning_rate'],\n                        max_drop = int(params_lgb['max_drop']),\n                        min_data_in_leaf = int(params_lgb['min_data_in_leaf']),\n                        num_leaves = int(params_lgb['num_leaves']),\n                        num_trees = int(params_lgb['num_trees']))\n\nlg.fit(x_train, y_train.values, eval_set=[(x_train, y_train), (x_validate, y_validate)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"cefb06b4c71603f153bfa363896ce5471db7a25a"},"cell_type":"code","source":"feature_importance = pd.DataFrame(lg.feature_importances_, columns = ['importance'])\nfeature_importance['feature'] = train.columns[:-1]\nfeature_importance.sort_values(by='importance', inplace = True, ascending = False)\nfeature_importance.reset_index(drop = True, inplace = True)\nfeature_importance","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"df18dfa732380fd1e9ab2cac6a0ed85977c64414"},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(10, 15))\nsns.barplot(y = 'feature', x = 'importance', data = feature_importance[0:20])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"81147cbc45b6cf878708bbb189b313c1fc10e428"},"cell_type":"code","source":"lg = lgb.LGBMRegressor(\n                        objective = 'regression',\n                        metric = 'rmse',\n                        early_stopping_round = 50,\n                        drop_rate = params_lgb['drop_rate'],\n                        feature_fraction = params_lgb['feature_fraction'],\n                        lambda_l1 = params_lgb['lambda_l1'],\n                        lambda_l2 = params_lgb['lambda_l2'],\n                        learning_rate = params_lgb['learning_rate'],\n                        max_drop = int(params_lgb['max_drop']),\n                        min_data_in_leaf = int(params_lgb['min_data_in_leaf']),\n                        num_leaves = int(params_lgb['num_leaves']),\n                        num_trees = 592)\n\nlg.fit(x_total, y_total.values, eval_set=[(x_train, y_train), (x_validate, y_validate)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"4dee8033ce36b566570efbbe7bdf5fbef09ce59c"},"cell_type":"code","source":"y_test_p = pd.Series(lg.predict(x_test))\nsubmission['revenue'] = np.expm1(y_test_p)\nsubmission.to_csv(\"submission.csv\", index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"6115089381d6f3124dca3dfaeef759791a74243f"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.3"}},"nbformat":4,"nbformat_minor":1}