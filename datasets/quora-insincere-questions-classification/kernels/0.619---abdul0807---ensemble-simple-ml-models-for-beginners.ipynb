{"cells":[{"metadata":{"trusted":true,"_uuid":"a5883b382e1ca80d72aab0d2f4bb321f30aae694"},"cell_type":"markdown","source":"Hi All,\n\nI have come across a lot of kernels solving the problem using various deep learning techniques.<BR>\n\nThis kernel will try to solve the problem using some useful Machine learning techniques. I hope this kernel will help newbies get an initial start to this problem.\n\n**If you are an expert in NLP please skip this notebook and save your valuable time. :)**"},{"metadata":{"_uuid":"62e3eec94f5350b475d28c021310c43366ad223a"},"cell_type":"markdown","source":"# Importing necessary packges"},{"metadata":{"ExecuteTime":{"end_time":"2019-01-15T20:46:47.174365Z","start_time":"2019-01-15T20:46:24.582987Z"},"trusted":true,"_uuid":"ea5bcc6de2cfd593b4fa38d71a2a031d206f9fb1"},"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom nltk.tokenize import WordPunctTokenizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom scipy.stats import mode\nfrom sklearn.naive_bayes import BernoulliNB, MultinomialNB\nfrom sklearn.metrics import f1_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport pandas as pd\nimport numpy as np\nfrom time import time\nfrom tqdm import tqdm\n\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f1a9639cd4631872fa44e28d0ab729fb35f816f7"},"cell_type":"markdown","source":"# Reading Data"},{"metadata":{"ExecuteTime":{"end_time":"2019-01-15T20:49:38.687088Z","start_time":"2019-01-15T20:49:38.471054Z"},"trusted":true,"_uuid":"267dfd4ed8305a5f34af89e33a8fbb8d50fd421d"},"cell_type":"code","source":"train = pd.read_csv(\"../input/train.csv\", index_col=None)\ntest = pd.read_csv(\"../input/test.csv\", index_col=None)\n\ntrain.shape, test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f8f78c44935506328787dca7873739c56c0d4d30"},"cell_type":"markdown","source":"# Performing Cross validation using StratifiedKFold\n    * The stratified KFold ensures that the class balance is maintained. \n    * Since the data is biased we would want to ensure it is considered during train and validation split."},{"metadata":{"ExecuteTime":{"end_time":"2019-01-15T21:18:05.648270Z","start_time":"2019-01-15T21:18:05.506303Z"},"trusted":true,"_uuid":"c6562b1032f4bf7a837af960f08bf005b8ba9b7f"},"cell_type":"code","source":"X, y = train.drop('target', axis=1), train['target']","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2019-01-15T21:27:48.388964Z","start_time":"2019-01-15T21:27:48.230872Z"},"trusted":true,"_uuid":"d194f104f6bbb87c9931c025061d7cca68fda56c"},"cell_type":"code","source":"splits = list(StratifiedKFold(n_splits=5, shuffle=True, random_state=42).split(X, y))","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2019-01-15T21:29:42.770907Z","start_time":"2019-01-15T21:27:50.383908Z"},"trusted":true,"_uuid":"07de39f7f2f4b78fba9523137a6db36f6e837d52"},"cell_type":"code","source":"valid_score = []\ntest_preds = []\ntt = WordPunctTokenizer()\n\nfor i, (train_idx, valid_idx) in tqdm(enumerate(splits)):\n    start_time = time()\n    #print(f'Fold {i+1} started')\n    X_train = X.iloc[train_idx]\n    y_train = y.iloc[train_idx]\n    X_valid = X.iloc[valid_idx]\n    y_valid = y.iloc[valid_idx]\n        \n    tf_idf = TfidfVectorizer(tokenizer=tt.tokenize, stop_words='english', ngram_range=(1, 3))\n    train_tf = tf_idf.fit_transform(X_train.question_text)\n    valid_tf = tf_idf.transform(X_valid.question_text)\n    test_tf = tf_idf.transform(test.question_text)\n    #print(\"Completed tf-idf transformation\")\n\n    # Model 1\n    lr_tf1 = LogisticRegression(C=100)\n    lr_tf1.fit(train_tf, y_train)\n    #print(\"Completed Logistic Model 1 training\")\n    \n    # Model 2\n    lr_tf2 = LogisticRegression(C=100, solver='saga', max_iter=500, tol=0.001)\n    lr_tf2.fit(train_tf, y_train)\n    #print(\"Completed Logistic Model 2 training\")\n\n    # Model 3\n    nb_tf1 = BernoulliNB(alpha=0.011)\n    nb_tf1.fit(train_tf, y_train)\n    #print(\"Completed Naive Bayes Model training\")\n\n    pred1 = lr_tf1.predict(valid_tf)\n    pred2 = lr_tf2.predict(valid_tf)\n    pred3 = nb_tf1.predict(valid_tf)\n    valid_pred = mode([pred1, pred2, pred3])[0][0]\n    \n    validation_f1_score = f1_score(y_valid, valid_pred)\n    valid_score.append(validation_f1_score)\n    \n    pred1 = lr_tf1.predict(test_tf)\n    pred2 = lr_tf2.predict(test_tf)\n    pred3 = nb_tf1.predict(test_tf)\n    test_pred = mode([pred1, pred2, pred3])[0][0]\n\n    test_preds.append(list(test_pred))\n    elapsed_time = time() - start_time\n    \n    print('Fold {} \\t val_score={:.4f} \\t time={:.2f}s'.format(\n            i + 1, validation_f1_score, elapsed_time))\nprint(\"Cross validation score is \", np.mean(valid_score))","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2019-01-15T21:29:55.846082Z","start_time":"2019-01-15T21:29:55.692063Z"},"trusted":true,"_uuid":"9a483b1d672a7232deb86ded8f6a35bfe5835460"},"cell_type":"code","source":"final_prediction = mode(test_preds)[0][0]","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2019-01-15T21:29:57.391075Z","start_time":"2019-01-15T21:29:57.198095Z"},"trusted":true,"_uuid":"65a66aa70ff312868ea2f1b5abd3d3d4b4306ab6"},"cell_type":"code","source":"submission = pd.DataFrame()\nsubmission['qid'] = test['qid']\nsubmission['prediction'] = final_prediction\nsubmission.to_csv(\"submission.csv\", index=None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"10462ae32aebf9d086682d63d98e107a056b0a9b"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{"height":"calc(100% - 180px)","left":"10px","top":"150px","width":"278.818px"},"toc_section_display":true,"toc_window_display":true},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false}},"nbformat":4,"nbformat_minor":1}