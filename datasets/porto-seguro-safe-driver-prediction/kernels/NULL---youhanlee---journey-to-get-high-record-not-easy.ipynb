{"nbformat": 4, "metadata": {"language_info": {"nbconvert_exporter": "python", "codemirror_mode": {"version": 3, "name": "ipython"}, "version": "3.6.1", "pygments_lexer": "ipython3", "mimetype": "text/x-python", "file_extension": ".py", "name": "python"}, "kernelspec": {"display_name": "Python 3", "name": "python3", "language": "python"}}, "nbformat_minor": 1, "cells": [{"metadata": {}, "source": ["# Introduction\n", "This kernel is ref from my kernel. [EDA+StratifiedShuffleSplit+xgboost for starter](https://www.kaggle.com/youhanlee/eda-stratifiedshufflesplit-xgboost-for-starter)\n", "\n", "I'll update this kernel soon!"], "cell_type": "markdown"}, {"metadata": {"_cell_guid": "214470b0-a2a7-439e-bdd8-b39f5d6cf1a6", "_uuid": "c0be39b81eace54a3dd2cb84dfbd4ee84d268c0e", "collapsed": true}, "outputs": [], "execution_count": null, "source": ["import numpy as np\n", "import pandas as pd\n", "import matplotlib.pyplot as plt\n", "import matplotlib\n", "%matplotlib inline\n", "\n", "import seaborn as sns # visualization\n", "import missingno as msno\n", "\n", "from sklearn.model_selection import train_test_split\n", "# from sklearn.cluster import MiniBatchKMeans\n", "from sklearn.model_selection import StratifiedShuffleSplit\n", "\n", "import xgboost as xgb # Gradient Boosting\n", "from xgboost import XGBClassifier\n", "import warnings\n", "\n", "warnings.filterwarnings(\"ignore\")\n", "# Any results you write to the current directory are saved as output"], "cell_type": "code"}, {"metadata": {}, "outputs": [], "execution_count": null, "source": ["np.random.seed(1989)\n", "train = pd.read_csv(\"../input/train.csv\")\n", "test = pd.read_csv(\"../input/test.csv\")\n", "print(\"Train shape : \", train.shape)\n", "print(\"Test shape : \", test.shape )"], "cell_type": "code"}, {"metadata": {}, "outputs": [], "execution_count": null, "source": ["train.head()"], "cell_type": "code"}, {"metadata": {"scrolled": true}, "outputs": [], "execution_count": null, "source": ["train.info()"], "cell_type": "code"}, {"metadata": {"scrolled": true}, "outputs": [], "execution_count": null, "source": ["test.info()"], "cell_type": "code"}, {"metadata": {}, "outputs": [], "execution_count": null, "source": ["print(\"Train has these types: {}\".format(train.dtypes.unique()))\n", "print(\"Test has these types: {}\".format(test.dtypes.unique()))"], "cell_type": "code"}, {"metadata": {}, "outputs": [], "execution_count": null, "source": ["targets = train['target'].values\n", "# sns.set(style=\"darkgrid\")\n", "ax = sns.countplot(x = targets)\n", "for p in ax.patches:\n", "    ax.annotate('{:.2f}%'.format(100*p.get_height()/len(targets)), (p.get_x()+ 0.3, p.get_height()+10000))\n", "plt.title('Distribution of Target', fontsize=20)\n", "plt.xlabel('Claim', fontsize=20)\n", "plt.ylabel('Frequency [%]', fontsize=20)\n", "ax.set_ylim(top=700000)"], "cell_type": "code"}, {"metadata": {}, "outputs": [], "execution_count": null, "source": ["print('Id is unique.') if train.id.nunique() == train.shape[0] else print('Oh no')\n", "print('Train and test sets are distinct.') if len(np.intersect1d(train.id.values, test.id.values)) == 0 else print('Oh no')\n", "print('We do not need to worry about missing values.') if train.count().min() == train.shape[0] else print('Oh no')"], "cell_type": "code"}, {"metadata": {}, "outputs": [], "execution_count": null, "source": ["train_nan = train\n", "train_nan = train_nan.replace(-1, np.NaN)\n", "\n", "msno.matrix(df=train_nan.iloc[:, :], figsize=(20, 14), color=(0.8, 0.5, 0.2))   "], "cell_type": "code"}, {"metadata": {}, "outputs": [], "execution_count": null, "source": ["test_nan = test\n", "test_nan = test_nan.replace(-1, np.NaN)\n", "\n", "msno.matrix(df=test_nan.iloc[:, :], figsize=(20, 14), color=(0.8, 0.5, 0.2))   "], "cell_type": "code"}, {"metadata": {"collapsed": true}, "outputs": [], "execution_count": null, "source": ["# Extract columns with null data\n", "train_nan = train_nan.loc[:, train_nan.isnull().any()]\n", "train_nan_columns = train_nan.columns\n", "\n", "test_nan = test_nan.loc[:, test_nan.isnull().any()]\n", "test_nan_columns = test_nan.columns"], "cell_type": "code"}, {"metadata": {}, "outputs": [], "execution_count": null, "source": ["print('Columns \\t Number of NaN')\n", "for column in train_nan.columns:\n", "    print('{}:\\t {}'.format(column,len(train_nan[column][np.isnan(train_nan[column])])))"], "cell_type": "code"}, {"metadata": {}, "outputs": [], "execution_count": null, "source": ["print('Columns \\t Number of NaN')\n", "for column in test_nan.columns:\n", "    print('{}:\\t {}'.format(column,len(test_nan[column][np.isnan(test_nan[column])])))"], "cell_type": "code"}, {"metadata": {}, "outputs": [], "execution_count": null, "source": ["feature_list = list(train.columns)\n", "def groupFeatures(features):\n", "    features_bin = []\n", "    features_cat = []\n", "    features_etc = []\n", "    for feature in features:\n", "        if 'bin' in feature and 'calc' not in feature:\n", "            features_bin.append(feature)\n", "        elif 'cat' in feature:\n", "            features_cat.append(feature)\n", "        elif 'id' in feature or 'target' in feature:\n", "            continue\n", "        else:\n", "            features_etc.append(feature)\n", "    return features_bin, features_cat, features_etc\n", "\n", "feature_list_bin, feature_list_cat, feature_list_etc = groupFeatures(feature_list)\n", "print(\"# of binary feature : \", len(feature_list_bin))\n", "print(\"# of categorical feature : \", len(feature_list_cat))\n", "print(\"# of other feature : \", len(feature_list_etc))"], "cell_type": "code"}, {"metadata": {}, "outputs": [], "execution_count": null, "source": ["colormap = \"jet\"\n", "plt.figure(figsize=(16, 12))\n", "plt.title('Pearson correlation of continuous features', y=1.05, size=15)\n", "data = train.drop(['id'], axis=1)\n", "sns.heatmap(data.corr(),linewidths=0.1,vmax=1.0, vmin=-1.0, square=True, cmap=colormap, linecolor='white')"], "cell_type": "code"}, {"metadata": {"collapsed": true}, "outputs": [], "execution_count": null, "source": ["feature_list_calc = []\n", "feature_list_without_calc = []\n", "for feature in train.columns:\n", "    if 'calc' in feature or 'target' in feature:\n", "        feature_list_calc.append(feature)\n", "    else:\n", "        feature_list_without_calc.append(feature)\n", "        \n", "train_without_calc = train.drop(feature_list_calc, axis=1).drop(['id'], axis=1)\n", "train_with_calc = train.drop(feature_list_without_calc, axis=1)"], "cell_type": "code"}, {"metadata": {}, "outputs": [], "execution_count": null, "source": ["colormap = \"jet\"\n", "plt.figure(figsize=(20, 20))\n", "plt.title('Pearson correlation of continuous features', y=1.05, size=15)\n", "sns.heatmap(train_with_calc.corr(),linewidths=0.1,\n", "            vmax=1.0, vmin=-1.0, square=True, cmap=colormap, linecolor='white')"], "cell_type": "code"}, {"metadata": {}, "outputs": [], "execution_count": null, "source": ["colormap = \"jet\"\n", "plt.figure(figsize=(20, 20))\n", "plt.title('Pearson correlation of continuous features', y=1.05, size=15)\n", "sns.heatmap(train_without_calc.corr(),linewidths=0.1,\n", "            vmax=1.0, vmin=-1.0, square=True, cmap=colormap, linecolor='white')"], "cell_type": "code"}, {"metadata": {"collapsed": true}, "outputs": [], "execution_count": null, "source": ["def TrainTestHistogram(train, test, feature):\n", "    fig, axes = plt.subplots(len(feature), 2, figsize=(10, 40))\n", "    fig.tight_layout()\n", "\n", "    left  = 0  # the left side of the subplots of the figure\n", "    right = 0.9    # the right side of the subplots of the figure\n", "    bottom = 0.1   # the bottom of the subplots of the figure\n", "    top = 0.9      # the top of the subplots of the figure\n", "    wspace = 0.3   # the amount of width reserved for blank space between subplots\n", "    hspace = 0.7   # the amount of height reserved for white space between subplot\n", "\n", "    plt.subplots_adjust(left=left, bottom=bottom, right=right, \n", "                        top=top, wspace=wspace, hspace=hspace)\n", "    count = 0\n", "    for i, ax in enumerate(axes.ravel()):\n", "        if i % 2 == 0:\n", "            title = 'Train: ' + feature[count]\n", "            ax.hist(train[feature[count]], bins=30, normed=False)\n", "            ax.set_title(title)\n", "#             ax.text(0, 1.2, train[feature[count]].head(), horizontalalignment='left',\n", "#                     verticalalignment='top', style='italic',\n", "#                 bbox={'facecolor':'red', 'alpha':0.2, 'pad':10}, transform=ax.transAxes)\n", "        else:\n", "            title = 'Test: ' + feature[count]\n", "            ax.hist(test[feature[count]], bins=30, normed=False)\n", "            ax.set_title(title)\n", "#             ax.text(0, 1.2, test[feature[count]].head(), horizontalalignment='left',\n", "#                     verticalalignment='top', style='italic',\n", "#                 bbox={'facecolor':'red', 'alpha':0.2, 'pad':10}, transform=ax.transAxes)\n", "            count = count + 1"], "cell_type": "code"}, {"metadata": {}, "outputs": [], "execution_count": null, "source": ["TrainTestHistogram(train, test, feature_list_bin)"], "cell_type": "code"}, {"metadata": {}, "outputs": [], "execution_count": null, "source": ["TrainTestHistogram(train, test, feature_list_cat)"], "cell_type": "code"}, {"metadata": {}, "outputs": [], "execution_count": null, "source": ["TrainTestHistogram(train, test, feature_list_etc)"], "cell_type": "code"}, {"metadata": {}, "outputs": [], "execution_count": null, "source": ["left  = 0  # the left side of the subplots of the figure\n", "right = 0.9    # the right side of the subplots of the figure\n", "bottom = 0.1   # the bottom of the subplots of the figure\n", "top = 0.9      # the top of the subplots of the figure\n", "wspace = 0.3   # the amount of width reserved for blank space between subplots\n", "hspace = 0.7   # the amount of height reserved for white space between subplot\n", "\n", "fig, axes = plt.subplots(13, 2, figsize=(10, 40))\n", "plt.subplots_adjust(left=left, bottom=bottom, right=right, \n", "                    top=top, wspace=wspace, hspace=hspace)\n", "\n", "for i, ax in enumerate(axes.ravel()):\n", "    title = 'Train: ' + feature_list_etc[i]\n", "    ax.hist(train[feature_list_etc[i]], bins=20, normed=True)\n", "    ax.set_title(title)\n", "    ax.text(0, 1.2, train[feature_list_etc[i]].head(), horizontalalignment='left',\n", "            verticalalignment='top', style='italic',\n", "       bbox={'facecolor':'red', 'alpha':0.2, 'pad':10}, transform=ax.transAxes)"], "cell_type": "code"}, {"metadata": {"collapsed": true}, "outputs": [], "execution_count": null, "source": ["# For ordinal group\n", "ordianal_features = ['ps_ind_01', 'ps_ind_03', 'ps_ind_14', 'ps_ind_15', 'ps_reg_01',\n", "                    'ps_reg_02', 'ps_car_11', 'ps_calc_01', 'ps_calc_02', 'ps_calc_03',\n", "                    'ps_calc_04', 'ps_calc_05', 'ps_calc_06', 'ps_calc_07', 'ps_calc_08',\n", "                    'ps_calc_09', 'ps_calc_10', 'ps_calc_11', 'ps_calc_12', 'ps_calc_13',\n", "                    'ps_calc_14']\n", "\n", "continuous_features = ['ps_reg_03', 'ps_car_12', 'ps_car_13', 'ps_car_14', 'ps_car_15']"], "cell_type": "code"}, {"metadata": {"collapsed": true}, "outputs": [], "execution_count": null, "source": ["ordinal_feature_with_calc = [feature for feature in ordianal_features if 'calc' in feature]\n", "ordinal_feature_without_calc = [feature for feature in ordianal_features if 'calc' not in feature]"], "cell_type": "code"}, {"metadata": {}, "outputs": [], "execution_count": null, "source": ["sns.set(font_scale=1.5)\n", "for i in range(len(feature_list_cat)):\n", "    feature_number = i\n", "    temp_data = train.loc[train[feature_list_cat[feature_number]] != -1]\n", "    g = sns.factorplot(x=feature_list_cat[feature_number], y=\"target\", data=temp_data, kind=\"bar\",\n", "                   size=6, palette = \"muted\")\n", "    g = g.set_ylabels(\"Claim probability\")"], "cell_type": "code"}, {"metadata": {}, "outputs": [], "execution_count": null, "source": ["sns.set(font_scale=1.5)\n", "for i in range(len(feature_list_bin)):\n", "    feature_number = i\n", "    temp_data = train.loc[train[feature_list_bin[feature_number]] != -1]\n", "    g = sns.factorplot(x=feature_list_bin[feature_number], y=\"target\", data=temp_data, kind=\"bar\",\n", "                   size=6, palette = \"muted\")\n", "    g = g.set_ylabels(\"Claim probability\")"], "cell_type": "code"}, {"metadata": {}, "outputs": [], "execution_count": null, "source": ["sns.set(font_scale=1.5)\n", "for i in range(len(ordinal_feature_with_calc)):\n", "    feature_number = i\n", "    temp_data = train.loc[train[ordinal_feature_with_calc[feature_number]] != -1]\n", "    g = sns.factorplot(x=ordinal_feature_with_calc[feature_number], y=\"target\", data=temp_data, kind=\"bar\",\n", "                   size=6, palette = \"muted\")\n", "    g = g.set_ylabels(\"Claim probability\")"], "cell_type": "code"}, {"metadata": {}, "outputs": [], "execution_count": null, "source": ["sns.set(font_scale=1.5)\n", "for i in range(len(ordinal_feature_without_calc)):\n", "    feature_number = i\n", "    temp_data = train.loc[train[ordinal_feature_without_calc[feature_number]] != -1]\n", "    g = sns.factorplot(x=ordinal_feature_without_calc[feature_number], y=\"target\", data=temp_data, kind=\"bar\",\n", "                   size=6, palette = \"muted\")\n", "    g = g.set_ylabels(\"Claim probability\")"], "cell_type": "code"}, {"metadata": {}, "outputs": [], "execution_count": null, "source": ["for feature in continuous_features:\n", "    g = sns.FacetGrid(train, col='target')\n", "    g = g.map(sns.distplot, feature)"], "cell_type": "code"}, {"metadata": {"collapsed": true}, "outputs": [], "execution_count": null, "source": ["def make_overlap_histogram(df, feature, target):\n", "    fig, ax = plt.subplots(figsize=(6, 6))\n", "    g = sns.kdeplot(df[feature][(df[target] == 0) & (df[feature] != -1)], color=\"Red\", shade = True)\n", "    g = sns.kdeplot(df[feature][(df[target] == 1) & (df[feature] != -1)], ax=g, color=\"Blue\", shade= True)\n", "    g.set_xlabel(feature)\n", "    g.set_ylabel(\"Frequency\")\n", "    g = g.legend([\"Not claim\",\"Claim\"])"], "cell_type": "code"}, {"metadata": {"scrolled": false}, "outputs": [], "execution_count": null, "source": ["for feature in continuous_features:\n", "    make_overlap_histogram(train, feature, 'target')"], "cell_type": "code"}, {"metadata": {"collapsed": true}, "outputs": [], "execution_count": null, "source": ["def show_skew(df, feature):\n", "    fig, ax = plt.subplots(figsize=(8, 8))\n", "    g = sns.distplot(train[feature][train[feature] != -1], color=\"m\", label=\"Skewness : %.2f\"%(train[feature].skew()))\n", "    g = g.legend(loc=\"best\")"], "cell_type": "code"}, {"metadata": {}, "outputs": [], "execution_count": null, "source": ["dataset = pd.concat(objs=[train, test], axis=0).reset_index(drop=True)\n", "dataset = dataset.replace(-1, np.NaN)\n", "dataset.tail()"], "cell_type": "code"}, {"metadata": {"scrolled": false}, "outputs": [], "execution_count": null, "source": ["for feature in continuous_features:\n", "    show_skew(dataset, feature)"], "cell_type": "code"}, {"metadata": {"scrolled": false}, "outputs": [], "execution_count": null, "source": ["for feature in continuous_features:\n", "    fig, ax = plt.subplots(figsize=(6, 6))\n", "    temp = dataset[feature].map(lambda i: np.log(i) if i > 0 else 0)\n", "    g = sns.distplot(temp[temp != -1], color=\"m\", label=\"Skewness : %.2f\"%(temp.skew()))\n", "    g = g.legend(loc=\"best\")"], "cell_type": "code"}, {"metadata": {"collapsed": true}, "outputs": [], "execution_count": null, "source": ["feature_list_not_using = ['ps_calc_01', 'ps_calc_02', 'ps_calc_03', 'ps_calc_04', 'ps_calc_08', 'ps_calc_09']\n", "feature_list_log_function = ['ps_car_13']"], "cell_type": "code"}, {"metadata": {"collapsed": true}, "outputs": [], "execution_count": null, "source": ["dataset = dataset.drop(feature_list_not_using, axis=1)"], "cell_type": "code"}, {"metadata": {"collapsed": true}, "outputs": [], "execution_count": null, "source": ["dataset['ps_car_13'] = dataset['ps_car_13'].map(lambda i: np.log(i) + 1 if i > 0 else 0)"], "cell_type": "code"}, {"metadata": {"collapsed": true}, "outputs": [], "execution_count": null, "source": ["for feature in (set(train_nan_columns).union(set(test_nan_columns))):\n", "    if 'cat' in feature or 'bin' in feature:\n", "        # For categorical and binary features with postfix, substitue null values with the most frequent value to avoid float number.\n", "        dataset[feature].fillna(dataset[feature].value_counts().idxmax(), inplace=True)\n", "    elif feature in continuous_features:\n", "        dataset[feature].fillna(dataset[feature].median(), inplace=True)\n", "    elif feature in ordianal_features:\n", "        # For ordinal features which was assumed, substitue null values with the most frequent value to avoid float number.\n", "        dataset[feature].fillna(dataset[feature].value_counts().idxmax(), inplace=True)\n", "    else:\n", "        print(feature)"], "cell_type": "code"}, {"metadata": {}, "outputs": [], "execution_count": null, "source": ["msno.matrix(df=dataset.iloc[:, :], figsize=(20, 14), color=(0.8, 0.5, 0.2))  "], "cell_type": "code"}, {"metadata": {}, "outputs": [], "execution_count": null, "source": ["for feature in feature_list_cat:\n", "    print(\"{}: \\t{}\".format(feature, dataset[feature].value_counts().shape[0]))"], "cell_type": "code"}, {"metadata": {"collapsed": true}, "outputs": [], "execution_count": null, "source": ["def oneHotEncode_dataframe(df, features):\n", "    for feature in features:\n", "        if df[feature].value_counts().shape[0] < 8:\n", "            temp_onehot_encoded = pd.get_dummies(df[feature])\n", "            column_names = [\"{}_{}\".format(feature, x) for x in temp_onehot_encoded.columns]\n", "            temp_onehot_encoded.columns = column_names\n", "            df = df.drop(feature, axis=1)\n", "            df = pd.concat([df, temp_onehot_encoded], axis=1)\n", "        else:\n", "            continue\n", "    return df"], "cell_type": "code"}, {"metadata": {"collapsed": true}, "outputs": [], "execution_count": null, "source": ["dataset = oneHotEncode_dataframe(dataset, feature_list_cat)"], "cell_type": "code"}, {"metadata": {"collapsed": true}, "outputs": [], "execution_count": null, "source": ["x_train = dataset.loc[:train.shape[0]-1, :]\n", "x_test = dataset.loc[train.shape[0]:, :]"], "cell_type": "code"}, {"metadata": {}, "outputs": [], "execution_count": null, "source": ["x_train.tail()"], "cell_type": "code"}, {"metadata": {}, "outputs": [], "execution_count": null, "source": ["x_test.head()"], "cell_type": "code"}, {"metadata": {"collapsed": true}, "outputs": [], "execution_count": null, "source": ["# Define the gini metric - from https://www.kaggle.com/c/ClaimPredictionChallenge/discussion/703#5897\n", "def gini(actual, pred, cmpcol = 0, sortcol = 1):\n", "    assert( len(actual) == len(pred) )\n", "    all = np.asarray(np.c_[ actual, pred, np.arange(len(actual)) ], dtype=np.float)\n", "    all = all[ np.lexsort((all[:,2], -1*all[:,1])) ]\n", "    totalLosses = all[:,0].sum()\n", "    giniSum = all[:,0].cumsum().sum() / totalLosses\n", "    \n", "    giniSum -= (len(actual) + 1) / 2.\n", "    return giniSum / len(actual)\n", " \n", "def gini_normalized(a, p):\n", "    return gini(a, p) / gini(a, a)\n", "\n", "def gini_xgb(preds, dtrain):\n", "    labels = dtrain.get_label()\n", "    gini_score = gini_normalized(labels, preds)\n", "    return 'gini', gini_score"], "cell_type": "code"}, {"metadata": {}, "source": ["I will add ensemble method on this part. \n", "Coming soon :)."], "cell_type": "markdown"}]}