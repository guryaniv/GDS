{"cells":[{"metadata":{"_uuid":"8a994b5558652984553bef5beb4af26954d68e26"},"cell_type":"markdown","source":"This kernel is base on the [Ceshine Lee's kernel](https://www.kaggle.com/ceshine/pytorch-bert-endpointspanextractor-kfold/notebook) and [Matei lonita's kernel](https://www.kaggle.com/mateiionita/taming-the-bert-a-baseline). Thanks for providing a good starting point for this competition."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\nimport zipfile\nimport sys\nimport time\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"930a7efc5e7a5ccf140dc0e59e6cb92a62ffa384","scrolled":false},"cell_type":"code","source":"!conda remove -y greenlet\n!pip install allennlp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"64c3f40f620c50434a4645a76fe5ad1c9d34ec74"},"cell_type":"code","source":"!wget https://raw.githubusercontent.com/google-research-datasets/gap-coreference/master/gap-development.tsv\n!wget https://raw.githubusercontent.com/google-research-datasets/gap-coreference/master/gap-validation.tsv\n!wget https://raw.githubusercontent.com/google-research-datasets/gap-coreference/master/gap-test.tsv\n!ls","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"da0c83431d968925a8761e50526c5160af483809"},"cell_type":"code","source":"from allennlp.commands.elmo import ElmoEmbedder\nfrom allennlp.data.tokenizers import word_tokenizer\nfrom sklearn.preprocessing import OneHotEncoder","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7945fb72466e073bdc2dcba8ce57a379662f6f4a"},"cell_type":"code","source":"def get_elmo_fea(data, op, wg):\n\tdef get_nearest(slot, target):\n\t\tfor i in range(target, -1, -1):\n\t\t\tif i in slot:\n\t\t\t\treturn i\n    # op = 'models/elmo_2x4096_512_2048cnn_2xhighway_options.json'\n    # wg = 'models/elmo_2x4096_512_2048cnn_2xhighway_weights.hdf5'\n\n\telmo = ElmoEmbedder(options_file=op, weight_file=wg, cuda_device=0)\n\n\ttk = word_tokenizer.WordTokenizer()\n\ttokens = tk.batch_tokenize(data.Text)\n\tidx = []\n\n\tfor i in range(len(tokens)):\n\t\tidx.append([x.idx for x in tokens[i]])\n\t\ttokens[i] = [x.text for x in tokens[i]]\n\n\tvectors = elmo.embed_sentences(tokens)\n\n\tans = []\n\tfor i, vector in enumerate([v for v in vectors]):\n\t\tP_l = data.iloc[i].Pronoun\n\t\tA_l = data.iloc[i].A.split()\n\t\tB_l = data.iloc[i].B.split()\n\n\t\tP_offset = data.iloc[i]['Pronoun-offset']\n\t\tA_offset = data.iloc[i]['A-offset']\n\t\tB_offset = data.iloc[i]['B-offset']\n\n\t\tif P_offset not in idx[i]:\n\t\t\tP_offset = get_nearest(idx[i], P_offset)\n\t\tif A_offset not in idx[i]:\n\t\t\tA_offset = get_nearest(idx[i], A_offset)\n\t\tif B_offset not in idx[i]:\n\t\t\tB_offset = get_nearest(idx[i], B_offset)\n\n\t\temb_P = np.mean(vector[1:3, idx[i].index(P_offset), :], axis=0, keepdims=True)\n\n\t\temb_A = np.mean(vector[1:3, idx[i].index(A_offset):idx[i].index(A_offset) + len(A_l), :], axis=(1, 0),\n                        keepdims=True)\n\t\temb_A = np.squeeze(emb_A, axis=0)\n\n\t\temb_B = np.mean(vector[1:3, idx[i].index(B_offset):idx[i].index(B_offset) + len(B_l), :], axis=(1, 0),\n                        keepdims=True)\n\t\temb_B = np.squeeze(emb_B, axis=0)\n        \n\t\tans.append(np.concatenate([emb_A, emb_B, emb_P], axis=1))\n\n\temb = np.concatenate(ans, axis=0)  \n\treturn emb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6c992f5e4143b4e0c0beafeed1981270447a72a5"},"cell_type":"code","source":"def _row_to_y(row):\n\tif row.loc['A-coref']:\n\t\treturn 0\n\tif row.loc['B-coref']:\n\t\treturn 1\n\treturn 2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8298947fa33285e722bdaae2df41bca5dd795732"},"cell_type":"code","source":"enc = OneHotEncoder(handle_unknown='ignore')\nop = \"https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_options.json\"\nwg = \"https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_weights.hdf5\"\n\nprint(\"Started at \", time.ctime())\ntest_data = pd.read_csv(\"gap-test.tsv\", sep = '\\t')\nX_test = get_elmo_fea(test_data, op, wg)\nY_test = test_data.apply(_row_to_y, axis=1)\n\nvalidation_data = pd.read_csv(\"gap-validation.tsv\", sep = '\\t')\nX_validation = get_elmo_fea(validation_data, op, wg)\nY_validation = validation_data.apply(_row_to_y, axis=1)\n\ndevelopment_data = pd.read_csv(\"gap-development.tsv\", sep = '\\t')\nX_development = get_elmo_fea(development_data, op, wg)\nY_development = development_data.apply(_row_to_y, axis=1)\n\nprint(\"Finished at \", time.ctime())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"29ba41d2570238ec22735909c76cd86c2742517c"},"cell_type":"code","source":"from keras import backend, models, layers, initializers, regularizers, constraints, optimizers\nfrom keras import callbacks as kc\nfrom keras import optimizers as ko\n\nfrom sklearn.model_selection import cross_val_score, KFold, train_test_split\nfrom sklearn.metrics import log_loss\nimport time\n\n\ndense_layer_sizes = [37]\ndropout_rate = 0.6\nlearning_rate = 0.001\nn_fold = 5\nbatch_size = 32\nepochs = 1000\npatience = 100\n# n_test = 100\nlambd = 0.1 # L2 regularization","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"351a99cfb2b3616216fe17f78c4c70484704c2da"},"cell_type":"markdown","source":"We define a model with two hidden layers and one output layer in Keras."},{"metadata":{"trusted":true,"_uuid":"763ec8591474c45d6b065cad0c7efc2bbe9ad514"},"cell_type":"code","source":"def build_mlp_model(input_shape):\n\tX_input = layers.Input(input_shape)\n\n\t# First dense layer\n\tX = layers.Dense(dense_layer_sizes[0], name = 'dense0')(X_input)\n\tX = layers.BatchNormalization(name = 'bn0')(X)\n\tX = layers.Activation('relu')(X)\n\tX = layers.Dropout(dropout_rate, seed = 7)(X)\n\n\t# Second dense layer\n# \tX = layers.Dense(dense_layer_sizes[0], name = 'dense1')(X)\n# \tX = layers.BatchNormalization(name = 'bn1')(X)\n# \tX = layers.Activation('relu')(X)\n# \tX = layers.Dropout(dropout_rate, seed = 9)(X)\n\n\t# Output layer\n\tX = layers.Dense(3, name = 'output', kernel_regularizer = regularizers.l2(lambd))(X)\n\tX = layers.Activation('softmax')(X)\n\n\t# Create model\n\tmodel = models.Model(input = X_input, output = X, name = \"classif_model\")\n\treturn model","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"eb7ea0c73fe3479ae464df563c90965b0d179db6"},"cell_type":"markdown","source":"We use the method defined above to parse the contextual embeddings, for each of the 3 GAP data files. The variable names here may be a bit counter-intuitive. Keep in mind that we will use X_test and X_validation for training, and then make predictions on X_development."},{"metadata":{"trusted":true,"_uuid":"3b942d4712312b1c03b657b83aafd6d6595179c4"},"cell_type":"code","source":"# There may be a few NaN values, where the offset of a target word is greater than the max_seq_length of BERT.\n# They are very few, so I'm just dropping the rows.\nremove_test = [row for row in range(len(X_test)) if np.sum(np.isnan(X_test[row]))]\nX_test = np.delete(X_test, remove_test, 0)\nY_test = np.delete(Y_test, remove_test, 0)\n\nremove_validation = [row for row in range(len(X_validation)) if np.sum(np.isnan(X_validation[row]))]\nX_validation = np.delete(X_validation, remove_validation, 0)\nY_validation = np.delete(Y_validation, remove_validation, 0)\n\n# We want predictions for all development rows. So instead of removing rows, make them 0\nremove_development = [row for row in range(len(X_development)) if np.sum(np.isnan(X_development[row]))]\nX_development[remove_development] = np.zeros(3*1024)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"25de1c995bf67e68ffd342063eb5a363c43fe459"},"cell_type":"code","source":"# Will train on data from the gap-test and gap-validation files, in total 2454 rows\nX_train = np.concatenate((X_test, X_validation), axis = 0)\nY_train = np.concatenate((Y_test, Y_validation), axis = 0)\n\n# Will predict probabilities for data from the gap-development file; initializing the predictions\nprediction = np.zeros((len(X_development),3)) # testing predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2538dcc94af4b17eb2c34112f62cf5244d2329ca"},"cell_type":"code","source":"# Training and cross-validation\nfolds = KFold(n_splits=n_fold, shuffle=True, random_state=3)\nscores = []\nfor fold_n, (train_index, valid_index) in enumerate(folds.split(X_train)):\n\t# split training and validation data\n\tprint('Fold', fold_n, 'started at', time.ctime())\n\tX_tr, X_val = X_train[train_index], X_train[valid_index]\n\tY_tr, Y_val = Y_train[train_index], Y_train[valid_index]\n\n\t# Define the model, re-initializing for each fold\n\tclassif_model = build_mlp_model([X_train.shape[1]])\n\tclassif_model.compile(optimizer = optimizers.Adam(lr = learning_rate), loss = \"sparse_categorical_crossentropy\")\n\tcallbacks = [kc.EarlyStopping(monitor='val_loss', patience=patience, restore_best_weights = True)]\n\n\t# train the model\n\tclassif_model.fit(x = X_tr, y = Y_tr, epochs = epochs, batch_size = batch_size, callbacks = callbacks, validation_data = (X_val, Y_val), verbose = 0)\n\n\t# make predictions on validation and test data\n\tpred_valid = classif_model.predict(x = X_val, verbose = 0)\n\tpred = classif_model.predict(x = X_development, verbose = 0)\n\n\t# oof[valid_index] = pred_valid.reshape(-1,)\n\tscores.append(log_loss(Y_val, pred_valid))\n\tprediction += pred\nprediction /= n_fold\n\n# Print CV scores, as well as score on the test data\nprint('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\nprint(scores)\nprint(\"Test score:\", log_loss(Y_development,prediction))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"198c1b2df108afcf2292538c1b12b10c05d9c12b"},"cell_type":"code","source":"# Write the prediction to file for submission\nsubmission = pd.read_csv(\"../input/sample_submission_stage_1.csv\", index_col = \"ID\")\nsubmission[\"A\"] = prediction[:,0]\nsubmission[\"B\"] = prediction[:,1]\nsubmission[\"NEITHER\"] = prediction[:,2]\nsubmission.to_csv(\"submission_bert.csv\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}